---

title: WAN optimizer for logical networks
abstract: Some embodiments provide a non-transitory machine readable medium of a controller of a network control system for configuring a wide area network (WAN) optimizer instance to implement a WAN optimizer for a logical network. The controller receives a configuration for the WAN optimizer to optimize network data from the logical network for transmission to another WAN optimizer. The controller identifies several other controllers in the network control system on which to implement the logical network. The controller distributes the configuration for implementation on the WAN optimizer.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09172603&OS=09172603&RS=09172603
owner: NICIRA, INC.
number: 09172603
owner_city: Palo Alto
owner_country: US
publication_date: 20121115
---
This application claims the benefit of U.S. Provisional Application 61 560 279 entitled Virtual Middlebox Services filed Nov. 15 2011. U.S. Application 61 560 279 is incorporated herein by reference.

Many current enterprises have large and sophisticated networks comprising switches hubs routers middleboxes e.g. wide area network WAN optimizers servers workstations and other networked devices which support a variety of connections applications and systems. The increased sophistication of computer networking including virtual machine migration dynamic workloads multi tenancy and customer specific quality of service and security configurations require a better paradigm for network control. Networks have traditionally been managed through low level configuration of individual network components. Network configurations often depend on the underlying network for example blocking a user s access with an access control list ACL entry requires knowing the user s current IP address. More complicated tasks require more extensive network knowledge forcing guest users port 80 traffic to traverse an HTTP proxy requires knowing the current network topology and the location of each guest. This process is of increased difficulty where the network switching elements are shared across multiple users.

In response there is a growing movement towards a new network control paradigm called Software Defined Networking SDN . In the SDN paradigm a network controller running on one or more servers in a network controls maintains and implements control logic that governs the forwarding behavior of shared network switching elements on a per user basis. Making network management decisions often requires knowledge of the network state. To facilitate management decision making the network controller creates and maintains a view of the network state and provides an application programming interface upon which management applications may access a view of the network state.

Some of the primary goals of maintaining large networks including both datacenters and enterprise networks are scalability mobility and multi tenancy. Many approaches taken to address one of these goals results in hampering at least one of the others. For instance one can easily provide network mobility for virtual machines within an L2 domain but L2 domains cannot scale to large sizes. Furthermore retaining user isolation greatly complicates mobility. As such improved solutions that can satisfy the scalability mobility and multi tenancy goals are needed.

Some embodiments provide a non transitory machine readable medium of a controller of a network control system for configuring a wide area network WAN optimizer instance to implement a WAN optimizer for a logical network. The controller receives a configuration for the WAN optimizer to optimize network data from the logical network for transmission to another WAN optimizer. The controller identifies several other controllers in the network control system on which to implement the logical network. The controller distributes the configuration for implementation on the WAN optimizer.

The preceding Summary is intended to serve as a brief introduction to some embodiments of the invention. It is not meant to be an introduction or overview of all inventive subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly to understand all the embodiments described by this document a full review of the Summary Detailed Description and the Drawings is needed. Moreover the claimed subject matters are not to be limited by the illustrative details in the Summary Detailed Description and the Drawing but rather are to be defined by the appended claims because the claimed subject matters can be embodied in other specific forms without departing from the spirit of the subject matters.

In the following detailed description of the invention numerous details examples and embodiments of the invention are set forth and described. However it will be clear and apparent to one skilled in the art that the invention is not limited to the embodiments set forth and that the invention may be practiced without some of the specific details and examples discussed.

Some embodiments provide a non transitory machine readable medium of a controller of a network control system for configuring a wide area network WAN optimizer instance to implement a WAN optimizer for a logical network. The controller receives a configuration for the WAN optimizer to optimize network data from the logical network for transmission to another WAN optimizer. The controller identifies several other controllers in the network control system on which to implement the logical network. The controller distributes the configuration for implementation on the WAN optimizer.

Several more detailed embodiments of the invention are described in the sections below. Section I conceptually describes details of several types of WAN optimizer deployments in a logical network according to some embodiments of the invention. Next Section II conceptually describes details of the managed network architecture that is used to implement a logical network according to some embodiments of the invention. Section III follows this with a description of a configuration data flow for the managed network architecture of some embodiments. Next Section IV describes several logical processing examples according to some embodiments of the invention. Finally Section V describes an electronic system that implements some embodiments of the invention.

In some embodiments a logical data path set defines a logical network element. A logical data path set in some embodiments is a set of network data paths through the set of network infrastructure switching elements that implement the logical network element and the logical network element s defined functionalities. As such the logical network in this example is a conceptual representation of the LDPS specified by the user.

As shown the logical network includes a logical layer 3 L3 router the WAN optimizer logical layer 2 L2 switches and and VMs . The L3 router handles layer 3 routing of network data e.g. packets between the L2 switches and the WAN optimizer and a WAN . The L2 switch forwards network data between the L3 router and the VMs and while the L2 switch forwards network data between the L3 router and the VMs .

The VMs of some embodiments are host machines implemented as virtual machines running on separate and or shared physical machines. The VMs of some embodiments are each assigned a set of network layer host addresses e.g. a MAC address for network layer 2 an IP address for network layer 3 etc. and can send and receive network data to and from other network elements over the network.

In some embodiments the WAN optimizer is a middlebox device for increasing the efficiency of data transfers across the WAN e.g. accelerating the flow of data across the WAN . In some embodiments the WAN optimizer is implemented as a physical device a set of physical devices e.g. a cluster of physical devices a virtual machine a software application or module running on a computing device or a virtual machine etc. The WAN optimizer of different embodiments use any number of different WAN optimization techniques to increase the efficiency of data transfers across the WAN . Examples of WAN optimization techniques include data deduplication data compression latency optimization caching and or proxying forward error correction protocol spoofing traffic shaping equalizing connection limiting simple rate limiting etc.

As shown in the WAN optimizer is arranged as bounded or isolated by the L3 router in the logical topology of the logical network . That is network data must go through the L3 router in order to reach the WAN optimizer . As such network data from within the logical network that is specified 1 to be sent over the WAN and 2 to be processed by the WAN optimizer is sent to the WAN optimizer through the L3 router . After the WAN optimizer processes such network data the data is sent back through the L3 router before it is sent over the WAN .

In some embodiments the WAN optimizer generates a copy of the processed network data and sends the copy of the processed network data back to the L3 router for the L3 router to send over the WAN . In other words the L3 router receives back from the WAN optimizer new network data new packets that is generated by the WAN optimizer . In some embodiments the arrangement of the WAN optimizer is referred to as a one armed out of path deployment or a one arm deployment.

In addition to the logical network illustrates a shared public service that includes a host and a WAN optimizer a private data center Internet and the WAN . In some embodiments the WAN is a network that spans a large area e.g. a city a county a region a state a country etc. . The WAN of some embodiments is used to connect networks e.g. local area networks LANs campus area networks CANs metropolitan area networks MANs etc. public and or private together to allow communication between the networks. As shown the WAN facilitates communication between the logical network the private data center and the shared public service through the Internet .

In some embodiments the Internet is a large public network of networks that connects computing devices around the world. The transmission control protocol TCP Internet protocol IP is used as a communication protocol through the Internet in some embodiments. As illustrated in the Internet provides communication between the shared public service and the WAN .

In some embodiments the private data center is a dedicated space that contains anywhere from several computing devices to hundreds of computing devices not shown in . Some or all of the computing devices are used to provide any number of different services and or functions such as email proxy and domain name system DNS servers web hosting application servers file servers data backup etc. In some embodiments some or all of the computing devices are used for hosting virtual machines that in turn are used to provide any number of the aforementioned services and or functions.

The shared public service of some embodiments is a service available to the public that is accessible through the Internet . Examples of a shared public service include workloads hosted in public clouds e.g. infrastructure as a service software as a service platform as a service other cloud computing services etc. In some embodiments the host is a machine e.g. a computing device a virtual machine etc. that provides a service for the shared public service .

In some embodiments the WAN optimizer is similar to the WAN optimizer . In other words the WAN optimizer of some such embodiments is a middlebox device for increasing the efficiency of data transfers across the WAN between the VM and the host in this example . In some embodiments the WAN optimizer is implemented as a physical device a set of physical devices e.g. a cluster of physical devices a virtual machine a software application or module running on a computing device or a virtual machine etc. In different embodiments the WAN optimizer use any number of the different WAN optimization techniques mentioned above e.g. data deduplication data compression latency optimization caching and or proxying forward error correction protocol spoofing traffic shaping equalizing connection limiting simple rate limiting etc. to increase the efficiency of data transfers across the WAN .

In this example network data communicated between VM in the logical network and the host in the shared public service is optimized by the WAN optimizer and the WAN optimizer . In some embodiments the WAN optimizer is referred to as a local endpoint and the WAN optimizer is referred to as a remote endpoint. As shown in a dotted line represents the path of the optimized network data. Specifically the L2 switch forwards the network data received from the VM to the L3 router . When the L3 router receives the network data the L3 router routes it to the WAN optimizer for processing. After the WAN optimizer processes the network data e.g. compresses the network data and returns the optimized data to the L3 router the L3 router routes the network data over the WAN and the Internet to the shared public service . When the shared public service receives the optimized data the WAN optimizer processes the network data e.g. decompresses the network data and sends the data to the host .

In some embodiments the extender enables communication between hosts in a managed network and hosts in unmanaged networks. For this example the extender facilitates communication between the VMs which are part of a managed network and the host which is part of an unmanaged network. In some embodiments the extender is implemented as a physical machine e.g. a computing device such as computer system while in other embodiments the extender is implemented as a virtual machine not shown in running on a physical machine. As shown the extender includes a software switching element referred to as an Open Virtual Switch OVS for forwarding and routing network data between network elements coupled to the OVS the WAN optimizer and the OVSs in this example . In some embodiments the OVS and the WAN optimizer communicate with each other through a tunnel e.g. a generic routing encapsulation GRE tunnel a Control And Provisioning of Wireless Access Points CAPWAP tunnel a web cache communication protocol WCCP tunnel etc. .

The hosts are physical machines e.g. computing devices such as computer system in some embodiments. As shown the hosts each includes an OVS software switching element for forwarding and routing network data between network elements coupled to the OVSs the VMs and the extender in this example . In some embodiments the OVSs operate in a virtual machine running on the hosts .

The OVSs of some embodiments are referred to as edge switching elements because they are managed switching elements at the edge of the network infrastructure. That is the OVSs are directly connected to network hosts the VMs in this example . In contrast a non edge switching element the pool node in this example is a switching element that interconnects the edge switching elements. In some embodiments non edge switching elements are referred to as interior switching elements. Additionally in some embodiments the OVSs are referred to as managed switching elements as they are managed by a network control system in some embodiments as opposed to unmanaged switches which are not managed by the network control system in the network in order to implement the logical network . Each of the OVSs communicates with each of the other OVSs through tunnels e.g. a GRE tunnel a CAPWAP tunnel a WCCP tunnel etc. in some embodiments.

As described above by reference to a user in some embodiments specifies the logical network by providing input that describes an LDPS which is conceptually represented by the logical network and is implemented by a set of managed switching elements. For this example the OVSs are used to implement the LDPS. As explained below to configure the set of managed switching elements the network control system of some embodiments receives input from the user and converts the user provided data into logical control plane LCP data and then converts the LCP data into logical forward plane LFP data which the network control system in turn converts into physical control plane PCP data. The network control system sends the PCP data to the set of managed switching elements the OVSs in this example to convert to physical forwarding plane PFP data in order to implement the LDPS described by the user provided LCP data.

In some embodiments the network control system converts the LFP data to universal PCP UPCP . UPCP data in some embodiments is a data plane that enables the control system of some embodiments to scale even when it contains a large number of managed switching elements e.g. thousands to implement a LDPS. The UPCP abstracts common characteristics of different managed switching elements in order to express PCP data without considering differences in the managed switching elements and or location specifics of the managed switching elements.

In some embodiments network control system translates the UPCP data into customized PCP CPCP data for each managed switching element in order to completely implement LDPSs at the managed switching elements. In some such embodiments the network control system 1 generates CPCP data for each managed switching element by expanding the UPCP data to characteristics specific and or local to the managed switching element e.g. ports on the managed switching element and 2 sends the CPCP data to the managed switching element.

Instead of generating CPCP for each of the managed switching elements the network control system of some embodiments sends the UPCP data to each of the managed switching elements for the managed switching elements to each generate its own CPCP data which is used to generate PFP data for the managed switching element. To communicate with and configure the managed switching elements the network control system of some embodiments uses the OpenFlow or OVS application programming interfaces APIs provided by the managed switching elements.

To configure the WAN optimizer the network control system of some embodiments pushes the user provided WAN optimizer configuration data to the WAN optimizer through a set of APIs provided by the WAN optimizer . In some embodiments the WAN optimizer configuration data includes 1 local endpoint information related to a local WAN optimizer such as a name for the local endpoint an IP address of the local WAN optimizer and in some cases an external interface of the local WAN optimizer and 2 remote endpoint information related to a remote WAN optimizer such as a name for the remote endpoint an IP address of the remote WAN optimizer and a set of rules for filtering network data passing through the remote WAN optimizer. The set of rules in some embodiments includes a combination of any number of a source IP subnet a destination IP subnet and a list of ports or port ranges and a name of an optimization profile which is described below.

The WAN optimizer configuration data includes information for a deduplication feature provided by a WAN optimizer of some embodiments. Such information includes in some embodiments a deduplication flag for enabling and disabling the deduplication feature a cache size for the deduplication feature and a dedpulication mode that specifies a storage medium e.g. memory disk hybrid memory and disk etc. to which the deduplication feature is applied.

An optimization profile specifies the manner in which network data passing through a WAN optimizer is optimized. In some embodiments information for an optimization profile includes a name of the optimization profile information for an application profile a deduplication flag for enabling and disabling a deduplication feature a compression flag for enabling and disabling a compression feature and a transparency flag for enabling and disabling an IP transparency feature. In some embodiments the application profile information may include an application protocol a set of destination ports of the application protocol and a key value pair specific to the application protocol. The WAN optimizer of some such embodiments optimizes network data using the application protocol specified in the application profile.

Different embodiments use any number of additional and different WAN optimizer configuration to configure a WAN optimizer. For instance in some embodiments the WAN optimizer configuration data includes an enable flag for enabling and disabling a WAN optimizer a logging setting for specifying a setting of the standard logging feature.

In some embodiments the network control system also pushes attachment data along with the WAN optimizer configuration data to the WAN optimizer through the set of APIs. In some embodiments the attachment data for the WAN optimizer includes a tunnel type e.g. a GRE tunnel a CAPWAP tunnel a WCCP tunnel etc. for the WAN optimizer to use for sending and receiving network data to and from each of the OVSs . The tunnel type is specified by the user as part of the WAN optimizer configuration data in some embodiments while the network control system automatically determines the tunnel type in other embodiments.

In some embodiments the network control system generates slicing data for the WAN optimizer and pushes this data along with the WAN optimizer configuration data to the WAN optimizer through the set of APIs. The slicing data of some embodiments includes a unique identifier associated with a middlebox in a logical network e.g. the WAN optimizer in the logical network described above by reference to . In some embodiments the WAN optimizer uses the unique identifiers of slicing data to implement 1 different WAN optimizers for a particular logical network and or 2 different WAN optimizers for multiple different logical networks.

As illustrated in the WAN optimizer is arranged as bounded or isolated by the extender in the physical network architecture. That is network data must go through the extender in order to reach the WAN optimizer . Accordingly network data from the VMs that is specified 1 to be sent over that WAN and 2 to be processed by the WAN optimizer is sent through the extender to the WAN optimizer for processing and then back through the extender for the extender to sent over the WAN .

For this example network data communicated between VM in the physical network architecture and the host in the shared public service is optimized by the WAN optimizer and the WAN optimizer . The path of the optimized network data is illustrated in by a dotted line. In particular the OVS forwards the network data received from the VM to the extender which routes the network data to the WAN optimizer for processing. After the WAN optimizer processes the network data e.g. compresses the network data and returns the optimized data to the extender the extender routes it over the WAN and the Internet to the shared public service . When the shared public service receives the optimized data the WAN optimizer processes the network data e.g. decompresses the network data and sends the data to the host .

As described above by reference to some embodiments utilize a one arm deployment of a WAN optimizer in a logical network. Alternatively or in conjunction with the one arm deployment some embodiments deploy a WAN optimizer differently.

As illustrated in the logical network includes the L3 router the WAN optimizer the L2 switches and and the VMs . In this example the WAN optimizer is arranged between the L3 router and the WAN in the logical topology of the logical network . Under this type of deployment of the WAN optimizer network data that is specified to be sent over the WAN must pass through the WAN optimizer regardless of whether the network data is specified to be processed by the WAN optimizer . In some embodiments the arrangement of the WAN optimizer shown in is referred to as a physical in band deployment or an in line deployment.

In addition illustrates the shared public service the private data center that includes a host and the WAN optimizer the Internet and the WAN . As shown the host is a machine e.g. a computing device a virtual machine etc. within the private data center . The WAN optimizer in this example is increasing the efficiency of data transfers across the WAN between the VM and the host . That is network data communicated between VM in the logical network and the host in the private data center is optimized by the WAN optimizer and the WAN optimizer .

As shown in a dotted line represents the path of the optimized network data. Specifically the L2 switch forwards the network data received from the VM to the L3 router . When the L3 router receives the network data the L3 router routes it to the WAN optimizer for processing. Once the WAN optimizer processes the network data e.g. compresses the network data the WAN optimizer sends the network data over the WAN to the private data center . When the private data center receives the optimized data the WAN optimizer processes the network data e.g. decompresses the network data and sends the data to the host .

The extender is similar to the extender described above by reference to to the extent that the extender 1 enables communication between hosts in a managed network and hosts in unmanaged networks the VMs which are part of a managed network and the host which is part of an unmanaged network in this example and 2 forwards and routes network data between network elements coupled to the extender the WAN optimizer and the managed switching elements in this example . In some embodiments the extender is implemented as a physical switching element a virtual switching element a software switching element e.g. an OVS switching element or any other type of network element that is capable of forwarding and routing network data. The extender and the WAN optimizer communicate with each other through a tunnel e.g. a generic routing encapsulation GRE tunnel a Control And Provisioning of Wireless Access Points CAPWAP tunnel a web cache communication protocol WCCP tunnel etc. in some embodiments.

In some embodiments the managed switching elements are switching elements that forward and route network data between network elements coupled to the managed switching elements . Like the extender each of the managed switching elements is implemented as a physical switching element a virtual switching element a software switching element e.g. an OVS switching element or any other type of network element that is capable of forwarding and routing network data. In some embodiments each of the managed switching elements communicates with each of the other managed switching elements through tunnels e.g. a GRE tunnel a CAPWAP tunnel a WCCP tunnel etc. .

In some embodiments the managed switching elements are referred to as edge switching elements because they are managed switching elements at the edge of the network infrastructure. That is the managed switching elements are directly connected to network hosts the VMs in this example . On the other hand a non edge switching element the extender in this example which is also referred to as an interior switching element is a switching element that interconnects the edge switching elements.

As described above by reference to a user in some embodiments specifies the logical network by providing input that describes an LDPS which is conceptually represented by the logical network and is implemented by a set of managed switching elements. For this example the managed switching elements are used to implement the LDPS. As explained below to configure the set of managed switching elements the network control system of some embodiments converts the user provided data into LCP data and then converts the LCP data into LFP data which the network control system in turn converts into PCP data. The network control system sends the PCP data to the managed switching elements to convert to PFP data in order to implement the LDPS described by the user provided LCP data.

In some embodiments the network control system converts the LFP data to UPCP data and generates CPCP data for each of the managed switching elements. As mentioned above UPCP data in some embodiments is a data plane that enables the control system of some embodiments to scale even when it contains a large number of managed switching elements e.g. thousands to implement a LDPS. The UPCP abstracts common characteristics of different managed switching elements in order to express PCP data without considering differences in the managed switching elements and or location specifics of the managed switching elements.

In some embodiments network control system translates the UPCP data into customized PCP CPCP data for each managed switching element in order to completely implement LDPSs at the managed switching elements. In some such embodiments the network control system 1 generates CPCP data for each managed switching element by expanding the UPCP data to characteristics specific and or local to the managed switching element e.g. ports on the managed switching element and 2 sends the CPCP data to the managed switching element.

Instead of generating CPCP for each of the managed switching elements the network control system of some embodiments sends the UPCP data to each of the managed switching elements for the managed switching elements to each generate its own CPCP data which is used to generate PFP data for the managed switching element. To communicate with and configure the managed switching elements the network control system of some embodiments uses the OpenFlow or OVS APIs provided by the managed switching elements.

To configure the WAN optimizer the network control system of some embodiments pushes the user provided WAN optimizer configuration data to the WAN optimizer through a set of APIs provided by the WAN optimizer . In some embodiments the network control system also pushes attachment data along with the WAN optimizer configuration data to the WAN optimizer through the set of APIs. In some embodiments the attachment data for the WAN optimizer includes a tunnel type e.g. a GRE tunnel a CAPWAP tunnel a WCCP tunnel etc. for the WAN optimizer to use for sending to and receiving from network data to each of the managed switching elements . The tunnel type is specified by the user as part of the WAN optimizer configuration data in some embodiments while the network control system automatically determines the tunnel type in other embodiments.

In some embodiments the network control system generates slicing data for the WAN optimizer and pushes this data along with the WAN optimizer configuration data to the WAN optimizer through the set of APIs. The slicing data of some embodiments includes a unique identifier associated with a middlebox in a logical network e.g. the WAN optimizer in the logical network described above by reference to . In some embodiments the WAN optimizer uses the unique identifiers of slicing data to implement 1 different WAN optimizers for a particular logical network and or 2 different WAN optimizers for multiple different logical networks.

In this example network data communicated between VM in the physical network architecture and the host in the private data center is optimized by the WAN optimizer and the WAN optimizer . A dotted line shown in represents the path of the optimized network data. Specifically the managed switching element forwards the network data received from the VM to the extender which routes the network data to the WAN optimizer for processing. Once the WAN optimizer processes the network data e.g. compresses the network data the WAN optimizer sends it over the WAN to the private data center . When the private data center receives the optimized data the WAN optimizer processes the network data e.g. decompresses the network data and sends the data to the host .

While illustrate a particular arrangement of networks and network elements one of ordinary skill in the art will realize that different arrangements are possible in different embodiments. For instance in some embodiments just a WAN as opposed to a WAN and the Internet may facilitate communication between the shared public service the private data center and the logical network.

As described above the network control system of some embodiments manages a set of switching elements in the physical network infrastructure in order to implement LDPSs i.e. logical networks . conceptually illustrates a managed network architecture of some embodiments that is used to implement a logical network e.g. the logical networks and described above by reference to respectively . Specifically illustrates a user a logical controller physical controllers and managed switching elements and a virtual machine VM that implements a WAN optimizer of some embodiments.

In some embodiments each of the controllers in a network control system has the capability to function as a logical controller and or physical controller. Alternatively in some embodiments a given controller may only have the functionality to operate as a particular one of the types of controller e.g. as a physical controller . In addition different combinations of controllers may run in the same physical machine. For instance the logical controller and the physical controller may run in the same computing device with which a user interacts.

The logical controller in some embodiments is responsible for implementing LDPSs by computing UPCP data e.g. universal flow entries that are generic expressions of flow entries for the physical controllers and and the managed switching elements to implement the LDPSs. For a particular LDPS only one logical controller is responsible for implementing the particular LDPS e.g. is a master of the particular LDPS in some such embodiments. However more than one logical controller can be masters of the same LDPS in some embodiments. In addition a logical controller of some embodiments can be the master of more than one LDPS.

As noted above in some embodiments a user specifies a logical network by providing input that describes an LDPS. The input might be related to creating a logical network modifying the logical network and or deleting the logical network in some embodiments. In this example the logical controller allows the user to specify a logical network through the logical controller . When the user specifies a WAN optimizer for the logical network the user may also provide policy based routing data that specifies the type of network data to be optimized by the WAN optimizer.

In some embodiments the logical controller includes an input module not shown in such as an input translation application for translating the input provided by the user into LCP data while in other embodiments the input module runs on a separate controller and the logical controller receives the LCP data from the input module on the separate controller. The logical controller of some embodiments provides the user input to the input module in the form of API calls. In some embodiments the logical controller also includes a control module e.g. a control application that generates LFP data from the LCP data output by the input module. The logical controller of some embodiments further includes a virtualization module e.g. a virtualization application that generates UPCP from the LFP data output by the control module and sends the UPCP data to the physical controllers and .

In some embodiments a logical controller identifies a set of physical controllers that are masters of the managed switching elements that implement LDPSs. In this example the managed switching elements are responsible for implementing LDPSs and thus the logical controller identifies the physical controllers and and sends each of the physical controllers and the generated UPCP data.

When the user specifies a WAN optimizer for the logical network the logical controller of some embodiments identifies WAN optimizer data for creating a WAN optimizer service instance on the VM and configuring the WAN optimizer service instance . In some embodiments the logical controller sends WAN optimizer data to the physical controllers and along with the generated UPCP data.

In some embodiments only one physical controller manages a particular managed switching element. For this example only the physical controller manages the managed switching elements and and only the physical controller manages the managed switching elements and . The physical controllers and of some embodiments generate CPCP data e.g. customized flow entries from universal flow entries and push these CPCP data down to the managed switching elements and the WAN optimizer s running on the VM . Alternatively the physical controllers and of some embodiments push the UPCP data to the managed switching elements and the managed switching elements each generates CPCP data for its own respective managed switching element.

In some embodiments the physical controllers and access the managed switching elements by using the OpenFlow or OVS APIs provided by the switching elements. Additionally the physical controllers and uses a set of APIs to create a WAN optimizer service instance on the VM and to send WAN optimizer data to the WAN optimizer service instance .

For a VM that implements WAN optimizer service instances only one physical controller is responsible for managing the VM in some embodiments. As shown in the physical controller manages the VM . To configure a WAN optimizer service instance on the VM the physical controller of some embodiments pushes user provided WAN optimizer configuration data to the VM through a set of APIs provided by the VM . In some embodiments the physical controller also pushes attachment data to the VM through the set of APIs. The attachment data in some embodiments includes a tunnel type e.g. a GRE tunnel a CAPWAP tunnel a WCCP tunnel etc. for the WAN optimizer service instance to use for sending and receiving network data to and from each of the managed switching elements . In some embodiments the tunnel type is specified by the user as part of the WAN optimizer configuration data while in other embodiments the physical controller automatically determines the tunnel type.

In some embodiments the physical controller generates slicing data for the WAN optimizer service instance and pushes this data along with the WAN optimizer configuration data to the VM through the set of APIs. As mentioned above the slicing data of some embodiments includes a unique identifier associated with a middlebox in a logical network e.g. the WAN optimizer in the logical network described above by reference to .

As explained above the managed switching elements of some embodiments handle the implementation of LDPSs. In some embodiments the managed switching elements implement LDPSs by generating PFP data based on the CPCF that the managed switching elements receives from the physical controllers and . Instead of receiving CPCP data the managed switching elements of some embodiments receives UPCP data from the physical controllers and . In some such embodiments each of the managed switching elements generates CPCP data from the UPCP data and then generates the PFP data from the generated CPCP data.

In some embodiments the VM receives configuration data from the physical controller and in response translates the configuration data into a form that is usable by the VM . For instance in some embodiments the WAN optimizer configuration data is in a particular language that expresses the packet processing analysis modification etc. rules. The VM of some such embodiments compiles these rules into more optimized packet classification rules. In some embodiments this transformation is similar to the PCP data to PFP data translation. When the VM receives a packet the VM applies the compiled optimized rules in order to efficiently and quickly perform its operations on the packet. In some embodiments the VM is a physical device a set of physical devices e.g. a cluster of physical devices a software application or module running on a computing device or a virtual machine etc.

The virtual machine is responsible for creating and managing WAN optimizer service instances in some embodiments. When the virtual machine receives a request from one of the physical controllers and through an API to create a WAN optimizer service instance the virtual machine instantiates a WAN optimizer service instance and configures it using the WAN optimizer data received from one of the physical controllers and to configure the WAN optimizer service instance . In some embodiments the VM sends to the logical controller state information and or statistical information regarding a particular WAN optimizer service instance when the VM receives requests for such information from the logical controller through API calls.

In some embodiments the logical controller the physical controllers and and the managed switching elements use a table mapping engine referred to as nLog that is based on a variation of the datalog database language in order to generate the different types of data e.g. LCP data LFP data UPCP data CPCP data PFP data WAN configuration data etc. . For instance the logical controller inputs LCP data to an input table of the table mapping engine of some embodiments and the table mapping engine automatically generates LFP data which the table mapping engine stores in one of its output tables. Details of the table mapping engine of some embodiments are described below by reference to .

In addition to processing input provided by the user the managed network architecture illustrated in processes non user changes to LDPSs. The logical controller computes UPCP data based on the changes and propagates the UPCP to the physical controllers and to in turn propagate to the managed switching elements and the virtual machine that implements the WAN optimizer s .

The previous Section II describes several examples of managed network architectures that are used to implement LDPSs according to some embodiments of the invention. conceptually illustrates an example flow of configuration data for the managed network architecture illustrated in according to some embodiments of the invention. In particular the left side of illustrates the flow of configuration data for a LPDS and the right side of illustrates the flow of configuration data for a WAN optimizer . The WAN optimizer in some embodiments is a WAN optimizer service instance or the WAN optimizer .

As shown on the left side of the logical controller receives network configuration data from a user which includes policy based routing data through a set of APIs provided by the logical controller . The network configuration data in this example describes an LDPS i.e. a logical network . As noted above in some embodiments the logical controller includes an input module not shown in such as an input translation application for generating LCP data from the network configuration data from a user specifying an LDPS while in other embodiments the input module runs on a separate controller and the logical controller receives the LCP data from the input module on the separate controller.

The logical controller generates the UPCP data from the LCP data by converting the LCP data to LFP data and then converting the LFP data to UPCP. In some embodiments the logical controller includes a control module not shown in that is responsible for generating the LFP data from the LCP data and a virtualization module not shown in that handles the generation of the UPCP data from the LFP data. Once the logical controller generates the UPCP the logical controller sends the generated UPCP data to the physical controllers and .

As illustrated on the left side of the physical controllers and each generates from the received UPCP data CPCP data for each of the managed switching elements and sends the CPCP data to each of the managed switching elements . In some embodiments the physical controllers and communicate with and configure the managed switching elements through the OpenFlow or OVS APIs provided by the managed switching elements .

The physical controllers and of some embodiments generates and sends attachment data and slicing data for a WAN optimizer along with the CPCP data to the managed switching elements . In some embodiments attachment data includes a tunnel type e.g. a GRE tunnel a CAPWAP tunnel a WCCP tunnel etc. for the WAN optimizer to use for sending and receiving network data e.g. to and from an extender .

In some embodiments the physical controller generates the slicing data for the WAN optimizer and pushes this data along with the WAN optimizer configuration data to the WAN optimizer through the set of APIs. The slicing data of some embodiments includes a unique identifier associated with a WAN optimizer in a logical network. In some embodiments a WAN optimizer can be used to implement 1 multiple WAN optimizer service instances for a particular logical network and or 2 multiple WAN optimizer service instances for multiple different logical networks. When the WAN optimizer of some such embodiments receives network data that includes the unique identifier the WAN optimizer identifies e.g. using a table that the WAN optimizer maintains for mapping unique identifiers to WAN optimizer service instances the WAN optimizer service instance associated with the unique identifier and uses the identified WAN optimizer service instance to process the packet.

For each of the managed switching elements when the managed switching element receives the CPCP data the managed switching element generates PFP data for implementing the LDPS. Instead of sending CPCP data in some embodiments the physical controllers and send the UPCP data to the managed switching elements . The managed switching elements of some such embodiments each generates its own CPCP data from the UPCP data and then generates the PFP data from the generated CPCP data.

The right side of shows that the network configuration data which is provided to the logical controller through a set of APIs also includes WAN optimizer configuration data. As shown the logical controller receives the WAN optimizer configuration data and sends it to the physical controllers and . Then the physical controllers and forward the WAN optimizer configuration data and the attachment data and or slicing data to the WAN optimizer through a set of API calls.

Once the WAN optimizer receives the WAN optimizer configuration data the WAN optimizer translates the WAN optimizer configuration data by creating a configuration of the WAN optimizer that includes the manner in which the WAN optimizer sends and receives network data based on the attachment data when the configuration is used. In addition the WAN optimizer binds e.g. associates the slicing data to the created WAN optimizer configuration so that the WAN optimizer is able to apply the WAN optimizer configuration to network data that specifies e.g. through a virtual local area network VLAN tag the slicing data s unique identifier or another shorter unique identifier e.g. represented by less bits that is associated with the slicing data s unique identifier.

In some embodiments the input tables include tables with different types of data depending on the role of the controller in the network control system. For instance when the controller functions as a logical controller for a user s logical forwarding elements the input tables include LCP data and LFP data for the logical forwarding elements. When the controller functions as a physical controller the input tables include LFP data. The input tables also include WAN optimizer configuration data received from the user or another controller. The WAN optimizer configuration data is associated with a logical datapath set parameter that identifies the logical switching elements to which the WAN optimizer is to be integrated.

In addition to the input tables the control application includes other miscellaneous tables not shown that the rules engine uses to gather inputs for its table mapping operations. These miscellaneous tables include constant tables that store defined values for constants that the rules engine needs to perform its table mapping operations e.g. the value 0 a dispatch port number for resubmits etc. . The miscellaneous tables further include function tables that store functions that the rules engine uses to calculate values to populate the output tables .

The rules engine performs table mapping operations that specifies one manner for converting input data to output data. Whenever one of the input tables is modified referred to as an input table event the rules engine performs a set of table mapping operations that may result in the modification of one or more data tuples in one or more output tables.

In some embodiments the rules engine includes an event processor not shown several query plans not shown and a table processor not shown . Each query plan is a set of rules that specifies a set of join operations that are to be performed upon the occurrence of an input table event. The event processor of the rules engine detects the occurrence of each such event. In some embodiments the event processor registers for callbacks with the input tables for notification of changes to the records in the input tables and detects an input table event by receiving a notification from an input table when one of its records has changed.

In response to a detected input table event the event processor 1 selects an appropriate query plan for the detected table event and 2 directs the table processor to execute the query plan. To execute the query plan the table processor in some embodiments performs the join operations specified by the query plan to produce one or more records that represent one or more sets of data values from one or more input and miscellaneous tables. The table processor of some embodiments then 1 performs a select operation to select a subset of the data values from the record s produced by the join operations and 2 writes the selected subset of data values in one or more output tables .

Some embodiments use a variation of the datalog database language to allow application developers to create the rules engine for the controller and thereby to specify the manner by which the controller maps logical datapath sets to the controlled physical switching infrastructure. This variation of the datalog database language is referred to herein as nLog. Like datalog nLog provides a few declaratory rules and operators that allow a developer to specify different operations that are to be performed upon the occurrence of different events. In some embodiments nLog provides a limited subset of the operators that are provided by datalog in order to increase the operational speed of nLog. For instance in some embodiments nLog only allows the AND operator to be used in any of the declaratory rules.

The declaratory rules and operations that are specified through nLog are then compiled into a much larger set of rules by an nLog compiler. In some embodiments this compiler translates each rule that is meant to address an event into several sets of database join operations. Collectively the larger set of rules forms the table mapping rules engine that is referred to as the nLog engine.

Some embodiments designate the first join operation that is performed by the rules engine for an input event to be based on the logical datapath set parameter. This designation ensures that the rules engine s join operations fail and terminate immediately when the rules engine has started a set of join operations that relate to a logical datapath set i.e. to a logical network that is not managed by the controller.

Like the input tables the output tables include tables with different types of data depending on the role of the controller . When the controller functions as a logical controller the output tables include LFP data and UPCP data for the logical switching elements. When the controller functions as a physical controller the output tables include CPCP data. Like the input tables the output tables may also include the WAN optimizer configuration data. Furthermore the output tables may include a slice identifier when the controller functions as a physical controller.

In some embodiments the output tables can be grouped into several different categories. For instance in some embodiments the output tables can be rules engine RE input tables and or RE output tables. An output table is a RE input table when a change in the output table causes the rules engine to detect an input event that requires the execution of a query plan. An output table can also be an RE input table that generates an event that causes the rules engine to perform another query plan. An output table is a RE output table when a change in the output table causes the exporter to export the change to another controller or a MSE. An output table can be an RE input table a RE output table or both an RE input table and a RE output table.

The exporter detects changes to the RE output tables of the output tables . In some embodiments the exporter registers for callbacks with the RE output tables for notification of changes to the records of the RE output tables. In such embodiments the exporter detects an output table event when it receives notification from a RE output table that one of its records has changed.

In response to a detected output table event the exporter takes each modified data tuple in the modified RE output tables and propagates this modified data tuple to one or more other controllers or to one or more MSEs. When sending the output table records to another controller the exporter in some embodiments uses a single channel of communication e.g. a RPC channel to send the data contained in the records. When sending the RE output table records to MSEs the exporter in some embodiments uses two channels. One channel is established using a switch control protocol e.g. OpenFlow for writing flow entries in the control plane of the MSE. The other channel is established using a database communication protocol e.g. JSON to send configuration data e.g. port configuration tunnel information .

In some embodiments the controller does not keep in the output tables the data for logical datapath sets that the controller is not responsible for managing i.e. for logical networks managed by other logical controllers . However such data is translated by the translator into a format that can be stored in the PTD and is then stored in the PTD. The PTD propagates this data to PTDs of one or more other controllers so that those other controllers that are responsible for managing the logical datapath sets can process the data.

In some embodiments the controller also brings the data stored in the output tables to the PTD for resiliency of the data. Therefore in these embodiments a PTD of a controller has all the configuration data for all logical datapath sets managed by the network control system. That is each PTD contains the global view of the configuration of the logical networks of all users.

The importer interfaces with a number of different sources of input data and uses the input data to modify or create the input tables . The importer of some embodiments receives the input data from another controller. The importer also interfaces with the PTD so that data received through the PTD from other controller instances can be translated and used as input data to modify or create the input tables . Moreover the importer also detects changes with the RE input tables in the output tables .

The logical network includes the WAN optimizer the WAN the L3 router the L2 switches and and the VMs . The arrangement of the WAN optimizer in the logical network is a one arm deployment or one armed out of path deployment that is similar to the WAN optimizer deployment described above by reference to .

As indicated by a dotted line in the left section of the path of the packet in this example starts from the VM and travels through the WAN optimizer for processing and then through the L3 router and over the WAN . In particular the packet travels from the VM through logical port 2 of the L2 switch and out the logical port 3 of the L2 switch to the logical port 1 of the L3 router . Once at the L3 router the packet travels out the logical port 3 of the L3 router and through of the WAN optimizer . After the WAN optimizer optimizes the packet data e.g. compresses the data the WAN optimizer forwards the packet back to the logical port 3 of the L3 router . The L3 router then routes the packet out of its logical port 4 and over the WAN .

In some embodiments the path of network data through the logical network is based on policy based routing data that the user provides as part of the network configuration data. Specifically in this example the user provides a policy specifying that network data sent from the VM e.g. packets that have the VM s IP address as the packet s source IP address and over the WAN is to be routed through the WAN optimizer for optimizing. Additional and or different policies may be used in different embodiments. For instance a policy may specify that network data sent from the VM e.g. packets that have the VM s IP address as the packet s source IP address and over the WAN to a particular host e.g. the host in is to be routed through the WAN optimizer for optimizing. Another policy might specify that network data sent from VMs in the logical broadcast domain managed by the L2 switch the VMs and in this example .

As mentioned above the right section of illustrates the logical processing of the packet through the logical network and the path the packet travels through a set of managed network elements that is used for implementing the logical network . As illustrated the set of managed network elements for this example includes the OVSs and the WAN optimizer and the OVS which is part of the extender not shown in .

Since the OVS is the edge switching element that is directly coupled to the VM the OVS in some embodiments is responsible for performing the logical processing referred to as first hop processing of the packet through the logical network from the VM to the WAN optimizer . In this example the logical port 2 of the L2 switch corresponds to the physical port 5 of the OVS . When the OVS receives the packet at the physical port 5 the OVS processes the packet through the logical network using the OVS s forwarding plane e.g. a set of forwarding tables .

After the OVS performs the logical L2 processing e.g. determining a forwarding decision through the L2 switch and the logical L3 processing e.g. determining a routing decision through the L3 router on the packet the OVS routes the packet to a physical network element based on the logical L2 and L3 processing. For this example the L2 and L3 processing of the packet results in a decision to route the packet to the logical port 3 of the L3 router which corresponds to the physical port 1 of the WAN optimizer . Based on the logical L2 and L3 processing the OVS forwards the packet through a tunnel e.g. a GRE tunnel a CAPWAP tunnel a WCCP tunnel etc. out of the physical port 7 of the OVS to the physical port 7 of the OVS which in turn forwards the packet out the physical port 8 of the OVS to the WAN optimizer s physical port 1 through a tunnel e.g. a GRE tunnel a CAPWAP tunnel a WCCP tunnel etc. .

When the WAN optimizer receives the packet at its physical port 1 the WAN optimizer processes the packet according to the WAN optimizer configuration data that the user provides as part of the network configuration data. As mentioned above slicing data which includes a unique identifier associated with a WAN optimizer in a logical network allows a WAN optimizer to implement 1 multiple WAN optimizers in a particular logical network and or 2 multiple WAN optimizers for multiple different logical networks. To process the packet the WAN optimizer identifies the unique identifier specified in the packet e.g. in the VLAN tag field and identifies the WAN optimizer configuration that corresponds to the unique identifier. The WAN optimizer uses the identified WAN optimizer configuration to process the packet and send the packet out of its physical port 1 through the tunnel between the WAN optimizer and the OVS and back to the physical port 8 of the OVS . Instead of processing the packet the WAN optimizer of some embodiments generates a copy of the packet processes the copy of the packet and sends the processed copy to the OVS . In other words the OVS receives back from the WAN optimizer a new packet that is generated by the WAN optimizer .

Once the OVS receives the packet back from the WAN optimizer the OVS performs L3 processing on the packet or a copy of the packet in order to determine a routing decision through the L3 router . In this example the OVS s L3 processing yields a decision to route the packet out the logical port 4 of the L3 router which corresponds to the physical port 6 of the OVS out to the WAN . Accordingly the OVS forwards the packet out of its physical port 6 over the WAN .

In some embodiments the forwarding decisions specified in the OVS s forwarding plane are derived from attachment and slicing data for the WAN optimizer and either 1 CPCP data that the OVS receives from a physical controller or 2 CPCP data that the OVS generates based on UPCP data received from the physical controller. In addition the forwarding decisions specified in the OVS s forwarding plane are similarly derived from attachment and slicing data for the WAN optimizer and either 1 CPCP data that the OVS receives from a physical controller or 2 CPCP data that the OVS generates based on UPCP data received from the physical controller.

As shown the left section of illustrates the packet traversing the logical network which is a conceptual representation of an LDPS in some embodiments. Furthermore the right section of illustrates a logical processing pipeline for processing the packet through the logical network and the corresponding path of the packet through a set of managed network elements used for implementing the logical network .

As illustrated in the logical network includes the WAN optimizer the WAN the L3 router the L2 switches and and the VMs . As noted above the arrangement of the WAN optimizer in the logical network is a physical in arm deployment or in line deployment that is similar to the WAN optimizer deployment described above by reference to .

A dotted line in the left section of shows the path of the packet in this example as starting from the VM and traveling through the WAN optimizer for processing and then over the WAN . In particular the packet travels from the VM through logical port 2 of the L2 switch and out the logical port 3 of the L2 switch to the logical port 1 of the L3 router . The packet then travels through the L3 router and out the logical port 3 of the L3 router and through the WAN optimizer . After the WAN optimizer optimizes the packet data e.g. compresses the data the WAN optimizer forwards the packet over the WAN .

The path of network data through the logical network is in some embodiments based on policy based routing data that the user provides as part of the network configuration data. In particular the user in this example provides a policy specifying that network data sent from the VM e.g. packets that have the VM s IP address as the packet s source IP address and over the WAN is to be routed through the WAN optimizer for optimizing. Additional and or different policies may be used in different embodiments. For example a policy may specify that network data sent from the VM e.g. packets that have the VM s IP address as the packet s source IP address and over the WAN to a particular host e.g. the host in is to be routed through the WAN optimizer for optimizing. Another policy might specify that network data sent from VMs in the logical broadcast domain managed by the L2 switch the VMs and in this example .

As mentioned above the right section of illustrates the logical processing of the packet through the logical network and the path the packet travels through a set of managed network elements that is used for implementing the logical network . As shown the set of managed network elements in this example includes the managed switching elements and the WAN optimizer and the extender .

Since the managed switching element is the edge switching element that is directly coupled to the VM the managed switching element in some embodiments is responsible for performing the logical processing referred to as first hop processing of the packet through the logical network from the VM to the WAN optimizer . In this example the logical port 2 of the L2 switch corresponds to the physical port 5 of the managed switching element . When the managed switching element receives the packet at the physical port 5 the managed switching element processes the packet through the logical network using the managed switching element s forwarding plane e.g. a set of forwarding tables .

After the managed switching element performs the logical L2 processing e.g. determining a forwarding decision through the L2 switch and the logical L3 processing e.g. determining a routing decision through the L3 router on the packet the managed switching element routes the packet to a physical network element based on the logical L2 and L3 processing. In this example the L2 and L3 processing of the packet results in a decision to route the packet out the logical port 3 of the L3 router which corresponds to the physical port 3 of the WAN optimizer . Based on the logical L2 and L3 processing the managed switching element forwards the packet through a tunnel e.g. a GRE tunnel a CAPWAP tunnel a WCCP tunnel etc. out of the physical port 7 of the managed switching element to the physical port 4 of the extender which in turn forwards the packet out the physical port 5 of the extender to the WAN optimizer s physical port 3 through a tunnel e.g. a GRE tunnel a CAPWAP tunnel a WCCP tunnel etc. .

When the WAN optimizer receives the packet at its physical port 3 the WAN optimizer processes the packet according to the WAN optimizer configuration data that the user provides as part of the network configuration data. As noted above slicing data which includes a unique identifier associated with a WAN optimizer in a logical network allows a WAN optimizer to implement 1 multiple WAN optimizers in a particular logical network and or 2 multiple WAN optimizers for multiple different logical networks. To process the packet the WAN optimizer identifies the unique identifier specified in the packet e.g. in the VLAN tag field and identifies the WAN optimizer configuration that corresponds to the unique identifier. The WAN optimizer uses the identified WAN optimizer configuration to process the packet and send the packet out of its physical port 6 and over the WAN .

In some embodiments the forwarding decisions specified in the managed switching element s forwarding plane are derived from attachment and slicing data for the WAN optimizer and either 1 CPCP data that the managed switching element receives from a physical controller or 2 CPCP data that the managed switching element generates based on UPCP data received from the physical controller. Similarly the forwarding decisions specified in the extender s forwarding plane are derived from attachment and slicing data for the WAN optimizer and either 1 CPCP data that the extender receives from a physical controller or 2 CPCP data that the extender generates based on UPCP data received from the physical controller.

Many of the above described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium also referred to as computer readable medium . When these instructions are executed by one or more computational or processing unit s e.g. one or more processors cores of processors or other processing units they cause the processing unit s to perform the actions indicated in the instructions. Examples of computer readable media include but are not limited to CD ROMs flash drives random access memory RAM chips hard drives erasable programmable read only memories EPROMs electrically erasable programmable read only memories EEPROMs etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.

In this specification the term software is meant to include firmware residing in read only memory or applications stored in magnetic storage which can be read into memory for processing by a processor. Also in some embodiments multiple software inventions can be implemented as sub parts of a larger program while remaining distinct software inventions. In some embodiments multiple software inventions can also be implemented as separate programs. Finally any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments the software programs when installed to operate on one or more electronic systems define one or more specific machine implementations that execute and perform the operations of the software programs.

The bus collectively represents all system peripheral and chipset buses that communicatively connect the numerous internal devices of the electronic system . For instance the bus communicatively connects the processing unit s with the read only memory the GPU the system memory and the permanent storage device .

From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of the invention. The processing unit s may be a single processor or a multi core processor in different embodiments. Some instructions are passed to and executed by the GPU . The GPU can offload various computations or complement the image processing provided by the processing unit s .

The read only memory ROM stores static data and instructions that are needed by the processing unit s and other modules of the electronic system. The permanent storage device on the other hand is a read and write memory device. This device is a non volatile memory unit that stores instructions and data even when the electronic system is off. Some embodiments of the invention use a mass storage device such as a magnetic or optical disk and its corresponding disk drive as the permanent storage device .

Other embodiments use a removable storage device such as a floppy disk flash memory device etc. and its corresponding drive as the permanent storage device. Like the permanent storage device the system memory is a read and write memory device. However unlike storage device the system memory is a volatile read and write memory such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some embodiments the invention s processes are stored in the system memory the permanent storage device and or the read only memory . From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of some embodiments.

The bus also connects to the input and output devices and . The input devices enable the user to communicate information and select commands to the electronic system. The input devices include alphanumeric keyboards and pointing devices also called cursor control devices cameras e.g. webcams microphones or similar devices for receiving voice commands etc. The output devices display images generated by the electronic system or otherwise output data. The output devices include printers and display devices such as cathode ray tubes CRT or liquid crystal displays LCD as well as speakers or similar audio output devices. Some embodiments include devices such as a touchscreen that function as both input and output devices.

Finally as shown in bus also couples electronic system to a network through a network adapter not shown . In this manner the computer can be a part of a network of computers such as a local area network LAN a wide area network WAN or an Intranet or a network of networks such as the Internet. Any or all components of electronic system may be used in conjunction with the invention.

Some embodiments include electronic components such as microprocessors storage and memory that store computer program instructions in a machine readable or computer readable medium alternatively referred to as computer readable storage media machine readable media or machine readable storage media . Some examples of such computer readable media include RAM ROM read only compact discs CD ROM recordable compact discs CD R rewritable compact discs CD RW read only digital versatile discs e.g. DVD ROM dual layer DVD ROM a variety of recordable rewritable DVDs e.g. DVD RAM DVD RW DVD RW etc. flash memory e.g. SD cards mini SD cards micro SD cards etc. magnetic and or solid state hard drives read only and recordable Blu Ray discs ultra density optical discs any other optical or magnetic media and floppy disks. The computer readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code such as is produced by a compiler and files including higher level code that are executed by a computer an electronic component or a microprocessor using an interpreter.

While the above discussion primarily refers to microprocessor or multi core processors that execute software some embodiments are performed by one or more integrated circuits such as application specific integrated circuits ASICs or field programmable gate arrays FPGAs . In some embodiments such integrated circuits execute instructions that are stored on the circuit itself. In addition some embodiments execute software stored in programmable logic devices PLDs ROM or RAM devices.

As used in this specification and any claims of this application the terms computer server processor and memory all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification the terms display or displaying means displaying on an electronic device. As used in this specification and any claims of this application the terms computer readable medium computer readable media and machine readable medium are entirely restricted to tangible physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals wired download signals and any other ephemeral signals.

While the invention has been described with reference to numerous specific details one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. In addition a number of the figures conceptually illustrate processes. The specific operations of these processes may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations and different specific operations may be performed in different embodiments. Furthermore the process could be implemented using several sub processes or as part of a larger macro process. Thus one of ordinary skill in the art would understand that the invention is not to be limited by the foregoing illustrative details but rather is to be defined by the appended claims.

