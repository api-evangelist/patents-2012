---

title: Capture device movement compensation for speaker indexing
abstract: Embodiments of the invention compensate for the movement of a meeting capture device during a live meeting when performing speaker indexing of a recorded meeting. In one example, a first position of a capture device is determined. A second position of the capture device is determined after the capture device has been moved from the first position to the second position. The movement data associated with movement of the capture device from the first position to the second position is determined. The movement data is outputted and used in speaker indexing of the recorded meeting.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08749650&OS=08749650&RS=08749650
owner: Microsoft Corporation
number: 08749650
owner_city: Redmond
owner_country: US
publication_date: 20121207
---
This application is a divisional of and claims priority under 35 U.S.C. 120 to application Ser. No. 11 771 786 filed Jun. 29 2007 entitled CAPTURE DEVICE MOVEMENT COMPENSATION FOR SPEAKER INDEXING indicated to be issued as U.S. Pat. No. 8 330 787 on Dec. 11 2012 which is hereby incorporated by reference in its entirety.

Meetings are often conducted using videoconferencing systems. A meeting may be conducted using one or more capture devices such as a video camera and microphone. The meeting may also be recorded and viewed at a later time by the meeting participants or by those who were unable to attend the live meeting.

A meeting recording may be indexed by slides and speaker sites e.g. conference room remote office remote office etc. . Another method of indexing the meeting recording is by speakers within a conference room e.g. speaker etc. . To index speakers a cluster analysis on the sound source localization directions from a microphone array may be performed to determine location and number of speakers in the conference room in reference to a capture device. In one instance speaker indexing assumes speakers don t change seats or walk around the room during a meeting. Today s speaker indexing works well when the capture device is fixed in place such as when a video camera is attached to a conference room table.

However if the capture device is moved during a meeting e.g. rotated then the speaker indexing performed on the meeting recording may have flaws. The speaker indexing after the capture device movement may not match the speaker indexing before the capture device movement. Current videoconferencing systems fail to determine when capture device movement occurs and fail to compensate for the capture device movement in speaker indexing.

The following presents a simplified summary of the disclosure in order to provide a basic understanding to the reader. This summary is not an extensive overview of the disclosure and it does not identify key critical elements of the invention or delineate the scope of the invention. Its sole purpose is to present some concepts disclosed herein in a simplified form as a prelude to the more detailed description that is presented later.

Embodiments of the invention compensate for the movement of a capture device during a live meeting when performing speaker indexing of a recorded meeting. In one embodiment a vision based method is used to detect capture device movement. Vision based methods may use image features edge detection or object modeling to detect capture device movement. In another embodiment a hardware based method is used to determine capture device movement. Hardware based methods include using accelerometers and or magnetometers at the capture device. Movement data associated with capture device movement may be applied to speaker indexing.

Many of the attendant features will be more readily appreciated as the same become better understood by reference to the following detailed description considered in connection with the accompanying drawings.

The detailed description provided below in connection with the appended drawings is intended as a description of the present examples and is not intended to represent the only forms in which the present examples may be constructed or utilized. The description sets forth the functions of the examples and the sequence of steps for constructing and operating the examples. However the same or equivalent functions and sequences may be accomplished by different examples.

Turning to examples of capture device movement that may occur when a capture device is recording a live meeting will be discussed. show top views of a table surrounded by six chairs in a conference room . Capture device is on top of table for capturing audio and video of a meeting conducted in conference room . In one embodiment capture device includes an omni directional camera and at least one microphone for capturing audio and video. An example capture device is described below in conjunction with .

In capture device is moved translationally while capturing a meeting. Video capture device is moved from a first position shown by X Y to a second position shown by X Y . It will be appreciated that translational movement may also include a Z direction. It will also be appreciated that capture device movement may also include tilting of video capture device . In video capture device is moved azimuthally from azimuth to azimuth where the azimuth is measured in reference to reference point on capture device . In another example only the camera portion of capture device is moved but the base of capture device remains stationary.

Turning to a distributed meeting system in accordance with embodiments of the invention is shown. One or more computing devices of system may be connected by one or more networks. System includes a meeting room server e.g. a PC a notebook computer etc. connected to capture device . Capture device captures the audio and video of meeting participants in room . Live meeting clients and are also connected to meeting room server when a meeting is conducted. Live meeting clients and enable remote users to participate in a live meeting. Audio and video from capture device may be sent to live meeting clients and by server during the live meeting. Video and or audio of users at live meeting clients and may be sent to meeting room server using capture devices not shown at live meeting clients and . Meeting room server may record the audio video captured by capture device . Meeting room server is connected to archived meeting server . Archived meeting server may archive the recorded meeting.

Meeting room server may perform video post processing including speaker indexing with capture device movement compensation as described herein. In alternative embodiments other computing devices such as archived meeting server or capture device may perform capture device movement compensation as described herein. An example computing device for implementing one or more embodiments of the invention is discussed below in conjunction with .

In system archived meeting clients and may connect to archived meeting server for viewing a recorded meeting. The recorded meeting may have received post processing which may include speaker indexing. The speaker indexing may use embodiments of capture device movement compensation as described herein.

Turning to an embodiment of an archived meeting client User Interface UI is shown. UI may be used to view a recorded meeting. UI includes speaker video playback controls meeting timeline whiteboard image whiteboard key frame table of contents and panoramic view . Live meeting clients and have UIs similar to UI during the live meeting except they may not include meeting timeline and whiteboard key frame table of contents .

Speaker video shows video of the current speaker in the video recording. This video may have been captured by capture device in room by another capture device in room or a capture device at a remote live client .

Playback controls allow the user to adjust the playback of the recorded meeting such as fast forward rewind pause play play speed e.g. 1.0 1.5 2.0 volume control and the like. In one embodiment when play speed is increased the speaker s voice is played without changing the speaker s voice pitch. In another embodiment the play speed may be selected on a per person basis e.g. whenever speaker speaks play speed is 1.0 but whenever speaker speaks play speed is 1.5 . In yet another embodiment time compression may be applied to the recorded meeting to remove pauses to enhance the playback experience.

Whiteboard image shows the contents of a whiteboard in room . Pen strokes on the whiteboard are time stamped and synchronized to the meeting audio video. Key frames for the whiteboard are shown in whiteboard key frame table of contents . Panoramic view shows video captured by capture device .

Meeting timeline shows the results of speaker segmentation. Speakers at the meeting are segmented and assigned a horizontal line i.e. an individual speaker timeline in meeting timeline . Speakers can be filtered using checkboxes so only the selected speakers will playback. Playback speed for each individual speaker may also be selected. Also a separate line in meeting timeline may show special events such as key frames annotations projector switch to shared application and the like. A user may click on a position in meeting timeline to jump the playback to the selected timeline position. Speaker segmentation in meeting timeline is producing using speaker indexing. Speaker indexing uses speaker clustering techniques to identify the number of speakers at a meeting and the speaker locations in relation to a video recording. The speaker segmentation in meeting timeline has been adjusted for capture device movement during the live meeting using embodiments as described herein.

In a speaker indexing system is shown that uses cluster analysis. An Active Speaker Detector ASD receives real time video footage and real time Sound Source Localization SSL as input. SSL analyzes the microphone array audio captured during the meeting and detects when a meeting participant is talking. SSL may be input as a probability distribution function. ASD analyzes video and SSL and determines when each meeting participant is talking.

Virtual Cinematographer VC takes the determination made by ASD and applies further analysis and cinemagraphic rules to compute a speaker azimuth for each speaker where the speaker azimuth is referenced from capture device . VC is used for real time speaker control. Cluster analysis is performed during post processing. As shown in audio video information is stored in a file by VC . In post processing cluster analysis module may use file to perform cluster analysis for use in playback of the recorded videoconference. File may include information for performing capture device movement compensation as described in embodiments herein.

However if capture device is rotated 30 degrees to the right during the live meeting then the speaker indexing after the device movement will be 30 degrees off. This will cause problems in the video playback in UI . Embodiments of the invention compensate for such device movement to provide users a robust and high quality playback experience.

Turning to a flowchart shows the logic and operations of capture device movement compensation for speaker indexing in accordance with an embodiment of the invention. Vision based and hardware based implementations of flowchart are discussed below. In one embodiment at least a portion of the logic of flowchart is performed during post processing of a recorded meeting. In alternative embodiments capture device motion compensation as described herein may be conducted during a live meeting.

Starting in block the initial position of the capture device is determined. Proceeding to block capture device movement occurs. Next in block the current position of the device is determined Next in block movement data associated with the movement of the capture device from the initial position to the current position is determined This movement data may indicate a change in the translational position of the capture device e.g. X Y and or Z a change in the azimuth of the capture device e.g. and or a change in camera tilt angle. Proceeding to block the movement data is outputted.

Next in block the movement data is applied to speaker indexing. In one embodiment the movement data may be applied to audio video that occurs after the capture device movement. For example if the capture device rotated 30 degrees then speaker indexing after the device movement may be corrected by 30 degrees. It will be appreciated that this correction technique may lead to integration error i.e. compounding of multiple errors when multiple movements of the device occur during a recorded meeting.

In another embodiment the clustering analysis of the recorded meeting may be restarted after the movement of the device is detected. This correction technique may reduce integration errors in video playback but may be computationally expensive. In restarting the cluster analysis the resulting speaker segmentation may be used to generate multiple timelines corresponding to each time the capture device is moved e.g. if the capture device was moved once during a live meeting then restarting the cluster analysis may result in two timelines . The results may be displayed as two separate timelines in UI . For example if the capture device moved at time t then meeting timeline may show a new set of speakers starting at time t. Alternatively the results may be merged into a single timeline in UI . To merge the timelines movement data may be used to correlate speaker in timeline to the same speaker in timeline . For example if the movement data indicates capture device rotated 45 degrees clockwise then the logic may use this movement data to match speakers from the two timelines.

Turning to a flowchart shows the logic and operations of capture device movement compensation in accordance with an embodiment of the invention. In one embodiment at least a portion of the logic of flowchart may be implemented by computer readable instructions executable by one or more computing devices. At least a portion of the logic of flowchart may be conducted during the post processing of a recorded meeting.

Starting in block feature points in an image captured by capture device are detected. Continuing to block one or more stationary points of the feature points are selected. The stationary points are selected from the feature points that do not move over a period of time.

Next in block device movement occurs. Device movement may be detected from the image because during device movement there are no stationary points. Next in block current stationary points are detected and matched to the last stationary points before device movement. Matching of current C i and last L i stationary points may be conducted using a Hough transform and a rotation i.e. azimuth camera motion model. For example to determine if point C i matches point L j let

Proceeding to block movement data is determined from the comparison of the last and current stationary points. The movement data is then output as shown in block .

In one embodiment flowchart assumes mostly azimuthal i.e. rotational movement of capture device but limited translational movement e.g. less than approximately 20 centimeters translational movement of the device . One skilled in the art having the benefit of this description will appreciate that flowchart may be extended to include translation and camera tilt orientation movement in the Hough transform.

Turning to an example of determining capture device movement using stationary feature points is shown. In the last stationary image stationary feature points have been identified in the image. For example a corner of the whiteboard shown at has been identified as a stationary point. Last stationary image has a reference azimuth of 0 degrees.

At the current image after capture device movement has occurred is shown. Also at stationary feature points from current image have been aligned with stationary features points from last stationary image . As shown at the corner of the whiteboard has been used as a stationary point for aligning the images. Hough transform results shown at indicate the capture device azimuth has changed to 300 degrees i.e. the device has been turned 60 degrees to the left .

Turning to an example of determining capture device movement using stationary feature points is shown. In last stationary image stationary feature points have been identified in the image. For example a door knob shown at has been identified as a stationary point. Image has a reference azimuth of 0 degrees.

Image shows the current image after movement has occurred. Stationary points from current image have been aligned with stationary points from the last stationary image . As shown at the doorknob has been used as a stationary point for aligning the images. Hough transform results shown at indicate the capture device azimuth has rotated to 035 degrees i.e. the device has been rotated 35 degrees to the right .

Turning to a flowchart shows the logic and operations of capture device movement compensation using a correlation based technique as opposed to a feature based technique in accordance with an embodiment of the invention. In one embodiment at least a portion of the logic of flowchart may be implemented by computer readable instructions executable by one or more computing devices. At least a portion of the logic of flowchart may be conducted during the post processing a recorded meeting.

In one embodiment flowchart assumes azimuthal camera motion and little translational motion. One skilled in the art having the benefit of this description will appreciate that flowchart may be extended to include translation and camera tilt orientation movement to the Hough transform.

Starting in block the edges in an image are detected and an edge image is produced. In one embodiment an edge detector such as Canny edge detection is used as a feature detector. The edges are filtered over time to detect stationary edges and spatially smoothed. Next in block the stationary edges are selected from the edge image. Proceeding to block capture device movement occurs. During device movement there are no stationary edges. Next in block stationary edges in the current edge image after capture device movement are matched to the stationary edges in the last stationary edge image. In one embodiment a Hough transform is used to determine the best azimuth that minimizes image correlation error. Proceeding to block from this matching movement data for the capture device is determined. Next in block the movement data is outputted.

It will be appreciated that edge detection in flowchart may be distinguished from using feature points as described in flowchart .

Turning to an example of determining capture device movement using stationary edges is shown. The current camera image is shown. The current edge image has been derived from camera image . The last stationary edge image is compared to the current edge image as shown by alignment image . Hough transform results shown at indicate the least correlation error at azimuth 300 degrees. Thus the capture device has been rotated to 300 degrees i.e. the device has been turned 60 degrees to the left .

Turning to an example of determining capture device movement using stationary edges is shown. The current camera image is shown. The current edge image has been derived from camera image . The last stationary edge image is compared to the current edge image as shown by alignment image . Hough transform results shown at indicate the least correlation error at azimuth 030 degrees. Thus the capture device has been rotated to 030 degrees i.e. the device has been turned 30 degrees to the right .

Turning to a flowchart shows the logic and operations of capture device movement compensation using object modeling in accordance with an embodiment of the invention. In one embodiment at least a portion of the logic of flowchart may be implemented by computer readable instructions executable by one or more computing devices. At least a portion of the logic of flowchart may be conducted during the post processing of a recorded meeting.

In one embodiment the logic of flowchart determines the size and orientation of the meeting room table that the capture device is positioned on. The capture device learns a table model parametrically and then fits the model to the table during the meeting or during post processing including table orientation and table position. Tests show robust results in normal lighting conditions and with 50 random occlusions of the table in the image e.g. the open laptop of a meeting participant may partially block the capture device s view of the table .

Starting in block an object model is learned from an image of a stationary object in the meeting room captured by the capture device. Proceeding to block the object model is fit to the stationary object. Next in block capture device movement occurs. Device movement may be detected by comparing the model parameters of the current and previous frames.

Continuing to block the current object model is matched again to the corresponding stationary object. The current object model position is matched to the last object model position. Next in block movement data is determined from the change in object model positions. Then the movement data is outputted as shown in block .

Turning to an example of object modeling using a conference room table is shown. It will be appreciated that embodiments of stationary object modeling are not limited to modeling tables. In a real image from the capture device is shown at . At an edge map has been extracted from the real image. The edge map includes noise i.e. edges of other objects such as people doors etc. in addition to the edges of the table boundaries of the table of interest. To filter the edge map it is observed that most conference tables are bilaterally symmetric. This symmetry is used to filter out the noise.

The filtering operation uses a symmetry voting scheme to filter out the noise in the edge map. After applying the filtering operation to the edge map shown at a symmetry enhanced edge map is produced as shown at .

A fitting algorithm is used to fit the symmetry enhanced edge map to the table in the edge map shown at . In one embodiment a trigonometry fitting is used. Points on two of the four table edges are used. As shown in a first section of table is between cut and cut and a second section of table is between cut and cut . A limitation of the trigonometry fitting is that it assumes a rectangular table. In another embodiment a quadratic fitting is used. The quadratic fitting does not assume the shape of the table. In quadratic fitting two quadratic curves are used to fit the table edge points.

The result of a fitting algorithm is shown at . A table model shown as a dotted line curve has been fit to the table in the edge map. After device movement occurs table model may be re aligned to the table in the current edge map. The difference between the last stationary table model position and the current table model position may be used to determine the movement of the capture device.

It is noted that under some conditions such as low lighting or insufficient visual texture e.g. a mostly white room vision based motion detection methods may have limitations. Still under such conditions the vision based methods may detect that the capture device has moved but may not necessarily be able to determine the movement data. In such cases the speaker indexing may be reset when motion has been detected and the cluster analysis will be restarted.

Turning to an embodiment of a capture device is shown. As will be discussed below capture device may include a magnetometer and or an accelerometer for use in device movement compensation for speaker indexing. It will be appreciated that capture device is not limited to the design shown in .

Capture device includes a base coupled to a neck which in turn is coupled to a head . Base includes a speaker and one or more microphones . Capture device may be powered using power cord . A cable e.g. USB IEEE 1394 etc. may connect capture device to another computing device such as meeting room server . Alternatively capture device may connect to another computing device wirelessly. Head may include an omni directional camera that captures 360 degrees of video. The omni directional camera may have several individual cameras. The images from each camera may be stitched together to form a panoramic view.

Capture device may include one or more accelerometers and or one or more magnetometers. In the embodiment of head includes a magnetometer and base includes an accelerometer . In one embodiment magnetometer is a 2 axis magnetometer and accelerometer is a 3 axis accelerometer.

Turning to a flowchart shows the logic and operations of capture device movement compensation in accordance with an embodiment of the invention. In one embodiment the movement data is determined and output by the logic of flowchart during the live meeting. The logic of flowchart may be performed at device at a computing device coupled to device or any combination thereof. The movement data may be stored with the recorded meeting such as in file and then used for device motion compensation during post processing of the recording.

Starting in decision block the logic waits for detection of movement of the capture device by the accelerometer. Once movement is detected the logic proceeds to block where the magnetometer measures a start azimuth. In one embodiment the measurement in block happens very quickly e.g. 

Once the capture device motion has stopped the logic proceeds to block . In block the magnetometer measures a stop azimuth. Next in block the translational difference is determined by the accelerometers and an azimuth change is determined by the accelerometers.

Proceeding to block the translation change of the capture device is outputted. Next in decision block the logic determines if the azimuth change detected by the magnetometer is substantially equal to the azimuth change detected by the accelerometer. If the answer is no then the logic proceeds to block where the accelerometer azimuth difference is outputted. If the answer to decision block is yes then the magnetometer azimuth difference is outputted.

It will be appreciated that the magnetometer may provide a more reliable azimuth measurement than the accelerometer because the accelerometer may experience integration errors over time after several device movements. However the magnetometer measurements are cross checked with the accelerometer azimuth measurement in decision block because the magnetometer is susceptible to error from artificial magnetic field changes such as from a local Magnetic Resonance Imaging MRI machine.

Alternative embodiments of the invention may use only a magnetometer or only an accelerometer for detecting capture device movement. For example a magnetometer may be used to measure device rotation while other means such as vision based models discussed above may be used to determine translational movement. In an accelerometer only example translation as well as azimuth changes may be detected and measured by one or more accelerometers.

Embodiments of the invention provide capture device movement compensation for speaker indexing. Vision based techniques may use images captured by the capture device itself and hardware based techniques may use magnetometers and or accelerometers at the capture device. Embodiments herein provide reliable speaker indexing that in turn results in more robust speaker segmentation for viewing recorded meetings in a client UI.

It will be appreciated that vision based techniques and or hardware based techniques may be combined as desired for capture device movement compensation. Techniques may be combined to cross check device movement data and consequently enhance the user experience. For example movement data determined using stationary feature points may be compared to movement data determined using stationary edges in edge images. If the movement data determined by the two techniques differs by a threshold then the techniques may be repeated or a different technique such as object modeling may be applied to ensure accurate speaker indexing.

Although not required embodiments of the invention are described in the general context of computer readable instructions being executed by one or more computing devices. Computer readable instructions may be distributed via computer readable media discussed below . Computer readable instructions may be implemented as program modules such as functions objects Application Programming Interfaces APIs data structures and the like that perform particular tasks or implement particular abstract data types. Typically the functionality of the computer readable instructions may be combined or distributed as desired in various environments.

In other embodiments device may include additional features and or functionality. For example device may also include additional storage e.g. removable and or non removable including but not limited to magnetic storage optical storage and the like. Such additional storage is illustrated in by storage . In one embodiment computer readable instructions to implement embodiments of the invention may be in storage . Storage may also store other computer readable instructions to implement an operating system an application program and the like.

The term computer readable media as used herein includes computer storage media. Computer storage media includes volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions or other data. Memory and storage are examples of computer storage media. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM Digital Versatile Disks DVDs or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by device . Any such computer storage media may be part of device .

Device may also include communication connection s that allow device to communicate with other devices. Communication connection s may include but is not limited to a modem a Network Interface Card NIC an integrated network interface a radio frequency transmitter receiver an infrared port a USB connection or other interfaces for connecting computing device to other computing devices. Communication connection s may include a wired connection or a wireless connection. Communication connection s may transmit and or receive communication media.

The term computer readable media may include communication media. Communication media typically embodies computer readable instructions or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term modulated data signal means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media includes wired media such as a wired network or direct wired connection and wireless media such as acoustic radio frequency infrared Near Field Communication NFC and other wireless media.

Device may include input device s such as keyboard mouse pen voice input device touch input device infrared cameras video input devices and or any other input device. Output device s such as one or more displays speakers printers and or any other output device may also be included in device . Input device s and output device s may be connected to device via a wired connection wireless connection or any combination thereof. In one embodiment an input device or an output device from another computing device may be used as input device s or output device s for computing device .

Components of computing device may be connected by various interconnects such as a bus. Such interconnects may include a Peripheral Component Interconnect PCI such as PCI Express a Universal Serial Bus USB firewire IEEE 1394 an optical bus structure and the like. In another embodiment components of computing device may be interconnected by a network. For example memory may be comprised of multiple physical memory units located in different physical locations interconnected by a network.

In the description and claims the term coupled and its derivatives may be used. Coupled may mean that two or more elements are in contact physically electrically magnetically optically etc. . Coupled may also mean two or more elements are not in contact with each other but still cooperate or interact with each other for example communicatively coupled .

Those skilled in the art will realize that storage devices utilized to store computer readable instructions may be distributed across a network. For example a computing device accessible via network may store computer readable instructions to implement one or more embodiments of the invention. Computing device may access computing device and download a part or all of the computer readable instructions for execution. Alternatively computing device may download pieces of the computer readable instructions as needed or some instructions may be executed at computing device and some at computing device . Those skilled in the art will also realize that all or a portion of the computer readable instructions may be carried out by a dedicated circuit such as a Digital Signal Processor DSP programmable logic array and the like.

Various operations of embodiments of the present invention are described herein. In one embodiment one or more of the operations described may constitute computer readable instructions stored on one or more computer readable media which if executed by a computing device will cause the computing device to perform the operations described. The order in which some or all of the operations are described should not be construed as to imply that these operations are necessarily order dependent. Alternative ordering will be appreciated by one skilled in the art having the benefit of this description. Further it will be understood that not all operations are necessarily present in each embodiment of the invention.

The above description of embodiments of the invention including what is described in the Abstract is not intended to be exhaustive or to limit the embodiments to the precise forms disclosed. While specific embodiments and examples of the invention are described herein for illustrative purposes various equivalent modifications are possible as those skilled in the relevant art will recognize in light of the above detailed description. The terms used in the following claims should not be construed to limit the invention to the specific embodiments disclosed in the specification. Rather the following claims are to be construed in accordance with established doctrines of claim interpretation.

