---

title: Updating printed content with personalized virtual data
abstract: The technology provides for updating printed content with personalized virtual data using a see-through, near-eye, mixed reality display device system. A printed content item, for example a book or magazine, is identified from image data captured by cameras on the display device, and user selection of a printed content selection within the printed content item is identified based on physical action user input, for example eye gaze or a gesture. Virtual data is selected from available virtual data for the printed content selection based on user profile data, and the display device system displays the selected virtual data in a position registered to the position of the printed content selection. In some examples, a task related to the printed content item is determined based on physical action user input, and personalized virtual data is displayed registered to the printed content item in accordance with the task.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09229231&OS=09229231&RS=09229231
owner: Microsoft Technology Licensing, LLC
number: 09229231
owner_city: Redmond
owner_country: US
publication_date: 20120109
---
This application claims priority as a continuation in part to U.S. patent application Ser. No. 13 313 368 entitled Making Static Printed Content Dynamic with Virtual Data having the same inventors and filed Dec. 7 2011 which is hereby incorporated by reference.

Static printed material may be thought of as a form of read only memory which requires no power and stores its data in a form visible to the human eye. Additionally the data is presented generally in a format comfortable for human eye reading which presents printed text against a contrasting physical background of white or another contrasting colored paper. Furthermore a large amount of information can be stored in printed material such as a textbook. However the permanent setting of literary content on cards books magazines and other printed mediums for expressing literary content also means the content does not adapt to the reader.

Mixed reality is a technology that allows virtual imagery to be mixed with a real world view. A see through near eye mixed reality display device may be worn by a user to view the mixed imagery of real objects and virtual objects displayed in the user s field of view. A near eye display device such as a head mounted display HMD device can update the data embodied in the static printed material. In other words the physical book magazine or other embodiment of static printed material becomes a form of memory which is dynamic in the sense that what appears on a printed sheet of paper a printed card or other printed medium can change and can change based on user profile data.

The technology provides an embodiment of a method for updating printed content with personalized virtual data using a see through near eye mixed reality display device. The method comprises identifying a printed content item in a field of view of a see through near eye mixed reality display device and identifying user selection of a printed content selection within the printed content item based on physical action user input. Some examples of a printed content item are a book or magazine. Some examples of a printed content selection is a word phrase illustration or paragraph. Some examples of physical action user input are a gesture eye gaze and a sound or speech generated by a user.

Whether virtual data is available for the printed content selection is determined and responsive to virtual data being available for the printed content selection virtual data is selected from the available data based on user profile data. The virtual data is displayed in a position registered to a position of the printed content selection. Virtual data is image data of a virtual object and may be two dimensional 2D or three dimensional 3D . A virtual object or virtual data which is registered to a real object like the printed content selection means the virtual object tracks its position in a field of view of the see through display device in reference to or dependent upon a position of the printed content selection seen through the see through display device.

The technology provides an embodiment of a system for a see through near eye mixed reality display device system for updating printed content with personalized virtual data. The system comprises a see through display positioned by a support structure. An example of a support structure is an eyeglasses frame. At least one outward facing camera is positioned on the support structure for capturing image data of a field of view of the see through display. One or more software controlled processors have access to stored user profile data and are communicatively coupled to a search engine having access to one or more datastores including content layout and virtual data for works and printed content items embodying the works.

The one or more software controlled processors are communicatively coupled to the at least one outward facing camera for receiving image data of the field of view. Based on the image data of the field of view the one or more software controlled processors identify user physical action selecting a printed content selection in a printed content item. The one or more software controlled processors select virtual data from the one or more datastores based on user profile data and the printed content selection and are communicatively coupled to at least one image generation unit for the see through display to cause the unit to display the virtual data.

The technology provides an embodiment of one or more processor readable storage devices having instructions encoded thereon which instructions cause one or more processors to execute a method for updating printed content with personalized virtual data using a see through near eye mixed reality display device. The method comprises identifying a printed content item in a field of view of a see through near eye mixed reality display device and a task is determined which task is related to the printed content item based on physical action user input. The task is performed and personalized virtual data is displayed registered to the content item in accordance with the task.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter.

The technology provides various embodiments for updating static printed content with virtual data based on user profile data and displayed by a see through near eye mixed reality display device system. The see through display device system identifies a real book magazine newspaper or other real printed material in a user field of view. A book magazine newspaper a card or a separate sheet of paper are all examples of a printed content item which object recognition software can identify from image data captured by front facing cameras positioned on the display device system to capture objects in the field of view of the display device which approximates the user field of view when looking through the display device.

In some instances eye gaze data identifies where a user is focusing in the field of view and can thus identify at which portion of the printed content item a user is looking. A gaze duration on a portion of the printed material can identify the portion as a printed content selection. Gaze duration is an example of a physical action of a user using a body part. A gesture performed by a user body part such as a hand or finger and captured in image data is also an example of physical action user input. A blink or blinking sequence of an eye can also be a gesture. A pointing or particular movement gesture by a hand finger or other body part can also indicate a printed content selection like a word sentence paragraph or photograph. A user generated sound command such as a voice command may also be considered an example of a physical action indicating user input. Sound based actions typically accompany other physical actions like a gesture and eye gaze.

Once the user selects a printed content item or a printed content selection like a picture or text within an item associated 2D or 3D virtual data selected based on user profile data such as videos images text and holograms can be registered to the selection or a section of the item. Additionally different tasks or applications can be executed with respect to the printed content selection or content item which display personalized virtual data in accordance with the tasks. Some examples of such tasks which display personalized virtual data are described below and include examples from a learning assistance task a directions task and a task which allows a user to substitute or fill in one or more words like names of a character or a location.

The use of the term actual direct view refers to the ability to see real world objects directly with the human eye rather than seeing created image representations of the objects. For example looking through glass at a room allows a user to have an actual direct view of the room while viewing a video of a room on a television is not an actual direct view of the room. Each display optical system is also referred to as a see through display and the two display optical systems together may also be referred to as a see through display.

Frame provides a support structure for holding elements of the system in place as well as a conduit for electrical connections. In this embodiment frame provides a convenient eyeglass frame as support for the elements of the system discussed further below. The frame includes a nose bridge portion with a microphone for recording sounds and transmitting audio data in this embodiment. A temple or side arm of the frame rests on each of a user s ears. In this example the right temple includes control circuitry for the display device .

As illustrated in an image generation unit is included on each temple in this embodiment as well. Also not shown in this view but illustrated in are outward facing cameras for recording digital images and videos and transmitting the visual recordings to the control circuitry which may in turn send the captured image data to the processing unit which may also send the data to one or more computer systems over a network .

The processing unit may take various embodiments. In some embodiments processing unit is a separate unit which may be worn on the user s body e.g. a wrist or be a separate device like the illustrated mobile device as illustrated in . The processing unit may communicate wired or wirelessly e.g. WiFi Bluetooth infrared RFID transmission wireless Universal Serial Bus WUSB cellular 3G 4G or other wireless communication means over a communication network to one or more computing systems whether located nearby or at a remote location. In other embodiments the functionality of the processing unit may be integrated in software and hardware components of the display device as in .

A remote network accessible computer system may be leveraged for processing power and remote data access. An application may be executing on computing system which interacts with or performs processing for display system or may be executing on one or more processors in the see through mixed reality display system . An example of hardware components of a computing system is shown in .

In many embodiments the two cameras provide overlapping image data from which depth information for objects in the scene may be determined based on stereopsis. In some examples the cameras may also be depth sensitive cameras which transmit and detect infrared light from which depth data may be determined. The processing identifies and maps the user s real world field of view. Some examples of depth sensing technologies that may be included on the head mounted display device without limitation are SONAR LIDAR Structured Light and or Time of Flight.

Control circuits provide various electronics that support the other components of head mounted display device . In this example the right temple includes control circuitry for the display device which includes a processing unit a memory accessible to the processing unit for storing processor readable instructions and data a wireless interface communicatively coupled to the processing unit and a power supply providing power for the components of the control circuitry and the other components of the display like the cameras the microphone and the sensor units discussed below. The processing unit may comprise one or more processors including a central processing unit CPU and a graphics processing unit GPU .

Inside or mounted to temple are ear phones inertial sensors one or more location or proximity sensors some examples of which are a GPS transceiver an infrared IR transceiver or a radio frequency transceiver for processing RFID data. Optional electrical impulse sensor detects commands via eye movements. In one embodiment inertial sensors include a three axis magnetometer A three axis gyro B and three axis accelerometer C. The inertial sensors are for sensing position orientation and sudden accelerations of head mounted display device . From these movements head position may also be determined. In this embodiment each of the devices using an analog signal in its operation like the sensor devices and as well as the microphone and an IR illuminator A discussed below include control circuitry which interfaces with the digital processing unit and memory and which produces and converts analog signals for its respective device.

Mounted to or inside temple is an image source or image generation unit which produces visible light representing images. In one embodiment the image source includes micro display for projecting images of one or more virtual objects and coupling optics lens system for directing images from micro display to reflecting surface or element . The microdisplay may be implemented in various technologies including transmissive projection technology micro organic light emitting diode OLED technology or a reflective technology like digital light processing DLP liquid crystal on silicon LCOS and Mirasol display technology from Qualcomm Inc. The reflecting surface directs the light from the micro display into a lightguide optical element which directs the light representing the image into the user s eye. Image data of a virtual object may be registered to a real object meaning the virtual object tracks its position to a position of the real object seen through the see through display device when the real object is in the field of view of the see through displays .

In some embodiments one or more printed content selections being tracked for augmentation may be printed with one or more markers to improve detection of a content selection. Markers may also include metadata describing the content selection. For example a photograph in a magazine may be printed with IR retroreflective markers or RFID tags which include the identifiers for the people in the photograph as well as the place date and time of day at which it was taken. Additionally an identifier of one or more printed or electronic versions of a work in which it has been printed may be included. An IR or RFID unit may detect the marker and send the data it contains to the control circuitry .

In the illustrated embodiment the display optical system is an integrated eye tracking and display system. The system includes a light guide optical element opacity filter and optional see through lens and see through lens . The opacity filter for enhancing contrast of virtual imagery is behind and aligned with optional see through lens lightguide optical element for projecting image data from the microdisplay is behind and aligned with opacity filter and optional see through lens is behind and aligned with lightguide optical element . More details of the light guide optical element and opacity filter are provided below.

Light guide optical element transmits light from micro display to the eye of the user wearing head mounted display device . Light guide optical element also allows light from in front of the head mounted display device to be transmitted through light guide optical element to eye as depicted by arrow representing an optical axis of the display optical system thereby allowing the user to have an actual direct view of the space in front of head mounted display device in addition to receiving a virtual image from micro display . Thus the walls of light guide optical element are see through. Light guide optical element includes a first reflecting surface e.g. a mirror or other surface . Light from micro display passes through lens and becomes incident on reflecting surface . The reflecting surface reflects the incident light from the micro display such that light is trapped inside a waveguide a planar waveguide in this embodiment. A representative reflecting element represents the one or more optical elements like mirrors gratings and other optical elements which direct visible light representing an image from the planar waveguide towards the user eye .

Infrared illumination and reflections also traverse the planar waveguide for an eye tracking system for tracking the position of the user s eyes. The position of the user s eyes and image data of the eye in general may be used for applications such as gaze detection blink command detection and gathering biometric information indicating a personal state of being for the user. The eye tracking system comprises an eye tracking illumination source A and an eye tracking IR sensor B positioned between lens and temple in this example. In one embodiment the eye tracking illumination source A may include one or more infrared IR emitters such as an infrared light emitting diode LED or a laser e.g. VCSEL emitting about a predetermined IR wavelength or a range of wavelengths. In some embodiments the eye tracking sensor B may be an IR camera or an IR position sensitive detector PSD for tracking glint positions.

The use of a planar waveguide as a light guide optical element in this embodiment allows flexibility in the placement of entry and exit optical couplings to and from the waveguide s optical path for the image generation unit the illumination source A and the IR sensor B. In this embodiment a wavelength selective filter passes through visible spectrum light from the reflecting surface and directs the infrared wavelength illumination from the eye tracking illumination source A into the planar waveguide through wavelength selective filter passes through the visible illumination from the micro display and the IR illumination from source A in the optical path heading in the direction of the nose bridge . Reflective element in this example is also representative of one or more optical elements which implement bidirectional infrared filtering which directs IR illumination towards the eye preferably centered about the optical axis and receives IR reflections from the user eye . Besides gratings and such mentioned above one or more hot mirrors may be used to implement the infrared filtering. In this example the IR sensor B is also optically coupled to the wavelength selective filter which directs only infrared radiation from the waveguide including infrared reflections of the user eye preferably including reflections captured about the optical axis out of the waveguide to the IR sensor B.

In other embodiments the eye tracking unit optics are not integrated with the display optics. For more examples of eye tracking systems for HMD devices see U.S. Pat. No. 7 401 920 entitled Head Mounted Eye Tracking and Display System issued Jul. 22 2008 to Kranz et al. see U.S. patent application Ser. No. 13 221 739 Lewis et al. entitled Gaze Detection in a See Through Near Eye Mixed Reality Display filed Aug. 30 2011 and see U.S. patent application Ser. No. 13 245 700 Bohn entitled Integrated Eye Tracking and Display System filed Sep. 26 2011 all of which are incorporated herein by reference.

Another embodiment for tracking the direction of the eyes is based on charge tracking. This concept is based on the observation that a retina carries a measurable positive charge and the cornea has a negative charge. Sensors in some embodiments are mounted by the user s ears near earphones to detect the electrical potential while the eyes move around and effectively read out what the eyes are doing in real time. See Feb. 19 2010 http www.wirefresh.com control your mobile music with eyeball actvated headphones which is hereby incorporated by reference. Eye blinks may be tracked as commands. Other embodiments for tracking eyes movements such as blinks which are based on pattern and motion recognition in image data from the small eye tracking camera B mounted on the inside of the glasses can also be used. The eye tracking camera B sends buffers of image data to the memory under control of the control circuitry .

Opacity filter which is aligned with light guide optical element selectively blocks natural light from passing through light guide optical element for enhancing contrast of virtual imagery. When the system renders a scene for the mixed reality display it takes note of which real world objects are in front of which virtual objects and vice versa. If a virtual object is in front of a real world object then the opacity is turned on for the coverage area of the virtual object. If the virtual object is virtually behind a real world object then the opacity is turned off as well as any color for that display area so the user will only see the real world object for that corresponding area of real light. The opacity filter assists the image of a virtual object to appear more realistic and represent a full range of colors and intensities. In this embodiment electrical control circuitry for the opacity filter not shown receives instructions from the control circuitry via electrical connections routed through the frame.

Again only show half of the head mounted display device . A full head mounted display device would include another set of optional see through lenses and another opacity filter another light guide optical element another micro display another lens system physical environment facing camera also referred to as outward facing or front facing camera eye tracking assembly earphones and sensors if present. Additional details of a head mounted display are illustrated in U.S. patent application Ser. No. 12 905 952 entitled Fusing Virtual Content Into Real Content Filed Oct. 15 2010 fully incorporated herein by reference.

Virtual data engine processes virtual objects and registers the position and orientation of virtual objects in relation to one or more coordinate systems. Additionally the virtual data engine performs the translation rotation scaling and perspective operations using standard image processing methods to make the virtual object appear realistic. A virtual object position may be registered or dependent on a position of a corresponding real object. The virtual data engine determines the position of image data of a virtual object in display coordinates for each display optical system . The virtual data engine may also determine the position of virtual objects in various maps of a real world environment stored in a memory unit of the display device system or of the computing system . One map may be the field of view of the display device with respect to one or more reference points for approximating the locations of the user s eyes. For example the optical axes of the see through display optical systems may be used as such reference points. In other examples the real world environment map may be independent of the display device e.g. a 3D map or model of a location e.g. store coffee shop museum .

One or more processors of the computing system or the display device system or both also execute the object recognition engine to identify real objects in image data captured by the environment facing cameras . As in other image processing applications a person can be a type of object. For example the object recognition engine may implement pattern recognition based on structure data to detect particular objects including a human. The object recognition engine may also include facial recognition software which is used to detect the face of a particular person.

Structure data may include structural information about targets and or objects to be tracked. For example a skeletal model of a human may be stored to help recognize body parts. In another example structure data may include structural information regarding one or more inanimate objects in order to help recognize the one or more inanimate objects. The structure data may store structural information as image data or use image data as references for pattern recognition. The image data may also be used for facial recognition. As printed material typically includes text the structure data may include one or more image datastores including images of numbers symbols e.g. mathematical symbols letters and characters from alphabets used by different languages. Additionally structure data may include handwriting samples of the user for identification. Based on the image data the dynamic printed material application can convert the image data to a computer standardized data format for text with a smaller memory footprint. Some examples of computer standardized text data formats are Unicode based on the Universal Character Set UCS and the American Standard Code for Information Interchange ASCII format. The text data can then be searched against databases for identification of the content including the text or for related information about the content of the text.

Upon detection of one or more objects by the object recognition engine image and audio processing engine may report to operating system an identification of each object detected and a corresponding position and or orientation which the operating system passes along to an application like dynamic printed material application .

The outward facing cameras in conjunction with the gesture recognition engine implements a natural user interface NUI in embodiments of the display device system . Blink commands or gaze duration data identified by the eye tracking software are also examples of physical action user input. Voice commands may also supplement other recognized physical actions such as gestures and eye gaze.

The gesture recognition engine can identify actions performed by a user indicating a control or command to an executing application. The action may be performed by a body part of a user e.g. a hand or finger typically in reading applications but also an eye blink sequence of an eye can be gestures. In one embodiment the gesture recognition engine includes a collection of gesture filters each comprising information concerning a gesture that may be performed by at least a part of a skeletal model. The gesture recognition engine compares a skeletal model and movements associated with it derived from the captured image data to the gesture filters in a gesture library to identify when a user as represented by the skeletal model has performed one or more gestures. In some examples a camera in particular a depth camera in the real environment separate from the display device in communication with the display device system or a computing system may detect the gesture and forward a notification to the system . In other examples the gesture may be performed in view of the cameras by a body part such as the user s hand or one or more fingers.

In some examples matching of image data to image models of a user s hand or finger during gesture training sessions may be used rather than skeletal tracking for recognizing gestures.

More information about the detection and tracking of objects can be found in U.S. patent application Ser. No. 12 641 788 Motion Detection Using Depth Images filed on Dec. 18 2009 and U.S. patent application Ser. No. 12 475 308 Device for Identifying and Tracking Multiple Humans over Time both of which are incorporated herein by reference in their entirety. More information about the gesture recognition engine can be found in U.S. patent application Ser. No. 12 422 661 Gesture Recognizer System Architecture filed on Apr. 13 2009 incorporated herein by reference in its entirety. More information about recognizing gestures can be found in U.S. patent application Ser. No. 12 391 150 Standard Gestures filed on Feb. 23 2009 and U.S. patent application Ser. No. 12 474 655 Gesture Tool filed on May 29 2009 both of which are incorporated by reference herein in their entirety.

The computing environment also stores data in image and audio data buffer s . The buffers provide memory for receiving image data captured from the outward facing cameras image data from an eye tracking camera of an eye tracking assembly if used buffers for holding image data of virtual objects to be displayed by the image generation units and buffers for audio data such as voice commands from the user via microphone and instructions to be sent to the user via earphones .

Device data may include a unique identifier for the computer system a network address e.g. an IP address model number configuration parameters such as devices installed identification of the operation system and what applications are available in the display device system and are executing in the display system etc. Particularly for the see through mixed reality display device system the device data may also include data from sensors or determined from the sensors like the orientation sensors the temperature sensor the microphone the electrical impulse sensor if present and the location and proximity transceivers .

In this embodiment the display device system and other processor based systems used by the user execute a client side version of a push service application which communicates over a communication network with an information push service engine . The information push service engine or application is cloud based in this embodiment. A cloud based engine may include one or more software applications which execute on and store data by one or more networked computer systems. The engine is not tied to a particular location. Some examples of cloud based software are social networking sites and web based email sites like Yahoo and Hotmail . A user may register an account with the information push service engine which grants the information push service permission to monitor the user s executing applications and data generated and received by them as well as user profile data and device data for tracking the user s location and device capabilities. Based on the user profile data aggregated from the user s systems the data received and sent by the executing applications on systems used by the user and location and other sensor data stored in device data the information push service can determine a physical context a social context a personal context like a state of being or a combination of contexts for the user.

The local copies of the user profile data may store some of the same user profile data and the client side push service applications may periodically update their local copies with the user profile data stored by the computer system in an accessible database over a communication network . Some examples of user profile data are the user s expressed preferences the user s friends list the user s preferred activities the user s favorites some examples of which are favorite color favorite foods favorite books favorite author etc. a list of the user s reminders the user s social groups the user s current location and other user created content such as the user s photos images and recorded videos. In one embodiment the user specific information may be obtained from one or more data sources or applications such as the information push service a user s social networking sites contacts or address book schedule data from a calendar application email data instant messaging data user profiles or other sources on the Internet as well as data directly entered by the user. As discussed below a state of being may be derived from eye data and be updated and stored in the user profile data both locally and by the remote push service application . In this embodiment a network accessible state of being rules links identified eye data with a state of being as a reference for deriving the state of being.

Trust levels may be determined by user profile data which identifies people known to the user for example as social networking friends and family members sharing the same gaming service which may be subdivided into different groups based on trust levels. Additionally the user may explicitly identify trust levels in their user profile data using a client side push service application . In one embodiment the cloud based information push service engine aggregates data from user profile data stored on the different user computer systems of the user.

Each version of the push service application also stores in user profile data a tracking history of the user. Some examples of events people and things tracked in the tracking history are locations visited transactions content and real things purchased a reading history a viewing history including viewings of television movies and videos and people detected with whom the user has interacted. If electronically identified friends e.g. social networking friends are registered with the push service application too or they make information available to the user or publicly through other applications the push service application can use this data as well to track the content and social context of the user.

As discussed further below the dynamic printed material application may access one or more search engines for accessing information for identifying a printed content selection and a printed content item including it as well as related virtual data . Examples of resources which may be searched for identification and pertinent virtual data are illustrated as publisher databases and printed content related resources indexed for Internet searching. For example a general purpose search engine like Bing or Google may be accessed as well as a search engine for the Library of Congress university libraries or publisher databases made available to the public or on a subscription basis as may be identified in user profile data. Publishers may have pointers to virtual content in their databases as publishers may have a business model for encouraging virtual content to be developed for their print material. Additionally entities not associated with the publishers or who wish to maintain their own data resources may wish to make virtual content available through their own websites which are Internet indexed resources. From searching on information derived from image data of the printed content selection and the printed content item containing it data fields in metadata for a printed content selection can be filled with values. discussed below provides an example of a printed content selection metadata record.

One advantage of the technology is the ability to update previously published material which was printed without any plan for virtual augmentation. As discussed below a user may be requested to view printed version identifying data on the printed content for example a title page of a book or newspaper or a table of contents of a magazine. Other examples of version identifying data are standardized identifiers an example of which is the International Standard Book Number ISBN for books. The ISBN number on the book identifies data such as a language group publisher title and edition or variation of the book. For periodicals the International Standard Serial Number ISSN identifies the title of a periodical and a Serial Item and Contribution Identifier SICI is a standard used to identify specific volumes articles or other identifiable parts of the periodical. For example the ISSN may identify a periodical for example the Journal of Head Mounted Displays and a SICI identifies an article by bibliographic items some examples of which are title volume and number date of publication beginning and ending pages and content format e.g. TX for printed text. Other content formats may indicate web publication and audiovisual formats. The image data from the outward facing cameras or text converted from image data of the viewed identifying data is sent to one or more search engines .

Once the printed version of the work the user is looking at is identified and the printed content selection is located within it the dynamic printed material application can query one or more search engines to search for virtual content data for the printed content selection based on the printed content item including it. In some embodiments the virtual content data is associated with a work or a work version including the content selection independent of the medium expressing the content. For example paper or other printable material is an example of a medium. Another medium expressing a work is an electronic display or audio recording.

In some instances the virtual data is data specifically generated for appearing in relation to the content selection as laid out in a specific printed version for example on a particular page of a book or other subdivision of printed material. For example a museum may have a plurality of virtual content data selections available for a museum placard associated with an exhibit item based on an age and tracked reading and viewing history of a user indicating an interest in the subject. Image data for each virtual data selection may be specifically formatted to overlay the placard or just portions of the placard.

In other examples virtual content is tied to a medium independent work or work version. For example a professor may store her notes she has made at different points in her printed copy of a textbook to be available for any version of the textbook independent of medium. In other words the content of the textbook is a work. Current previous and future versions of the textbook are versions of the work. The dynamic printed material application links each note to a subdivision of the work in a medium independent organization of the work. For example a note may be linked to a phrase in a particular paragraph which can be identified by executing software instructions for text matching. A paragraph is a medium independent subdivision while a page is dependent on the particular printing or electronic layout. A paperback copy of a textbook with smaller print is a different printed work version from a hardback copy of the textbook in larger print although they contain the exact same version of the textbook content. The professor may allow her virtual notes to be available for storage or streaming at her discretion to students who take her class or past students by granting permission and access to them.

For example the poem Beowulf is a work. The original old English form of the poem is a work version as would be a version which has substituted modern English terms for some of the words. Another example of a version would be a French translation. Another example would be the original old English poem footnoted with comments. A printed version identifier may identify a printed version of the poem on one or more sheets of vellum maintained in a library. This printed version would also have the work version identifier for the original old English form and the work identifier for Beowulf associated with it. A different printed content item version identifier identifies an anthology of English literature which has printed the version of Beowulf footnoted with comments beginning on its page 37. This different printed version has a different printed content item version identifier and work version identifier than the original old English form of the poem but has the same work identifier. For content within the anthology version of the poem selected by a user the position data of the printed content selection is in terms of page 37. In this instance likely the work version position data and the work position data indicate the same stanza.

Publishers may provide access to their datastores of copyrighted works for identification purposes and as a reference for the layout of the work work version or printed version for developers of virtual content. By being able to access the layout of the works particular work versions and particular printed content item versions developers can create virtual content for medium independent and medium dependent versions of a work. As illustrated the databases and and the virtual content may cross reference each other.

For works not subject to copyright datastores under control of libraries particularly those with large collections like the Library of Congress other national libraries universities and large public libraries and book compilation websites like Google Books and sites maintained by universities may be searched for copies of a work a work version or a printed content version for layouts to which to reference position data .

Responsive to the available virtual data selections being identified the dynamic printed material application selects virtual data from the candidates of available virtual data selections based on virtual data selection rules stored in accessible memory which can be local but which can also be network accessible. The virtual data selection rules provide logic for identifying pertinent user data in user profile data related to the content item or selection and the available candidates of virtual data.

Embodiments of methods for the technology and example implementation processes for some of the steps of the methods are presented in figures below. For illustrative purposes the method embodiments below are described in the context of the system embodiments described above. However the method embodiments are not limited to operating in the system embodiments described above and may be implemented in other system embodiments.

If there is virtual data available for the printed content selection then in step the application selects virtual data from the available virtual data based on user profile data and causes display in step of the virtual data in a position registered to a position of the printed content selection.

Some examples of tasks which display personalized virtual data are described below and include a learning assistance task a directions task and a task which allows a user to substitute or fill in a name of a character or a location. As mentioned above a user can define tasks as well.

In step a query is formulated based on the one or more version identifying sections and sent in step to a search engine for a printed content item version identifier. The dynamic printed material application receives a printed content item version identifier in step . Optionally in step responsive to verifying the identity of the printed content item the dynamic printed material application receives a medium independent work identifier and any applicable medium independent work version identifier. The dynamic application may also receive a work identifier and work version identifier by using the printed content item version identifier as an index into publisher databases or Internet indexed resources .

The dynamic printed material application in step receives user input selecting define gesture and in step receives user input selecting a task or subtask from the menu. The outward facing cameras in step capture image data of a gesture performed by the user of which the dynamic printed material application is notified and in step the dynamic printed material application associates the gesture as a request for the task or sub task selected in the menu.

Some printed material like books and periodicals may be printed with a layout including designated spots for virtual data. For example next to a photograph with a marker with metadata identifying the photograph and related virtual content or data may be a space of predetermined dimensions where the related virtual data fits. The space may also have a marker e.g. an RFID tag or an IR marker identifying the virtual data to display there. However even for content pre printed for augmentation by virtual data a user may activate a task such as a search task and receive data for which the page has not been preformatted. The software executing in the computing environment on the display device system the remote computer system or both determines where to place the virtual data. A user may also designate placement through physical action. For example a user may gaze at virtual data for a duration and then gaze at a blank spot on a sheet or a page. In another example a user may point to a virtual object with a finger and drag the finger to another spot on the sheet or page.

If the interline position is not suitable in step the dynamic application determines whether the virtual data content fits any margin positions and still satisfies visibility criteria. If one or more satisfactory margin positions are available the dynamic application selects a satisfactory margin position closest to the printed content selection in step . If a satisfactory margin position is not available the dynamic printed material application formats the virtual content into one or more sections having the layout characteristics of the current section in step and in step displays the one or more sections with the formatted virtual content after the current section in the layout of the printed material. An example of a current section is a page. Layout characteristics for a page as a section include typical page layout settings. Some examples of such settings are margins page number placement interline spacing spacing around pictures font and font size. Some examples of the layout of the printed material may be a newspaper a book a magazine or a greeting card. In the example of printed material as a book the one or more sections formatted with the virtual content may be made to appear as additional pages of the book.

In the example of the virtual data is formatted to appear within the perimeter of the physical printed material. In other examples a floating position may also be a position option. For example a margin space may appear to be extended to include a picture linked to a content selection for which annotations already take up the nearest margin space. In another example a floating explanatory paragraph may appear to pop up perpendicularly out of the page in an interline space near a concept it explains. As illustrated in the example of below a virtual version of the printed content selection may be assigned a floating position linked to a user field of view rather than the printed material itself.

A state of being may be determined from data sensed of the user s body as well as information tracked from other applications and location data as well. For example based on location data and a calendar application a state of being application may indicate the user is early or late for a meeting. Image data of the user s eye from an eye tracking assembly also referred to as eye data may indicate the user is experiencing strong emotion while also being late. More than one state of being may apply.

Some examples of user physical characteristics which may be identified in eye data and linked with a state of being in the state of being rules are blinking rate pupil size and pupil size changes. In some embodiments the see through display device system may also have a biometric sensor like a pulse rate measuring sensor which presses against a user temple. One example of a physical characteristic which can indicate a state of being is blinking beyond a certain level as detected from image data glint data or sensors . Such blinking may indicate strong emotion. More simply a detected closed eyelid for a period of time can indicate a state of being as sleeping. A tearful state of the eye can also be detected from its reflectivity to indicate crying.

Pupil size and the pupil size stability may indicate a state of being sleepy or tired. Pupil size changes with lighting changes. If the pupil is treated as an ellipse if the lighting does not change one axis of the ellipse the major axis remains constant as it represents the diameter of the pupil. The width of the minor axis of the ellipse changes with gaze changes. The light meters not shown of the front facing cameras can detect lighting changes. Therefore pupil dilation due to factors other than lighting changes can also be determined. Sleepiness and sleep deprivation may cause the pupil s overall size to shrink if tired and the pupil size to become less stable fluctuating in size. Pupil dilation beyond a criteria under steady state lighting conditions may also indicate a reaction to an emotional stimuli. However pupil dilation may also be associated with activity.

Therefore software such as a client push service application discussed below may correlate the pupil dilation with at least a state of being data setting of strong emotion if from image data from the outward or physical environment facing cameras and small head movement indicated by the motion sensors indicate the user is not exercising for example the user appears to be sitting down in his or her office. The object being viewed as indicated by the image data from the outward facing cameras may provide more data e.g. a young child looking at a picture of a real dinosaur skeleton or the printed content item has been identified as a particular scary novel and based on time of day location data and field of view data over time the reader is home alone at night. In another example the image data indicated a view of one of the user s typical lunchtime running paths and the motion sensors indicated a running or jogging speed within a time period e.g. two minutes before a newspaper has been identified by the dynamic printed material application in image data of the field of view. In this example the state of being data settings may include awake and neutral emotion and may include exercising and running as activity data settings depending on time periods from stopping for identifying an end of an activity.

In one embodiment either version client or server of the push application may include software for determining the state of being. The software may implement one or more heuristic algorithms based on the state of being rules to determine a state of being of the user based on both the eye data and image and audio data of the user s surroundings. The client push service application updates the current state of being data stored in user profile data in step .

A current user location based on sensor data of the display device system is determined in step . For example the current user location can be identified by GPS data image data of locations and even IP addresses of network access points associated with particular locations. In step the dynamic printed material application identifies data available to the user of other persons linked to the user in user profile which data is related to the printed content selection. For example if a printed content item is a scientific journal with an article by authors and one of the user s social networking friends commented on the article on his social networking site page the friend s comment would be identified.

In this embodiment the dynamic printed application in step assigns weightings to the user profile data based on virtual data selection rules . In some examples the virtual data selection rules may prioritize items from user profile data. For example the following categories of user profile items may be prioritized in order starting from highest as identity data state of being data location data the current or most recent printed content selection the printed content item being viewed and other users data related to the selection or item. In step the dynamic printed application selects virtual data from virtual data candidates for the printed content selection based on the weighted user profile data. For example identity data may include languages known by the user. If the user only knows English virtual data with English text has a higher probability of being selected than Spanish language virtual data. Also as per the example below for a five year old coloring in a coloring book pictorial virtual data may be selected whereas virtual data including simple text may be displayed for a seven year old.

Each of the man and the boy is looking at a respective printed medium of a real museum placard in this example near an exhibit of a partial skeleton of a stegosaurus dinosaur. An invisible marker transmits an identifier which the dynamic printed material application can include in requests to the museum s database of virtual data via RFID or IR or Bluetooth to a corresponding receiver e.g. one or more location sensors on the display device system within a range of the marker. In other examples the content of the placard e.g. the title Stegosaurus would be the identifier. The printed material on the placard states the name of the dinosaur type and includes brief explanatory text represented by XXX . . . . Based on user profile data the adult man is identified as a fluent English speaker being 34 years old of height 5 foot 7 inches of neutral emotional state and having a college degree in accounting. His TV tracking history indicates he has viewed 24 episodes of the TV program Nature over the past year and a half including one on dinosaur bones found in the Western United States. Metadata for the available virtual data candidates includes age language US reading levels educational background and interest level in particular natural history subjects.

The dynamic printed material application applies virtual data selection rules and give highest priority to the man s age and language fluency. Based on the metadata available for the virtual data candidates the man s educational background and TV history of watching natural history related programs also form a basis for forming a query for searching the metadata of the virtual data candidates. Image data of virtual text section has been selected which provides additional details about the stegosaurus bone structure. Metadata stored for the virtual text section indicates the text is suitable for person with an average U.S. college reading level and above average knowledge of dinosaurs with respect to the general U.S. population based on a 2008 survey.

Line shows the adult s height compared to the height of the stegosaurus species from which the exhibit bones came.

Identity user profile data indicates the boy is 6 years old. His eye data indicates strong emotion and crying has been indicated in the last 30 minutes. Speech recognition software has identified I m scared being said by the voice of the boy 34 minutes ago from monitored audio received by the microphone on his device. Based on his locations being near dinosaur skeleton exhibits for the past hour a heuristic algorithm implemented by his client push service application selects a state of being as fearful. According to the virtual data selection rules a state of being of fearful causes state of being to receive a higher priority weighting from the dynamic application in user profile data than if his state were a more emotionally neutral state. Age also has a highest priority weighting in this example. Virtual data of an animation of a stegosaurus with softer features and a friendly eye as may be found in a children s book illustration is selected and displayed based on metadata associated with virtual data indicating a target audience of children 7 and under and animation. For a 6 year old user the selection rules weight non text image data higher than text image data.

A parent may select content categories for a child via the information push service application which stores the parental controls in the child s user profile data for access by the child s dynamic printed material application . That way a child can see content or content from categories that are not otherwise blocked by parental controls.

In this example a user is viewing a guidebook on U.S. National Parks including Yosemite National Park as a printed content item. As illustrated by the eye lines of sight and the user s point of gaze is Half Dome in a picture of the Yosemite Valley at a location known as the Valley View rest stop. The user s finger tip of the user s hand is pointing to the picture which the dynamic printed material application identifies from field of view image data from the cameras as a gesture which is physical action user input selecting photo as a printed content selection.

In this example the client push service application is executing and monitoring the applications executing on the user display device system including the dynamic printed material application . The client application sends at least a subset of the metadata for the printed content selection to the information push service which monitors applications and user profile data of other users who have granted access permission to the user. The information push service identifies a photo having keywords in its metadata matching the metadata of the printed content selection on a social networking site of a user friend Sue. Based on a tracking history of which friends pages on social networking sites the user has visited or who has visited Sue s page the information push service identifies that the user has not seen this recently posted photo. The information push service application sends a copy of Sue s photo to the user s client push service application which notifies the dynamic printed material application . The dynamic application may perform image comparison with the photo in the print layout of the guidebook to make sure the metadata on Sue s photo is accurate or to determine what is actually depicted. Assuming data results from the image comparison indicate a satisfactory relevancy score for Sue s photo according to the virtual data selection rules Sue s photo is displayed as shown in .

Besides personalizing data based on a connection between a printed content selection or item and user profile data a task or action implemented by an application is performed with respect to a printed content selection. illustrate example embodiments of tasks which may select and display virtual data based on user profile data.

In the implementation process example illustrated in under the control of the dynamic printed material application the sound recognition engine in step monitors audio data received via the display device microphone for one or more words of the printed content item and in step identifies a question by the user in the audio data with respect to the one or more words. For example the sound recognition engine can identify a question from the words used and their order in a sequence between gaps in voice data and voice change rules such as a voice goes up in volume near the end of a question. The sound recognition engine accesses models of human speech patterns particularly for the linguistic background of the user for identifying a question from a statement. The dynamic printed material application is notified and in step formulates a query based on the one or more words of the printed content item and the question and in step searches one or more datastores including virtual data based on the query. An example of such a datastore is the publishers databases e.g. the printed content item s publisher and the Internet indexed resources . In step the dynamic application determines whether the search returned virtual data based on the query and if so the dynamic application causes the image generation unit in step to display the virtual data in a position registered to the one or more words in the printed content item.

If the search did not return virtual data based on the query the dynamic printed material application may proceed with an Internet search in step and then format and cause to be displayed in step any Internet search results in a position registered to a position of the one or more words in the printed content item.

In step the dynamic printed material application receives one or more words for replacement and in step receives one or more substitution words for the one or more words for replacement via user input. For example a user may enter the words for replacement and substitution via a virtual menu a user input device of the processing unit or via audio data listing for example locations or pointing at people and character names or gaze duration at people and character names and the like. In step each paragraph to include the one or more substitution words is identified and in step a virtual version of each identified paragraph is generated based on layout characteristics of the printed content item. Some examples of layout characteristics are font margin spacing and line spacing. Basically the goal is to make the substituted words appear as if they were originally printed in the book.

Other than the unusual case where a word substituted and replaced has the same dimensions e.g. same number of letters and the combined letter width is the same the substituted words will either shorten or lengthen paragraphs. In step the dynamic application determines whether there are any paragraphs having visibility of their content effected by a virtual version of a paragraph. Visibility of a printed paragraph may be effected by being partially overlaid by a virtual paragraph. If not then the virtual paragraph versions for the printed content item are complete and then each virtual version paragraph is displayed while the paragraph is in the field of view of the see through display device in step . Otherwise a virtual version of each paragraph having its visibility effected is generated in step . As each time a virtual paragraph is generated the visibility of a printed paragraph may be effected the determination of step is repeated until there are no more printed paragraphs with effected visibility. There may be extra line spaces here and there if the substituted words are shorter using this process but that is less distracting than changes in font size or line spacing and such.

Device may also contain communications connection s such as one or more network interfaces and transceivers that allow the device to communicate with other devices. Device may also have input device s such as keyboard mouse pen voice input device touch input device etc. Output device s such as a display speakers printer etc. may also be included. All these devices are well known in the art and need not be discussed at length here.

As discussed above the processing unit may be embodied in a mobile device . is a block diagram of an exemplary mobile device which may operate in embodiments of the technology. Exemplary electronic circuitry of a typical mobile phone is depicted. The phone includes one or more microprocessors and memory e.g. non volatile memory such as ROM and volatile memory such as RAM which stores processor readable code which is executed by one or more processors of the control processor to implement the functionality described herein.

Mobile device may include for example processors memory including applications and non volatile storage. The processor can implement communications as well as any number of applications including the interaction applications discussed herein. Memory can be any variety of memory storage media types including non volatile and volatile memory. A device operating system handles the different operations of the mobile device and may contain user interfaces for operations such as placing and receiving phone calls text messaging checking voicemail and the like. The applications can be any assortment of programs such as a camera application for photos and or videos an address book a calendar application a media player an internet browser games other multimedia applications an alarm application other third party applications like a skin application and image processing software for processing image data to and from the display device discussed herein and the like. The non volatile storage component in memory contains data such as web caches music photos contact data scheduling data and other files.

The processor also communicates with RF transmit receive circuitry which in turn is coupled to an antenna with an infrared transmitted receiver with any additional communication channels like Wi Fi WUSB RFID infrared or Bluetooth and with a movement orientation sensor such as an accelerometer. Accelerometers have been incorporated into mobile devices to enable such applications as intelligent user interfaces that let users input commands through gestures indoor GPS functionality which calculates the movement and direction of the device after contact is broken with a GPS satellite and to detect the orientation of the device and automatically change the display from portrait to landscape when the phone is rotated. An accelerometer can be provided e.g. by a micro electromechanical system MEMS which is a tiny mechanical device of micrometer dimensions built onto a semiconductor chip. Acceleration direction as well as orientation vibration and shock can be sensed. The processor further communicates with a ringer vibrator a user interface keypad screen biometric sensor system a speaker a microphone a camera a light sensor and a temperature sensor .

The processor controls transmission and reception of wireless signals. During a transmission mode the processor provides a voice signal from microphone or other data signal to the RF transmit receive circuitry . The transmit receive circuitry transmits the signal to a remote station e.g. a fixed station operator other cellular phones etc. for communication through the antenna . The ringer vibrator is used to signal an incoming call text message calendar reminder alarm clock reminder or other notification to the user. During a receiving mode the transmit receive circuitry receives a voice or other data signal from a remote station through the antenna . A received voice signal is provided to the speaker while other received data signals are also processed appropriately.

Additionally a physical connector can be used to connect the mobile device to an external power source such as an AC adapter or powered docking station. The physical connector can also be used as a data connection to a computing device. The data connection allows for operations such as synchronizing mobile device data with the computing data on another device.

A GPS receiver utilizing satellite based radio navigation to relay the position of the user applications is enabled for such service.

The example computer systems illustrated in the figures include examples of computer readable storage devices. Computer readable storage devices are also processor readable storage device. Such devices may include volatile and nonvolatile removable and non removable memory devices implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Some examples of processor or computer readable storage devices are RAM ROM EEPROM cache flash memory or other memory technology CD ROM digital versatile disks DVD or other optical disk storage memory sticks or cards magnetic cassettes magnetic tape a media drive a hard disk magnetic disk storage or other magnetic storage devices or any other device which can be used to store the desired information and which can be accessed by a computer.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

