---

title: Apparatus and method for implementing a scratchpad memory using priority hint
abstract: An apparatus and method for implementing a scratchpad memory within a cache using priority hints. For example, a method according to one embodiment comprises: providing a priority hint for a scratchpad memory implemented using a portion of a cache; determining a page replacement priority based on the priority hint; storing the page replacement priority in a page table entry (PTE) associated with the page; and using the page replacement priority to determine whether to evict one or more cache lines associated with the scratchpad memory from the cache.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09158702&OS=09158702&RS=09158702
owner: INTEL CORPORATION
number: 09158702
owner_city: Santa Clara
owner_country: US
publication_date: 20121228
---
This invention relates generally to the field of computer processors. More particularly the invention relates to an apparatus and method for implementing a scratchpad memory.

Scratchpad memories are local high speed memories that are manually controlled by the application. By precisely controlling data movements to and from scratchpads applications can maximize performance utilization and energy efficiency. IBM s Cell and NVIDIA s recent GPUs for example provide such mechanisms.

Due to high hardware costs and a large increase to the architectural state however scratchpad memories are sometimes emulated on top of a cache based memory hierarchy typically by adjusting the cache line replacement policy e.g. such as a least recently used LRU policy . For example a processor will provide user level instructions to directly adjust the replacement priority of a cache line so that the application can effectively pin a region of memory in the cache.

However allowing user level code to directly modify the cache replacement priority exposes fairness and security issues. For example malicious code may aggressively mark its cache lines as pseudo pinned resulting in unfair utilization of shared cache space. Additionally since the cache replacement priority is not maintained by the operating system priority adjustments may survive context switching boundaries and inadequately endow privileges to inappropriate software contexts i.e. a process that is switched out may still occupy most all of the cache space with pseudo pinned lines .

In the following description for the purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of the embodiments of the invention described below. It will be apparent however to one skilled in the art that the embodiments of the invention may be practiced without some of these specific details. In other instances well known structures and devices are shown in block diagram form to avoid obscuring the underlying principles of the embodiments of the invention.

In a processor pipeline includes a fetch stage a length decode stage a decode stage an allocation stage a renaming stage a scheduling also known as a dispatch or issue stage a register read memory read stage an execute stage a write back memory write stage an exception handling stage and a commit stage .

The front end unit includes a branch prediction unit coupled to an instruction cache unit which is coupled to an instruction translation lookaside buffer TLB which is coupled to an instruction fetch unit which is coupled to a decode unit . The decode unit or decoder may decode instructions and generate as an output one or more micro operations microcode entry points microinstructions other instructions or other control signals which are decoded from or which otherwise reflect or are derived from the original instructions. The decode unit may be implemented using various different mechanisms. Examples of suitable mechanisms include but are not limited to look up tables hardware implementations programmable logic arrays PLAs microcode read only memories ROMs etc. In one embodiment the core includes a microcode ROM or other medium that stores microcode for certain macroinstructions e.g. in decode unit or otherwise within the front end unit . The decode unit is coupled to a rename allocator unit in the execution engine unit .

The execution engine unit includes the rename allocator unit coupled to a retirement unit and a set of one or more scheduler unit s . The scheduler unit s represents any number of different schedulers including reservations stations central instruction window etc. The scheduler unit s is coupled to the physical register file s unit s . Each of the physical register file s units represents one or more physical register files different ones of which store one or more different data types such as scalar integer scalar floating point packed integer packed floating point vector integer vector floating point status e.g. an instruction pointer that is the address of the next instruction to be executed etc. In one embodiment the physical register file s unit comprises a vector registers unit a write mask registers unit and a scalar registers unit. These register units may provide architectural vector registers vector mask registers and general purpose registers. The physical register file s unit s is overlapped by the retirement unit to illustrate various ways in which register renaming and out of order execution may be implemented e.g. using a reorder buffer s and a retirement register file s using a future file s a history buffer s and a retirement register file s using a register maps and a pool of registers etc. . The retirement unit and the physical register file s unit s are coupled to the execution cluster s . The execution cluster s includes a set of one or more execution units and a set of one or more memory access units . The execution units may perform various operations e.g. shifts addition subtraction multiplication and on various types of data e.g. scalar floating point packed integer packed floating point vector integer vector floating point . While some embodiments may include a number of execution units dedicated to specific functions or sets of functions other embodiments may include only one execution unit or multiple execution units that all perform all functions. The scheduler unit s physical register file s unit s and execution cluster s are shown as being possibly plural because certain embodiments create separate pipelines for certain types of data operations e.g. a scalar integer pipeline a scalar floating point packed integer packed floating point vector integer vector floating point pipeline and or a memory access pipeline that each have their own scheduler unit physical register file s unit and or execution cluster and in the case of a separate memory access pipeline certain embodiments are implemented in which only the execution cluster of this pipeline has the memory access unit s . It should also be understood that where separate pipelines are used one or more of these pipelines may be out of order issue execution and the rest in order.

The set of memory access units is coupled to the memory unit which includes a data TLB unit coupled to a data cache unit coupled to a level 2 L2 cache unit . In one exemplary embodiment the memory access units may include a load unit a store address unit and a store data unit each of which is coupled to the data TLB unit in the memory unit . The instruction cache unit is further coupled to a level 2 L2 cache unit in the memory unit . The L2 cache unit is coupled to one or more other levels of cache and eventually to a main memory.

By way of example the exemplary register renaming out of order issue execution core architecture may implement the pipeline as follows 1 the instruction fetch performs the fetch and length decoding stages and 2 the decode unit performs the decode stage 3 the rename allocator unit performs the allocation stage and renaming stage 4 the scheduler unit s performs the schedule stage 5 the physical register file s unit s and the memory unit perform the register read memory read stage the execution cluster perform the execute stage 6 the memory unit and the physical register file s unit s perform the write back memory write stage 7 various units may be involved in the exception handling stage and 8 the retirement unit and the physical register file s unit s perform the commit stage .

The core may support one or more instructions sets e.g. the x86 instruction set with some extensions that have been added with newer versions the MIPS instruction set of MIPS Technologies of Sunnyvale Calif. the ARM instruction set with optional additional extensions such as NEON of ARM Holdings of Sunnyvale Calif. including the instruction s described herein. In one embodiment the core includes logic to support a packed data instruction set extension e.g. AVX1 AVX2 and or some form of the generic vector friendly instruction format U 0 and or U 1 described below thereby allowing the operations used by many multimedia applications to be performed using packed data.

It should be understood that the core may support multithreading executing two or more parallel sets of operations or threads and may do so in a variety of ways including time sliced multithreading simultaneous multithreading where a single physical core provides a logical core for each of the threads that physical core is simultaneously multithreading or a combination thereof e.g. time sliced fetching and decoding and simultaneous multithreading thereafter such as in the Intel Hyperthreading technology .

While register renaming is described in the context of out of order execution it should be understood that register renaming may be used in an in order architecture. While the illustrated embodiment of the processor also includes separate instruction and data cache units and a shared L2 cache unit alternative embodiments may have a single internal cache for both instructions and data such as for example a Level 1 L1 internal cache or multiple levels of internal cache. In some embodiments the system may include a combination of an internal cache and an external cache that is external to the core and or the processor. Alternatively all of the cache may be external to the core and or the processor.

Thus different implementations of the processor may include 1 a CPU with the special purpose logic being integrated graphics and or scientific throughput logic which may include one or more cores and the cores A N being one or more general purpose cores e.g. general purpose in order cores general purpose out of order cores a combination of the two 2 a coprocessor with the cores A N being a large number of special purpose cores intended primarily for graphics and or scientific throughput and 3 a coprocessor with the cores A N being a large number of general purpose in order cores. Thus the processor may be a general purpose processor coprocessor or special purpose processor such as for example a network or communication processor compression engine graphics processor GPGPU general purpose graphics processing unit a high throughput many integrated core MIC coprocessor including 30 or more cores embedded processor or the like. The processor may be implemented on one or more chips. The processor may be a part of and or may be implemented on one or more substrates using any of a number of process technologies such as for example BiCMOS CMOS or NMOS.

The memory hierarchy includes one or more levels of cache within the cores a set or one or more shared cache units and external memory not shown coupled to the set of integrated memory controller units . The set of shared cache units may include one or more mid level caches such as level 2 L2 level 3 L3 level 4 L4 or other levels of cache a last level cache LLC and or combinations thereof. While in one embodiment a ring based interconnect unit interconnects the integrated graphics logic the set of shared cache units and the system agent unit integrated memory controller unit s alternative embodiments may use any number of well known techniques for interconnecting such units. In one embodiment coherency is maintained between one or more cache units and cores A N.

In some embodiments one or more of the cores A N are capable of multi threading. The system agent includes those components coordinating and operating cores A N. The system agent unit may include for example a power control unit PCU and a display unit. The PCU may be or include logic and components needed for regulating the power state of the cores A N and the integrated graphics logic . The display unit is for driving one or more externally connected displays.

The cores A N may be homogenous or heterogeneous in terms of architecture instruction set that is two or more of the cores A N may be capable of execution the same instruction set while others may be capable of executing only a subset of that instruction set or a different instruction set. In one embodiment the cores A N are heterogeneous and include both the small cores and big cores described below.

Referring now to shown is a block diagram of a system in accordance with one embodiment of the present invention. The system may include one or more processors which are coupled to a controller hub . In one embodiment the controller hub includes a graphics memory controller hub GMCH and an Input Output Hub IOH which may be on separate chips the GMCH includes memory and graphics controllers to which are coupled memory and a coprocessor the IOH is couples input output I O devices to the GMCH . Alternatively one or both of the memory and graphics controllers are integrated within the processor as described herein the memory and the coprocessor are coupled directly to the processor and the controller hub in a single chip with the IOH .

The optional nature of additional processors is denoted in with broken lines. Each processor may include one or more of the processing cores described herein and may be some version of the processor .

The memory may be for example dynamic random access memory DRAM phase change memory PCM or a combination of the two. For at least one embodiment the controller hub communicates with the processor s via a multi drop bus such as a frontside bus FSB point to point interface such as QuickPath Interconnect QPI or similar connection .

In one embodiment the coprocessor is a special purpose processor such as for example a high throughput MIC processor a network or communication processor compression engine graphics processor GPGPU embedded processor or the like. In one embodiment controller hub may include an integrated graphics accelerator.

There can be a variety of differences between the physical resources in terms of a spectrum of metrics of merit including architectural microarchitectural thermal power consumption characteristics and the like.

In one embodiment the processor executes instructions that control data processing operations of a general type. Embedded within the instructions may be coprocessor instructions. The processor recognizes these coprocessor instructions as being of a type that should be executed by the attached coprocessor . Accordingly the processor issues these coprocessor instructions or control signals representing coprocessor instructions on a coprocessor bus or other interconnect to coprocessor . Coprocessor s accept and execute the received coprocessor instructions.

Referring now to shown is a block diagram of a first more specific exemplary system in accordance with an embodiment of the present invention. As shown in multiprocessor system is a point to point interconnect system and includes a first processor and a second processor coupled via a point to point interconnect . Each of processors and may be some version of the processor . In one embodiment of the invention processors and are respectively processors and while coprocessor is coprocessor . In another embodiment processors and are respectively processor coprocessor .

Processors and are shown including integrated memory controller IMC units and respectively. Processor also includes as part of its bus controller units point to point P P interfaces and similarly second processor includes P P interfaces and . Processors may exchange information via a point to point P P interface using P P interface circuits . As shown in IMCs and couple the processors to respective memories namely a memory and a memory which may be portions of main memory locally attached to the respective processors.

Processors may each exchange information with a chipset via individual P P interfaces using point to point interface circuits . Chipset may optionally exchange information with the coprocessor via a high performance interface . In one embodiment the coprocessor is a special purpose processor such as for example a high throughput MIC processor a network or communication processor compression engine graphics processor GPGPU embedded processor or the like.

A shared cache not shown may be included in either processor or outside of both processors yet connected with the processors via P P interconnect such that either or both processors local cache information may be stored in the shared cache if a processor is placed into a low power mode.

Chipset may be coupled to a first bus via an interface . In one embodiment first bus may be a Peripheral Component Interconnect PCI bus or a bus such as a PCI Express bus or another third generation I O interconnect bus although the scope of the present invention is not so limited. As shown in various I O devices may be coupled to first bus along with a bus bridge which couples first bus to a second bus . In one embodiment one or more additional processor s such as coprocessors high throughput MIC processors GPGPU s accelerators such as e.g. graphics accelerators or digital signal processing DSP units field programmable gate arrays or any other processor are coupled to first bus . In one embodiment second bus may be a low pin count LPC bus. Various devices may be coupled to a second bus including for example a keyboard and or mouse communication devices and a storage unit such as a disk drive or other mass storage device which may include instructions code and data in one embodiment. Further an audio I O may be coupled to the second bus . Note that other architectures are possible. For example instead of the point to point architecture of a system may implement a multi drop bus or other such architecture.

Referring now to shown is a block diagram of a second more specific exemplary system in accordance with an embodiment of the present invention. Like elements in bear like reference numerals and certain aspects of have been omitted from in order to avoid obscuring other aspects of .

Referring now to shown is a block diagram of a SoC in accordance with an embodiment of the present invention. Similar elements in bear like reference numerals. Also dashed lined boxes are optional features on more advanced SoCs. In an interconnect unit s is coupled to an application processor which includes a set of one or more cores A N and shared cache unit s a system agent unit a bus controller unit s an integrated memory controller unit s a set or one or more coprocessors which may include integrated graphics logic an image processor an audio processor and a video processor an static random access memory SRAM unit a direct memory access DMA unit and a display unit for coupling to one or more external displays. In one embodiment the coprocessor s include a special purpose processor such as for example a network or communication processor compression engine GPGPU a high throughput MIC processor embedded processor or the like.

Embodiments of the mechanisms disclosed herein may be implemented in hardware software firmware or a combination of such implementation approaches. Embodiments of the invention may be implemented as computer programs or program code executing on programmable systems comprising at least one processor a storage system including volatile and non volatile memory and or storage elements at least one input device and at least one output device.

Program code such as code illustrated in may be applied to input instructions to perform the functions described herein and generate output information. The output information may be applied to one or more output devices in known fashion. For purposes of this application a processing system includes any system that has a processor such as for example a digital signal processor DSP a microcontroller an application specific integrated circuit ASIC or a microprocessor.

The program code may be implemented in a high level procedural or object oriented programming language to communicate with a processing system. The program code may also be implemented in assembly or machine language if desired. In fact the mechanisms described herein are not limited in scope to any particular programming language. In any case the language may be a compiled or interpreted language.

One or more aspects of at least one embodiment may be implemented by representative instructions stored on a machine readable medium which represents various logic within the processor which when read by a machine causes the machine to fabricate logic to perform the techniques described herein. Such representations known as IP cores may be stored on a tangible machine readable medium and supplied to various customers or manufacturing facilities to load into the fabrication machines that actually make the logic or processor.

Such machine readable storage media may include without limitation non transitory tangible arrangements of articles manufactured or formed by a machine or device including storage media such as hard disks any other type of disk including floppy disks optical disks compact disk read only memories CD ROMs compact disk rewritable s CD RWs and magneto optical disks semiconductor devices such as read only memories ROMs random access memories RAMs such as dynamic random access memories DRAMs static random access memories SRAMs erasable programmable read only memories EPROMs flash memories electrically erasable programmable read only memories EEPROMs phase change memory PCM magnetic or optical cards or any other type of media suitable for storing electronic instructions.

Accordingly embodiments of the invention also include non transitory tangible machine readable media containing instructions or containing design data such as Hardware Description Language HDL which defines structures circuits apparatuses processors and or system features described herein. Such embodiments may also be referred to as program products.

In some cases an instruction converter may be used to convert an instruction from a source instruction set to a target instruction set. For example the instruction converter may translate e.g. using static binary translation dynamic binary translation including dynamic compilation morph emulate or otherwise convert an instruction to one or more other instructions to be processed by the core. The instruction converter may be implemented in software hardware firmware or a combination thereof. The instruction converter may be on processor off processor or part on and part off processor.

As illustrated in scratchpad memories are sometimes emulated on top of a cache based memory hierarchy typically by adjusting the cache line replacement policy employed by cache line replacement logic e.g. such as a least recently used LRU policy . For example a processor core will provide user level instructions to directly adjust the replacement priority of a cache line so that the user application can effectively pin a region of memory in the cache.

However allowing user level code to directly modify the cache replacement priority exposes fairness and security issues. For example malicious code may aggressively mark its cache lines as pseudo pinned resulting in unfair utilization of shared cache space. Additionally since the cache replacement priority is not maintained by the operating system priority adjustments may survive context switching boundaries and inadequately endow privileges to inappropriate software contexts i.e. a process that is switched out may still occupy most all of the cache space with pseudo pinned lines .

One embodiment of the invention employs a software hardware co design approach to solve the foregoing problems. Specifically an operating system is used to enforce fair resource sharing and access control and hardware is used for acceleration. As illustrated in one embodiment of the invention includes components within the user application programming interface API operating system OS and processor core microarchitecture .

In this embodiment an API is exposed to user applications so that the user can supply replacement priority hints on a page granularity e.g. one hint per page . An example C style pseudo code for providing replacement priority hints is shown below 

As indicated in the pseudo code to implement a scratchpad memory on top of a cache a user may specify a hint REPL PRIORITY HINT SCRATCHPAD to indicate a priority for the scratchpad implementation. Also shown are hints related to eviction policy least recently used LRU and most recently used MRU . Thus the same priority hint mechanism described herein for scratchpad memory may be used to specify priority in other implementations e.g. LRU low priority MRU high priority etc .

Internally the API will result in a system call to convey the priority hint information to the OS . Alternatively the replacement hint could be provided with a memory allocation request in a similar manner to the existing mechanism for applications to request large pages and the application must be prepared to handle denied requests e.g. if an application reaches a limit on the number of scratchpad pages .

To determine the actual per page replacement priority one embodiment of the OS includes replacement priority logic which combines the hint conveyed through the user API with other metrics which may be determined from the processor core microarchitecture such as hardware performance counters and monitoring schemes. Such metrics allow the replacement priority logic to keep track of shared cache usage and to adjust the priority for each page to enable fair sharing. In this sense the priority information provided through the user API is not strictly binding. Rather it is used in combination with other available information to arrive at a replacement priority for each page .

Once the actual replacement priority has been determined one embodiment of the OS records the information as a separate field in a page table entry PTE . illustrates an example modification to an PTE for an x86 implementation. It should be noted however that the underlying principles of the invention are not limited to an x86 architecture.

As illustrated in in one embodiment the replacement priority information stored in a page table entry will be loaded into the TLB through the existing TLB refill mechanism. Conveyed priority information can then be used by cache line replacement logic to determine the replacement candidates in a cache set including the scratchpad portion of the cache . In one embodiment per page priority information is then recorded alongside the per line state that is already maintained by the cache line replacement policy . Thus when determining a cache line victim the cache line replacement logic takes both the per page and per cache line priorities into account and selects the victim accordingly.

A method in accordance with one embodiment of the invention is illustrated in . At a priority hint is provided for one or more pages e.g. via the user API as described above . At the priority hint is combined with other metrics e.g. hardware counters cache miss rate for the page etc to arrive at a replacement priority for the page. The replacement priority is then stored in the PTE for that page within the page table. At the cache line replacement logic uses both the per page and per line priorities or other per line variables to identify victims i.e. cache lines to be replaced . At the identified victims are removed from the cache using standard cache management techniques.

Two specific implementations are contemplated identified below as implementation 1 and implementation 2 

1. Whenever a cache line is brought into a cache the corresponding priority information for the cache line is retrieved from the TLB . If a specific cache level does not have TLBs e.g. L2 caches priority information can be propagated from the upper level e.g. L1 caches to lower levels through cache line request messages.

2. Retrieved per page priority information is then recorded alongside the per line state that is maintained by the replacement policy.

3. When a victim has to be determined for replacement the replacement algorithm implemented by the cache line replacement logic takes both the per page and per line priorities into account and selects the victim accordingly.

4. To prevent a high priority being maintained for cache lines on a switched out process the operating system may on a context switch optionally reset the priority state in the cache. When the previously running context returns items 1 3 described above will replenish the per page priority information. In an alternative embodiment when the operating system switches out a process it systematically flushes all lines from high priority pages owned by that process when it switches in a process it prefetches all lines from all high priority pages.

1. For a cache level that has a TLB whenever a cache line needs to be replaced the cache accesses the TLB to retrieve the corresponding per page priority information for each cache line in a given set.

3. The operating system can prevent incorrect priority endowment by using the already existing TLB shootdown mechanism. As is understood by those of skill in the art a TLB shootdown flushes a group of TLB lookup translations.

Embodiments of the invention may include various steps which have been described above. The steps may be embodied in machine executable instructions which may be used to cause a general purpose or special purpose processor to perform the steps. Alternatively these steps may be performed by specific hardware components that contain hardwired logic for performing the steps or by any combination of programmed computer components and custom hardware components.

As described herein instructions may refer to specific configurations of hardware such as application specific integrated circuits ASICs configured to perform certain operations or having a predetermined functionality or software instructions stored in memory embodied in a non transitory computer readable medium. Thus the techniques shown in the figures can be implemented using code and data stored and executed on one or more electronic devices e.g. an end station a network element etc. . Such electronic devices store and communicate internally and or with other electronic devices over a network code and data using computer machine readable media such as non transitory computer machine readable storage media e.g. magnetic disks optical disks random access memory read only memory flash memory devices phase change memory and transitory computer machine readable communication media e.g. electrical optical acoustical or other form of propagated signals such as carrier waves infrared signals digital signals etc. . In addition such electronic devices typically include a set of one or more processors coupled to one or more other components such as one or more storage devices non transitory machine readable storage media user input output devices e.g. a keyboard a touchscreen and or a display and network connections. The coupling of the set of processors and other components is typically through one or more busses and bridges also termed as bus controllers . The storage device and signals carrying the network traffic respectively represent one or more machine readable storage media and machine readable communication media. Thus the storage device of a given electronic device typically stores code and or data for execution on the set of one or more processors of that electronic device. Of course one or more parts of an embodiment of the invention may be implemented using different combinations of software firmware and or hardware. Throughout this detailed description for the purposes of explanation numerous specific details were set forth in order to provide a thorough understanding of the present invention. It will be apparent however to one skilled in the art that the invention may be practiced without some of these specific details. In certain instances well known structures and functions were not described in elaborate detail in order to avoid obscuring the subject matter of the present invention. Accordingly the scope and spirit of the invention should be judged in terms of the claims which follow.

