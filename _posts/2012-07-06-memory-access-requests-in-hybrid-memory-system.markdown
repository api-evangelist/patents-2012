---

title: Memory access requests in hybrid memory system
abstract: Incoming memory access requests are routed in a set of incoming queues, the incoming memory access requests comprise a range of host logical block addresses (LBAs) that correspond to a memory space of a primary memory. The host LBA range is mapped to clusters of secondary memory LBAs, the secondary memory LBAs corresponding to a memory space of a secondary memory. Each incoming memory access request queued in the set of incoming queues is transformed into one or more outgoing memory access requests that include a range of secondary memory LBAs or one or more clusters of secondary memory LBAs. The outgoing memory access requests are routed in a set of outgoing queues. The secondary memory is accessed using the outgoing memory access requests.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09477591&OS=09477591&RS=09477591
owner: SEAGATE TECHNOLOGY LLC
number: 09477591
owner_city: Cupertino
owner_country: US
publication_date: 20120706
---
According to some embodiments a memory device includes a hybrid controller configured to manage data transfers between a host processor and a secondary memory. The secondary memory is configured to serve as a cache for a primary memory that includes a memory space corresponding to host logical block addresses LBAs . The hybrid controller is arranged to receive incoming memory access requests from the host processor the memory access requests including a range of host LBAs route the incoming memory access requests to a set of incoming queues map the range of host LBAs into clusters of secondary memory LBAs transform each incoming memory access requests into one or more outgoing memory access requests each outgoing memory access request including a range or cluster of secondary memory LBAs route the outgoing memory access requests from the incoming queues into a set of outgoing queues and access the secondary memory using the outgoing memory access requests.

Some embodiments involve a method of operating a hybrid memory system that includes a primary memory and a secondary memory. Incoming memory access requests are routed in a set of incoming queues the incoming memory access requests comprise a range of host logical block addresses LBAs that correspond to a memory space of the primary memory. The host LBA range is mapped to clusters of secondary memory LBAs the secondary memory LBAs corresponding to a memory space of the secondary memory. Each incoming memory access request queued in the set of incoming queues is transformed into one or more outgoing memory access requests. The outgoing memory access requests include a range of secondary memory LBAs or one or more clusters of secondary memory LBAs. The outgoing memory access requests are routed in a set of outgoing queues. The secondary memory is accessed using the outgoing memory access requests.

Some embodiments involve a controller for a hybrid memory system the controller comprising a hybrid controller configured to manage data transfers between a host processor and a flash memory the flash memory configured to serve as a cache for a magnetic disk. The hybrid controller includes multiple layers including a flash control and transfer management FCTM layer the FCTM layer is configured to 

These and other features and aspects of the various embodiments disclosed herein can be understood in view of the following detailed discussion and the accompanying drawings.

Some memory devices use at least two types of memory in a hybrid or tiered memory system where at least one type of memory is used as a primary memory and at least one other type of memory is used as a secondary memory that operates as a cache. The primary memory may have greater storage capacity but slower access times than the secondary memory for example. In this arrangement the secondary memory can serve as a read cache and or a write cache for the primary memory. One example of such a tiered memory device is a hybrid drive in which the primary memory may comprise nonvolatile memory such as magnetic disk magnetic tape and or optical disk and the secondary memory may comprise solid state flash memory and or the secondary memory may be a nonvolatile or volatile memory with or without battery backup. Note that the terms primary memory and secondary memory are used herein to denote differences in memory e.g. usage capacity performance memory class or type etc. and not necessarily order or preference. Furthermore although examples provided herein refer to the primary memory as magnetic disk and to secondary memory as flash memory the disclosed approaches are applicable to any types of primary and secondary memory.

The host sends memory access requests to the hybrid drive to read or write data. The memory access requests may specify a host LBA range used for the operation of the memory access request. For example a memory access request from the host may request that a host LBA range be written to the hybrid drive and or a memory access request may request that a host LBA range be read from the hybrid drive . The memory access requests are managed by the hybrid controller to cause data to be written to and or read from the hybrid drive with optimal efficiency. The second cache in this example may optionally be read only in that only data marked for read operations by the host are placed in the second cache . In such a configuration data marked for writing are sent directly to the main storage either directly or via the first cache .

According to some embodiments the hybrid memory device also denoted hybrid drive may be implemented using a controller configured as a hierarchy of abstraction layers. Pairs of the abstraction layers are communicatively coupled through application programming interfaces APIs . The organization of the hybrid controller into abstraction layers to some extent allows each layer to work relatively independently and or can reduce potential conflicts that arise from processing multiple threads of execution. For purposes of discussion some examples provided below are based on the use of a magnetic disk as the main memory dynamic random access memory as the first or primary cache and solid state flash memory as the second or secondary cache. It will be apparent to those skilled in the art that the various memory components are not restricted to these types of memory and may be implemented using a wide variety of memory types.

In some configurations the cache may be configured as a secondary cache being faster and smaller than the main storage . The cache is a primary cache being faster and smaller than the secondary cache . Generally the terms primary and secondary or first and second refer generally to hierarchy of time and or priority relative to commands received via the host interface . For example current read write requests from the host may be processed first via the primary cache e.g. identified by the data s logical block address . This enables host commands to complete quickly should the requested data be stored in the primary cache . If there is a miss in the primary cache the requested data may be searched for in the secondary cache . If not found in either requested data may be processed via the main storage .

Some of the data stored in the primary cache may either be copied or moved to the secondary cache as new requests come in. The copying movement from primary cache to secondary cache may also occur in response to other events e.g. a background scan. Both copying and moving involve placing a copy of data associated with an LBA range in the secondary cache and moving may further involve freeing up some the LBA range in the primary cache for other uses e.g. storing newly cached data.

The host processor communicates with the hybrid memory device also referred to herein as hybrid drive through a host interface . As previously discussed the main memory includes a memory space that corresponds to a number of memory sectors each sector addressable using a unique a logical block address LBA . The sectors of the main memory are directly accessible by the host using the LBAs and thus the corresponding LBAs of the main memory are referred to herein as host LBAs.

The host sends memory access requests to the hybrid drive for example the host may request that data be written to and or read from the hybrid memory device. The host interface is configured to transfer memory access requests from the host to the hybrid memory device and to transfer data between the host and the hybrid memory device.

The hybrid controller illustrated in includes number of layers wherein each layer communicates to its nearest neighboring layer s e.g. through a set of requests. For example each layer may only communicate to its nearest neighboring layer s without communicating to other layers. As an example the layer may only communicate directly to layer and layer without communicating directly with the layer or to the host interface . As an operation such as a memory access request from the host is being carried out each layer is configured to pass control to the next lower layer as the operation is implemented.

The example illustrated in includes four layers which are described in terms applicable to the use of flash memory as a cache. It will be appreciated that these terms are not restrictive and if other types of memory were used as the secondary memory if desired different terminology could be used to reflect the type of secondary memory. Nevertheless the basic functions of the layers can be similar regardless of the type of memory used for primary and or secondary memory and or the terminology used to describe the layers.

The layers illustrated in include the flash cache interface FCI layer the flash cache control and transfer management FCTM layer the solid state drive SSD layer and the programmable state machine PSM layer . Requests may be passed as indicated by arrows from a higher layer to the next lower layer starting with the FCI layer and proceeding to the PSM layer which interacts directly with the flash memory . The layered architecture of the hybrid controller described herein allows for handling host memory access requests which can be serviced from either the magnetic memory or one of the caches . The layered structure used in conjunction with the flash cache can be configured to achieve specified rates and response times for servicing memory access requests.

The FCI layer decides whether a host read request should be serviced from the primary memory or from one of the caches . The FCI layer implements processes to determine which data should be promoted to the flash secondary cache and or the primary cache based on various criteria to achieve optimal workload for the hybrid memory device. The flash content and transfer management FCTM layer maintains a mapping e.g. a fully associative mapping as discussed below of the host LBAs to a memory space corresponding to the flash memory space arranged as LBAs which are referred to as solid state drive SSD LBAs. The SSD layer interacts with programmable state machine PSM layer and performs tasks such as optimal scheduling of promotion requests among dies of the flash referred to as die scheduling wear leveling garbage collection and so forth. The SSD layer maps the SSD LBAs of the FCTM layer to physical flash locations die block and page locations . The PSM layer programs hardware controllers to generate the required signals to read from and write to the flash for example.

In some cases one or more of the layers of the hybrid controller may be implemented by circuitry and or by one or more processors e.g. such as reduced instruction set computer RISC processors available from ARM. In some cases each layer may be implemented by a separate processor. The processes discussed herein are implementable in hardware interconnected electronic components that carry out logic operations and or by a processor implementing software instructions and or by any combination of hardware and software.

Embodiments described herein involve processes implemented by the FCTM layer to manage memory access requests received from the FCI layer and sent to the SSD layer. The memory access requests may involve reading the flash memory writing to the flash memory and so forth. In various embodiments management and implementation of the memory access requests is accomplished in the FCTM layer using a set of incoming queues and a set of outgoing queues. is a flow diagram that illustrates a process of managing memory access requests in the FCTM layer. Memory access requests are received by the FCTM layer from the FCI layer and SSD memory access requests are sent to the SSD layer by the FCTM layer. Memory access requests sometimes referred to herein as incoming memory access requests or as incoming requests because these requests are incoming from the perspective of the FCTM layer are received by the FCTM layer from the FCI layer. The incoming requests are routed into a set of incoming queues. The memory access requests queued in the incoming queues are transformed into SSD requests sometimes referred to herein as outgoing memory access requests or outgoing requests because these requests are outgoing from the perspective of the FCTM layer . The outgoing requests are routed to a set of outgoing queues. The outgoing requests in the outgoing queues are sent to the SSD layer which carries out the SSD requests to perform the operations specified in the SSD requests.

The overall structure of the incoming queues and the outgoing queues is illustrated in . The incoming queues include an incoming free queue of nodes which are used to control the flow of memory access requests into the receive queue . Generally the FCTM scheduler routes incoming memory access requests from the FCI layer into the receive queue only if a node is available in the incoming free queue . In other words the number of nodes in the incoming free queue represent the capacity of the FCTM layer at any particular time to process incoming memory access requests.

As illustrated in if a node is available in the incoming free queue that node becomes occupied by an incoming memory access request when the memory access request is routed into the receive queue . When a node is occupied by a memory access request information about the memory access request is stored in the node. For example the node may store information about the type of memory access request the host LBAs involved in the memory access request information about the progress of the memory access request such as how much data has been transferred in conjunction with the memory access request how much work to complete the memory access request is pending and so forth. If a node is not available in the incoming free queue then the FCTM layer does not have the capacity to process the incoming memory access request and an error message is generated.

The FCTM layer can process a number of types of memory access requests received from the FCI layer. illustrate the process for three types of memory access requests read requests promotion requests and invalidate requests. As explained in more detail herein read requests are requests from the FCI layer to read host LBAs from the flash promotion requests are requests to promote write host LBAs into the flash and invalidate requests are requests to mark certain host LBAs in the flash as invalid not containing valid data . In the illustrated example of the FCTM layer includes a separate ready queue for each type of request. However in some implementations at least some of the queues may be shared queues between different types of requests. Memory access requests are moved to the appropriate ready queue when they do not have any overlaps with other requests in the execute or ready queues and they are waiting to be routed to the execute queue to begin execution. A memory access request remains in the ready queue until there is at least one SSD resource available to execute the request. In in illustrated example SSD nodes that are available in the outgoing free queue represent SSD resources thus if there is at least one available SSD node the memory access request can be moved to the execute queue. The incoming memory access requests are transformed into a number of outgoing SSD requests which transferred to the SSD execute queues as outgoing nodes in the outgoing free become available.

Each read request promotion request and invalidate request has associated with it a particular address range host LBA range . The FCTM layer may transform one incoming memory access request e.g. read request promotion request invalidate request from the FCI layer into one or multiple SSD requests which the FCTM layer issues to the SSD layer. Requests issued by the FCI layer and received by the FCTM layer are referred to herein as memory access requests or incoming FCI requests or incoming memory access requests. Requests issued by the FCTM layer to the SSD layer are referred to herein as SSD requests outgoing SSD requests or as outgoing memory access requests. The FCTM layer transforms each incoming memory access request that has an associated host LBA range into one or multiple outgoing memory access requests that each have an associated SSD LBA range or cluster of SSD LBAs. For example the FCTM layer implements an incoming a read request by generating one or more SSD requests that include an SSD LBA range. The FCTM layer implements an incoming promotion request by generating one or more outgoing SSD requests that include a cluster of SSD LBAs the FCTM layer implements an incoming invalidate request by generating one or more outgoing SSD requests that include a cluster of SSD LBAs.

As previously discussed the FCTM layer includes an overlap checker which operates in conjunction with an overlap queue . As best illustrated in requests in the receive queue may be routed into the overlap queue when an overlap in memory access requests is identified by the overlap checker . An overlap may occur if there is an overlap in the address range e.g. host LBA range of two memory access requests. For example an overlap can occur if a read request and a promotion request have overlapping address ranges. In this example assuming the promotion request is received in the FCTM layer before the read request the promotion request may be completed before the read request is moved to the read ready queue to avoid overlap conflicts. During the time that the promotion request is in the promotion ready queue or the execute queue the read request remains in the overlap queue . Appropriate management of overlapped memory access requests avoids erroneous data being read from or written to the flash memory . After the overlap has been cleared the memory access request is routed into the appropriate ready queue 

If the FCTM layer has at least one resource an SSD node available the memory access request is transferred from a ready queue to the execute queue . Execution of a memory access request in the execute queue involves transforming the memory access request in the execute queue into a number of SSD requests that provide instructions to the SSD layer to carry out the memory access request. A memory access request from the FCI layer includes a host LBA range and this host LBA range is transformed to an SSD LBA range by the FCTM layer. In some implementations the FCTM internally keeps track of the host LBA range in terms of clusters groups of n host LBAs referred to as host LBA clusters and keeps track of the SSD LBA range in terms clusters of n SSD LBAs. Transformation of contiguous clusters of host LBAs may or may not be transformed into contiguous clusters of SSD LBAs. After the SSD request is transferred to the SSD layer the SSD layer may convert the SSD LBA range included in the SSD request to flash address die block and page . For execution of a memory access request to begin the outgoing free queue must have available at least one available node otherwise an error message is generated. Different types of memory access requests may be transformed into different numbers of SSD requests which is associated with the amount of work required by the type memory access request. For example an invalidate request may occupy a first number of nodes e.g. only one node whereas a read or promotion request may occupy a larger number of nodes.

In some cases when a particular memory access request in the execute queue is transformed into a number of SSD requests one e.g. only one of the SSD execute queues will include all the SSD requests associated with the incoming memory access request in the execute queue . Each of the SSD requests represents outstanding work to the SSD layer. As nodes become available in the outgoing free queue to execute a memory access request in the execute queue those available outgoing nodes become occupied by the SSD requests associated with the memory access request which is being executed. The SSD requests associated with the memory access request being executed are transferred to an SSD execute queue . The memory access request being executed may remain in the execute queue occupying a node from the incoming free queue until execution of the memory access request by the FCTM layer is complete. Execution of a memory access request in the FCTM layer may be deemed to be complete when the responsibility for processing the memory access request is transferred from the FCTM layer to the SSD layer . This occurs after all the SSD requests associated with a memory access request are issued to the SSD layer. For example responsibility may be transferred when the last SSD request associated with a memory access request is successfully transferred to the SSD layer or when the last SSD request associated with the memory access request has been successfully completed by the SSD layer and acknowledgement of the successful completion of the SSD request has been received by the FCTM layer.

When execution of a memory access request is complete the node from the incoming free queue that was previously occupied by the incoming memory access request in the execute queue is returned to the incoming free queue . The previously occupied node becomes available again for being occupied by subsequent memory access requests. Each of the nodes in the SSD execute queue associated with the memory access request being executed are returned to the outgoing free queue as the SSD requests occupying these nodes are completed. The previously occupied SSD nodes become available again to be occupied by subsequent SSD requests. In some cases an error occurs when one or more SSD requests are transferred to the SSD layer. When an error occurs in the processing of SSD requests associated with a memory access request the node used to process the incoming memory access request may be returned to the incoming free queue and the SSD nodes used to process the outgoing SSD requests may be returned to the outgoing free queue. In other words the processing of the incoming memory access request is cancelled and not completed when an error occurs.

In some implementations incoming memory access requests from the FCI layer to the FCTM layer is restricted meaning that during a time that the FCTM layer is processing a memory access request then the FCI layer is barred from issuing another memory access request to the FCTM layer. Implementations that restrict additional incoming memory access requests from the FCI layer protects the FCTM layer from excessive combinations of possible events affecting the FCTM layer and enhances the thread safety of the layer. In some implementations the code e.g. all of the code that manages the queues is executed on a single thread and none of the data structures of the FCTM layer e.g. the queues can be used are manipulated by external entities e.g. other layers of the hybrid controller.

A priority scheme may be used for transferring the incoming and or outgoing memory access requests between queues. In some cases the priority scheme may be multi tiered wherein a first level of priority is implemented by the FCTM scheduler to select incoming memory access requests from the ready queue and a second level of priority is implemented by the FCTM scheduler when assigning SSD nodes from the outgoing free queue.

A priority scheme e.g. first level priority may be used to select requests from the ready queues for transfer to the execute queue. According to one priority scheme requests that require the least resources and or are faster to execute may be selected for execution before requests that require more resources and or are slower to execute. For example invalidate requests present in the invalidate ready queue may be selected for execution before read or promotion requests in the read ready queue or promotion ready queue respectively because invalidate requests are the faster to execute. In general invalidate requests execute faster than either read or promotion requests and read requests execute faster than promotion requests thus the priority scheme may follow this order. For example the invalidate requests may not require input output I O transferred via the PSM layer to the flash and may be executed by updating metadata in the FCTM layer and performing an unmap in the SSD layer which also only involves the updating of metadata in the SSD layer. Requests that do not require I O to the flash typically take the least amount of time to execute. Despite not requiring I O to the flash the FCTM scheduler may use an SSD node to keep track of and regulate the flow of invalidate requests.

The flow diagram of conceptually illustrates an overview of one priority scheme that may be implemented by the FCTM scheduler for routing memory access requests from the ready queues to the execute queue. As previously discussed for each incoming memory access request the FCTM scheduler routes the memory access request to the receive queue if there is a node available in the incoming free queue . If there are no nodes available in the incoming free queue the FCTM scheduler generates an error response which is sent to the FCI layer.

The FCTM overlap checker determines if the address range host LBA range of the memory access request that is routed to the receive queue overlaps with the address range of other memory access requests. If an overlap is not detected the memory access request is routed to the appropriate ready queue. If an overlap is detected the memory access request is routed to the overlap queue. Data access requests routed to the overlap queue wait there until the overlap is resolved. If the overlap is resolved the previously overlapped memory access request is routed to the appropriate ready queue.

Data access requests wait in the ready queue until at least one SSD node in the outgoing free queue is available for execution of the memory access request. Once an SSD node is available the priority scheme for routing memory access requests to the execute queue is implemented. If there is an invalidate request in the ready queue the invalidate request is routed to the execute queue and the process returns to the implementation of the priority scheme at step . According to the priority scheme if multiple invalidate requests are present in the invalidate ready queue these invalidate requests would be processed until the invalidate ready queue is empty. If the invalidate ready queue is empty and there is a read request in the read ready queue the read request is routed to the execute queue and the process returns to the implementation of the priority scheme at step . If there are no invalidate requests or read requests in their respective ready queues and there is a promotion request in the promotion ready queue the promotion request is routed to the execute queue and the process returns to the implementation of the priority scheme at step .

In some scenarios a priority scheme may be pre emptive involving pre empting requests in the execute queue with requests in the ready queue. In some implementations such a pre emption takes place if the request in the ready queue would take less time resources for execution than the request in the execute queue. In one example invalidate requests in the ready queue preempt promotion requests in the execute queue. Execution of the invalidate requests may cause a delay in the completion of the execution of the promotion request however this delay may be minimal because the invalidate requests can be executed very quickly if there is no I O to the flash.

One possible implementation of a pre emptive priority scheme is conceptually illustrated by the flow diagram of . Such a priority scheme may be implemented alone or as a second level of priority in conjunction with another priority scheme e.g. the priority scheme discussed in connection with . After the work associated with an SSD request occupying an SSD node is completed the SSD node is returned to the outgoing free queue and becomes available again. A priority scheme is implemented that determines the memory access request to which this available SSD node is next assigned. According to the priority scheme of if there is an invalidate request in the execute queue the SSD node is assigned to the invalidate request. If there is a read request in the execute queue the SSD node is assigned to the read request. If there is an invalidate request in the ready queue the invalidate request is moved to the execute queue and the SSD node is assigned to the invalidate request. If there is a read request in the ready queue the read request is moved to the execute queue and the SSD node is assigned to the read request. If there is a promotion request in the execute queue the SSD node is assigned to the promotion request. If there is a promotion request in the ready queue the promotion request is moved to the execute queue and the SSD node is assigned to the promotion request. The priority scheme illustrated in provides for an optimal ordering in the execution of requests to achieve minimal host request latency.

Note that the priority scheme illustrated in may mean that a request in the execute queue may be pre empted by a request for which execution has not yet started. The preempting request may be in the ready queue and if so the pre empting request would be moved to the execute queue and the available SSD node would be assigned to it. Thus the pre empting request may delay the execution of a request in the execute queue that is currently being executed.

In some scenarios the flash memory may be full when a promotion request is executed. If so the FCTM may cause some data stored in flash to be evicted. To implement evictions as illustrated in the FCTM layer maintains a most valuable least valuable MVLV list of clusters which ranks the value of the clusters according to some criteria which may be based on one or a number of factors such as which of the clusters was most least recently used and or which of the clusters is most frequently least frequently used for example. One end of the MVLV list is referred to herein as the head which is the position of the currently most valuable cluster and the opposite end of the MVLV is referred to as the tail which is the position of the currently least valuable cluster. If the flash memory is full and a promotion request is executed the cluster at the tail of the MVLV list is selected for eviction. In some implementations when a cluster is read or written that cluster becomes the most valuable cluster because it was most recently used and is moved to the head of the MVLV list .

The FCTM layer maintains list e.g. linked list of free SSD LBA clusters denoted the free list and or maintains a list e.g. linked list of in use SSD clusters denoted the use list . The free list includes SSD clusters that are available for use. The use list includes SSD clusters that contain valid data and are not available to accept new data. In some cases one or more SSD clusters may be in a detached state during which the SSD clusters are not in either the use state or the free state. An SSD cluster may be in a detached state for example during the time that the clusters are involved in execution of a request e.g. during the time that data is written to the clusters.

The flow diagrams of conceptually illustrate some steps involved in the execution of invalidate read and promotion requests respectively performed by the FCTM layer. As previously discussed each incoming memory access request includes a command portion and a host LBA range. The command portion identifies the type of request and the host LBA range indicates the host LBAs involved in the request. In addition a promotion request is associated with the data to be written to the LBA range specified in the promotion request.

An invalidate request issued by the FCI layer identifies a cluster aligned range of host LBAs to be invalidated. A cluster aligned LBA range means that the start of the LBA range and the end of the LBA range are not arbitrary but are multiples of n which is the number of sectors per cluster. The invalidate request is transferred to the execute queue. The FCTM layer maps the cluster aligned LBA range of the incoming memory access request to the SSD LBA clusters and determines the SSD LBA clusters involved in the invalidate request. The SSD clusters are invalidated marked as containing invalid data in the FCTM metadata. An SSD request sent by the FCTM layer to the SSD layer comprises an unmap request for the corresponding flash memory clusters. The invalidated SSD LBA clusters can be moved to the free cluster list maintained by the FCTM layer in its metadata. Note that implementation of an invalidate request does not require any work performed by the flash

A read request involves reading data corresponding to an arbitrary range of LBAs from the flash memory. The host LBA range of a read request from the FCI is not necessarily cluster aligned. There may be an upper bound on the number of LBAs that can be included in the read request. In the example illustrated in the FCTM layer initially performs a check to determine if the range of host LBAs specified by the read request is fully present in the flash. If the range of host LBAs is not fully present the read request is rejected and an error response to the FCI layer is generated . The error response notifies the FCI layer to obtain the data requested from the primary memory e.g. the magnetic disk. If the range of LBAs specified by the read request is fully present in the flash memory then the read request is moved to the execute queue. The FCTM maps the host LBA range to the SSD LBAs. A list of SSD LBAs in the read request is created and the FCTM layer issues one more SSD requests to the SSD layer that specify the SSD LBAs to be read. The list of SSD LBA clusters that include the SSD LBAs of the read request may be made most valuable e.g. moved to the head of the MVLV list.

A promotion request involves writing a cluster aligned range of host LBAs to the flash memory. There may be an upper bound imposed on the number of LBAs that can be included in one promotion request. The promotion request is moved to the execute queue . A list of the SSD LBA clusters corresponding to the cluster aligned host LBA range specified in the promotion request that are already present in the flash is created . The clusters already present in the flash are denoted overlapped clusters. A bitmap is created to skip over the SSD LBA clusters that are already present in the flash. The process of determining the clusters already present in the flash and creating the bitmap mask facilitates conservative use of the flash memory space by maintaining a single copy of any host LBA in the flash. The overlapped SSD LBA clusters and the non overlapped SSD LBA clusters are made most valuable by moving these clusters to the head of the MVLV list. The FCTM determines if there are a sufficient number of clusters available to store the clusters to be written into the flash. The clusters to be written to the flash are the clusters implicated by the promotion request that are not already present in the flash. If there are a sufficient number of clusters available then clusters for storing the data are allocated and the SSD LBA clusters to be stored are transferred via the SSD layer to the flash. The metadata of the FCTM layer i.e. the use list is updated to indicate that these clusters are in use. If a sufficient number of clusters is not available the SSD LBA space is saturated then the FCTM layer will perform evictions to free up a sufficient number of clusters.

Eviction overlap may lead to data errors. Eviction overlap can occur when the address range being evicted overlaps with the address range of an outstanding command that is in the ready queue or the execute queue. The FCTM scheduler described in various embodiments discussed herein is arranged to operate so that eviction overlap is avoided.

If the flash memory is not saturated i.e. there is a sufficient free space in the flash for promotion without evictions being performed non overlapping requests from the FCI layer can execute in any order. For an unsaturated flash only overlapped requests are placed in the overlap queue. If the flash is saturated evictions must take place in order to make room for promotion requests to be implemented.

As illustrated in certain steps are carried out during the ready queue to execute transition for read promotion and invalidate requests. These steps may be implemented as atomic operations that are completed without interruption. Performing these steps atomically without interruption ensures that no other requests remove these SSD LBA clusters from the SSD LBA space before the request has executed. For example if an invalidate request is received while a read request is executing the invalidate request will move to the overlap queue so that the invalidate request does not interfere with the execution of the read request. If the invalidate request were serviced during execution of the read request there is a possibility that the invalidate request would invalidate some LBAs involved in the read request.

For read requests during the ready queue to execute queue transition the FCTM scheduler verifies if the LBA range in specified in the request is fully present in the flash. If the range is not fully present the read request is not executed and an error response is generated. The SSD clusters that correspond to the host LBA range of the read request whether or not fully present are made most valuable by moving these clusters to the head of the MVLV list. If the SSD clusters that correspond to the host LBA range of the read request are fully present in the flash the FCTM scheduler creates a list the SSD clusters and implements the read request as previously discussed in connection with .

For promotion requests during the ready queue to execute queue transition the FCTM scheduler checks to determine which SSD LBA clusters are already present in the flash and creates a bitmap of the overlapped SSD LBA clusters already present in the flash. The bitmap is used to skip writing the overlapped clusters to the flash. If the flash is saturated the required number of clusters may be evicted to make room for the new clusters to be written as part of the promotion request.

For invalidate requests implemented during the ready queue to execute queue transition the FCTM scheduler migrates the SSD LBA clusters being invalidated into the free list of SSD clusters. The FCTM scheduler issues an unmap SSD request for the invalidated clusters.

Note that evictions can occur in response to promotion requests. To analyze the potential for eviction overlap the following scenarios are considered as illustrated in .

1. P e R a read request R precedes a promotion request with eviction P e . Eviction overlap in this situation is not possible because during the read requests transition from the ready queue to the execute queue the SSD clusters involved in the read request were moved to the head of the MVLV list. The SSD clusters of the read request will not be selected for eviction during the promotion operation with eviction because the clusters evicted prior to the promotion will be selected from the tail of the MVLV list.

2. R P e a read request follows a promotion request with eviction . The host LBA range of the read request may no longer be fully present in the flash when the read request is executed . If this occurs the read request will be handled accordingly e.g. by sending an error message to the FCI layer.

3. P e I an invalidate request I precedes a promotion request with eviction . Eviction overlap in this situation is not possible because invalidate requests are completed synchronously and never remain in the execute queue. The same call chain that places an invalidate request in the execute queue also moves the node occupied by the invalidate request from the execute queue back to the free queue.

4. I P e an invalidate request follows a promotion request with eviction . When the invalidate request reaches the execute queue the cluster range may not be fully present in the flash. In this scenario only clusters present in the flash will be invalidated.

5. P e P a promotion request e.g. without eviction P precedes a promotion request with eviction . Eviction overlap in this is not possible because when the preceding promotion request P is being executed the sectors being written to are detached temporarily removed from the use list and the free list and thus will not be evicted. The sectors written by the preceding promotion request P are moved to the head of the MVLV list after they are written.

6. P P e a promotion request e.g. without eviction follows a promotion request with eviction . When the following promotion request P reaches the execute queue there is a possibility that clusters specified in its SSD LBA cluster range may no longer be present in the flash if these clusters were evicted by the preceding P e request. If the clusters are no longer present this may result in these clusters being written to the flash.

The mapping of the host LBA clusters to the SSD clusters by the FCTM layer is fully associative meaning that any host LBA cluster can be mapped to any of the SSD LBA clusters so long as there is room in the cache. diagrammatically depicts mapping of the host LBA space to the SSD LBA space . In the FCTM layer the host LBA space is partitioned into clusters of host LBAs and the SSD LBA space is partitioned into clusters of SSD LBAs. In the host LBA space each cluster of host LBAs is uniquely identified by a number between 0 and N 1 and each cluster includes n contiguous sectors. In the SSD LBA space each SSD cluster is uniquely identified by a number between 0 and K 1 K is typically less than N and each cluster includes n sectors. The number of sectors per cluster n may be fixed and can depend on the size of a host sector the geometry of the flash memory the error correction code ECC used to store data in the flash memory and or other factors. In the example illustrated in n 32 however in other implementations n may be greater than or less than 32. Furthermore in general n need not be a power of two.

The mapping from host LBA space to SSD LBA space is accomplished by a hash function . As previously discussed the hash function can support fully associative caching with regard to clusters. In other words the hash function allows any host cluster to be mapped to any SSD cluster as indicated by arrows . However the mapping may be constrained such that any host LBA can exist in only one SSD cluster at any given time. The offset within a cluster where an LBA is located within a cluster is fixed and is can be determined by the host LBA modulo the number of host LBAs per cluster i.e. the remainder resulting from dividing the host LBA by n. Allowing a host LBA cluster to be mapped into any SSD cluster and ensuring that promotes and invalidates implemented by the FCTM layer are aligned to cluster boundaries avoids cache fragmentation.

The hash function is used to convert the tag upper L bits of the host LBA into a hash table index in the hash table . The entry in the hash table indicated by the hash table index the tag converted by the hash function points to one or more clusters in the SSD LBA space. For example for a host LBA of L M bits the lower M bits can be used as a sector offset to identify the sector within an SSD cluster. The remaining L bits are used for the tag. The hash function operates on the tag to generate the index into the hash table . For example the hash function may discard the upper L H bits of the tag and use the lower H bits as the hash table index. Discarding a portion of the tag means that in some cases a number of different host LBAs will map to the same entry in the hash table and a collision will occur. An entry in the hash table is associated with more than one cluster identification ID only if a collision occurs. In this scenario 2host LBAs mapped to a cluster will all have the same tag. If the hash function discards the upper bits leaving only H lower bits for the hash table index the theoretical maximum number of possible collisions i.e. the number of clusters that map into the same SSD LBA space is 2. The L H bits of the tag identify the cluster ID. The collisions are resolved using a linked list . The linked list contains the cluster IDs that are hashed to the same entry in the hash table i.e. have the same hash index . To access a particular cluster the linked list is scanned for an entry with the correct cluster ID. For example when the FCI layer requests a look up involving a particular host LBA cluster the FCTM layer applies the hash function and if there is a collision two clusters that map to the same space then the FCTM layer traverses through the linked list to locate the requested cluster.

The above description assumes that the number of host sectors per cluster is a power of two. However non power of two sector sizes may also be used. A representative set of host sector sizes that are supportable by the fully associative cache structure described herein include but is not limited to the following sector sizes 512 520 524 528 4096 4192 and 4224 bytes. For example based on sector to cluster mapping calculations there may be 30 5XX byte sectors per cluster assuming a cluster is 16 KB of the flash such as an 8 KB flash page size with dual plane support .

Non powers of two can be handled by modifying the mapping described above as follows The tag is determined as tag host LBA sectors per cluster where indicates an integer division via truncation and the host sector offset within the cluster is determined by host LBA modulo the sectors per cluster i.e. the remainder after dividing the host LBA by the sectors per cluster.

The division and modulo operations can be implemented by executing a multiply instruction e.g. a 64 bit multiply instruction on the FCTM processor assuming the FCTM processor supports 64 bit multiple instructions. To facilitate the multiply the value p 0xFFFFFFFF sectors per cluster is pre computed is a constant value. The tag is now determined by tag host LBA p 32 where indicates a 64 bit multiply operation and where 32 means that the result of host LBA p is right shifted 32 times. Using this process there is a possibility that the tag is off by one. To correct for this occurrence the tag is incremented by one if the following condition is satisfied Host LBA tag sectors per cluster sector per cluster. The remainder can be similarly determined.

It is to be understood that this detailed description is illustrative only and various additions and or modifications may be made to these embodiments especially in matters of structure and arrangements of parts and or processes. Accordingly the scope of the present disclosure should not be limited by the particular embodiments described above but should be defined by the claims set forth below and equivalents thereof.

