---

title: Apparatus, system, and method for annotation of media files with sensor data
abstract: Embodiments of methods for multimedia annotation with sensor data (referred to herein as Sensor-rich video) includes acquisition, management, storage, indexing, transmission, search, and display of video, images, or sound, that has been recorded in conjunction with additional sensor information (such as, but not limited to, global positioning system information (latitude, longitude, altitude), compass directions, WiFi fingerprints, ambient lighting conditions, etc.). The collection of sensor information is acquired on a continuous basis during recording. For example, the GPS information may be continuously acquired from a corresponding sensor at every second during the recording of a video. Therefore, the acquisition apparatus generates a continuous stream of video frames and a continuous stream of sensor meta-data values. The two streams are correlated in that every video frame is associated with a set of sensor values. Note that the sampling frequency (i.e., the frequency at which sensor values can be measured) is dependent on the type of sensor. For example, a GPS sensor may be sampled at 1-second intervals while a compass sensor may be sampled at 50 millisecond intervals. Video is also sampled at a specific rate, such as 25 or 30 frames per second. Sensor data are associated with each frame. If sensor data has not changed from the previous frame (due to a low sampling rate) then the previously measured data values are used. The resulting combination of a video and a sensor stream is called a sensor-rich video.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09509968&OS=09509968&RS=09509968
owner: National University of Singapore
number: 09509968
owner_city: Singapore
owner_country: SG
publication_date: 20120221
---
This application is a national phase application under 35 U.S.C. 371 of International Application No. PCT SG2012 000051 filed 21 Feb. 2012 which claims priority to U.S. Provisional Patent Application No. 61 445 036 entitled APPARATUS SYSTEM AND METHOD FOR ANNOTATION OF MEDIA FILES WITH SENSOR DATA filed on Feb. 21 2011. The entire contents of each of the above referenced disclosures is specifically incorporated herein by reference without disclaimer.

The present invention relates generally to capture of multimedia in mobile devices and more particularly to systems and methods for multimedia annotation with sensor data.

User generated video content is experiencing significant growth which is expected to continue and further accelerate. As an example users are currently uploading twenty hours of video per minute to YouTube. Making such video archives effectively searchable is one of the most critical challenges of multimedia management. Current search techniques that utilize signal level content extraction from video struggle to scale.

Camera sensors have become a ubiquitous feature in the environment and more and more video clips are being collected and stored for many purposes such as surveillance monitoring reporting or entertainment. Because of the affordability of video cameras the general public is now generating and sharing their own videos which are attracting significant interest from users and have resulted in an extensive user generated online video market catered to by such sites as YouTube. AccuStream iMedia Research has released a report forecasting that the user generated video market size is expected to expand 50 from 22 million in 2007 to 34 million in 2008. The report was based on data from popular video content providers including AOLUncut Broadcaster.com Crackle.com Ebaumsworld LiveDigital Metacafe MySpace TV Revver.com Vsocial.com VEOH.com Yahoo Video and YouTube. By 2010 more than half 55 of all the video content consumed online in the US is expected to be user generated representing 44 billion video streams. Companies are developing various business models in this emerging market with one of the more obvious ones being advertising. In 2008 Forrest Research and eMarketer reported that the global online video advertising market will reach more than 7.2 billion by 2012.

Many of the end user cameras are mobile such as the ones embedded in smartphones. The collected video clips contain a tremendous amount of visual and contextual information that makes them unlike any other media type. However currently it is still very challenging to index and search video data at the high semantic level preferred by humans. Effective video search is becoming a critical problem in the user generated video market. The scope of this issue is illustrated by the fact that video searches on YouTube accounted for 25 of all Google search queries in the U.S. in November of 2007. Better video search has the potential to significantly improve the quality and usability of many services and applications that rely on large repositories of video clips.

A significant body of research exists going back as early as the 1970s on techniques that extract features based on the visual signals of a video. While progress has been very significant in this area of content based video retrieval achieving high accuracy with these approaches is often limited to specific domains e.g. sports news and applying them to large scale video repositories creates significant scalability problems. As an alternative text annotations of video can be used for search but high level concepts must often be added manually and hence its use is cumbersome for large video collections. Furthermore text tags can be ambiguous and subjective.

Recent technological trends have opened another avenue to associate more contextual information with videos the automatic collection of sensor metadata. A variety of sensors are now cost effectively available and their data can be recorded together with a video stream. For example current smartphones embed GPS compass and accelerometer sensors into a small portable and energy efficient package. The meta data generated by such sensors represents a rich source of information that can be mined for relevant search results. A significant benefit is that sensor meta data can be added automatically and represents objective information e.g. the position .

Some types of video data are naturally tied to geographical locations. For example video data from traffic monitoring may not have much meaning without its associated location information. Thus in such applications one needs a specific location to retrieve the traffic video at that point or in that region. Unfortunately current devices and methods are not adequate.

Embodiments of methods for multimedia annotation with sensor data referred to herein as Sensor rich video includes acquisition management storage indexing transmission search and display of video images or sound that has been recorded in conjunction with additional sensor information such as but not limited to global positioning system information latitude longitude altitude compass directions WiFi fingerprints ambient lighting conditions device orientation etc. . The collection of sensor information is acquired on a continuous basis during recording. For example the GPS information may be continuously acquired from a corresponding sensor at every second during the recording of a video. Therefore the acquisition apparatus generates a continuous stream of video frames and a continuous stream of sensor meta data values. The two streams are correlated in that every video frame is associated with a set of sensor values. Note that the sampling frequency i.e. the frequency at which sensor values can be measured is dependent on the type of sensor. For example a GPS sensor may be sampled at 1 second intervals while a compass sensor may be sampled at 50 millisecond intervals. Video is also sampled at a specific rate such as 25 or 30 frames per second. Sensor data are associated with each frame. If sensor data has not changed from the previous frame due to a low sampling rate then the previously measured data values are used. The resulting combination of a video and a sensor stream is called a sensor rich video.

Embodiments of a system for multimedia annotation with sensor data includes one or more cameras microphones configured to acquire and manage multimedia that are being tagged with sensor properties captured by one or more sensors of the camera or microphone environment during recording for example but not limited to geographic information . The collected and correlated sensor properties subsequently also called meta data are then utilized for storing indexing searching handling and presenting large collections of videos in novel and useful ways. By considering video related meta data and cross correlating all the information videos can be better managed searched and presented. The current Sensor rich video system implementation demonstrates a system that consists of several acquisition applications for mobile devices and a sample video search portal. One of the novel Sensor rich video processing aspects is the use of an estimation model of a camera s viewable scene. For Sensor rich video video acquisition the system uses automated annotation software that captures videos their respective field of views FOV and other sensor information. Prototype Sensor rich video recording applications have been implemented for both Android and iOS devices. It could also be made available for suitably equipped laptops tablets and other computers. As one possible use case the acquisition software apps allow community driven data contributions to the Sensor rich video portal. Various other application scenarios can also be envisioned.

In sensor rich videos the video and the sensor stream are correlated i.e. every video frame is associated with a set of sensor values and vice versa. The sensor stream can also be considered a sensor description of the video. Importantly note that the sensor stream is generally much smaller in its size than the corresponding video. For example while a video stream may be 10 megabytes in size the associated sensor stream may be only 10 kilobytes in size i.e. 1000 times smaller . This difference in size of the sensor stream allows for novel and efficient ways to describe a video in a compact manner.

The rich sensor information is used to describe the geographic coverage of the video content i.e. video viewable scenes . One na ve traditional approach is the abstraction of the spatial video coverage using a circular sector around the camera location point. This approach overestimates the scene coverage by disregarding the viewing direction. In Sensor rich video a novel viewable scene model is introduced which is derived from the fusion of location and direction sensor information with a video stream. Using this model videos are stored as spatial objects in the database and searched based on their geographic properties. This model strikes a balance between the complexity of its analytical description and the efficiency with which it can be used for fast searches. Note that the sensor data provides an abstraction of the video content hence the videos can be managed based on this abstraction.

In one embodiment sensor data streams can be managed and or handled separately from the bulky binary video streams. The small in size abstraction provided by the sensor data is often sufficient in many video applications. Overall the properties of sensor rich video result in a number of interesting technical solutions such as real time transmission and search.

The generally small size of the sensor information allows for its efficient storage and transmission. For example when sensor rich video is captured on a mobile device for example an Android or iOS based smartphone then the sensor stream may be transmitted to a server first and the bulky video data later. The small size of the sensor data allows for robust and fast transmission to say a portal on which the information can be further processed and made available to other users. Wireless networks such as 3G in general provide limited upload bandwidth and hence submitting the small sensor data is beneficial as it results in fast transmissions with a smaller chance of connection disruptions. In general the sensor information can be detached from the video and handled separately while the correlation between the video and the sensor streams is maintained via a unique and shared identifier. The streams can later be re combined if so desired.

In order to allow the sharing of sensor information with very short delay a mobile device can continuously transmit the sensor stream to a portal server while at the same time recording and storing video in its local memory e.g. on flash storage . The continuously uploaded sensor stream can be processed and shared with other users with very short delay i.e. in real time such that other users can understand which geographical regions are currently being video recorded. As an application scenario consider a brush fire battled by firefighters that are equipped with mobile devices that they use to momentarily record sensor rich video about the fire situation in their area. By uploading the sensor information continuously during the recording the mission supervisor has excellent situational awareness of where his men are operating and of which areas video has been captured. The supervisor could then remotely request for video uploads from specific crew members if he she would like to see the captured video. The portal system would then contact the corresponding mobile device and initiate a video upload. Note that this kind of on demand video transmission is very energy and bandwidth efficient for the mobile devices since only the requested video bulk data is transmitted. This would be significantly better than uploading video from all devices during all the recording time. In fact this may not be technically feasible because of bandwidth limitations or it may be cost prohibitive. Note also that on demand video upload can be very targeted as a remote user e.g. the supervisor can request a specific segment of a video delimited in various ways such as by time location etc. and there is no need to upload all the video. A user can also request small video snippets first to assess the relevance of its content and then slowly direct his attention to larger video segments from the area of greatest interest.

In addition to uploading sensor information about a recorded video in real time a mobile device could also upload key frames of the video at specific intervals. Key frames are extracted from the video and represent snapshots that allow a user to gain a quick understanding of the visual content of a video. Since key frames would be extracted and transmitted at intervals say every 10 seconds or triggered by certain events their transmission would still be more energy efficient than the transmission of a continuous video stream. The benefit of key frames is that they would provide visual information in addition to the sensor stream data e.g. location and direction of the camera . Key frames could be presented to the user on a map along the trajectory of the camera path.

Sensor data as it is scalar and alphanumeric in nature can be efficiently indexed on the portal or within a large video archive. The benefits of such indexing is that it allows for efficient search of identifying and selecting videos with certain properties for example videos or video segments that show a certain geographical area.

Large archives of sensor rich videos may be browsed and presented in various ways. One natural representation is to draw the camera path also called a trajectory on a map. In addition to the camera location the camera viewing direction or the viewable scene representing the observable area of the camera lens i.e. a volumetric cone can also be drawn. Multiple camera trajectories representing different captured videos can be shown on a map simultaneously. A user can select an area for example by visually drawing a rectangle to select the videos he she would like to see. The system then finds and presents the representative video segments either on the map or in a separate window. Video segments are the portions of a video that satisfy the user search criteria. For example when a geographical range search is performed only the video segments that overlap with the search area are returned as the results. Browsing and presentation can also be performed on a mobile device. Depending on the devices capability and its screen resolution various ways to allow for user input i.e. specifying the search criteria and presentation of the resulting videos are envisioned. For example the current location of the mobile device could be used to search for geographically related videos.

In various embodiments a search may result in a large number of results being returned. In such a case similar to textual searches on a search engine such as Google the results may be ranked according to relevance. Video relevance can be computed in various ways. One possibility is to compute the spatio temporal overlap between the video trajectory and viewable scenes with the search region specified by the user. The more considerable and longer the overlap is the higher the videos relevance ranking. Other overlap properties such as the proximity of the search region to the camera s location and search region s appearance within the video frames i.e. how close is the query region to the center of the frame can also be used in relevance calculation. The closer and the more at the center the search region is the higher the videos relevance ranking will be.

Embodiments of the method and system leverage sensor collected meta data camera locations and viewing directions which are automatically acquired as continuous streams together with the video frames. Existing smartphones can easily handle such integrated recording tasks. By considering a collective set of videos and leveraging the acquired auxiliary meta data the approach is able to detect interesting regions and objects and their distances from the camera positions in a fully automated way. Such embodiments exhibit computational efficiency and therefore are very scalable.

The term coupled is defined as connected although not necessarily directly and not necessarily mechanically.

The term substantially and its variations are defined as being largely but not necessarily wholly what is specified as understood by one of ordinary skill in the art and in one non limiting embodiment substantially refers to ranges within 10 preferably within 5 more preferably within 1 and most preferably within 0.5 of what is specified.

The terms comprise and any form of comprise such as comprises and comprising have and any form of have such as has and having include and any form of include such as includes and including and contain and any form of contain such as contains and containing are open ended linking verbs. As a result a method or device that comprises has includes or contains one or more steps or elements possesses those one or more steps or elements but is not limited to possessing only those one or more elements. Likewise a step of a method or an element of a device that comprises has includes or contains one or more features possesses those one or more features but is not limited to possessing only those one or more features. Furthermore a device or structure that is configured in a certain way is configured in at least that way but may also be configured in ways that are not listed.

Other features and associated advantages will become apparent with reference to the following detailed description of specific embodiments in connection with the accompanying drawings.

Various features and advantageous details are explained more fully with reference to the nonlimiting embodiments that are illustrated in the accompanying drawings and detailed in the following description. Descriptions of well known starting materials processing techniques components and equipment are omitted so as not to unnecessarily obscure the invention in detail. It should be understood however that the detailed description and the specific examples while indicating embodiments of the invention are given by way of illustration only and not by way of limitation. Various substitutions modifications additions and or rearrangements within the spirit and or scope of the underlying inventive concept will become apparent to those skilled in the art from this disclosure.

The cameras embedded in phones have improved rapidly in recent years. Therefore users can capture high quality videos with devices that they carry with them all the time. The Sensor rich video system leverages this phenomenon and introduces the concept of sensor rich video capture which can be further used in various ways for example for real time video sharing. An embodiment of a sensor rich video system consisting of mobile acquisition app components and a server portal component is shown in .

Current generation smartphones and other mobile devices contain many sensors such as a GPS receiver a compass an accelerometer etc. all embedded into a small portable and energy efficient are automatically annotated with sensor rich information. Uploading to a server is performed at the push of a button. Video and sensor information is automatically correlated and the information may be interleaved or transmitted in various ways see attached research manuscript .

In a sensor rich video system videos are uploaded and are made available to be searched. The system manages both the uploaded videos and the related sensor meta data information. One of the functions is to maintain the correspondence between the data. The system presents a user with a map as an interaction interface. The user may then place an indicator that indicates the search area on the map to the desired location. The user can search for and play matching videos where the videos show the region in the target area. The system may also allow a user to index and or smooth data compute distances between two points or the like.

In one embodiment search and display are also available on mobile phones. In one embodiment the system takes advantage of the sensors in the phones to specify some parts of the query. For example when the user holds the phone in one direction this will trigger a search to the server by sending the GPS location and orientation information. The portal will send back relevant videos to users by finding all the videos within the relevant proximity of the user. This is very useful as users could use their smartphone to quickly explore a location. They could also check its history and find out about the changes by viewing videos that were taking with the same direction and at the same location some time ago.

Embodiments of the system of generate sensor rich multimedia including video recordings and management. For example video may be automatically annotated with relevant meta data information. An important aspect is that the meta data is automatically collected relieving the user from the task of entering annotation keywords to describe the video. Furthermore the collected sensor data is non ambiguous i.e. objective unlike keyword annotation performed by users where ambiguous and non relevant descriptions may be entered. The sensor enriched multimedia may be suitable to be processed efficiently and effectively in a number of ways. For example videos can be searched managed and presented based on their geographic properties.

One benefit of the described embodiments is that sensor annotation can be done very conveniently and cost effectively on current generation mobile devices e.g. smartphones . These devices contain many or all of the sensors and are now available at relatively low cost and in high volume numbers. Therefore users can capture high quality videos with devices that they carry with them all the time.

Sensor rich video includes advanced features based on augmented sensor information which makes the sharing experience more social enjoyable and technically scalable beyond simple video uploading and viewing.

The mobility of cameras in the framework is of particular interest because of the ubiquity of mobile devices and the prominent importance of geographic properties of moving cameras. More and more user generated videos are produced from mobile devices such as cellular phones. To address the issues of sensor rich video search the framework consists of the three main parts 1 the data collection at mobile devices 2 the search engine to store index search and retrieve both the meta data and video contents and 3 the user interface to provide web based video search services. These main parts may be communicating through the Internet and or cellular network.

Data Collecting Device A mobile device can be any camera equipped with sensors and a communication unit. A good example is Apple s iPhone 3GS that includes GPS digital compass accelerometer 5 mega pixel camera WiFi Broadband data connection and programming capability.

At the mobile device level the main objective is to capture the sensor inputs and to fuse them with the video for future storage and retrieval of videos. Sampling individual sensor data especially widely available location data may be straightforward due to the wide availability of sensors and their interface software. However various types of sensors provide different characteristics such as sampling rate and precision of the sampled value. For example videos are recorded at the rate of 30 frames per second while the GPS location can be sampled just once per second at most and a digital compass can sample more than 40 times per second. Moreover there can be missing sample values or measurement errors due to various reasons such as bad weather which may impact the accuracy of search.

The following issues need to be considered at the device level. First the sampled sensor data should be synchronized and tagged in accordance with the recorded video frames. This automatic synchronized annotation forms the basis of the proposed framework. To accommodate the different sampling rates of devices sample values might be manipulated using numerical methods such as interpolating averaging etc. The sampling frequency can be periodic or aperiodic using any adaptive methods. An adaptive method can be more efficient and desirable in a large scale application since it can minimize the amount of the captured data and support a more scalable system. Second it is desirable to represent the collected meta data using a standard format regardless of the number type and precision of sensors for general acceptance in diverse applications. This is critical when the video search is extended to user generated videos on public web sites. Third the meta data are either 1 embedded into the video contents or 2 handled separately from the video contents. The embedding granularity can be at each frame segment scene or clip level of the video. The former requires a standard embedding method based on a specific video technology. The problem is that there are so many different video coding techniques. The latter works independently from the video coding techniques however it presents a verification problem between a video file and its meta data.

The captured video and sensor data are in general immediately transmitted to the server in their entirety. This approach may work well for small sized applications. However when a large scale video collection from thousands of mobile devices is considered which have expensive communication cost or limited communication capability this immediate video and meta data transmission scheme might not be cost efficient. One important observation is that not all collected videos are important or requested immediately. In many applications the providers of data and their consumers are working independently. In other words providers are collecting data at their own schedules and consumers are querying data at their convenience which results in a time gap between the data collection and the data retrieval. Another observation is the expensive wireless communication cost from mobile devices such as cellular phones. When a huge amount of video is transmitted immediately after being collected but not requested for a while in an application it may not be the most cost efficient way to use expensive cellular communication.

There are ways to overcome the drawbacks of the immediate data transmission scheme. For example video data can be delivered on an on demand basis. Assuming that not all captured videos from mobile devices will be immediately requested by users i.e. if they are not selected as query results such immediate data transmission through costly wireless networks may not be necessary. When the meta data and the captured video data are separated the system may first transmit the meta data to the server while transmitting video data in a delayed manner. Then searching on the captured video can be performed immediately without actual data transmission. Depending on the search results the actual video data can be transmitted on an on demand basis. Any requested videos will be transmitted as soon as possible however the captured but not immediately requested videos may be transmitted to the server through a high speed wired network i.e. cost effective during less busy periods of a day i.e. better distribution of server load . This separate management of the geographic properties of videos i.e. small amount of textual data and actual video i.e. large amount of binary data can provide a higher flexibility for the design of service architectures service models and policies. Real time meta data transmission and on demand based video transmission enable a real time search at the server while utilizing a more cost efficient transmission of large video data over expensive cellular network. Note that this separation of the meta data from video contents incurs extra processing for the synchronization and verification of the meta data. However since it provides more advantages than disadvantages it may be assumed that the meta data is separately managed from the video content.

Any mobile device includes the following modules in the framework Sensor data collection module following a predetermined rate and precision sensor inputs are captured through various sensors including video GPS compass accelerometer etc. Sensor signal processing module some input values may need to be checked and refined. For example video content can be checked for clarity or occlusion. Accelerometer input can be filtered for a clearer signal. Sensor measurement errors can be detected and missing sample values can be estimated here. Format module for a broader and transparent use of the collected data a standard way to annotate the collected sensor inputs is implemented here which also includes the synchronization among video contents and sensor data. The format should cover the following information identification of device and video real time of recorded data types of sensors sample values precision of the sampled data synchronization among video and other sensor inputs etc. Communication module defines and controls the data transmission between the mobile device and the server based on a predefined protocol. This module needs to provide versatile ways to accommodate diverse applications and service models.

Search Engine Since videos are searched only by the abstraction of their geographic properties not by their visual information meta data is managed by a database server while videos are managed by a media server. Any conventional database and media server can be used. The collected meta data from the mobile device are delivered to the database server while the video contents are stored in the media server. In shows a tightly coupled database and media server on the same platform. However sensor rich video search applications can be implemented at various scales. As the amount of video grows a single media server might not be enough to provide the storage and streaming of videos while the amount of meta data can be managed by a single database server. Then the search engine can consist of a single database and multiple media servers even heterogenous media servers to efficiently support various types of videos . For the most general and large scale applications multiple database servers and media servers are distributed across a wide area and collaborate with each other. For a focused discussion this paper assumes the simplest configuration i.e. a single database and media server on the same machine.

Database Server To utilize the captured geographic properties of videos for searching the framework represents the coverage area of video scenes as spatial objects in a database i.e. it models the coverage area using the meta data. Then the problem of video search is converted to the problem of spatial object selections in a database. Modeling the video scenes depends on the availability of sensor data. For example in the presence of only GPS input the camera location data the potential coverage area of video frames can be represented as a circle centered at the camera location CircleScene in . With extra camera direction data from a digital compass the coverage area can be more accurately refined like a pie slice also called the field of view FOV shown in more precisely in . Thus videos represented by the pie model can be searched more effectively. Modeling may facilitate indexing and searching of video contents in a database because the query functionality and performance are greatly impacted by the spatial modeling of videos.

Using the defined model videos are stored and searched as spatial objects in a database. Video searching should be able to fully take advantage of the collected meta data for various requests of applications. Beyond the importance of the geographic information where a video is taken there are other obvious advantages in exploiting the spatial properties of video because the operation of a camera is fundamentally related to geometry. When a user wants to find images of an object captured from a certain viewpoint and from a certain distance these semantics can be interpreted as geometric relations between the camera and the object such as the Euclidean distance between them and the directional vector from the camera to the object. Thus more meaningful and recognizable results can be achieved by using spatial queries on sensor rich videos.

Search types exploiting the geographic properties of the video contents may include not only conventional point and range queries i.e. overlap between the covered area of video and the query range but also new types of video specific queries. For example one might want to retrieve only frames where a certain small object at a specific location appears within a video scene but with a given minimum size for better visual perception. Usually when the camera is closer to the query object the object appears larger in the frame. Thus a search with a range restriction for the distance of the camera location from the query point can be devised which is termed as a distance query. Similarly the camera view direction can be an important factor for the image perception of an observer. Consider the case where a video search application would like to exploit the collected camera directions for querying say a directional query. An example search is For a given query point in geo space find all video frames taken with the camera pointing in the Northwest direction and overlapping with the query point. The view direction can be defined as a line of sight from the camera to the query point i.e. an object or place pictured in the frame . For the implementation of the video domain specific query types new indexing techniques or database query functionalities need to be introduced. Moreover the evaluation of new query types should be fast enough to be practical especially for a large scale video search. There has been little research on these issues.

After a search results in multiple videos in a large application another challenge is that human visual verification of the video results may take a significant amount of time. To enhance the effectiveness of the result presentation an approach is to quantify the relevance of resulting videos with respect to the given query and to present the results based on their relevance ranking. The difficulty lies in the fact that the human appreciation process of relevance is very subjective and so it is challenging to be quantified. The framework harnesses objective measurements to quantify the relevance between a query and videos in two different ways 1 spatial relevance overlapping coverage area between query range and a video 2 temporal relevance overlapping covered time.

The database server can consist of the following modules Modeling module received sensor data will be processed for a specific application by applying a model for sensor rich video search. It can be as simple as a geospatial coverage of camera field of view FOV as shown in or a more sophisticated spatio temporal multidimensional model. The adopted model affects the query functionalities and performance of the system. Storage Module based on the model the meta data are stored using an appropriate data structure and indexes. Query processing module this module receives user queries and performs the actual searching. Not only the conventional spatio temporal query types such as point and range queries but also domain specific query types such as directional queries are implemented and serviced. Ranking module the results from the query processing module i.e. a group of video segments found can be ranked and ordered by the relevance to the given query. Communication module defines and controls the data transfer between a mobile device and the server based on a predefined protocol. It needs to provide a versatile way to accommodate diverse applications and service models.

Media Server The role of media server is to store actual video contents and to provide streaming service to users. In general the media server obtains a list of video segments as query results and transmits them to the user in a predefined way. Different user interfaces can present the search results in different ways so the media server corresponds to the requests of the user interface. One critical functionality of the media server in the framework is the ability to randomly access any portion of stored videos in a fast way. Since the amount of data in an entire video clip can be very large and the user might be interested in watching only a portion of the video where the query overlaps random access is very important for humans to verify the results.

Transcoding module a video might need to be transcoded when it arrives at the server. For example a user can collect videos in any format but the application might require certain predefined formats for the service. Similarly when users request the display of the query results videos can be transcoded to accommodate the different supported video formats between the server and user devices.

Storage module videos are stored based on the underlying storage system and the media server technology.

Retrieval scheduler module the query results from the database server are analyzed by this module to provide the most efficient way to retrieve the requested data. The media server is requested to access random video segments to provide a fast presentation of the searched video segments.

Database communication module depending on the service model and application the database server and the media server can be coupled in two ways 1 tightly coupled two servers are customized for a specific application so that more sophisticated service is possible through a close communication between the two. For example two servers can be implemented on the same machine so any content based ranking can be applied before presenting the query results for a more precise ranking. 2 loosely coupled independent and general media servers are collaborating with the database server through the standard network protocols. It is usually not expected to closely interact between two servers but can be more appropriate for distributed server environment peer to peer video sharing or web video crawling. The media server is straightforwardly streaming video contents to users under the database server s direction.

User Interface Users should be able to send video search queries to the search engine from both wired computers and mobile devices. Then depending on the type of devices the user interface can be designed in different ways. The framework assumes a web based user interface to communicate with the search engine. Depending on the computing power of the user s machine and the availability of other supporting software e.g Google Maps media player web browser the features of the sensor rich video search applications can be significantly affected.

Users can search videos in multiple ways. For example one intuitive method is making a map based query when users are familiar with the interested area. Drawing a query directly on any kind of map see the implementation examples in might provide the most human friendly and effective interface in the paradigm. Alternatively text based query can be also effective when users are searching for a known place or object. For example the user interface can maintain a local database of the mapping between places and their geo coordinates. Then the textual query can be converted into spatial query with exact longitude and latitude input. The search engine can maintain a large collection of such mappings for well known places.

The user interface receives the ranked query results from the search engine. As well as a good ranking method the presentation style or format of the results also greatly affects the effectiveness of the presentation. Thus human friendly presentation methods should be considered such as using key frames thumbnail images any textual descriptions etc. A map based user interface for both query input and video output can also be an effective tool by coordinating the vector map and actual video display. Note that relevance ranking and presentation style are not just technical issues but may require an intensive user study.

The framework includes the following modules at the user side User interface module For regular computers a normal web based interface with maps can be expected. Users can exploit all possible tools for an effective and efficient querying communication presentation of results. The video contents can be delivered and presented without changing the format i.e. a high quality video if communication bandwidth is enough. For the mobile devices the user interface design should be adjusted for the limited size of the display computing power and ability to handle video formats. Media player The media player software plays out the query results. One important aspect of video presentation is the capability to display only relevant segments of videos at the user side avoiding unnecessary transmission of video data. Thus the media player at the user side and the streaming media server are expected to closely collaborate.

Here the solutions of the modules in implementing the framework is introduced. Note that the present disclose is not necessarily providing optimal solutions for all modules discussed but instead intend to provide an example solution for each critical module to demonstrate the feasibility and adequacy of the proposed sensor rich video search framework.

Collecting Sensor Data and Modeling Viewable Scene The implementation focuses on the geographic properties of the video contents i.e. the exact area on the ground covered by the video scenes. To capture the geographic properties the video recording system is prototyped with three devices a video camera a 3D digital compass and a Global Positioning System GPS . It is assumed that the optical properties of the camera are known. The digital compass mounted on the camera heads straight forward as the camera lens. The compass periodically reports the direction in which the camera is pointing with the current heading angle with respect to North and the current pitch and roll values. The GPS also mounted on the camera reports the current latitude longitude and altitude of the camera. Video can be captured with various camera models. The custom written recording software receives direction and location updates from the GPS and compass devices as soon as new values are available and records the updates along with the current computer time and coordinated universal UTC time.

A camera positioned at a given point p in geo space captures a scene whose covered area is referred to as camera field of view FOV also called a viewable scene see . The meta data related to the geographic properties of a camera and its captured scenes are as follows 1 the camera position p is the latitude longitude coordinates read from GPS 2 the camera direction a is obtained based on the orientation angle 0 

Based on the collected meta data the viewable area is modeled in 2D space which is represented as a circular sector as shown in . For a 3D representation shown in the altitude of the camera location point and the pitch and roll values are needed to describe the camera heading on the zx and zy planes i.e. whether the camera is directed upwards or downwards . It is believed that the extension to 3D is straightforward especially since the altitude level is already acquired from the GPS and the pitch and roll values from the compass. Thus the FOV is represented in 2D space.

One important point in data collection is the difference in the data sampling frequencies. The GPS location updates are available every one second whereas compass can produce 40 direction updates per second. And for a 30 fps video stream there will be 30 frame timecodes for every 1 second video. An intuitive way is to create the combined dataset as the sensor data is received from the devices and use a common timestamp for the combined tuple. Such a tuple will include the last received updates for the location and direction values. Because of the heterogeneity in data frequencies it is possible to match data items which are not temporally closest. A better way is to create separate datasets for GPS updates compass readings and frame timecodes and later combine the data items from each data set that has the closest time match. Since all sensor values will be refreshed at most every second intuitively the data frequency for the combined dataset will be a second.

A periodic data collection was used in the implementation i.e. an update per one second. Thus in the implementation a n second long video is represented with n FOVs each representing the geographic properties of one second long video frames. Table 1 shows the collected geo tagged meta data for a 5 sec long video where each row i.e. tuple corresponds to an FOV. In Table 1 Timestamp is the computer time when the meta data is recorded and Timecode is the corresponding frame timecode in the actual video.

The user query is either a geographical region or some textual or visual input that can be interpreted as a region in geospace. The Query Interpreter in translates the user query into a spatio temporal query. As an example the query Find the videos of the University of Idaho Kibbie Dome is translated into the coordinates of the corner points of the rectangular region that approximates the location of the dome. The database is then searched for this spatial region and the video segments that capture the Kibbie Dome are retrieved. If the query specifies a temporal interval only the videos that were recorded during the specified time window are returned. The FOVScene coverage of a moving camera over time is analogous to a moving region in the geo spatial domain therefore traditional spatio temporal query types such as range queries k nearest neighbor kNN queries or spatial joins can be applied to the IF data. One of the tasks is to extract the video segments that capture a given region of interest during a given time interval. As explained the F t P right arrow over d R description can be constructed for every time instance. Hence for a given query Q the sequence of video frames whose viewable scenes overlap with Q can be extracted. Going from most specific to most general the region of query Q can be a point a line e.g. a road a poly line e.g. a trajectory between two points a circular area e.g. neighborhood of a point of interest a rectangular area e.g. the space delimited with roads or a polygon area e.g. the space delimited by certain buildings roads and other structures . Details of range query processing can be found in the prior work. The proposed viewable scene model which is based on the camera location P and view direction right arrow over d provides a rich information base for answering more complex geospatial queries. For example if the query asks for the views of an area from a particular angle more meaningful scene results can be returned to the user. Alternatively the query result set can be presented to the user as distinct groups of resulting video sections such that videos in each group will capture the query region from a different view point. Some further aspects of a complete system for querying georeferenced videos such as indexing and query optimization will be explored as part of the future work.

The query processing mechanism does not differentiate between highly relevant and irrelevant data and presents all results to a user in random order. In many applications this will not be acceptable. The present disclosure ranks the search results based on their relevance to the user query. The Video Ranking module in rates search results according to the spatio temporal overlap properties i.e. how much and how long the resulting video segments overlap with the query region.

Although the objective of the study is to rank results based on the queries spatio temporal attributes for some applications the video ranking accuracy can further be improved by leveraging the features extracted from the visual video content. In the Concept Detection module provides information about the semantic content of the video segments to aid the ranking process. A detailed discussion of content based video search and ranking techniques is out of the scope of the paper. A review of state of the art solutions can be found in the literature. It is provided here some example queries for which the ranking accuracy can be improved by leveraging visual features in addition to spatio temporal overlap properties. Note that content based feature extraction is currently not implemented in the prototype system.

In video search when results are returned to a user it is critical to present the most related videos first since manual verification viewing videos can be very time consuming. This can be accomplished by creating an order which ranks the videos from the most relevant to the least relevant. Otherwise although a video clip completely captures the query region it may be listed last within the query results. The relevance of each video may be questioned with respect to the user query and an ordering based on estimated relevance may be provided. Two pertinent dimensions to calculate video relevance with respect to a range query are its spatial and temporal overlap.

Analyzing how the FOVScene descriptions of a video overlap with a query region gives clues on calculating its relevance with respect to the given query. A natural and intuitive metric to measure spatial relevance is the extent of region overlap. The greater the overlap between and the query region the higher the video relevance. It is also useful to differentiate between the videos which overlap with the query region for time intervals of different length. A video which captures the query region for a longer period will probably include more details about the region of interest and therefore can be more interesting to the user. Note that during the overlap period the amount of spatial overlap at successive time instances changes dynamically for each video. Among two videos whose total overlap amounts are comparable one may cover a small portion of the query region for a long time and the rest of the overlap area only for a short time whereas another video may cover a large portion of the query region for a longer time period. illustrate the overlap between the query Qand the videos Vand V respectively. Although the actual overlapped area of the query is similar for both videos the coverage of Vis much denser. Consequently among the two videos V s relevance is higher.

In the following how the overlap between the video FOVScenes and query regions is explained and three basic metrics for ranking video search results are disclosed. A summary of the symbolic notations used in the discussion is provided in Table 2.

Preliminaries Let Q be a polygon shaped query region given by an ordered list of its polygon corners lon lat 1 where lon lat is the longitude and latitude coordinate of the jcorner point of Q and m is the number of corners in Q. Suppose that a video clip Vconsists of n F regions. tand tare the start time and end time for video V respectively. The sampling time of the ith is denoted as t. The starting time of a video tis defined as t. The i represents the video segment between tand tand the n which is the last FOVScene represents the segment between tand t for convenience say t t . The set of FOVScene descriptions for Vis given by V F t P right arrow over d R 1 i n. Similarly the at time tis denoted as V t .

If Q is viewable by V then the set of that capture Q is given by SceneOverlap for all 1 where overlaps with The overlap between Vand Q at time t forms a polygon shaped region as shown in . Let O V t Q denote the overlapping region between video Vand query Q at time t. It is defined as an ordered list of corner points that form the overlap polygon. Therefore 

Three Metrics to Describe the Relevance of a Video Three fundamental metrics may be used to describe the relevance R of a video Vwith respect to a user query Q as follows 

Total Overlap Area R The total overlap area of O V Q is given by the smallest convex polygon which covers all overlap regions formed between Vand Q. This boundary polygon can be obtained by constructing the convex envelope enclosing all corner points of the overlap regions. Equation 5 formulates the computation of the total overlap coverage. The function ConvexHull provides a tight and fast approximation for the total overlap coverage. It approximates the boundary polygon by constructing the convex hull of the polygon corner points where each point is represented as a longitude latitude pair. shows examples of the total overlap coverage between the query Qand videos Vand V. The total overlap area is calculated as follows.

Overlap Duration R . The Relevance using Overlap Duration R is given by the total time in seconds that Voverlaps with query Q. Equation 7 formulates the computation of R.

Total Overlap Area and Overlap Duration capture the spatial and temporal extent of the overlap respectively. However both relevance metrics express only the properties of the overall overlap and do not describe how individual FOVScenes overlap with the query region. For example in for videos Vand V although R V Q R V Q and R V Q R V Q Voverlaps with around 80 of the query region Qduring the whole overlap interval whereas Voverlaps with only 25 of Qfor most of its overlap interval and overlaps with 80 of Qonly for the last few FOVScenes. In order to differentiate between such videos the Relevance using Summed Overlap Area RSA may be used as the summation of areas of all overlap regions during the overlap interval. The following equation formalizes the computation of Rfor video Vand query Q.

Algorithm 1 outlines the computation of the proposed relevance metrics R R and Rfor a given video Vand query Q. Note that the relevance score for Vis computed only when Voverlaps with Q. In Algorithm 1 a tri level filtering step lines and may be applied to effectively eliminate the irrelevant videos and video segments. First the algorithm checks whether query Q overlaps with the MBR enclosing all V. If so the algorithm looks for the regions whose MBR overlap with Q. And finally the algorithm further refines the overlapping regions by checking the overlap between query Q and actual V. Such a filtering process improves computational efficiency by gradually eliminating the majority of the irrelevant video sections. Algorithm 1 calls the subroutine MBR which computes the minimum bounding rectangle for a given region. Functions RectIntersect M Q and SceneIntersect Q V t return true if the given query Q overlaps with the rectangle M or the FOVScene V t respectively.

These proposed metrics describe the most basic relevance criteria that a typical user will be interested in. Rdefines the relevance based on the area of the covered region in query Q whereas Rdefines relevance based on the length of the video section that captures Q. Rincludes both area and duration of the overlap in the relevance calculation i.e. the larger the overlap is the bigger the Rscore will be. Similarly the longer the overlap duration the more overlap polygons will be included in the summation. Since each metric bases its relevance definition on a different criteria it may not be expected to obtain a unique ranking for all three metrics. Furthermore without feedback from users it is difficult to ascertain whether one of them is superior to the others. However a certain metric provides the best ranking when the query is specific in describing the properties of videos that the user is looking for. As an example in video surveillance systems the videos that give the maximum coverage extent within the query region will be more relevant. Then metric Rwill provide the most accurate ranking. In real estate applications users often would like to see as much detail as possible about the property and therefore both extent and time of coverage are important. In such applications metric Rwill provide a good ranking. And in traffic monitoring systems where the cameras are mostly stationary the duration of the video that captures an accident event will be more significant in calculating relevance. Therefore metric Rwill produce the best ranking.

Based on the query specification either a single metric or a combination of the three can be used to obtain the video ranking. Calculating the weighted sum of several relevance metrics Equation 9 is a common technique to obtain an ensemble ranking scheme. Relevance 6 

To obtain the optimal values for weights w wand wa training dataset which provides an optimized ranking based on several metrics is needed. However constructing a reliable training dataset for georeferenced videos is not trivial and requires careful and tedious manual work. There is extensive research on content based classification and ranking of videos using Support Vector Machines SVM and other classifiers which train their classifiers using publicly available evaluation data for example the TRECVID benchmark dataset . There is a need for a similar effort to create public training data for georeferenced videos.

The visual content of the videos can be leveraged into the ranking process to improve the ranking accuracy. For example for the Kibbie Dome query the video segments in the search results might be analyzed to check whether the view of the camera is occluded with some objects such as trees cars etc. Some state of the art concept detectors can be adopted to identify such objects within the video content. The video frames where the camera view is occluded can be weighted less in calculating the spatial and temporal overlap for the metrics R Rand R. In addition to content based features text labels extracted from video filenames surrounding text and social tags can be useful in video ranking.

The ranking methods calculate the overlap regions for every overlapping FOVScene to obtain the video relevance scores. Since the overlap region computation is computationally expensive these techniques are often not practical for large scale applications. Thus several histogram based ranking techniques that provide comparable ranking results are also introduced but at the same time dramatically improve the query response time. Using a predefined grid structure a histogram pre computes and stores the amount of overlap between a video s FOVScenes and the grid cells. During query execution only the histogram data is accessed and queried.

Histogram based ranking techniques not only enable faster query computation but also provide additional information about how densely the video overlaps with the query. For example although the exact shape of the overlap region is calculated for each individual the computed relevance scores do not give the distribution of the overlap throughout the query region i.e. which parts of the query region are more frequently captured in the video and which parts are captured only in a few frames. The distribution of the density of overlap can be meaningful in gauging a video s relevance with respect to a query and in answering user customized queries therefore should be stored.

The histogram based relevance scores are analogous to precise relevance scores except that precise ranking techniques calculate the overlap amounts for every user query whereas histogram based techniques use the pre computed overlap information to obtain the rankings.

The whole geospace may be partitioned into disjoint grid cells such that their union covers the entire service space. Let Grid c 1 i M and 1 j N be the set of cells for the M H N grid covering the space. Given the descriptions Vof video V the set of grid cells that intersect with a particular V t can be identified as overlaps with and Grid 7 V t is the set of overlapping grid cells with V t at time t i.e. a grid representation of an . To obtain V t the method searches for the cells that overlap with the borderline of V t and then include all other cells enclosed between the border cells. The implementation details for computing Vcan be found in the extended technical report. Then Vis a grid representation of Vwhich is a collection of V t 1 i n. The histogram for V denoted as OH consists of grid cells C V t .

For each cell cin C OverlapHist counts the number of samples that coverlaps with. In other words it calculates the appearance frequency f of cin V Equation 11 . OverlapHist Count for all 1 8 

Function Count calculates the number of V t that cell cappears in. Note that OverlapHist describes only the spatial overlap between the grid and the video FOVScenes. However in order to calculate the time based relevance scores the histogram that summarizes the overlap durations needs to be created. OverlapHistTime constructs a set of time intervals when cj overlaps with V. A set Iholds overlap intervals with cell cand Vas pairs of . Then the histogram for V i.e. OH consists of grid cells each attached with a appearance frequency value and a set of overlapping intervals.

The histogram based implementation counts the number of overlaps between the Fs and grid cells and therefore the histogram bins can only have integer values. Alternatively for the histogram cells that partially overlap with the floating point values that quantify the amount of overlap may be used. Allowing floating point histogram bins will improve the precision of the Rmetric by assigning lower relevance scores to the videos that partially overlap with the query region compared to those that fully overlap with the query. However storage and indexing of floating point numbers might introduce additional computational overhead when the size of the histogram is fairly large. Also note that the gain in precision by allowing floating point histogram bins is highly dependent on the size of the histogram cells. The tradeoff between precision and performance should be explored through careful analysis. In order to obtain reliable results the performance evaluations should be done using a large video dataset.

A given polygon shaped query region Q may be represented as a group of grid cells in geospace all grid cells that overlap with 9 The definition of overlap region may be redefined as a set of overlapping grid cells O between Vand Q. Using the histogram of V i.e. OH the overlapping grid cell set can be defined as of 10 Note that the grid cells in Oinherit corresponding frequencies and intervals from OH. Let Qbe a query region that consists of fthe grid cells Q . Then the overlapping cells with the video in Example 1 become .

Histogram Based Relevance Scores Using the grid based overlap region O three proposed metrics are redefined as follows.

Total Overlap Cells R Ris the extent of the overlap region on Q i.e. how many cells in Qare overlapping with V. Thus Ris simply the cardinality of the overlapping set O V Q . In Example 1 R 2.

Overlap Duration R The duration of overlap between a query Qand Vcan be easily calculated using the interval sets in OH OverlapHistTime. CombineIntervals 11 Function CombineIntervals combines the intervals in the histogram. Note that there may be time gaps when the intervals for some of the cells are disjoint. There are also overlapping time durations across cells. In Example 1 R 20 seconds.

Summed Number of Overlapping Cells R Ris the total time of cell overlap occurrences between Vand Qand therefore is a measure of how many cells in Qare covered by video Vand how long each overlap cell is covered. Since the histogram of a video already holds the appearance frequencies f of all overlapping cells RS becomes 

As mentioned above a histogram gives the overlap distribution within the query region with discrete numbers. Knowing the overlap distribution is helpful for interactive video search applications where a user might further refine the search criteria and narrow the search results.

Power Model An estimation model is defined to describe the power levels of a mobile device operating under different modes. The target device is the HTC G1 a smartphone that is based on the open source Google Android mobile device platform.

Modeled Hardware Components The power estimation model is adapted. A linear regression based power estimation model is proposed which uses high level measurements of each hardware component on the mobile device to estimate the total system power consumption. This model at the device level is used to understand and evaluate the efficiency and feasibility of the proposed video search technique.

Relevant details of each hardware component on the target HTC G1 mobile phone are described next. The search system incorporates an additional GPS receiver unit to obtain location meta data. Therefore the original model is adapted to include the power consumption for the GPS receiver.

CPU The processor supports dynamic frequency scaling DFS and it is rated at 528 MHz but is scaled down in the platform to run at 124 MHz 246 MHz and 384 MHz. The highest frequency of 528 MHz is not used. The lowest frequency is never used on consumer versions of the phone and is too slow to perform basic tasks. Thus only the high 384 MHz and medium 246 MHz frequencies are considered in the model. CPU power consumption is strongly correlated with the CPU utilization and frequency. CPU hi and CPU lo parameters represent the average CPU utilization while operating at 384 MHz and 246 MHz respectively.

Screen The display is described by two parameters a boolean parameter LCD indicating whether the screen is on or off and a Brightness parameter which models the effect of the screen brightness with 256 uniformly spaced levels.

WiFi The boolean parameter WiFi on describes whether the WiFi network interface is turned on or off additionally WiFi trf and WiFi bytes indicate network traffic and the number of bytes transmitted during a particular time interval.

Storage The number of sectors transferred to or from the MicroSD flash memory card per time interval are represented by the parameter SD.

System There exists also a residual power consumption parameter System. This parameter subsumes all power that is not accounted for the hardware components listed above. This is referred as the baseline System power.

Analytical Power Model The described modeling parameters are incorporated into the analytical power model that is utilized in the simulation experiments. The power model determines the relationship between the system statistics e.g. the value for screen brightness and the power consumption for each relevant hardware component. The inputs to the model are the statistics collected from the device values and the output represents the total power consumption. The overall system power consumption as a function of time t is determined as follows 

The overall power consumption is calculated by substituting the statistics collected at time t for the selected hardware components into P t .

To evaluate the accuracy of the power model the power consumption of an HTC G1 is measured with Power Tuto an application for Android based phones that displays the power consumed by major system components such as CPU network interface display and GPS receiver see . According to the authors PowerTutor was developed on the HTC G1 in collaboration with Google and its accuracy should be within 5 of actual values for the G1.

With PowerTutor the various statistics is obtained for different hardware units. Specifically logs is collected on a G1 phone for different usage scenarios. For instance video for one minute is captured or uploaded. During the tests all non essential processes were disabled. After multiple experiments the values were determined and shown in Table 3.

In the next step the measured parameters were substituted into the power model. Same usage scenarios is performed with the Android G1 phone for about two minutes and the trace logs from PowerTutor is collected. The power consumption is measured for various phone usage scenarios such as capture GPS capturing video and using GPS to obtain location information capture WiFi capturing video and using WiFi to obtain the location capture GSM capturing video and using GSM to obtain the location and transmission WiFi transmitting data via WiFi . Grouped by usage scenario the average power consumption obtained from the power model was compared to the power values reported by PowerTutor. The results are shown in .

The modeled and measured power consumptions match very well for each of the usage scenario. To calculate the accuracy of the model he following error metric e is used 

The results indicate that the power estimation model accurately predicts the system level power consumption. The error e for each scenario is less than 4.9 and the average error across all the scenarios is 1.7 .

An important point to note is that capturing video and then transmitting it through WiFi are both very energy consuming activities. With its standard 1 150 mAh capacity battery the G1 phone would last less than three hours in the worst case when continuously capturing and transmitting video. The proposal is to extend battery life through more selective transmissions.

The present embodiments may save considerable battery energy by delaying the costly transmission of the large binary video data that have not been requested especially when the transmission speed is low.

A camera positioned at a given point P in geo space captures a scene whose covered area is referred to as the camera field of view FOV also called the viewable scene . The FOV model describes a camera s viewable scene in 2D space with parameters camera location P camera orientation a viewable angle and visible distance R FOV 14 

The camera position P consists of the latitude and longitude coordinates read from a positioning device e.g. GPS and the camera direction a is obtained based on the orientation angle provided by a digital compass. R is the maximum visible distance from P at which a large object within the camera s field of view can be recognized. The angle is calculated based on the camera and lens properties for the current zoom level. The collected meta data streams are analogous to sequences of nid vid t t P R tuples where nid represents the ID of the mobile device vid is the ID of the video file and tindicates the time instant at which the FOV is recorded. The timecode associated with each video frame is denoted by t. In 2D space the field of view of the camera at time forms a pie slice shaped area as illustrated in .

When a mobile device begins video capture the GPS and compass sensors are turned on to record the location and orientation of the camera. The custom written data acquisition software fetches such sensor values as soon as new values are available. Video data are processed in real time to extract frame timecodes t . The visible distance R is calculated based on the camera specifications. All collected meta data i.e. location direction viewable distance frame timecode and video ID are combined as a tuple and uploaded to the server.

An appropriate meta data upload rate should be determined such that the server is updated immediately for realtime video search while the energy consumption for metadata uploads is minimized. Two policies are possible. First the system may send the meta data whenever it is generated. Second it may buffer the meta data locally and then send the accumulated data periodically. Such meta data aggregation and delivery may utilize available network bandwidth more efficiently. For the first policy since meta data is always ready to be uploaded its is assume that the WiFi interface is always on when recording. Whereas for the second policy WiFi will be turned on and off periodically. Some startup energy is consumed when WiFi is turned on.

Another aspect is energy efficient collection of location meta data. GPS WiFi and GSM pose a challenging tradeoff between localization accuracy and energy consumption. While GPS offers good location accuracy of around 10 m it incurs a serious energy cost that can drain a fully charged phone battery very fast. WiFi and GSM based schemes are less energy hungry however they incur higher localization errors approximately 40 m and 400 m respectively . The current disclosure employs GPS based and GPS save strategies. GPS based scheme refers to sampling GPS data periodically while GPS save uses a more complicated strategy. When the device orientation change is within a limited range it is assumed that the device user does not change his her moving direction and the GPS receiver is turned off to save energy. Once the direction changes the GPS receiver is turned on reporting the current location. When meta data with two consecutive GPS data points is uploaded the device location can be interpolated between the two GPS locations on the server. With this method considerable energy can be saved.

This module implements a storage server that manages the video files and the associated meta data streams. It separately stores the video content and the meta data. The video files are linked to the meta data streams by device ID nid and video ID vid . Each FOV tuple in a meta data stream includes a frame timecode tthat points to a particular frame within the video content. This ensures a tight synchronization between the two streams.

The server keeps a data structure nodeInfo for each mobile node which includes the device MAC address the unique device ID and the IP address. While the storage server receives the meta data from mobile devices nid is added automatically to each FOV tuple. An additional binary tag inServer is maintained for each FOV tuple indicating whether the corresponding binary data of the video frame exists or not on the server. Spatial indices are built and maintained to facilitate the efficient search of FOVs.

When a user issues a query the video meta data in the server is searched to retrieve the video segments whose viewable scenes overlap with the geographical region specified in the query. The query region can be a point a line e.g. a road a poly line e.g. a trajectory between two points a circular area e.g. neighborhood of a point of interest a rectangular area e.g. the space delimited with roads or a polygon area e.g. the space delimited by certain buildings roads and other structures .

Given a query Q the query processing module returns a list of the video segments whose corresponding FOVs overlap with the query Q. Each video segment is identified with a tuple nid vid tr t where tand tare the timecodes for the first and last FOVs.

For each video segment in the query results the query processor checks for the availability of the corresponding video content on the server. Recall that the storage server keeps track of which video files are uploaded to the server and what parts of the meta data they do belong to. For the FOVs with the inServer field set to 1 the corresponding video content is available on the server. And conversely for those with the inServer field equal to 0 the video content is not available and therefore needs to be requested from the capturing mobile device. To acquire a missing video segment a Video Request Message VRM is sent to the mobile device. A VRM message specifies the IP address of the target mobile device as well as the corresponding video ID and the beginning and ending timecodes for the requested video segment.

If the requested video with video ID vid is still available on the mobile device the video segment from tto tis uploaded to the storage server. The inServer tags for the corresponding FOVs are set to 1. However if the requested video cannot be located the mobile device notifies the query processor by sending a Video does not Exist Message VNEM . If no response is received from the device after n trials the device is assumed to be turned off and the VRM message is dismissed. If the query processor can locate the videos for the search results on the server it immediately sends the video data to the user. The video segments requested from the mobile devices are sent as soon as they arrive at the server.

To provide synchronization between meta data and video streams the duration encoded date and time are extracted from the video. Timestamp information is then added to every sensor data record to establish the relationship between a video clip and its corresponding geo sensor information. Time is represented in Greenwich Mean Time GMT to avoid time zone issues. Files include the timestamp as part of their filename to avoid ambiguity.

In one embodiment a method of estimating the effective visible distance R includes the process of determining the distance to object or scene locations where one or many camera views are pointing. As used herein the visable distance R may be referred to by terms such a location or its grid cell representation on a 2D map as region of interest. Such regions of interest may be part of an attraction or landmark or consist of a more diffuse area that contains no specific physical objects but may be of interest to users e.g. a beautiful valley .

In one embodiment regions of interest are detected from profiling of data collected from a large set of sensor recordings and then the visible distances R are estimated between the camera and region of interest locations. An embodiment of this method is illustrated in . The framework utilizes the meta data which may be collected with video frames to describe the geographic properties related to the camera view. One of ordinary skill in the art will recognize that multiple methods for determining a region of interest may exist. One such method generates a popularity map based on how often an area appears in different camera views and then identifies the most popular places. Another embodiment of a region of interest detection method computes the intersection points of all the camera views and then identifies the clusters inferred from this point cloud as region of interest. The effective visible distance R may be estimated by calculating the distance between camera locations and the closest region of interest.

A camera positioned at a given point P in geo space captures a scene whose covered area is referred to as the camera field of view FOV also called the viewable scene . In one embodiment the FOV model may describe a camera s viewable scene in 2D space by using four parameters the camera location P the camera orientation a the viewable angle and the maximum visible distance R FOV 15 

The camera position P may include the latitude and longitude coordinates read from a positioning device e.g. GPS sensor and the camera direction a is obtained based on the orientation angle provided by a digital compass. Ris the maximum visible distance from P at which a large object within the camera s field of view can be recognized. The angle is calculated based on the camera and lens properties for the current zoom level. The collected meta data streams consist of sequences of nid vid t t P tuples where nid represents the ID of the mobile device vid is the ID of the video file and tindicates the time instant at which the FOV was recorded. The timecode associated with each video frame is denoted by t.

In 2D space the field of view of the camera at time tforms a pie slice shaped area as illustrated in . To acquire sensor annotated videos a custom recording app for Android and iOS based smart phones or any other platform may be used. When a mobile device begins to capture video the GPS and compass sensors are concurrently turned on to record the location and orientation of the camera. Data acquisition software fetches such sensor values as soon as new values are available. Video data are processed in real time to extract frame timecodes t . All collected meta data i.e. location direction frame timecode and video ID are combined as a tuple and stored for later uploading to a server.

Multiple approaches for detecting region of interest may be used. For example a first embodiment of a method utilizes a grid counting method to obtain a popularity distribution of FOVs. A second embodiment applies a clustering algorithm on a cloud of intersection points of the FOV center lines.

The target space is assumed to be a 2D geographical map i.e. . Let the sensor measurement results be expressed as sets of data of D d . . . d where each dis a multi dimensional observation consisting of GPS coordinates compass direction angles etc.

The target space is first partitioned into equally spaced square grid cells g. Assuming H is the set of region of interest given D then the probability that a grid cell g is a region of interest given the sensor observations is expressed as p g E H D or simply p h D .

To obtain the posterior probability p g D a grid counting based popularity method may be used. Unlike existing popularity estimation methodologies for geo tagging where GPS locations are typically used as the popularity measure embodiments of the present invention may leverage the visual coverage model conveyed by the field of view of a camera because the camera position may be displaced from the area or location in which users are actually interested in when recording video. In such an embodiment if an area is pointed to by cameras more often it will more likely be a region of interest. As illustrated in regions of interest tend to be pointed to or experience overlap from many camera FOVs. One situation needs to be specially handled namely when important objects are located across from each other and hence camera direction vectors coincidentally intersect in an area between the actual region of interest see the example in . Such areas are termed as phantom region of interest .

The algorithm maintains a 2 dimensional data structure representing every map grid cell and containing a monotonically increasing counter of interest. Without prior knowledge of the underlying landmarks or attractions the counter is incremented whenever its grid cell is visually covered by an FOV. Two visual coverage models are investigated a sector based coverage model and a line based model. The sector based coverage model uses an FOV that is abstracted as a sector whose maximal visual distance is R e.g. 1 km . As illustrated in the the counter of all the grid cells that overlap partially or fully with the sector is increased. Since this exhaustive coverage is time consuming to process a lightweight solution is introduced namely a center line based coverage model. It uses a line vector with length R whose origin coincides with the GPS location and whose heading is the camera direction see . With this model only the counters of the grid cells that intersect with the center vector is incremented. The rationale for this approach is that the main focus of interest in videos is often on objects located in the center of the frame or the FOV.

Using either of these coverage models a posterior popularity probability for every grid cell is generated p g D by normalizing the counters as follows

Among all the grid cells the local maxima are computed across the map and are identified as region of interest if their probability is higher than those of all their neighboring cells and the difference exceeds a certain threshold K where Nis the set of g s neighboring cells.

As is illustrated in an intuitive way to detect region of interest is to use the intersection points of the center vector of FOVs. All the intersection points together form a point cloud on which a clustering algorithm can be applied to obtain clusters as the detected region of interest. However this method suffers from a major performance issue due to the generally large number of intersection points. Hence a pre processing procedure is proposed to reduce the number of input points for the clustering algorithm.

The center line of FOV dis separated into m segments d . . . dof equal length R m. Next the intersection points of segment dwith all the other center lines is calculated. The algorithm maintains a data structure for each segment containing a monotonically increasing counter of intersection points. Subsequently for each center line it is represented as a curve to describe the distribution of the intersection points. To increase the signal to noise ratio SNR the curve is smoothed with a moving average filter. Among all the segments of dthe local maxima of the curve are computed and their positions are identified as the points of interest of d. The rationale behind this operation is that the intersection points tend to crowd around the interesting points where the FOVs really focus on.

After collecting the interesting points for each center line a density based clustering algorithm DBSCAN is applied to detect region of interest regions. A number of clustering algorithms are available and the reasons DBSCAN is selected are as follows. 1 DBSCAN does not require the number of clusters to be selected in advance. 2 DBSCAN can find arbitrarily shaped clusters so that the method can identify irregularly shaped region of interest. 3 DBSCAN has a notion of noise suppression so that unpopular areas can be pruned. Finally 4 DBSCAN is insensitive to the ordering of the input points.

Computational Complexity Analysis It is considered that there are NFOVs sensor measurement results in the target 2D space. For the grid based method the time complexity of computing the counters for N grid cells covered by NFOVs is O N . The complexity of computing the local maxima of the map is O N . Hence the overall time complexity for the gird based method is For the clustering based method computing the intersection points of Ncenter lines of FOVs results in time complexity O N . Next the local maxima along the m segments of each center line are computed and the complexity for this operation is O N m . Assume for each center line there are k k

From the above analysis it can be observed that when Nis much larger than N the grid based approach is much more efficient than the clustering based method. On the other hand when Nis much smaller than N the clustering based method is more efficient. Thus the grid based approach is more suitably applicable to dense areas with a lot of FOV data which is the more frequent case while the clustering based method is best applied to sparse data areas.

Let H be a set of estimated region of interest computed from the previous stage. When new sensor values related to a camera view arrive in the system they are transformed into a measurement vector X x . . . x where xconsists of the subtended angle a of a compass direction to the region of interest i H and the Euclidean distance d to the region of interest.

The effective visible distance estimation is based on the observation that a camera view tends to point to an interesting place more often than to an unattractive place. Therefore this information can be leveraged to estimate the effective visible distance by the closeness in terms of lower angular disparity and higher popularity rank to a region of interest and select the closest one. The following equation expresses the effective visible distance as the distance to the region of interest which is most likely pointed to by a camera view by choosing the minimum subtended angle along with a higher posterior popularity probability i.e. 1 p i D 

This computation however may result in an incorrect decision because of the existence of phantom region of interest. As seen in a phantom region of interest should not be chosen as the closest region of interest since it may be erroneous. Such phantom region of interest may be excluded with information from third party landmark databases such as Google Maps or OpenStreetMap. However such landmark information is generally only available for some highly popular areas.

To eliminate such unwanted phantom region of interest when no landmark databases are available a cross intersection elimination method may be used. This exhaustive technique is based on the observation that a phantom region of interest is generated by multiple cross intersections by at least two lines of two independent region of interest. This implies that some candidate region of interest near the intersection points are highly likely to be phantom region of interest. Algorithm 1 outlines the method. It first creates all possible intersection points of two disjoint lines from a set of candidate region of interest lines . If the intersection points and their corresponding region of interest are too close they are discarded lines . Next the algorithm computes how far each region of interest is located from the intersection points and select some within a given threshold distance lines . Finally the algorithm recovers the region of interest that contribute to the elimination of other region of interest and return the remaining region of interest without any cross intersections lines .

After videos and sensor metadata have been recorded automatic video tagging can be applied. Given a set of geographic objects O for a region in geospace and a set of video files V with associated sensor meta data M effective algorithms are designed for the following steps 1 determining the visible objects in video ViVi by querying O through the meta data Mand extracting textual information about the visible objects to serve as tags 2 scoring and ranking the tags based on the geospatial and temporal properties of their appearances in the video and associating tags with the video segments where they appear.

The first step is to determine which objects have been captured in the video. The viewable scene information calculated based on the sensor meta data identifies the geographic region covered in the video. However in order to describe the video semantics further information about the geographic objects in the captured scene needs to be acquired. These objects may include buildings landmarks roads parks mountains or any other geo object that is associated with a location. In this system information about these objects is retrieved from two geo information services namely OpenStreetMap and GeoDec. These systems provide sophisticated information such as name type location polygon outline and even a 3D model of the geographic objects in geo space. These 3D models reproduce the visual scene of the real world. For example captured videos may be compared to corresponding 3D models with a Google Earth interface. In order to determine the objects that appear captured video the geo information services may be queried and all the objects within a certain distance R around the camera location points may be retrieved. A custom R value can be applied to affect the level of detail required in the tagging process. Next the objects that are within the videos viewable scenes are extracted through geometrical computations. A considerable challenge is that determining the actually visible objects is not straightforward due to occlusions.

Defining 3D VQ. visualizes the viewable scene of the aforementioned video snapshot on Google Earth the visible area is highlighted in blue while the occluded area is highlighted in yellow . Not all the objects are equally visible in the highlighted areas. The objects can be classified into three categories according to their visibility level 

Front Object An object to which there exists a line of sight horizontal tracing ray from the camera location which does not intersect with any other objects see . The buildings labeled with alphabets belong to this category. In the front objects enclose the front area highlighted in blue.

Vertically Visible Object An object which is not in the front but is vertically visible because its height exceeds what the front object is blocking. When the vertical viewable angle range of the object is wider than the angle of the object right in front of it the object will be marked as vertically visible. The buildings labeled with numbers in belong to this category. Note that an object can be vertically visible even behind several objects.

Occluded Object An object which is neither front nor vertically visible therefore it is completely occluded.

When the query algorithm retrieves objects within the viewable scene the object set will also include occluded objects. Thus a more sophisticated method is needed to retrieve the visible objects only. A 3D visibility query is disclosed. In the present disclosure a method to determine the visible objects and their visible angle ranges when observing from a specific viewpoint is disclosed. One limitation of this technique is that the visibility computation is restricted within 2D space and is based only on the outline of the footprint of the objects. Therefore the vertically visible objects will be considered as occluded by the front objects and will be eliminated. For example when their method is applied the Fullerton Hotel completely blocks the camera s sight to the skyscrapers in the back which is not the case in reality as seen in . Hence the method may be extended to 3D space to satisfy the demands of the system.

A 3D visibility query takes an object set O and the sensor meta data M t i.e. the camera location p and the camera viewable angle range at time t as inputs and returns a set of the visible objects and their horizontally and vertically visible angle ranges o V V . Notably the algorithm not only obtains the visible objects but also quantifies their degree of visibility with angle ranges which will later be used for tag ranking.

As illustrated in each visible object has a horizontal angle range such as a for B and b for D which indicates the visual width of it. However a horizontal angle range may not only be associated with one visible object. For example the non front object is also located in the range a. In a general case since vertical visibility is considered as well at an arbitrary horizontal angle value within a there may exist multiple visible objects which are sorted according to their horizontal occlusion forming a visible object queue. Hence an atomic horizontal angle range is defined within which the visible object queue is the same at every angle value. In the range a is not atomic because there exist some angles whose visible object queues are different. On the other hand the three sub ranges of a are all atomic and respectively associated with unique object queues i.e. B D 3 B 2 3 and B 1 .

An incremental algorithm is disclosed to determine the queue of visible objects associated with an atomic range as follows. Initially the queue is empty. When a new object arrives the algorithm will check the horizontal occlusion between the newcomer and the objects in the queue one by one stopping at the first object that is horizontally occluded by the newcomer or at the queue tail. If the newcomer is not entirely vertically occluded it will be inserted right before the last checked object or into the queue tail. Afterwards the vertical visibility of the objects behind the newcomer will be checked and the ones that are wholly occluded by the newcomer will be removed from the queue e.g. the NTUC Center in . The process will stop when no more objects can be added to the queue. The final result will consists of the objects that are sorted according to their horizontal occlusion and are visible to some extent. shows an example of the result queue.

Algorithm 1 sketches the overall procedure of processing the 3D visibility query. To initialize the algorithm a mock object is introduced which is assumed to be infinitely far away from the camera and have zero height so that it is always occluded by any object. First for each object the part of the horizontal angle range that is outside the camera viewable range is filtered from further computation to improve the performance. The camera viewable angle range is split into a number of atomic horizontal angle ranges in each of which the candidate objects will be added to the corresponding queue according to both horizontal and vertical occlusion as is described above. The procedure HorizontalOcclusion implements the 2D horizontal occlusion proposed by Lee et al. while VerticalOcclusion checks whether the object is entirely vertically occluded or not. At the end the algorithm post processes the internal data structure and outputs the set of visible objects and their visible angle ranges i.e. R o . V V .

Note that the visibility query outlined in Algorithm 1 returns the set of visible objects only for a single FOVScene. Hence this query needs to be repeated for every FOVScene in the video.

Effects of Sensor Errors on Visibility The accuracy of the viewable scene calculation is somewhat dependent on the precision of the camera location and heading measurements obtained from the GPS and compass sensors. Zandbergen et al. evaluates the accuracy of locations obtained using different positioning modes on the 3G iPhone. The A GPS locations which is the measurement mode used in the iPhone recording software achieves an average median error of 8m which is small compared to the size of the viewable scene considered e.g. at full zoom out about 13 000 mwhen R 300m . Additionally there are techniques e.g. Kalman filter to manage uncertainty and accuracy issues from insufficient and inaccurate data. An error in the compass heading may be more significant. However since the sensor metadata used in this study have a fine sampling granularity sample sec the amount of overlap between two consecutive FOVs is usually large. The missing visible objects in some video frames due to errors in the compass readings will probably still appear in the adjacent frames. However such errors might have a minor effect on the tag scores. But any false positives would have very low scores and be ignored in the final video tagging.

Assigning Tags to Visible Objects To generate textual tags for visible objects the object set O retrieved from geo information sources is utilized which contain detailed information about the objects such as name type location address etc. A sample schema for object information looks as follows 

In the prototype the names of visible objects are used to serve as tags. As described in the introduction location and geographic names play an important role within all types of tags associated with videos and images of media sharing and search sites. Note that the method could be used to further add versatile information to the visible objects and expand the tag type beyond the geographic domain. For example the method can incorporate event databases e.g. Upcoming http upcoming.yahoo.com and use geo information and time to extract event information tags.

Moreover natural language processing methods can be introduced to generate better and more descriptive tags. In the prototype implementation if there exists a Wikipedia page for a visible object this fact is used to upgrade its rank. The content of such pages can be processed in depth to further expand the tags which is certainly an option in the future.

The visual appearance of objects within video scenes can be quite diverse. As an example an object that is close to the camera would usually be seen clearly whereas another object which is far away from the camera may be barely recognizable. Consequently object tags should have different importance or relevance based on how the corresponding object appears in the video scene. Tag ranking is an indispensable process to increase the effectiveness of tags especially when the tag cardinality is large e.g. dozens of tags are generated for most of the test videos . Hence the tags need to be prioritized which in other words means to prioritize the visible objects.

Scoring Tag Relevance for each FOV Scene The method assess and quantify the relevance of visible objects in an individual FOVScene according to the six relevance criteria below.

Closeness to the FOVScene Center Research indicates that people tend to focus on the center of an image. Based on this observation the method favors objects whose horizontal visible angle range is closer to the camera direction which is the center of the scene and score it with the formula

Distance A closer object is likely to be more prominent in the video. The score for the distance criterion is formulated as

An object that occupies a wider range of the scene either along the width or height is more prominent. The normalized scores for these two criteria can be computed with the formulas

These two criteria focus on the completeness of the object s appearance in the video. The video scenes that show a larger percentage of an object are preferable over scenes that show only a small part of it. The scoring formulas are

After obtaining the scores for each criterion they are linearly combined to compute the overall score of an object in an individual FOVScene i.e. S S where wrepresents the weight to adjust the contribution of each criterion to the overall score. Additionally the method promotes the scores of well known objects or landmarks which are more likely to be searched for.

The object information retrieved from the geo information services include several clues to identify important landmarks. For example in OpenStreetMap data some landmarks e.g. the Singapore Flyer are given an attraction label. Others are augmented with links to the Wikipedia pages which might be an indirect hint for object importance since something described in Wikipedia is believed to be important.

Due to the Nature of Videos the Temporal trends of the scores during a period should be explored. shows an example of score variations over time the Esplanade receives the top score in the first half while the Marina Bay Sands takes the first place in the second half. These score variations increase decrease reaching a peak or dropping to zero are caused by the movement and the rotation of the camera. One observation from this example is that a method can determine the segment where an object stays visible by detecting the two boundaries where the score starts from and returns to zero. For example in the Esplanade is considered visible from t to t. As an important contribution of the technique it can detect what objects appear in the video as well as when they appear. In other words a tag can be specifically associated with a certain segment or segments of the video. Another observation in is that the segments t t and t t are very close to each other. This short time gap between two segments may be caused by camera shake or some other unexpected actions of the videographer. It may not really disturb human visual perception of continuity. As a further improvement a threshold is set to merge close segments e.g. the segments t and t are too far to be merged preventing videos from becoming too fragmented. Notably the segmentation discussed above is logical and dynamic in that the videos are not physically split.

Finally for each segment associated with an object its overall scores is calculated. Formally let S t denote the score that the object obtains for the FOVScene at time t and tand trespectively denote the start and the end time of the isegment among the ones where the object appear. Then the score of the object for this segment can be computed through the formula

Geospatial video acquisition applications have been created for both Android and iOS based mobile phones. These applications concurrently acquire location and orientation meta data from multiple sensors in the phones while recording video. Subsequently the collected meta data is used to model the coverage areas of the video scenes as spatial extents i.e. a camera s set of field of views FOVs .

The acquired sensor data sequences provide a rich source of information which is utilized in the SRV TAGS system to query existing geo information databases with the goal of identifying the geographical objects in the viewable region of the video taking into account occlusions . Subsequently it extracts textual descriptions of the visible objects to serve as tags and scores them based on their relevance to the video and the hints provided by the geo information databases. Notably SRV TAGS can accurately associate tags with not only whole video clips but also delimited sub segments. Then SRV TAGS uses the tags to index the videos leveraging Apache Lucene http lucene.apache.org to facilitate textual search and integrating Apache CXF http cxf.apache.org to expose a programming interface for applications. Finally the prototype SRV TAGS system provides a client web interface where users can search for videos and then watch the results and their coverage visualization on a map. illustrates the overall architecture. Unlike content based methods SRV TAGS does not analyze video content throughout the procedures.

One distinguishing feature of SRV TAGS over prior methods is that it does not rely on analyzing the video content but instead queries geo information sources presently GeoDec and OpenStreetMap http www.openstreetmap.org in order to acquire the information of geographic objects within the FOVs of the video. Since not all the acquired objects are visible due to occlusions a 3D visibility checking algorithm is devised to filter out objects that are occluded. Subsequently SRV TAGS retrieves the names of the remaining objects to serve as tags.

As SRV TAGS is prolific in generating multiple tags it needs to rank them to enhance their utility. Thus the system leverages multiple relevance criteria to score a tag s or its corresponding object s relevance to the video the object s closeness to the scene center the distance between the camera and the object location and the visible angle ranges percentages of the object. Additionally the scores of popular objects are promoted according to the hints provided by the geo information sources. Furthermore the system can determine the start and end timestamps between which the object remains visible. Therefore SRV TAGS provides users with answers of delimited video segments which is especially useful for long videos.

Given the tags and their relevance scores of the video clips segments the textual queries from users can be answered. However the queries may not exactly match the tags because of typos incompletions etc. For example users may expect to retrieve videos showing either the Fullerton Hotel or the One Fullerton by entering the keyword Fullerton. Hence Apache Lucene a sophisticated full text search engine is leveraged to judge the correspondence between the query text and the tags.

When a tag entry tag name relevance score video ID segment boundaries is generated it is fed into the Lucene indexing engine. SRV TAGS maintains two separate indices one at the granularity of whole clips and the other at the segment level. As a query arrives the tags similar to the query text will be retrieved and sorted according to their Lucene scores. Note that this score is calculated based on the similarity between the query and the tag while the ranking score reflects the relevance between the tag and the video clip segment. To calculate the most relevant results SRV TAGS first groups all the items by the tags then sorts the groups by their Lucene score and finally sorts the items in each group by the ranking score.

To be easily accessible SRV TAGS leverages Apache CXF a web service framework to wrap itself as a RESTful service. Three APIs are exposed to potential applications 

Search Provides the basic functionality of the service. It takes in the query text and produces a list of matched tag entries.

Get Acquires all the tags of a specific video and their associated segment details. The input for this function is a video ID.

Add Allows the adding of a new video into SRV TAGS. It parses the sensor data supplied by the recording apps generates tags and indexes them. Afterwards the new video becomes immediately searchable through text queries.

The sensor data input and the output of the API are organized in JSON or XML format. illustrates a snapshoot of one embodiment of a web interface that can be generated according to the methods of the present embodiments.

The objective is to find sensor rich video segments within a relevant area and then to present them as augmentations in a virtual three dimensional environment such as Google Earth. illustrates the overall architecture of the system. The main components are a browser based web client a web server with an associated database a streaming media server and remote access to Google s Earth and Maps servers.

The acquisition of videos that are fused with detailed geo reference information is needed. In order to collect sensor rich videos with the sensor meta data several acquisition prototypes and software applications have been built. shows an excerpt from the XML encoded geo data that is collected by the iPhone application and uploaded to a suitable search engine that provides the backend functionalities. One of the challenges in virtual environments is that it may not be very easy to specify the user s region of interest i.e. the query area . For example currently Google Earth does not support the specification of a spatial query rectangle to delineate a search area. For this reason and because a query area is more naturally expressed in 2D Google Maps is used to let a user select a query window. The search results are then shown properly placed in Google Earth.

The development environment is based on open source tools and software such as XAMPP Apache MySQL PHP the Wowza Media Server and the Flowplayer a video player to render Flash encoded and other content and with technologies such as Ajax IFRAME shim and KML. The Wowza Media Server allows the streaming of video content similar to the Adobe s Flash media server. Using this combination of media server and player any segment within a video specified by start and end timecodes can be played. This feature is used to extract the most relevant clips from videos which may potentially be very long and cover a large geographical area. The client implementation is browser based and hence it is convenient to access from almost anywhere. There are three important components in the system. First meta data collection part which is the previous work in the research. Second the database implementation which contains the 3D meta data. Third the Web interface which is the main component in this system. In this part users can not only search videos through entering a location based rectangle but also get the query results in the same page.

The meta data is stored in a MySQL database to allow for efficient access and search. The design can accommodate a variety of sensor meta data information as is shown in Table 4. illustrates the data collected on with the iPhone application however some other implementations can provide additional information e.g. altitude viewable distance . The most significant 3D meta data in the current system is the heading in the database it is the theta attribute latitude and longitude. The collected 3D data basically represent the camera direction and location as a vector which describes a 3D field of view FOV . Based on this model videos in the correct positions within the 3D scene view of Google Earth can be shown.

In MySQL the proper functionality is provide through User Defined Functions UDF to perform FOV matching as part of the query execution. The use of UDFs allows us to perform operations on data types that are not natively supported in MySQL. An UDF is executed within the engine of the database hence being able to take advantage of the execution environment. The UDF was developed and implemented as part of the prior work. One current limitation is that only searches in 2D space are supported. Because of this the altitude parameter is not implemented. In other words the search is still performed on the 2D data and the results shown in Google Earth are then displayed as 3D sensor data.

Perspective video i.e. transforming video from a 2D plane into a projected plane in a 3D virtual space in accordance with the user s viewpoint is one of the major tasks for web based video overlapping applications. In this domain there exist several viable solutions 

Existing plug in based Rich Internet Application RIA technologies such as Adobe Flash and Microsoft Silverlight support 3D video rendering capabilities. While available for rapid prototyping these environments re quire overlapped web services to provide corresponding RIA compatible APIs.

Pixel level image transformation is also a feasible solution but it requires significant client side processing power.

A Cascaded Style Sheets CSS 3D transform has been proposed by Apple Inc. and it is now under development by the W3C CSS level 3. This method transforms the coordinate space of a video element through a simple change of its transform properties.

An IFRAME shim can establish a layer on top of the Google Earth web browser plug in or other web pages . The IFRAME can aid in the process of rendering videos and is flexible in any environment.

Considering both practicality and feasibility the IFRAME shim approach is chosen as the main technique to overlay 3D perspective video. Hence when the viewing direction changes by a certain angle the video also changes accordingly. With this notion the users will get a more intuitive and immersive experience. Additionally the meta data will be stored in a KML file which allows to automatically invoke an animated the through Google Earth. This is a relatively new capability of Google Earth which can help us automatically traverse the environment. Furthermore the camera trajectory will also be shown in the 3D world. With the presentation of the trajectory the users will explicit In Google Earth the number of modeled 3D buildings varies among different cities but overall the number is steadily increasing. When 3D building structures exist the captured video can be more convincingly overlaid with the virtual world. When viewing these buildings it is determined whether the scene in a video matches the same position in the virtual world. It can also observed how accurate the these 3D buildings have been modeled. Note that due to the current limitation of Google Earth which does not allow the drawing of dynamic rectangles Google Maps are used to enter the query region by applying the Google API. Therefore the query mode is currently 2D which may be extended in the future. There are also a number of other technologies used for the web interface. To embed Google Earth and Google Maps in the same web page Ajax may be used. To achieve the interaction between Google Earth and Google Maps interfaces the google.earth namespace the GEView interface and the Maps API GPolygon are used. The specific details of each technology are given below.ly follow the camera movement associated with the video.

The google.earth namespace contains global functions to support to the use of the Earth API interfaces. A listener may be attached to Google Earth for a specific event which means that if Google Earth moves the program will be aware of the movement and simultaneously move Google Maps.

The GEView interface checks the view behavior of the observer camera in Google Earth. There is a function that can return the global view region that is visible. It is noteworthy that the returned region may not be very accurate because it will be larger than what is strictly visible.

Maps API GPolygon is an interface to create a polygon in Google Maps. Through this the users will directly get a view of the query region.

In the system data needs to be exchanged between the clients and server corresponding to the query and result data. The overall architecture is shown in . The numbers 1 through 5 indicate the sequence of interactions between a client and the server. Initially the user sends a query window to the server. There the data will be processed by means of using PHP to invoke the UDF then returning the query results to the PHP code. The query results contain a KML file and video clip information which is used to play back the video clips from the media server. Finally the results will be sent to the user where they are shown in Google Earth.

The main functionality in this client server interaction is coded with Ajax. With this technology web applications can retrieve data from the server asynchronously without interfering with the display and behavior of the existing page. At the same time because of the properties of Ajax an dynamic interface can be established for the web page.

A prototype sensor rich video acquisition module was designed and implemented on an Apple iPhone 3GS handset which provides the built in GPS receiver and compass functionality.

The Sensor rich video App was developed with Apple s Xcode development environment for iPhone OS version 3.1.2 or later. The Sensor rich video App is composed of six functional modules 1 video stream recorder 2 location receiver 3 orientation receiver 4 data storage and synchronization control 5 data uploader and 6 battery status monitor.

This module employs the UIKit Framework of the iPhone OS Cocoa Touch Layer to invoke the built in camera. Among the three video formats that the iPhone supports the medium quality profile is used. However this could be changed based on user needs. Table 5 summarizes the audio and video acquisition parameters.

To engage and control the built in GPS receiver and magnetometer the Core Location Framework in iPhone OS Core Services Layer is used. Location data consists of longitude and latitude and the position can be regarded as the mobile phone exactly as the position of the camera. For the orientation information however an interesting difference between the true pointing direction and the device heading is discovered. Therefore the system also fetches the accelerometer data from the UIKit Framework to determine an adjustment and ensure that the data that is recorded represents the camera s direction even when the phone is held vertically.

An interesting aspect in sensor data acquisition is the sampling frequency. The update frequency may be set based on a distance filter for the location data and a fixed sample rate for the orientation information. A location update is triggered whenever a distance movement of more than 10 meters is detected. The sampling rate for the compass is set to 30 per second. Experimentally with these settings viewable scene changes may be discovered well while saving battery energy as much as possible. Furthermore an expiration deadline is set for every data item obtained. If the location coordinates were obtained longer than 5 seconds ago this data is considered as stale. For orientation data its lifetime is set to 2 seconds because of its higher variability.

This module manages the storage of the sensor data on the device s flash disk. The goal is to utilize a flexible data format that can be easily ingested at a server. In this situation a Property List may be chosen in the Core Data Framework as the structured data representation. The Property List provides an XML based abstraction for expressing simple hierarchies of data.

To provide synchronization the duration encoded date and time is extracted from the video via the MOV multimedia framework. Timestamp information is then added to every sensor data record to establish the relationship between a video clip and its corresponding geo sensor information. Time is represented in Greenwich Mean Time GMT to avoid time zone issues. Files include the timestamp as part of their filename to allow for easy disambiguation.

Data Uploader This module employs an open source wrapper around the CFNetwork API in iPhone OS Core OS Layer named ASIHTTPRequest. This third party class makes some of the more tedious aspects of communicating with a web servers easier and it is suitable for performing basic HTTP requests and interacting with REST based services GET POST PUT DELETE . The Data Uploader transparently utilizes Wi Fi 3G or 2G cellular networks to transmit data files. Importantly this module implements the two different strategies 1 both video and sensor files are uploaded concurrently and 2 only the sensor files are uploaded first while the video files may be transmitted later. Video files on the flash disk are tagged whether they still need to be uploaded.

Methods for automatically positioning and displaying videos in 3D environments are disclosed. First the acquired sensor data in the case was not using the same coordinate system as Google Earth or Google Maps. Therefore the data needs to be converted so that it is compatible with systems such as Google Earth. The experimental GPS sensor data information is based on a format of degrees minutes and seconds. However the longitude and latitude in Google Earth uses a decimal degree format as represented by the WGS84 coordinate system. The broader issue here is that multiple coordinate systems need to be supported and data needs to be correctly identified and converted to support large scale video acquisition and applications.

Second sensor values by their very nature are sometimes noisy and errors and drop outs may occur in practice at times. This problem of data quality will require further study for example to investigate interpolation and error correction methods. Another issue may be the accurate registration of 3D buildings in Google Earth or other virtual worlds . Furthermore the 3D datasets are far from complete and only a few cities have extensive 3D structures in Google Earth. When buildings are missing then naturally there will be a visual mismatch between any video and the 3D world in that area. This may disrupt a user s navigational experience.

Third as mentioned earlier current display technology is mostly 2D and this makes it difficult for the user to specify a 3D query region through for example mouse interactions. In the prototype Google Maps is used to aid in the query area definition but eventually a full 3D query input would be desirable.

Finally there is the practical challenge of overlaying videos on an application such as Google Earth. Some interfaces exist to deal with images and videos. Although they have rudimentary support for geo location information they are still not suitable for the research. For example existing applications in Google Earth only show YouTube videos which are specified by some URL information. More flexibility is helpful for the selection of the IFRAME shim method instead using the Google API. In addition the system uses the own media server which can manipulate the source video clips by extracting segments or perform other operations. A current limitation is related to 3D perspectives. With the technology of IFRAME shim under Mac OSX 3D perspectives may be implemented in Google Earth with the latest webKit.

In the prototype system it is demonstrated that the automatic placement of videos into the three dimensional coordinate system of Google Earth and the result is very promising. There exist some challenges that still need to be overcome such as the sensor accuracy of the collected dataset because of weather conditions and other environmental effects. However very significantly most of the data can be placed well and fully automatically in the experiments. For large scale datasets such an automatic processing is of critical importance.

All of the methods disclosed and claimed herein can be made and executed without undue experimentation in light of the present disclosure. While the apparatus and methods of this invention have been described in terms of preferred embodiments it will be apparent to those of skill in the art that variations may be applied to the methods and in the steps or in the sequence of steps of the method described herein without departing from the concept spirit and scope of the invention. In addition modifications may be made to the disclosed apparatus and components may be eliminated or substituted for the components described herein where the same or similar results would be achieved. All such similar substitutes and modifications apparent to those skilled in the art are deemed to be within the spirit scope and concept of the invention as defined by the appended claims.

