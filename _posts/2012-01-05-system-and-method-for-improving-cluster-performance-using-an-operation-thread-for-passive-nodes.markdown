---

title: System and method for improving cluster performance using an operation thread for passive nodes
abstract: A system for providing improved cluster operation performance comprises a storage system and a cluster system communicatively coupled to the storage system. The cluster system comprises an active node and a plurality of passive nodes. The active node comprises a storage system interface engine and at least one initiator engine and each of the plurality of passive nodes comprises a storage system interface engine and at least one initiator engine. The storage system interface engine of the active node is configured to coordinate communication between the cluster system and the storage system, and simultaneously communicate an operation request from each of the plurality of passive nodes of the cluster system to the storage system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08726072&OS=08726072&RS=08726072
owner: NetApp, Inc.
number: 08726072
owner_city: Sunnyvale
owner_country: US
publication_date: 20120105
---
This application is a Divisional of U.S. application Ser. No. 11 729 531 filed Mar. 29 2007 now allowed. Said U.S. application Ser. No. 11 729 531 is hereby incorporated by reference in its entirety.

Embodiments of the present invention relate generally to the field of data management and more particularly to a system and method for improving operation performance in cluster systems.

A server cluster is a group of independent servers running a server system and working together as a single system to provide high availability of services for clients. When a failure occurs on one node in a cluster resources are redirected and the workload redistributed to another node in the cluster. Typical uses for server clusters include file servers print servers database servers and messaging servers.

The servers comprising a cluster are often referred to as cluster nodes. Cluster nodes may be active nodes or passive nodes. An active node may refer to a host that is actively running the system. The active node is also referred to as the active cluster primary server or active server. A passive node may be an available network node that listens for transmissions but is not actively involved in passing them along the network typical of a node on a bus network.

Certain operations performed on active and passive nodes of a server cluster often require the operation to be performed on the active node and then on the passive nodes of the cluster. Performance of an operation such as an operation to connect or disconnect a virtual disk may be necessary for processes such as cluster failover storage resource provisioning and the like. Typically an operation performed on multiple passive nodes in a cluster is required to be performed in sequence on each individual passive node. For instance an application such as a file system manager may initiate the operation on a first passive node of the cluster and after the file system manager has verified that the operation has either successfully or unsuccessfully performed on the first passive node the operation may be performed on a subsequent passive node. Thus the time to perform an operation is the sum of the operation performance time of the active node and the operation performance time of each passive node in the cluster. Referring to a flow diagram of a prior art method for initiating a connect or disconnect operation on the nodes of a cluster is shown. An active node may require T seconds to perform a requested connect or disconnect operation . The time required to connect or disconnect on a subsequent passive node may be T1 seconds and the time required to connect or disconnect on a passive node following the first passive node may be T1 seconds . The third passive node and the fourth passive node may also require T1 seconds to connect or disconnect . Therefore the total time t taken to complete a connect or disconnect operation on the cluster may be represented as 1 

Consequently it would be advantageous to provide a system and method for improving operation performance in cluster systems by providing simultaneous operation initiation for all passive nodes in a cluster.

Accordingly the various embodiments of the present invention provide a system and method for enhancing cluster performance. In accordance with a first aspect of the present invention a system for enhancing cluster performance is provided. Such a system may comprise a cluster system and a storage system coupled to the cluster system via a network. The cluster system may be a collection of servers which may be referred to as nodes. Nodes may be active nodes or passive nodes with respect to a resource group and represent individual servers physical or virtual of the cluster system. A resource group is a collection of resources such as a volume a particular network address or an application such as a web server . A passive node may be a cluster node that listens for transmissions but is not actively involved in passing them along a network such as a standby server. Active node refers to the node that is actively running the system and managing the resource group and may also be referred as a primary server or an active server. Advantageously the present invention coordinates communication between the cluster system and the storage system and simultaneously communicates an operation request from each of the plurality of passive nodes of the cluster system to the storage system. Simultaneous execution of operation requests on passive nodes allows the cluster system to perform an operation more rapidly than if the operations are performed in series on each passive node. The time savings for operation execution increases as the number of nodes in the cluster system increases.

Nodes may each comprise a storage system interface engine configured to receive an operation request and communicate the operation request to a node initiator engine at the node. In one embodiment the node initiator engine of an active node is suitable for communicating the operation request to a communication engine of the storage system. The communication engine is configured to execute the operation request and communicate the status of the operation execution to the initiator engine of the active node. Storage system interface engine of the active node is also configured to communicate the operation request simultaneously to the storage system interface engines of each of the plurality of passive nodes. Passive node initiator engines are configured to simultaneously communicate the operation request to the communication engine of the storage system. Communication engine may then simultaneously perform the operation request for each of the plurality of passive nodes.

It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory only and are not restrictive of the invention as claimed. The accompanying drawings which are incorporated in and constitute a part of the specification illustrate an embodiment of the invention and together with the general description serve to explain the principles of the invention.

Reference will now be made in detail to the presently preferred embodiments of the invention examples of which are illustrated in the accompanying drawings.

Embodiments of the present invention provide a mechanism by which an active cluster node storage system interface engine is suitable for performing an operation on the active node of the cluster. Storage system interface engine may generate a thread comprising a set of operation performance instructions on a first node. Storage system interface engine may subsequently perform the operation on the remaining nodes in parallel following the execution of the operation on the first node. To perform the operation on each of the plurality of passive cluster nodes storage system interface engine may generate an operation sub thread for each of the plurality of passive cluster nodes. Sub threads may comprise the set of operation performance instructions executed on the passive node. Storage system interface engine may execute the sub threads simultaneously on each of the plurality of passive nodes in the cluster. Storage system interface engine may also be suitable for presenting the result of the operation execution to a client application.

In accordance with an embodiment of the present invention illustrates a diagram of network environment capable of implementing the cluster system improvement according to an embodiment of the present invention. System may comprise a cluster system and a storage system coupled to the cluster system via a connection system . Each node in the cluster system is connected to multiple clients .

The cluster system which may be referred to as a node cluster or cluster may be a server system further comprising a plurality of nodes representing individual servers physical or virtual of the cluster system . In one embodiment cluster system may be a high availability HA parallel or distributed cluster system comprising a collection of interconnected computer systems or nodes utilized as a single unified computing unit. Physical or logical servers are generally referred to as cluster nodes and there may be two or more cluster nodes in a cluster. As will be described nodes may each comprise one or more cluster disk arrays or cluster storage and a connection device such as a bus. The connection device may provide interconnectivity for the storage system and the nodes of the cluster.

A typical cluster system implemented with a system in accordance with an exemplary embodiment of the present invention may comprise at least 3 nodes. However system may be suitable for a cluster environment comprising any number of nodes including a multi geographic node cluster comprising any number of nodes as may be desired. It is contemplated that cluster nodes in the cluster may be located in different physical systems. Advantageously the improvement gain achieved by the system may increase exponentially as nodes and sub clusters are added to a cluster .

Each node may be a computer that handles requests for data electronic mail file transfers and other network services from other computers i.e. clients . As will be described in greater detail herein nodes may execute Microsoft Exchange Server and Microsoft SQL Server both products provided by Microsoft Corp. of Redmond Wash. Microsoft Exchange Server is a messaging and collaboration software system that provides support for electronic mail e mail to various clients such as clients connected to nodes . Microsoft SQL Server is a relational database management system. A person of ordinary skill in the art would understand that although the present invention is described in the context of Microsoft Exchange Server and Microsoft SQL Server for illustrative purposes only nodes can execute any other application. Nodes can be connected to clients over a connection system such as a local area network LAN a wide area network WAN a virtual private network VPN a wireless network or a like network utilizing communication links over the internet for example or a combination of LAN WAN and VPN implementations may be established. For the purposes of this description the term connection system should be taken broadly to include any acceptable network architecture.

Each node utilizes services of storage system to store and manage data such as for example files on one or more writable storage device media such as magnetic disks video tape optical DVD magnetic tape and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored on the disk . The disk within a volume are typically organized as one or more groups of Redundant Array of Independent or Inexpensive Disks RAID .

As used herein the word file encompasses a container an object or any other storage entity. Interaction between nodes and storage system can enable the provision of storage services. That is nodes may request the services of the storage system and the storage system may return the results of the services requested by the nodes by exchanging packets over the connection system . The nodes of the cluster system may issue packets using file based access protocols such as the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information in the form of files and directories. Alternatively the nodes may issue packets including block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP when accessing information in the form of blocks. Storage system is coupled to nodes over the connection system .

Storage system can be connected to a user console such as a monitor with a keyboard. A user can provision storage space on disks via user console . To provision storage space a user takes into consideration various factors such as a schedule according to which data will be backed up on disk s the retention plan i.e. how long the data is going to be maintained whether the data is going to be mirrored and how often the data on disk s is going to change.

The memory comprises storage locations that are addressable by the processors and adapters for storing software program code and data structures associated with the present invention. The processors and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate various data structures.

Storage operating system portions of which are typically resident in memory and executed by the processing elements functionally organizes the storage system by invoking storage operations in support of the storage service implemented by the storage system . It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the invention described herein. As illustrated herein storage operating system is preferably the NetApp Data ONTAP operating system available from Network Appliance Inc. of Sunnyvale Calif. that implements a Write Anywhere File Layout WAFL file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein to facilitate access to disks. As used herein the term storage operating system generally refers to the computer executable code operable on a computer that manages data access.

The file system virtualizes the storage space provided by the disks. The file system logically organizes the information as a hierarchical structure of named directory and file objects hereinafter directories and files on the disks. Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored.

Communication engines are configured to communicate with initiator engines of nodes these nodes are shown in and will be described in greater detail with reference to . Communication engines are configured to execute an operation request initiated by a client and sent to an active node. As will be described the active node storage system interface engine receives the request and transmits the request to an active node initiator engine . The active node initiator engine sends the request to one of the plurality of communication engines . Upon execution of the request by the storage system the communication engine communicates the status of the operation execution to the initiator engine of the node that sent the request.

The network adapter comprises a plurality of ports adapted to couple storage system to one or more nodes over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise mechanical electrical and signaling circuitry.

The storage adapter cooperates with the storage operating system to access information requested by nodes . The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC link topology.

Referring now to each of nodes comprises a node processor s node memory a node network adapter a node storage adapter and a local storage coupled by a bus .

The node processors are the central processing units CPUs of the nodes and thus control the overall operation of the nodes . In certain embodiments the node processors accomplish this by executing software such as that described in more detail herein. Node processors may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such devices.

Node memory comprises storage locations that are addressable by the processor and adapters a node network adapter and a node storage adapter for storing software program code such as node software and data structures associated with the present invention. The node processor and node adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate various data structures. Node memory can be a random access memory RAM a read only memory ROM or the like or a combination of such devices. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the invention described herein.

The node network adapter comprises a plurality of ports adapted to couple the nodes to one or more clients shown in over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The node network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network.

The node storage adapter cooperates with the node operating system shown in executing on the nodes to access data from disk The node storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the disk over an I O interconnect arrangement such as a conventional high performance fibre channel FC link topology.

Node local storage is a device that stores information within nodes such as node software node operating system and data. Each node loads node software into node memory from which they are accessed by node processors .

Referring now to according to embodiments of the present invention nodes execute the following software a client application s a backup management engine a user interface engine a storage system interface engine the node operating system a cluster management engine and a plurality of initiator engines . In one embodiment engines are implemented as modules. As used herein the term module refers to computer program code adapted to provide the functionality attributed to the module upon execution by a processor. Client application s can be for example Microsoft Exchange Server and Microsoft SQL Server although the present invention contemplates that other client applications can be executed at nodes . In the case of Microsoft Exchange Server electronic messages are received from clients by Microsoft Exchange Server and are passed to a database DB engine . DB Engine in turn stores the messages on disk as files.

In Microsoft Exchange Server a storage group SG is a unit for storing data. Each storage group may include a database file and a transaction log which describes changes made to the database file. Backup management engine causes the storage system to back up storage groups using for example snapshot capability. The process of creating snapshots is described in U.S. patent application Ser. No. 10 090 963 entitled System and Method for Creating a Point in time Restoration of Database File by Dennis Chapman the contents of which are incorporated by reference herein. A snapshot is a persistent point in time PPT image of an active file system that enables quick recovery of data after data has been corrupted lost or altered. An active file system is a file system to which data can be both written and read.

Backup management engine also causes storage system to create snapinfo files that are stored on disk s . A snapinfo file may include information about a particular snapshot. This information may be for example a timestamp when a particular snapshot was taken and location where the snapshot is stored on disk s . A backup dataset includes one or more snapshots of application data. A backup dataset may further include a transaction log of changes to the application data and one or more snapinfo files describing a backup dataset.

As described herein in reference to after backup management engine initiates creation of snapshots by sending a command to storage system via storage system user interface engine storage operating system of storage system shown in creates snapshots and snapinfo files. Storage operating system reports back to backup management engine when the operation is completed. Storage system interface engine is configured to act as an interface between nodes and storage system . Engine communicates with storage system using an application programming interface API for example Zephyr Application and Programming Interface ZAPI protocol. An API is a source code interface that a computer system or program library provides in order to support requests for services to be made of it by a computer program. In one implementation engine is a SnapDrive for Windows a product provided by Network Appliance Inc. of Sunnyvale Calif.

Storage system interface engine may provide optimum storage management for file systems and filer volumes. Examples of filer volumes include NetApp filer volumes and Snapshot backup and restore operations both available from Network Appliance of Sunnyvale Calif. Storage system may also be suitable for implementation with additional storage managers supporting Snapshot backups such as NetApp SnapManager for Microsoft Exchange. Additionally storage system supports the iSCSI and Fibre Channel FCP protocols. However storage system interface engine may be independent of underlying storage access media and protocol and may be capable of accessing virtual disks of iSCSI and FCP on one or more file systems simultaneously.

Storage system interface engine may also provide a file system extension feature to a client application that may be utilized to automatically increase file system capacity for managed clients when utilization reaches a specified level. This function allows for the provisioning of logical unit numbers LUNs when there is not enough space available in a volume group to extend a file system. In computer storage a LUN is an address for identifying an individual disk drive and by extension the disk device itself. Logical unit numbering may be utilized to define SCSI devices on a single SCSI bus and identify SCSI devices so the client may address and access the data on each disk drive in an array. Logical units may directly correspond to a volume drive for example C may be a logical unit . LUN may be a three bit identifier for a logical unit. A LUN may also be referred to as a virtual disk which may be defined as a group of hard disks bound together at the controller level and presented to the operating system as a single disk. Virtual disks are externally addressable within a target that implements the functions of a device module e.g. part of a node on a SCSI bus . The LUN is the second level of SCSI addressing where the target is level one and the tag is level three. Typically there are pluralities of SCSI disks drives on a single SCSI interface connect.

Provisioning storage in a cluster system refers to allocating physical memory for a specific application or client. One way a storage system interface engine provisions storage resources is by making available resources such as LUNs available to the application or client. When provisioning resources throughout the storage system the storage system interface engine specifies certain parameters for each LUN. These parameters may include the the storage pool the connectivity between the client and the and the RAID type of the storage pool. Additionally the parameters for newly provisioned LUNs may be client specific as each client may require or perform best using a certain size type or otherwise distinct LUN structure. When the physical memory allocated to a specific file system is substantially exhausted because of the amount of data stored within the file system the storage system interface engine may provision one or more LUNs to extend the file system. When storage system interface engine provisions a LUN for a file system the storage system interface engine creates and or allocates additional storage within a storage pool. Preferably the provisioned LUN is from the same storage pool as other LUNs allocated to the same file system. Alternately the provisioned LUN may be from a different storage pool on the same server. In a further embodiment the provisioned LUN maybe from a different storage pool on a different server.

The storage system interface engine may create new LUNs prior to assigning them to a specific client. To make a LUN available to a client application a storage system interface engine of an active node executes for instance a connect command to one of plurality of initiator engines for instance initiator engine .

Nodes may each further execute the initiator engines . Initiator engines are configured to communicate with the storage system interface engine and the communication engines located on the storage system . Communication between initiator engines and communication engines provides LUN creation and mapping for active and passive nodes of the cluster system . To this end storage system interface engine sends a command s to an initiator engine for example initiator engine which in turn passes the command to for example communication engine .

Nodes may be connected to a system administrator s console through which a system administrator can configure various settings such as thresholds to define the bounds of resource utilization for example a maximum amount of disk space allowed connection time thresholds virtual memory thresholds and the like that effect execution of the operation request GUI engine provides a user interface to a system administrator at console . A system administrator can modify the settings via console .

In addition the system administrator performs migration of files to storage system using backup management engine and GUI engine which are components of SnapManager for Microsoft Exchange and SnapManager for Microsoft SQL Server. To this end the system administrator physically copies the files from nodes to storage system . Location of the files such as a directory path and a volume s on which the files reside is stored in node memory .

Nodes further execute the node operating system . Node operating system is a program that is after being initially loaded into node memory manages client applications such as client application executed on nodes . The node operating system can be for example UNIX Windows NT Linux or any other general purpose operating system.

Node may comprise a cluster management engine . A cluster management engine may be a collection of software on each node that operates clusters and configures and manages cluster related activity. The cluster management engine sees all resources as identical objects. Resources may include physical hardware devices such as disk drives and network cards or logical items such as logical disk volumes TCP IP addresses entire applications and databases among other examples. A group is a collection of resources to be managed as a single unit. Generally a group contains all of the components that are necessary for running a specific application and allowing a user to connect to the service provided by the application. Operations performed on a group typically affect all resources contained within that group.

Cluster management engine prevents a passive node from unauthorized access to a resource group. It is contemplated that only one node of a cluster system may own a cluster resource. This node is referred to generally as an active node. For the purposes of this application any one of nodes may be an active node with respect to resource ownership. Furthermore any one of nodes that is not the active node with respect to the resource group may be a passive or non owner node with respect to the resource group.

Examples of a cluster management engine include but are not limited to Microsoft Cluster Server MSCS designed by Microsoft Corporation of Redmond Wash. for clustering for its Windows NT 4.0 and Windows 2000 Advanced Server operating systems Novell Netware Cluster Services NWCS and Veritas Clustering Server. Each of these clustering technologies each have their own way to configure manage and operate clusters. However any cluster management engine may be suitable for integration with a system in accordance with the exemplary embodiments of the present invention by configuring operation instructions to the respective cluster management engine.

Referring now to a flow diagram illustrating a process for executing an operation in a system according to an embodiment of the present invention is shown. Process may be implemented in a system such as system described above and in . In one embodiment process may comprise performing an operation such as a LUN connect or disconnect operation on an active node of a cluster . Performing the LUN connect or disconnect operation on the active node may require T seconds. Process may further comprise performing a LUN connect or disconnect operation on each passive node in the cluster in parallel . In this manner the total time required to connect or disconnect on both the active node and passive nodes of the cluster may be defined by the equation 1 where T1 is the time required to perform the connect or disconnect operation on each of the plurality of passive nodes.

It is contemplated that not all passive nodes may perform an operation as requested by an active node simultaneously. Therefore process may be expanded such that the total time taken t is the representation of the time required to connect on the active node T plus the time required to connect on the passive node requiring the greatest connection time such that total connectivity time may be defined by the equation where n is the passive node requiring the greatest connection time. It is further contemplated that the operation to be performed may be any suitable operation initiated by the active node cluster. For instance operation may be a create or delete operation or a like operation. LUN Creation and Mapping

Referring to a flow diagram illustrating the steps to execute an operation on a plurality of cluster nodes simultaneously as illustrated in is shown. Steps may be initiated by a storage system interface engine of an active node of a system such as has been described in detail above and shown in . Specifically any one of the nodes of the system may be an active node with respect to a resource group suitable for receiving an operation request and initiating the steps described herein. In one embodiment an active node requests creation of LUN at step . The request may be initiated by a user by a client application or by any other entity. Upon receiving request from user for creating a LUN storage system user interface engine of the active node calls an application programming interface API on the storage system to request creation of the LUN. Specifically a LUN creation request initiated by the storage system interface engine of the active node may be communicated to an initiator engine of the active node. Initiator engine passes all required parameters such as LUN Path LUN size LUN type etc. to a communication engine of the storage system .

In response to a LUN creation request the storage system creates the LUN at step and communicates the result of the operation at step to the initiator engine of the active node which in turn communicates the result to the storage system interface engine . The result may be in the form of a return value of the API and the active node may receive the result at step . This step may include the step for establishing a session connection between a communication engine at the storage system and an initiator engine at the active node. If there is no session already established between a storage system communication engine and a node initiator engine storage system interface engine can request corresponding initiator engine for creation of a session.

After creation of a LUN LUN may be mapped to the node initiator of the active node by sending a LUN mapping request to the storage system . For instance once the storage system interface engine receives the success result for LUN creation the storage system interface engine of the active node communicates an API on the storage system requesting to map the LUN to the specified initiator s on the node. In response to the mapping request storage system creates LUN mappings at step and reports the result of the operation to the storage system interface of the active node at step . The result may be a return value of the API call.

In addition storage system communication engine notifies the node initiator engine of the addition of the new disk. Active node initiator engine notifies the operating system OS and OS sends a notification of the new disk arrival to the node storage system interface engine . In an embodiment compatible with a Windows operating system storage system interface engine formats the disk as a Windows disk with the NTFS file system. However it is contemplated that the system may be compatible with any operating system and may be suitable for formatting the disk as a usable media for the operating system. Active node storage system interface engine may subscribe to these OS events during the startup of storage system interface engine service.

Once the active node storage system interface engine receives a notification such as a New Disk Arrival notification on the active node i.e. where a user initiated LUN Create operation the storage system interface engine simultaneously calls a connect API on the other storage system interface engine instances running on the passive nodes of the cluster system step . As described herein connect API may utilize a ZAPI protocol API. Similar to performing a connect operation on the active node connect operation on a passive node comprises of mapping a passive node initiator engine to the storage system communication engine step . Passive node initiator engine mapping provides a passive node access to the LUN in the instance of failover. Cluster management engine may prevent a passive node from accessing the LUN in instances other than failover. The storage system maintains a list of initiators including any passive node initiators mapped to the LUN. If a passive node initiator is not mapped to the LUN storage system does not allow the passive node to access the LUN. Connect operation on passive node further comprises adding a passive node as a possible owner of the resource. Cluster maintains the list of passive nodes available for failover. In this manner failover of the LUN s resource to a passive node is facilitated in the instance of an active node failure. Storage system communication module may then notify the active node initiator module of successful passive node mappings step . Active node initiator module may receive notification of successful passive node mapping step and communicate successful passive node mapping notification to active node storage system interface engine. Storage system interface engine may also create new resource in cluster for the newly created LUN. Storage system interface of the active node may also add successfully mapped passive nodes as possible owners of the new resource .

Active node may performs steps to create a new resource for a physical disk. For instance on successful completion of connect operation on all nodes active node performs the following steps. Active node calls a cluster API to create a physical disk resource for the created and mapped LUN step . If a user specified a new resource group active node storage system interface requests the cluster API to create a new resource with the specified resource group. The newly created physical disk resource is then added to the resource group step .

Storage system interface engine of a node may present the result of the operation execution to a client application. Disk appears as a local disk to the client applications and users. To this end nodes may support a variety of operating systems including Windows Solaris HP UX AIX Red Hat Enterprise Linux or any other operating system suitable for presenting a storage resource to a client.

System may improve cluster failover performance when an active node has failed. In one embodiment system may be implemented with a cluster management engine utilizing a shared nothing architectural model where each node server has ownership of specific storage resources located on a shared storage bus SCSI FCP or SSA except in times of failover. During the failover storage resources are transferred in ownership via SCSI lock release commands from the active node which has failed to a passive node. The passive node may comprise multiple instances of applications designated for failover. Failover occurs when an active node cannot respond to read or write commands. Failover can also be initiated by using cluster management engine s CLI API or Cluster Administrator utility provided by Microsoft Corporation of Redmond Wash. During failover cluster management engine initiates transfer of storage resource ownership along with all the other resources if any contained in a failover group. A surviving node after this transfer and subsequent start up of its copies of the failed application or service then resumes operations that were interrupted at the time of the failover e.g. file and print sharing web services database transactions and queries via roll back restart to the last committed transaction . The surviving node also takes ownership of a quorum resource a special disk or volume that contains the cluster database. This mode of operation may continue until such time as the failed node is revived and brought back on line.

Referring now to a flow diagram of a process for providing performance operation initiation in accordance with exemplary embodiments of the present invention is shown. Process may comprise creating a LUN on a storage system step . An operation for creating a LUN on a storage system may be initiated on an active node of a cluster. On cluster nodes creation of LUN may include the following steps Creating LUN on storage system from active node and connecting the created LUN to all the passive nodes Similarly a LUN connect or disconnect can also be initiated on active node of a cluster. LUN create connect or disconnect initiation may be implemented by a storage system interface engine which may create a thread comprising instructions suitable for performing the create connect or disconnect operation. Process may comprise mapping the LUN to a node step such as an active node. Process may further comprise simultaneously connecting at least two passive nodes to the LUN step . Simultaneously connecting passive nodes to LUN may comprise mapping the passive node initiator engines to a communication engine of the storage system and adding each of the plurality of passive nodes as a possible owner of a resource group. To map the plurality of passive nodes to the storage system a sub thread or worker thread may be created for each of the plurality of passive nodes in the cluster. Each sub thread may execute simultaneously on each of the plurality of passive nodes in the cluster. Parallel execution may be accomplished by simultaneously creating a separate sub thread for each of the plurality of passive nodes from the main thread created to perform an operation on the active node of the cluster. Process may initiate a remote call from a sub thread to a passive node associated with the respective sub thread to perform the required operation on the passive node. Process may then request passive node notification of an operation status step . For example the passive node may indicate the status of the connect or disconnect operation on the node. The status may be a successful unsuccessful connect or disconnect notification. Process may comprise receiving the status notification and report the status to a client application step . Prior to reporting the status to a client application process may inspect the status reported by the individual passive nodes. Process may also allow for multiple tasks to be performed on a parallel basis. It is also multi threaded in design to allow multiple operations to be processed simultaneously.

Although the present invention for purpose of explanation has been described with reference to specific exemplary embodiments it will be understood that the invention is not limited to the embodiments described herein. A person of ordinary skill in the art would understand that the present invention can be practiced with modifications and alternations to those embodiments or can be practiced in other embodiments within the spirit and scope of the appended claims.

Moreover non dependent acts may be performed in parallel. The embodiments were chosen and described in order to best explain the principles of the invention and its practical applications to thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications as are suited to the particular use contemplated.

Furthermore the use of the phrase one embodiment throughout does not necessarily mean the same embodiment. Although these particular embodiments of the invention have been described the invention should not be limited to these particular embodiments. Accordingly the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense.

Moreover the teachings of this invention can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment and a storage area network. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or system.

Unless specifically stated otherwise it is to be appreciated that throughout the discussions utilizing terms such as processing or computing or calculating or determining or displaying or the like refer to the action and processes of a computer system or similar electronic computing device that manipulates and transforms data represented as physical e.g. electronic quantities within the computer systems registers and memories into other data similarly represented as physical quantities within the computer system.

The present invention can be implemented by an apparatus for performing the operations herein. This apparatus may be specially constructed for the required purposes or it may comprise a machine such as a general purpose computer selectively activated or reconfigured by a computer program such as a collection of instructions for execution by a machine or processor for example stored in the computer. Such a computer program may be stored in a computer readable storage medium such as but not limited to any type of disk including floppy disks optical disks magnetic optical disks read only memories random access memories EPROMS EEPROMS magnetic or optical cards or any type of media suitable for storing physical e.g. electronic constructions and each coupled to a computer system bus. Each of these media may be coupled to a computer system bus through use of an appropriate device for reading and or for writing the media.

