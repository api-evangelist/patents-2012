---

title: Asynchronous consistent snapshots in persistent memory stores
abstract: Crash recovery with asynchronous consistent snapshots in persistent memory stores of a processing environment. A processing environment includes a user program and infrastructure-maintained data structures. The infrastructure-maintained data structures include a log of updates made to program data structures and a snapshot of the state of the program data structures. The systems and methods include writing log entries in the log to a transient memory. The log entries correspond to store instructions and memory management instructions operating on a nonvolatile memory (NVM), and input/output (I/O) operations executed by program instructions of the user program. Each of the log entries represents an effect of a corresponding operation in the program instructions. The systems and methods also include creating a snapshot in the NVM after a consistent program point based on the log of updates. The snapshot provides a rollback position during restart following a crash.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09612757&OS=09612757&RS=09612757
owner: Hewlett Packard Enterprise Development LP
number: 09612757
owner_city: Houston
owner_country: US
publication_date: 20121019
---
Non volatile memory NVM retains stored data even after the power is turned off. The most widely used type of NVM has been flash memory. However flash memory has to be written in larger blocks than typical computers can automatically address and typically only lasts for a finite number of write erase cycles before deteriorating. Newer types of NVM offer better performance while maintaining the non volatility aspect of flash memory. These technologies include for example memristors phase change memory PCM and spin torque transfer RAM STT RAM .

Memristor devices are based on resistance switching effects and allow greater data density than hard drives with fast access times. PCM devices handle read write operations based on changes in electrical resistance offering high storage density. STT RAM devices flip the active elements used in magnetic random access memory RAM and offers lower power consumption with better scalability.

New non volatile memory NVM technologies such as but not limited to memristors phase change memory PCM and spin torque transfer RAM STT RAM offer an opportunity to redefine the data persistence model used by applications. Memory accesses at the byte granularity along with latencies on par with dynamic random access memory DRAM enable fine grain and efficient persistence of data through central processing unit CPU store instructions. However the combination of caches and the presence of faults in the computing environment pose a challenge in ensuring that persistent data is consistent for example during recovery following a crash.

The term consistent state is used herein to mean that all program invariants are met. Without a mechanism to enforce program invariants a hardware or software failure may render persistent data corrupt and hence unusable to applications.

Write back caching the default for many systems today complicate matters by delaying visibility of writes in persistent memory. Consider for example an instruction sequence where a chunk of memory is allocated then initialized and then a globally visible pointer is made to point to the chunk. In the presence of caching the globally visible location may appear in persistent memory before the initialized values. In this scenario if a program crashes before the initial values are written to persistent memory the persistent state will be left with a globally visible memory location pointing to uninitialized memory resulting in an undesirable operating condition.

In order to reduce or altogether prevent such inconsistencies programmers may employ a combination of instructions that act as a barrier between two data stores. This barrier will henceforth be referred to as a visibility barrier . For example a combination of a memory fence and a cache line flush may be used between two stores of persistent memory to ensure that the first store is visible on NVM before the second store becomes visible. Given a virtual address a cache line flush command flushes the corresponding cache line to memory and invalidates the cache line. But these low level constructs make programming tedious and error prone.

Programmers may also use transactional consistency semantics for critical sections and atomic sections of multithreaded programs. A critical section is a code segment where the shared data are protected by locks. An atomic section is a code segment that has to appear to execute in an indivisible and isolated manner. In serial and distributed programs a construct that provides failure atomicity and hence consistency for a section of code can be provided. Frameworks that try to provide the appropriate guarantees for these code constructs may use undo logging and make use of a similar barrier as described above. If the program crashes the state of the application can be reestablished by replaying undo logs during recovery.

However the process has to ascertain that an undo log entry is visible on the NVM before the corresponding store is visible for example by adding a visibility barrier between an entry in the undo log and the original store to the NVM. Additional instances of the visibility barrier are used while creating a given entry in the undo log to ensure that different parts of the entry are mutually consistent. Executing a visibility barrier can be computationally expensive e.g. taking hundreds of CPU cycles or longer to execute .

The systems and methods described herein reduce the overhead of capturing a consistent state using infrastructure maintained data structures i.e. data structures created maintained in transient memory and NVM and includes logs and a snapshot. The logs are created in transient memory e.g. DRAM instead of in the NVM itself. In addition the consistent snapshot is maintained in the NVM. Thus the question of correct ordering of visibility of log entries does not arise any more because the log entries are no longer in the NVM. Hence the visibility barriers can be removed from the code executed by the user threads. Removing visibility barriers from the user threads i.e. the critical processing path improves performance of failure free operation by allowing user threads to operate at or near the full execution speed of the underlying program. The systems and methods described herein may be efficiently implemented in the processing environment on machines utilizing NVM technologies e.g. memristors PCM and STT RAM as well as on machines utilizing block devices such as hard disks and solid state drives SSDs .

Before continuing it is noted that as used herein the terms includes and including mean but are not limited to includes or including and includes at least or including at least. The term based on means based on and based at least in part on. 

System may be implemented in the processing environment of any of a wide variety of computing devices such as but not limited to stand alone desktop laptop netbook computers workstations server computers blade servers mobile devices and appliances e.g. devices dedicated to providing a service to name only a few examples. The computing devices not shown may include memory storage and a degree of data processing capability at least sufficient to execute the program code described herein.

The system may be implemented in a region of abstraction to present persistent memory to applications. Accordingly the region of abstraction is referred to herein as a persistent user region or simply as user region . An example user region Ris shown in block in as the user region Rmay be implemented as data structures in the NVM that are used by the program during program execution. Without lack of generality and for the sake of simplicity only one user region Ris shown in although more than one user region may be implemented.

The user region Ris addressable by a range of virtual addresses that are mapped to physical pages in the NVM. An Application Programming Interface API for creating and accessing the user region Ris not the focus of this disclosure and therefore the API is not described in detail. Briefly however if a programmer wants to create a user region Rin the program the user region Ris created e.g. by calling CreateRegion and persistent data structures are allocated out of the user region R e.g. by calling nvm alloc . The nvm alloc has a corresponding nvm free and the user region Rhas a persistent root e.g. within the same persistent region at a fixed offset for the purpose of accessing the data structures therein. It is noted that data within a persistent region is not serialized and so pointers may be embedded in the program.

Without loss of generality the user region Ris mapped at the same base address in the virtual address space and hence the embedded pointers can be used as is without relocation. While the program is executing any data within the user region Rthat is not reachable from the region s persistent root or from a program root is considered garbage and the physical space can be recycled by returning the memory to general use. The physical space can be recycled during program execution or during recovery after a program crash.

During program execution the user threads write logs L to transient memory . It is noted that by writing the logs L to transient memory the logs L will be cleared from memory in the event of a program crash or system restart. Accordingly the helper thread reads the logs L from transient memory and generates a snapshot region Rin the NVM. The snapshot region or simply snapshot Rincludes data of all consistent user regions in the NVM. The snapshots are written to a persistent region of the processing environment e.g. in NVM . The helper thread generates snapshots Rwhen the processing environment is at a consistent state e.g. after the user threads complete a transaction .

In an example the helper thread utilizes a queue for input output I O operations. The queue may be maintained in transient memory e.g. DRAM to coordinate I O between the user threads and the helper thread . As noted above the queue is written to transient memory and thus disappears if the program crashes. However the snapshot Rs has been written to NVM and thus can be utilized during a system recovery to restore the processing environment to the most recent consistent state.

The operations described above may be implemented by program code or machine readable instructions executed by the processing environment. In an example the machine readable instructions may be implemented as but not limited to firmware or other program code. The machine readable instructions may execute the functions as self contained modules. These modules can be integrated within a self standing component or may be implemented as agents that run on top of existing program code. The machine readable instructions may be stored on non transient computer readable media and are executable by one or more processing unit s to perform the operations described herein. It is noted however that the components shown in are provided only for purposes of illustration of an example operating environment and are not intended to limit implementation to any particular architecture. In the example referenced by the architecture of machine readable instructions may execute as follows.

For every store to user region R the user thread creates a log entry L in transient memory. Logging may be performed in a lazy manner. That is there is no ordering requirement between visibility of the log entry and the store to R. Additionally there is no requirement to use a visibility barrier between different log entries because the data entity L is not stored in the NVM regions or . Thus creating the log L can proceed asynchronously in relation to the user thread .

At various times e.g. on a timer pop the helper thread examines the data entity and computes a globally consistent state G. A globally consistent state Gis typically defined at the completion of a transaction although other definitions may be employed e.g. completion of multiple transactions or at predefined times or points in the program code . Every time a globally consistent state Gis computed the state of the snapshot Ris updated in a failure atomic manner. This update may occur serially because only one helper thread needs to be instantiated to update the data entity with the snapshot R. A failure atomic update means that if a crash happens during an update either all or none of the values being updated are visible in data entity .

It is noted that while this step can be said to be operationally expensive this is the only step that uses a failure atomic update. This is the only step that may use visibility barriers. Any cost during this step is incurred by the helper thread not the user threads and therefore this update does not slow execution by the user threads . As such failure free operation can proceed fairly efficiently.

It is noted that the helper thread may prune the logs L from data entity upon completion of a successful snapshot R. For example the helper thread may clear the unnecessary entries from logs L and return the associated memory space in transient memory to the processing environment for use by the user threads . Once created the snapshot Ris not removed until the end of the program thus providing a rollback position for the system to a globally consistent state Gduring restart following a system crash.

If the program crashes or the system otherwise needs to return to a globally consistent state G a recovery process can be executed during restart as follows. The recovery process reads the snapshot R stored in the nonvolatile region of the NVM to overwrite a potentially inconsistent user region R in the nonvolatile memory. If Rwas updated after the last snapshot was written into data entity R and before the program crash then these updates may be lost. However user region Ris now guaranteed to be in a globally consistent state G based on the snapshot R. Unreachable regions in Rcan be garbage collected and memory space returned to the NVM.

It is noted that the recovery process does not have to be failure atomic because the snapshot Ris not removed until the recovery process successfully completes. If a system crash occurs again e.g. during the recovery process or before a new snapshot is generated the recovery process can be re applied using the most recent snapshot R.

Any data in the snapshot region Rcan be logically described as a list of duples where each duple is a pair containing a memory address M and a corresponding value M . Memory address Mdenotes a virtual address allocated within user region R. However to exploit spatial locality chunks of memory may be maintained in Rto shadow allocated chunks of memory in R. In effect these have a one to one mapping. An example build process of snapshot region Rmay be implemented as illustrated in .

For every store to user region R the user thread writes a redo log entry in logs L. The log entry may have an operation code an address of the location and the value written.

A log entry is created for every call to CreateRegion. The operation code e.g. one identifying CreateRegion the returned address e.g. the start address of the assigned region S and the size of the region are added to the log entry.

A log entry is also created for every memory manipulation request. Without lack of generality the operations may be memory allocation e.g. nvm alloc and memory deallocation e.g. nvm free . For the operation nvm alloc the log entry stores an operation code the returned address and the size. For the operation nvm free the log entry stores an operation code and the address provided. It is noted that the address logged for either operation nvm alloc or operation nvm free belongs to user region R and no memory manipulation happens in the snapshot region Rat the point of creating these log entries.

The snapshot region Ris created and manipulated by the helper thread . Hence the snapshot region Rcan be manipulated in a serial fashion. When a program starts no corresponding snapshot region Rexists. If the program crashes before snapshot region Rcan be created the situation is detected during recovery and the root pointer in user region Ris set to NULL. Thus all other data in user region Rbecome unreachable and are garbage collected. In essence this scenario reverts user region Rto the initial state as it was just after creation. If a program crashes after Ris created Ris used to overwrite Rduring the recovery phase.

When the helper thread computes a globally consistent state the helper thread proceeds to update snapshot region Rwith this newly computed state in a failure atomic manner. So if the program crashes during this update of snapshot region R none of the changes in this globally consistent state are visible in snapshot region Rduring the recovery process and user region Rreverts to the previous globally consistent state.

The globally consistent state Gis computed from the log entries in the log L and thus the globally consistent state Gis made up of a subset of those log entries in data entity L. Snapshot region Ris updated with this newly computed state by walking the associated log entries with accompanying actions depending on the operation code of the log entry walked. The log entries may be walked in the order created or in some other order that preserves dependencies among log entries. It is noted that during this walk only snapshot region Ris updated and user region Ris not updated.

Specific actions may be executed during creation of a snapshot depending on the particular opcode. The following are examples of some opcodes.

If opcode is CreateRegion a corresponding region is created in snapshot region R. The start address of the newly created region in snapshot region Rmay be denoted as S. The start address Sof the region previously created by the user thread is available from the log entry. Thus the offset can be determined e.g. offset S S . At the same time table Tis updated with a new entry e.g. start addresses .

Consider opcode nvm alloc. If is encountered memory is allocated within Rat the address snapshot return address return address offset of size size s. Table Tis updated with the duple .

Consider opcode nvm free. If is encountered memory at address snapshot free address p offset is freed within snapshot region R. Table Tis updated with the duple .

It is noted that the tables T T and Tmay be used during the recovery process to overwrite the state in user region R with the state in snapshot region R. Thus the tables provide a mapping of data between these memory states.

In an example the update to snapshot region Rmay occur in a failure atomic manner. This means that when the snapshot is updated from one globally consistent state to a more recent consistent state either all or none of these updates should be visible in the snapshot on NVM.

To correctly handle input output I O or externally visible operations these operations may be deterred until the snapshot Ris updated with the globally consistent state Gthat overlaps with the I O operation. Globally consistent state Goverlaps with an operation if a log entry exists within a globally consistent state Gthat corresponds to that operation. For an I O operation to be issued it does not matter whether the corresponding updates to user region Rhave actually made their way all the way to persistent memory.

In this example the potential crash point occurs just before an I O statement occurring after the first durable section has completed. As such a globally consistent state exists for recovery and a corresponding snapshot may be available on NVM.

It is noted that the globally consistent state available at program point does not include the subsequent print statement illustrated as print success in the code shown in . Consequently the user thread does not execute this I O operation immediately and the operation is instead buffered. Buffering may be implemented by encoding the operation code e.g. the print statement and the values of the corresponding arguments when the log entry is created. Buffering may occur in a logical manner within the helper thread so that when the appropriate globally consistent state is reached the helper thread can perform the externally visible operation on behalf of the user thread. I O operations are executed by the helper thread in the order that these operations are supposed to be executed by the user thread.

If the program successfully executes beyond crash point then a subsequently computed globally consistent state computed at program point overlaps with the print statement. Accordingly the helper thread executes the print statement subsequent to computation of this globally consistent state.

If a crash occurs at the program point the most recent globally consistent state is the one computed at program point . Thus on recovery the values corresponding to globally consistent state are copied from the snapshot region to the user region of the NVM. Thus on recovery the available consistent state based on the snapshot on persistent memory is in sync with the I O statements executed before the crash. For example in the I O statement print success does not have to be re issued after recovery.

Creating logs L in the user thread and having the helper thread consume logs L establishes a producer consumer relationship where the user thread produces and the helper thread consumes. Accordingly a two way communication may be implemented transparently between the user thread and the helper thread to coordinate the I O operations and the values exchanged.

In an example a queue Q see may be utilized. The queue Q may be created in transient memory. Thus if a crash occurs at this point queue Q may disappear. If the user thread needs the result of an I O operation which does not yet have the ready status in this queue Q then the user thread is blocked. The queue Q may be implemented transparently wherein the helper thread maintains the identity of the I O operation completed in addition to its status. The helper thread is considered the producer and the user thread is the considered the consumer of this data structure Q . The queue Q is finalized only after the corresponding persistent snapshot is successfully updated.

With reference again to the example illustrated in if the program successfully executes through all durable sections e.g. to program point then the region root points to x. In turn x next points to y y next points to z z next points to w and so forth.

If instead the program crashes at program point the persistent user data is the region root points to x x next points to y and y next points to z. It is also possible that some of these updates are not yet visible on non volatile memory. However the snapshot R has been updated prior to the crash point such that snapshot R corresponds to the end of the first durable section. Hence snapshot R includes the following information the region root points to x x next points to y y next points to null. When data from the snapshot R is copied to the user region R during recovery then z is unreachable and it can be garbage collected. Similarly if existing locations were freed in the second incomplete durable section the effects of the de allocation operations won t be visible and show up as still being allocated. Thus it can be seen that correct semantics are still presented even in the face of failures.

Data operations may be better understood with reference to the example shown in . illustrates example states of a user region R log entries in log L operations in queue Q and data in a snapshot region R for a program. The example illustrated by is based on the following program segment 

It is noted that the initial state is empty for all data structures. However the log continues to grow as more instructions are executed. The end of the first durable section corresponds to line in the code illustrated by block in . The log entries end at end durable at line . The snapshot region may not have been updated at this point so it is seen to be empty in . Because no I O operation has been issued is also empty. The state of the logs and other data structures at program point identified by line is shown in block .

The state of the log L at line is the union of the log entries shown for line and line . That is at some point between lines and a globally consistent state is computed and a corresponding snapshot is created and copied to the NVM in a failure atomic manner. This snapshot only contains x as shown in

After generating the snapshot the helper thread issues the print start command and records this action in the queue Q as shown in . In the event of a crash at in the above code segment the recovery operation overwrites the state of the user region with the snapshot. Just before the crash point the user region has both x and y with x next pointing to y. But this is an inconsistent state because the second durable section started in line 8 in the above code segment has not yet completed. On recovery the data in snapshot region which has only x as shown in is used to overwrite the data in the user region. It is noted that this is also consistent with the print start command having already been issued although the print success command has not yet been issued.

Before continuing it should be noted that the examples described above are provided for purposes of illustration and are not intended to be limiting. Other devices and or device configurations may be utilized to carry out the operations described herein.

In operation store and I O instructions such as writing to a file are issued by the user thread s to persistent memory . Store instructions change the state of persistent memory and I O instructions are interactions with the outside world e.g. outside of the processing environment . In operation the store and I O instructions are logged by the user threads to transient memory . The output is held in operation for a snapshot and later issued in operation when a snapshot is generated . When the program stores to a persistent data structure the effect of that store is visible in a user region. As noted above the user region is maintained in persistent memory .

Operation includes the helper thread creating a snapshot region in the persistent memory for each of the log entries in transient memory . In an example the helper thread creates the snapshot only after reaching a consistent state during execution of the program. The snapshot region provides a rollback position during restart following a crash. Accordingly in operation during the recovery phase the processing environment can revert to a previous state represented by the snapshot of data at a most recent consistent state.

The operations shown and described herein are provided to illustrate example implementations. It is noted that the operations are not limited to the ordering shown. It is also noted that various operations described herein may be automated or partially automated. Still other operations may also be implemented.

The method reduces processing overhead by creating the logs in transient memory instead of in the NVM itself and maintaining a consistent snapshot in the NVM. Thus the log entry does not need to be visible on the NVM before the corresponding persistent store. This also allows removal of all instances of a visibility barrier from the user threads and improves the performance of failure free operation dramatically because the user threads can now operate at or near the full execution speed of the underlying program.

It is noted that the examples shown and described are provided for purposes of illustration and are not intended to be limiting. Still other examples are also contemplated.

