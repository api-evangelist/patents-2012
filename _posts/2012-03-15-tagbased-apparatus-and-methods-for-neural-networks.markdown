---

title: Tag-based apparatus and methods for neural networks
abstract: Apparatus and methods for high-level neuromorphic network description (HLND) using tags. The framework may be used to define nodes types, define node-to-node connection types, instantiate node instances for different node types, and/or generate instances of connection types between these nodes. The HLND format may be used to define nodes types, define node-to-node connection types, instantiate node instances for different node types, dynamically identify and/or select network subsets using tags, and/or generate instances of one or more connections between these nodes using such subsets. To facilitate the HLND operation and disambiguation, individual elements of the network (e.g., nodes, extensions, connections, I/O ports) may be assigned at least one unique tag. The tags may be used to identify and/or refer to respective network elements. The HLND kernel may comprises an interface to Elementary Network Description.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08712939&OS=08712939&RS=08712939
owner: Brain Corporation
number: 08712939
owner_city: San Diego
owner_country: US
publication_date: 20120315
---
The present application is a continuation in part of U.S. patent application Ser. No. 13 239 123 filed Sep. 21 2011 entitled ELEMENTARY NETWORK DESCRIPTION FOR NEUROMORPHIC SYSTEMS which is expressly incorporated by reference herein.

This application is related to a co owned U.S. patent application Ser. No. 13 239 163 entitled ELEMENTARY NETWORK DESCRIPTION FOR EFFICIENT IMPLEMENTATION OF EVENT TRIGGERED PLASTICITY RULES IN NEUROMORPHIC SYSTEMS filed on Sep. 21 2011 a co owned U.S. patent application Ser. No. 13 239 155 entitled ELEMENTARY NETWORK DESCRIPTION FOR EFFICIENT MEMORY MANAGEMENT IN NEUROMORPHIC SYSTEMS filed on Sep. 21 2011 and a co owned U.S. patent application Ser. No. 13 239 148 entitled ELEMENTARY NETWORK DESCRIPTION FOR EFFICIENT LINK BETWEEN NEURONAL MODELS NEUROMORPHIC SYSTEMS filed on Sep. 21 2011 each of the foregoing incorporated herein by reference in its entirety.

A portion of the disclosure of this patent document contains material that is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent files or records but otherwise reserves all copyright rights whatsoever.

The file of this patent includes duplicate copies of compact disc CD ROM with forty six 46 read only memory files in ASCII file format. The file details are presented in Table 1 below. These ASCII files contain lines of code which represent exemplary implementations of a Computer Program Listing for this disclosure. This CD ROM and each of the files contained thereon and listed in Table 1 is incorporated herein by reference in its entirety.

Most existing neuronal models and systems include networks of simple units called neurons which interact with each other via connections called synapses. The information processing in such neuronal systems may be carried out in parallel.

There are many specialized software tools that may help neuroscientists simulate models of neural systems. Examples of these tools may include high level implementations such as one or more of NEURON GENESIS NEST BRIAN and or other high level implementations may be designed primarily for use by neuroscientists. Such tools may typically require substantial specialized knowledge may be cumbersome and may require customization in order to achieve efficient performance during simulations that are executed using specific software and hardware engines particularly when real time performance is required as in autonomous robotics applications.

Similarly low level implementations such as one or more of assembly languages low level virtual machine LLVM language Java Bytecode chip specific instruction sets and or other low level implementations may be designed for efficient hardware implementations on x86 ARM and or other silicon chips. However such implementations may be unsuitable for parallel simulations of neuronal systems mostly because the silicon chips are not designed for such parallel neuronal simulations.

Overall existing approaches have substantial shortcomings as they do not provide sufficient flexibility in designing neural networks require expert knowledge and or platform specific customization in order to take advantage of specialized hardware.

Accordingly there is a salient need for a universal high level network description for defining network architectures in a simple and unambiguous way that is both human readable and machine interpretable.

The present invention satisfies the foregoing needs by providing inter alia apparatus and methods for high level network description for neuromorphic systems.

One aspect of the invention relates to a computer realized method of implementing a neural network. In some implementations the network may comprise a plurality of elements. The method may comprise identifying a subset of the plurality of elements. The method may comprise assigning a tag to individual ones of the elements of the subset. The assigning of a given tag may be configured to enable generation of a new network element comprising at least a portion of elements of the subset.

In some implementations the tag may comprise a unique identifier configured to identify one or more individual elements.

In some implementations individual elements may be selected at random from the plurality of elements. Individual elements of the subset may comprise a unit. The tag may comprise a string identifier.

In some implementations the tag may comprise an alphanumeric identifier which may be adapted to identify a spatial coordinate of a respective element of the subset. The subset may comprise a plurality of nodes. The alphanumeric identifier may comprise an identifier of at least one node of the plurality of nodes.

In some implementations the new network element may comprises a connection. The connection may comprise one or more of i a synapse ii a junction and or other features associated with a connection.

Another aspect of the invention relates to a computer realized method of generating a plurality of connections in a neural network. The neural network may comprise a plurality of elements. In some implementations the method may comprise one or more of i executing a first logical expression comprising at least a first tag and a second tag based at least in part on the executing ii identifying a first subset and a second subset of the plurality of elements iii generating a plurality of connections between at least a portion of the first subset and least a portion of the second subset and or other operations. In some implementations one or more individual elements of the first subset may comprise the first tag. Individual elements of the second subset may comprise the second tag.

In some implementations individual elements of the first subset and or the second subset may comprise a node of the network. The method may comprise assigning the first tag to one or more individual elements of the first subset.

In some implementations one or both of the first tag or the second tag may be characterized by a finite lifespan.

Yet another aspect of the invention relates to a method of dynamic partitioning of a computerized neural network. In some implementations the method may comprise one or more of i identifying a subset of elements of the network ii assigning a tag to each element of the subset and or other operations. The identifying and the assigning may cooperate to enable selection of one or more individual elements of the subset using a single selection operation according to some implementations.

In some implementations the network may comprise a plurality of elements. The subset may comprise a plurality of nodes of the plurality of elements.

In some implementations identifying the subset may be based at least in part on executing a Boolean expression comprising one or more keywords including AND NOT OR and or other keywords.

In some implementations the method may comprise one or more of i identifying another subset of elements of the network ii assigning one other tag to each element of the one other subset iii enabling a plurality of connections between at least a portion of elements within of the subset and elements of the one other subset and or other operations.

In some implementations one or more individual connections of the plurality of connections may comprise one of synapse and junction. One or more individual connections of the plurality of connections may be enabled based at least in part on the tag and the other tag.

Still another aspect of the invention relates to a processing apparatus. In some implementations the processing apparatus may comprise a nonvolatile storage medium configured to store a plurality of instructions which when executed may effect dynamic partitioning of a neural network according to a method. The method may comprise one or more of i identifying a subset of elements of the neural network ii assigning a tag to each element of the subset of elements and or other operations. The tag may comprise an identifier configured to identify one or more individual elements. In some implementations assigning the tag may be configured to enable generation of a new network element comprising the subset of elements.

In some implementations the method may be implemented using an Application Specific Integrated Circuit ASIC using ASIC instruction set.

In some implementation the method may comprise executing by the processing apparatus a mathematical expression configured to identify one or more individual elements of the subset. The mathematical expression may comprise a Boolean operation.

In some implementations one or more individual elements of the subset may be selected using a random selection operation.

In some implementations the method may comprise assigning the tag to the new network element. Assigning the tag to the subset may be configured to enable representation of the network as a directed graph.

In some implementation the method may comprise assigning a second tag to the subset. The second tag may be distinct from the tag.

A further aspect of the invention relates to neuronal network logic. In some implementations the neuronal network logic may comprise a series of computer program steps and or instructions executed on a digital processor. In some implementations the logic may comprise hardware logic e.g. embodied in an ASIC or FPGA .

A still further aspect of the invention relates to a computer readable apparatus. In some implementations the apparatus may comprise a storage medium having at least one computer program stored thereon. The program may be configured to when executed implement an artificial neuronal network.

Another aspect of the invention relates to a system. In some implementations the system may comprise an artificial neuronal e.g. spiking network having a plurality of nodes associated therewith a controlled apparatus e.g. robotic or prosthetic apparatus and or other components.

These and other objects features and characteristics of the present disclosure as well as the methods of operation and functions of the related elements of structure and the combination of parts and economies of manufacture will become more apparent upon consideration of the following description and the appended claims with reference to the accompanying drawings all of which form a part of this specification wherein like reference numerals designate corresponding parts in the various figures. It is to be expressly understood however that the drawings are for the purpose of illustration and description only and are not intended as a definition of the limits of the disclosure. As used in the specification and in the claims the singular form of a an and the include plural referents unless the context clearly dictates otherwise.

Implementations of the present disclosure will now be described in detail with reference to the drawings which are provided as illustrative examples so as to enable those skilled in the art to practice the disclosure. Notably the figures and examples below are not meant to limit the scope of the present disclosure to a single implementation but other implementations are possible by way of interchange of or combination with some or all of the described or illustrated elements. Wherever convenient the same reference numbers will be used throughout the drawings to refer to same or similar parts.

Where certain elements of these implementations can be partially or fully implemented using known components only those portions of such known components that are necessary for an understanding of the present disclosure will be described and detailed descriptions of other portions of such known components will be omitted so as not to obscure the disclosure.

In the present specification an implementation showing a singular component should not be considered limiting rather the disclosure is intended to encompass other implementations including a plurality of the same component and vice versa unless explicitly stated otherwise herein.

Further the present disclosure encompasses present and future known equivalents to the components referred to herein by way of illustration.

As used herein the term bus is meant generally to denote all types of interconnection or communication architecture that is used to access the synaptic and neuron memory. The bus may be optical wireless infrared and or another type of communication medium. The exact topology of the bus could be for example standard bus hierarchical bus network on chip address event representation AER connection and or other type of communication topology used for accessing e.g. different memories in pulse based system.

As used herein the terms computer computing device and computerized device may include one or more of personal computers PCs and or minicomputers e.g. desktop laptop and or other PCs mainframe computers workstations servers personal digital assistants PDAs handheld computers embedded computers programmable logic devices personal communicators tablet computers portable navigation aids J2ME equipped devices cellular telephones smart phones personal integrated communication and or entertainment devices and or any other device capable of executing a set of instructions and processing an incoming data signal.

As used herein the term computer program or software may include any sequence of human and or machine cognizable steps which perform a function. Such program may be rendered in a programming language and or environment including one or more of C C C Fortran COBOL MATLAB PASCAL Python assembly language markup languages e.g. HTML SGML XML VoXML object oriented environments e.g. Common Object Request Broker Architecture CORBA Java e.g. J2ME Java Beans Binary Runtime Environment e.g. BREW and or other programming languages and or environments.

As used herein the terms connection link transmission channel delay line wireless may include a causal link between any two or more entities whether physical or logical virtual which may enable information exchange between the entities.

As used herein the term memory may include an integrated circuit and or other storage device adapted for storing digital data. By way of non limiting example memory may include one or more of ROM PROM EEPROM DRAM Mobile DRAM SDRAM DDR 2 SDRAM EDO FPMS RLDRAM SRAM flash memory e.g. NAND NOR memristor memory PSRAM and or other types of memory.

As used herein the terms microprocessor and digital processor are meant generally to include digital processing devices. By way of non limiting example digital processing devices may include one or more of digital signal processors DSPs reduced instruction set computers RISC general purpose CISC processors microprocessors gate arrays e.g. field programmable gate arrays FPGAs PLDs reconfigurable computer fabrics RCFs array processors secure microprocessors application specific integrated circuits ASICs and or other digital processing devices. Such digital processors may be contained on a single unitary IC die or distributed across multiple components.

As used herein the term network interface refers to any signal data and or software interface with a component network and or process. By way of non limiting example a network interface may include one or more of FireWire e.g. FW400 FW800 etc. USB e.g. USB2 Ethernet e.g. 10 100 10 100 1000 Gigabit Ethernet 10 Gig E etc. MoCA Coaxsys e.g. TVnet radio frequency tuner e.g. in band or OOB cable modem etc. Wi Fi 802.11 WiMAX 802.16 PAN e.g. 802.15 cellular e.g. 3G LTE LTE A TD LTE GSM etc. IrDA families and or other network interfaces.

As used herein the term synaptic channel connection link transmission channel delay line and communications channel include a link between any two or more entities whether physical wired or wireless or logical virtual which enables information exchange between the entities and may be characterized by a one or more variables affecting the information exchange.

As used herein the term Wi Fi includes one or more of IEEE Std. 802.11 variants of IEEE Std. 802.11 standards related to IEEE Std. 802.11 e.g. 802.11a b g n s v and or other wireless standards.

As used herein the term wireless means any wireless signal data communication and or other wireless interface. By way of non limiting example a wireless interface may include one or more of Wi Fi Bluetooth 3G 3GPP 3GPP2 HSDPA HSUPA TDMA CDMA e.g. IS 95A WCDMA etc. FHSS DSSS GSM PAN 802.15 WiMAX 802.16 802.20 narrowband FDMA OFDM PCS DCS LTE LTE A TD LTE analog cellular CDPD satellite systems millimeter wave or microwave systems acoustic infrared i.e. IrDA and or other wireless interfaces.

The present disclosure provides among other things a computerized high level network description apparatus and methods that may be configured to define neuronal network architectures in a simple and unambiguous way.

In some implementations a computerized apparatus may be configured to implement a High Level Network Description HLND kernel. The HLND kernel may enable users to define neuromorphic network architectures using a unified and unambiguous representation that is both human readable and machine interpretable.

In some implementations the HLND format may be used to define nodes types node to node connection types instantiate node instances for different node types dynamically identify and or select subsets of the network using tags generate instances of connection between these nodes using such subsets and or other information associated with nodes and or tags.

The HLND format may provide some or all of the flexibility required by computational neuroscientists and may provide a user friendly interface for users with limited experience in modeling neurons.

In some implementations the HLND kernel may comprise an interface to Elementary Network Description END . The END engine may be configured for efficient representation of neuronal systems in hardware independent manner and or may enable seamless translation of HLND model description into hardware instructions for execution by various processing modules.

In some implementations the HLND framework may comprise a graphical user interface GUI configured to enable users to inter alia create nodes select node subsets connect selected subsets using graphical actions via the GUI and or perform other operations consistent with the disclosure. The GUI engine may be configured to generate HLND statements which may correspond to the above user actions without further input from the user. The HLND framework may be configured to convert HLND statements into a graphical representation of the network that is presented via the GUI. The HLND may include one or more components including i the network graphical depiction using the GUI ii the corresponding list of HLND statements and or other components. One or more components of the HLND may be configured to consistently represent the same information about the network as changes in one representation may be consistently applied to the other representation thereby reflecting some or all of the modifications to the network.

In some implementations the HLND may be applicable to arbitrary graph structures e.g. neural network with arbitrarily complex architecture .

Detailed descriptions of the various implementations of the apparatuses and methods of the disclosure are now provided. Although certain aspects of the disclosure can best be understood in the context of the High Level Network Description format used for designing neural network architecture the disclosure is not so limited and implementations of the disclosure may be used for implementing an instruction set that is optimized for efficient representation of other systems e.g. biological or financial in a hardware independent manner.

Implementations of the disclosure may be for example deployed in a hardware and or software implementation of a neuromorphic computer system. In some implementations a robotic system may include a processor embodied in an application specific integrated circuit which may be adapted or configured for use in an embedded application such as a prosthetic device .

According to one or more implementations the exemplary HLND framework may be configured to facilitate design of neural networks such as network of . Some implementations may provide an ability to describe neuronal network of an arbitrary complexity. Some implementations may facilitate use of pre defined node and or pre defined connection types for the network generation process. That is multiple instances of different node types may be generated laid out and or connected using multiple instances of different connection types. Some implementations may provide a flexible definition of new node types such that a new node type may comprise an implementation of an elementary network description END unit class and or an implementation of a Network Object e.g. a layout of nodes a set of connectivity and or a combination of these . The newly defined node types may be used in the network generation process. The END framework is described in U.S. patent application Ser. No. 13 239 123 entitled ELEMENTARY NETWORK DESCRIPTION FOR NEUROMORPHIC SYSTEMS incorporated supra. Some implementations may provide a flexible definition of connection types. A connection type may include an implementation of an END Junction Class an END Synapse Class and or other classes. In one implementation the newly defined connection types may be used in the network generation process. Some implementations may facilitate use of universal tags or labels for some or all of the network elements including nodes connections collections of nodes and or other network elements. In some implementations tags may be used to identify groups of nodes and or connections. Tags may be used to dynamically select parts of the network. One or more Boolean operations such as AND OR NOT and or other Boolean operations may be applied to tags. Some implementations may provide an ability to implement a HLND network using a graphical user interface GUI . Individual description constructs may correspond to a user action in the GUI. In some implementations models of moderate complexity may be built using a HLND GUI interface without requiring the use of a keyboard. In some implementations the HLND GUI may be operated using a touch screen a light pen input device and or other input technique. Some implementations may facilitate presentation of HLND statements for defining network anatomy. Defining network anatomy may include laying out nodes and or connections in a user readable natural English language to facilitate easy understanding for non computer expert network designers. Some implementations may provide an ability to use HLND to generate END instances.

Defining a neural network may comprise defining how many and or what type of nodes to create how to lay out these nodes how to connect these node instances e.g. the network layout of and or other operations. In some implementations the HLND definition method comprises 1 defining new node types and or connection types for the new node types 2 defining a node layout within the network e.g. how many and or what type of nodes to create and how to arrange these nodes within the network that is being created 3 defining how the nodes connect to one another and or other operations. During neural network construction the above steps 1 3 may be individually and or sequentially repeated multiple times. In some implementations the above step 1 may be skipped and pre defined classes defining the desired node type may be used instead in defining the network.

In some implementations a dedicated software package may be configured to i process the HLND statements that define the network and or ii instantiate the network nodes and connections. This processing and or instantiation may be subject to one or more constraints including i only the defined node types and the defined connection types can be instantiated and used in the HLND network definition process ii only the connections between the existing node instances can be instantiated and or other constraints. In other words only connections corresponding to the defined node instances may be used in the HLND process of defining the connections in accordance with one or more implementations. In some implementations the dedicated software package may comprise the END engine which may be configured to generate the END instance of network models as described in a co owned U.S. patent application Ser. No. 13 239 123 entitled ELEMENTARY NETWORK DESCRIPTION FOR NEUROMORPHIC SYSTEMS incorporated supra.

The definition of a node type may provide the implementation instructions of the node which may be configured to instruct a network processing apparatus to perform specific steps during instantiation of the node in accordance with the node type. In some implementations the node definition may further specify internal implementation of the node type for example specify the dynamics of the neuron type . In one or more implementations the node definition may comprise definitions of input ports and or output ports for the node

In some implementations the node type may be defined as a Simple Node where the node definition specifies the internals of that node. Internals of a node may include the implementation of an END unit i.e. a neuron and or an END implementation of a neuronal compartment.

In some implementations the node type may be defined as a complex Network Object which may provide instructions on how to instantiate pre defined node types instructions on how to connect the nodes and or other instructions. Instructions on how to connect nodes may include a HLND description of a network of an arbitrary complexity an algorithm configured to specify details of node and or connection instance generation and or other instructions. It will be appreciated by those skilled in the arts that the term Network Object may be used to describe any network that is implementable using the HLND framework.

Within the HLND framework description individual node types may comprise intra node connections and may define interface or interfaces for incoming connections and or outgoing connections.

In some implementations the END Unit class may be generated using node implementation details e.g. update rule event rule . See e.g. U.S. patent application Ser. No. 13 239 123 for additional detail related to the END Unit classes.

In some implementations the definition of a network object may be configured similarly to the definition of a network with the main difference being that the network object is reusable. That is multiple instances of the network object may be instantiated. Some or all elements of the network object e.g. units tags and or other elements may be scoped that is they may have a finite lifetime associated with a particular scope. In some implementations network objects may be configured to provide an I O interface that may be used to connect the network object with other nodes. A network object may be similar to a building block in a Simulink model see e.g. http www.mathworks.com products simulink index.html a p cell in a computer aided design CAD software a function class in C language code and or other programming elements.

In some implementations a network object may be allowed to use pre defined nodes that are defined externally i.e. outside the scope of this node type . Thus the parent node i.e. the network object and the child node s i.e. the node types used in the network object may not comprise nodes of the same type. In other words the definition of an X type node may not instantiate X type nodes according to some implementations.

Referring now to one exemplary implementation of a network object is illustrated and described in detail. The network object may comprise one or more of a network definition specification of the object input output I O interface and or other information.

The network definition may specify one or more steps of the object instance generation. The network definition may be implemented via a standard HLND network definition. A standard HLND network definition may comprise an instantiation and or layout of nodes that specifies the number of instances of each pre defined node and or their spatial arrangement using pre defined distribution functions. A standard HLND network definition may comprise a connectivity description which may define connectivity between nodes. In some implementations the connectivity description may define and or use spatial projection for the node. By way of non limiting example defining spatial projection for a node may include defining axonal and or dendritic projections for the nodes e.g. i dendritic spread ii distribution of synaptic boutons iii distribution of axon terminals defining how the axon of a node e.g. model neuron connects to the dendrite of another node e.g. model neurons and or defining other information associated with spatial projection for a node.

By way of non limiting example the standard HLND network definition may be used in i defining a specific layout of pre defined nodes ii defining a multi compartmental neuron e.g. a collection of pre defined END units connected with pre defined END junctions iii defining an arbitrarily complex network comprising a plurality of neurons and or defining other information associated with the standard HLND network definition. In some implementations a network may comprise synapses and or junctions.

In some implementations the definition of the network may comprise instance generation with an algorithm which may be configured to describe one or more steps of Network Object instance generation using the above Object definition. By way of non limiting example the algorithm may include one or more of i an algorithm that uses pre defined node types and or defines the instance generation process of such node types ii an arbitrary algorithm that uses pre defined node types and or connection types and or define the instance generation process of such node and or connection types iii an algorithm that defines a dendritic tree and or other algorithms.

The above exemplary algorithms may utilize multiple pre defined END unit types implementing neural compartments and or pre defined END junction types designed to connect such compartments with the algorithm defining the layout of compartments and the connection between them. See e.g. Cuntz H. Forstner F. Borst. A and H usser M. 2010 One Rule to Grow Them All A General Theory of Neuronal Branching and Its Practical Application. 6 8 incorporated herein by reference in its entirety . The I O interface may specify an input output connection implemented for the network object.

Within HLND definition of a connection type may provide the necessary implementation details e.g. pre event rules post event rules update rules and or other details to generate a connection that comprises one or both of i an END synapse or ii an END junction in accordance with one or more implementations.

In some implementations the HLND may define rules that govern node instantiation. The HLND node instantiation instructions may be subsequently provided to a software package e.g. the END kernel that interprets the instructions and instantiates the appropriate nodes. According to some implementations during instantiation some or all node types may be treated equally regardless of whether they are simple nodes e.g. the END implementation of a neuron or network objects e.g. the entire network description . In some implementations the following information may be needed to instantiate and lay out nodes i node type to be instantiated ii number of instances of the node type to be instantiated and or other information.

In a basic form an HLND instantiation statement may create n instances of a given node type using default definitions corresponding to the node type. During instantiation additional parameters may be used to inter alia i set parameters used to initialize the instantiated node types ii set how the instantiated nodes are laid out in space e.g. how position tags are assigned iii add additional tags to the new node instances and or perform other operations associated with HLND instantiation. Within HLND a defined node type available within the scope of operation may be instantiated without restriction.

In some implementations position coordinates i.e. spatial tags may be assigned to the generated node instances during node instantiation. In order to implement this functionality the HLND framework may support usage of predefined distribution functions that assign spatial tags to each instantiated node. Such a distribution function may be configured to sample n points from a given probability density function. By way of non limiting example the HLND statement 

may describe sampling of n points that are uniformly distributed within a space range defined by the boundary parameters argument. Similarly the HLND statement 

may describe sampling of n normally distributed points within a space range specified by the boundary parameter.

In addition to the unique id tags that individual generated nodes may have extra tags may be optionally assigned during the instantiation process and may be used to identify the set of newly instantiated nodes. In some implementations special reserved tags e.g. IN OUT or other special reserved tags may be used to specify that the generated units are input or output interfaces for the network therefore making these nodes accessible readable and or writable from outside. Exemplary HLND calls are shown in the Listing 1 below 

In some implementations morphology extensions may be used during connection instantiation. The morphology definition described with respect to supra specifies how an instantiated node may project and or extend into network space. That is the morphology may define spatial extent where an instantiated node is allowed i to receive incoming connections from and or ii to send outgoing connections to. Note that according to some implementations an addition of a node extension may not alter the size and or the location of the node. Instead extensions may enable a node to search for other nodes during instantiation of node interconnections. In other words an extension may provide an additional view of a node which may be used during the process of connecting nodes.

In some implementations connecting nodes using spatial tags may only be allowed if the nodes are overlapping. Nodes may have a zero extension by default. In some implementations only co located nodes may be connected. In order to extend node connectivity non zero node input dendrite and node output axon projections may be defined. In some implementations these projections may be used when connecting any two nodes. For example the output projection of one node may be connected to the input projection of another node.

To create an extension some or all of the following information may be required in accordance with one or more implementations 1 source tags used to identify the nodes for which the extension is created 2 extension tags that are used to identify the extensions to be created and or 3 define I O point distribution to define the space extension where the node can receive incoming connections and define the space where the nodes can have outgoing connections.

For incoming extensions the distribution of receiving terminals may be specified. The distribution of receiving terminals may be analogous to dendritic spread and or distribution of synaptic boutons in case of a neuron. For an outgoing extension the projection spread may be specified. The projection spread may be analogous to axon terminals.

For the distribution of receiving terminals and spread of projections HLND may support predefined functions e.g. bounded Gaussian and or uniform distribution . In general arbitrary density functions may be used.

The HLND connection statement may comprise instructions configured to instantiate connections of a given connection type from one set of nodes to another set of nodes. In some implementations some or all of the following information may be required in order to enable these node to node connections 1 from subset 2 to subset and or 3 connection type . A from subset may include a node subset selected by tags that uniquely identify source nodes extensions e.g. nodes extensions that a connection will originate from . A to subset may include a node subset selected by tags that uniquely identify destination nodes extensions e.g. nodes extensions that a connection will terminate at . A connection type may include the connection type used to connect nodes extensions to nodes extensions.

In some implementations the HLND connection statement may direct connection instantiation with a given connection type from all available nodes to all available nodes. According to some implementations the connection parameters may be used to filter out connections. That is filter constrains may be applied to the some or all possible to connections. Therefore a subset of possible to connections may be instantiated which may allow instantiation of arbitrary connection maps from to . In some implementations connections may be expressed as function calls. In some implementations connections may be expressed using a table. In some implementations a connection filter may be configured to generate all to all connections where all of the are connected to all of the .

As an aside two notation formats and may be used in various implementations as both notations may cause the HLND to generate connections for a subset. For example notation may describe a collection of nodes e.g. Collection that have both tags FromTag and FromTag . Accordingly notation may be used instead while producing the same results.

In some implementations the HLND connections statement may be configured to enable parameterized connection establishment such that parameters may be passed to the connection type to set the connection variables in the connection instances. In some implementations the connection parameters may be used to set the weights of the synaptic node connection. In some implementations the node information e.g. the position of the from node and the to node may be used to set up connection weights based on the distance between the nodes.

By way of non limiting example one or more of the following connection statements may be employed 1 connect each node to N node 2 connect to each node N node and or 3 randomly sample N from all possible connections.

In some implementations nodes may comprise position tags and or may have zero default extension. Such nodes may connect to co located nodes. Connecting nodes with spatial tags may require overlap so that overlapping nodes may be connected.

Referring now to an exemplary implementation of an HLND framework node interconnection is shown and described in detail. The network of may comprise a group of A nodes and a group of B nodes . For clarity the network may be configured using a one dimensional configuration so that nodes with matching X coordinates i.e. the node index i 1 7 in are allowed to be connected via the connections . Specifically a node a i from the node group may be allowed to be connected to the respective node to b j of the node group when i j such as for example the nodes respectively in . Several possible connections are illustrated by solid lines .

In some implementations such as illustrated in node extensions may be added to the nodes of the node groups respectively in order to implement complex connections and to enable more flexible connection mapping. The node extensions may be used to inter alia map spatial coordinates of the source nodes e.g. the nodes A of the group in to the receiving nodes e.g. the node in . The extensions may be used to define the probability density function of potential connections such as in the exemplary implementation illustrated in .

The network configuration illustrated in may allow connections between the node a i of the A group to the b  node when the axons overlap with the spatial dimension of the dendrite . As illustrated in the nodes a  a  may be connected to the node b  via the connections depicted by solid arrows in . Inactive e.g. not allowed connections between other A nodes are depicted by dashed arrows in . The tags and may refer to another view of the node.

As illustrated in the following extension may be constructed i a uniform circle extension denoted as the Dendrite to the B node in ii a uniform circle extension denoted as the Axon to all A nodes in and or iii Connect to . The A to B node connection may be possible and may be instantiated because both the sending and receiving extensions may be uniform. In this example the connect statement is looking for extension overlaps that is whether the extension of a node overlaps with the extension of a node .

Within the HLND framework nodes may comprise different views such as for example Axons or Dendrites. Node tags axon or dendrite may be used in HLND to refer to the same node. The Axon Dendrite may have different spatial properties.

The connectivity profile of the non uniform extension see may be configured using a shape of the Gaussian distribution and or other distributions. The extension may be centered at the node . The extension may be characterized by a certain variance and a radius .

Node connections via non uniform extension are illustrated in . The network configuration illustrated in may allow connections between the node a i of the A group to the b node when the axons overlap with the spatial dimension of the non uniform dendrite . Possible connections in the network include a  a  . . . a  axon to b dendrite as a  axon a  axon and a  axon do not overlap with the dendrite spatial dimension .

The non uniform extensions e.g. the extension in may bias the selection of connections towards axons that overlap with the highest probability area of the non uniform dendrite. While the extension dimension overlap may be used to identify all of the possible connections the sampling of the possible connections may follow the connectivity profile probability of the extensions. When selecting a subset of the possible connections the connectivity profile shape of the extension that describes connection likelihood may need to be taken into account. By way of non limiting example when connecting a single arbitrary to the see the most likely outcome may be a connection between the node a  and the node rather than connections between nodes a  a  to the node .

In some implementations the HLND exemplary operation sequence for connecting node populations using non uniform extension may be 1 add Gaussian extensions centered at the node with a fixed radius r to B nodes and tag these extensions as 2 add uniform extensions with a fixed radius rto A nodes and tag these extension as and 3 connect N random s to s. The possible connections may be where the extensions of nodes overlap with the extensions of nodes. Instantiated connections may correspond to the highest connectivity. In some implementations the highest connectivity may be determined based on a product of the Gaussian and the uniform functions.

In some implementations of the HLND framework network objects may comprise one or more members and an Input output I O interface. The I O interface may specify how to interface e.g. establish connections with other elements of the network. In some implementations the members of the network object may comprise nodes. In some implementations the members of the network object may comprise nodes and connections. The I O interface may define how the object members are accessible from outside the scope of the network object. During definition individual members of the network object and their values may be declared as public or private. Private members may not be visible i.e. directly not accessible to external network elements outside the network object. The private object members may be accessible via the I O interface defined for that member. That is private members of a network object may not be visible from outside the scope of the network object. In this case the I O interface may be required to implement connections.

In some implementations the network objects may be defined as open . Members of a network object defined as open may be public and visible from outside the scope of that network object. This may alleviate a requirement to publish the I O interface.

Public members of the network object may be visible and or accessible by external elements for input and or output connections. In some implementations members of the network objects may be scoped by default. That is some or all variables within the network object may be scoped inside the network object. Members of multiple instances of the same network object type may not interfere when these members use the same tags.

In some implementations a network object may be defined as a macro . A network object defined as a macro may not be treated as scoped object. Such macro definition may allow some or all of the variables defined within the macro object to be accessible and or visible by external elements.

By way of non limiting example node a may be a public member of network object NO node b that is not a member of NO may connect directly to the node a and or receive connections from the node b. A member node a in NO may be accessed by using the tags and and or with NO.node a scoped notation. A node c that is a private member of network object NO may not be visible and or accessible from outside unless the I O interface is defined for the node c member of the NO according to some implementations. An external node that is not member of the NO may not connect to and or receive a connection from the node a member directly. In other words a public member of a network object may be accessed using tags and or other publicly available information.

An exemplary implementation of the HLND network object illustrating a public multi compartment neuron MCN is shown and described in connection with . The MCN neuron may comprise one or more public nodes which may include dendritic compartments and soma compartment . The term public may refer to members of the network object e.g. the compartments that are visible outside the scope of their definitions e.g. outside the MCN . Individual compartments may be assigned two tags which may include DENDRITE COMP and or other tags. Compartment may be assigned three tags which may include DENDRITE COMP SOMA and or other tags. The compartment and individual compartments may be connected via junction connections .

By way of non limiting example to illustrate functionality of public network elements in accordance with some implementations two instances of public MCN neuron may be considered. One instance may be tagged as the neuron a and another instance may be tagged as the neuron b . The notation collection may refer to the member of MCN with the SOMA Tag in the neuron a instance. The collection may refer to the member of MCN with the SOMA Tag in the neuron b instance. As some or all members of the MCN may be public they may be visible to outside entities e.g. the MCN which may enable direct connection of the to the .

As will be appreciated by those skilled in the arts the above notation is exemplary and various other notations may be used in order to identify select and or access node members using their tags.

An exemplary pseudo code corresponding to the implementation illustrated in is presented in . The statements and in may be configured to generate the node instances and of respectively. The statements and in may be configured to define the out projections and or the in projections of respectively. The last statement may be configured to define the connection .

By way of non limiting illustration two instances of the private MCN type may be considered. One instance may be tagged as neuron a and another instance may be tagged as neuron b . As the MCN type is defined as private the MCN members e.g. the collections may be invisible from outside the MCN . Directly connecting the to the may not be permitted for the node configuration type illustrated in in accordance with some implementations. In order to enable neuron external connectivity the MCN definition may comprise input ports and and the output port may be used to specify the I O interface for the MCN node type. The input interface of the MCN may comprise direct internal connections to the private members of the MCN . Direct internal connections to the private members of the MCN may include the connections and from the input interface IN to the private members with tags DENDRITE and COMP . Direct internal connections to the private members of the MCN may include the connection from the input interface IN to the private member with the tag SOMA . The link may connect the private member to the Output interface which may allow the node to be used for outgoing connections and or for incoming connections. The neuron a.OUT may be configured to be linked connected to the neuron b.IN. The neuron b.OUT may be configured to be linked connected to the neuron a.IN.

Conversely while individual ones of the members of the node instance may be kept private an input interface may be used to specify how the node members connect to the input port according to some implementations. Although a single port is illustrated in the implementation of this is not intended to be limiting as multiple uniquely tagged input output ports may be used in some implementations.

As the output interface of the node instance and the input interface of the node instance may be exposed and or visible to external network elements a link connection may be established between the node instances of network objects and by using in and out interfaces and in a manner that is similar to the connection establishment described with respect to supra.

In some implementations a third network object may create the instances inst a and inst b of types A and B respectively and or may connect the inst a instance to the inst b instance using the a out port of the node instance and the in port of the node instance . The exemplary HLND definition steps shown in may include 1 creating an instance of A inst a 2 creating an instance of B inst b and or connecting and or linging inst a to inst b.

As the members of the node instances A and B may be private the object C may not directly connect the members of the instance A to the members of the instance B. Instead the object C may use the exposed ports inst a.a out to the inst b.in. In the connection declaration may use the equal sign notation to denote that the inst bin is assigned to e.g. is the same as the inst a.a out. In some implementations the HLND compiler may use the definition and a definition of the private node B members to resolve the connection between the virtual b.in port to the actual members of the node B by linking the inst b.in with the corresponding member s of the node instance A that is indirectly establishing the connection between the inst a of A and the inst b of B . As illustrated in the node type and may specify projection extensions and or synapse connection types that may e required to effect the node to node connection.

In accordance with some implementations individual elements of the network i.e. nodes extensions connections I O ports may be assigned at least one unique tag to facilitate the HLND operation and disambiguation. The tags may be used to identify and or refer to the respective network elements e.g. a subset of nodes of the network that is within a specified area .

In some implementations tags may be used to form a dynamic grouping of the nodes so that these dynamically created node groups may be connected with one another. That is a node group tag may be used to identify a subset of nodes and or to create new connections within the network as described in detail below in connection with . These additional tags may not create new instances of network elements but may add tags to existing instances so that the additional tags are used to identify the tagged instances.

Using the tag MyNodes a node collection may be selected. The node collection may comprise individual ones of nodes and or see e.g. . The node collection may represent nodes tagged as . The node collection may comprise individual ones of the nodes . The node collection may represent the nodes tagged as Subset . The node collection may comprise individual ones of the nodes see e.g. .

In some implementations the HLND framework may use two types of tags which may include string tags numeric tags and or other tags. In some implementations the nodes may comprise arbitrary user defined tags. Numeric tags may include numeric identifier ID tags spatial tags and or other tags.

Upon instantiating a node the instantiated node may have a string tag the node type and a unique numerical tag the unique numerical identifier . In some implementations position tags may be assigned during the instantiation process.

As shown in the tags may be used to identify a subset of the network. To implement this functionality one or more Boolean operations may be used on tags. In some implementations mathematical logical operations may be used with numerical tags. The . . . notation may identify a subset of the network where the string encapsulated by the chevrons may define operations configured to identify and or select the subset. By way of non limiting illustration may select individual nodes from the network that have the tag MyTag may select individual members from the network that have both the MyTag and the MyTag string tags may select individual members from the network that has the MyTag or the MyTag string tags may select individual members from the network that have the string tag MyTag but do not have the string tag MyTag and 

In some implementations the HLND framework may comprise hierarchical tag inheritance. In some implementations individual members that are instantiated within a network object may inherit string tags of its parent. For example individual members of a network object with tags ParentTag and ParentTag may comprise the tags ParentTag and ParentTag in addition to the member specific tags assigned for example during member instantiation.

In some implementations member spatial tag data may refer to local coordinates referenced relative to the space defined by the network object of the member. In some implementations global coordinates referenced relative to the space of the entire network may be inferred from the nested structure of network objects and or members.

HLND tag properties and or characteristics according to one or more implementations may be summarized as tag types may include string tags and numerical tags numerical tags may comprise the numerical identifier Boolean operations may be used on tags math functions may be allowed on numerical tags optional spatial and string tags may be assigned individual node instances may comprise a unique numerical identifier tag string tag inheritance may be hierarchical spatial tags may refer to local coordinates global tag coordinates may be inferred from the nested structure of nodes and or other properties and or characteristics.

In some implementations the HLND framework implementation of tags may be configured to require the following functionality i an interface to tag data generator and data handler and ii implementation of nested objects so as to enable creation of complex network objects from any number of existing network objects.

In some implementations the tag data handler may be implemented using a database such as e.g. MySQL. Instances of network objects may be generated using arbitrary string tags. In some implementations the network objects may be generated using position tags as well as the string tags. Tag data may be placed into a database. Additional network data e.g. instances of connections such as junctions synapses etc. may be generated. The instantiation of connections may depend on position tags and or query results. The new data may be stored in the database.

Tag implementation configuration may enable partitioning of the network software application into two parts which may include a data generation block a data storage block and or other parts. The data generation block e.g. implemented in c may be configured to generate data based on its own intelligence and or by interacting with the database e.g. MySQL . In some implementations the data generator functionality may be embedded within the database server. The data generator may be implemented using server side procedures activated by triggers. Such triggers may include insert and connect call trigger procedures stored on the database server.

In some implementations instantiating an END synapse junction may require information such as one or more of class and ID of pre synaptic unit class and id of post synaptic unit spatial position of pre unit and post unit and spatial projection of pre unit.out and post unit.in. and or other information.

A synapse junction instance may be generated. In some implementations additional external parameters may be used for instantiation of an END synapse junction. Examples of the external parameters may include synaptic weights synaptic delays and or other external parameters. The use and functionality of synaptic weights and or delays in node to node connections is described in further detail in a co owned U.S. patent application Ser. No. 13 152 105 filed on Jun. 2 2011 entitled APPARATUS AND METHODS FOR TEMPORALLY PROXIMATE OBJECT RECOGNITION and or in a co owned U.S. patent application Ser. No. 13 152 084 filed on Jun. 2 2011 entitled APPARATUS AND METHODS FOR PULSE CODE INVARIANT OBJECT RECOGNITION each of the foregoing incorporated herein by reference in its entirety.

To connect different network objects that use different spatial coordinates the coordinate system used for each network object may be published. That is the coordinate system configuration may be available to individual nodes within a certain scope.

Within the HLND framework connections between network objects may be established in one or more ways. In some implementations the connection may be established based on an overlap between the axon terminal distribution and the synaptic bouton distribution. In some implementations the overall connection map may be obtained using a joint probability distribution function PDF of the axon terminal distribution and the synaptic bouton distributions. The joint PDF may be used to establish the required connections synapses . In some implementations the HLND framework may be configured to distribute individual ones of the potential connection points. Connection points may be subject to one or more particular condition such as spatial coordinate and or other conditions. The HLND connection algorithm may be configured to select all or a subset of these connection points. The HLND connection algorithm may be configured to instantiate the corresponding connections. In some implementations the HLND may be configured to generate arbitrary user defined connection sets. According to some implementations the HLND may be configured to generate all to all connections.

In some implementations the HLND may implemented entirely using SQL. According to some implementations such SQL implementation may be effected using MySQL database and stored functions procedures. The HLND statements may be constructed according to the English language grammar.

As described above individual network elements defined within the HLND regardless whether it is a node unit synapse or junction or just an empty placeholder may comprise a tag. This property of the HLND network description may allow tagged elements to inter alia be addressed and manipulated as a group. In some implementations spatial coordinates may be implemented using tags in the form x y z .

By way of non limiting example a network unit may comprise one or more tags including the unit ID numerical identifier QIF soma pyramidal layer V spatial coordinates tag 0.1 0.3 0.5 and or other tags. A synapse may have tags such as UD pre neuron post neuron and or other tags denoting a pre synaptic and a post synaptic node IDs respectively apical exc glu and a spatial coordinates tag 0.1 0.3 0.4 .

In some implementations storing tags in a database may allow fast access to groups of elements. Individual database query statements that operate on tags may act as a tag filter or a search statement that selects particular elements that match the query terms from the database. For example specifying in the query may result in a selection of individual elements that comprise V in any of its tags e.g. the entire V subset. Specifying 

Some implementations may allow addressing of the elements that satisfy the tag filter without copying and pasting the filter statement.

According to some implementations a network comprising 800 excitory exc and 200 inhibitory inh neurons may be split into two equal sub networks subnetwork and subnetwork each comprising 400 exc and 100 inh randomly selected neurons as follows 

In some implementations the PUT operator may be used to create unit instances by using a filter parameter as follows 

In some implementations the synapse class may be replaced by the junction class in the CONNECT statement so that the synaptic junctions may be generated. The construction function synapse class may have access to individual tags of pre synaptic and or post synaptic elements. The construction function synapse class may determine the delay and or other relevant parameters needed.

The statements in Listing 15 may use randomly selected subsets. This may be implemented by a random walk through the list of all pre tagged units and random selection of elements of the subset.

The first statement in Listing 16 may be configured to instantiate synapses comprising a random subset of the fully connected matrix of pre to post synapses. The term fully connected matrix may be used to describe a network configuration where all of the pre synaptic units are connected to all of the pus synaptic units. Unlike the examples shown in Listing 15 the first statement in the Listing 16 does not guarantee that all pre synaptic units or all post synaptic units comprise the same number of synapses.

The second and or the third statement in Listing 16 may be configured to generate synaptic connections that are based on the coordinates of pre synaptic units and the post synaptic. The second statement may comprise a loop configured to connect each pre synaptic unit to the nearest post synaptic unit that satisfies the tag mask. The third statement may loop through each post tag and finds the nearest pre tag.

In some implementations the parameter NEAREST 1 OF may be used in place of the parameter NEAREST in the statements of Listing 16.

In some implementations individual pre synaptic units may be connected to n nearest post synaptic units using the statement 

In some implementations a set of units may be tagged with cones . A set of random coordinates may be tagged with retina . Random retinal coordinates may be assigned to the cones using the expression 

When the number of cones is greater than the number of coordinates then multiple cones may be assigned the same coordinates.

The ON operator may be used to return a sample of n points from a probability density function defined by the parameter pdf as 

The operator PER may be used to iterate through a list of tags specified e.g. by a tag filter. For individual elements of the list the operator PER may call the statement passing to it all the tags of the list element. The format of the PER operator may be 

The PER operator may return a table comprising data describing generated network elements. In some implementations the PER operator may be used to create multiple synapses per neuron. In some implementations the PER operator may be used to create multiple neurons per location. In some implementations the PER operator may be used to create multiple cortical columns per cortical surface.

In some implementations which may be applicable to SPNET the unit classes exc and inh and the synaptic classes glu and gaba may be defined within the SPNET definitions.

The first line of Listing 21 may be configured to generate 800 units of exc type. The second line of Listing 21 may be configured to generate 200 units of inh type. The third line of Listing 21 may be configured to connect individual units with tag exc note that the class type is automatically can be used as a tag to 100 randomly selected units with tags exc or inh with connection type glu. The fourth line of listing 21 may be configured to connect individual units with tag inh to 100 randomly selected units with tag exc with connection type gaba.

In some implementations the HLND description may be used to describe retinal pixel to cone mapping. Generally speaking the cone cells or cones may be or refer to photoreceptor cells in the retina of the eye that are responsible for color vision. Cone cells may be densely packed in the fovea but gradually become sparser towards the periphery of the retina. Below several examples are provided that describe various aspects of the retinal mapping.

In some implementations assigning of the tag network subsets may be configured to enable representation of the network as a directed graph. A directed graph or digraph may comprise a pair G V A of elements where a set V whose elements may be called vertices or nodes and a set A of ordered pairs of vertices called arcs directed edges or arrows. In HLND the term node may be used for vertices and connections are the edges.

In some implementations the HLND may comprise a Graphical User Interface GUI . The GUI may be configured to translate user actions e.g. commands selections etc. into HLND statements using appropriate syntax. The GUI may be configured to update the GUI to display changes of the network in response to the HLND statements. The GUI may provide a one to one mapping between the user actions in the GUI and the HLND statements. Such functionality may enable users to design a network in a visual manner by inter alia displaying the HLND statements created in response to user actions. The GUI may reflect the HLND statements entered for example using a text editor module of the GUI into graphical representation of the network.

This one to one mapping may allow the same or similar information to be unambiguously represented in multiple formats e.g. the GUI and the HLND statement as different formats are consistently updated to reflect changes in the network design. This development approach may be referred to as round trip engineering .

In some implementations the GUI may support user actions including creating nodes selecting one or more subsets of the network connecting nodes tagging nodes within a selected subset and or other user actions. In some implementations the GUI may support defining of network objects. Some exemplary user actions are described in detail below.

Referring now to node creation using GUI is illustrated in accordance with one or more implementations. In some implementations creating nodes of the neural network may require information including a type of node to be instantiated and or generated a number of nodes to be created and or other information. In some implementations users may provide information including list TAGs to be assigned to the crated nodes additional parameters for instantiating and or initializing nodes and or other information. Additional parameters for instantiating and or initializing nodes may depend on a specific network implementation such as for example instructions on how to lay out the nodes to be instantiated that is how to assign numerical spatial tags.

The above GUI Node Creation functionality may be supported by one or more appropriate instructions of the HLND kernel that implement node generation. See for example Listing 10 supra for more detail. When a user enters the HLND node generation instruction statement the GUI may generate a graphical representation e.g. a unique symbol pictogram and or an icon in the graphical editor corresponding to the respective node or nodes . Entry of HLND statements by users may be effected by various means including but not limited to text entry speech soft keys icons and or other means configured for HLND statement entry.

A user may employ the GUI of to perform node creation. According to some implementations a user may select a node type from available list of node types the list in drag and drop as illustrated via the arrow in the selected node type e.g. the type in into an editor panel where the node is represented with a unique node symbol provide additional parameters if needed in via a supplementary entry means such as for example a pop up menu in that is associated with the specific node type and or perform other actions to create a node.

The search box may allow the user to filter the list of displayed node types and using one or more keywords. This may facilitate node type selection where there are large number of node types available. The pop up menu may enable the user to graphically specify a number of nodes parameters for the node instantiation layout process additional tags and or other information associated with node creation.

The GUI may allow the user to switch back and forth between the text editor HLND statement and the GUI node creation. By way of non limiting example selecting different parameters option for the layout of the nodes in the GUI may update the HLND statement. Changing the additional TAGs assigned to the nodes created in the HLND statement may update this information in the GUI.

The GUI interface shown in is not intended to be limiting as other implementations are contemplated and within the scope of the disclosure. For example in some implementations the GUI may include pull down lists radio buttons and or other elements.

Referring now to A and B different exemplary implementations of node subset selection are shown and described in detail. The GUI of may comprise a network layout panel two or more selection description panels and and or other components. The network shown in the panel may comprise a plurality of nodes with different tags and depicted as respectively. The selection description panels and may comprise a Boolean portion of HLND statements corresponding to respective subset.

In some implementations Select Network Subset user action may correspond to selecting members of the network using the GUI editor e.g. the GUI of . A user may select a subset of the network for example by using a mouse or other pointing device such as trackball fingers on a touch pad device like and iPad from Apple light pen and or other technology . The subset selection using GUI may be achieved via a selective clicking tapping graphical symbols corresponding to the desired members of the network click and drag to select an area of the network a combination thereof and or other actions to select a subset. The subset selection action of the GUI may be supported by the respective instructions of the HLND kernel that implement subset selection. See for example listings 6 7 supra for more detail.

As shown in the node subset may comprise nodes comprising the tag while the subset may comprise nodes having both tags and . Once the subsets and are selected the Boolean expressions in the panels and may be updated accordingly.

In some implementations the network shown in the GUI of comprises two subsets including the subset comprising tags and the subset comprising the tag and or other subsets. The selection description panel may be updated to reflect the Boolean expression corresponding to the tag content of the subset . In some implementations additional subsets may be generated by forming a subset comprising an intersect between the subsets and as indicated by the Boolean expression .

By way of non limiting example responsive to a user entering the Boolean expression for the subset selection statement e.g. the expression in the GUI may display for example using a shaded rectangle in the corresponding selected members of the subset in the graphical editor. In some implementations the GUI may generate a graphical representation in the graphical editor corresponding to the subset or subsets selection as illustrated with respect to below. Examples of graphical representations may include one or more of a unique symbol pictogram an icon and or other graphical representations. In some implementations a graphical representation may include a change in a graphical attribute including a change in one or more of color shading pattern and or other graphical attribute.

In some implementations the GUI user actions may be represented using unique symbols and as illustrated in network shown in . The unique symbols and may represent subsets and respectively and may be subset specific. By way of non limiting example the color and or other identifying quality of the unique symbol may be configured in accordance with the tags that are used to identify the subset. The shape and position of the symbol in the graphical editor panel may be configured according with the spatial tags of the members of the subset. This may be illustrated by symbols and which may depict subsets of the network illustrated GUI implementations and . In some implementations the symbol may be configured based on the element type. In some implementations the symbol choice may depend on whether the subset comprises i only nodes ii only connections or iii nodes and connections.

In some implementations the same network configuration e.g. the subset of may be represented within the GUI graphical panel e.g. the panel of using different symbols icons. In some implementations which may correspond to a low level detail network view corresponding to for example no or low zoom the subset may be represented using a symbol e.g. the symbol of without showing individual elements of the subset.

In some implementations which may correspond for example to a network with limited processing capability or a network configured for batch updates the subset may be represented using a symbol without showing individual elements of the subset.

In some implementations which may be associated with a high level detail network view corresponding to for example to high zoom level and or when processing resources are available to process information related to individual element of the subset the subset may be represented using graphical depiction providing further detail of the subset e.g. the representation of that illustrates individual subset elements at their appropriate locations.

In some implementations the HLND framework selection operation may be performed to assign additional tags to the selected members to use the selected members nodes in this case in a connection statement and or to perform other actions.

The GUI may allow the user to switch back and forth between the text editor HLND statement and the GUI subset selection. By way of non limiting example selecting different node members using the GUI may cause updates of respective the HLND statements. Changing the selection in the text editor may update the selection in the graphical editor. In some implementations updating the selection may include highlighting and or otherwise visually emphasizing the selected members.

The node connection user action may correspond to creating connections between network nodes. According to some implementations in creating inter node connections the HLND kernel may require one or more of a first subset selection e.g. a subset of nodes the connections will originate from a second subset selection e.g. a subset of nodes the connections will terminate at a connection type used to connect the first subset to the second subset and or other information associated with connecting nodes.

In some implementations one or more additional parameters may be provided to the HLND kernel including one or more of parameters for setting the connectivity mapping e.g. an all to all one to one one to nearest and or other user defined connection mappings parameters for instantiating and or initializing connection instances e.g. initializing the synaptic weights list TAGs to be assigned to the created connection instances and or other parameters.

The HLND kernel may implement instructions configured to connect nodes in the network. See for example listings 16 17 supra for more detail . By way of non limiting example when a user enters the connection instruction the GUI may create a corresponding graphical representation e.g. draws a link arrow from source to destination selection of nodes in the graphical editor to illustrate the connections.

In accordance with one or more implementations the user may use the GUI to select a source subset selection of network members select a destination selection of network members drag and drop the source selection onto the destination selection to connect first selection to the second selection and or perform other actions. The GUI may generate link arrow elements in the graphical view representing the respective connections between the source and the target members.

In some implementations a pop up menu associated with the connection elements link arrow may allow users to select a connection type from the available list of connection types. In some implementations the pop up menu may allow users to provide additional parameters for instantiating and or initializing connection instances. In some implementations the pop up menu may allow users to set parameters for setting the connectivity mapping.

The GUI may allow the user to switch back and forth between the text editor HLND statement and the GUI connection creation. By way of non limiting example selecting different node members using the GUI may cause updates of the HLND statement that are associated with the node description. Changing the selection in the text editor may update the selection e.g. highlights the selected members in the graphical editor.

According to some implementations additional tags may be assigned to the collections and by invoking for example right click on a selection to assign new tags for the selected members of the network. The HDLN statement for the tagging may be automatically generated.

A supplementary graphical data entry means e.g. the pop up menu in may be invoked in some implementations using e.g. a right click as illustrated by the line . The menu may be used to inter alia assign additional tags or new tags to the selected nodes.

In some implementations by using drag and drop action using a mouse and or a finger on a touch pad device illustrated by the arrow in in the GUI field the first selection may be dropped onto the second selection which may instruct the HLND engine to create connections between the nodes of the collection and the nodes and of the collection .

Additional supplementary graphical data entry means e.g. the pop up menu in may be used to inter alia specify parameters for the connections. Specifying parameters for the connections may include one or more of setting connection type initializing parameters for the connection type specifying connectivity pattern assigning tags to the connections and or other actions.

For one or more user actions performed with the node selections and using the GUI corresponding HLND statements may be automatically generated and displayed in the statement fields and respectively.

The GUI interfaces shown in are not intended to be limiting as other implementations are contemplated with in the scope of the disclosure. For example some implementations may include pull down lists radio buttons and or other components.

The HLND format may be designed to be compatible and or to be used in conjunction with the Elementary Network Description END format which is described in the U.S. patent application Ser. No. 13 239 123 entitled ELEMENTARY NETWORK DESCRIPTION FOR NEUROMORPHIC SYSTEMS filed on Sep. 21 2011 incorporated supra. In some implementations instances of END units may be generated based on a HLND description e.g. END implementations of model neurons . Instances of END junctions and or END synapses may directionally connect the units. The HLND may define the anatomy while the neural and synaptic dynamics may be defined in the applied END classes. HLND may hide complexity and or low level difficulties of END and may make network design a simple process.

The generated END instances may be used to generate a neural network engine that implements and or runs the specified model. That is END instances may be used to generate an engine that implements the network defined by the HLND description and or the applied END classes. The engine may be executed on arbitrary hardware platform from PC FPGA any specialized END compatible hardware and or other computer hardware.

In some implementations individual basic structures e.g. unit doublet and or triplet may be implemented as a single thread on a multi thread processor. In some implementations individual structures may be implemented as a super unit super doublet and or super triplet which may comprise dedicated circuits configured to processing units doublets and or triplets respectively using time multiplexing. Some implementations may include three different circuits one for each of units doublets and triplets.

In some implementations unit may represent a neuron and or a part of a neuron e.g. a dendritic compartment . In another example unit may represent a population of neurons. The activity of the neuron may represent a mean firing rate activity of the population and or some other mean field approximation of the activity of the population. Individual units may be associated with memory variables and an update rule that describes what operations may be performed on its memory. The operations may be clock based i.e. executed every time step of the simulation or they may be event based i.e. executed when certain events are triggered .

Depending on the values of the unit variables the units may generate events e.g. pulses or spikes that trigger synaptic events in other units via doublets. For example a unit in may influence unit via the doublet which may represent a synapse from pre synaptic neuron pre synaptic unit to post synaptic neuron post synaptic unit .

Individual units may have after event update rules which may be triggered after the event is triggered. These rules may be responsible for modification of unit variables that are due to the events e.g. the after spike resetting of voltage variables.

Individual doublets may be associated with memory variables. Individual doublets may access variables of the post synaptic unit. Such access may include read write and or access mechanisms. Individual doublets may be associated with a doublet event rule that makes a change to the doublet memory to implement synaptic plasticity. Individual doublets may be associated with a doublet event rule that makes a change to the post synaptic unit memory to implement delivery of pulses. A doublet event rule may encompass some or all the synaptic rules described in the END formats above.

Because multiple doublets e.g. in may connect corresponding multiple pre synaptic units to a single post synaptic unit the doublets may modify the post synaptic unit memory in parallel and or in arbitrary order. The result may be order independent. This may be achieved when the operation on the post synaptic unit memory is atomic addition as in GPUs atomic multiplication which is equivalent to addition via logarithmic transformation and or resetting to a value with all the doublets trying to reset to the same value . The post synaptic unit variable that is being modified by the doublet event rule may not be used in the rule. Otherwise the result may depend on the order of execution of doublet event rules.

Referring now to an exemplary implementation of a neural network definition system comprising HLND kernel and END description is shown and described in detail. In the circles and may represent different higher level network description methods or formats. The circle may represent the END description of the network. Arrows from and may denote the process of conversion to the END description. For example a software that processes the HLND description of a network e.g. the HLND statements may generate the END description of the same network. The rectangles and in may denote various hardware platform implementations of the network defined by the END description . The arrows between the circle and the rectangles and may denote the engine generation process. By way of non limiting example the arrow between the END description and the rectangle may represent the process of generating an executable that implements the END network and is configured to runs on a CPU. The HLND definition may be processed and converted into the END description . The END description may be configured to be processed e.g. by various software applications to generate platform specific machine executable instructions. The platform specific machine executable instructions may be configured to be executed on a variety of hardware platforms including but not limited to elements general purpose processor graphics processing unit ASIC FPGA and or other hardware platforms.

Other network description formats may be used with the process such as for example BRIAN and or other neuromorphic network description format e.g. NEURON configured to generate the END description of the network as illustrated in .

An exemplary implementation of a computerized network processing apparatus configured to utilize HLND framework in designing a neural network e.g. the network of is shown and described with respect to . The computerized apparatus may comprise a processing block e.g. a processor coupled to a nonvolatile storage device random access memory RAM user input output interface and or other components. The user input output interface may include one or more of a keyboard mouse a graphical display a touch screen input output device and or other components configured to receive input from a user and or output information to a user.

In some implementations the computerized apparatus may be coupled to one or more external processing storage devices via an I O interface such as a computer I O bus PCI E wired e.g. Ethernet or wireless e.g. WiFi network connection.

In some implementations the input output interface may comprise a speech input device e.g. a microphone configured to receive voice commands from a user. The input output interface may comprise a speech recognition module configured to receive and recognize voice commands from the user. Various methods of speech recognition are considered within the scope of the disclosure. Examples of speech recognition may include one or more of linear predictive coding LPC based spectral analysis algorithm run on a processor spectral analysis comprising Mel Frequency Cepstral Coefficients MFCC cochlea modeling and or other approaches for speech recognition. Phoneme word recognition may be based on HMM hidden Markov modeling DTW Dynamic Time Warping NNs Neural Networks and or other processes.

The END engine may be configured to convert an HLND description of the network into a machine executable format which may be optimized for the specific hardware or software implementation. The machine executable format may comprise a plurality of machine executable instructions that are executed by the processing block .

It will be appreciated by those skilled in the arts that various processing devices may be used with various implementations including but not limited to a single core multicore CPU DSP FPGA GPU ASIC combinations thereof and or other processors. Various user input output interfaces may be applicable to various implementations including for example an LCD LED monitor touch screen input and display device speech input device stylus light pen trackball and or other user interfaces.

In some implementations the network design system e.g. the system of may automatically convert the GUI actions into HLND instructions and or into END statements. The HLND instruction may cause automatic updates of the GUI representation and or of the END description.

As illustrated in a single object e.g. the object may have one or more representations related thereto which may include GUI representation e.g. using the GUI editor of the HLND representation e.g. using the HLND statement described supra with respect to the END representation see e.g. and or other representations depicted by the rectangle . Responsive to an object property i.e. the object data element being generated and or updated the corresponding representations of the object e.g. the representations and may be updated using bi directional pathways and respectively.

In some implementations responsive to a user GUI action modifying a selection the corresponding HLND statement s e.g. the HLND representation in may be updated. In some implementations the END instructions e.g. the END representation in may be updated.

In some implementations the END instructions may be executed by the apparatus thereby enabling a more detailed and accurate representation of the network.

In some implementations nodes may be rendered within the GUI using unique color symbol when the statement to create units is available in the network description framework.

In some implementations nodes may be rendered within the GUI at their correct location with unique for the whole subset symbols responsive to the coordinates of the nodes being available that is when the connections statement is at least partially processed.

In some implementations connections between two subsets may be rendered by the GUI using for example a single line when the connect instruction is available in the network description framework.

In some implementations connections between two subsets may show the detailed connectivity structure once the pre node and post node information for the connection instances are available i.e. have been previously generated that is once the connection statement is at least partially processed.

In some implementations connections between two subsets may show the detailed connectivity structure with unique properties e.g. line width representing a connection per connection responsive to the pre node information the post node information and or the initial weight for the connection instances being available that is once the connection statement is at least partially processed.

As is appreciated by those skilled in the arts other representations e.g. as depicted by the rectangle in may exist and may be compatible with various implementations provided they conform to the update framework described herein.

In some implementations data exchange between different representations e.g. the representations and in may be enabled via direct links denoted by arrows and in . For clarity not all direct connections between the representations and are shown in .

It will be recognized that while certain aspects of the disclosure are described in terms of a specific sequence of steps of a method these descriptions are only illustrative of the broader methods of the disclosure and may be modified as required by the particular application. Certain steps may be rendered unnecessary or optional under certain circumstances. Additionally certain steps or functionality may be added to the disclosed implementations or the order of performance of two or more steps permuted. All such variations are considered to be encompassed within the disclosure disclosed and claimed herein.

Although the disclosure has been described in detail for the purpose of illustration based on what is currently considered to be the most practical and preferred implementations it is to be understood that such detail is solely for that purpose and that the disclosure is not limited to the disclosed implementations but on the contrary is intended to cover modifications and equivalent arrangements that are within the spirit and scope of the appended claims. For example it is to be understood that the present disclosure contemplates that to the extent possible one or more features of any implementation can be combined with one or more features of any other implementation.

