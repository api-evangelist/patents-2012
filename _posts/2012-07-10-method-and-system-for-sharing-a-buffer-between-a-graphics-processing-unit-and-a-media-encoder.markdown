---

title: Method and system for sharing a buffer between a graphics processing unit and a media encoder
abstract: A program module executing in a first process space of a mobile computing device receives a buffer request from a graphics driver running in a second process space of the mobile computing device, wherein the second process space is isolated from the first process space. The program module assigns a buffer to the graphics driver to store image data processed by a graphical processing unit (GPU) controlled by the graphics driver. The program module receives a release of the buffer from the graphics driver. The program module assigns the buffer to a media encoder driver for a hardware media encoder to encode the image data in the buffer into a file.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08928680&OS=08928680&RS=08928680
owner: Google Inc.
number: 08928680
owner_city: Mountain View
owner_country: US
publication_date: 20120710
---
Embodiments of the present invention relate to the field of image processing and more particularly to a pipeline for processing and storing image data on a mobile device.

Many modern mobile devices e.g. such as mobile phones have integrated cameras and processors. However image data consumes considerable resources. Moreover image processing also consumes considerable resources. Accordingly mobile devices that are resource limited may have restrictions with regards to capturing images performing image processing and or storing processed images.

The following is a simplified summary of the disclosure in order to provide a basic understanding of some aspects of the disclosure. This summary is not an extensive overview of the disclosure. It is intended to neither identify key or critical elements of the disclosure nor delineate any scope of the particular implementations of the disclosure or any scope of the claims. Its sole purpose is to present some concepts of the disclosure in a simplified form as a prelude to the more detailed description that is presented later.

In an embodiment of the present invention a program module executing in a first process space of a mobile computing device receives a buffer request from a graphics driver running in a second process space of the mobile computing device wherein the second process space is isolated from the first process space. The program module assigns a buffer to the graphics driver to store image data processed by a graphical processing unit GPU controlled by the graphics driver. The program module receives a release of the buffer from the graphics driver. The program module assigns the buffer to a media encoder driver for a hardware media encoder to encode the image data in the buffer into a file.

In addition methods and systems for performing the operations of the above described implementations are also implemented. Further a computer readable storage media is provided to store instructions for performing the operations of the above described implementations.

A system and method for capturing processing and encoding image data are disclosed. A mobile computing device includes an image capture device a graphical processing device GPU and a hardware media encoder. In one embodiment the GPU receives and processes image data captured by the image capture device. This processing may include analyzing the image data and or modifying the image data. After the image data is processed by the GPU the hardware media encoder encodes the processed image data into a file. Accordingly a user may record a video the GPU may modify frames of the video and the hardware media encoder may encode those modified frames into a video file.

A system and method for sharing an image buffer between a GPU and a hardware media encoder are also disclosed. In one embodiment a program module executing in a process space shared with the media encoder manages a buffer pool. The program module may assign buffers to a graphics driver running in a different process space of the mobile computing device and to an encoder driver for the hardware media encoder. After the GPU has processed image data and written it to a buffer the program module may assign that buffer containing the processed image data to the media encoder without generating any copies of the image data. The media encoder may then encode the processed image data.

Embodiments of the present invention are described with reference to image data. The image data may include still images as well as videos. The image data may be digital image data which may include a single image in the case of a still image or multiple image frames that may be chronologically connected in the case of a video . Digital images and videos generally comprise multiple picture elements e.g. pixels arranged in a two dimensional array. Each pixel may have a color and or a color value associated with the pixel. Information associated with the location and color of each pixel may be stored and or used by a computer system to display the digital image and or video.

Note that though embodiments of the present invention are discussed with reference to image data embodiments of the present invention also apply to audio data. Accordingly it should be understood that teachings presented herein on image data and image processing are equally applicable to audio data and audio processing. Additionally image data may be associated with audio data e.g. in the case of a video with visual and audio components and may be processed in parallel to or together with the audio data.

Each of the hardware components may be controlled by a driver. In particular the image capture device may be controlled by an image capture device driver GPU may be controlled by a graphics driver and media encoder may be controlled by an media encoder driver . Each driver may be configured to control specific hardware or set of hardware e.g. manufactured by a particular vendor . Additionally graphics driver may be configured to support a graphics API such as open graphics library embedded system OpenGL ES . Similarly the media encoder driver may be configured to support an encoder decoder API such as OpenMax integration layer OpenMax IL .

Image data may originate from the image capture device be processed by the GPU and then ultimately be encoded by the media encoder . This may be performed with minimal or no copying of the image data. For example a single copy of the image data may be shared by the GPU and the media encoder . Alternatively or additionally image data may originate from a media decoder which may correspond to media encoder that has decoded a stored image. The image data may then be processed by the GPU and then be encoded by the media encoder .

As mentioned the image processing pipeline may also operate as an audio processing pipeline and or as a multimedia processing pipeline e.g. that processes audio and video . For example the image capture device may be replaced or supplemented with an audio capture device e.g. a microphone and the image capture device driver may be replaced or supplemented with an audio capture device driver. Additionally the GPU may be replaced or supplemented with an audio digital signal processor DSP and the graphic driver may be replaced with an audio driver. Note that some GPUs and or graphics drivers are able to perform digital audio processing. Accordingly the GPU and or graphics driver may operate on both image data and audio data. Additionally the media encoder driver and media encoder may operate on image data both still images and video as well as audio data.

The device drivers interface with a mobile filter framework that runs in the application process space and or with a media framework that runs in the media server process space . The mobile filter framework abstracts operations performed by the drivers and provides a plug and play framework that is accessible by applications e.g. application . Similarly the media framework may abstract operations performed by media encoder driver and or other drivers controlled from the media server process space . The application may request a particular operation and the mobile filter framework and or media framework may interface with the device drivers to instruct them to perform individual operations that will achieve the operation requested by the application . For example the application may request that an image be taken with a specific image effect enabled. The mobile filter framework may instruct the image capture device driver to cause the image capture device to record an image. The mobile filter framework may then instruct the graphics driver to cause the GPU to apply the selected effect to the image. The media framework may then instruct the media encoder driver to cause the media encoder to encode the processed image into a file.

The image capturing image processing and or image encoding performed by the image processing pipeline architecture may be performed in real time or near real time. As used herein near real time performance means that image data e.g. image frames are processed by the GPU as they are output by the image capture device and that processed images are encoded by the media encoder as they are output by the GPU . Accordingly for near real time performance a processing time and encoding time per frame may be approximately equal to a sampling period of the image capture device . Therefore a user may point a camera at a person s head record a stream of camera images identify the person s face in those camera images process the camera images to add effects e.g. enlarging the person s nose and or eyes and encode the processed camera images with the added effects into a file. The image processing pipeline architecture may enable these operations to be performed in a resource efficient manner with minimal or no copying of image data.

In one embodiment the application image capture device driver and graphics driver run in an application process space and the media encoder driver runs in a media server process space that is isolated from the application process space . The mobile filter framework may run in the application process space and the media framework may run in the media server process space . Communication between the application process space and the media server process space may be facilitated by software components residing in one or both of the application process space and the media server process space as described below.

The image processing pipeline architecture may be divided into an image producer pipeline shown in and an image consumer pipeline shown in . The image producer pipeline connects the image capture device to the GPU . The image consumer pipeline connects the GPU to the media encoder . Together the image producer pipeline and the image consumer pipeline enable data originating from the image capture device to be processed by the GPU and then encoded by the media encoder .

Referring now to in one embodiment the image producer pipeline includes an image capture device as an image source and a GPU as an image consumer. The image capture device is controlled by an image capture device driver and the GPU is controlled by a graphics driver which communicate via one or more intermediary modules e.g. surface texture client and surface texture . The graphics driver may be designed specifically to control the GPU but may be configured to support the OpenGL ES application programming interface API . Therefore higher level software e.g. an application and or framework that interfaces with the graphics driver may communicate with the graphics driver using the OpenGL ES API.

In response to a user generating an image capture command or in anticipation of the user generating such a command the image capture device driver may request an image buffer for the image capture device to write image data to. The image capture device driver may send an image buffer request to surface texture client . The image buffer request may include a series of buffer properties for image buffers that the image capture device writes to are expected to have. The buffer properties may include a specified height width pixel format and so on. The buffer request that surface texture sends to the graphic buffer allocator may be in the form of a dequeue buffer command. The dequeue buffer command is a request to assign an image buffer from a pool or queue of available image buffers. This image buffer pool may be managed by the surface texture . The dequeue buffer command may contain the buffer properties that were specified by the image capture device driver .

Surface texture client may communicate the image buffer request to surface texture . In one embodiment surface texture client and surface texture run in different processes. Accordingly the communication between surface texture client and surface texture may be over an inter process boundary. Such communications across inter process boundaries are described in greater detail below. If surface texture has been assigned any image buffers having the specified buffer properties and or there are free image buffers available having the specified buffer properties then surface texture assigns such an image buffer to the surface texture client . Otherwise surface texture may send an image buffer request to graphic buffer allocator .

Graphic buffer allocator manages a memory . In response to receiving a buffer request graphic buffer allocator sets up one or more image buffers e.g. buffers A B through N that comply with specified buffer properties. These image buffers may be formatted for a specific device or devices such as the image capture device and or GPU . Graphic buffer allocator then assigns the one or more image buffers to surface texture .

Surface texture may manage a pool or queue of image buffers which are assigned to surface texture by graphic buffer allocator . Once surface texture has an available image buffer surface texture may assign the buffer to surface texture client . Surface texture client may in turn assign the image buffer to image capture device driver . Image capture device driver may then instruct the image capture device to capture image data and write the image data to the assigned image buffer.

After image capture device has written to an image buffer image capture device driver may issue a queue buffer call on that image buffer. Surface texture client may receive the queue buffer command and forward it to surface texture . Surface texture may then reassign the image buffer to graphics driver and GPU may process the contents of the image buffer.

In one embodiment image capture device driver communicates with surface texture client via a native window interface. The native window interface may be a native API for managing windows and may provide an ability to draw directly to image buffers. Surface texture client may generate a native window object for the image capture device driver . The image capture device driver may request an image buffer by issuing a dequeue buffer call to the native window object. The image capture device may then draw directly to the image buffers that are associated with the native window object. Subsequently image capture device driver may return the image buffer by issuing a queue buffer call for the image buffer to the native window object.

Surface texture client may communicate with the surface texture via a surface interface which may be a native platform graphics interface such as EGL . The surface interface is an interface between higher rendering APIs e.g. the OpenGL ES API supported by the graphics driver and the underlying native platform window system e.g. the native window interface . Surface texture client may create a surface object which may use the native window interface. The surface object may create graphics contexts for APIs and may synchronize drawing by native platform rendering APIs and higher rendering APIs. The image capture device driver and or image capture device may then interact with the surface object directly in native code using the native window object. The GPU and or graphics driver may interact with the surface object as well. Accordingly when the image capture device writes image data to the surface object in the native window object the GPU will be able to process that image data. Thus surface texture may take images from some arbitrary image producer e.g. image capture device and expose those images to image consumers e.g. to GPU .

Surface texture may send a request to graphic buffer allocator for one or more image buffers to be associated with the surface object. The graphic buffer allocator may allocate and map graphics memory e.g. image buffers to application processes. In one embodiment surface texture communicates with the graphic buffer allocator using a graphics allocation interface e.g. a graphics allocation gralloc API . The graphics allocation interface may accept dequeue buffer commands queue buffer commands create buffer commands and so forth. If no buffer is allocated when a dequeue buffer call is made this may trigger a request buffer call if a buffer is not already prepared.

In response to receiving a dequeue buffer command from surface texture for a surface object graphic buffer allocator may determine whether an unassigned image buffer A N has buffer properties included in the request. If no such image buffer is already available then graphic buffer allocator may generate a buffer having the requested properties from available memory. Once an image buffer having the requested buffer properties is available graphic buffer allocator assigns that image buffer to the surface object associated with the request.

In one embodiment an image buffer is assigned by passing an image buffer handle to surface texture for the surface object. The image buffer handle may include a pointer to a section of graphics memory to an image buffer as well as additional information that can be interpreted by native or device specific code e.g. by the graphics driver and or image capture device driver . The additional information in a buffer handle may include pixel format width height and so on.

The form that the image buffer handle takes e.g. the pixel format may be a device specific implementation detail. Moreover the form that the image buffer handle takes may also be device specific. Accordingly device specific code may interpret the image buffer handle e.g. determine a memory location to which it points but higher level software such as an application or operating system may not be able to interpret the image buffer handle . Therefore the image buffer handle may act as a layer of abstraction. By keeping a format of these image buffer handles opaque to the operating system level different hardware components are able to use whatever formats they select. Thus hardware components may use whatever format is most efficient without any constraints imposed by higher level software. Note that graphic buffer allocator may assign multiple image buffer handles to a single surface object.

Surface texture client then provides image capture device driver with information about the native window object surface object and any image buffer handles that have been assigned to the surface object. The image buffers associated with the image buffer handles can then be utilized by the image capture device .

The image capture device will write to a memory region the image buffer specified in the image buffer handle and will then call into the native window object to indicate that it is done filling the image buffer. In one embodiment this is done by issuing a queue command for the image buffer to the native window object which relinquishes control of the image buffer from the image capture device driver to the native window object. In response the surface texture client may forward the queue command to surface texture via the surface object. The surface texture may then notify a consumer e.g. the graphics driver or an application that the image buffer has been filled. Subsequently surface texture may assign the surface buffer handle to the graphics driver . When the GPU is ready to use the image buffer the GPU may use the image buffer handle to access the image data stored in the image buffer. In one embodiment no copies are made to provide the captured image data to the GPU for processing.

The principles of operation shown with reference to the image producer pipeline apply equally to any media source. Accordingly it should be noted that the image producer pipeline may be modified by replacing the image capture device driver and image capture device with other media sources. For example image capture device may be replaced with a hardware image decoder and image capture device driver may be replaced with a decoder driver. Alternatively image capture device may be replaced with an audio capture device and image capture device driver may be replaced by an audio capture device driver. Moreover the image producer pipeline may operate in parallel to other media producer pipelines.

Referring now to the image consumer pipeline includes a GPU as an image source and a hardware video encoder as an image consumer. The GPU is controlled by a graphics driver and the video encoder is controlled by a video encoder driver which communicate via one or more intermediary modules e.g. surface texture client and surface media source . The graphics driver may support for example the OpenGL ES API. The video encoder driver may support for example the Openmax IL API.

The graphics driver that controls the GPU and a surface texture client that communicates with the graphics driver run in an application process space . The video encoder driver that controls the video encoder may run in a media server process space that is isolated from the application process space . A surface media source that is configured to direct image buffers from the GPU to the hardware video encoder and graphic buffer allocator may also run in the media server process space . The isolation between the application process space and the media server process space provides a layer of security for graphics data that may be displayed on a screen of a mobile device. Such isolation may for example prevent applications from screen scraping to obtain data controlled by other applications.

GPU may receive an image buffer that was populated with data by an image capture device e.g. image capture device of as set forth above. GPU may then process the contents of that image buffer. Before during or after the GPU processes the image data graphics driver may request a buffer from surface texture client for the GPU . Surface texture client may in turn request the image buffer from surface media source . Surface media source may then request the image buffer from graphic buffer allocator which may manage memory e.g. containing image buffers A B through N . Graphic buffer allocator may assign an image buffer to surface media source which may assign the image buffer to surface texture client which may assign the image buffer to graphics driver .

Once a new image buffer is available for the GPU the GPU may write processed image data to that image buffer. The processed image data may include data on a color space used pixel data for multiple pixels and so forth. The graphics driver may pass a handle for the populated image buffer to surface texture client . Surface texture client may then pass the handle for the populated image buffer to surface media source which in turn may pass the handle for the populated image buffer to video encoder driver via an encoder interface . Video encoder may then encode the contents of the image buffer.

In one embodiment the image producer pipeline and the image consumer pipeline use different buffer pools. Therefore modifications may be made to the image data in an image producer buffer and those modifications may be written to an image consumer buffer. This may cause original unmodified image data to remain in an original format while the image data is processed by the GPU. Alternatively the image producer pipeline may share a buffer pool with the image consumer pipeline. In such an embodiment data transfers from the image producer pipeline to the image consumer pipeline may be performed without generating data copies. However original image data may be overwritten with modifications caused by the image processing.

In one embodiment graphics driver communicates with surface texture client via a native window interface. Surface texture client may generate a native window object for the graphics driver . The graphics driver may then request buffers from the native window object have the GPU draw directly into those buffers and return the buffers to the native window object.

Surface texture client may communicate with surface media source via a surface interface e.g. an EGL surface . Surface media source may assign a surface to the native window created by the surface texture client . The graphics driver and or GPU may then interact with the surface object directly in native code using the native window object. The media encoder may interact with the surface object as well. Accordingly when the GPU writes image data to the surface in the native window the media encoder will be able to encode that image data. Thus surface media source may take images from the GPU and expose those images to media encoder running in a different process space than the GPU .

Surface media source may additionally send a request to graphic buffer allocator for one or more image buffers to be associated with the surface object. In one embodiment surface media source communicates with the graphic buffer allocator using a graphics allocation interface e.g. a graphics allocation gralloc API .

In response to receiving a dequeue buffer command from surface media source for a surface object graphic buffer allocator may determine whether an available image buffer A N has buffer properties included in the request. If no such image buffer is already available then graphic buffer allocator may generate an image buffer having the requested properties and return the image buffer to surface media source . In one embodiment graphic buffer allocator assigns the image buffer to the surface object associated with the request.

In one embodiment an image buffer is assigned by passing an image buffer handle to surface media source for the surface object. The image buffer handle may include a pointer to a section of graphics memory to an image buffer as well as metadata such as pixel format width height and so on. Note that graphic buffer allocator may assign multiple image buffer handles for multiple image buffers to a single surface object. Surface media source may then provide the image buffer handles to surface texture client across a process boundary between the media server process space and the application process space .

In one embodiment the cross process communication of the image buffer handles is controlled based on file descriptors. Image buffer handles may each include one or more file descriptors and may be passed between process spaces based on the file descriptors. An operating system kernel may keep track of which processes have access to each file descriptor. A process may be unable to gain access to an image buffer handle and thus the underlying image buffer that it points to until the kernel associates that process with a file descriptor for that image buffer handle. The kernel may associate a new process to one or more file descriptors in response to receiving a request from a process that already has access to the file descriptors e.g. surface media source to grant access to the new process. In one embodiment the file descriptors are passed into the kernel and the kernel duplicates the file descriptors from one process to another process to give that other process access to them. The other information from the image buffer handles may then be copied and passed along with the duplicated file descriptors to the new process.

In order for surface media source to assign an image buffer handle to surface texture client surface media source may perform a callback into the kernel and instruct the kernel to grant the application process access to the file descriptor associated with the image buffer handle . Once this has been performed surface texture client provides image capture device driver with information about the native window object surface object and image buffer handles that have been assigned to the surface object. The image buffers associated with the image buffer handles can then be utilized by the image capture device .

The GPU will write to a memory region the image buffer specified in the image buffer handle and will then call into the native window object to indicate that it is done filling the image buffer. In one embodiment this is done by issuing a queue command for the image buffer to the native window object to relinquish control of the image buffer. In response the surface texture client may forward the queue command to surface media source via the surface interface over an inter process communication. The surface media source may then provide the image buffer handle to the encoder interface which may pass the image buffer handle to video encoder driver for use by hardware video encoder .

In one embodiment encoder interface periodically queries surface media source for image buffer handles associated with image buffers that have been populated with processed image data by GPU . Alternatively surface media source may notify encoder interface when such image buffer handles become available. When an image buffer handle pointing to an image buffer populated with processed image data is available encoder interface takes control of that image buffer handle and passes it to the media or video encoder driver . In one embodiment encoder interface issues a dequeue command on the image buffer handle to obtain access to the image buffer.

Video encoder driver may have its own memory buffer pool with buffers having a different format and corresponding to a different protocol than the discussed image buffers. Encoder interface may query video encoder driver for a buffer to populate with data. Encoder driver may then assign a buffer to encoder interface . This buffer may be associated with application level memory space. Traditional encoder APIs accept actual pixel data as input. However in one embodiment rather than filling the buffer with pixel data encoder interface and or surface media source populate the buffer with an image buffer handle . Video encoder driver may then access the image data associated with the image buffer handle .

In one embodiment encoder interface passes a buffer e.g. a pointer to the buffer from the media encoder driver s memory pool to surface media source with a request for the buffer to be filled in. Surface media source may then block if it doesn t have image data e.g. a frame to write into the buffer. If image data is available e.g. when an image buffer handle is returned to surface media source by graphics driver surface media source takes that image buffer handle and copies it into the buffer that it got from encoder interface . Surface media source may indicate to the encoder interface and eventually to the encoder driver that the buffer type that was copied is a handle to the buffer as opposed to the actual buffer data itself. Encoder interface then indicates to media encoder driver that it is done filling in that buffer and may request a next buffer.

After an image buffer of the media encoder driver is populated with an image buffer handle the media encoder takes the graphics data that was processed by the GPU and puts it in some encoded format that can be played or read. This may include compressing the image data e.g. performing image compression video compression and or audio compression to reduce a size of the image data. This may further include wrapping the image data in a container e.g. a file and storing the container. In one embodiment media encoder driver includes one or more codecs for encoding and decoding still images videos and or audio. Examples of audio codecs include advanced audio coding AAC motion picture experts group 4 MPEG 4 Dolby Digital Codec AC 3 MPEG Audio Layer 3 MP3 Windows Media Audio WMA Apple Lossless Audio Codec ALAC and so on. Examples of video codecs include MPEG 1 MPEG 2 MPEG 4 VC 1 DivX and so on. Examples of still image codecs include bitmap BMP graphics interchange format GIF joint photograph experts group JPEG tagged image file format TIFF portable network graphics PNG and so on.

Notably the processed image data in the image buffer may be encoded by the video encoder without any copies of the processed image data having been generated. In other words the video encoder may access the same memory space image buffer used by the GPU in order to encode the processed image data. Such shared use of the image buffers may be implemented in spite of the graphics driver and the video encoder driver running in two different isolated process spaces. As discussed above the surface texture client and or surface media source may act as intermediaries to facilitate buffer sharing between the GPU and media encoder across process boundaries. Moreover such sharing of image buffers may be performed without using tunneling. This enables implementation details of the image buffer sharing to be abstracted so that a single buffer sharing solution can be achieved across different hardware implementations.

In one embodiment surface texture client and surface media source enable a flexible color space conversion. Different hardware vendors have different capabilities and different hardware components. Color spaces that the GPU uses and that the video encoder uses are generally different. For example GPU may use a red green blue RGB color space and video encoder may use a luma chrominance YUV color space.

In one embodiment surface texture client directs the graphics driver to cause the GPU to convert image data from the color space supported by the GPU e.g. RGB into the color space supported by the hardware video encoder e.g. YUV . Surface media source may determine the color space supported by the hardware video encoder e.g. by querying the encoder driver . Surface texture client may then notify graphics driver of the determined color space to enable such a conversion. In an alternative embodiment surface media source directs video encoder driver to cause hardware video encoder to convert the image data from the color space supported by the GPU into the color space supported by the video encoder . Surface texture client may determine the color space supported by the GPU e.g. by querying the graphics driver . Surface media source may then notify the media encoder driver of the determined color space to enable such a conversion. Notably the color conversion may be performed by either the GPU or by the media encoder . The color conversion process and color space information may be hidden from higher level software such as the OS and or applications. For example at the device agnostic OS level information on whether the values in the image buffer are RGB values or YUV values may be unavailable. Thus the surface media source may orchestrate the ownership of the image buffers without having knowledge on the contents of those image buffers.

The mobile filter framework is a multimedia processing and analysis framework. The mobile filter framework utilizes GPU capabilities on a mobile platform to perform computationally intensive tasks in real time or near real time such as video analysis and effects. The filter framework provides an API for the application to interface with underlying hardware components such as the GPU image capture device and or media encoder. Therefore the application may issue commands for the underlying components without having any information about any underlying protocols or configurations. The framework includes a series of filters that can be arranged into a graph configuration to create various effects such as funny faces virtual backgrounds image processing effects etc. In one embodiment the output of these filters is an image created using the OpenGL embedded system ES API. However other output formats may also be used.

Each filter in the mobile filter framework may be a plug and play component that is suitable for use in a filter graph environment. In one embodiment the application instructs the mobile filter framework to perform an operation on an image and the mobile filter framework determines which filters to use to perform the operation. The application may specify a particular filter graph e.g. a particular arrangement of filters or may rely upon the mobile filter framework to automatically configure a filter graph based on a requested operation or operations. In either case the media filter framework may launch and or connect the filters and may take care of scheduling and type checking on filter ports.

Each filter may generate appropriate commands which are directed to one or more underlying device drivers. For example a filter may issue OpenGL commands to communicate with a graphics driver and media encoder filter may issue OpenMax IL commands to communicate with media encoder driver . Additionally media encoder filter may also issue OpenGL commands to communicate with graphics driver . By way of illustration some example commands that may be issued to graphics driver include commands to perform object detection and or image recognition on an image e.g. to perform face detection commands to change colors commands to modify an image e.g. modify an appearance of a face such as enlarge a head nose eyes etc. and so forth. Filters may also be configured to identify foreground and or background elements in images to blend images e.g. to replace background elements of an image with a virtual background from another image and so forth. Each filter may provide an API to talk to the underlying components e.g. OpenGL Openmax IL etc. . Each filter on its own can choose whether particular image processing takes place on the GPU or on the CPU.

Each filter may include an input and or an output. Filters that have an output but no input may represent image sources e.g. a filter that represents an image capture device or image decoder . Filters that have an input but no output may represent image sinks e.g. a filter that represents a display or an encoder . Filters that have both an input and an output may represent intermediate processes that can be performed on image data. Mobile filter framework may select filters and arrange them into a filter graph and may connect the filters in accordance with the filter graph. Alternatively the filters and arrangement of filters may be specified by the application in a filter graph. Source nodes are arranged at a top of the graph and sink nodes are arranged at a bottom of the graph and may be connected by one or more intermediate nodes. To connect filters together each filter specifies which inputs it can receive and what outputs it produces. The filter graph may be arranged by matching up inputs to outputs of the same type. One example of a filter graph is discussed below with reference to .

The mobile filter framework is shown to include a media encoder filter and one or more additional filters that provide image data to the media encoder filter . Media encoder filter provides a wrapper around the image consumer pipeline described with reference to . The media encoder filter is a plug and play component of the mobile filter framework for encoding the output of any other filter e.g. additional filters .

In one embodiment media encoder filter establishes the media consumer pipeline of . This may include loading the surface texture client in the application process space and loading the surface media source and or encoder interface in the media server process space . This may further include setting up connections between the graphics driver the surface texture client the surface media source the encoder interface and or the video encoder driver .

Referring back to in one embodiment media encoder filter includes a surface texture client creator a surface media source creator and a pipeline interactor . When media encoder filter is loaded surface texture client creator creates a surface texture client in a process space shared with the media encoder filter and the application. Surface media source creator also causes a surface media source to be created in a distinct media server process space. Once the surface texture client and surface media source are created these components connect together and to appropriate drivers and or other components to establish a pipeline.

Media encoder filter may include a pipeline interactor that interacts with an established pipeline via a surface object. The pipeline interactor may perform a cross process call to the media server process to request that the surface media source create the surface object. The created surface object may correspond to the previously discussed surface object that is included in a native window associated with the surface texture client. Pipeline interactor can then request and send image data e.g. frames using the surface object.

Media encoder filter receives as an input a blended image output by the image blending filter and generates an output of an encoded media file e.g. an MPEG 4 file . Media encoder filter may correspond to media encoder filter of .

At block of method processing logic receives image data that has been captured by an image capture device e.g. a camera . The image capture device may write the image data to an image buffer and then release the image buffer to processing logic. Processing logic may provide the image data to a GPU e.g. by assigning the image buffer to a graphics driver for the GPU .

At block the processing logic directs the GPU to perform one or more operations on the received image data. Some example operations include performing object detection and or image recognition blending the image data with other image data applying effects to the image data and so forth.

Once the GPU has processed the image data processing logic provides the image data to a media encoder e.g. by assigning the image buffer to the media encoder . The media encoder may operate using a different color space than the GPU. Accordingly at block processing logic directs the GPU or the media encoder to convert the image data from a first color space supported by the GPU into a second color space supported by the media encoder. Processing logic may determine which of the GPU and the media encoder is to perform the color space conversion. This determination may be performed based on comparing total and or available resources of the GPU and the media encoder. For example if it is determined that the GPU has more available resources than the media encoder then processing logic may direct the GPU to perform the color space conversion.

At block processing logic directs the media encoder to encode the image data. Since the image data has been converted into the color space used by the media encoder the media encoder can encode the image data using its natively supported color space. At block processing logic stores the encoded image data to a data store. The data store may be incorporated into the mobile device or may be remote storage e.g. a network storage device or cloud storage to which the mobile device is wirelessly connected.

Notably the image capture device GPU and media encoder discussed with reference to method may be set up in a pipeline. Accordingly as soon as the image capture device sends a first frame of image data to the GPU the image capture device may begin capturing a second frame. The GPU can then operate on the first frame while the image capture device captures the second frame. The GPU can send the first frame to the media encoder once it has completed processing the first frame and may then receive the second frame from the image capture device. The media encoder may encode the first frame while the GPU processes the second frame and the image capture device captures a third frame. Thus each of the image capture device GPU and media encoder may operate in parallel for maximum efficiency.

At block of method processing logic receives a command from an application to modify media data in accordance with an image effect. The image effect may be selected by a user or may be auto selected by processing logic. At block processing logic identifies a single filter or a collection of filters that together can accomplish the selected image effect. These filters may be specified in a filter graph provided by the application or may automatically be determined based on a request received from the application. Each filter may be associated with metadata that identifies the capabilities of the filter as well as input data types and or output data types usable by that filter.

At block processing logic selects a media encoder filter that once loaded will cause a media encoder to encode an output of the one or more other filters that were selected. At block processing logic arranges the filters into a filter graph with the media encoder filter arranged at an end of the filter graph. Filters may be arranged by attaching outputs of filters to inputs of subsequent filters in the filter graph.

At block processing logic loads the filters in the filter graph. At block upon being loaded the filters in the filter graph set up an image processing pipeline. The one or more filters that instruct the GPU to perform operations to modify the image data may set up a first leg of the pipeline that connects an image capture device to a GPU referred to with reference to as a producer pipeline . The media encoder filter may establish a second leg of the pipeline that connects the GPU to a media encoder referred to with reference to as a consumer pipeline . Setting up the consumer pipeline may include loading a first module in a process space shared with the application and loading a second module in a remote process space that controls the media encoder. These modules may then connect together and may connect to a graphics driver that controls the GPU and to a media encoder driver that controls the media encoder. Once the image processing pipeline is established captured image data may be processed and then encoded and stored with minimal to no copying of data.

At block of method a program module executing in a first process space of a mobile computing device receives a buffer request. The buffer request may be received from a graphics driver running in a second process space of the mobile computing device that is isolated from the first process space. In one embodiment the buffer request originates from the graphics driver but is received from an additional program module that runs in the second process space and acts as an intermediary between the graphics driver and the program module. The buffer request may be received in the form of a dequeue buffer call.

At block the program module assigns a buffer to the graphics driver. The graphics driver may then direct the GPU to store image data that the GPU has processed in the buffer. In one embodiment the buffer is represented by an image buffer handle. The image buffer handle may be assigned to the graphics driver to grant access to modify the buffer. To grant access to the image buffer handle across process space boundaries the program module may call an operating system kernel to request that the kernel grant the second process space and thus the graphics driver running in the second process space access to the image buffer handle.

The GPU will populate the image buffer with processed image data. At block the graphics driver releases the buffer. This may include the graphics driver issuing a queue buffer call for the image buffer to the program module.

At block the program module assigns the buffer to a media encoder driver that may run in the same process space as the program module. The media encoder driver may then direct a hardware media encoder to encode the processed image data stored in the buffer into a file. The method then ends.

The exemplary computer system includes a processing device processor a main memory e.g. read only memory ROM flash memory dynamic random access memory DRAM such as synchronous DRAM SDRAM or Rambus DRAM RDRAM etc. a static memory e.g. flash memory static random access memory SRAM etc. and a data storage device which communicate with each other via a bus .

Processor represents one or more general purpose processing devices such as a microprocessor central processing unit or the like. More particularly the processor may be a complex instruction set computing CISC microprocessor reduced instruction set computing RISC microprocessor very long instruction word VLIW microprocessor or a processor implementing other instruction sets or processors implementing a combination of instruction sets. The processor may also be one or more special purpose processing devices such as an application specific integrated circuit ASIC a field programmable gate array FPGA a digital signal processor DSP network processor or the like. The processor is configured to execute instructions for performing the operations and steps discussed herein.

The data storage device may include a computer readable storage medium on which is stored one or more sets of instructions e.g. software embodying any one or more of the methodologies or functions described herein. The instructions may also reside completely or at least partially within the main memory and or within the processor during execution thereof by the computer system the main memory and the processor also constituting computer readable storage media. The instructions may further be transmitted or received over a network via the network interface device .

In one embodiment the instructions include instructions for one or more image processing pipeline components e.g. a media encoder filter a surface texture a surface texture client a surface media source etc. and or a software library containing methods that call components of an image processing pipeline. While the computer readable storage medium is shown in an exemplary embodiment to be a single medium the term computer readable storage medium should be taken to include a single medium or multiple media e.g. a centralized or distributed database and or associated caches and servers that store the one or more sets of instructions. The term computer readable storage medium shall also be taken to include any medium that is capable of storing encoding or carrying a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present invention. The term computer readable storage medium shall accordingly be taken to include but not be limited to solid state memories optical media and magnetic media.

The computer system may further include a network interface device . The computer system also may include a video display unit e.g. a liquid crystal display LCD or a cathode ray tube CRT an alphanumeric input device e.g. a keyboard a cursor control device e.g. a mouse and a signal generation device e.g. a speaker . Additionally the computer system may include an image capture device a hardware media encoder decoder and or a graphics processor GPU .

In the foregoing description numerous details are set forth. It will be apparent however to one of ordinary skill in the art having the benefit of this disclosure that the present invention may be practiced without these specific details. In some instances well known structures and devices are shown in block diagram form rather than in detail in order to avoid obscuring the present invention.

Some portions of the detailed description have been presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here and generally conceived to be a self consistent sequence of steps leading to a desired result. The steps are those requiring physical manipulations of physical quantities. Usually though not necessarily these quantities take the form of electrical or magnetic signals capable of being stored transferred combined compared and otherwise manipulated. It has proven convenient at times principally for reasons of common usage to refer to these signals as bits values elements symbols characters terms numbers or the like.

It should be borne in mind however that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussion it is appreciated that throughout the description discussions utilizing terms such as capturing performing causing encoding storing receiving allocating or the like may refer to the actions and processes of a computer system or similar electronic computing device that manipulates and transforms data represented as physical e.g. electronic quantities within the computer system s registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage transmission or display devices.

The present invention also relates to an apparatus for performing the operations herein. This apparatus may be specially constructed for the required purposes or it may comprise a general purpose computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a computer readable storage medium such as but not limited to any type of disk including floppy disks optical disks CD ROMs and magnetic optical disks read only memories ROMs random access memories RAMs EPROMs EEPROMs magnetic or optical cards or any type of media suitable for storing electronic instructions.

The words example or exemplary are used herein to mean serving as an example instance or illustration. Any aspect or design described herein as example or exemplary is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather use of the words example or exemplary is intended to present concepts in a concrete fashion. As used in this application the term or is intended to mean an inclusive or rather than an exclusive or . That is unless specified otherwise or clear from context X includes A or B is intended to mean any of the natural inclusive permutations. That is if X includes A X includes B or X includes both A and B then X includes A or B is satisfied under any of the foregoing instances. In addition the articles a and an as used in this application and the appended claims should generally be construed to mean one or more unless specified otherwise or clear from context to be directed to a singular form. Moreover use of the term an embodiment or one embodiment or an implementation or one implementation throughout is not intended to mean the same embodiment or implementation unless described as such.

For simplicity of explanation the methods have been depicted and described as a series of acts. However acts in accordance with this disclosure can occur in various orders and or concurrently and with other acts not presented and described herein. Furthermore not all illustrated acts may be required to implement the methods in accordance with the disclosed subject matter. In addition those skilled in the art will understand and appreciate that the methods could alternatively be represented as a series of interrelated states via a state diagram or events. Additionally it should be appreciated that the methods disclosed in this specification are capable of being stored on an article of manufacture e.g. a computer readable storage medium to facilitate transporting and transferring such methods to computing devices. The term article of manufacture as used herein is intended to encompass a computer program accessible from any computer readable device or storage media.

It is to be understood that the above description is intended to be illustrative and not restrictive. Many other embodiments will be apparent to those of skill in the art upon reading and understanding the above description. The scope of the invention should therefore be determined with reference to the appended claims along with the full scope of equivalents to which such claims are entitled.

