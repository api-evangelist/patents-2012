---

title: System and method for monitoring an application or service group within a cluster as a resource of another cluster
abstract: Various systems and methods for monitoring an application or service group within one cluster as a resource of another cluster are disclosed. In one embodiment, a method involves detecting an error indication generated by a first cluster (e.g., the error indication can be generated by a cluster controller or service group within the first cluster). The first cluster is implemented on a cluster resource (e.g., a virtual machine) of a second cluster. In response to detecting the error indication, restart of the cluster resource is initiated.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08464092&OS=08464092&RS=08464092
owner: Symantec Operating Corporation
number: 08464092
owner_city: Mountain View
owner_country: US
publication_date: 20120521
---
This application is a continuation of U.S. patent application Ser. No. 10 954 593 entitled System and Method For Monitoring An Application or Service Group Within A Cluster As A Resource of Another Cluster filed Sep. 30 2004 now U.S. Pat. No. 8 185 776 and naming James C. Gentes Anand Hariharan Indira M. Uppuluri as the inventors. This application is assigned to Symantec Operating Corporation the assignee of the present invention and is hereby incorporated by reference in its entirety and for all purposes as if completely and fully set forth herein.

This invention relates to clustered computer systems and more particularly to allowing one cluster to monitor activities taking place within another cluster.

Clustering involves configuring multiple computer systems also referred to as nodes to cooperate with each other in order to provide increased availability and or load balancing of certain services. A cluster is usually viewed as a single entity by clients that use the cluster s services. In a system that is designed to provide high availability a cluster can continue providing a particular application service to clients even if one of the nodes included in that cluster fails. In other words programs executing on the cluster can continue to function despite a problem with one node in the cluster.

In order to reduce costs and increase utilization of physical computer systems various virtual computing techniques are being used to subdivide a single physical computing resource into two or more virtual machines. By subdividing a physical computing resource into multiple virtual machines a user can achieve greater utilization of that physical computing resource.

Using traditional clustering methods in systems that employ virtual machines tends to undermine the basic benefits of clustering virtual computing or both. For example in order to use traditional clustering methods with virtual machines each virtual machine is typically handled as a node within a cluster. However since several virtual machines may be implemented on the same physical computing resource this configuration does not provide the fault tolerance normally expected in clustering applications. Instead the physical computing resource represents a single point of failure.

Furthermore in order to be able to failover an application from one virtual machine to another one virtual machine is required to be running in standby mode. Until a failure occurs the standby virtual machine is using up valuable physical computing resources that could otherwise be used for other purposes. Additionally additional software licenses may be required for the standby virtual machine increasing the cost to the user. As these examples show improved techniques for combining clustering and virtual computing are desired.

Various embodiments of systems and methods are disclosed for monitoring an application or service group within one cluster as a resource of another cluster. In one embodiment a method involves detecting an error indication generated by a first cluster e.g. the error indication can be generated by a cluster controller or service group within the first cluster . The first cluster is implemented on a cluster resource e.g. a virtual machine that is monitored by a second cluster. In response to detecting the error indication restart of the cluster resource is initiated.

In another embodiment a method involves monitoring a resource implemented on a virtual machine which is in turn implemented on the first node of a cluster. An error condition is detected in the resource. In response to the error condition restart of the virtual machine is initiated.

In one embodiment a system includes a cluster controller which includes a remote agent and a cluster engine. The remote agent is configured to detect an error indication corresponding to a resource implemented on a virtual machine. The virtual machine is implemented on the first node of a cluster. The cluster engine is configured to initiate restart of the virtual machine in response to the remote agent detecting the error. The error indication is generated by a monitoring application executing on the virtual machine.

The foregoing is a summary and thus contains by necessity simplifications generalizations and omissions of detail consequently those skilled in the art will appreciate that the summary is illustrative only and is not intended to be in any way limiting. The operations disclosed herein may be implemented in a number of ways and such changes and modifications may be made without departing from this invention and its broader aspects. Other aspects of the present invention as defined solely by the claims will become apparent in the non limiting detailed description set forth below.

While the invention is susceptible to various modifications and alternative forms specific embodiments of the invention are provided as examples in the drawings and detailed description. It should be understood that the drawings and detailed description are not intended to limit the invention to the particular form disclosed. Instead the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope of the invention as defined by the appended claims.

A first cluster is implemented on a cluster resource monitored by a second cluster. The second cluster monitors the first cluster in order to detect errors within the cluster resource. If an error is detected within the first cluster the second cluster restarts the cluster resource on which the first cluster is implemented. As an example the first cluster can be a single node cluster executing on a virtual machine which is in turn implemented on one of the nodes within the second cluster. The virtual machine is a cluster resource that is being made highly available by the second cluster. The cluster controller of the second cluster can monitor the cluster controller of the single node cluster and or a service group within the single node cluster. The cluster controller or service group of the single node cluster in turn monitors an application or other cluster resource implemented on the virtual machine. If the cluster controller or service group of the single node cluster generates an error indication in response to failure of the application the cluster controller of the second cluster can restart the virtual machine e.g. on another node within the second cluster .

Cluster resources are monitored and controlled e.g. brought online or started taken offline or stopped and or monitored by a cluster controller. Cluster resources can include resources e.g. virtual machines applications server software and the like that a cluster monitors in order to ensure that those resources are highly available as well as other resources e.g. storage devices operating systems file systems network addresses databases and the like on which the proper operation of the highly available resources is dependent. Some cluster resources e.g. virtual machines hardware partitions and the like can execute an instance of an operating system. Accordingly clustering software can be installed on such a cluster resource allowing another cluster to be implemented on that cluster resource.

In some embodiments various cluster resources are grouped into service groups. A service group includes a cluster resource e.g. an application and other resources cluster resources and or non cluster resources upon which the cluster resource depends. For example if the cluster resource a web application the service group can include disk groups on which web pages are stored a logical volume built in the disk group a file system using the volume network interface cards to export the web service one or more IP addresses associated with the network cards and the web application program and associated code libraries. The resources within a service group can be dependent on each other e.g. in the above example the logical volume depends on the disk groups the file system depends on the logical volume and so on . It is noted that in some embodiments a service group can include a single resource.

Each cluster can include one or more service groups. Each service group can monitor the resources within that service group for failures. The cluster controller can control and monitor each service group within the cluster as a unit allowing service groups to be stopped and started independently of each other. The cluster controller starts or stops a service group by starting or stopping each resource within the service group in an order that satisfies dependencies between the resources e.g. when starting the web application above the disk groups would be started first then the logical volume then the file system and so on . The cluster controller can also treat each service group as a unit when performing failovers. For example instead of failing over all cluster resources within a node in response to a failure within one of several service groups the cluster controller can fail over only the resources within the particular service group that experienced the failure.

Several virtual machines and are implemented on node . Virtual machine controller operates to control the resources e.g. processors memory network interfaces and the like of node in order to implement virtual machines and . Similarly virtual machine controller operates to implement several virtual machines and on node . Each virtual machine is a machine that is implemented on physical resources e.g. the computing devices used to implement nodes and that can be shared with one or more other virtual machines. Virtual machine controllers and operate to control each virtual machine s utilization of the underlying physical resources. For example if node includes four processors virtual machine controller can allocate one of the processors to each of the virtual machines and . Alternatively a node that includes four processors can be used to implement more than four virtual machines e.g. by allocating a certain amount of processor usage of one or more of the processors to each virtual machine . Likewise virtual machine controller can allocate a portion of the memory in node to each of the virtual machines. Virtual machine controllers and can be implemented in software such as VMWARE software from VMware Inc. and MICROSOFT Virtual Server.

A respective cluster controller or executes on each node and . Cluster controllers and communicate with each other in order to manage cluster . Cluster controllers and can be implemented using VERITAS Cluster Server software. In one embodiment VMWARE ESX SERVER is used to implement virtual machine controllers and and cluster controllers and execute on the service console e.g. a modified version of the Linux operating system provided by VMWARE ESX SERVER . Cluster controllers and do not execute on the virtual machines.

Another cluster controller referred to herein as a sub cluster controller in order to distinguish from cluster controllers and executes on each virtual machine. Sub cluster controller executes on virtual machine sub cluster controller executes on virtual machine sub cluster controller executes on virtual machine and sub cluster controller executes on virtual machine . Like cluster controllers and sub cluster controllers can be implemented using VERITAS Chister Server software.

In the illustrated embodiment each sub cluster controller controls a node of a single node cluster. A single node cluster is a node on which some sort of clustering controller e.g. such as sub cluster controllers is implemented. The cluster controller of a single node cluster can communicate with another cluster e.g. sub cluster controllers can communicate with cluster controllers and or monitor a group of cluster resources e.g. such as one or more applications executing on the single node and restart or attempt to restart a cluster resource locally on the single node if the resource experiences an error condition e.g. an application failure a hardware error or the like . However the cluster controller of a single node cluster cannot failover the cluster resource since there is no other node to failover to within the single node cluster. It is noted that in alternative embodiments each sub cluster controller can be implemented as a controller of a multi node cluster as opposed to a single node cluster . In such embodiments a sub cluster controller could potentially failover a cluster resource to another node within the multi node cluster.

Implementing the sub cluster controllers as controllers of single node clusters provides the functionality needed to monitor cluster resources within a virtual machine as well as the functionality needed to communicate with another cluster controller e.g. cluster controllers and or . This functionality allows the sub cluster controllers to monitor a resource within a virtual machine for another cluster controller. Using a cluster controller to provide this functionality promotes the reuse of existing components i.e. an existing cluster controller can be modified and or configured to provide such functionality without having to redesign the entire cluster controller . However as noted below certain functionality of the sub cluster controllers may not be necessary at least in some embodiments to provide the monitoring and communication functionality. Accordingly in some embodiments this functionality can be implemented in specialized applications or devices which do not incorporate all of the functionality that would be provided by a cluster controller instead of a cluster controller.

It is noted that several service groups groups of one or more resources such as applications can be implemented within single node cluster . Each service group can monitor an application within that service group and sub cluster controller can in turn control and monitor each service group. In such an embodiment either sub cluster controller or the service group or both can generate an error indication in response to a failure of the application. This error indication can then be monitored by cluster controller which communicates with sub cluster controller to detect errors detected within the service group.

Sub cluster controller controls a node implemented on virtual machine which is the only node in cluster . Since cluster is a single node cluster sub cluster controller does not need to communicate with corresponding sub cluster controllers on other nodes e.g. sub cluster controller may not perform heartbeat messaging or other typical cluster communication . Application executes on the node implemented by virtual machine . Sub cluster controller controls and monitors application . For example sub cluster controller can stop and start application as a cluster resource. Sub cluster controller can also monitor application for errors. If application cannot be restarted after an error sub cluster controller generates an error indication.

Like sub cluster controller sub cluster controller controls a node implemented on virtual machine which is the only node in cluster . As part of controlling the node within single node cluster sub cluster controller monitors application which is executing on the node implemented by virtual machine . Similarly sub cluster controller controls a node implemented on virtual machine which is the only node in cluster . Sub cluster controller controls and monitors application which executes on the node implemented by virtual machine . Sub cluster controller controls a node implemented on virtual machine which is the only node in cluster . Sub cluster controller monitors and controls application which executes on the node implemented by virtual machine . While this embodiment illustrates sub cluster controllers that monitor applications executing on virtual machines it is noted that the sub cluster controllers can monitor other resources e.g. virtual hardware within their respective virtual machines instead of and or in addition to applications.

Cluster controller controls and or monitors various cluster resources within node . In this particular example the cluster resources monitored by cluster controller include virtual machines and . If cluster controller detects an error within one of virtual machines or cluster controller can communicate with cluster controller on node in order to restart the virtual machine that experienced the error condition to node .

Cluster controller also monitors sub cluster controllers and which in turn respectively monitor applications and . If sub cluster detects an error in application sub cluster controller can attempt to restart the application. If the attempt is unsuccessful sub cluster controller generates an error indication.

Cluster controller can routinely communicate with sub cluster and in response to detecting the error indication generated by sub cluster controller cluster controller can restart e.g. as part of a failover or switchover operation virtual machine on a different node such as node . Cluster controller can similarly restart virtual machine on a different node in response to a failure in application . Thus cluster controller can restart a virtual machine in response to either a failure of the virtual machine or a failure within an application executing on or other resource of the virtual machine. Cluster controller can similarly restart virtual machines and based on either failures of the virtual machines or failures in applications executing on or other resources of the virtual machines. In one embodiment in order for cluster controllers and to be able to restart virtual machines on different nodes within cluster virtual machine controllers and each register each virtual machine such that virtual machines can each be started on either node or .

Since cluster controllers and can handle virtual machines as cluster resources as opposed to handling the virtual machines as cluster nodes the number of virtual machines that can be implemented within the system may be much greater than the maximum number of nodes supported by the clustering technology. For example even though the clustering technology may support a maximum of 32 nodes the clustering technology can nevertheless be used in a system that has more than 32 virtual machines. Thus by controlling virtual machines as cluster resources the scalability of the clustering technology can be increased.

While the above examples show cluster controllers that can communicate with sub cluster controllers executing on virtual machines it is noted that alternative embodiments can implement cluster controllers that communicate with sub cluster controllers executing in non virtual machine environments. For example in an alternative embodiment a cluster controller executing on a first physical computing device can communicate with a sub cluster controller that is executing on a second physical computing device. In such a system a sub cluster controller is simply a cluster controller that is monitored by a cluster controller of another cluster.

Application specific agent manages resources of a predefined cluster resource type according to commands received from cluster engine . In particular application specific agent is designed to monitor and or control applications of the same type as application . In one embodiment application specific agent is designed to start and stop application as well as to perform various operations to verify the functionality of application . Application specific agent can be a multi threaded agent that can monitor several applications of the same type at substantially the same time.

Cluster controller also includes several components including cluster engine virtual machine agent and remote agent . Like cluster engine cluster engine performs basic clustering functions. For example cluster engine can be responsible for building the cluster configuration for cluster shown in from configuration files distributing information when new nodes join cluster responding to operator input and taking corrective action when a resource within cluster fails. Cluster engine can use agents such as virtual machine agent and remote agent to monitor and manage cluster resources. Cluster engine collects information about cluster resource states from agents on the local node and forwards this information to all other cluster members. Cluster engine also receives similar information about cluster resource states from other cluster members.

Virtual machine agent is designed to monitor virtual machine cluster resources such as virtual machine . Virtual machine agent reports the status of the monitored virtual machine s to cluster engine . If cluster engine determines that a virtual machine should be failed over to another node virtual machine agent can also be used to initiate the failover e.g. by suspending the virtual machine . In some embodiments e.g. embodiments using ESX Server as a virtual machine controller virtual machine agent executes on the virtual machine controller service console allowing virtual machine agent to obtain information indicating whether the virtual machine is failed. In other embodiments public application programming interfaces APIs to the virtual machine are used to retrieve information indicating whether the virtual machine is failed.

Remote agent is designed to communicate with sub cluster controller in order to monitor one or more cluster resources e.g. application or a service group that includes application implemented within virtual machine . If remote agent detects an error indication generated by sub cluster controller or by a service group controlled by sub cluster controller remote agent can communicate that error indication to cluster engine in much the same way that an application specific agent executing as part of cluster controller would communicate an application error. Accordingly remote agent acts as a proxy for application specific agent within cluster controller allowing application to be monitored as if application were executing on the service console running on node . In this example remote agent can monitor application within virtual machine by sending sub cluster controller requests for information generated by and or the status of application specific agent . It is noted that in alternative embodiments remote agent can monitor application by communicating directly with application specific agent .

It is noted that if node includes several virtual machines remote agent can be implemented as a multi threaded agent that is configured to monitor applications and or other resources within each of the different virtual machines at substantially the same time. Alternatively cluster controller can include multiple instances of remote agent each configured to monitor one or more service groups applications or other resources within a respective virtual machine. It is also noted that multiple different types of applications or other cluster resources of the single node cluster can be implemented on virtual machine and that sub cluster controller can include a resource specific agent such as application specific agent to monitor each different application or other cluster resource. Also multiple service groups can be implemented within the single node cluster and each service group can monitor a different application and related resources.

In one embodiment remote agent communicates with sub cluster controller at the kernel level. In such an embodiment the communication can take place entirely within node . Such communication can involve remote agent accessing functionality provided by virtual machine controller shown in for use in controlling and or monitoring virtual machines.

In an alternative embodiment remote agent communicates with sub cluster controller via network messaging e.g. via Internet Protocol IP packets . For example an IP address can be assigned to each virtual machine implemented on node . An IP address can also be assigned to node . Remote agent can communicate with sub cluster controller by sending an IP packet to the IP address assigned to virtual machine . Similarly sub cluster controller can communicate with remote agent by sending an IP packet to the IP address assigned to node . It is noted that in at least some situations such IP packets do not leave node . For example the IP packets can be communicated between Network Interface Cards NICs within node and or between virtual network interfaces implemented on one or more NICs within node .

Remote agent can monitor application periodically e.g. by sending a request for the status of application and or application specific agent to sub cluster controller periodically . Remote agent reports the status of application to cluster engine .

Based on the reported status of application and on the reported status of virtual machine cluster engine determines whether virtual machine should be restarted e.g. as part of a failover operation on another node. Thus the availability of both a virtual machine and an application executing on that virtual machine can be monitored and on failures of either the virtual machine can be failed over to another physical node. Cluster engine can cause virtual machine to be failed over to another node by causing virtual machine agent to suspend operation of the virtual machine on node and communicating with cluster engines in one or more cluster controllers on other nodes that have been configured to implement virtual machine . The virtual machine agent within one of those other cluster controllers can then cause operation of the suspended virtual machine to resume on a different node.

It is noted that a given cluster controller can include several different remote agents each configured to monitor a respective application executing on a virtual machine. These remote agents can monitor different applications on different virtual machines different applications on the same virtual machine or the same application on different virtual machines. Similarly each virtual machine can include more than one service group. For example if there are several different applications executing or configured to execute on a virtual machine the single node cluster implemented on the virtual machine can include a different service group for each of the applications. In still other embodiments several single node clusters can be implemented on a virtual machine e.g. one for each application being monitored and operated as proxies for a remote agent of a larger cluster.

While the embodiment of shows the functionality of cluster controller and sub cluster controller subdivided into several different logical components it is noted that alternative embodiments can implement that functionality using different numbers of logical components and that the functionality can be subdivided among logical components differently. For example instead of using different application specific agents to monitor various types of resources a cluster controller can include a single monitoring agent that handles several different types of resources. As another example the functionality of remote agent and virtual machine agent is combined into a single logical component. Similarly the communication capabilities and application specific agent of sub cluster controller can be implemented as a single logical component which can monitor application and communicate with cluster controller . In one embodiment such a logical component does not include certain cluster functionality e.g. the logical component can exclude functionality needed to initiate application failovers .

As shown in this example cluster controllers and execute on nodes and respectively. Cluster controllers and respectively monitor applications and . Cluster controller monitors virtual machines and which are configured on node .

Virtual machines and are configured as backups for the physical machines implementing nodes and . Accordingly if one or both of nodes and fails applications and or can be restarted on virtual machines and .

If a failure occurs at one or both of the nodes at the primary site cluster controllers and or operate to restart applications and or on virtual machines and or . For example if both nodes and fail application can be restarted on virtual machine and application can be restarted on virtual machine . Cluster controller can then continue to monitor the applications which are now executing on virtual machines while also monitoring the virtual machines on which the applications are executing using techniques like those described above. For example a sub cluster controller can execute on each virtual machine in order to monitor the state of application s executing on each virtual machine. Remote agents within cluster controller can communicate with such sub cluster controllers in order to inform cluster controller of the status of the applications.

If one or both of nodes and are brought back online cluster controller can operate to restart applications and or on nodes and . For example in response to detecting that node is functional again cluster controller can use a remote agent to communicate with a sub cluster controller operating on virtual machine in order to shutdown an application executing on the virtual machine. The application can then be restarted on node at the primary site.

At a single node cluster is configured on the virtual machine. Performing function can involve setting up a cluster controller such as VERITAS Cluster Server to execute on the virtual machine and configuring that cluster controller to operate as part of a single node cluster. The cluster controller can also be configured not shown to monitor one or more resources of the single node cluster such as applications e.g. operating systems or user applications or service groups executing on the single node. For example the cluster controller can be configured to monitor an application as well as to bring that application online and offline.

The virtual machine is configured as a cluster resource that is monitored by a multi node cluster as indicated at . For example a cluster controller executing on the same physical computing device as the virtual machine can be configured to monitor the virtual machine as a cluster resource. In one embodiment the cluster controller of the multi node cluster executes on an operating system provided by the software used to implement the virtual machine. It is noted that a different type of operating system may be executing on the virtual machine itself e.g. a UNIX based operating system may be executing on the physical computing device while a WINDOWS based operating system executes on the virtual machine . The type of cluster controller implemented on the virtual machine may correspondingly differ from the type of cluster controller implemented on the physical computing device.

The cluster controller of the multi node cluster is also configured to communicate with the controller of the single node cluster as shown at in order to monitor the status of a resource that is in turn monitored by the single node cluster. For example the cluster controller of the multi node cluster can include a remote agent as described above that is configured to communicate with the cluster controller executing on the virtual machine.

In one embodiment configuring the cluster controller of the multi node controller to communicate with the cluster controller of the single node cluster involves assigning a username and password to the cluster controller of the multi node cluster allowing the cluster controller of the multi node controller to access the controller of the single node cluster via a network. For example the cluster controller of the multi node cluster can be configured with a set of attributes that can be used to access the cluster controller of the single node cluster. Such attributes can include the fully qualified network name of the virtual machine and appropriate domain name information the node name associated with the virtual machine the name of a group of cluster resources being monitored within the virtual machine and or a user name and password.

A resource that is monitored by a cluster can depend upon another resource of the cluster. Cluster controllers track such dependencies in order to be able to start and stop cluster resources in the appropriate order during situations such as cluster initialization and failover. Accordingly in this example performing function can also involve configuring the cluster controller of the multi node cluster to recognize that the cluster resource of the single node cluster that is being monitored within the virtual machine is dependent upon the virtual machine which is a resource that is monitored by the multi node cluster .

The method of ends at . It is noted that at least some of the functions and depicted in can be performed in a different order in other embodiments and that some embodiments may include other functions instead of and or in addition to those functions illustrated in .

If an error is detected within a cluster resource on a node the cluster resource is restarted as shown at and . In the cluster resource is restarted on a different node in some situations such as when failures are caused by transient conditions the cluster resource may be restarted on the same node . For example if a virtual machine implemented on the node experiences a failure the virtual machine can be shut down on the node and restarted on the other node. It is noted that several cluster resources can be organized into a service group and if any one of those cluster resources experiences a failure the entire service group may be restarted on another node. Thus performing function can involve restarting a service group of cluster resources. It is also noted that the cluster controller monitoring the cluster resource can initiate restart of the cluster resource by sending a communication to another cluster controller e.g. executing on the node on which the cluster resource will be restarted and or initiating the shutdown of the cluster resource.

In addition to checking for an error in cluster resource as shown at the method of also involves checking for an error in a sub cluster implemented on one of the cluster resources. If an error is detected within the sub cluster the cluster resource on which the sub cluster is implemented is restarted as shown at and . Again this example shows that the cluster resource is restarted on a different node however in some situations the cluster resource may be restarted on the same node e.g. if the failure is due to a transient error or a simple software failure .

Checking for an error in the sub cluster can involve a cluster controller which is implemented on the node communicating with a sub cluster controller implemented on a virtual machine within the node. If the sub cluster controller has detected a failure in one of the sub cluster resources the cluster controller can restart the virtual machine on another node.

Interfaces can include network interfaces to various networks and or interfaces to various peripheral buses. Interfaces can include an interface to one or more storage devices . Interfaces can also include an interface to a network for use in communicating with other nodes and or for use in communicating with networked storage devices. For example cluster controller and or sub cluster controller can use interfaces to communicate heartbeat messages with other nodes to communicate with clients or each other and or to access a storage volume via a SAN.

Memory stores the data and program instructions executable by one or more of processors to implement one or more applications. In this example memory stores data and program instructions implementing cluster controller including cluster engine virtual machine agent and remote agent sub cluster controller and virtual machine controller . It is noted that a portion of memory may be allocated among various virtual machines by virtual machine controller and that sub cluster controller which executes on a virtual machine is stored in a portion of memory allocated to that virtual machine. The program instructions and data implementing cluster controller sub cluster controller and virtual machine controller can be stored on various computer readable media such as memory . In some embodiments such software is stored on a computer readable medium such as a CD Compact Disc DVD Digital Versatile Disc hard disk optical disk tape device floppy disk and the like . In order be executed by processor s the instructions and data implementing cluster controller sub cluster controller and virtual machine controller are loaded into memory from the other computer readable medium. Such instructions and or data can also be transferred to node for storage in memory via a network such as the Internet or upon a carrier medium. In some embodiments a computer readable medium is a carrier medium such as a network and or a wireless link upon which signals such as electrical electromagnetic or digital signals on which the data and or instructions implementing cluster controller sub cluster controller and virtual machine controller are encoded are conveyed.

Although the present invention has been described with respect to specific embodiments thereof various changes and modifications may be suggested to one skilled in the art. It is intended that such changes and modifications fall within the scope of the appended claims.

