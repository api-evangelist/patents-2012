---

title: Drive partitioning for automated storage tiering
abstract: Delivering different data response time performance from a plurality of disk drives having similar performance characteristics includes subdividing each disk drive platter of the disk drives into at least two separate portions, where a first portion has a first average response time and the second portion has a second average response time that is greater than the first average response time and includes placing data that is relatively frequently accessed in the first portion of the disk platters of the disk drives to provide a subset of data having a relatively higher data response time performance than other data. Data having a relatively lower data response time performance may be placed on disk drives containing data having a relatively higher data response time performance.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08838887&OS=08838887&RS=08838887
owner: EMC Corporation
number: 08838887
owner_city: Hopkinton
owner_country: US
publication_date: 20120330
---
This application relates to computer storage devices and more particularly to the field of managing available resources provided on a computer storage device.

Host processor systems may store and retrieve data using storage devices containing a plurality of host interface units host adapters disk drives and disk interface units disk adapters . Such storage devices are provided for example by EMC Corporation of Hopkinton Mass. and disclosed in U.S. Pat. No. 5 206 939 to Yanai et al. U.S. Pat. No. 5 778 394 to Galtzur et al. U.S. Pat. No. 5 845 147 to Vishlitzky et al. and U.S. Pat. No. 5 857 208 to Ofek which are incorporated herein by reference. The host systems access the storage device through a plurality of channels provided therewith. Host systems provide data and access control information through the channels of the storage device and the storage device provides data to the host systems also through the channels. The host systems do not address the disk drives of the storage device directly but rather access what appears to the host systems as a plurality of logical volumes. Different sections of the logical volumes may or may not correspond to the actual disk drives.

An individual storage system may contain multiple tiers of storage each of which may have different performance characteristics and operating costs. For example one tier may be comprised of Flash drives another of Fibre Channel drives and another of SATA drives. The character of the storage request access loads that these each drive type is best suited for may vary greatly. In order to efficiently exploit the different capabilities provided by these drive types the drive type to which a each region of logical storage is mapped would be selected so that the resulting host access workload for the drives would effectively exploit that drives capabilities. For example SATA drives may handle workloads with low access density and tolerance for higher response times while Flash drives may handle workloads with high access density and low response time requirements. Fibre Channel drives may handle workloads that do not fall into either of these extremes. The flash drive may be the most expensive storage the SATA drives the least expensive storage and the Fibre Channel drives may have a cost that is in between the other two types of drives.

It has been found that a system that uses multiple tiers of data storage can be effective by storing more frequently used data in relatively faster and more expensive tiers while storing less frequently used data in relatively slower and less expensive tiers. See for example U.S. patent application Ser. No. 13 135 265 titled LOCATION OF DATA AMONG STORAGE TIERS filed on Jun. 30 2011 which is incorporated by reference herein. However in some cases there may not a sufficient number of different types of storage available having different speeds and corresponding cost. For example it may be desirable to have three tiers of data storage but only SATA and flash drives are available while Fibre Channel drives are not available.

Accordingly it is desirable to provide a system that uses a particular number of separate and distinguishable storage tiers while having less than the particular number of different types of data storage.

According to the system described herein delivering different data response time performance from a plurality of disk drives having similar performance characteristics includes subdividing each disk drive platter of the disk drives into at least two separate portions where a first portion has a first average response time and the second portion has a second average response time that is greater than the first average response time and includes placing data that is relatively frequently accessed in the first portion of the disk platters of the disk drives to provide a subset of data having a relatively higher data response time performance than other data. Delivering different data response time performance from a plurality of disk drives having similar performance characteristics may also include placing data that is relatively infrequently accessed in the second portion of the disk platters of the disk drives to provide a subset of data having a relatively lower data response time performance than other data. Data having a relatively lower data response time performance may be placed on disk drives containing data having a relatively higher data response time performance. Data having a relatively lower data response time performance may be placed on different disk drives than data having a relatively higher data response time performance and the different disk drives may have similar performance characteristics. The first portion may be in the middle of the disk drive platter and the second portion may be inside and outside the first portion. Delivering different data response time performance from a plurality of disk drives having similar performance characteristics may also include subdividing each disk drive platter of the disk drives into at least one other additional portion wherein each of the additional portions has a different average response time than the first and second portions and placing data on the at least one other additional portion. The first portion may be in the middle of the disk drive platter the second portion may be inside and outside the first portion a third portion may be inside and outside the second portion and any subsequent portions may be inside and outside previous portions.

According further to the system described herein computer software provided in a computer readable medium delivers different data response time performance from a plurality of disk drives having similar performance characteristics. The software includes executable code that subdivides each disk drive platter of the disk drives into at least two separate portions where a first portion has a first average response time and the second portion has a second average response time that is greater than the first average response time and executable code that places data that is relatively frequently accessed in the first portion of the disk platters of the disk drives to provide a subset of data having a relatively higher data response time performance than other data. The software may also include executable code that places data that is relatively infrequently accessed in the second portion of the disk platters of the disk drives to provide a subset of data having a relatively lower data response time performance than other data. Data having a relatively lower data response time performance may be placed on disk drives containing data having a relatively higher data response time performance. Data having a relatively lower data response time performance may be placed on different disk drives than data having a relatively higher data response time performance and the different disk drives may have similar performance characteristics. The first portion may be in the middle of the disk drive platter and the second portion may be inside and outside the first portion. The computer software may also include executable code that subdivides each disk drive platter of the disk drives into at least one other additional portion wherein each of the additional portions has a different average response time than the first and second portions and executable code that places data on the at least one other additional portion. The first portion may be in the middle of the disk drive platter the second portion may be inside and outside the first portion a third portion may be inside and outside the second portion and any subsequent portions may be inside and outside previous portions.

According further to the system described herein a fully automated storage tiering system includes a plurality of disk drives each of the disk drives having a platter that is subdivided into at least two separate portions where a first portion has a first average response time and the second portion has a second average response time that is greater than the first average response time and includes at least one director that places data that is relatively frequently accessed in the first portion of the disk platters of the disk drives to provide a subset of data having a relatively higher data response time performance than other data. The at least one director may place data that is relatively infrequently accessed in the second portion of the disk platters of the disk drives to provide a subset of data having a relatively lower data response time performance than other data. Data having a relatively lower data response time performance may be placed on disk drives containing data having a relatively higher data response time performance. Data having a relatively lower data response time performance may be placed on different disk drives than data having a relatively higher data response time performance and the different disk drives may have similar performance characteristics. The first portion may be in the middle of the disk drive platter and the second portion may be inside and outside the first portion. Each disk drive platter of the disk drives may be subdivided into at least one other additional portion where each of the additional portions has a different average response time than the first and second portions and where the at least one director places data on the at least one other additional portion and where the first portion is in the middle of the disk drive platter the second portion is inside and outside the first portion a third portion is inside and outside the second portion and any subsequent portions are inside and outside previous portions.

Referring now to the figures of the drawings the figures comprise a part of this specification and illustrate exemplary embodiments of the described system. It is to be understood that in some instances various aspects of the system may be shown schematically or may be shown exaggerated or altered to facilitate an understanding of the system.

Each of the HA s of the storage device may be coupled to one or more host computers that access the storage device . The host computers hosts access data on the disk drives through the HA s and the DA s . The global memory contains a cache memory that holds tracks of data read from and or to be written to the disk drives as well as storage for tables that may be accessed by the HA s the DA s the RA and the EA . Note that for the discussion herein blocks of data are described as being a track or tracks of data. However it will be appreciated by one of ordinary skill in the art that the system described herein may work with any appropriate incremental amount or section of data including possibly variable incremental amounts of data and or fixed incremental amounts of data and or logical representations of data including but not limited to compressed data encrypted data or pointers into de duplicated data dictionaries.

In some embodiments one or more of the directors may have multiple processor systems thereon and thus may be able to perform functions for multiple directors. In some embodiments at least one of the directors having multiple processor systems thereon may simultaneously perform the functions of at least two different types of directors e.g. an HA and a DA . Furthermore in some embodiments at least one of the directors having multiple processor systems thereon may simultaneously perform the functions of different types of director and perform other processing with the other processing system. Generally the system described herein could work with any appropriate hardware configuration including configurations where at least some of the memory is distributed among at least some of the directors and in configurations where a number of core processors are coupled to a generic interface card.

The storage devices may be provided as stand alone devices coupled to the host as shown in or alternatively the storage devices may be part of a storage area network SAN that includes a plurality of other storage devices as well as routers network connections etc. The storage devices may be coupled to a SAN fabric and or be part of a SAN fabric. The system described herein may be implemented using software hardware and or a combination of software and hardware where software may be stored in a computer readable medium and executed by one or more processors.

The system described herein is suitable for use with the technique of thin provisioning. Thin provisioning allows for the creation of logical volumes of storage space where allocation of physical storage space occurs only when space is actually needed e.g. when data is written in the first time to the storage space . Logical storage space may be identified to a user as being available even though no physical storage space has been committed at least initially. When data is written to the logical storage space physical storage space is drawn for use from a pool of physical storage space as further described elsewhere herein. In addition as described in more detail elsewhere herein stored data may be moved between physical locations using the storage infrastructure described herein.

The storage device may also include one or more thin devices . Each of the thin devices may appear to a host coupled to the storage device as a logical volume logical device containing a contiguous block of data storage. Each of the thin devices may contain tables that point to some or all of the data devices or portions thereof as further discussed elsewhere herein. In some instances the thin devices may be concatenated to form a metavolume of thin devices. In some embodiments only one thin device may be associated with the same data device while in other embodiments multiple thin devices may be associated with the same data device as illustrated in the figure with arrows having broken lines.

In some embodiments it may be possible to implement the system described herein using storage areas instead of storage devices. Thus for example the thin devices may be thin storage areas the data devices may be standard logical areas and so forth. In some instances such an implementation may allow for hybrid logical devices where a single logical device has portions that behave as a data device and or portions that behave as a thin device. Accordingly it should be understood that in appropriate instances references to devices in the discussion herein may also apply to storage areas that may or may not correspond directly with a storage device.

Each of the entries of the table may correspond to another table that may contain information for one or more logical volumes such as thin device logical volumes. For example the entry may correspond to a thin device table . The thin device table may include a header that contains overhead information such as information identifying the corresponding thin device information concerning the last used data device and or other information including counter information such as a counter that keeps track of used group entries described below . The header information or portions thereof may be available globally to the storage device .

The thin device table may include one or more chunks that contain information corresponding to a group of tracks on the data device or some other collection of data. The chunk may include one or more tracks the number of which may be configured as appropriate and or may include a different increment of data. In an embodiment herein each chunk corresponds to twelve tracks although this number may be configurable or dynamically adjustable based on appropriate criteria. Each track may be 64 k bytes. Note that for the same logical device different ones of the extents may have different sizes.

One chunks for example the chunk of the thin device table may identify a particular one of the data devices or a portion thereof having a track table that contains further information such as a header having overhead information and a plurality of entries each corresponding to one of the tracks of the particular one of the data devices . The information in each of the entries may include a pointer either direct or indirect to the physical address on one of the disk drives of the storage device or a remote storage device if the system is so configured that maps to the logical address es of the particular one of the data devices . Thus the track table may be used in connection with mapping logical addresses of the logical devices corresponding to the tables to physical addresses on the disk drives of the storage device .

The tables may be stored in the global memory of the storage device . In addition the tables corresponding to particular logical devices accessed by a particular host may be stored cached in local memory of the corresponding one of the HA s . In addition the RA and or the DA s may also use and locally store cache portions of the tables .

For each extent of storage the storage system maintains access statistics including for example short and long term rates at which random read misses pre fetches and writes are occurring. In some embodiments the size of the extent used for access monitoring may be significantly larger than a chunk and may be changed dynamically. An extent may in turn be organized into a set of contiguous sub extents which are themselves comprised of a contiguous range of chunks. For each sub extent the storage system may maintain a smaller amount of access information that may be combined with information from the extent level to determine access statistics specific to a particular sub extent. In some cases the chunks may be organized into sub extents of ten chunks each and the sub extents may be organized into extents each of which contains forty eight sub extents.

The entry may be a pointer to a head of the first allocated track for the one of the data devices indicated by the data device ID entry . Alternatively the entry may point to header information of the data device track table immediately prior to the first allocated track. The entry may identify a cylinder of a first allocated track for the one the data devices indicated by the data device ID entry . The entry may contain other information corresponding to the chunk and or the corresponding thin device. As described in more detail elsewhere herein the entry may contain information corresponding to usage data both measured and derived that may be used to promote and or demote data.

In some embodiments entries of the chunk table may identify a range of cylinders of the thin device and a corresponding mapping to map cylinder track identifiers for the thin device to tracks cylinders of a corresponding data device. In an embodiment the size of the chunk table may be eight bytes or may be made larger with additional information as described elsewhere herein.

Accordingly a thin device presents a logical storage space to one or more applications running on a host where different portions of the logical storage space may or may not have corresponding physical storage space associated therewith. However the thin device is not mapped directly to physical storage space. Instead portions of the thin storage device for which physical storage space exists are mapped to data devices which are logical devices that map logical storage space of the data device to physical storage space on the disk drives . Thus an access of the logical storage space of the thin device results in either a null pointer or equivalent indicating that no corresponding physical storage space has yet been allocated or results in a reference to a data device which in turn references the underlying physical storage space.

If it is determined at the step that there is physical data corresponding to the logical tracks being read then processing proceeds to a step where one or more of the data devices associated with the logical tracks being read are identified from the group table . After the step processing proceeds to a step where the track table is read from the identified one or more of the data devices and the corresponding location of the physical data i.e. cylinder and track is determined. As further discussed elsewhere herein physical storage space may be provided in connection with one data device and or by a concatenation of multiple data devices or portions thereof. Logical storage space maps to space on the data devices.

After the step processing proceeds to a step where a request may be sent to one or more disk adapters corresponding to disk drives that provide the physical storage space associated with the identified one of the data devices and corresponding location information. After the step processing proceeds to a step where the physical data is read. Note that the data may be stored in a cache or other memory for example the memory in connection with being read. In some cases if the data being read is already in the cache then the processing at the step and following steps may not be necessary. Note also that reading the data may include updating any metadata used to provide the processing described herein such as the time last accessed the host user making the request frequency of use and or any other appropriate metric. After the step processing proceeds to a step where the data may be received by an appropriate one of the host adapters e.g. by reading the memory . After the step processing is complete.

Following the step is a test step where it is determined whether physical space had been previously allocated i.e. in a prior write operation for the logical tracks being written. If so then processing proceeds to a step where the data device that includes the logical tracks is identified. After the step is a step where the track table is read from the identified one or more of the data devices and the corresponding location of the physical data i.e. cylinder and track is determined. As further discussed elsewhere herein physical storage space may be provided in connection with one data device and or by a concatenation of multiple data devices or portions thereof. Logical storage space maps to space on the data devices. Following the step processing proceeds to a step where the data being written is directed to the appropriate physical storage space. The incoming data overwrites the appropriate portion of the data where directed. After the step processing is complete.

If it is determined at the step that there is no physical storage that has been allocated for the logical track s being written then control transfers from the step to a step where a next available data device identifier i.e. the data device is determined. This information may be obtained from the header of the device table . In an embodiment herein data device identifiers are provided by numbers so that a next available data device identifier is simply one more than a last allocated data device. However as discussed in more detail elsewhere herein selection of a data device at the step may include application of other criteria.

After the step processing proceeds to a step where available physical storage space on the disk drives is determined. In an embodiment herein available physical storage space is allocated sequentially from one or more of the disk drives . Following the step is a step where a request may be sent to a disk adapter or possibly the RA and or the EA to allocate the physical storage space for the write. Also at the step header info is updated to reflect the newly allocated data device and physical tracks. After the step processing proceeds to the step discussed above where the data being written is directed to the one or more data devices. After the step processing is complete.

After the read and write processes illustrated in information concerning access of the data such as access frequency time of last access or use and or other characteristics and statistics may be updated and stored by the system described herein. The updated data access information or other characteristic information of the data and or any portion of the data may for example be stored as an entry in a group element of the thin device table for example the entry of the group element as shown in . Alternatively the data characteristic information may be stored in a memory such as the global memory of the storage device and a pointer to this information stored in the group element . Other implementations for storing and access of the data characteristic information are possible.

The allocation of the physical storage space for a thin device at the time of writing the data as well as the policies that govern the allocation may be transparent to a user. For example a user s inquiry into how much storage space is available on a particular thin device may indicate a maximum amount of physical storage space that could be allocated for a thin storage device provisioned storage space even though the corresponding physical storage space had not yet been allocated. In an alternative embodiment the policy for the thin device may be to report something less than the total maximum that could be allocated. In some embodiments used physical storage space may not exceed 30 of the provisioned storage space.

In an embodiment herein different portions of the physical data may be automatically moved between different physical disk drives or other storage devices different tiers with the same or different characteristics according to one or more policies. For example data may be initially allocated to a particular fast disk drive relatively high tier but a portion of the data that has not been used over a period of time for example three weeks may be automatically moved according to the system described herein to a slower and perhaps less expensive disk drive relatively lower tier . The physical data may then be automatically moved back to the higher tier if the data is subsequently used and or accessed according to a policy or other criteria for example accessed twice in any given week as further described herein. Thus the system described herein may operate to automatically move data between tiers within the same machine according to the one or more policies.

A policy may be configured by an administrator on a system wide level or may be specific to a particular user on a specific logical device. The system described herein allows for the remapping of physical data based on policy criteria or other statistics. For example the policy may be based on the last time data was used and or accessed. Alternatively the policy may be based on anticipated use of data over specific times and or dates. For example data that is expected to be used at a particular time may be stored on or relocated to relatively fast tier and then moved to relatively slow tier when it is expected that the data will not be used again for a lengthy period of time. Moreover different policies and or criteria may be implemented corresponding to different users and or different levels of importance or security of data. For example it may be known that user A accesses particular data more frequently than user B and accordingly the policy for moving physical data according to the system described herein may be to leave more data associated with user A on the relatively fast disk drive as compared with the data associated with user B. Alternatively user A may access data that is generally of a higher level of importance or requires higher security than that of user B and accordingly the system described herein may maintain and or move more data associated with user A on a disk drive that is relatively more reliable available and or secure as compared with the data associated with user B.

In an embodiment herein data may be moved between physical disk drives or other physical storage having different characteristics such as speed cost reliability availability security and or other characteristics. As discussed elsewhere herein logical data devices may be established having different classes corresponding to characteristics of the physical disk drives or other physical storage to which the data devices are mapped. Further it should be noted that any section of the logical device may be moved according to the system described herein based on the characteristics of the data and governed by default or specific policies .

After the step processing proceeds to a step where policy information is accessed. The policy information provides the specific criteria used for data storage and management. After the step processing proceeds to a step where the policy is applied to the stored data. The policy may include criteria used for managing stored data such as criteria concerning frequency of use of data and or criteria with respect to specific users and or other criteria such as file name file type file path requesting application expected time to re use of the data temporary storage only life expectancy of the data data type e.g. compressed encrypted de duped and or protection requirements of the data e.g. store on an encrypted tier . The policy may be applied to identify data for lifecycle management according to characteristics of entire data volumes or any portions thereof. The policy may also consider the access history effective performance or other characteristics about the data that might be utilized to optimize the performance cost availability or retention requirements of the data.

After the step processing proceeds to a step where the data for which characteristics have been determined is managed according to the policy and based on the characteristics of the data. For example data that is frequently used may be moved to a relatively fast storage device whereas data that has not been used over a certain period of time may be moved to a relatively slow storage device according to the data processing as discussed elsewhere herein. As noted herein the data that is moved may be entire data volumes or portions thereof. As discussed elsewhere herein it may be possible to provide fairly sophisticated analysis to determine whether to promote data move to a relatively faster storage device or demote data move to a relatively slower storage device .

After the step processing proceeds to a test step where it is determined if another policy with other criteria should be applied to the stored data being managed. If an additional policy is to be applied then processing proceeds to the step . If no further policies are to be applied then processing proceeds to a test step where it is determined whether there is more data to be managed according to the system described herein. If there is further stored data to manage then processing proceeds back to the step . If no further stored data is to be managed then after the test step processing is complete. In some cases tracking avoiding and resolving conflicting priorities would be handled to ensure that two policies do not create a ping pong effect moving data blocks up and down in a never ending cycle.

As discussed elsewhere herein the data devices may be associated with physical storage areas e.g. disk drives tape solid state storage etc. having different characteristics. In various embodiments the physical storage areas may include multiple tiers of storage in which each tier of storage areas and or disk drives that may be ordered according to different characteristics and or classes such as speed technology and or cost. The thin devices may appear to a host coupled to the storage device as a logical volume logical device containing a contiguous block of data storage as discussed herein. Each thin device may correspond to a particular data device a portion thereof and or multiple data devices. Accordingly each thin device may map to storage areas across multiple storage tiers. As a result although each thin device may appear as containing a logically contiguous block of storage each thin device may allow for blocks of data to be transparently stored and or retrieved from discontiguous storage pools made up of the varying classes of data storage devices. In this way the granularity at which the system for tiered storage described herein operates may be smaller than at the file level for example potentially as small as a single byte but more practically at the granularity of a single logical block or collection of sequential data blocks. A data block may be of any size including file system or database logical block size physical block track or cylinder and or other size. Multiple data blocks may be substantially the same size or different sizes such as different size data blocks for different storage tiers or different sized data blocks within a single storage tier.

The thin device may map to different storage areas devices across multiple tiers. As discussed elsewhere herein the granularity of the system may be less than at the file level and allow for blocks of data of any size to be stored across multiple storage tiers of the storage device in a process that is transparent to the host and or host application. For example in the illustrated embodiment the thin device may map blocks of data to storage areas devices such as a storage area in the pool of storage of the top storage tier a storage area in the pool of storage of the next storage tier storage areas in pool of storage of the next storage tier and storage areas in the pool of storage of the next storage tier . As discussed elsewhere herein the last storage tier may include external storage and the system described herein may map to a storage area in the pool of storage in the tier .

Mirroring backup of data may also be facilitated by tiered storage across multiple tiers according to the system described herein. For example data that is accessed frequently may be stored on a fast storage device tier 0 while a mirrored copy of the data that is not expected to be accessed may be stored on a slower storage device in one or more other tiers e.g. tiers 1 4 . Accordingly the same data may be stored on storage devices of multiple tiers of storage pools.

In an embodiment herein a write target policy may be applied to data that is being written according to the system described herein. For example data that is expected to be used frequently for example database indices may be initially written directly to fast storage e.g. tier 0 flash SSD storage whereas data that is not expected to be accessed frequently for example backup or archived data may be initially written to slower storage devices e.g. tier 4 MAID or external storage . In this manner data is efficiently stored by targeting the write to storage areas and devices according to the estimated or expected access frequency of the data beginning with the initial write of the data and also applying to subsequent data writes that jump across multiple tiers.

The process for determining the appropriate target storage location of the write of the data may be made based on the logical unit number LUN ID of the device from which the data is being written where the storage device may have or obtain information about the types of data stored on specific logical units. Alternatively additional policies and capabilities may be enabled by adding host resident extension software for example to tag I O requests with information about the requesting application or user so that the determination may be made based on other information provided by the host and or entity accessing the storage device e.g. a target policy indicator provided with each write or class of writes . Other possible criteria include the time of day the size of the incoming write operation e.g. very large sequential writes vs. smaller random writes file name file type host OS type data type access patterns inter dependent accesses to other data etc. It is also possible that hints from the host could also be used particularly relating to performance and availability requirements of the data etc.

The system described herein may include autonomic promotion and demotion policies to facilitate optimization of performance storage availability and power. For example a least recently used LRU policy may be used to demote data blocks in order to pro actively make room for new writes of data blocks and or promotions of data blocks within the system. A most frequently used MRU policy may be used to promote data blocks that are frequently used to faster storage tiers. Predictive policies may be used to recognize that data blocks that will be needed before they are actually needed and promote the data blocks accordingly for example nightly batch jobs etc. . Alternatively the system described herein may include an application programming interface API that allows a hosts users applications to inform the storage that certain blocks should be promoted or demoted to different tiers. Note that promotion and demotion may relate to a relative ordering of tiers where promotion refers to moving data to physical storage that is relatively faster and demotion refers to moving data to physical storage that is relatively slower.

Other special purpose policies may also be used. As discussed elsewhere herein mirroring of data blocks across multiple tiers may be used. For example for frequently used data blocks one copy may be written to flash SSD memory at a top storage tier and a second copy mirrored to another storage tier e.g. tier 3 or tier 4 . Another policy may include promoting and or demoting a data block but not deleting the data block from its pre promoted or demoted location until the data block is modified. This policy offers advantages including when subsequently demoting the block if unmodified a copy may already exist on a slower storage tier and an additional copy does not need to be made only the copy on the faster storage tier deleted . When a data block is modified the previous copy on a different storage tier may be deleted.

Other policies may include manual or automatic pre promotion and post demotion policies. For example blocks may be promoted in the background immediately before batch runs e.g. billing runs etc. . Additionally writes for such processes as back ups may required the fastest possible write but never or only infrequently read. In this case writes may be written to a top storage tier and immediately scheduled for demotion to a lower storage tier. With MAID storage data blocks that are rarely or never used may be consolidated onto individual spindles that may then be powered off providing a reduction in power consumption for storage of data blocks infrequently accessed. Further sequential contiguous blocks may be coalesced and relocated in an optimization process that may include other advanced strategies including aligning indices near to data being indexed. It is also possible to have a de duplication policy in which nothing is deleted from storage in a de dup tier. Data blocks in storage pools of a de dup storage tier may be promoted to fast storage tiers as needed but block and index metadata in the de dup storage may be maintained even if a data block is promoted to a faster storage tier and modified or deleted. Maintenance of de dup storage tiers may involve use counters and other mechanisms that may be used with known data cleaning processes such as garbage collection etc.

In an embodiment herein one or more policies may be provided to guarantee a particular level of performance and or that a particular percentage of data is to be provided on storage having certain characteristics e.g. speed . The policy may guarantee a minimum number for I Os per second IOPS and or may guarantee greater than a minimum TOPS for any I O operation.

After the step processing proceeds to a step where the determined information associated with the data is processed according to the target policy and the data block is written to a storage location in the storage device according thereto. Accordingly the data block may initially be written to a storage area device in a pool of storage of a storage tier corresponding to the anticipated frequency of use of the data block and or according to other criteria. After the step processing proceeds to a step where information concerning the location of the data block is updated in a table of information in the thin device as further discussed elsewhere herein. After the step processing is complete.

In some cases there may be insufficient available free space to write data to the storage tier corresponding to the storage policy at the step . This may be addressed in a number of ways. One possibility is to maintain the data in cache memory until space becomes available which can occur when data is moved from the target tier as a result deletion of promotion demotion based on storage policies. Note also that it is possible to temporarily store the data in a lower tier and then schedule the data for promotion to the appropriate tier using any appropriate mechanism such as setting a flag that causes the data to be promoted before any other data.

If it is determined at the test step that the storage tier is full then control passes from the test step to a step where wait processing is performed. The wait at the step could be for any appropriate amount of time. Following the step control passes back to the test step for a new iteration.

If it is determined at the test step that the storage tier is full then control passes from the test step to a step where the data is written to a different storage area such as a lower or higher storage tier or to global memory of the storage device e.g. cache memory as further discussed herein. The data may be placed in the different storage area temporarily. Following the step is a step where the data is scheduled to be moved to the appropriate storage area the originally destined storage tier . Following the step processing is complete.

In an embodiment at the step the write data may be temporarily stored in a global memory such as the global memory until storage space in the particular requested tier becomes available that is sufficient to handle the write request. At the step scheduling for the movement of the data may include relocating data in the particular requested tier e.g. faster storage tier to a lower tier e.g. slower storage tier to make memory available for the data temporarily stored in the global memory. In another embodiment at the step data for the requested write may be immediately written to a lower tier than that requested for the write and at the step a future promotion of the data to the particular requested higher tier originally destined storage tier may be scheduled. The embodiments discussed herein provide for the dynamic re allocation and re ordering of data to accommodate write policies usage patterns and the like.

At the step data blocks are to be promoted and or demoted according to the one or more policies. If a data block is promoted the data block is moved to a storage area device in a pool of storage of a higher storage tier for example faster storage. If a data block is to be demoted the data block is moved to a storage area device in a pool of storage of a lower storage tier for example slower storage. As further discussed elsewhere herein in some cases the promotion and or demotion procedure may include moving copies of data blocks to other storage tiers and the deleting the old data blocks from their original storage location and or copies of data blocks previously stored at the subsequent storage tiers may be used and movement of the data block is to make the previously stored version of the data block become again the current accessible data block.

After the step processing proceeds to a step where information concerning the location of the data block is updated in a table of information in the thin device as further discussed elsewhere herein. After the step processing proceeds to a test step where it is determined whether additional stored data is to be managed according to the system described herein. If more stored data is to be managed promoted demoted processing proceeds back to the step . Otherwise processing is complete. Note that data access may be guaranteed even while data is being moved from one tier to another.

In principal the advantages of a multi tiered configuration increase as the size of the storage portions for which optimal tier selection are performed decreases. However the use of smaller portions may result in the need to determine optimal placement for a significant number e.g. billions of regions. Thus there are challenges presented in connection with scaling a tiering system for a relatively large storage area. In the system described herein the overall process of optimizing tier usage is divided into a movement policy determination operation and an operation that applies the movement policy moves the storage regions to the optimal tier for that region per the movement policy in such a way that the movement policy determination operation can if desired be executed on processing hardware separate from that used by the movement policy application operation. For example in some embodiments the movement policy application operation may execute on a storage array containing the data and the movement policy determination operation may execute on a processing complex that is separate from the storage array so that the movement policy determination operation can take advantage of larger processing and or memory capacity than is available on the storage array. The system described herein provides that the movement policy may specify the preferred tier for a region of storage as a statically defined function of the host access rate or other metrics for the storage region. This allows a given instance of the movement policy to have longer useful life than a movement policy that specifies the preferred tier of a region of storage simply as a function of the logical address of the region since with the system described herein the preferred tier for a given region can change as a result of a change in the host access rate or other metrics for the region even without a change to the movement policy. Furthermore such a movement policy enables embodiments in which the movement policy determination operation can be performed without needing an input indicating which tier a given storage region is currently mapped to. Note that the movement policy determination and movement policy application operations may in turn be implemented in a distributed fashion in a given embodiment and as will be discussed herein may be optimized to minimize execution time and to minimize the adverse impacts of storage movement on storage system performance.

As discussed elsewhere herein policies may be used to determine when to promote data map the data to a relatively faster tier and when to demote data map the data to a relatively slower tier . In an embodiment herein this may be performed by first determining a score for different portions of a storage space based on relative activity level and then constructing promotion and demotion histograms based on the different scores and the frequency of each. The policy may then be applied to each of the histograms to determine which data is promoted and which data is demoted. This is explained in more detail below.

In an embodiment herein each of the storage portions may correspond to an extent where each extent corresponds to forty eight sub extents and each sub extent corresponds to ten chunks and each chunk corresponds to twelve tracks so that an extent includes four hundred and eighty chunks five thousand seven hundred and sixty tracks . Note also that for the same logical device different ones of the extents may have different sizes. Note also that the storage space may be one or more of a single logical device a group of logical devices all logical devices corresponding to particular physical space logical devices corresponding to a particular application or set of application etc.

Referring to a histogram illustrates a plurality of activity bins buckets and the frequency thereof. Each vertical line of the histogram represents a bin corresponding to a number of storage portions e.g. extents having the corresponding score. Determination of a score for a storage portion is discussed in more detail elsewhere herein. In an embodiment herein there are five thousand bins. Of course a different number of bins may be used instead.

The height of each bin represents a number frequency of storage portions having a particular score. Thus the longer a particular vertical line the more storage portions there are having the corresponding score. Note that the sum of all of the frequencies of the histogram equals the total number of storage portions being considered for promotion in the particular iteration. Note also that the sum of frequencies of a portion between a first score and a second score equals the total number of storage portions having a score between the first and second scores.

The histogram also shows a first range indicator and a second range indicator . The first range indicator corresponds to bins having a score from S1 to SMAX the maximum score . The second range indicator corresponds to bins having a score of S2 to S1 1. In an embodiment herein there are three levels of physical storage and storage portions having a score corresponding to the first range indicator are promoted mapped to a highest fastest level of storage and storage portions having a score corresponding to the second range indicator are promoted mapped to a medium level of storage. The remainder of the portions i.e. that do not correspond to either the first range indicator or the second range indicator are not changed are not promoted based on the histogram . Of course it is possible to have any number of storage levels.

In an embodiment herein it may be possible to inhibit thrashing by only promoting a certain percentage of data that is above a particular threshold e.g. promote the top 95 of data above the threshold while leaving the remaining 5 of the data at its current tier. The remaining 5 may still be processed as if it had been promoted e.g. not considered for demotion in certain cases see discussion elsewhere herein .

Referring to a flow chart illustrates steps performed in connection with promoting mapping storage portions to various levels. Processing begins at a first step where a histogram is constructed like the histogram discussed above. Scoring the storage portions to construct the histogram is discussed in more detail elsewhere herein. Following the step is a step where storage portions are promoted to a highest level of storage using the histogram constructed at the step . The specific processing provided at the step varies according to the specific policy that is used. However generally it is possible to select up to a specific number of storage portions having a highest score where the specific number is based on the policy e.g. 85 of available space at a particular level . Processing provided at the step is described in more detail elsewhere herein. Other policies may include a particular percentage of all space used for the storage space data having a particular score possibly up to some limit of the amount of data etc.

Following the step is a step where storage portions are promoted to intermediate level s of storage in a manner similar to promoting to the highest level of storage described elsewhere herein . Following the step is a step where data is moved if necessary to an assigned level of storage in a manner described elsewhere herein. Following the step processing is complete. Note that any number of different levels of storage is possible. In an embodiment herein there are three levels of storage a lowest level using SATA storage technology an intermediate level using Fibre Channel technology and a highest level using flash drive technology.

Referring to a flow chart illustrates in more detail processing performed in connection with the step described above. Processing begins at a first step where a variable that keeps track of the amount of storage portions AMT is initialized to zero. Following the step is a step where an index variable I is set equal to the maximum score highest bin . In an embodiment herein there are five thousand bins so I would be set to five thousand at the step . Of course other numbers of bins are also possible.

Following the step is a step where AMT is incremented by FREQ I the amount of data mapped to bin I. Following the step is a test step where it is determined if AMT exceeds a target value set by a policy. In an embodiment herein a policy may indicate a certain percentage of data to be assigned to a particular storage level e.g. 85 of flash drive capacity . As discussed elsewhere herein in other embodiments the target value may be set according to different policies and or different types of policies. In an embodiment herein the target value represents an amount of data that can be compared to AMT at the step .

If it is determined at the test step that AMT does not exceed the target value then control passes from the test step to a test step where it is determined if I is less than a floor threshold MIN . In some embodiments a floor threshold may be set to prevent relatively inactive data data with a relatively low score from being promoted to a tier that is relatively high. That is for each tier a minimum value for I may be set so that data below that minimum is not promoted to that tier. If it is determined at the test step that I is not less than the floor threshold MIN then control passes from the test step to a step where the index variable I is decremented. Following the step control passes back to the step for another iteration. If it is determined at the test step that AMT does exceed the target value or if it is determined at the test step that I is less than the floor value MIN then control passes to a step where a score threshold is set to the value of the index variable I. Data portions having a score of I or higher are promoted to the highest level of storage. In other embodiments only data portions having a score greater than I are promoted. Following the step processing is complete. Note by the way that it is possible in some instances for the physical capacity of a storage level to be exceeded in which case the threshold may be set lower than otherwise indicated by the target. In other words the test at the step may be modified to be AMT TARGET OR CAPACITY EXCEEDED .

Note that the test steps effectively set the threshold according to either the policy for a particular tier e.g. fill up to fifty percent of capacity or according to a value for I that will provide that data not promoted to a higher tier will still be serviced properly. For example if a policy provides that the highest tier may be filled with up to fifty percent of its capacity but most of the data of a particular storage group is rarely accessed has relatively low scores then the test at the step prevents data that is rarely accessed from being promoted to the highest tier.

Note that the methodology for determining score values used to assign storage portions to one or more intermediate storage levels may be similar to that described above in connection with the flow chart . In the case of intermediate storage levels though the index variable I would be initialized to a score that is one less than the lowest score of the next highest storage level. For example if storage portions having a score of 4500 to 5000 are assigned to the highest storage level then the index variable I would be initialized to 4499 in connection with determining scores for an intermediate storage level just below the highest storage level.

Referring to a histogram is like the histogram discussed above and illustrates a plurality of scores and the frequency thereof. The histogram may be used to determine which of the storage portions if any are to be demoted mapped to relatively slower physical storage . In some embodiments the histogram may be identical to the histogram . In other embodiments the histogram may be different than the histogram because the scores for the histogram used for promotion may be different than the scores for the histogram used for demotion. Determination of promotion and demotion scores is discussed in more detail elsewhere herein.

The histogram also shows a first range indicator and a second range indicator . The first range indicator corresponds to bins having a score from SMIN the minimum score to S1. The second range indicator corresponds to bins having a score of S1 1 to S2. In an embodiment herein there are three levels of storage and storage portions having a score corresponding to the first range indicator are demoted mapped to a lowest slowest level of physical storage and storage portions having a score corresponding to the second range indicator are demoted mapped to a medium level of storage. The remainder of the portions i.e. that do not correspond to either the first range indicator or the second range indicator are not changed are not demoted based on the histogram . Of course it is possible to have any number of storage levels. Note that as discussed elsewhere herein in some embodiments storage portions that have been recently marked as promoting to a highest tier are not candidates for demotion and storage portions that have been recently been marked as promoting to a middle tier are not candidates for demotion to a lowest tier.

In an embodiment herein the mechanism for demoting storage portions extents may be analogous to the mechanism for promoting storage portions. Some of the processing may be reversed so that for example the storage portions to be demoted to the lowest level of storage would be determined first and the index variable I from the flow chart of would be initially set to SMIN and then incremented on each iteration.

In some embodiments when a storage portion e.g. an extent is selected for promotion only active sub portions e.g. sub extents are promoted while inactive sub portions remain at their current storage level. In an embodiment herein a sub portion is considered active if it has been accessed in the previous 4 days and is considered inactive otherwise. Of course other appropriate criteria may be used to deem sub portions either active or inactive. In some embodiments when a storage portion e.g. an extent is selected for demotion the entire storage portion is demoted irrespective of activity level of sub portions. Note that it may be possible to promote some sub extents of a super extent and then demote the remaining sub extents of the same super extent since whether data is active or inactive may be determined on a sub extent level. Note also that in the particular embodiment illustrated herein a past state tier of the data is not factored in to a decision to promote or demote data.

In some cases it may be desirable to minimize the amount of data that is demoted. A mechanism for doing this may take into account the capacity and amount of data that has been placed onto the higher tiers and set the demotion threshold lower so less data is demoted if the amount of promoted data is less than the capacity or specified percent of capacity of the higher tiers. For example if the policy indicates a desire to fill the higher tiers within fifty percent of capacity but the promotion portion of the algorithm has only promoted data so that the higher tiers are thirty percent full the demotion threshold may be set lower so that less data is demoted.

Referring to a flow chart illustrates steps performed in connection with creating the histograms . Processing begins at a first step where an index variable I is set to one. The index variable I is used to iterate through the storage portions e.g. extents of a storage space. Following the step is a test step where it is determined if I is greater than MAX a maximum value for I e.g. the number of extents of the storage space . The test at the step determines if all of the storage portions of a particular storage space have been processed. If not then control transfers from the test step to a step where the raw promotion score and the raw demotion scores are calculated.

The raw promotion score and the raw demotion score reflect an amount of I O activity for a particular extent. Any appropriate mechanism may be used to calculate the scores. In an embodiment herein the raw promotion score is provided by the formula 123456 Active Subext 1 where s rrm is the rate of short term random read misses s w is the rate of short term writes s p is the rate of short term pre fetches l rrm is the rate of long term random read misses l w is the rate of long term writes and l p is the rate of long term pre fetches for a given extent. The coefficients p1 p6 may be set as appropriate. In an embodiment herein the values used may be 12 4 4 3 1 and 1 respectively. Of course different values may be used to emphasize or deemphasize different I O characteristics in connection with determination of the promotion raw score. In an embodiment herein the different short term and long term rates my be provided using the mechanism described in U.S. patent Ser. No. 12 924 396 filed on Sep. 27 2010 and titled TECHNIQUES FOR STATISTICS COLLECTION IN CONNECTION WITH DATA STORAGE PERFORMANCE which is incorporated by reference herein. Of course any appropriate technique may be used for collection of the statistics used herein.

The demotion raw score may be determined using the following formula 456123 where s rrm s w p1 etc. are as set forth above.

Following the step is a step where the promotion bucket index and the demotion bucket index are both calculated. The indexes are used to add data to the histograms . Determination of the bucket indexes is discussed in more detail elsewhere herein. In some embodiments the promotion raw score may be multiplied by a priority factor e.g. one two or three prior to obtaining the bucket index. The priority factor may be used to give higher priority i.e. increase the likelihood of promotion for some of the storage possibly selected by a user or by a policy. For example important operations in an organization may be assigned a higher priority so that storage associated therewith is provided with a priority factor of two or three or some other value .

Following the step is a test step where it is determined if the promotion and demotion bucket indices determined at the step are the same as the promotion and demotion indices determined for the most recent extent or set of extents. If so then control passes from the test step to a step where the current extent being processed is added to a super extent data element for the most recent extent or set of extents. The super extent represents data for a number of contiguous extents having the same promotion and demotion indices. The super extents are provided to increase efficiency and decrease the amount of storage needed. Note that other criteria may be used to combine information for contiguous extents.

If it is determined at the test step that the promotion and demotion bucket indices determined at the step are the same as the promotion and demotion indices determined for the most recent extent or set of extents then control passes from the test step to a step where a new super extent is created. Adding to an existing super extent at the step and creating a new super extent at the step are both discussed in more detail elsewhere herein. Following the step and following the step is a step where the index variable I is incremented. Following the step control transfers back to the test step for another iteration.

If it is determined at the test step that I the index variable used to iterate through the storage portions e.g. extents is greater than a maximum value the number of extents being processed then control transfers from the test step to a step where a delay is introduced. Following the step control transfers back to the step to reprocess the extents of a data storage space to reconstruct the histograms .

The amount of delay at the step represents the cycle time for repeatedly reconstructing the histograms . The delay may be a constant and or may be adjustable depending upon the amount of time spent performing other processing associated with promotion and demotion of data. In some embodiments the delay may be set so that the histograms are recalculated every ten minutes. It is also possible to keep track of instances where the algorithm does not complete in a certain amount of time e.g. ten minutes . In such a case a counter could be incremented each time the algorithm does not complete and decremented when it does. If the counter reaches a certain value e.g. ten the system may operate in a degraded mode indicating that data tiering is not being performed properly.

Referring to a flow chart illustrates steps performed in connection with providing values for converting the raw promotion and demotion scores into promotion and demotion indices buckets . In an embodiment herein each of the buckets bins has a lower boundary of a raw score that falls into a particular bucket. Thus for example given a bucket I a raw score will map to bucket I if the raw score has a value between the lower boundary of bucket I and one less than the lower boundary of bucket I 1. The following describes how the lower boundary values are set for the bins and thus describes how to map raw scores to particular histogram buckets bins . Accordingly the processing performed at the step discussed above where raw scores are mapped into particular buckets involves finding a particular bucket where the raw score falls between the low boundary thereof and the lower boundary of the next bucket.

Processing begins at a first step where I an index variable is set to one. The index variable I is used to iterate through all of the buckets bins . Following the step is a test step where it is determined if I is greater than NBUCKETS the number of buckets histogram values used by the system. In an embodiment herein NBUCKETS is five thousand although other values may be used. If it is determined at the step that I exceeds the number of buckets then process is complete. Otherwise control transfers from the step to test step where it is determined if I is greater than a pivot value. In an embodiment herein a linear scale is used for setting a lower boundary for buckets below the pivot value and a logarithmic scale is used for setting the lower boundary for buckets above the pivot value. Determination of the pivot value is discussed in more detail elsewhere herein.

If it is determined at the test step that I is not greater than the pivot value then control passes from the test step to a step where a linear scale is used for setting the lower boundary of bucket I. In an embodiment herein the lower boundary is set equal to I the bucket number at the step but of course other mappings are possible. If it is determined at the test step that I is greater than the pivot value then control passes from the test step to a step where a logarithmic mapping is used. In an embodiment herein the following formula is used lower boundary exp log pivot value logperbucket pivot value where logperbucket equals maxlog minlog numbuckets pivot value 2 maxlog log max raw score minlog log pivot value and numbuckets is the total number of buckets. In an embodiment herein numbuckets is five thousand and max raw score is 4 800 000. Of course other values may be used.

Following the step or the step is a step where I is incremented. Following the step control transfers back to the step for another iteration.

As discussed elsewhere herein determining the low boundary for each of the buckets allows mapping the raw scores into particular buckets at the step discussed above. A raw score maps to a particular bucket when the raw score is greater than or equal to the low boundary and when the raw score is less than the lower boundary of the next higher bucket. The processing illustrated by the flow chart constructs a table used to map raw promotion and demotion scores into buckets. The mapping may be performed using a binary search of the table.

Referring to a diagram illustrates a data structure that may be used for storing metadata for a super extent. The data structure includes an entry for the promotion bucket index for the super extent and an entry for the demotion bucket index for the super extent. The data structure also includes an entry for the number of extents in the super extent.

As discussed elsewhere herein an extent may include a number of sub extents. In an embodiment herein there are forty eight sub extents for each extent. Some of the sub extents may be active i.e. have been accessed within a particular amount of time . In an embodiment herein a sub extent is considered active if there has been at least one I O operation thereon within the last 4 days and is considered inactive otherwise. The data structure includes a field indicating the average number of active sub extents for all of the extents of the super extent. The value of the average number of active sub extents field is provided by total number of active sub extents number of extents 

The data structure also includes a flag field that indicates whether data for a particular super extent was recently promoted or demoted and to which tier. In some embodiments it is possible to use the flag field to decide to add a particular super extent to a histogram and or whether to demote or promote data corresponding to a particular super extent. For example in an embodiment herein data that had been recently promoted to the first or second highest level according to the flag field is not considered for demotion to the lowest level and data that been recently promoted to the highest level according to the flag field is not considered for demotion at all. Note that handling promotion first followed by demotion may be considered part of the policy. The data structure may also include other information. In an embodiment herein the flag indicates whether a corresponding data portion had been promoted or demoted in connection with the previous histogram e.g. the most recent iteration . Thus after running the promotion algorithm the flag may be used to eliminate from the demotion histogram any data that had just been promoted as described above.

Referring to a flow chart illustrates steps performed in connection with creating a new super extent at the step described above. Processing begins at a first step where space is allocated for the metadata for the super extent. In an embodiment herein the super extent metadata may be provided in the global memory although other storage locations are possible provided that the metadata is accessible to provide the processing described herein.

Following the step is a step where the promotion bucket index is set to the value determined at the step described above. Following the step is a step where the demotion bucket index is set. Following the step is a step where the number of extents field is set to one. Following the step is a step where the value for the average number of active sub extents field is set according to the number of active sub extents for the extent and where the number of allocated chunks for the super extent is set. Following the step is a step where the flag field is initialized e.g. cleared because the data corresponding to the new super extent had not been recently promoted or demoted i.e. marked according to the flag as having been promoted or demoted in the previous operation . Following the step processing is complete.

Referring to a flow chart illustrates steps performed in connection with adding an extent to an existing super extent at the step described above. Processing begins at a first step where the number of extents field is incremented. Following the step is a step where the average number of active sub extents field is adjusted to account for the active sub extents in the extent being added. Following the step is a step where the flag field is modified e.g. cleared . Following the step processing is complete.

Referring to a flow chart illustrates steps performed in connection with determining the pivot value. As described elsewhere herein the pivot value is used to determine when to use a linear scale to set the lower boundary for a bucket index and when to use a log scale to set the lower boundary. It is desirable to set the pivot value so that the log scale does not cause the same lower boundary value to be provided for adjacent buckets. In an embodiment herein the pivot value is set so that the difference between adjacent lower boundary values is at least one but a different value for the desired difference may also be used.

Processing for the flow chart begins at a first step where a variable DIFF is set to zero. The DIFF variable is used to keep track of the difference between the lower boundaries of adjacent buckets as the pivot value is increased. Following the step is a test step where it is determined if DIFF is less than one. If not then processing is complete. Otherwise control transfers from the test step to a step where the pivot value is calculated using the formula pivot value 1 1 multiplier 1 where multiplier equals exp logsperbucket and where determination of logsperbucket is described above. For the initial determination of logsperbucket prior to first performing the step it is assumed that pivot value is one so that the initial value of minlog is zero.

Following the step is a step where minlog is recalculated. As discussed elsewhere herein minlog is the log of the pivot value. Following the step is a step where logsperbucket is recalculated. Determination of logsperbucket is discussed elsewhere herein. Following the step is a step where multiplier which equals exp logsperbucket is recalculated. Following the step is a step where DIFF is recalculated using the formula DIFF pivot value multiplier pivot value

In some embodiments it is possible to have multiple independent storage groups that share the same physical storage space where each storage group has its own independent set of thresholds used for tiering data. Note also that since it is not necessary to provide absolute comparison of statistics of all of the data stored in physical storage space the system could be implemented with multiple independent processes possibly executed by different processors that determine histograms and provide tiering for different storage groups. Constructions of the histograms may be independent of each other and may be independent of processing that uses the histograms to promote and demote data.

Of course an appropriate technique may be used to determine which data is provided on which storage tier. In addition there may be a number of storage tiers different than the three tiers illustrated herein.

As discussed elsewhere herein in some embodiments three tiers of data storage may be provided using three different types of data storage devices SATA drives for the slowest and least expensive storage flash drives for the fastest and most expensive storage and Fibre Channel FC drives for storage that is in between the SATA and the flash drives with respect to both cost and speed. However in some cases the FC drives may not be available. Accordingly it may be desirable to provide three tiers of data storage while only two different types of storage devices are available. Generally it may be desirable to provide N tiers of data storage while having less than N different types of storage devices.

A SATA disk drive includes a disk platter containing the data and a drive head that reads and writes data as the disk platter spins past. The drive head is mounted on a disk arm that positions the head radially with respect to the disk platter according to the location of data on the disk platter. Read and write times and thus performance of the drive is a function of the time required to move the drive head to the correct location as well as the time required to spin the disk do that the data is accessible by the drive head. Accordingly performance of a SATA drive and or many other types of disk drives may be enhanced by reducing the time for drive head movement and or the time for spinning the disk platter.

Referring to a chart shows a plot of average read write response time for a disk drive vs. the location of data on the disk platter. The vertical axis of the chart represents response time while the horizontal axis represents location of the data on the disk platter where the far left corresponds to data located in the outer cylinders of the platter while the far right represents data located near the center of the platter. A line corresponds to an average response time for all of the data on the disk drive.

The chart represents response times when data is randomly distributed throughout the disk platter. Nonetheless the response time is less than average for a portion of the plot corresponding to data stored in a middle section of the disk platter i.e. a section between the inside cylinders and the outside cylinders of the platter . Note that for data stored in the middle section of the platter the maximum distance and the average distance travelled by the disk arm is less than the maximum and average distance the disk arm travels for data stored at either the outer cylinders or the inner cylinders.

Referring to a chart shows a plot of average read write response time for a disk drive vs. the location of data on the disk platter. The chart is similar to the chart . A line corresponds to an average response time for all of the data on the disk drive. The response time is less than average for a portion of the plot corresponding to data stored in a middle section of the disk platter.

The chart is different from the chart in that the portion of the plot shows response times significantly lower than the average response time when compared with the portion of the plot . That is the difference between the average response time and the response times for the portion of the plot is significantly greater than the difference between the average response time and the response times for the portion of the plot . Unlike the plot where the data is distributed randomly on the disk platter the plot represents data being distributed on the platter so that more frequently accessed data is stored in the middle section of the platter while less frequently accessed data is stored at the inside and outside cylinders of the platter.

Note that the average distance travelled by the disk arm is decreased for data stored in the middle section of the disk platter when it becomes more likely that the disk arm is travelling from a location in the middle section of the platter. That is providing frequently accessed data in the middle section of the platter causes the disk arm to travel less by increasing the likelihood that the arm will travel from a location in the middle section of the platter to a relatively close location in the middle of the platter.

Accordingly it is possible to use a single type of storage to provide multiple classes of storage having different response times. In an embodiment herein conventional SATA disk drives may be used to provide two tiers of storage a first relatively fast tier having data stored in the middle section of the disk platter and a second relatively slow tier having data stored at the inner and outer cylinders of the disk platter. In such a case a three tier system may be provided without using fibre channel drives by using flash drives for the fastest tier and SATA drives for the medium speed and slowest tiers. Alternatively an N tier system may be provided using a single type of disk drive e.g. a SATA drive that stores data for the different tiers on different portions of the disk drive. In one embodiment different sets of the same type of disk drive e.g. SATA may be used where one set is used for the relatively fast tier and the other set is used for the relatively slow tier. In another embodiment a single set of disk drives may be used for storing both tiers of data. In yet another embodiment some disk drives are used to store only one of the tiers of data while other disk drives are used to store both tiers of data where substantially all of the disk drives are the same type. This is explained in more detail elsewhere herein.

Referring to a diagram illustrates an embodiment where different tiers of data are stored on different sets of disk drives. A first plurality of disk drives may be used for a relatively slow tier e.g. the slowest tier in the embodiment disclosed herein while a second plurality of disk drives may be used for a faster tier e.g. the medium speed tier in the embodiment disclosed herein . All of the disk drives may be the same type of hardware e.g. SATA disk drives having a particular rotation speed or may be different types of hardware. The disk drives may have similar performance characteristics. As explained in more detail elsewhere herein differences between the sets of drives is the result of where data is stored on the disk platter.

The shaded portion shows that data is stored on inside and outside cylinders of the disk platter for the plurality of disk drives . As discussed elsewhere herein storing data on the inside and outside cylinders of a disk platter increases the response time of a disk drive. For the plurality of disk drives the shaded shows that data is stored in the middle section of the disk platters. As discussed elsewhere herein storing data in the middle section of a disk platter tends to reduce response time of a disk drive.

Note that for the plurality of disks for the relatively slow tier it is not necessary to restrict data storage to the inside and outside cylinders. Instead in some embodiments it may be acceptable to randomly distribute data on the platter to provide a response time performance like that illustrated in the chart discussed elsewhere herein. That is in some embodiments the slowest tier uses the entire platter in a random fashion rather than restricting data to the inside and outside cylinders of the platter. Note also that for different types and configurations of disk drives the slowest portion longest response time and the fastest portion quickest response time may be different. For example in some cases the fastest portion of the drives may be the outside edge of the disk platters. Accordingly the system described herein may be generalized so that it is understood that the disks have data stored on a slowest portion thereof while the disks have data stored on the fastest portion thereof.

Referring to a diagram shows a plurality of disks having some data provided at inner and outer cylinders of the disk platters and other data provided at the middle section of the disk platters. Data for a relatively slow tier is provided at the inner and outer cylinders while data for a faster tier is provided in the middle section. Unlike the embodiment illustrated by the diagram discussed above the same disk drives are used for multiple tiers of data.

Note that it is possible to have more than two tiers of data on one or more of the disks . In an embodiment herein a fastest tier is provided at a middle portion of the disks a second fastest tier is provided in portions adjacent to the fastest portion a third fastest tier is provided in portions adjacent to but further from the middle from the second fastest portion etc.

Referring to a diagram illustrates an embodiment where different tiers of data are stored on different sets of disk drives for some of the disk drives while different tiers of data are stored on the same disk drive for other ones of the disk drives. A first plurality of disk drives may be used for a relatively slow tier e.g. the slowest tier in the embodiment disclosed herein . The shaded portion shows that data is stored on inside and outside cylinders of the disk platters for the plurality of disk drives . However as discussed elsewhere herein it is also possible to store data randomly on platters for the disk drives .

A second plurality of disk drives may be used for a faster tier e.g. the medium speed tier in the embodiment disclosed herein while a third plurality of disks may have some data provided at inner and outer cylinders of the disk platters and other data provided at the middle section of the disk platters. All of the disk drives may be the same type of hardware e.g. SATA disk drives having a particular rotation speed .

Referring to a flow chart illustrates processing performed in connection with determining where to store data in connection with the system described herein. Processing begins at a first step where it is determined if the data is being stored for a relatively high tier e.g. the medium speed tier discussed in connection with the system described herein . If so then control transfers from the test step to a step where the data being stored is placed in a middle section of a disk platter of a disk drive storing the data. Following the step processing is complete.

If it is determined at the test step that the data being stored is not for a relatively high tier e.g. is for the slowest speed tier discussed in connection with the system described herein then control passes from the test step to a step where the data is stored in the inner or outer cylinder of the platter of a disk drive. Following the step processing is complete. As discussed elsewhere herein it is possible to randomly store data for the slowest tier on a device rather than restrict storage to the inner and outer cylinders. This is illustrated by an alternative path from the test step to a step where data that is not for a high tier is stored randomly. Following the step processing is complete.

Although the system described herein is used for storing data on different storage tiers it may also be used in any situation where it is desirable to provide different storage performance characteristics using the same storage hardware. Thus for example the system described herein may be used where different users are provided N different levels of storage performance even though there are less than N different types of storage hardware.

The above noted steps and other steps described herein may be performed automatically by the system described herein. For example steps described herein may be performed periodically at designated times and or after particular trigger events such as receipt of a write request access by a particular user to the system log in and or log out after assessment of space usage on the disk drives for example space usage on the fast disk drive and or after some other suitable I O action. Alternatively the steps described herein may be activated manually by a user and or a system administrator. In an embodiment the system described herein may include a system having at least one processor that performs any of the above noted steps. The ordering of the steps may be changes in certain instances as appropriate. Further computer software stored in a computer readable medium non transitory computer readable medium may be provided according to the system described herein including executable code for carrying out any of the steps and processes described herein.

Other embodiments of the invention will be apparent to those skilled in the art from a consideration of the specification or practice of the invention disclosed herein. It is intended that the specification and examples be considered as exemplary only with the true scope and spirit of the invention being indicated by the following claims.

