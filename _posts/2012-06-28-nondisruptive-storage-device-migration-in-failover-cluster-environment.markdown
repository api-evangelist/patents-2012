---

title: Non-disruptive storage device migration in failover cluster environment
abstract: A method of performing data migration from a source storage device to a target storage device in a failover cluster includes use of a roll-forward flag to signal successful completion of a migration operation from a migration node to failover nodes of the cluster, reliably controlling host access to the target storage device to ensure that it is used only when it has been successfully synchronized to the source storage device and a commit operation has occurred that ensures that subsequent read and write operations are directed exclusively to the target storage device.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08775861&OS=08775861&RS=08775861
owner: EMC Corporation
number: 08775861
owner_city: Hopkinton
owner_country: US
publication_date: 20120628
---
The present invention relates to migration of data from a source data storage device to a target data storage device in a data processing system.

Data migration techniques are used to move or migrate data from one storage device or logical unit to another for any of a variety of purposes such as upgrading storage hardware or information lifecycle management. Generally migration involves synchronizing the source device to the target device i.e. achieving an operating state in which the target device stores the same data as the source device and then switching operation so that subsequent accesses are directed to the target device instead of the source device. Once the switching is successfully accomplished the source device can be taken out of service or put to some other use.

Non disruptive migration is performed while there is ongoing application level accesses to the source storage device. In non disruptive migration there are two parts to achieving synchronization the data has to be copied from source to target and any application write operations occurring while the copying is in progress have to be synchronized with the copying and in general cloned i.e. sent to both source and target devices. Example descriptions of non disruptive migration can be found in the following US patents whose entire contents are incorporated by reference herein 

Clustering is a technique used in multiple node computer systems to provide certain desirable functionality and characteristics from the perspective of external users. One such characteristic is high availability generally achieved by providing redundancy that can be used to continue operations even in the face of generally hardware failures. Two general types of clusters are failover and parallel or shared all clusters. In parallel clusters the storage devices are allowed to be actively accessed from all nodes hosts in the cluster. Synchronization is left to the applications. In a failover cluster while all nodes can see a storage device it can only be accessed by one node at a time. Synchronization is part of the cluster failover mechanism.

It is desirable to support data migration in a failover cluster environment but providing such support can present certain challenges. Non disruptive migration involves several sensitive operations where input output I O is temporarily suspended and from which it is necessary to recover in a non obtrusive manner. The fine control over I O and the possibility of aborting and restarting at multiple steps of the process would require significant communication and coordination among the nodes of the cluster most of it needed only for the unlikely event of a failure and therefore an inefficient use of system resources.

A method is disclosed of non disruptively migrating contents of a source storage device to a target storage device in a data processing system having a set of host computers organized into a failover cluster. The hosts include one labeled as a migration node where an application is executing that is accessing the source device to be migrated and the rest being labeled as failover nodes that stand ready to begin execution of an application program executing on the migration node. The source storage device is identified by the application using a device name. The method includes creating metadata indicating that a migration operation is in progress the metadata including a roll forward flag initially being reset and also including access control data initially set to allow access to the source storage device and disallow access to the target storage device. This metadata creation may be done as part of or in conjunction with an initial setup operation at the beginning of a migration.

Subsequently the target storage device is synchronized to the source storage device and the target storage device is configured to each of the failover nodes. Synchronizing leads to a source selected operating state in which read and write operations continue to be directed to the source storage device and write operations continue to be duplicated to the target storage device.

Subsequently a commit operation is performed that causes subsequent read and write operations to be directed exclusively to the target storage device. The commit operation includes i setting the roll forward flag ii setting the access control data to disallow access to the source storage device and allow access to the target storage device and iii changing or remapping the device name to cause the application program to access the target storage device instead of the source storage device through that device name.

The migration node is operative when the source storage device is coming online i if the roll forward flag is not set to abort the migration operation and maintain the initial setting of the access control data allowing access to the source storage device and disallowing access to the target storage device and ii otherwise if the roll forward flag is set to ensure completion of the commit operation. Each failover node is operative when the source storage device is coming online and the roll forward flag is set to i set the access control data to disallow access to the source storage device and allow access to the target storage device and ii change or remap the device name to cause an application program executing on the failover node to access the target storage device instead of the source storage device. By this operation an interrupted migration can be completed if possible and otherwise aborted and the failover nodes are prevented from accessing the target storage device until either a migration operation either an initial attempt or a subsequent attempt after one or more aborts successfully completes.

For simplicity the migration of a single storage device is described. Generally in a real system more than one device can be migrated at a time and more than one application could be migrated at a time. As the node labeling is used with respect to one migration a given node may simultaneously serve as a migration node for one migration and as a failover node for another migration.

The LUNs include a source LUN S and a target LUN T participating in a migration operation by which the target LUN T functionally replaces the source LUN S in the system. It is assumed that prior to migration the source LUN S is operated as a storage resource having a resource name known to application programs executing on the hosts . A migration operation moves this resource to the target LUN T so that future accesses are directed to the target LUN T rather than to the source LUN S. Reasons for such migration of storage resources include a desire for additional capacity or improved performance or to upgrade to more current and well supported hardware for example. In some cases the source LUN S is to be removed from the system although in other cases it may be maintained and reused for other purposes.

The interconnect includes one or more storage oriented networks providing pathways for data transfer among the hosts and devices . An example of the interconnect is a FibreChannel storage area network SAN either by itself or in conjunction with Ethernet or other network components. The cluster device provides storage for clustering related data used by all the hosts . In some embodiments the cluster device may be referred to as a quorum disk. More generally the cluster employs a quorum database which may not necessarily reside on a block storage device it may reside in so called cloud storage or distributed across multiple disks. The user devices are logical units of storage allocated for more general use for example to store databases file systems etc. used by application programs executing on the hosts . Generally the devices are visible to the hosts as block oriented storage devices such as disk drives.

The application s may be conventional user level applications such as a web server database application simulation tool etc. These access data of the user storage devices using system calls as known in the art. The filter driver is a component working in conjunction with a standard device driver not shown as part of an operating system that implements the system calls reading and writing data to from the user devices as requested by the applications . The filter driver may provide specialized and or enhanced input output functionality with respect to the user devices . For example in one embodiment the filter driver may be a multipathing driver having an ability to access individual LUNs via multiple paths and it manages the use of the paths for increased performance and or availability. An example of a multipathing driver is the PowerPath driver sold by EMC Corporation.

The migration tool is a specialized application used to migrate one LUN source LUN S to another target LUN T as explained above and described in the above referenced US patents. Pertinent details of the migration process are described below. The user level part carries out higher level logic and user facing functionality. For example it may provide a command line interface or command application programming interface API for interacting with a human or machine user that exercises control over a migration process. In operation it uses the UMD to track progress and control execution under certain conditions as described more below. The kernel level part of the migration tool performs lower level operations some as extensions of kernel level operations performed by the filter driver . One example is duplication of writes used to maintain synchronization between the source LUN S and the target LUN T as described below. Another is to prevent access to the target prior to commit and the source once the transition to committed state has occurred. The kernel level part may be realized in one embodiment as an extension component having a defined interface to a basic or core set of components of the filter driver .

The cluster components are a set of components providing cluster related functionality of the host MIG. Examples of such functionality include cluster formation and maintenance membership health monitoring and failover operation the latter referring to the ability of one node a host to begin execution of an application when a separate node previously executing the application has failed. Failover includes tracking the state of all executing applications in a shared data structure on the cluster device along with mechanisms for transferring resources from a failed node to a new or failover node where application execution is continuing. Such techniques are generally known in the art. The migration component must be able to write into the KMD which is done from user space as explained above. It must also be able to change the kernel state in the kernel level portion of the migration tool at the same time explained below . The migration component may be implemented as an extension or plug in in the set of cluster components . It provides a migration aware dimension to failover cluster operation to properly handle certain operating conditions as described detail below In general as part of cluster software selected actions such as execution of selected programs routines can be configured to be performed before a resource is brought on line on a particular node. The migration cluster component is one of these actions.

A brief description of the use of the UMD and KMD is provided. Every time there is a state transition during a migration the beginning of that transition is recorded in the UMD which is on the migration host MIG. Then a series of steps specific to that transition is executed where each step is idempotent i.e. produces the same results if executed once or multiple times . Some steps involve calling into the kernel level component to change its behavior. In general when such a step happens there is also a call to update the KMD so that the next time the host reboots the kernel level component will resume the same behavior though there are specific exceptions to this. Thus the KMD is written from user space but read from the kernel when a host reboots. Similar steps need to be followed for properly setting kernel and KMD state on the failover nodes FO as well but the presently disclosed technique simplifies the times the steps have to be invoked on the failover nodes FO. Once all the steps for a state transition have been successfully executed the UMD is updated so that it shows the new state. If there s any interruption in this process the UMD is used to tell the system that there was a state transition happening and it needs to be attempted again. Because the steps are idempotent it does no harm to repeat any that were already successfully executed.

While is used to specifically describe the migration host MIG for present purposes the failover hosts FO may be similarly arranged. The failover hosts FO includes the migration component in their cluster components to write to portions of the KMD and to perform certain migration aware operations as described below. There is a separate KMD on each host private to that host . The migration tool in conjunction with the migration cluster component ensure that all the individual KMDs and the kernel states on all hosts are in the correct state.

The migration operation described herein is under control of one host identified as the migration host MIG and the failover hosts FO play a migration aware failover role with respect thereto. The labels MIG and FO are used only for purposes of describing pertinent functionality. In general a migration host MIG also serves as a failover host with respect to other hosts of the cluster . Also it is possible that multiple hosts in a cluster have an instance of the migration tool and UMD meaning that each such host plays the role of a migration host MIG with respect to migrations under its control.

It is assumed that prior to the process both the source and target LUNs S T have become configured for use by the migration host MIG and that the source LUN S is configured for use by the other hosts . Additionally it is assumed that the contents of the source LUN S are viewed as a storage resource having a device name used by the application s to identify that resource as the target of an I O operation read or write . As known in the art such a device name may be either location dependent or location independent depending on whether the name identifies a physical location of the resource. An example of a location dependent name is one that includes the name of a physical storage array that contains the source LUN S. The name itself identifies the physical location of the target of the I O operation. In contrast a location independent name is a more abstract name that at any time merely maps to a particular physical storage array so that a translation or lookup is required in order to identify the physical location of the target of an I O operation. An example of a location independent name is a pseudo device name as used in the PowerPath product such as emcpower10 . Additionally in some operating systems names are not used to map applications to their storage resources. The present description is directed primarily to systems employing location independent resource naming with identification of different processing where necessary in the case of location dependent naming.

Referring to to enter a setup state the migration tool configures the migration component of each failover host FO for the resource being migrated. An indicator or flag called a roll forward flag or RFF is created. The RFF is maintained by the cluster as an instance specific property for the migration component as are target and source device ids needed for swapping and access control . The RFF is initially in a reset or non asserted state. The RFF is a single flag that all the hosts see and that can be changed atomically with respect to all the hosts. It could be stored on the cluster device for instance. There might be an optimization that stores the KMD on the cluster device also but typically the KMD is stored on the boot device of each host accessible early in boot to correctly set up the state of the kernel level portion .

Also at this point or earlier access control data in the KMD is set to a value that disallows host access to the target LUN T. It may be that when the target device T is configured to an FO node FO that will cause the kernel migration code to read the KMD for that device and set the access control to prevent access to the target. Alternatively a call into the kernel migration code to set the kernel state to prohibit access to the target may be made at the time that kernel state is written into the KMD . The kernel migration code while it doesn t yet see the target device can put this access prohibition into place proactively.

Also at this point the target device T is configured to the FO hosts FO. This occurs after the access control setup described above to prevent access to these devices before the commit operation is invoked.

During the synchronizing state a synchronization operation is performed in which the contents of the source LUN S are copied to the target LUN T so as to make the contents of the target LUN T identical to the contents of the source LUN S. Synchronization has two components. One is copying all the existing data as of the time of initiating synchronization and the other is establishing ongoing duplication or cloning of any write operation performed on the source LUN S to the target LUN T as well. Synchronization is complete when all existing data has been copied and all newly written data continues to be duplicated. The copying of existing data may be done in any of a variety of fashions which may depend in part on the exact configuration of the system and the availability of any specialized tools or functions that support such copying. In one embodiment host based copying may be used in which the migration host MIG performs reads to the source LUN S and writes the data to the target LUN T. Other techniques may employ copying using mechanisms provided by a specialized storage controller which may be part of a storage array or subsystem and relatively independent of the hosts . Examples include Open Replicator for Symmetrix available from EMC Corporation. Write duplication is provided by the kernel level portion of the migration tool .

Once synchronization is complete at the system operates for some period with source selected meaning that reads to the storage resource continue to be directed to the source LUN S while writes are duplicated to both the source LUN S and target LUN T. Generally this phase will last only as long as necessary for the higher level control e.g. a storage administrator user to command a commit operation to transition into a committed state in which the target LUN T is used to the exclusion of the source LUN S. The source LUN S is no longer synchronized with the target LUN T which continues to have the correct current data as seen by the rest of the system. The commit operation is described in detail below.

At a cleanup operation is performed which can be used to remove all remaining metadata associating the source LUN S with the named resource. At that point the source LUN S may be removed from the system or it may be re configured for another use in the system. One important task performed during cleanup is to erase any information on the source device S that might cause it to be identified mistakenly as the storage resource that has been migrated to the target device T. Earlier in the migration access control prevents this mistaken identity . Also in connection with re use the contents of the source LUN S may be erased perhaps with replacement by a known pattern such as all zeros for security or other operational reasons. The portions of the UMD and KMD used in connection with this migration may be deleted.

The process of includes an abort path leading from the synchronizing state and source selected state back to setup . Aborting may occur by user command or by automatic operation when problems are encountered during the process. For example if either the source LUN S or target LUN T fails or otherwise becomes unavailable during the process such failure may be detected either manually or automatically and lead to aborting the migration.

A device fault is a write failure to either the source or target. Since all writes are duplicated migration can only proceed if both writes succeed. If one succeeds and the other fails migration must be aborted. In this case the migration will go into a target device faulted state at this point and the user will have to execute the abort and start over perhaps first curing whatever problem caused the fault. The copy process could also fail due to a read failure on the source or a write failure on the target. This is not a device fault but will cause the synchronization to stop. An explanation of the handling of device faults in a non cluster environment can be found in the above referenced U.S. Pat. No. 7 770 053. Device fault handling in the cluster environment may be generally similar. It should be noted though that in the non cluster environment as described in the 053 patent there is a target selected state in addition to a source selected state and in the target selected state reads are directed to the target device T instead of the source device S. When the system is shutdown unexpectedly the non selected side is faulted because there s no guarantee that all writes made it to both sides. Thus the source device is faulted if this occurs during operation in the target selected state. Also if a fault happens during normal I O then the side that fails the write will get the fault and therefore the source side is faulted when operating in the source selected state and a write to the source device fails. In contrast in the cluster environment as described herein operation proceeds directly from the source selected state to the committed state there is no target selected state. Only the target device T is ever faulted when a write fails no matter which side the write fails on.

Details of a process for name swapping performed during step can be found in the above referenced U.S. Pat. No. 7 904 681. Other processes may be employed and the nature of the process may be dictated in part by specifics of the operating system of the migration host MIG.

The process of as described above is specifically applicable to systems using location independent or pseudo device naming. In systems using location dependent or native naming steps 11 16 and 18 would be skipped and the state would be CommitedAndRedirected.

Referring to at the user level part of the migration tool ascertains whether the roll forward flag RFF is still reset not yet set . If so then the migration is aborted at . Aborting avoids potential data loss which might occur because while the host was down or the migration had failed over data may have been written to the source but not the target. If at the roll forward flag is set then at the process of transitioning to the committed state is completed as described above resulting in future I O requests being directed to the target LUN T. This action is not only safe at this point but required for data integrity because writes may have already happened to just the target device T.

If the RFF is not set but the UMD shows that a transition to the committed state has begun then it is necessary to perform a limited rollback and then abort the migration. Rollback involves undoing the steps covered in of done after updating UMD but before setting RFF .

For systems using native resource naming steps 4 9 and 11 would be skipped. Further steps not described herein are necessary on the FO nodes FO to remove redirection when the application is reconfigured.

While various embodiments have been particularly shown and described it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the scope of the invention as defined by the appended claims.

