---

title: System and method for handling storage events in a distributed data grid
abstract: A system and method can handle storage events in a distributed data grid. The distributed data grid cluster includes a plurality of cluster nodes storing data partitions distributed throughout the cluster, each cluster node being responsible for a set of partitions. A service thread, executing on at least one of said cluster nodes in the distributed data grid, is responsible for handling one or more storage events. The service thread can use a worker thread to accomplish synchronous event handling without blocking the service thread.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09621409&OS=09621409&RS=09621409
owner: ORACLE INTERNATIONAL CORPORATION
number: 09621409
owner_city: Redwood Shores
owner_country: US
publication_date: 20120502
---
This application claims the benefit of priority on U.S. Provisional Patent Application No. 61 535 238 entitled UNIFIED EVENT MODEL FOR USE IN A DISTRIBUTED DATA GRID filed Sep. 15 2011 which application is herein incorporated by reference.

A portion of the disclosure of this patent document contains material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever.

The current invention relates to data caching and accessing techniques in distributed computing environments and more particularly to handling storage events in a distributed data grid.

In the context of distributed data management systems event models are sometimes used to provide information about data changes. In a typical example a client can register a listener to listen for a particular event on a piece of data. When the data changes an event is generated and the client is informed of the event by way of a listener.

One limitation with this approach is that it is asynchronous in the sense that the event has already occurred meaning that the client has no effective means to affect the data change because the client is informed after the event has already occurred. Thus to make a change to the event the client may be forced to perform a whole new transaction to make that change. Another limitation is often the size and the scalability of the solution because the client is registered to listen to all events in what is a frequently very large data set.

Additionally various extensible hooks with different semantics and idiosyncratic forms of configuration are often presented to users as separate features. There can be little documentation about how these hooks relate to one another and it is not always clear which one is the most appropriate to use. In addition the programming model varies for each hook many of which have unique limitations.

A new solution is thus desirable one which would address or resolve the above limitations as well as provide a number of other advantages.

Described herein is a system and method that can handle storage events in a distributed data grid. The distributed data grid cluster includes a plurality of cluster nodes storing data partitions distributed throughout the cluster each cluster node being responsible for a set of partitions. A service thread executing on at least one of said cluster nodes in the distributed data grid is responsible for handling one or more storage events. The service thread can use a worker thread to accomplish synchronous event handling without blocking the service thread.

In accordance with various embodiments a system and method can handle storage events such as data partition transfer and storage entry events in a distributed data grid. The distributed data grid cluster includes a plurality of cluster nodes storing data partitions distributed throughout the cluster each cluster node being responsible for a set of partitions. One exemplary distributed data grid is an Oracle Coherence data grid. A service thread executing on at least one of said cluster nodes in the distributed data grid can be responsible for handling one or more storage events. The service thread can use a worker thread to accomplish synchronous event handling without blocking the service thread.

In accordance with various embodiments the distributed data grid system can use a server side event model which can be implemented as a server side event handling application programming interface API to support the raising and handling of server side events. Using a server side event model in a distributed data grid system is disclosed in U.S. application Ser. No. 13 462 719 filed May 2 2012 now U.S. Pat. No. 9 348 668 issued May 24 2016 entitled SYSTEM AND METHOD FOR SUPPORTING A SERVER SIDE EVENT MODEL IN A DISTRIBUTED DATA GRID which application is incorporated herein by reference in its entirety.

In accordance with an embodiment storage events can be raised within the context of storage enabled members in a distributed data grid. The storage events can be defined as a set of interfaces that correspond to the functional area in which they are raised. These storage events can provide the infrastructure for various features provided by a distributed data grid such as push replication and continuous aggregation.

As shown in the event hierarchy also includes a StorageEvent interface for a storage event. The StorageEvent interface extends directly from the Event interface. Additionally the event hierarchy includes a StorageCacheEvent interface a StorageEntryEvent interface a StorageInvocationEvent interface and a StorageTransferEvent interface for different types of storage events.

Attached is Appendix A that provides further information regarding the various aspects of the storage events throughout this disclosure. The information in Appendix A is provided for illustrational purposes and should not be construed to limit all of the embodiments of the invention.

In accordance with an embodiment the internal architecture of a distributed data grid supports raising storage events directly. The Coherence includes several components for handling the storage events a PartitionedService component a PartitionedCache component and a PartitionedCache Storage data structure.

The PartitionedService component is the base for clustered service components which is responsible for managing and handling partition events including partition transfer events. The PartitionedCache component extends PartitionedService and is responsible for handling all distributed caching requests within a storage enabled member. The PartitionedCache component is also responsible for handling requests relating to partitioned cache operations often referred to as distributed caches and ultimately asking the underlying storage to perform the requested operation. The PartitionedCache Storage component is the underlying data structure responsible for storing and managing entries on behalf of Coherence. Coherence can support various storage implementations and the Storage component facilitates this functionality by providing an abstraction layer between the underlying storage implementations and the PartitionedCache service.

The Coherence includes various methods for handling storage events and supporting the data partition transfer. These methods include 

Additionally the Coherence can include different methods and components for supporting continuation. These methods and components include DispatchUEMevent in which a message component can be posted back on to the service so that a worker thread can pick up the request to offload it from the service thread DispatchTransferDeparting DispatchTransferArrived PostDeparted and PostArrived.

Data partitions can be transferred between the data storage nodes of the distributed data grid. During a partition transfer an internal transfer request can be sent from one server node to another along with the data that is to be transferred. Upon receiving the request the recipient can store the data and inform the other cluster nodes that it is now responsible for storing that particular data partition.

Event interceptors can be injected in this critical path of the partition transfer so that events can be fired and actions can be inserted during the partition transfers. Additionally the event interceptors can follow a synchronous model which delays the partition transfer until the event interceptors have completed processing allowing the event interceptors to complete their functionality before the partition transfer is completed.

In one example the transferred data partition may contain information that relates to a trading account pending several trades each of which changes the total amount of the trading account. The system can ensure that all trades are fully processed before the data partition transfer is committed in order to avoid data corruptions of the total amount in the trading account.

In another example a query can be performed over all the data in the data grid for the purpose of continuous aggregation. The system can ensure that every time after a data partition is transferred the information on the related cluster node is updated so that a query result can be quickly returned to the user.

In accordance with an embodiment all these functionalities can occur on a single thread e.g. service thread . Partition transfers can be performed on the service thread in order to ensure that the user can lock and unlock a set of partitions from the same thread the service thread when dealing with continuations.

In accordance with an embodiment the synchronous event interceptor completion can be performed without blocking the service thread by supporting continuation. Partition transfer events can lead an event interceptor to iterate over an entire partition. If the main service thread is blocked for a significant period of time latency can be introduced to the system or a crash can occur.

The continuation support for the partition transfer events in a distributed data grid can be achieved by posting the storage events to the worker thread. In this manner the event interceptors can be executed synchronously complete before the transfer partition is completed while at the same time without blocking the service thread. As a result the service thread do not need to wait for the completion of the transfer partition and can continue executing requests while the second thread executes the continuation event dispatch.

The service thread on the cluster node A can use a worker thread to accomplish synchronous event intercepting without blocking the service thread. When the service thread receives an event or a request for handling an event it can package the event up as a continuation e.g. a data structure which includes a task to be executed by the worker thread. When the worker thread picks up the task it can execute the task and post a response message back to the service thread which allows the handling of the event to switch back to the service thread which can send a data partition transfer request to a service thread on the cluster node B. In a similar fashion the service thread on the cluster node B can dispatch a receiving task to a worker thread and receives a response when the data partition transfer operation is done. Finally the receiving node cluster node B can send a message back to the sending node cluster node A as well as the rest of the cluster that it is responsible for the transferred data partition.

The following Listing 1 outlines a primary partition transfer workflow. In Coherence a PartitiondService.transferPrimary method can be responsible for the flow of control with respect to transferring partitions. It is responsible for transferring a set of partitions at a time while keeping the partition transfers to a reasonable size on the wire. The following workflow can ensure that the system only switch the context once to raise the partition departing events.

The following Listing 2 outlines a partition transfer workflow. In the example of Coherence the partition transfer workflow can raise StorageTransferEvents at proper points and support passing an Iterator to the data partition transfer events. A PartitionedCache.transferPartition method can gather all entries from a partition into a list to be transferred to another member and raise the StorageTransferEvent.DEPARTING event for a particular partition. The partition transfer workflow can be embedded in the workflow for PartitionedService.transferPrimary.

Additionally similar continuation flows can be implemented in PartitionedService.onTransferRequest and PartitionedCache.receivePartition methods for handling the StorageTransferEvent.DEPARTING event and the StorageTransferEvent.ARRIVED event. A DispatchTransferDeparting component can extend DispatchUEMEvent and be responsible for raising the StorageTransferEvent.DEPARTING event to the event dispatcher on the Coherence EventDispatcher thread. The runAfter method is responsible for posting the PostDeparted message back on the service thread. The PostDeparted message can extend the basic PartitionedCache Acknowledgement component. A DispatchTransferArrived component can extend the DispatchUEMEvent and be responsible for raising the StorageTransferEvent.ARRIVED event to the event dispatcher on the Coherence EventDispatcher thread. The runAfter method is responsible for posting the PostArrived message back on the service thread. The PostArrived message can also extend the basic PartitionedCache Acknowledgement component.

In accordance with an embodiment a set of event interceptors can be utilized in the distributed data grid for handling different storage events. An event interceptor can handle a pre insert event e.g. a storage entry event and defer the completion of the data change pending the completion of the pre insert event. This allows a server side client to veto the event or modify the event before the handling of the event is completed. Additionally the event interceptors allow raising post events which are not mutable and cannot be used to veto the event for example because they have already completed.

It may not be feasible for an entry locking mechanism to lock a key on one thread and unlock it from another. When multiple worker threads are in use the system may not be able to control which worker thread will pick up the continuation. If a continuation model can not offload the requests the INSERTING UPDATING and REMOVING storage entry events types need to be executed in the context of the thread processing the operation that triggers these events. In order to keep the service thread safe from user code a server side event model can allow that a service configured with event interceptors for these event types is also configured with worker threads.

For a pre commit storage entry event the execution of the event interceptors may not be completely isolated because the pre commit storage entry event can have the ability to stop roll back a request by throwing an exception. In accordance with an embodiment the pre commit workflow knows whether the event interceptors finish processing an exception before moving forward to the commit phase of request processing. In order to isolate the execution of the event interceptors continuation functionality can be added to each of the mutating operations on an data entry e.g. INSERT UPDATE DELETE such that the request handling can be set aside while the interceptor chains are executed on another thread and picked up again once the interceptors finish handling the request .

In accordance with an embodiment by executing the event interceptors on a worker thread instead of a service thread the system allows the event interceptors to operate in a similar fashion as an entry processor. In Coherence the following events can be run on a worker thread StorageEntryEvent.INSERTING StorageEntryEvent.UPDATING and StorageEntryEvent.REMOVING

The invocation workflow in Coherence can take into account not only the mutations made to storage but also the execution of EntryProcessors on entries within storage. As such handling of invocation requests requires the code to raise not only StorageInvocationEvents but also StorageEntryEvents. The onInvokeRequest and onInvokeAllRequest can be the entry points for raising events caused by invocations.

Interceptors can be triggered during this workflow on a worker thread while the entry or set of entries is locked for the before events and after the entry is unlocked for the after events.

Additionally the workflows for onPutRequest onPutAllRequest onRemoveRequest and onRemoveAll request can be similar. There can be corresponding methods for each of these operations onPutRequest Storage.put storage.postPut etc. .

The following Listing 4 outlines the workflow for a put request which can be replicated for operations such as putAll remove and removeAll requests.

Again interceptors can be triggered during this workflow on a worker thread while the entry or set of entries is locked for the before events and after the entry is unlocked for the after events. The above workflow for put putall can be implemented for remove removeAll as well.

Furthermore in order to make sure that the system raises events for both standard modifications as well as out of band modifications all the post modification events can be raised in the publishChanges method.

The following Listing 5 outlines the workflow for publishChanges which is a function called by processChanges and is responsible for collecting all modifications in the current sandbox along with any eviction type changes and publishing them to storage.

Throughout the various contexts described in this disclosure the embodiments of the invention further encompass computer apparatus computing systems and machine readable media configured to carry out the foregoing systems and methods. In addition to an embodiment consisting of specifically designed integrated circuits or other electronics the present invention may be conveniently implemented using a conventional general purpose or a specialized digital computer or microprocessor programmed according to the teachings of the present disclosure as will be apparent to those skilled in the computer art.

Appropriate software coding can readily be prepared by skilled programmers based on the teachings of the present disclosure as will be apparent to those skilled in the software art. The invention may also be implemented by the preparation of application specific integrated circuits or by interconnecting an appropriate network of conventional component circuits as will be readily apparent to those skilled in the art.

The various embodiments include a computer program product which is a storage medium media having instructions stored thereon in which can be used to program a general purpose or specialized computing processor s device s to perform any of the features presented herein. The storage medium can include but is not limited to one or more of the following any type of physical media including floppy disks optical discs DVDs CD ROMs microdrives magneto optical disks holographic storage ROMs RAMs PRAMS EPROMs EEPROMs DRAMs VRAMs flash memory devices magnetic or optical cards nanosystems including molecular memory ICs paper or paper based media and any type of media or device suitable for storing instructions and or information. The computer program product can be transmitted in whole or in parts and over one or more public and or private networks wherein the transmission includes instructions which can be used by one or more processors to perform any of the features presented herein. The transmission may include a plurality of separate transmissions. In accordance with certain embodiments however the computer storage medium containing the instructions is non transitory i.e. not in the process of being transmitted but rather is persisted on a physical device.

The foregoing description of the preferred embodiments of the present invention has been provided for purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many modifications and variations can be apparent to the practitioner skilled in the art. Embodiments were chosen and described in order to best explain the principles of the invention and its practical application thereby enabling others skilled in the relevant art to understand the invention. It is intended that the scope of the invention be defined by the following claims and their equivalents.

The following Listing 6 shows an exemplary Event interface. The Event interface is the base of all Events in the system. All Events in the system can have one or more event type associated with it. Consumers of an Event can decide what action to perform based on the event type.

The following Listing 7 shows an exemplary StorageEvents interface. StorageEvents are Events that occurs for a specific named cache instance.

The following Listing 8 shows an exemplary StorageCacheEvents interface. StorageCacheEvents are StorageEvents that can be raised on all storage members supporting a specific named cache. These represent global operations related to the named cache.

The following Listing 9 shows an exemplary StorageEntryEvents interface. StorageEntryEvents are StorageEvents that occurs on a specific Entry in a specific named cache instance. Some instances of this event will be raised before an operation on an Entry has been committed to storage. These before events allow the associated event interceptor to mutate the underlying Entry having these mutations committed to storage similar to Triggers . Exceptions thrown from the before events prevent the operation from completing successfully and keep it from being committed to storage. Other instances of this event can be raised after an operation on an Entry has been committed to storage. In all of these after events the contents of the underlying Entry are immutable.

The semantics for writing EventInterceptors for StorageEntryEvents can match that of writing EntryProcessors. Using a concurrency model interceptors handling this type of event can be dispatched on a worker thread with the Entry provided being locked.

The following Listing 10 shows an exemplary StorageInvocationEvents interface. StorageInvocationEvents are StorageEntryEvents and correspond to EntryProcessors that are being executed on a given member. StorageInvocationEvents can be raised before executing an EntryProcessor and after the results of the EntryProcessor have been committed to storage.

The following Listing 11 shows an exemplary StorageTransferEvents interface StorageTransferEvents are StorageEvents that represent transfers of entries from one member to another. The semantics for writing EventInterceptors for StorageTransferEvents can be different from StorageEntryEvents. Event interceptors can be dispatched on a worker thread while a lock is held for the partition being transferred. This means that while handling a StorageTransferEvent the partition being transferred can be locked across the cluster.

