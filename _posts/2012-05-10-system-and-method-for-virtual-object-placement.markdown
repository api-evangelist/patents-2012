---

title: System and method for virtual object placement
abstract: A computer system and method according to the present invention can receive multi-modal inputs such as natural language, gesture, text, sketch and other inputs in order to manipulate graphical objects in a virtual world. The components of an agent as provided in accordance with the present invention can include one or more sensors, actuators, and cognition elements, such as interpreters, executive function elements, working memory, long term memory and reasoners for object placement approach. In one embodiment, the present invention can transform a user input into an object placement output. Further, the present invention provides, in part, an object placement algorithm, along with the command structure, vocabulary, and the dialog that an agent is designed to support in accordance with various embodiments of the present invention.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08893048&OS=08893048&RS=08893048
owner: 
number: 08893048
owner_city: 
owner_country: 
publication_date: 20120510
---
This application claims the benefit of U.S. patent application No. 61 485 680 entitled System and Method for Virtual Object Placement filed May 13 2011.

The present invention relates to virtual object manipulation and more particularly to virtual object manipulation using multi modal inputs including where instructions provided via any of the inputs are underspecified.

Object placement or layout design is a vital component of many practical software applications. Background layout in animated movies video game map development crime or accident scene simulation interior design applications e.g. home design software and computerized virtual graphical social spaces such as Second Life are examples of applications that require various object placement functions to be carried out.

Such applications typically require arduous manipulation of graphical objects in the virtual world by means of a mouse or other input devices. For example one might place multiple block objects in an axis aligned manner in a 2D virtual world using a constraint satisfaction algorithm. However there is currently no ability for an application to understand and act upon a designer s command such as Put the wardrobe in that corner which uses a combination of natural language and gesture.

It will be appreciated that a computer system that can receive multi modal inputs such as natural language gesture text sketch and other inputs in order to manipulate graphical objects in a virtual world would provide desirable results.

It will further be appreciated that a computer system that can accurately manipulate graphical objects in a virtual world upon receiving multi modal inputs that underspecify the user s instructions would provide desirable results.

The present invention provides a computer system and method that can receive multi modal inputs such as natural language gesture text sketch and other inputs in order to manipulate graphical objects in a virtual world. As such the present invention can facilitate use of and interaction with virtual worlds. For example children and non expert adults can employ the present invention to interact in virtual environments which would otherwise be too complex. Additionally individuals with various physical limitations e.g. those who cannot work with an electronic mouse can employ multiple alternative input components in order to interact and manipulate objects in a virtual environment.

In one embodiment the present invention can be employed to transform natural language or text instructions for object assembly or disassembly into 3D visualizations. Such visualizations can include for example new product assembly visualizations or other self help maintenance and or repair visualizations. For example toy manufacturers and furniture retailers can employ the present invention to generate visualizations of product assembly steps in order to provide customers with better options beyond static instruction sheets customer service calls and online do it yourself websites with information such as videos purporting to assist product purchasers with assembly and other product operations.

The present invention provides in part an approach for implementing a Communicative Agent for Spatio Temporal Reasoning called CoASTeR in one embodiment that responds to multi modal inputs e.g. speech gesture and sketch to simplify and improve object placement in virtual worlds among other tasks. In one embodiment the present invention includes a software system workbench implementing one or more agents associated with the present invention. The components of an agent as provided in accordance with the present invention can include one or more sensors actuators and cognition elements such as interpreters executive function elements working memory long term memory and reasoners for object placement approach. In one embodiment the present invention can transform a user input into an object placement output. Further the present invention provides in part an object placement algorithm along with the command structure vocabulary and the dialog that an agent is designed to support in accordance with various embodiments of the present invention.

The present invention further provides a system and method for manipulating objects in virtual worlds including three dimensional virtual worlds with many constraints and rules in a non axes aligned manner i.e. the edge of the objects need not be perpendicular or parallel to each other . The present invention can further place individual objects without simultaneously solving location placement problems for multiple objects. The objects can be heterogeneous such as chairs tables books electronic devices etc. In one embodiment steps and or rules used in accordance with the present invention can be formulated as a multi level constraint satisfaction problem. For example the first step would be to solve the most probable activity station and the second step would be to solve for most probable placement and orientation. Alternatively the present invention can employ a customized attractor repulser algorithm for increased speed. The present invention further employs requesting agent characteristics with reachability intent and visibility parameters. The present invention can also be employed within a cognitive architecture and can adapt to linguistic input.

As shown in the system of the present invention can be implemented in a modular client server architecture with different sets of plug and play components including one or more operator clients access server runner engine and knowledge bases . The system of the present invention is a computer based system where the components can be implemented in hardware software firmware or combinations thereof. It will be appreciated that the system of the present invention incorporates necessary processing power and memory for storing data and programming that can be employed by one or more processors to carry out the functions and communications necessary to facilitate the processes and functionalities described herein. In one embodiment the components sub components and or modules associated with the present invention can be implemented using object oriented languages such as C or Java using well defined software interfaces such that the implementation of any component can replaced by another with equivalent functionality without affecting the overall functionality of the workbench. It will be appreciated that access can be provided over a public or private network including the Internet in various embodiments of the present invention. The present invention can comprise in one embodiment computer program instructions for receiving one or more inputs determining a valid and or optimal location for a virtual object in a virtual world and placing the object in the virtual world according to the determination. The computer program instructions can also perform the other processes and algorithm elements described hereinafter. The computer program instructions can be embodied in a computer readable medium which can be any data storage device capable of storing data which can thereafter be read by a computer or computer system. Example such devices are hard drives network storage read only memory ROM random access memory RAM compact discs magnetic tapes portable storage devices such as flash drives and other optical and non optical data storage devices. The computer readable medium can also be distributed over a network connected computer system to allow the computer program instructions to be stored and executed in a distributed manner. Computers and or computer systems capable of employing the present invention include mobile communication devices and other microprocessor based devices as well as minicomputers mainframe computers and networked computers for example.

With reference to the operator client comprises the user interface layer of the architecture of the system of the present invention and includes interaction consoles such as World View console and Agent View console . The World View console permits the user to interact with the virtual environment selected by the user and the Agent View console permits the user to interact with one or more agents that will then be employed in the virtual environment. The consoles can further include input output I O devices shown generally at to promote multi modal communication including devices such as a graphical user interface GUI with a text input chat a headset with microphone to provide speech input a sketch device such as an iPad gesture input device such as Microsoft Kinect or Nintendo Wii and a display screen such as those provided by a computer television or other display unit. It will be appreciated that all I O devices may not be included in particular implementations of the present invention. The consoles can further include agent imagination console and agent cognition console . These consoles can permit a user to view the traces of reasoning algorithm operation so as to for example identify potential reasoning errors. These consoles can further be used to identify the root causes of unexpected behavior and apply adjustments to correct such behavior.

The Server Runner enables the workbench of the present invention to operate in a client server mode where multiple operator clients can either locally or remotely connect to the engine of the present invention. The engine is the core of the system of the present invention and as shown in the embodiment of the invention represented by comprises a World Engine and a Cognition Engine . The World Engine is adapted to interact with third party components not shown such as for example a game engine application programming interface API or a 3 D graphics engine API e.g. Object Oriented Graphics Rendering Engine OGRE . An agent s physical representation or its embodiment resides in the World Engine . Through its embodiment API the agent can perceive or sense the virtual world in which it interacts gather information about the objects including their position and movements for example and create events or actions that change the world. The communicative abilities and programming of the present invention can be embedded in any existing 3 D graphical software application that provides a suitable API to access and manipulate the contents of the virtual world. For example the present invention can interact with OGRE a 3 D graphical rendering engine. The Cognition Engine houses an agent s reasoning and or its cognitive ability as described in more detail below.

The Knowledge Base layer includes a variety of data and knowledge that can be employed in the system of the present invention. For example programmed models for spatial reasoning gesture models parameters for the object placement algorithm in the Long Term Memory of the present invention and programmed graphical object models to retrieve and place in the virtual world can be stored in the Models and World Knowledge Base . Long Term Memory knowledge base can store dictionaries grammar rules semantics and knowledge bases such as outlined below for example.

The Environmental setup component of operator client can include a world selection sub component and an agent selection sub component for permitting the user to select a virtual world and an agent to act within the selected virtual world.

The Cognition component provides the various cognitive abilities for the agent and interfaces with the Embodiment component . In one embodiment of the present invention the Cognition component comprises the Sensor Interpreters sub component the Action Interpreters sub component the Working Memory sub component and the Executive sub component . The Sensor Interpreters sub component comprises interpreter elements that are adapted to transform raw sensory inputs such as text scene graphics and gesture signals into a logical form or mentalese. Logical forms are specialized computational representations that can be processed by reasoners or inference engines e.g. first order logic . Given the diversity of logic languages such as first order second order or higher order logics the present invention can employ in one embodiment its own internal representation language called CRL which stands for CoASTeR Representation Language . CRL is the present invention s canonical method for exchanging data across the agent sub components. In one embodiment of the present invention the syntax and the conventions of CRL are based on the ontologies and knowledge base associated with CycL developed by Cycorp Inc. of Austin Tex. in order to support its wide compatibility and acceptability among users. The present invention can include the following language components for example predicates entities logical connectives types modal operators and other higher order relations as shown in the exemplary screen display of .

The Working Memory sub component is adapted to include a version of the Belief Desires and Intentions software model BDI model as part of the working memory. Beliefs contain facts about the world as perceived by agents of the present invention. The Goals are high level objectives that the agent must accomplish and the Intentions are planned commitments to achieve the Goals.

The Executive sub component orchestrates the functioning of all of the other components in the agent s cognition. For example the Executive sub component sets the goals and activates the reasoners to plan schedule and prioritize actions. The Executive sub component includes a meta cognition component that can receive information from the Beliefs element and exchange information with the Goals element . A principal task of the meta cognition component is to based on the sensed situation set and prioritize goals to perform. For example when a command to place an object is received from the text input the meta cognition component s goal setting algorithm can post an object placement task in the agent s belief base. The reasoners include a set of swappable components that can take CRL representations of the virtual world and decide and develop an action response for the agent for example. Reasoners can include for example path planners that help the agent decide how to move from one location to another or an object placement component that helps the agent decide where to place a new object in the world. In one embodiment of the present invention the reasoners output their decisions in CRL mentalese which is then handed off to the Action Interpreters component via Intentions element for further processing. The Action Interpreters component is adapted to transform the CRL mentalese into executable information. For example a language interpreter can transform a logical form representation of what an agent needs to say into a text form. Alternatively a scene interpreter can transform a placement decision into a scene graph.

The Long term memory of an agent comprises a variety of knowledge resources needed by the interpreters and generators. Such knowledge can include for example the lexicon for interpreting natural language or knowledge about objects in the world.

Consider an interior design application where an end user interacts with the system of the present invention using client . The interior design application can be accessed via the World Engine of engine . In this example the user can be presented with a view of an office space on his or her computer screen or monitor. Further consider two possible scenarios of interaction. The first scenario involves only one modality of input. In this scenario the user speaks an object placement command into a microphone Place the printer on the table. The second scenario involves two modalities of input speech and gesture. In this scenario the user speaks a slightly different command into the microphone Place the printer over there and points to the table using a controller such as a Wii remote or a finger when the system is connected with Kinect or similar gesture sensing system . An agent in accordance with the present invention can transform each of these inputs into a concrete placement action as described below.

In the first example a speech recognizer component associated with the operator consoles of client transforms the speech signal into a string of words or text. The present invention can use in one embodiment an open source speech recognition system e.g. Sphinx along with a handcrafted dictionary and grammar covering the range of inputs expected in a particular application. The dictionary and the grammar are stored in the agent s Long Term Memory .

In the second scenario when the end user also interacts with the gesture modality by pointing to the table a gesture recognizer component associated with the operator consoles of client can transform the pointing gesture signal into a location in the image displayed on the user s monitor. In one embodiment of the present invention a remote such as a Wii remote can be used for identifying where the user is pointing. As the speech and gesture inputs are being concurrently processed the visual sensor associated with the Embodiment component of the agent extracts the scene graph from the virtual world based on the user s perspective i.e. the displayed scene . In one embodiment of the present invention the scene graph comprises the set of objects in the display their graphical elements such as surfaces mesh elements and relative positions. For example a table can be an object and the table has graphical elements such as a top surface bottom surface and edges with relative positions such as on the floor and in front of the wall .

Further processing according to one embodiment of the present invention can include a language interpreter as part of the sensor interpreter component receiving the text input and transforming it into a logical form in CRL format. An open source semantic interpreter called OpenCCG can be used for this purpose for example. The interpreter can use its own handcrafted dictionary of terms grammar rules and predicate semantics i.e. meaning units to cover the range of inputs expected in an application domain described below. This lexicon is stored in the agent s long term memory and loaded when the agent becomes ready for action. The interpreted logical form of the user utterance is posted in the working memory for further processing by the agent s reasoning components .

In the second scenario where the gesture input is included as well the gesture interpreter associated with sensor interpreter component receives the image location output by the gesture recognizer and selects an object and its surface for potential placement. For example it selects the table as the object its top surface and a potential region on the table for placement on the surface. This information about object surface and placement region is posted in the working memory .

A scene interpreter associated with the sensor interpreter component receives the scene graph as the input and computes a variety of qualitative spatial relations and object features. In one embodiment of the present invention the scene interpreter uses programmed spatial computation models according to the present invention stored in the agent s long term memory to compute these relations. For example the scene interpreter can compute whether two 3 D objects are touching how far apart they are and their relative orientations. If for example the present invention needs to compute the distance between two surfaces each belonging to a different object the scene interpreter can do so by 1 placing a grid on each surface 2 computing the distance between the center point of a cell in one grid and that of a cell in another grid and 3 performing the same computation in 2 for all possible cell pairs between the two surfaces. Then the scene interpreter can pick the minimum distance. If the minimum distance is 0 then the two objects are touching each other. Based on the object properties loaded from the agent s long term memory the scene interpreter can also associate meaning to various graphical objects. For example it knows that a set of polygons comprise a table. The scene interpreter can thus transform this information into the CRL format and post it in the working memory .

The Reasoners poll the working memory for the CRL outputs provided by the Language Gesture and Scene Interpreters described above. For example a Situation Understanding Reasoner can unify the various representation elements by unifying the table entity identity from the Language Interpreter with the table entity identity in the scene. When gesture is involved as described above in the second scenario the Situation Understanding Reasoner can unify the table entity from the scene interpreter with the gesture object. As a result a fully grounded CRL representation of the current state of the world is derived. This representation can be posted back into the working memory for subsequent processing. Using the unified CRL situation representation the object placement algorithm of the present invention can compute the location and the orientation of the new object and pass it on to a Scene Generator component associated with actuators .

The Scene Generator can retrieve a graphical model of the new object e.g. the printer from its long term memory and insert the new object into the existing scene graph at the location computed by the object placement reasoner. The end user sees the result of the action on the World View Console provided as part of the client of the present invention.

The Object Placement Reasoner can process and respond to a variety of object placement commands. In one embodiment of the present invention the present invention assumes English is used as the language for commands. However other natural languages could be used as well with suitable changes to speech recognition and language understanding lexicons. The command structure can include a combination of the following elements 

The present invention can thus adapt to a variety of spoken commands of different degrees of specificity.

In the course of interpreting the user commands agents in accordance with the present invention may encounter ambiguity or interpretation failure. For example when multiple tables are present in the scene the agent may not be able to correctly identify the table being referred to by the user. Alternatively interpretation failure may occur if the object being referred to is not available in the agent s memory. For example in the command Place the rack next to sphygmomanometer the agent may not know what a sphygmomanometer is. To resolve such ambiguities and interpretation failures the agent can employ the operator console to engage in a dialog with the end user according to one embodiment of the present invention. For example it can query the user Which table do you mean The one on your right or the one on your left The answers to such queries can be used to resolve ambiguities in accordance with the present invention.

As a separate example consider the task of imagining a scene described by text utterances or equivalently generating a static scene in a virtual environment. For example imagine a chair in front of the table and subsequently imagine a printer on the table. An example desired rendering of the scene is shown in with printer monitor table and chair .

The central issue addressed by the present invention in such a task is interpreting the rather vague spatial prepositions such as on and in front of into valid and acceptable object placements. The utterance printer on the table can only be judged as vague when attempting to place the printer in an existing virtual world. For instance the placements on the table could be to the left right front and behind a monitor that already exists on the table. However the placements in front and back of the monitor are functionally unacceptable for a human user. The utterance also does not specify the suitable orientation of the printer. Without such a specification the printer could be oriented in numerous ways in relation to a monitor and the chair only some of which would be valid. For example the orientation shown in is valid. However the orientation of printer with its back to the user would be invalid.

In accordance with the present invention functional knowledge of interaction between objects is considered for generating valid placements. The present invention incorporates a determination of what the content of such functional and world knowledge should be and how it can be utilized to recover any unspecified elements and generate a complete and valid specification of object placement.

In one embodiment of the present invention the virtual world is identified as W and the set of objects in the world is identified as O. Given W and with O located in various places within W and further given an underspecified linguistic command issued by the requesting agent a to place a target object oin W the present invention can find valid locations and orientations for the target object and can then place the object in W. It will be appreciated that a fully specified location command may be one for example that details specific x and y coordinates for the placement of an object. The present invention can work with such fully specified commands but also adapts to underspecified commands that require interpretation as described herein.

In one embodiment of the present invention an algorithm is employed that incorporates one or more of the following steps 

The present invention prepares the inputs by retrieving a linguistic placement constraint lpc identifying the requesting agent s as intent and retrieving the apreference as follows 

a. Obtain the lpc The present invention s Language Interpreter interprets a linguistic command such as Put the book on the table and places its semantic interpretation in CRL format in the agent s belief base BB . The linguistic command can be received by voice or text input for example and can be interpreted in CRL format by the Language Interpreter as part of sensor interpreter component as described above and shown in . The agent s belief base is maintained within working memory as described above and shown in . As part of this step the linguistic placement constraint is retrieved from long term memory for further processing. At this stage the present invention can also mark all potential relationships between agents in the world the target object oand the landmark objects o . The present invention can perform this by looking up an Agent Object Relation Database AOR DB described below. For example the phrase my book associates the owith aand the phrase his table would link the table to another agent in the world.

b. Obtain intent i for a The present invention can identify the intent associated with agent aand with lpc as the input parameter as follows. The present invention retrieves the type of object expressed as the landmark or container in lpc and retrieves the intent associated with it from an object knowledge base O KB associated with the present invention. For example the object kitchen sink has a default intent of maintenance and the object shelf has a default intent of storage associated with it.

c. Obtain placement preference s pp for a The present invention can then retrieve the aplacement preference such as right handed or left handed from the agent knowledge base A KB .

Once the inputs are prepared the present invention can retrieve the op from the object knowledge base O KB using the semantic category of the target object oas the input query. For example the semantic category can be a book or a table .

Using the lpc the present invention can identify oas all the objects associated with the landmark objects o covered by the lpc. The present invention can then identify all the surfaces associated with o. Next the present invention can filter the surfaces based on criteria. In one embodiment the present invention filters the surfaces based on the following criteria 1 surface outside normal 2 height of the surface from the floor and 3 installability of the target object o. The present invention can apply the following rules for example to select the cps 1 If ois not installable the present invention ignores all surfaces whose outside normal vector is not pointing upwards. 2 The present invention ignores any surface that is located outside the range of os height preference. 3 The present invention ignores a surface if its largest dimension is smaller than the smallest dimension of the target object.

An activity surface is one on which the requesting agent ais located while interacting with other candidate objects. For example this may be a floor or furniture such as a seat. The cas s are those surfaces that are within a s reachable distance from the cps. Reachable distance can vary based upon the agent type as described below.

For all the surfaces in cps and cas the present invention identifies all the objects that are in contact with these surfaces. The present invention then recursively identifies all the objects that are in contact with the objects thus far identified. Once all the objects are identified the present invention identifies their categories as well retrieves placement constraints associated with the type of an object.

Activity stations are locations in the world where an agent may be located while interacting with an object. In this step for every object in roic the present invention identifies whether it is an activity station as how many objects of the type are allowed in a particular activity station i.e. the object per station cardinality constraint and whether it is a self category attractor. The present invention does this by looking up a record in the object knowledge base O KB . For example a chair is an activity station and a book is not and typically only chair may be placed per activity station but many books may be placed per activity station. Furthermore a book attracts other objects of its own type.

Certain objects may be co located within the reach of the same activity stations while others may not. In this step for each pair of objects in roic the present invention retrieves an object to object activity and distance record from the Object to Object Activity and Distance Constraints Knowledge Base OOADP KB whether they preferably belong to the same activity station or a different one.

For all pairs of objects in roic aand their relevant surfaces the present invention retrieves the constraints from the surface to surface constraints knowledge base SSSC KB .

The most probable activity regions are those regions on the cas where ais most likely to be located to efficiently interact with all the objects covering role s. To compute this the present invention places a grid of cells over cas called the cas grids. In the example rendering of objects in the grid is illustrated at along with a table chair phone and agent . The present invention can control the cas cell size in a cas grid with a parameter with a smaller size representing a finer degree of control for placement. It will be appreciated however that a smaller cell size exponentially increases the computation time and needed computing resources. Next for each object oin roic the present invention places aat the object and votes on the cells in the grid. First if the cas cell is located on an activity station e.g. seat of a chair then it gets a high positive vote e.g. 5 . Next if a cas cell is reachable from a voting object oin roic then if they are not supposed to be associated with the same activity station then to prevent a common activity station ocasts a large negative vote of e.g. 100 on the cas cell or else it casts a positive vote of v e.g. 1 . For example a printer may not share the same activity station as a laptop. In that case this approach ensures that a printer and a laptop are not placed together. Also if ois of the same kind as o and the cardinality constraint for the ofor the station is 1 then it casts a large negative vote e.g. 100 to prevent using the same activity station. The present invention then totals the votes for each cas cell in the grid into a cas cell score.

The most probable placement regions are those where the target object omay be placed to satisfy a s linguistic directive. The present invention computes the mppr as follows. First the present invention places grids on each of the cps s i.e. cps grid on table as shown in for example. Next from each cas cell e.g. the present invention votes on all the cells i.e. cps cell in the cps grids. If a cps cell is reachable from a cas cell then the cas cell votes on the cps cell with its cas cell score. All of the relevant objects in roic also vote on a cps cell based on their interaction constraints with the o. If a cps cell is located on an o and as determined if oand oare of the same category and if the category is a self category attractor for the given intent then the cps cell gets a large positive vote e.g. 20 . For example a book attracts a book to be placed together. Since many cas cells can vote on a cps cell the present invention retains the maximum positive vote as the cps cell score. The constraints are available in sscthat were obtained as described above. It will be appreciated that an object in roic can cast a negative vote if it needs to prevent placement to maintain visibility. For example a projector may cast a negative vote in front for every object other than a projection screen. All placement cells with votes more than a certain pre determined threshold are included in the mppr. In one embodiment of the present invention upon selecting the placement area cell with the highest score the present invention has thus determined a valid location and a final placement area for the given object. The present invention can thus proceed with placing the object on in or at the selected placement area. The object can be provided with a default orientation or the present invention can further operate to assess the preferred orientation of the object before placing it as described below.

The present invention selects the preferred geo orientation of the ousing its surfaces for a given intent from ssc. Using this the present invention sets the geo orientation of the target object surface. For example a book face faces up. As shown in the phone is placed screen side up on the table .

The most probable interactive orientations of the target object surfaces are those that conform to the various orientation constraints such as visibility and readability from the activity regions as imposed by the constraints in ssc. To obtain the mpio for each mppr in one embodiment of the present invention the system of the present invention begins with an orientation in the horizontal plane that aligns the axis of an object to the edge of the placement surface rotates it by an increment of some angle alpha e.g. 45 degrees and checks for the applicable constraints with all the objects. If all of the constraints are satisfied then the orientation is selected as one of the placement constraints.

In one embodiment the present invention scores the mppr mpio outputs and picks the one with the highest score. Upon selecting the output with the highest score the present invention has thus determined a valid location and a final placement area and orientation for the given object. In for example the phone is placed in the approximate middle of the table . It will be appreciated that the final placement need not be axially aligned with a given surface. For example a rectangular shaped book object to be placed on a rectangular shaped table need not have one or more edges aligned with or parallel to one or more edges of the table. In this way the present invention can adapt to situations in three dimensional virtual worlds that require placement of objects in a non axes aligned manner. It will further be appreciated that the present invention need not determine both a placement location and a placement orientation at the same time. For instance the present invention may determine a valid placement location as described above and proceed to place the object in the valid placement location within the virtual world without regard to the orientation of the object or only with regard to a pre determined orientation.

As described above the object placement algorithm according to the present invention uses a variety of context free and context dependent knowledge of objects and their interactions with each other and the reference agent a. As described herein the present invention can further define the inventory of concepts that forms the basis for encoding the knowledge used by the object placement algorithm of the present invention. These concepts serve as the language to encode spatial constraints for interacting object in the knowledge base. A description of the structure and the content of the knowledge base in accordance with one embodiment of the present invention is provided hereinafter.

The present invention employs functional representation of objects in the world and no functional representation is feasible without a reference agent according to the present invention. In one embodiment of the present invention a reference human agent is the requester for whom the object placement is being considered. Further in one embodiment the present invention considers different properties of a human agent for describing placement constraints such as for example age gender pose and placement intention. Age determines the size and therefore the reach of the agent. In one embodiment the present invention uses adult youth child and infant as the variable labels. Gender helps determine the size and the reachability parameters for the agent. In one embodiment the present invention uses male and female as the values. Sitting standing and lying down are examples of principal poses used in accordance with the present invention. Other poses such as squatting and stretching can be employed as well. Placement intention is described in more detail below.

Qualitative Height In one embodiment the present invention describes the height of a surface from the floor using qualitative descriptions by reference to the requesting agent. In one embodiment of the present invention the height is measured from the floor on which the reference agent is standing. In one embodiment the present invention considers the following two parameters based on the acharacteristics.

For example the placement preference for a chair is floor and placement preference for printer would be waist knee Adult. The qualitative placement height for a picture or poster is face adult. Similarly the placement location preference for a chandelier is ceiling.

The intention or purpose of the reference agent ahas an impact on where the object should be placed when the directives are underspecified. In one embodiment the present invention considers the following four category labels 

The handedness description of an agent i.e. whether the agent is right handed or left handed can have an impact on where an object should be placed.

Physical objects comprising their 3 D shape occupy space in the virtual worlds in accordance with the present invention. These objects have various individual and context specific spatial properties. The object can be composed of parts each of which may have properties associated with it. In accordance with the present invention one of the important spatial properties is the frame of reference associated with an object.

The frame of reference comprises three orthogonal axes in a coordinate system. In one embodiment the present invention denotes the three axes with letters X Y and Z where nominally Z is vertical. Each axis has directions and depending on the direction it extends from the origin. In one embodiment the present invention uses the following axes description labels e.g. see 

For some objects one of these axes may be marked as the major axis. For example cylindrical long objects such as pens and pencils may have a major or principal axis e.g. X along their length. Some objects have a dominant intrinsic frame of reference depending on their shape and function in the world. For objects with a dominant frame of reference the present invention permits the selection of the object s frame of reference or else it uses the aframe of reference. For example a chair may have a front back top bottom left and right regardless of the context. Some objects can be symmetrical about 1 2 or 3 axes and may not have an unambiguous intrinsic frame. However most functional objects do have a strong intrinsic frame.

In one embodiment the present invention subdivides an object s geometric 3D extent into its logical parts or features. The features considered are point lines or curves surfaces or regions and volumes in 0 1 2 and 3 dimensions respectively. In one embodiment the associated part names are tip edge side and corner See Table 1 . Other features such holes and groves can be considered as well.

As shown in for example the table includes a table top and legs . The table top has a top surface a major axis X to X a minor axis Y to Y and vertical axis Z to Z . The Z portion of the vertical axis is referenced as inside normal as it extends inside of the origin. The Z portion is referenced as outside normal as it extends outside of the origin. Similarly the table leg has a face with a left right axis a vertical top bottom axis and an axis with an outside normal direction .

Objects that have parts that move relative to each other may have their spatial configurations referred by a pose label in accordance with one embodiment of the present invention. For example a desk lamp s stem may have several configurations that may have names. Similarly human like agents have various poses such as walking sitting and stretching for example. Certain objects such as doors and windows may have spatial functional configurations such as open closed and locked unlocked in accordance with the present invention.

In accordance with the present invention some objects such as a chair for example are activity stations and the present invention marks them as such in the knowledge base. The activity station cardinality limits the number of objects of a particular type that may be associated with an activity station. For example only one computer or one printer may be associated with an activity station. Some objects attract the same category of object. For example books and magazines may attract each other. Some objects may contain or support other objects. For example a table top supports other objects. Objects with spatial functions may have a default intent associated with them. For example a table top has a default intent of use while a book shelf has a default intent of storage . Some objects are installable and can be affixed to other objects. For example a picture may be hung on the wall.

In accordance with one embodiment of the algorithm of the present invention surfaces are the most relevant geometric extent of 3 D objects. In one embodiment the present invention reduces the interaction between two objects to interaction between two surfaces of the respective objects. For example bottom book cover face may interact with the face of a table top. For an object with a dominant intrinsic frame its surfaces may be identified by reference to the object s intrinsic frame. For example a surface may take on one of the following intrinsic labels top bottom left right front and back. In one embodiment of the present invention each surface has the following two normals e.g. see 

In one embodiment the present invention represents the geo orientation of a surface by the tuple where the geo orientation labels are 

In one embodiment the present invention represents the relative orientation of two surfaces by the following relation where targetObject.surface and referenceObject.surface refer to candidate surface whose relative orientation is specified and the planar axis alignment and the normal axes alignment are as follows 

Therefore in one embodiment the present invention represents the relative constraint of an agent facing a picture as follows 

The distance between two surfaces is the minimum distance between a pair of points located on the two different surfaces. In one embodiment the present invention uses the following labels to qualitatively describe the distances 

In one embodiment the present invention describes the visibility of a surface from another using the following labels 

In one embodiment the present invention describes a surface height by reference to reference agent s physical characteristics such as floor knee waist abdomen check neck and head for example.

In one embodiment the algorithm of the present invention uses the following knowledge bases Object Knowledge Base O KB Agent Database A DB Agent Object Relation Database AOR DB Object to Object Activity and Distance Constraints Knowledge Base OOADC KB and Surface to Surface Spatial Constraints Knowledge Base SSSC KB .

The O KB contains various context free properties of an object. In one embodiment of the present invention each record in the O KB contains the following information for a category of object 

The Agent Database A DB holds the characteristics of a particular instance of an agent. Each record in this database can contain for example the following information for an instance of an agent 

The Agent Object Relation Database AOR DB contains the relationship between specific instances of an object and agent instances. Each record in this database can contain for example the following information for each instance of the object and agent in the world 

The Object to Object Activity and Distance Constraints Knowledge Base OOADC KB contains the object to object relationship and distance constraints based on shared activity stations. Each record in the OOADC KB can contain for example the following information for a pair of object categories 

The Surface to Surface Spatial Constraints Knowledge Base SSSC KB stores the various spatial constraints between two surfaces which include height preferences geo orientations planar orientations and distances. Each record of this KB can contain for example the following information fields 

The invention may be embodied in other specific forms without departing from the spirit or essential characteristics thereof. The present embodiments are therefore to be considered in all respects as illustrative and not restrictive the scope of the invention being indicated by the claims of the application rather than by the foregoing description and all changes which come within the meaning and range of equivalency of the claims are therefore intended to be embraced therein.

