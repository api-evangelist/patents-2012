---

title: Restoring snapshots to consistency groups of mount points
abstract: A method is used for managing consistency groups in storage systems. A consistency group comprising multiple logical objects is created and multiple replicas of the consistency group are created. The consistency group is restored to one of the multiple replicas.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09218138&OS=09218138&RS=09218138
owner: EMC Corporation
number: 09218138
owner_city: Hopkinton
owner_country: US
publication_date: 20120330
---
This application is related to U.S. patent application Ser. No. 13 436 117 entitled MANAGING CONSISTENCY GROUPS IN STORAGE SYSTEMS filed concurrently herewith U.S. patent application Ser. No. 13 435 519 entitled RESIZING SNAPSHOT MOUNT POINTS filed concurrently herewith U.S. patent application Ser. No. 13 435 914 entitled RESTORING LOGICAL OBJECTS TO DIFFERENT SIZED SNAPSHOTS filed concurrently herewith and U.S. application Ser. No. 13 436 862 entitled MANAGING RESTORE OPERATIONS USING DATA LESS WRITES filed concurrently herewith which are incorporated herein by reference for all purposes.

This application relates to restoring snapshots to consistency groups of mount points in storage systems.

Computer systems may include different resources used by one or more host processors. Resources and host processors in a computer system may be interconnected by one or more communication connections. These resources may include for example data storage devices such as those included in the data storage systems manufactured by EMC Corporation. These data storage systems may be coupled to one or more servers or host processors and provide storage services to each host processor. Multiple data storage systems from one or more different vendors may be connected and may provide common data storage for one or more host processors in a computer system.

A traditional storage array herein also referred to as a data storage system disk storage array disk array or simply array is a collection of hard disk drives operating together logically as a unified storage device. Storage arrays are designed to store large quantities of data. Storage arrays typically include one or more storage array processors SPs for handling requests for allocation and input output I O requests. An SP is the controller for and primary interface to the storage array.

A storage array may be thought of as a system for managing a large amount of a resource i.e. a large number of disk drives. Management of the resource may include allocation of a portion of the resource in response to allocation requests. In the storage array example portions of the storage array may be allocated to i.e. exclusively used by entities that request such allocation.

Data storage systems such as disk drives disk storage arrays network storage devices storage area networks and the like are called upon to store and manage a significant amount of data e.g. gigabytes terabytes petabytes etc. that is written and read by many users. Storage arrays are typically used to provide storage space for a plurality of computer file systems databases applications and the like. For this and other reasons it is common for physical storage arrays to be logically partitioned into chunks of storage space called logical units or LUs. This allows a unified storage array to appear as a collection of separate file systems network drives and or volumes.

A host processor may perform a variety of data processing tasks and operations using the data storage system. For example a host processor may perform basic system I O operations in connection with data requests such as data read and write operations.

Host processor systems may store and retrieve data using a storage device containing a plurality of host interface units disk drives and disk interface units. The host systems access the storage device through a plurality of channels provided therewith. Host systems provide data and access control information through the channels to the storage device and the storage device provides data to the host systems also through the channels. The host systems do not address the disk drives of the storage device directly but rather access what appears to the host systems as a plurality of logical disk units. The logical disk units may or may not correspond to the actual disk drives. Allowing multiple host systems to access the single storage device unit allows the host systems to share data in the device. In order to facilitate sharing of the data on the device additional software on the data storage systems may also be used.

Presently there is a trend toward the use of larger operating systems larger applications or programs and larger file sizes. Understanding this trend a storage administrator is likely to request the provisioning i.e. allocation of a larger portion of storage space than is currently required for an operating system for example with the expectation that the space requirements will grow with upgrades bug fixes the inclusion of additional features and the like.

In recognition of the fact that more storage space may be provisioned for operating systems programs and users than can actually be used at first the concept of a sparsely populated logical unit LU such as a thin logical unit TLU was developed. Unlike the more traditional fully allocated logical unit which is created by fully provisioning an entire initial amount of storage area a sparsely populated logical unit is provisioned at creation but is not allocated any physical storage until the storage is actually needed. Specifically a TLU resolves this problem by allocating the storage space e.g. making the memory space physically available as it is needed when or shortly before data is written to the TLU. A TLU is created from a common pool of physical space and starts with minimal amount of physical space. As the application that is using the TLU starts to demand more storage the TLU incrementally requests the storage space from the common storage pool in portions referred to as slices.

Computer data is vital to today s organizations and a significant part of protection against disasters is focused on data protection. Conventional data protection systems use data replication by creating a copy of the organization s production site data on a secondary backup storage system and updating the backup with changes. The backup storage system may be situated in the same physical location as the production storage system or in a physically remote location. Data replication systems generally operate either at the application level at the file system level or at the data block level.

Data protection systems try to provide continuous data protection which enable the organization to roll back to any specified point in time within a recent history. Continuous data protection systems aim to satisfy two conflicting objectives as best as possible namely i minimize the down time in which the organization production site data is unavailable during a recovery and ii enable recovery as close as possible to any specified point in time within a recent history.

A method is used for managing consistency groups in storage systems. A consistency group comprising multiple logical objects is created and multiple replicas of the consistency group are created. The consistency group is restored to one of the multiple replicas.

Described below is a technique for use in managing consistency groups in storage systems which technique may be used to provide among other things creating a consistency group of a set of snapshot mount points SMPs wherein a snapshot mount point SMP is a logical object accessible for using a replica of a logical volume and performing an operation on the consistency group.

Conventionally it may be difficult or impossible for a conventional snapshot copy utility to create a consistency group of a set of snapshot mount points create a snapshot copy of the consistency group and to restore the consistency group to the snapshot.

By contrast in at least some implementations in accordance with the technique as described herein the use of managing consistency groups in storage systems enables a user to create and manage a consistency group of snapshot mount points such that each snapshot mount point represent a different point in time copy of a logical object such as a logical unit number LUN . Further the consistency groups may be restored or rolled back using a point in time snapshot.

In at least some implementations in accordance with the current technique as described herein the use of the managing consistency groups in storage systems technique can provide one or more of the following advantages lowering storage costs by improving efficiency of a data storage system and improving performance of a data storage system by efficiently managing snapshot copies in a consistency group and providing capability to create and restore to a consistent snapshot of a consistency group.

Referring now to shown is an example of an embodiment of a computer system that may be used in connection with performing the technique or techniques described herein. The computer system includes one or more data storage systems connected to host systems through communication medium . The system also includes a management system connected to one or more data storage systems through communication medium . In this embodiment of the computer system the management system and the N servers or hosts may access the data storage systems for example in performing input output I O operations data requests and other operations. The communication medium may be any one or more of a variety of networks or other type of communication connections as known to those skilled in the art. Each of the communication mediums and may be a network connection bus and or other type of data link such as hardwire or other connections known in the art. For example the communication medium may be the Internet an intranet network or other wireless or other hardwired connection s by which the host systems may access and communicate with the data storage system and may also communicate with other components not shown that may be included in the computer system . In at least one embodiment the communication medium may be a LAN connection and the communication medium may be an iSCSI or fibre channel connection.

Each of the host systems and the data storage systems included in the computer system may be connected to the communication medium by any one of a variety of connections as may be provided and supported in accordance with the type of communication medium . Similarly the management system may be connected to the communication medium by any one of variety of connections in accordance with the type of communication medium . The processors included in the host server computer systems and management system may be any one of a variety of proprietary or commercially available single or multiprocessor system such as an Intel based processor or other type of commercially available processor able to support traffic in accordance with each particular embodiment and application.

It should be noted that the particular examples of the hardware and software that may be included in the data storage systems are described herein in more detail and may vary with each particular embodiment. Each of the host server computers the management system and data storage systems may all be located at the same physical site or alternatively may also be located in different physical locations. In connection with communication mediums and a variety of different communication protocols may be used such as SCSI Fibre Channel iSCSI FCoE and the like. Some or all of the connections by which the hosts management system and data storage system may be connected to their respective communication medium may pass through other communication devices such as a Connectrix or other switching equipment that may exist such as a phone line a repeater a multiplexer or even a satellite. In at least one embodiment the hosts may communicate with the data storage systems over an iSCSI or fibre channel connection and the management system may communicate with the data storage systems over a separate network connection using TCP IP. It should be noted that although illustrates communications between the hosts and data storage systems being over a first connection and communications between the management system and the data storage systems being over a second different connection an embodiment may also use the same connection. The particular type and number of connections may vary in accordance with particulars of each embodiment.

Each of the host server computer systems may perform different types of data operations in accordance with different types of tasks. In the embodiment of any one of the host server computers may issue a data request to the data storage systems to perform a data operation. For example an application executing on one of the host server computers may perform a read or write operation resulting in one or more data requests to the data storage systems .

The management system may be used in connection with management of the data storage systems . The management system may include hardware and or software components. The management system may include one or more computer processors connected to one or more I O devices such as for example a display or other output device and an input device such as for example a keyboard mouse and the like. A data storage system manager may for example view information about a current storage volume configuration on a display device of the management system . The manager may also configure a data storage system for example by using management software to define a logical grouping of logically defined devices referred to elsewhere herein as a storage group SG and restrict access to the logical group.

It should be noted that although element is illustrated as a single data storage system such as a single data storage array element may also represent for example multiple data storage arrays alone or in combination with other data storage devices systems appliances and or components having suitable connectivity such as in a SAN in an embodiment using the techniques herein. It should also be noted that an embodiment may include data storage arrays or other components from one or more vendors. In subsequent examples illustrated the techniques herein reference may be made to a single data storage array by a vendor such as by EMC Corporation of Hopkinton Mass. However as will be appreciated by those skilled in the art the techniques herein are applicable for use with other data storage arrays by other vendors and with other components than as described herein for purposes of example.

An embodiment of the data storage systems may include one or more data storage systems. Each of the data storage systems may include one or more data storage devices such as disks. One or more data storage systems may be manufactured by one or more different vendors. Each of the data storage systems included in may be inter connected not shown . Additionally the data storage systems may also be connected to the host systems through any one or more communication connections that may vary with each particular embodiment and device in accordance with the different protocols used in a particular embodiment. The type of communication connection used may vary with certain system parameters and requirements such as those related to bandwidth and throughput required in accordance with a rate of I O requests as may be issued by the host server computer systems for example to the data storage systems .

It should be noted that each of the data storage systems may operate stand alone or may also included as part of a storage area network SAN that includes for example other components such as other data storage systems.

Each of the data storage systems of element may include a plurality of disk devices or volumes. The particular data storage systems and examples as described herein for purposes of illustration should not be construed as a limitation. Other types of commercially available data storage systems as well as processors and hardware controlling access to these particular devices may also be included in an embodiment.

Servers or host systems such as provide data and access control information through channels to the storage systems and the storage systems may also provide data to the host systems also through the channels. The host systems do not address the disk drives of the storage systems directly but rather access to data may be provided to one or more host systems from what the host systems view as a plurality of logical devices or logical volumes. The logical volumes may or may not correspond to the actual disk drives. For example one or more logical volumes may reside on a single physical disk drive. Data in a single storage system may be accessed by multiple hosts allowing the hosts to share the data residing therein. A LUN logical unit number may be used to refer to one of the foregoing logically defined devices or volumes. An address map kept by the storage array may associate host system logical address with physical device address.

In such an embodiment in which element of is implemented using one or more data storage systems each of the data storage systems may include code thereon for performing the techniques as described herein. In following paragraphs reference may be made to a particular embodiment such as for example an embodiment in which element of FIG. includes a single data storage system multiple data storage systems a data storage system having multiple storage processors and the like. However it will be appreciated by those skilled in the art that this is for purposes of illustration and should not be construed as a limitation of the techniques herein. As will be appreciated by those skilled in the art the data storage system may also include other components than as described for purposes of illustrating the techniques herein.

Referring to shown is an example representing how data storage system best practices may be used to form storage pools. The example illustrates how storage pools may be constructed from groups of physical devices. For example RAID Group may be formed from physical devices . The data storage system best practices of a policy may specify the particular disks and configuration for the type of storage pool being formed. For example for physical devices on a first data storage system type when forming a storage pool RAID 5 may be used in a 4 1 configuration e.g. 4 data drives and 1 parity drive . The RAID Group may provide a number of data storage LUNs . An embodiment may also utilize one or more additional logical device layers on top of the LUNs to form one or more logical device volumes . The particular additional logical device layers used if any may vary with the data storage system. It should be noted that there may not be a 1 1 correspondence between the LUNs of and the volumes of . In a similar manner device volumes may be formed or configured from physical devices . The storage pool of the example illustrates two RAID groups being used to define a single storage pool although more generally one or more RAID groups may be used to form a storage pool in an embodiment using RAID techniques.

The data storage system may also include one or more thin devices . A thin device also referred to as thin logical unit presents a logical storage space to one or more applications running on a host where different portions of the logical storage space may or may not have corresponding physical storage space associated therewith. However the thin device is not mapped directly to physical storage space. Instead portions of the thin storage device for which physical storage space exists are mapped to data devices such as device volumes which are logical devices that map logical storage space of the data device to physical storage space on the physical devices . Thus an access of the logical storage space of the thin device results in either a null pointer or equivalent indicating that no corresponding physical storage space has yet been allocated or results in a reference to a data device which in turn references the underlying physical storage space. Further a mapped LUN e.g. mapped devices may either be a direct mapped logical unit or thin logical unit.

Generally it is desirable to copy or replicate data for a variety of different reasons such as for example database related data may be critical to a business so it is important to make sure is not lost due to problems with the computer systems such as for example loss of electrical power. Some systems only copy replicate or back up data that has changed since the last such operation and this is known as incremental copying replicating or backup.

For mission critical applications requiring high availability of stored data various methods for enhancing data reliability are typically employed. One such method is to provide a mirror for each storage device. In a mirror arrangement data are written to at least two storage devices. Thus data may be read from either of the two storage devices so long as the two devices are operational and contain the same data.

In general copying data from a first location e.g. including one or more data volumes to a second may be done for a myriad of reasons including replication and backup versioning. In a replication operation a data set may be copied from the first location to the second to ensure that the second is a mirror of the first and that each stores a copy of the data set such that if there is a failure that results in the data set being inaccessible from the first location the second is available for access.

One illustrative method for forming a point in time copy of a data set is referred to as a snapshot and is described in detail in U.S. Pat. No. 6 792 518 to Armangau et al. which is incorporated herein by reference in its entirety.

A snapshot does not replicate a full copy of the data set referred to as a production data set . Rather the snapshot only stores differences between a current version of the production data set and the version of the data set at the point in time when the snapshot was last taken. Multiple snapshots can also be created at different times and can work together so that only the most recently created snapshot directly tracks changes to the production data set.

As should be appreciated from the foregoing snapshots can be used to determine previous states of a data set at past times without needing to make a full copy of the data set at those past times. Instead only the deltas or differences are stored in snapshots. Snapshot copies are in widespread use for on line data backup. If a file becomes corrupted the file is restored with its most recent snapshot copy that has not been corrupted.

A data storage system may be considered a source or local system and replication backup or other copying may be performed to a target or remote system. The term remote as used herein means being on a different storage system although source and target systems may actually be the same system but the data may be sent to a different storage device or even a different location on the same storage device in the same system. A remote system may include storage devices e.g. hard drives that may function to duplicate or simulate some or all of the local system on a volume by volume basis and that the volumes can by physical volumes or logical volumes. Devices and volumes in a logical sense are also used interchangeably throughout. With further regard to terminology copying is used throughout to generally refer to any way of duplicating data that is stored in one storage location e.g. production data on the source or local system to another storage location e.g. data copy on the target or remote system for any reason including replication backup restore or general mirroring.

In general in replication processes two or more data sets are maintained in a mirrored state such that there is a redundant copy of the data stored in the data set. A replication process may be synchronous i.e. data is stored to the mirror as it is committed to the production data set or asynchronous. Asynchronous replication volumes may therefore be a few seconds minutes or even hours out of sync with the data volume according to the particular replication processes implemented.

In some illustrative embodiments described below the data set for which a snapshot is taken to support replication is a logical unit mapped to a file system and the data units therefore are referred to as data blocks that are allocated to the file system. In addition the level of data units that are tracked as being changed need not correspond to data blocks but can be any suitable data unit. Similarly it should be appreciated that the embodiments of the technique described herein are not limited to use with a snapshot facility that employs particular data structures identified as a block or data unit and can be used with snapshot facilities that employ any suitable type of data structure or structures. In some implementations a snapshot may be adapted to be used only by replication processes and as such may only store information which is to be used by a replication process. However in accordance with other embodiments of the technique a snapshot that is used by a backup versioning process or any other data copying operation may also be used to support a replication process and accordingly the snapshot may store information usable by both types of processes.

In at least one embodiment of the current technique a data storage system may use at least two different snapshot copy facilities such that the first snapshot copy facility creates a copy on first write snapshot as described below herein in and the second snapshot copy facility creates a redirect on write snapshot based on a file system based snapshot copy utility as described below herein in . It should be noted that the current technique described herein is based on the second snapshot copy facility that creates a redirect on write snapshot.

The following definitions may be useful in understanding the specification and claims but are not meant to be limiting.

Base Logical Unit LUN may be a direct mapped logical unit DLU or thin logical unit TLU based on which a snapshot copy is created.

Fully provisioned Logical Unit LUN may be a LUN that completely allocates storage space accessible by a host at the time the LUN is created. Fully provisioned LUN may also be referred to as a traditional LUN that is based on a RAID group.

Thin LUN TLU may be a mapped LUN that does not allocate complete storage space required for the mapped logical unit at the time the mapped LUN is created. Instead storage space is allocated when a write request is issued to the mapped LUN. A thin LUN may include a set of slices. Each slice may be of 1 Gigabyte GB in size. Further a slice may include a set of data blocks. A data block may be of 8 Kilobyte KB in size.

Direct LUN DLU may be a logical unit that allocates storage space at the time the logical unit is created and uses an arithmetic computation for mapping an extent of the logical unit to a portion of the allocated storage space.

Slice may be a portion of a storage space within a storage pool. A slice may be of 1 gigabyte GB in size.

Pool may be a group of storage devices e.g. disks that are configured to be part of a storage pool. A storage pool may include a LUN e.g. DLU TLU . Further a storage pool may also include a snapshot mount point.

Reserved LUN pool RLP may be a set of LUNs that are used to store data of a copy on first write snapshot.

Snapshot may be a point in time copy also referred to herein as replica or snap of a logical object e.g. LUN a file system . Thus a snapshot may be a point in time copy of any one of the logical objects such as a base LUN e.g. DLU TLU another snapshot a consistency group CG and a snapshot mount point attached to a snapshot.

Consistency group CG may be a group represented by an object that includes a set of logical objects such that each logical object of the set of logical objects may be a base LUN a snapshot of a base LUN or a snapshot mount point representing a snapshot.

Snapshot mount point may be a logical abstraction of a LUN. A snapshot mount point is also referred to herein as a snapshot LUN that enables a user or host system to access and manage a snapshot. Thus a snapshot mount point enables a host to access and manage point in time copies of logical objects.

Source object may be a source of a snapshot such that a point in time copy is created based on the source object. A source object may be a logical object such as a base LUN a snapshot of a base LUN a consistency group or a snapshot mount point.

Snapshot family may be a set of snapshots and snapshot mount points associated with a base LUN. A snapshot family is also referred to herein as a replica family.

Mapped LUN MLU driver may be a component that manages logical units e.g. DLU TLU and snapshots of the logical units.

Referring to shown is an illustration of an example snapshot copy facility based on a copy on write method. In a backup versioning operation a copy on write method can be employed such that changes to a data set made after a point in time result in copies of the original data that was stored in a first data volume at the point in time being copied to a reserved LUN of a reserved LUN pool a data volume acting as for example a backup location before being overwritten in the first volume. In this way the data set can be restored or rolled back to the point in time. The copy on write method creates a copy on first write snapshot.

Thus a copy on first write COFW snapshot may be a point in time copy of a LUN based on a RAID group such that upon receiving a write I O request to the LUN original data stored on the LUN is first copied to a reserved LUN pool in order to preserve the point in time copy before performing the write I O request on the LUN. Thus for example as shown in upon receiving a write I O request from data source to source data data of snapshot is first copied to a reserved logical unit stored in the reserved LUN pool . Thus original snapped data is first copied to the reserved logical unit stored in the reserved LUN pool before writing new data associated with the write I O request to LUN . When data source issues a read I O request a snapshot mount point is first associated with the snap and the read I O request is then performed on the snapshot mount point by reading original data from the reserved logical unit stored in the reserved LUN pool and unchanged data from the source LUN .

Further a snapshot is any suitable collection of data structures that creates a copy on write point in time copy of a data set and store information relating to changes made to one or more data sets that affect both data units that store valid data for the data set when snapshot is created and those that do not. A snapshot may be associated with a point in time for example the time it was created or applied and adapted to store information regarding changes made to a data set after that point in time. The data set for the snapshot may be specified in any suitable manner including by specifying some collection of logically related storage objects e.g. a file system or database one or more volumes of storage or in any other suitable manner.

Referring to shown is an illustration of an example snapshot copy facility based on a redirect on write snapshot that may be included in an example embodiment using the techniques described herein. It should also be appreciated that at least some implementations of the example snapshot copy facility including alternative implementations may use a file system based snapshot copy facility also referred to herein as mapped replica process for maintaining a replica that does not require a copy on first write as described above herein in and does not require a save area. For example storage system replication may operate on fixed size extents of data residing on a LUN that is being replicated. Extents typically range from 512 bytes a disk sector to multiple megabytes MB . Further a LUN may be mapped to a file such that the file system based snapshot copy facility creates a replica of the file.

Typically the file system based snapshot copy facility also referred to herein as replication process creates a replica of a file by creating a version of the file using a delegated reference counting mechanism. The delegated reference counting mechanism is described in U.S. Pat. No. 8 032 498 entitled Delegated reference count base file versioning issued Oct. 4 2011 which is incorporated herein by reference in its entirety.

Thus a file may be replicated by using a snapshot copy facility that creates one or more replicas also referred to as snapshot copies of the file. A replica of a file is a point in time copy of the file. Further each replica of a file is represented by a version file that includes an inheritance mechanism enabling metadata e.g. indirect blocks and data e.g. direct data blocks of the file to be shared across one or more versions of the file.

In at least one embodiment of the current technique a redirect on write ROW snapshot may be a point in time copy of a base LUN that resides in a storage pool. In the case of a ROW snapshot of a base LUN unlike a COFW snapshot original data of the base LUN is not copied to a reserved LUN pool when writing to the base LUN. Instead with a ROW snapshot a write I O request to a base LUN writes data to a different storage pool or to a different location in a storage pool than the location at which the original data of the base LUN is stored.

A snapshot copy facility that creates a ROW snapshot also stores mapping information regarding changes between a ROW snapshot and a base LUN based on which the snapshot is created. Thus when a ROW snapshot is created based on a base LUN a new write I O request to the base LUN writes data to a new location in a storage pool. Further it should be noted that a ROW snapshot may be a point in time copy of any one of the logical objects such as a base LUN another ROW snapshot or a snapshot mount point. Thus as described above herein a redirect on write snapshot addresses a limitation of a copy on first write snapshot by not copying data to a reserved LUN pool. A ROW snapshot is based on a reserved LUN that is provisioned from a storage pool. In contrast a COFW snapshot is based on a LUN that is either provisioned from a RAID group or from a storage pool. Further a ROW snapshot may represent a point in time copy of either a snapshot mount point or a consistency group. Conversely a COFW snapshot can represent neither a point in time copy of a snapshot mount point nor a consistency group. A data storage system may choose a replication process based on the type of snapshot that is required by a user.

For example as shown in a write I O request received from data source writes new data associated with the write I O request to a new location in the storage pool without changing or copying original data . Further a read I O request directed to snap is performed by first associating a snapshot mount point with the snap and performing the read I O request on the snapshot mount point which enables data source to read snapped data from original location and new data from new location .

Replication processes may be carried out in any suitable manner. shows one example of performing replication that may be implemented by a replication facility in accordance with some of the embodiments described herein. It should be appreciated however that is merely illustrative of replication processes that may be implemented as implementations of the technique are not limited in this respect. In the illustrative example described below the replication process is described as operating on a data set. It should be appreciated that the data set may be any volume of storage or any collection of logical storage objects such as a file system or a database as the aspects of the technique described herein are not limited to use with replicating any particular type of data set.

Further a snapshot may be restored also referred to herein as roll back to a LUN representing a specific point in time copy. A restore operation replaces working data of a LUN with data of a specific point in time copy of the LUN. In at least one embodiment of the current technique a snapshot may be created and restored without copying data from one location to another.

In at least one embodiment of the current technique a snapshot may be created and then deleted at a later time. However a snapshot may not be available for performing an I O operation on the snapshot unless a snapshot mount point is associated with the snapshot. Associating a snapshot mount point to a snapshot is also referred to herein as attaching the snapshot mount point to the snapshot. Thus a snapshot mount point associated with a snapshot provides a set of interfaces for accessing data of the snapshot and performing I O operations e.g. read write on the snapshot.

In at least one embodiment of the current technique a user specifies a logical object such as a LUN or a snapshot of a LUN as a source object for creating a snapshot. A snapshot of a source object is created in the same storage pool in which the source object resides. A snapshot mount point is an abstraction of a logical object such as a LUN that does not include any data at the time the snapshot mount point is created. Thus an I O operation such as read and write to a snapshot mount point fails until the snapshot mount point is associated with a snapshot. When a snapshot mount point is associated with a snapshot an I O operation such as read and write directed to the snapshot mount point is performed on data of the snapshot.

In at least one embodiment of the current technique a snapshot mount point for a base LUN is created in the same storage pool in which the base LUN resides. Further a snapshot mount point is associated with a snapshot derived from a base LUN. A snapshot mount point is associated with a default set of properties at the time the snapshot mount point is created. When a snapshot mount point is attached to a snapshot a set of properties of the snapshot is associated with the snapshot mount point. Further at the time a snapshot mount point is detached from a snapshot the snapshot mount point includes a set of properties of the snapshot. A user may need to specify a snapshot and a snapshot mount point when attaching the snapshot mount point to the snapshot.

In at least one embodiment of the current technique a consistency group CG may be a group represented by an object that includes a set of logical objects such that each logical object of the set of logical objects may be a base LUN a snapshot of a base LUN or a snapshot mount point representing a snapshot. Logical objects in a consistency group are treated as a single object in order to create consistent snapshot copies of the consistency group. Thus a consistency group may be created by grouping together a set of LUNs e.g. DLU TLU . Alternatively a consistency group may also be created by grouping together a set of snapshot mount points. Consequently a consistency group is a collection of related logical volumes that need to be kept in a consistent state. Thus a consistency groups maintain write ordered data consistency across logical volumes included in the consistency group.

In at least one embodiment of the current technique a consistency group may be considered as a named list of logical objects such as LUNs and snapshot mount points. A consistency group may be created modified and deleted. Further a consistency group may be indicated as a source for creating a snapshot thereby resulting in creation of a consistent snapshot.

In at least one embodiment of the current technique a consistency group may be created by grouping together a set of snapshot mount points. When creating a consistency group a user may need to provide a name for the consistency group and optionally specify a list of snapshot mount points. Each snapshot mount point of a list of snapshot mount points included in a consistency group may be associated with a snapshot of a base LUN e.g. a DLU and TLU . If a user does not provide a name when creating a consistency group the mapped LUN driver may generate a unique name and assigns the unique name to the consistency group. The state of a consistency group is defined based on states of snapshot mount points that are included in the consistency group. Thus if any one of the snapshot mount point that is part of a consistency group is in an erroneous state such as a faulted or an offline state the state of the consistency group is defined as offline or faulted respectively.

In at least one embodiment of the current technique a consistent snapshot is a point in time copy of a consistency group. Thus a consistent snapshot is similar to a snapshot of a LUN. However instead of including a single point in time copy of a LUN a consistent snapshot includes a point in time copy of a list of logical objects e.g. LUN snapshot snapshot mount point that are part of a consistency group thereby indicating point in time copies of each logical object of the list of logical objects. A consistent snapshot maintains a write order consistency for each logical object of the list of logical objects included in a consistency group. A consistent snapshot may be created by using any one of the two mechanism described below herein. First a consistent snapshot may be created by specifying a consistency group as a source object for creating a snapshot. Second a consistent snapshot may be created by specifying another consistent snapshot as a source object for creating a snapshot thereby resulting in a snapshot of the other consistent snapshot.

A snapshot of a consistency group also referred to herein as snapshot CG includes a set of logical objects based on which snapshots may be created such that a write order consistency is maintained for the snapshots. A consistency group is identified by a name and includes information e.g. names of logical objects regarding a set of logical objects e.g. LUN snapshot of a LUN . When a snapshot copy of a consistency group is created a consistent snapshot is created such that the consistent snapshot is identified by a single name and represents a snapshot of each logical object in the set of logical objects that are created at the same point in time.

Further when a snapshot of a consistency group is created respective snapshot of each base LUN of the set of base LUNs is created at the same point in time thereby preserving a write order for the set of base LUNs. A consistent snapshot is also referred to herein as a snapshot set. Typically a set of LUNs used by an application are organized as a consistency group in order to create consistent snapshots of the set of LUNs.

In at least one embodiment of the current technique a snapshot set may be a set that includes snapshots from one or more snapshot families. A snapshot set results when a snapshot of a consistency group is created. A snapshot set is accessible to a user as a snapshot of one or more LUNs. A snapshot set is also referred to herein as a replica set.

In at least one embodiment of the current technique a snapshot mount point may be attached to a consistent snapshot. Further one or more snapshot mount points may be attached to one or more snapshots included in a consistent snapshot as the consistent snapshot includes snapshots of one or more source logical objects.

Referring to shown is an illustration of creating a snapshot of a LUN in a data storage system. In at least one embodiment of the current technique a snapshot copy facility creates snapshot copies of LUN at different point in times. For example snap is created at 9 00 AM snap is created at 10 00 AM and snap is created at 11 00 AM.

Referring to shown is an illustration of attaching a snapshot mount point to a snapshot of a LUN in a data storage system. With reference also to a snapshot mount point is attached to snap . Thus a host or user may perform an operation on the snapshot snap by using the snapshot mount point shown as snapLUN  in . The snapshot snap is marked as dirty indicating that an I O operation directed to the snapshot mount point attached to the snapshot may change contents of the snapshot .

Referring to shown is an illustration of creating a snapshot of a snapshot mount point attached to a snapshot of a LUN in a data storage system. With reference also to a snapshot copy facility creates a snapshot of the snapshot mount point . The snapshot of the snapshot mount point indicates a point in time copy of the snapshot mount point .

Referring to shown is an illustration of creating a snapshot set in a data storage system. With reference also to a snapshot of a consistency group results in a snapshot set such as snapset . that represents snapshots such as LUN . LUN . of logical objects e.g. LUN  LUN  included in the consistency group . Further for example second snapshot set may include snapshot copies such as LUN . and LUN . . Further for example third snapshot set may include snapshot copies such as LUN . and LUN . . Further for example fourth snapshot set may include snapshot copies such as LUN . and LUN . .

Referring to shown is an illustration of attaching a snapshot mount point to a snapshot set in a data storage system. With reference also to snapshot mount points and are attached to the snapshot set such that a user or host may access the snapshot set and perform an operation on the snapshot set .

Referring to shown is an illustration of creating a consistency group including a set of snapshot mount points in a data storage system. With reference also to a consistency group is created by grouping together a set of snapshot mount points such as snapLUN  and snapLUN  . Consequently a user host or an application may now access the consistency group in such a way that the set of snapshot mount points included in the consistency group are treated as a single logical object. Thus creating a snapshot of the consistency group results in a snapshot set similar to a snapshot set created when a snapshot of a consistency group including base LUNs is created as shown in . Referring back to for example a snapshot set snapset . is created by creating a snapshot of consistency group at a specific point in time e.g. 9 10 AM . Further a snapshot set snapset . is created by creating a snapshot of consistency group at another specific point in time e.g. 9 15 AM .

Referring to shown is more detailed representation of components that may be included in an embodiment using the techniques herein. In at least one embodiment of the current technique snapshot logic creates a snapshot of a base LUN which resides in storage pool . The snapshot is attached to a snapshot mount point in order to access the snapshot for performing an operation on the snapshot. A user host or an application may then access the snapshot mount point similar to how a LUN is accessed. The user host or application may perform an operation such as an I O operation LUN shrink operation rename operation restore operation expand operation and rollback operation on the snapshot mount point. Thus any one of the operations that may be performed on a LUN may also be performed on a snapshot by associating the snapshot with a snapshot mount point.

A snapshot of a snapshot mount point may also be created such that the snapshot represents a point in time copy of the snapshot mount point. Restore logic provides a mechanism to for example restore or roll back a LUN to a previous point in time replica. Additional details regarding the restore mechanism is described below with reference to . Consistency group management logic enables a user host or an application to create a consistency group of snapshot mount points such that the group of snapshot mount points are treated as a single logical object and write consistency is maintained when performing I O operations on snapshots associated with the snapshot mount points included in the consistency group. Further snapshot logic may create a snapshot of the consistency group thereby creating a consistent snapshot. Further a consistency group may store information regarding a set of snapshot mount points included in the consistency group in a persistent database on a data storage system.

Referring to shown is an illustration of restoring a snapshot of a LUN in a data storage system. In an example embodiment of the current technique a snapshot of LUN  is taken every half hour. For example Snap  is created at 9 00 Snap  is created at 9 30 AM Snap  is taken at 10 00 AM and Snap  is taken at 10 30 AM.

Referring to corrupted data is detected at 10 42 AM. A user may decide to restore the snapshot that was taken at 10 00 AM. At 10 52 AM a restore operation of Snap  i.e. the snapshot taken at 10 00 AM is initiated. Referring to the system performs the restore asynchronously. The system may also create a protective snapshot e.g. Snap  of the LUN that was restored before the data is applied and transitions to the restored snapshot. This protective snapshot Snap  may be used for example in the event the restored snap is also corrupted i.e. Snap  . A user may restore Snap  to revert to a state prior to the attempted restore.

Referring to shown are illustrations depicting a technique for restoring multiple LUNs from a snapshot set. Restoring a consistency group having multiple LUNs from a snapshot set employs techniques similar to those used to restore an individual LUN and makes use of snapshot sets such as those described above with reference to . In at least one example embodiment the snapshot set contains a list of its base LUNs for which it has snapshot data. These base LUNs may be restored by the system nearly instantaneously. The system may also create a new snapshot set that contains the pre restore state of the base LUNs. Current consistency group membership has no bearing on restoring from a snapshot set as consistency group membership may change over time and the snapshot set contains a particular set of LUNs at that point in time. In alternative example embodiments the snapshot sets may include snapshots of snap mount points i.e. a point in time copy of snapshot mount points such as those described above in conjunction with . In another alternative embodiment individual base LUN snapshots contained in the set may be restored without restoring all base LUNs.

Referring to an example embodiment illustrating snapshots of snap mount points is shown. In this example consistency group CG  includes multiple SMPs e.g. SMP  and SMP  where snapshot sets of CG  are taken every 5 minutes. For example SnapSet  is created at 9 00 AM SnapSet  is created at 9 05 AM SnapSet  is created at 9 10 AM and SnapSet  is created at 9 15 AM. Referring now to corrupted data is detected at 9 22 AM. A user may decide to restore the snapshot set SnapSet . which was taken at 9 10 AM. The system performs the restore almost immediately. The system may also create a new protective snapshot set at 9 22 AM e.g. SnapSet . representative of consistency group CG  as it was just before the data is applied and transitions to the restored snapshot. This protective snapshot set SnapSet . may be used for example in the event the restored snapshot set i.e. SnapSet . is also corrupted to allow a user to restore SnapSet . in order to revert to the state prior to the attempted restore.

While the invention has been disclosed in connection with preferred embodiments shown and described in detail their modifications and improvements thereon will become readily apparent to those skilled in the art. Accordingly the spirit and scope of the present invention should be limited only by the following claims.

