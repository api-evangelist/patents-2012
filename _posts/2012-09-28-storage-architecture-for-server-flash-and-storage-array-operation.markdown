---

title: Storage architecture for server flash and storage array operation
abstract: A storage architecture of a storage system environment has a storage connector interface configured to exchange data directly between flash storage devices on a server and a storage array of the environment so as to bypass main memory and a system bus of the server. According to one or more embodiments, the storage connnector interface includes control logic configured to implement the data exchange in accordance with one of a plurality of operational modes that deploy and synchronize the data on the flash storage devices and the storage array. Advantageously, the storage connector interface obviates latencies and bandwidth consumption associated with prior data exchanges over the main memory and bus, thereby enhancing storage architecture performance.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09146684&OS=09146684&RS=09146684
owner: NetApp, Inc.
number: 09146684
owner_city: Sunnyvale
owner_country: US
publication_date: 20120928
---
The present disclosure relates to storage system environments and more specifically to a storage architecture of a storage system environment.

A storage system environment may include a server configured to provide storage service relating to the organization of data on a storage array of writable persistent storage media such as disks. The storage system environment may employ a storage architecture that enables the server to serve the data from the storage array in file system and block formats with high reliability and integrity through the use of data protection and management techniques such as tiered storage persistent point in time read only images of the data and or Redundant Array of Independent or Inexpensive Disks RAID implementations. However access to the data stored on the disks may require the server to perform frequent input output I O operations over one or more internal system buses to the storage array which could adversely impact performance of the storage architecture.

Prior attempts to improve performance of the storage architecture included the use of solid state storage media such as flash storage devices to serve data stored on the server. Although the use of such storage may improve the access performance to the data flash devices generally have limited storage capacity and a high cost per terabyte of storage compared to disks . In addition the flash devices generally have no credible capability to protect and or manage the data at large scale. Accordingly protection of data stored on the server flash storage is often realized through techniques such as server replication i.e. replication of the data among a plurality of servers. Yet server replication may also require frequent data exchanges between the flash devices and main memory of the server over one or more system buses resulting in large amounts of data traffic over the buses prior to forwarding of the traffic to the other servers. Such data traffic typically consumes large amounts of memory bandwidth thereby adversely impacting performance of the storage architecture.

Embodiments described herein provide a storage architecture of a storage system environment having a storage connector interface configured to exchange data directly between flash storage devices on a server and a storage array of the environment so as to bypass main memory and a system bus of the server. The storage connector interface illustratively includes control logic configured to implement the data exchange in accordance with one of a plurality of operational modes that deploy and synchronize the data stored on the flash storage devices and the storage array. Advantageously the storage connector interface obviates latencies and bandwidth consumption associated with prior data exchanges over the main memory and bus thereby enhancing storage architecture performance.

In an embodiment the storage connector interface is illustratively contained on a storage connector adapter that also includes the flash storage devices coupled to an input output I O journal. The I O journal may be configured to temporarily record log one or more write operations received from an application executing on the server to process e.g. modify data stored on the flash storage devices. Specifically the I O journal may be configured to cooperate with the storage connector interface to log the write operation in accordance with an ordering constraint used to implement the operational mode thereby providing a consistent recovery point for the data in the event of a failure that interrupts deployment and synchronization of the data. To that end the I O journal may be further configured to provide a last state retention capability that logs a current state of the data with respect to storage on the flash devices and or storage array and that further enables re processing of the logged write operation in the event of the failure.

In one or more embodiments the server may include a processor a main memory and a storage connector adapter interconnected by a system bus . The main memory may comprise storage locations that are addressable by the processor and adapter for storing software programs and data structures associated with the embodiments described herein. The processor and adapter may in turn comprise processing elements and or logic circuitry configured to execute the software programs and manipulate the data structures. An operating system portions of which are typically resident in main memory and executed by the processing elements functionally organizes the server by inter alia invoking operations in support of one or more applications e.g. a table oriented database application executing on the server. A suitable operating system may include the UNIX series of operating systems and the Microsoft Windows series of operating systems however in an embodiment described herein the operating system is illustratively the Linux operating system. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used to store and execute program instructions pertaining to the embodiments herein.

The storage connector adapter comprises the mechanical electrical and signaling circuitry needed to connect the server to the storage array over network . As described herein the storage connector adapter may interact with the storage array to exchange data in accordance with a plurality of operational modes that deploy and synchronize the data stored on the server with the data stored on the storage array. Accordingly the storage connector adapter may include control logic configured to generate and issue packets including file based access protocols such as the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when exchanging data in the form of files with the storage array . Alternatively the adapter may issue packets including block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI serial attached SCSI SAS and SCSI encapsulated over Fibre Channel FC when exchanging data in the form of logical units LUNs .

In an embodiment the storage array is illustratively embodied as a storage system comprising a processor a memory one or more network adapters and a storage adapter interconnected by a bus . Each network adapter includes circuitry needed to connect the server to the storage array over network . The storage array also includes a storage operating system that provides a virtualization system and in particular a file system to logically organize the data as a hierarchical structure of named directory file and LUN storage objects on disks . The file system may be configured to provide volume management capabilities for use in block based access to the data stored on disks . These capabilities include i aggregation of the disks ii aggregation of storage bandwidth of the disks and iii reliability guarantees such as synchronous mirroring and or parity RAID .

The file system also has the capability to generate a persistent point in time read only image or snapshot of data stored on the disks . The snapshot is a space conservative point in time read only image of data that provides a consistent image of the data at some previous time. More particularly a snapshot is a point in time representation of a storage element such as an active file system file or database stored on a storage device e.g. on disk or other persistent memory and having a name or other identifier that distinguishes it from other snapshots taken at other points in time. A snapshot can also include other information metadata about the active file system at the particular point in time for which the image is taken.

Storage of data on the storage array may be implemented as one or more storage volumes that comprise a cluster of the disks defining an overall logical arrangement of disk space. The disks within a volume are typically organized as one or more RAID groups. RAID implementations enhance the reliability integrity of data storage through the writing of data stripes across a given number of physical disks in the RAID group and the appropriate storing of redundant information with respect to the striped data. The redundant information enables recovery of data lost when a storage device fails.

The storage adapter may cooperate with the storage operating system to access e.g. store via a write operation or retrieve via a read operation data requested by the server . The storage adapter may include I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC serial link topology. The data may be stored on disk by the storage adapter and upon completion an acknowledgement may be generated by the processor or the adapter prior to being forwarded over the bus to the network adapter where the acknowledgement is formatted into one or more packets and forwarded to the storage connector adapter of server .

The I O journal includes a controller configured to control access to electronic storage that may be embodied as a portion of the flash storage devices organized as e.g. a circular buffer. In an embodiment though the electronic storage of the I O journal may be embodied as solid state non volatile random access memory NVRAM having a plurality of entries . The NVRAM may also include either a back up battery or other built in last state retention capability e.g. non volatile semiconductor memory that is capable of maintaining data in light of a failure to the server and storage system environment. As a result the NVRAM of the I O journal may be configured to temporarily record log one or more write operations received from application executing on the server to process e.g. modify data stored on the flash storage devices .

According to one or more embodiments the storage architecture of the storage system environment may be enhanced through a storage connector interface configured to exchange e.g. forward data directly between the flash storage devices and the storage array so as to bypass main memory and system bus of server . In an embodiment the storage connector interface includes control logic configured to implement the data exchange in accordance with one of a plurality of operational modes that deploy and synchronize the data stored on the flash storage devices and the storage array . The storage connector interface may also include a plurality of queues configured and arranged to enforce one or more ordering constraints used to implement the operational modes as described herein.

In an embodiment the I O journal may be configured to cooperate with the storage connector interface to log a write operation issued by application in accordance with the ordering constraint used to implement the operational mode thereby providing a consistent recovery point for the data in the event of a failure that interrupts deployment and synchronization of the data on the flash storage devices and storage array . To that end the last state retention capability of the I O journal may enable logging of a current state of the data with respect to storage on the flash devices and or storage array to thus further enable re processing of the logged write operation in the event of the failure. As described herein the operational modes illustratively include a synchronous mirroring mode Mode A a journaled forced ordering mode Mode B a journaled partial ordering mode Mode C and an out of order mode Mode D .

According to Mode A a write operation request for a data set data may be issued by application and logged as an entry in the NVRAM by the I O journal controller . The write request may be processed by the processor to e.g. modify the data for storage on the flash storage devices . In response the I O journal controller may assert a flash bit FB associated with the logged entry indicating successful storage of the modified data on the flash storage devices . Concurrently the write request may be passed to the storage connector interface where it is temporarily stored on one or more queues prior to being forwarded i.e. synchronously mirrored as one or more packets to the storage array . The control logic of the storage connector interface may cooperate with the I O journal controller to assert a dirty bit DB associated with the logged entry . In an embodiment the DB indicates that the write request of the entry is dirty not completely processed pending acknowledgement of storage on the array. Thus in the event of a failure to the storage system environment prior to completion of storage of the mirrored data on the storage array the write request can be replayed e.g. re processed from the logged entry in the I O journal. Upon receiving an acknowledgement indicating completion of the write request at the storage array the storage connector interface forwards the acknowledgement to the I O journal controller which de asserts the DB indicating completion of the synchronous write request and notifies e.g. via an application programming interface API acknowledgement the application of the completion.

In essence Mode A enforces order by instituting synchronous mirroring on a per write request basis that is as each write request is issued by application it is synchronously forwarded to the storage array . However the write request is not considered completed until the modified data is stored on both flash storage and the storage array . Mode A is thus the safest of the operational modes that provides fast read access to the data without accelerating write requests. Accordingly this synchronous mode of operation may be used in banking transactions stock trading online transaction processing applications and other deployments where synchronous mirroring of data e.g. to a persistent storage array is mandatory.

In an embodiment Mode A may be extended to further enforce ordering on a multiple write i.e. batch request basis. Assume that while waiting for acknowledgement of completion a synchronous write request from the storage array multiple write requests are issued by application . These write requests may be processed and temporarily stored on the queues as described above waiting for the acknowledgement. Upon receiving the acknowledgement the queued write requests are then forwarded to the storage array as a first synchronous batch request. While waiting for acknowledgement of the first synchronous batch request a second batch of requests may be issued processed and temporarily stored on the queues. As long as the second batch of requests is not forwarded to the storage array until completion of the first synchronous batch request is acknowledged by the array this embodiment of Mode A may provide substantial performance improvement.

According to Mode B a write request for data may be issued by application and logged as entry in the NVRAM by the I O journal controller prior to modification of the data by the processor for storage on the flash storage devices . Upon successful storage on the devices the I O journal controller may assert the FB associated with the logged entry and notify the application of completion e.g. via the API acknowledgement of the write request. The write request may be passed to the storage connector interface where it is temporarily stored on one or more queues to enable enforcement of the forced ordering constraint. Specifically the control logic of the storage connector interface may forward the write request to the storage array as soon as possible subject to the constraint that it be ordered with respect to other write requests issued by application . Thus unlike Mode A Mode B does not require acknowledgement of storage of the modified data on the storage array before completion of the write request is acknowledged. However all write requests processed in accordance with the forced ordering constraint of Mode B are assumed e.g. by application to be fully ordered and dependent.

Implementation of Mode C is generally similar to Mode B except that acknowledgement of write request completion to application may be delayed by a short but adjustable period of time e.g. 1 millisecond . Here a write request for data may be issued by application and logged as entry in the NVRAM by the I O journal controller prior to modification of the data by the processor for storage on the flash storage devices . Upon successful storage on the devices the I O journal controller may assert the FB associated with the logged entry and delay notification of completion of the write request to application for the adjustable period of time. The write request may be passed to the storage connector interface where it is temporarily stored on one or more queues to enable enforcement of the partial ordering constraint.

Notably the adjustable delay allows additional write requests issued by application to be aggregated during this period of time before forwarding by the storage connector interface as one or more packets to the storage array . Any additional write requests received during the period of time may be known e.g. by application to be independent. In an embodiment the control logic may cooperate with the I O journal controller to enforce the partial ordering constraint via implementation of a queuing algorithm that considers inter alia the number of aggregated write requests before forwarding the aggregation to the storage array. Such cooperation further enables the I O journal controller to record a set of partially ordered write requests. In another embodiment the independent write requests may be forwarded by the storage connector interface to the storage array concurrently to thereby increase throughput.

Implementation of Mode D is generally similar to Mode C except that write requests may be forwarded to the storage array concurrently without order i.e. out of order. Here a write request for data may be issued by application and logged as entry in the NVRAM by the I O journal controller prior to modification of the data by the processor for storage on the flash storage devices . Upon successful storage on the devices the I O journal controller may assert the FB associated with the logged entry and delay notification of completion of the write request to application for the adjustable period of time so as to enable aggregation of additional write requests. The write requests may be passed to the storage connector interface where they are temporarily stored on one or more queues prior to forwarding without order to the storage array . Once the out of order requests are forwarded to the storage array the operational mode may temporarily shift to Mode A and wait for acknowledgement of completion of storage of the out of order requests on the storage array. The control logic of the storage connector interface may then request that the storage array perform a snapshot of the data stored on the disks thereby establishing a consistent recovery point in the event of a failure. Subsequently the operational mode may revert back to Mode D.

Advantageously the storage connector interface obviates latencies and bandwidth consumption associated with prior data exchanges over the main memory and system bus thereby enhancing performance of the storage architecture. In addition cooperation between the storage connector interface and I O journal enables implementation of the operational modes with ordering constraints to thereby provide one or more consistent recovery points for the data in the event of a failure that interrupts deployment and synchronization of the data between the flash storage devices and storage array .

According to one or more embodiments failure of the storage system environment may result from e.g. an unexpected loss of power to the server or storage array and or disconnection of the server from the array. In response to such failure the server and storage array may be initialized by a management tool such as a script running an API or an administrator entering command via a console not shown of the storage system environment. Upon power up the server e.g. the storage connector adapter may come up in one of the following states Normal Recovery Required or Failed.

In the Normal state the storage connector adapter shut down cleanly such that the entries of NVRAM were properly flushed processed and acknowledged and no further action was required. In other words all logged write requests were recorded in the I O journal their associated write data was processed and stored on the flash storage devices and or storage array and in the case of synchronous mirroring an acknowledgment that the modified data of the mirrored request was properly stored on the storage array was received and recorded by the I O journal for every logged entry .

In the Recovery Required state the storage connector adapter shut down without a chance to drain completely process any pending write requests recorded in entries of the NVRAM . Upon power up the I O journal controller may determine that there are one or more dirty logged write request entries via assertion and or deassertion of the respective DB and or FB in the journal . Accordingly the controller may replay each dirty entry by e.g. passing the write request to the storage connector interface for forwarding to the storage array and or notifying the application that the write request was not successfully stored on the flash storage devices . Upon receiving acknowledgement that the write request was successfully processed and stored the entry of the journal may be cleaned i.e. the respective DB and or FB deasserted and or asserted .

In the Failed state the current state of the storage array may not be consistent with the current state of the flash storage . Accordingly the management tool may re initialize the flash storage and storage array e.g. to empty states or request that the storage connector adapter trust either the current state of the flash storage or the current state of the storage array . In the event of the latter the adapter and array may thereafter reconcile their states and be brought on line.

While there have been shown and described illustrative embodiments of a storage architecture having a storage connector interface configured to exchange data directly between flash storage devices on a server and a storage array of a storage system environment it is to be understood that various other adaptations and modifications may be made within the spirit and scope of the embodiments herein. For example the embodiments have been shown and described herein with relation to deployment and synchronization of the data e.g. via a write request on the flash storage devices of the storage connector adapter and the storage array . However if the write request to the storage array fails during normal operation of the storage system environment then the storage connector adapter may fail all pending and future write requests that it receives e.g. from application . Read requests though may be processed normally. Subsequently write requests to the storage array may be retried periodically and if successful operation of the storage system environment returns to normal.

The foregoing description has been directed to specific embodiments. It will be apparent however that other variations and modifications may be made to the described embodiments with the attainment of some or all of their advantages. For instance it is expressly contemplated that the components and or elements described herein can be implemented as software being stored on a tangible non transitory computer readable medium e.g. disks and or CDs having program instructions executing on a computer hardware firmware or a combination thereof. Accordingly this description is to be taken only by way of example and not to otherwise limit the scope of the embodiments herein. Therefore it is the object of the appended claims to cover all such variations and modifications as come within the true spirit and scope of the embodiments herein.

