---

title: Method, apparatus, and system for modeling passive and active user interactions with a computer system
abstract: A method, apparatus, and system for modeling user interactions with a computer system associates semantic descriptions of passive and active user interactions, which are meaningful at a user level, with application events and user interaction data as a user interacts with one or multiple software applications with a computing device, and uses those associations to build and maintain a user-specific contextual model. In some embodiments, the contextual models of multiple users are leveraged to form one or more collective contextual user models. Such models are useful in many different applications.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09152221&OS=09152221&RS=09152221
owner: SRI INTERNATIONAL
number: 09152221
owner_city: Menlo Park
owner_country: US
publication_date: 20120928
---
This application claims the benefit of and priority to U.S. Provisional Patent Application Ser. No. 61 648 503 filed May 17 2012 which is incorporated herein by this reference in its entirety.

In a typical traditional computing system environment users interact with multiple different software applications e.g. email Internet browsers word processing calendars and so on. The semantic meaning of each interaction with each application may be implicitly known by the particular application. For example an email application may be aware of particular high level functions such as sending a message performed by the user within that application. However the computing system generally does not maintain a collective semantic model of all of the meaningful activities the user has performed or is performing across multiple different applications. In other words the computing system may be aware that an email application and a calendar application are both currently executing and may be aware of system level calls that those applications make but the system does not know what the user is functionally doing within each of those applications and thus is unable to communicate that information in a form that is understandable to the user. Rather that level of semantic awareness typically remains confined within the individual applications. This issue is compounded in the context of a collaborative group of users as a traditional computing system is not aware at a high level of what each of the individual users is doing across various applications it is all the more unaware of the significance of those actions for the group.

According to at least one aspect of this disclosure a method for modeling user activity with regard to multiple different software applications on a computing device includes with the computing device receiving gaze tracking data comprising one or more real time sensor inputs indicative of a user s gaze in relation to a display of the computing device identifying a user interface element displayed at a location on the display corresponding to the gaze tracking data obtaining a semantic description of the user interface element where the semantic description includes information about the user interface element associating the semantic description with the gaze tracking data and using the association of the semantic description with the gaze tracking data to model user activity at the computing device.

The user interface element may include text graphic video interactive content and or an interactive control. The semantic description may include functional information about the user interface element. The semantic description may include information about content of the user interface element. The method may include identifying from the different software applications a software application associated with the user interface element. The method may include obtaining the semantic description from the software application. The method may include storing the associations of semantic descriptions with gaze tracking data over time and developing a model of user activity in relation to the computing device based on the stored associations. The stored associations may relate to content viewed at the computing device.

The semantic descriptions may relate to user interface elements displayed by the different software applications and the method may include developing a model of user activity with respect to the multiple different software applications based on the stored associations. The gaze tracking data may include data relating to the duration of user attention to the user interface element and the method may include generating an inference based on the data relating to the duration of user attention to the user interface element. The inference may relate to user cognitive processing of content of the user interface element. The gaze tracking data may include data relating to the recency or frequency of user attention to the user interface element and the method may include generating an inference based on the recency or frequency of user attention to the user interface element.

The method may include developing a model of user activity in relation to the computing device based on associations of semantic descriptions with gaze tracking data over time initially storing the associations in a short term memory portion of the model and moving the associations to a long term memory portion of the model after a period of time. The method may include determining the period of time based on the recency of user attention to the user interface element the time of day demographic information location or context. The method may include receiving from one or more of the plurality of different software applications data relating to user interaction with the user interface element and associating the interaction data with the gaze tracking data. The method may include associating a cognitive processing parameter relating to the gaze tracking data wherein the cognitive processing parameter indicates a degree of user cognitive processing of content of the user interface element. The method may include receiving application event data from a software application at the computing device obtaining a semantic description of the application event the semantic description comprising information about the application event and using the association of the semantic description with the application event data to model user activity at the computing device.

The method may include receiving hand position data comprising one or more real time sensor inputs indicative of the proximity of at least a portion of a user s hand in relation to a component of the computing device identifying an on screen location corresponding to the hand position data obtaining a semantic description of a user interface element located at the on screen location the semantic description comprising information about the user interface element associating the semantic description with the hand position data and using the association of the semantic description with the hand position data to model user activity at the computing device.

The method may include receiving typing force data comprising one or more real time sensor inputs indicative of the amount of force or pressure applied by a user to a component of the computing device identifying an on screen location corresponding to the typing force data obtaining a semantic description of a user interface element located at the on screen location the semantic description comprising information about the user interface element associating the semantic description with the typing force data and using the association of the semantic description with the typing force data to model user activity at the computing device.

According to at least one aspect of this disclosure a computing system to develop a semantic model of user attention to user interface elements of a plurality of software applications includes a display a sensor subsystem to obtain gaze tracking data the gaze tracking data being indicative of a user s gaze in relation to the display a framework embodied in one or more machine accessible media the framework configured to over time determine locations on the display corresponding to the gaze tracking data identify user interface elements displayed by the software applications at each of the locations corresponding to the gaze tracking data obtain a semantic description of each of the user interface elements each of the semantic descriptions comprising information about the corresponding user interface element and associate the semantic descriptions with the gaze tracking data and a model embodied in one or more machine accessible media the model configured to store data relating to associations of the semantic descriptions with the gaze tracking data. The framework may be configured to communicate with the plurality of software applications to obtain the semantic descriptions. The framework may be configured to process the user interface elements to obtain the semantic descriptions.

According to at least one aspect of this disclosure a system for modeling user attention to user interface elements displayed by a plurality of software applications on a computing device the system embodied in one or more machine accessible storage media includes a contextual model comprising data relating to a plurality of real time sensor inputs received at a computing device the real time sensor inputs being indicative of a user s gaze in relation to a display of the computing device locations on the display corresponding to the real time sensor inputs and user interface elements displayed at the locations corresponding to the real time sensor inputs and a framework configured to derive gaze tracking data from the real time sensor inputs the gaze tracking data indicating an aspect of user attention to the user interface elements determine semantic descriptions of the user interface elements each of the semantic descriptions comprising information about the corresponding user interface element associate the semantic descriptions with the gaze tracking data and store data relating to the associations of the semantic descriptions with the gaze tracking data in the contextual model.

The system may include a cognitive processing model to determine a degree of user cognitive processing of the user interface elements. The framework may determine whether to store the data based on the degree of user cognitive processing. The system may include a locomotive interaction model to analyze the time to perform user interactions with the computing device.

According to at least one aspect of this disclosure a method for modeling user activity with regard to a plurality of different software applications on a computing device includes with the computing device receiving passive interaction data comprising one or more real time sensor inputs indicative of a passive user interaction with the computing device the passive user interaction being an interaction that does not result in an application event receiving active interaction data indicative of an active user interaction the active user interaction being an interaction that results in an application event identifying user interface elements displayed at on screen locations of the display corresponding to the passive and active interaction data obtaining semantic descriptions of the user interface elements the semantic descriptions comprising information about the user interface elements associating the semantic descriptions with the corresponding passive and active interaction data and using the associations of semantic descriptions with the passive and active interaction data to model user activity at the computing device.

While the concepts of the present disclosure are susceptible to various modifications and alternative forms specific embodiments thereof are shown by way of example in the drawings and are described in detail below. It should be understood however that there is no intent to limit the concepts of the present disclosure to the particular forms disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives consistent with the present disclosure and the appended claims.

Computer based methods and apparatus are disclosed for maintaining a semantically meaningful model of a user s interactions with multiple applications of a computing system. In some embodiments interactive applications of the computing system comprise or are enhanced with a high level description of each action that a user can potentially request or initiate. As and when an action is processed by an application the corresponding description of that action is published or posted to a stored model. Interactive applications may further comprise or be enhanced with a high level description of the content that is displayed by the application in various contexts. The current location of a user s gaze with respect to the screen or display of the computing may be tracked by a sensor subsystem. The application that is responsible for the displayed content being viewed by a user at a given moment as per the user s tracked gaze may report or post the corresponding description of that content to the stored model. The stored model may include short term and long term repositories as well as parameters representing a rough model of the user s relevant cognitive and locomotive capabilities e.g. duration of memories minimum times required to process displayed content or to readjust gaze etc. .

In this way a semantically meaningful contextual model of what a user is seeing or has seen and is doing or has done is maintained with respect to the user s activities across multiple different software applications. In various embodiments this model is leveraged by the system to facilitate intelligent automated analysis of how best to assist the user in a variety of ways such as input acceleration adaptive presentation discovery and cognitive contextual or workflow based searching. In further embodiments contextual models are maintained for a group of collaborating users and those models can be collectively and intelligently analyzed by e.g. an executive level management module in order to provide insights regarding group level behavior or otherwise leverage the interactions of the individual users for the benefit of the group.

Referring now to a cross application user interaction modeling system is embodied in a computing system as a number of computer executable modules each of which may be implemented as software firmware hardware or a combination thereof. The illustrative system includes a framework which interfaces with a number of user level software applications to N where N is a positive integer to while a user is working in one or more of the applications obtain high level semantic descriptions of the user s interactions with the various applications. As such each of the user applications includes a conventional now existing or later developed executable software application e.g. email calendar word processor etc. that is extended by or coupled with a semantic description .

As explained further below semantic description describes both passive and active user interactions with an application in a manner that is meaningful and useful at a user level as opposed to a lower level that is more useful to the computer . As used herein passive refers generally to user interactions with computing devices where the interactions do not directly result in an application event while active refers generally to user interactions with computing devices that do result in the occurrence of an application event. Some examples of active interactions include certain gestures e.g. swipe tap drag and other methods of physically contacting a keyboard keypad mouse touchscreen hardpanel control or other input mechanism of the computing device e.g. to activate a user interface control enter or select data or select a displayed option .

Some examples of passive user interactions include user movements or activity that may be detected by one or more sensors or other peripheral components of or in communication with the computing device such as optical sensors proximity sensors force sensors cameras and or others which may be integrated with the computing device or otherwise located in the user s environment e.g. cameras or other sensors located in a room of a building . For instance optical sensors or cameras may detect that a person s eyes are focused on a particular area of the display screen even though the person is not actively interacting with the computing device. Similarly proximity sensors may detect movement of a portion of the user s body e.g. fingers hands arms toward or away from a particular portion of a touch sensitive display screen mouse or keyboard without the person actively touching such component or with the person actively touching the screen or other component but with the system noticing the degree of force applied to the component which may be an indicator of the person s stress level for example or other conditions based on the force or pressure detected by the sensor. In other words passive may refer to the fact that the user is not actively aware of his or her activity e.g. the changes in force or pressure he or she is applying to the component of the computing device but the system may sense it. As another example cameras or other sensing devices may detect the user s movements or gestures e.g. hand waving pointing raising or lowering an arm and the system may incorporate semantic information relating to such movements or gestures into the user s current interaction context. Whereas such changes in force pressure or movement do not result in a conventional software application behaving any differently e.g. in response to a change in a person s stress or activity level the system can through e.g. the input acceleration module or the adaptive presentation module cause an application to respond differently or more appropriately based on these inputs.

The semantic description is created using common terms of reference e.g. an ontology for the various features functions and data involved in user interactions with each of the applications and may specify for example mandatory data fields and relationships among the fields. When a user interaction occurs the low level e.g. machine or operating system level representation of the interaction is annotated with the corresponding common terms of reference. For example an ontology for an email application may associate the system level representation of data in the addressee field of an email message with the label first name last name. In cross application environments multi user environments or other situations in which multiple different ontologies may be employed e.g. by the various applications or by different users the semantic description may resolve the differences between the ontologies by mapping the corresponding terms of reference across the ontologies. For instance data identified as last name in one application or ontology might map to data labeled as family name in another application or ontology.

The framework also receives inputs from a number of user sensors including for example sensors that can track the user s gaze in relation to a display screen of the computing system . As described in more detail below the framework maps the sensor inputs to the corresponding application s user interaction information including the semantic descriptions and posts the semantically enhanced interaction information to a contextual user model in real time where post is used herein to refer to any suitable type of data transmission technique e.g. push or pull techniques .

Various features of the system are implemented as computer executable modules that access and use the contextual user model via an interface which may include for example one or more application programming interfaces or APIs query interfaces and or other similar mechanisms which may be accessed programmatically or by a user and or a user activity inference engine . Illustratively these modules include an input acceleration module an adaptive presentation module a content discovery module a user activity inference engine a cognitive contextual or workflow based search module a common interest discovery module and one or more collective contextual user models each of which is described in greater detail below.

The illustrative inference engine is embodied as a computer executable module or subsystem that applies e.g. artificial intelligence methods algorithms and or techniques using e.g. probabilistic and or statistical models to make intelligent inferences about the user s current activity with respect to the computing system i.e. user interactions with a computer of the computing system based on the interaction data stored in the contextual user model . The inference engine makes these inferences available to the content discovery module as described below and the other modules and may utilize such inferences as well. The inference engine may draw inferences from the contextual user model and or one or more of the collective contextual user models in some embodiments and may store the inferences in one or more of the user models .

Presentation components and logic components also include respectively semantic description components and in order to support the creation and maintenance of the cross application semantic contextual user model as will be described below. Semantic visualization model describes at a useful high level the user interface elements e.g. dialogs controls text graphics etc. that are located on the display screen of the computing system at the location corresponding to the user s passive interaction as sensed by user sensor s e.g. the on screen location of the user s gaze or the on screen location corresponding to the user s gesture movement hand position etc. . For example if a gaze tracking system detects that the user has been looking at a particular paragraph of a document explaining cloud computing for a prolonged period of time semantic visualization model may make available to the framework the following information user user ID action reading document content cloud computing status read and absorbed in addition to the date time and or perhaps if location information is available to the system the geographic location at which the interaction occurred.

Semantic interaction model describes the application events that are initiated by various active user interactions e.g. touch typing voice gesture the business logic that is executed as a result of the application event and the results of the execution of the business logic. For example when a user presses the send button after creating a new email message semantic interaction model may make available to the framework the following information sender user ID action send email recipient John Smith status send complete in addition to the date time email content and or geographic location at which the interaction occurred.

The following example illustrates how application can provide real time posts to contextual user model of passive interactions e.g. what the user sees within the ambit of application . In the exemplary computing system gaze sensor s detect and track the gaze of a user of the system such as by means of one or more gaze tracking mechanisms that are conventionally available or by means of the multi camera sensor system for user gaze tracking described in pending U.S. patent application Ser. No. 13 158 109 Adaptable Input Output Device and Ser. No. 13 399 210 Adaptable Actuated Input Device with Integrated Proximity Detection . The current on screen location of a user s gaze is fed to framework which may be implemented as a system or middleware component of the system . Framework queries application to determine what content is displayed at the location of the current gaze in other words what does the user see Framework may itself determine that the current gaze location falls within the screen area currently occupied by the display of application alternatively framework might submit queries to all or several current applications which might themselves determine whether the gaze location falls within their on screen area. In any case query handler processes that query by obtaining from semantic visualization model a semantically meaningful description of what is currently displayed by application at the identified location. In various embodiments the semantic description provided by semantic visualization model may include a high level functional description of the specified portion of the display e.g. an indication that the subject line or addressee list of message XXX is displayed at the specified location as well as actual content of the display e.g. the text that is displayed at that location . This semantic level information is returned to framework which in turn posts the information to contextual user model for incorporation in the model.

Inputs from other sensors that capture more or less passive user interactions with the computing system such as proximity sensors which can detect user movements relative to a computer display keyboard or other component of the system or force or pressure sensors which can detect variations in the amount of force applied by the user when typing on e.g. a keyboard or keypad may be handled in a similar fashion.

Logic components provide real time posts to contextual user model of what the user does within the ambit of application . As outlined in business logic and event processing component interfaces with semantic description component . When an event occurs and is processed e.g. a message is deleted in response to the user s request event handler obtains a functional high level description of that event from semantic interaction model and posts that descriptive information of what the user did to contextual user model . By high level description of user interactions we mean describing an interaction at a level that is meaningful from a user perspective generally at a higher abstraction layer than raw system operations. For example reading and replying to email searching the web and modifying calendars are common user interactions.

In some embodiments the methods described are also applied to web based interactive material. Such material may be enhanced or processed so that its display and execution by a browser on the user s device will post to contextual user model high level descriptions of what the user sees and does as above. In some cases the HTML and or other markup that defines the display and behavior of a web page or application is suggestive of the high level structure and meaning of display elements and interactive links such that semantic descriptions analogous to descriptions and can be readily extracted. In a further embodiment web pages written in a standard mark up language e.g. HTML are augmented at their source with additional descriptive mark up in a semantic mark up language such as RDF or OWL for example . A browser application of the exemplary embodiment treats each piece of semantic mark up as semantic descriptions for the corresponding piece of HTML that is rendered by the browser and is viewed what the user sees or interacted with e.g. clicked what the user does by the user. In some embodiments the additional semantic mark up is encoded or formatted so as to be ignored by browsers in conventional systems not equipped with requisite elements of the present invention. In a still further embodiment instances of semantic descriptions and can be prepared on the fly by screen scraping online materials that are accessed essentially display frames rendered by the browser are processed OCR style e.g. by the browser or by framework in order to automatically recognize words and numbers as well as potentially other objects with conventional semantic meaning e.g. labeled or unlabelled boxes arrows or buttons that may connote controls e.g. OK Cancel Quit Close play button etc. and or other objects with semantic significance for the user such as faces. For common web based applications with relatively familiar interface features and controls such as hosted online email and calendars templates or rules can be provided to drive recognition of certain canonical words or shapes with conventional high level meanings e.g. send reply disk icon for save standard grid of a calendar etc. . In this way even displays generated by browser execution of web based Flash or JavaScript code can be handled as if semantically marked albeit more speculatively and less completely.

In the illustrative embodiment the information is stored at least initially in a user specific persistent memory . That is information about all of the user s passive and active interactions with the computing system or with one or more specific computing devices thereof is maintained in the memory . A contextual mapping function executes logic to determine whether a particular interaction is one that may be an indicator of the user s current interaction context and if so updates the contextual user model to include the interaction of interest. For example interactions such as scrolling or resizing a window on the display screen would not typically reveal much about what substantively the user is doing with the computing device and therefore would not be added to the contextual user model while interactions such as reading a document or sending an email message would be of interest in understanding the user s current interaction context and thus would be stored in the contextual user model . As used herein the term current interaction context refers to the user s situation and or circumstances as they relate to the user s interactions with a computing system at a particular moment in time what documents they may be reading or working in what other users they may be communicating with via e.g. email or messaging systems the degree to which the user is actively using the computing device the user s mode of interaction with the device e.g. whether the user prefers mouse keypad or voice etc.

The contextual mapping function also applies cognitive and or locomotive parameters to the various interaction data as described further below. Using those parameters the interaction data is classified as information that would typically be retained in the user s short term memory or long term memory as the case may be. Information that has not yet been cognitively processed by the user may be classified by the contextual mapping function as pre cognitive content. As the user specific persistent memory keeps track of all of the user s interactions not just those that would typically be retained in short term or long term memory or those that are considered pre cognitive the persistent memory can allow the system to help the user recall information that the user has forgotten. Accordingly in some embodiments portions of the persistent memory may be at least temporarily copied moved or associated with e.g. via pointers or meta data to the contextual user model and processed by the contextual mapping function . In the illustrative embodiment the interaction data stored in the persistent memory is semantically enhanced as described herein prior to storage in the persistent memory . In other embodiments the persistent memory may keep track of the more or less low level or machine accessible interaction data without semantic enhancement alternatively or in addition to the semantically enhanced interaction data.

The passive interaction data and active interaction data are stored in STM but are over time are aged and removed. Entries removed from STM may be selectively transferred to LTM for longer term storage. For example it is thought that much of the material that ends up in a human being s long term memory consists of things to which that individual is repeatedly exposed on a consistent basis thereby reinforcing the memory. Similarly in some embodiments entries from STM representing content which the user has repeatedly accessed or interacted with are transferred to LTM in an attempt to model albeit only roughly the set of content that the human user would tend to retain in their own long term memory. The intention is that at any given time STM should more or less reflect what actually is in the user s short term memory and awareness namely what has the user recently seen or done in the user s interactions with the system. Analogously LTM is intended to reflect what is in the user s longer term memory and is less likely to be within the user s current consciousness absent a reminder. The LTM differs from the persistent memory in that the persistent memory retains information that the user has completely forgotten i.e. that the user cannot recall even with a reminder. In some embodiments persistent memory pre cognitive content STM and LTM are each searchably indexed by the system in order to facilitate various forms of intelligent assistance and information retrieval as described below in connection with modules input accelerator adaptive presentation discovery cognitive contextual or workflow based search and common interest discovery .

Contextual user model further includes cognitive processing model which may include parameters for characterizing the duration of particular entries in STM and LTM . For example such parameters may be based on cognitive science findings regarding the typical lengths of time over which content can be presumed to have faded from a human user s short term memory and or attention and that should therefore be removed from STM and potentially transferred to LTM . In some embodiments the parameters of model may be functions of variables such as the time of day e.g. reflecting diminished retention later in the day or the user s location and context e.g. at home at work desk in meeting on telephone call etc. . In still further embodiments the parameters of model may be personalized to individual users e.g. through the system s experience with a particular user and or based on demographic factors such as age or disability. In additional embodiments information may be gradually faded from STM before it is transferred by being associated with a score e.g. percentage indicating a probability of whether that particular information is still within the user s short term recall.

In addition cognitive processing model may include parameters for further qualifying the entries of STM and LTM . For example model may include parameters for determining whether particular content that was seen by the user per posting was actually absorbed cognitively processed by the user such as parameters specifying a minimally sufficient amount of time for the user s gaze to be fixed on a given piece of content in order for that content to be read and or understood. For example the model may include a parameter specifying how long it takes for a user to read a line of text. Some examples of cognitive parameters are shown in Table 1 below.

If postings indicate for example that the user s gaze has been briefly fixed on a particular email message or on a displayed notice or alert the system can use the above described parameters to assess whether or not the user is likely to have actually read and processed the contents of the email or the notice. Here again in some embodiments such parameters are functions of variables such as time of day location context and or user demographics. Similarly if there is competition for the user s attention for example if the system observes that the user is currently engaged on a phone call or other live conversation then some embodiments may reflect presumptively diminished attention in cognitive model and or STM with respect to on screen content that is within the user s field of view according to postings during periods of such multi tasking by the user. Conversely highly repetitive and or sustained viewing of e.g. a particular portion of a document or other displayed item of information tends to indicate a relatively strong user awareness of and attention to that specific piece of information. Moreover in embodiments where gaze tracking sensor s detect pupil dilation and or other indicia of attentive focus e.g. gaze frequency or gaze hot spotting such metrics can also be reflected in the parameters of model and the assessment of whether a user has mentally processed viewed content can take into account this physical assessment of the user s current level of attention. In any case contextual user model and particularly cognitive model and STM can incorporate a variety of these factors and more realistically reflect whether a user has likely absorbed a given piece of information not just whether the user s eyeballs were temporarily aimed at it. Accordingly in some embodiments the system may filter postings on this basis and not incorporate a new posting in STM if it is determined based on model that the content seen by the user has probably not yet been actually processed in other embodiments such content might be incorporated in STM but flagged as not yet fully cognitively processed or as likely to be retained for a shorter or longer duration than average.

Contextual user model further includes locomotive processing model which may include parameters for determining where the user s hands are currently located in reference to a display screen or keyboard for example and projecting the likely execution time for the user s hands to move to another location. If postings indicate for example that the user s hands have been fixed in a particular location for a period of time the system can use the locomotive processing parameters to relocate user interface elements nearer to the user s hand position. On the other hand if postings indicate that the user s hands have been very active the system may decide to leave user interface elements in a central location rather than trying to follow the user s hands. As above in some embodiments the locomotive parameters may be functions of variables such as time of day location context and or user demographics. Some examples of locomotive parameters are shown in Table 2 below.

As noted above the illustrative contextual user model also includes pre cognitive content . By pre cognitive we mean interactions or material that the system is aware of which might to be of interest to the user or the user s current context based on the user s contextual model and or the user s involvement in one or more collective contextual user models described below but of which the user is not yet cognitively aware at a substantive level. Such content may include material determined by the gaze tracking system yet to be cognitively absorbed by the user e.g. content that the user glanced at briefly before being interrupted by a phone call or meeting notice . Such content may also include for example next actions or series or sequences of actions predicted by the input acceleration module and or content predicted to be of interest to the user as determined by the content discovery module described below. As another example such content may include a new email message received by the mail client e.g. client portion of email software which has already been processed by the system to obtain the sender subject and or other useful information as described above where such processing occurs before the user is even made aware of the new email message by the mail client. In this way the system can use the pre cognitive information information about the arrived message that the user has not yet seen and other information in the current user contextual model e.g. whether the user is working intently on a document in a software application or browsing the Internet to determine whether to make the user aware of the email message at that moment in time given the user s current interaction context.

Referring now to an illustrative method executable as computerized programs routines logic and or instructions by one or more of the various modules and or components of the system to develop and maintain a contextual user model is shown. The method handles various types of user interactions with the computing system . As such the method receives user interaction input e.g. a keystroke mouse click gesture gaze data proximity data or force data at block determines whether the input is active or passive i.e. whether it implicates an application event at block and handles the input accordingly in the subsequent blocks. In the case of active user input which may involve the user typing on a keyboard or keypad touching an interactive control on the display screen issuing a voice command or request or performing a gesture for example the method interfaces with the application with which the input is associated to determine the application event corresponding to the input block and obtains the semantic description of the application event from the semantic interaction model block as described above.

In the case of a passive interaction beginning with block the method processes the interaction data based on the type of passive interaction as needed. For gaze input the method obtains the related gaze tracking data which may include the date and time of the gaze the duration of the gaze and or other data indicating the user s degree of attentive focus e.g. pupil dilation blinking intervals etc. at block . At block the method determines the on screen locations e.g. the x and y coordinates that correspond to the user s gaze. At block the method accesses the semantic visualization model to obtain information about the semantic meaning of the user interface element currently displayed at the on screen location of the user s gaze. Generally speaking as used herein the term user interface element may refer to interactive controls content e.g. graphics text or video previous user inputs e.g. text entered into a fill in form or any other type of object that may be presented on a display of a computing device.

For proximity inputs the method may determine the on screen locations or keyboard locations e.g. x and y coordinates that correspond most closely to the position of the user s hands or other body portion as determined from the proximity input block and then obtain the semantic meaning of the user interface element displayed at that location or in the vicinity of that location from the semantic visualization model block . Information about user interface elements displayed in the vicinity of but not at the on screen location can be useful to among other features the adaptive presentation module as described below. Other indicators of passive user interactions such as typing intensity the amount of force the user applies to keys of a keyboard or a keypad and variations thereof over time may be obtained from sensors at block alternatively or in addition. At block semantic meanings may be associated with such sensor data. In some cases the semantic meanings obtained at block may be those associated with user interface elements displayed at on screen locations that are mapped to the sensor inputs and as such may be obtained from the semantic visualization model .

Once the user interaction data has been received and linked up with the desired semantic information it is stored in the persistent memory block . At block a periodic or ongoing process e.g. contextual mapping function evaluates the interaction data stored in the persistent memory and determines whether the interaction data may be useful in understanding the user s current context and thus should be posted to the contextual user model . For instance generic and or user specific rules regarding the relative significance of various user interactions may be stored in the contextual user model and applied by the contextual mapping function at block . At block the method applies the cognitive and or locomotive parameters described above e.g. those stored in the cognitive processing model and or the locomotive interaction model to determine whether to classify the interaction data as being associated with pre cognitive content short term memory or long term memory and stores the interaction data in the appropriate sub area of the contextual user model or associates the interaction data with the appropriate metadata or other indicators of the corresponding interaction type at block .

Thus contextual user model maintains an up to date cross application semantic model of what each user sees absorbs and does in their interactions with multiple applications on the system. This cross application semantic model can facilitate intelligent automated analysis by the system of how best to assist the user in a variety of ways. Four types of examples are now described for illustrative purposes although practitioners will appreciate that many other types of intelligent assistance are also possible by leveraging contextual user model .

In some embodiments the system includes an input accelerator module . Based on the contents of the contextual user model and particularly the STM the input accelerator infers what the user is currently attempting to accomplish and presents suggested next steps or actions as selectable options that might be helpful to the user in completing that task. Further details of the input accelerator are described in Senanayake et al. U.S. patent application Ser. No. 13 534 155 filed Jun. 27 2012 which is incorporated herein by this reference in its entirety.

Similarly based on user model and particularly STM the system can infer where the user s attention is currently focused and can take into account the assessment of current user attention as well as knowledge of what the user has recently seen and absorbed so as to present interactive controls in a manner and arrangement that is optimal for the user s context as well as for purposes of prioritizing and presenting informational notices alerts or information that is of interest to the user and the like. This functionality is represented in by adaptive presentation module .

For example alerts and other notifications may be positioned within the user s current visual focus. If contextual user model indicates the user s attention is deeply focused e.g. extended visual focus on a narrow set of materials the logic of module may call for audible or otherwise highlighted presentation of urgent and important alerts while suppressing or delaying less urgent notifications so as to avoid needlessly distracting the user. In a further example module may infer from sensor data and or additional current context information e.g. data from an accelerometer or motion sensor integrated in the user s interactive device and or calendar information that the user is currently in transit and may therefore dictate audible presentation of information rather than or in addition to visual presentation.

Similarly interactive on screen controls associated with the user s current or anticipated task for example the display of selectable next actions or sequences of actions or workflows generated under control of input accelerator described above may be positioned in a location and manner convenient for the user. In some embodiments particularly those with touch sensitive and or gesture sensitive input the position of a user s hands extremities is also tracked e.g. by proximity sensors included in user sensor s . Module may then dictate that interactive controls be positioned proximately to the user s hands for easy reach. If the user s visual focus and hands are in different locations then module may dictate displaying within the user s visual field a depiction of the user s hands and the relative positioning of interactive controls see the heads up interaction methodology described in pending U.S. patent application Ser. No. 13 399 210 Adaptable Actuated Input Device with Integrated Proximity Detection . In determining the optimal presentation e.g. positioning arrangement of interactive controls for a given context locomotive interaction model may be usefully referenced by adaptive presentation module to obtain parameters or rules reflecting the time required for a user to perform various interactive hand movements visual adjustments and the like see for examples of such parameters . For example if contextual user model captures that a user has begun moving a cursor e.g. via mouse towards a selectable set of choices then depending on the parameters of locomotive model and the relevant distances adaptive presentation module may dynamically relocate the selections so that the user can make a selection without completing the mouse traversal.

Referring now to an illustrative method executable as computerized programs routines logic and or instructions by one or more of the various modules and or components of the system to adapt the presentation of user interface elements based on the contextual user model is shown. At block the method detects the instantiation of a user interface element or a combination or sequence of user interface elements by an application . Meanwhile a gaze tracking system may be continuously monitoring changes in the user s gaze as it relates to various on screen locations at block . This process may determine the degree to which the user s attention is focused on particular areas of the display at given points in time block and identify content displayed in areas of the display that have received the user s attention block . So when a user interface element or combination or sequence of such elements is instantiated at block the method can assess the degree to which the user interface element s may be relevant to the user s current interaction context. Based on this assessment the method may invoke the input acceleration module to predict a next action or sequence of actions or workflow block . For example the method may go ahead and activate the instantiated user interface element s at block if it determines that the user is highly likely to do so given his or her current context.

Alternatively or in addition the method may adjust the user interface element s in one or more ways other than activating it. For example the method may change the on screen location or arrangement of the user interface element s block e.g. by moving one or more of the user interface elements to a new location or rearranging a group of user interface elements including the instantiated element. As an example the method may automatically rearrange user interface elements so that a window that the user has been looking at for a period of time is displayed at a convenient location or with a larger font size. The method may change a visual characteristic of the user interface element block . For instance the size or color of an interactive control may change in response to a determination by the system that the user s hand seems to be moving toward that element as determined via proximity sensors . The method may change an audio characteristic of the user interface element s block . For example if the system determines that the user is concentrating deeply on preparing work product in a word processing program the method may silence an otherwise audible notification. The method may change the interaction mode of the user interface element s block . That is the method may present the user interface element s in a different way depending on the user s context e.g. an audible alert in place of a visual notification or vice versa . The method may change the topology of a physical user control block in embodiments where such physical controls are dynamically adjustable see U.S. patent application Ser. No. 13 399 210 Adaptable Actuated Input Device with Integrated Proximity Detection . In these and other ways the system strives to facilitate the user s productivity and enhance the user s overall interaction experience with the computing system .

In some embodiments the method uses automated reasoning techniques to determine whether to go ahead and present the adjusted user interface element at block before actually presenting it at block . For instance if system is uncertain about the user s current context or has a lower degree of confidence in its analysis of the user s current interaction context the method may discard the adjusted user interface element or defer it to a later more appropriate time. The method receives or observes the user s active and passive responses to the various adjusted user interface elements block . For example at block the method may record in the contextual user model and or one or more collective contextual user models whether the user actually clicked on or simply ignored an adjusted user interface element. The method may then return to block and await the instantiation of another user interface element.

Referring now to an exemplary user interface display is shown. The illustrative display is at least a portion of a display screen of a computing device and includes a graphical user interface. At the outset the user s visual attention is directed at area . For example the user may be reading and or preparing a document e.g. Project Design.doc. Next an email notification is received and is initially displayed in an area of the display that is outside of the user s current visual focus. The user s gaze shifts to area in response to the notification . Based on the duration of the user s attention to the notification and or other factors e.g. the relevance of the notification the user s current interaction context as may be determined from the contextual user model the adaptive presentation module relocates the notification to the area and changes the interaction mode of the notification to an interactive control which reveals the sender of the message the subject and an selectable button to allow the user to immediately reply to the message if desired. In some embodiments the adaptive presentation module may adjust the presentation of the notification based on its perceived relevance to the user s current interaction context and without regard to the gaze tracking data. For instance the adaptive presentation module may be aware that the subject of the notification relates to the work that the user is currently doing in the document based on the contextual user model and automatically bring the notification to the user s attention accordingly.

Whereas input accelerator module automatically suggests next actions that are likely to be helpful in completing inferred user tasks discovery module automatically identifies relevant content whose discovery is likely to assist in the user s inferred activities. As background note that pending U.S. patent application Ser. No. 13 182 245 Method and Apparatus for Assembling a Set of Documents Related to a Triggering Item Ser. No. 13 149 536 Method and Apparatus for User Modelization and Ser. No. 12 632 491 Electronic Assistant describe various methods and techniques for automatically identifying and presenting documents and materials of likely relevance and usefulness to a user including in an observed or inferred situation or context. In some embodiments discovery module leverages semantic information in contextual user model about what the user is currently doing and seeing in order to infer the most pertinent aspects of the user s current context and can then utilize a variety of search and information retrieval technologies in order to identify highly relevant material of likely usefulness in light of those pertinent aspects.

For example discovery module may glean from contextual user model that a user has very recently been looking carefully at portions of several different documents e.g. email messages locally stored documents web pages and analysis of those specifically viewed portions reveals that the user appears to be particularly interested in certain keywords and or topics. Said analysis may be performed for example using semantic classification algorithms such as referenced in the above cited patent applications and or other information retrieval algorithms known to practitioners of the relevant arts. Discovery module then automatically initiates a search for material relevant to the intersection of the identified keywords and or topics of strong interest potentially including searches of the public internet e.g. using a conventional search engine the user s local e.g. desktop storage the user s personal cloud data enterprise intranet sites electronic libraries and or other available information repositories. The most relevant results of this search as ranked by discovery module are then presented to the user for consideration e.g. in a display of interactive thumbnails as described above in connection with an exemplary embodiment of input accelerator and step of . In some embodiments this display of suggestions is unobtrusive such that the user can browse it if desired but otherwise creates minimal interference with the user s work. In this regard one approach is to leverage the gaze tracking information delivered by user sensor and captured in model and to dynamically increase or reduce the size centrality or prominence of the suggestions depending on where and how intently the user s gaze is focused. In further embodiments based on the user s implicit or explicit feedback e.g. selection and sustained viewing or not of particular suggested content module can dynamically modify its model of what topics words and or other criteria currently interest the user for example in a manner analogous to the handling of feedback as described in U.S. pending application Ser. No. 13 149 536 referenced above.

Referring now to an illustrative method executable as computerized programs routines logic and or instructions by one or more of the various modules and or components of the system to discover information in an automated fashion based on the contextual user model is shown. With the inference engine more or less continuously assessing the user s current interaction context as described above the method receives an inference therefrom at block . The inference may be drawn from the user s own personal contextual model or one or more collective contextual user models to which the user belongs. Using the inference the method automatically e.g. without initiation by the user formulates a computer executable search query using conventional query development techniques and executes the search query using conventional information retrieval techniques where any of such techniques may be selected according to the requirements or design of a particular implementation of the system and may be now known or later developed blocks and . As noted above the search may be executed across multiple software applications. For example the method may search the user s email folders and also conduct a search across various data sources web sites etc. accessible via the Internet to find information that may be pertinent to the current interaction context. Blocks and operate in a similar fashion to blocks and described above. The method determines based on the user s current interaction context whether to go ahead and present a suggestion containing one or more results of the executed search query block presents the suggestion block receives active or passive user feedback block and updates the user s contextual model and or one or more collective contextual models to which the user belongs block with information about the user s interactions or lack thereof following the presentation of the suggestion or in the case where the method does not present a suggestion with information about the interaction context in which the suggestion was not offered. Following block the method returns to block and awaits the receipt of another inference from the inference engine .

In some embodiments the system includes a cognitive indexing and cognitive contextual search module allowing users to make workflow based search queries. As used herein cognitive indexing refers to the indexing of content in a searchable body of content e.g. corpus in cognitive user level terms that is with respect to cognitive events that are meaningful to the user. For example based on the interaction data contained in the contextual user model a recently viewed article may be indexed with terms such as read after the meeting about memory or downloaded after email from Phil. Similarly a recently edited document may be indexed with terms such as edited while in San Francisco after attending the conference on memory. Cognitive contextual search module can then use such indexing to productively conduct searches for information in response to queries that contain such loose cognitive associations or contextual references.

As used herein workflow refers to a sequence combination or series of events and or interactions that have occurred over the course of the user s existence with reference to one or more computing devices and about which the system is knowledgeable. In other words workflow typically implies a temporal component in connection with an interaction or series of interactions. The cognitive contextual or workflow based search capability enables retrieval of desired information based on queries that specify a combination of data characteristics e.g. keywords metadata and a history of user interactions related to the desired information. The capability is provided across multiple different applications both with respect to the body of information that can be searched as well as the types of user interactions that can be referenced. Moreover the ability to retrieve based on specified user interactions includes an ability to specify the user s broader context with respect to the specified interactions e.g. when where and in what context the user performed a specified interaction and or previously accessed the relevant data. This cognitive contextual or workflow based search capability provides quicker faster and more intuitive search for information that a user has forgotten but can describe with anecdotal contextual information.

In an exemplary embodiment search module helps deliver this capability by indexing and relating pieces of information to each other via high level descriptions of user interactions that have occurred with respect to such information. Search module thus records the evolution of information over time through high level user actions. Consequently information that is accessed and interacted with can subsequently be retrieved by the user based on a specification from the user s perspective of prior interactions relevant to the desired information.

Another feature that is facilitated by search module involves multi modal tracking of a user s context. In other words the system records in contextual user model and or elsewhere when where and in what context the user performs various actions. For example through recording the user s current geographical location information as well as tracking the user s gaze and activities the system can retrieve information based on where the user was located and what time it was when certain information was previously viewed and or when certain interactions took place as well as what else the user might have looked at or seen around the same time and what other salient things were present in the user s context at the time. Because this stored representation of historical information relates information data to a user s actions and the user level context of such actions search queries such as the examples listed above can be answered that are more closely related to the real world experience of the user.

As noted above in some embodiments entries in contextual user model can be probabilistic. This probabilistic nature can be exploited when answering queries e.g. by presenting multiple alternative responses to queries that are ranked or prioritized based on the probabilities assessed by the system. For instance consider the example query listed above What documents did I view shortly after I read Joe s email about transparent keyboards If the user viewed such an email on multiple occasions or received multiple emails matching that description search module can consider probabilities associated in contextual user model with each of the recorded viewings e.g. perhaps on a certain occasion the user only briefly glanced at the message and so is arguably less likely to have retained a strong memory of that occasion and can rank or filter the responsive documents accordingly.

Referring now to an illustrative method executable as computerized programs routines logic and or instructions by one or more of the various modules and or components of the system to conduct user initiated searching based on a contextual user model is shown. Here rather than automatically formulating and executing search queries based on the user s current interaction context as in the method the method is initiated by user input that it interprets as a search request. As noted above the types of search requests that can be handled by the method include contextual e.g. temporal and or interaction related details that are more or less loosely associated with material the user desires to retrieve as the user may normally use them to recall such an item. In this way the method strives to simulate normal human brain activity that occurs when a person attempts to remember or locate information.

At block the method interprets the user s search request using the contextual user model and or one or more collective contextual models to which the user belongs. That is vague or ambiguous references such as before the meeting may be resolved by determining e.g. from the semantic descriptions of the user s recent interactions with a calendar application stored in the user model when the user last attended a meeting. Similarly loose references to people places or things such as the message from Phil can be resolved by the method through a review of the user s recent interactions with an email application which are stored in the user model . In other words the method maps the elements of the search request e.g. data characteristics and interaction characteristics to semantic descriptions contained in the user model . At block the method develops a computer executable search query using both the user supplied search request and the contextual details gleaned from the user model s at block .

Blocks and operate in a similar fashion to blocks and described above. The method executes the search query using conventional information retrieval techniques where any of such techniques may be selected according to the requirements or design of a particular implementation of the system and may be now known or later developed block . The method determines based on the user s current interaction context whether to go ahead and present a suggestion containing one or more results of the executed search query block presents the suggestion block receives active or passive user feedback block and updates the user s contextual model and or one or more collective contextual models to which the user belongs block with information about the user s interactions or lack thereof following the presentation of the suggestion or in the case where the method does not present a suggestion with information about the interaction context in which the suggestion was not offered. Following block the method returns to block and awaits the receipt of another user initiated search request.

Referring now to an exemplary user interface display is shown. The illustrative display is at least a portion of a display screen of a computing device and includes a natural language e.g. voice or text interface to the search module . In the illustrative embodiment the system prompts the user with a natural language question . In other embodiments the system may simply provide a more traditional search dialog box or other mechanism for allowing the user to input a search request. The user responds with a natural language search request which contains data characteristics the article about memory as well as interaction characteristics I found online after Phil sent me an email after our last meeting . The system determines from the contextual user model the person who is most likely to be the Phil to which the user is referring the date and time of the user s last meeting in which Phil was also an attendee via e.g. the user s calendar interactions and any email messages received from Phil in the time period after the meeting based on the user s email interactions . The illustrative system responds to the search request with a natural language reply and presents the search result . As mentioned above the system can make note of the user s response to the result e.g. whether the user closes the dialog box clicks on the hyperlink and store that interaction data in the user model .

For example in a group of collaborating researchers or analysts interface and or content discovery module can share with other team members and or with a supervisor a shared reading record of what specific material has been viewed and focused on by other members of the team. The shared reading record facilitates noticing any gaps in coverage assessing how thoroughly a body of material has been reviewed minimizing needlessly redundant effort and or calling attention to areas that have attracted the interest of multiple team members. Because a collective contextual model can reflect not just which specific portions of content were viewed but also with what degree of attention and focus how long each such portion was actually viewed etc. as described above interface and or content discovery module and or a human reviewer can draw inferences regarding whether a given portion of content was actually read and likely understood and whether it was given sufficient attention by a user or by the group for purposes of comprehension. These capabilities are similarly valuable for purposes of education and training e.g. by providing a teacher with better data as to how thoroughly students have read particular material and to infer whether various portions seem to be requiring and or receiving greater or lesser effort from students. In some embodiments learning goals are specified and individual and or group contextual user models can be compared to learning goals in order to facilitate automated reasoning about learning progress and adaptation of learning training content for example.

Referring now to an illustrative method executable as computerized programs routines logic and or instructions by one or more of the various modules and or components of the system to develop and maintain collective contextual user models is shown. The method periodically or continuously e.g. as a background process monitors interaction data that is posted to the individual user models that are part of the system over time block . The method reviews the interaction data across multiple user models to identify common interests between or among the various users of the system block and then dynamically creates associations among the users determined to have such common interests. For example interaction data relating to email correspondence may reveal that a group of users are currently engaged in preparing a patent application and thus these users may be associated with each other for purposes of a collective contextual model at least temporarily. Once the patent application is prepared and filed there may be no further need for the collective contextual model and so the associations and the resulting collective model may be discontinued thereafter.

At block a collective contextual model as described above is formed autonomously by the system based on the associations created at block . In some embodiments the collective model may be formed by copying material from the relevant individual user models to a new location. In other embodiments the collective model may exist by virtue of references e.g. pointers or meta data associated with the interaction data entries in the relevant individual user models. In the illustrative embodiments the collective contextual models are designed to have a similar structure as the user model described above. That is each collective model may include pre cognitive content short term memory long term memory and or persistent memory as described above. The method applies the cognitive and or locomotive parameters to the collective interaction data and stores the interaction data in the corresponding subarea e.g. pre cognitive short term or long term in the collective contextual model at block .

Blocks and relate to the ability of the system to proactively or upon request provide collective interaction data or analyses thereof to other applications and or users of the system as described above. Block executes logic to determine whether to make other users of a collective model aware of an interaction recently received into the collective model. For example if one user of a group recently received an email containing an article that the system knows relates to the group s common interest the method may go ahead and autonomously forward that email to the other members of the collective model at block . At block the interaction data of the users of a collective model may be aggregated and collectively analyzed as described above e.g. a reading record before the information is made available via the interface . Feedback from the several users of a collective model can be received analyzed and incorporated into the collective model and or individual user models in a similar fashion as described above at blocks and and the method may return to block to continue monitoring the collective interaction context across the system .

Referring now to an exemplary user interface display is shown. The illustrative display is at least a portion of a display screen of a mobile computing device and may include a touch sensitive display or touch screen. In this example a user has prepared an email message forwarding an article to other members of a project team but may have inadvertently forgotten to add one of the team members as an addressee. Using the collective contextual model formed by the system based on the system having determined that the user Jim Sue Mary and John have a common interest the system interrupts the email send application event to ask the user whether he or she also intended to include John as an addressee via a notification . The exemplary notification is interactive in that the user can select Yes to return to the email message and edit the addressee information select No to continue sending the message to only Jim Sue and Mary or simply ignore the notification by closing it. In some embodiments the system may automatically insert John s email address into the addressee field using e.g. input acceleration module if the user selects Yes. 

In an exemplary embodiment the user interacts with the system through an interactive computing device such as a desktop computer workstation portable computer or mobile computing device. In some embodiments user sensor s are integrated in the user s computing device. In some embodiments the user s device includes adaptable input output facilities as described e.g. in pending U.S. patent application Ser. No. 13 158 109 Adaptable Input Output Device while in other embodiments the user s device may comprise a more conventional interactive computing device.

Portions of the system and or the system may be implemented in software firmware and or hardware as one or more logical modules or routines that are executable by a computing device or system which in some embodiments may be the same device with which the user locally interacts or may comprise a physically separate computing resource in communication with the user s interactive device e.g. one or more remote cloud based computer servers . In some embodiments one or more portions of the system and or the system may be implemented as integrated components of framework . Storage for the various components of contextual user model may be local to the user s interactive computing device physically separate and or remote or a combination. In some embodiments access to contextual user model may be shared across multiple user devices for example the user model may incorporate and reflect activities conducted on a primary desktop machine as well as on one or more portable devices of a given user. In some embodiments a version of user model can be maintained on each of a user s separate devices and periodically synchronized as logistics permit connectivity etc. .

In some embodiments the application can also be executed in a legacy system environment where elements such as framework component and contextual user model are not present. This can be readily achieved by coding application to detect the absence of the requisite elements in which case application generally executes in the usual manner but does not perform the operations that prepare and post semantic descriptions into contextual user model .

As illustrated in portions of the system and contextual user model are embodied in a computing device . Portions of the system the user model and or portions of the collective interaction modeling system including any one or more of the collective models may be distributed across multiple computing devices connected via one or more networks .

The illustrative computing device includes at least one processor e.g. microprocessor microcontroller digital signal processor etc. memory and an input output I O subsystem . The processor and the I O subsystem are communicatively coupled to the memory . The memory may be embodied as any type of suitable computer memory device e.g. volatile memory such as various forms of random access memory .

The I O subsystem is communicatively coupled to at least one touch sensitive display e.g. a touchscreen virtual keypad etc. a microphone one or more other input or user control devices e.g. a physical keyboard or keypad buttons hardpanel controls tactile or haptic interface etc. at least one data storage a gaze tracking system one or more other sensors e.g. any of the aforementioned sensors one or more still and or video cameras one or more audio speakers other output devices e.g. an LED display screen etc. one or more other peripheral devices e.g. GPS or other location service transceiver sound graphics or media adaptors etc. and at least one network interface .

The data storage may include one or more hard drives or other suitable data storage devices e.g. flash memory memory cards memory sticks and or others . Portions of the system and or the user model may reside at least temporarily in the data storage and may be copied to the memory during operation for faster processing or other reasons.

The network interface communicatively couples the computing device to one or more other computing systems or devices via the networks . The network s may include a local area network wide area network personal cloud enterprise cloud public cloud and or the Internet for example. Accordingly the network interface s may include a wired or wireless Ethernet mobile cell network WI FI BLUETOOTH VPN or NFC adapter or other suitable interface devices as may be needed pursuant to the specifications and or design of the particular networks . The other device s may be embodied as any suitable type of computing device such as for example a server an enterprise computer system a network of computers a combination of computers and other electronic devices a mobile device any of the aforementioned types of computing devices or other electronic devices.

The computing device may include other components sub components and devices not illustrated in for clarity of the description. In general the components of the computing device are communicatively coupled as shown in by one or more signal paths which may be embodied as any type of wired or wireless signal paths capable of facilitating communication between the respective devices and components.

In the foregoing description numerous specific details are set forth in order to provide a more thorough understanding of the present disclosure. It will be appreciated however that embodiments of the disclosure may be practiced without such specific details. Those of ordinary skill in the art with the included descriptions should be able to implement appropriate functionality without undue experimentation.

References in the specification to an embodiment etc. indicate that the embodiment described may include a particular feature structure or characteristic but every embodiment may not necessarily include the particular feature structure or characteristic. Such phrases are not necessarily referring to the same embodiment. Further when a particular feature structure or characteristic is described in connection with an embodiment it is submitted that it is within the knowledge of one skilled in the art to effect such feature structure or characteristic in connection with other embodiments whether or not explicitly indicated.

Embodiments in accordance with the disclosure may be implemented in hardware firmware software or any combination thereof. Embodiments may also be implemented as instructions stored using one or more machine readable media which may be read and executed by one or more processors. A machine readable medium may include any mechanism for storing or transmitting information in a form readable by a machine e.g. a computing device or a virtual machine running on one or more computing devices . For example a machine readable medium may include any suitable form of volatile or non volatile memory.

In the drawings specific arrangements or orderings of schematic elements may be shown for ease of description. However the specific ordering or arrangement of such elements is not meant to imply that a particular order or sequence of processing or separation of processes is required in all embodiments.

In general schematic elements used to represent instruction blocks or modules may be implemented using any suitable form of machine readable instruction and each such instruction may be implemented using any suitable programming language library application programming interface API and or other software development tools or frameworks. Similarly schematic elements used to represent data or information may be implemented using any suitable electronic arrangement or data structure. Further some connections relationships or associations between elements may be simplified or not shown in the drawings so as not to obscure the disclosure.

The foregoing disclosure is to be considered as exemplary and not restrictive in character and all changes and modifications that come within the spirit of the disclosure are desired to be protected. Further while aspects of the present disclosure may be described in the context of particular forms of electronic devices and systems it should be understood that the various aspects have other applications for example in other electronic devices or in any application in which it is desired to improve or enhance the human electronic device experience.

