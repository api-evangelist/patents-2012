---

title: Data recovery in a distributed computing environment
abstract: A computing system recovers volumes in a distributed computing environment while reducing downtime of storage servers. In an embodiment, a storage server contacts a control plane after a storage failure has occurred. If the storage server hosts an authoritative copy of an offline volume, the storage server is requested to restore the volume. Non-authoritative volumes are removed from the storage server and the storage server provides read access to the restored volume while resuming storage services.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09405815&OS=09405815&RS=09405815
owner: Amazon Technologies, Inc.
number: 09405815
owner_city: Reno
owner_country: US
publication_date: 20121002
---
Data centers provide computing resources for use by one or more clients. These services may include computing storage and networking services. For example a data center may provide a machine to host an application storage to store application data and networking to enable communication between the machine and the storage. By making use of the data center services a customer may pay for computing and or storage use rather than purchasing anticipated hardware needs. This enables a customer to expand and contract use of computing services according to demand. For example an application may be configured to request more storage as needed rather than a developer or administrator monitoring and anticipating use.

In the following description various embodiments will be described. For purposes of explanation specific configurations and details are set forth in order to provide a thorough understanding of the embodiments. However it will also be apparent to one skilled in the art that the embodiments may be practiced without the specific details. Furthermore well known features may be omitted or simplified in order not to obscure the embodiment being described.

Techniques described and suggested herein relate to recovering volumes in a distributed computing environment while reducing downtime of storage servers. In some embodiments a storage server contacts a computer system succinctly referred to as a control plane after a storage failure has occurred. If the storage server hosts an authoritative copy of an offline volume the storage server is requested to restore the volume. Non authoritative volumes are removed from the storage server and the storage server provides read access to the restored volume and provides remaining free space for provisioning new volumes. For example a power outage at a data center may be caused by severe weather such that the entire data center is without power for a time. Upon restoring power a storage server detects the failure and scans its storage for volumes. The detected volumes from the scan are reported to the control plane. The control plane determines which if any volumes should be restored because the volumes are an authoritative copy of customer data. An authoritative copy of customer data may include customer data that is not currently available from other storage systems. The control plane may inform the storage server which volumes should be restored and which volumes may be deleted. The storage server may restore the volumes into a read only state and remaining storage space on the storage server may be made available to provision other volumes. In some embodiments non restored volumes are deleted and the space made available to host other volumes.

The control plane may also pause input output requests I O requests directed to the volume. Paused requests may have a long latency but are neither accepted nor denied until unpaused. The storage server may then provide read only access to the restored volume until a customer has reviewed validated and or enabled the volume for use. Once enabled the paused requests may be unpaused. In some embodiments the control plane may verify that no data was lost and automatically re enable the volume without customer intervention.

In some embodiments data may be merged from multiple sources. For example a recent snapshot of the volume may be used as clean data during the restore. The snapshot data may come from a high durability data store while the data server provides a low latency data store. The clean data may be merged with changes identified in the recovery of dirty data from the volume hosted by the storage server.

A client may monitor and or direct this restoration process. The client may be provided with administrative read privileges unaffected by the paused I O requests. The read privileges may allow the client to verify the restored volume integrity. Upon completing the review of the restored volume the client may re enable the volume for use transfer the volume to another data center archive the volume and or delete the volume. This client interaction may be performed through an application programming interface and or a web administration interface.

In one example a major infrastructure failure occurs such as a data center power outage. This infrastructure failure may cause many storage servers to detect a storage failure and contact a control plane. Should all storage servers perform a restore operation at the same time data center resources may become restrained. However restore operations may be throttled by a control plane. In one embodiment the control plane answers a portion of the requests received from storage servers. Unanswered storage servers may continue to periodically send messages to the control plane. When the answered storage servers complete restore operations the process may repeat until the storage servers have been processed.

A control plane provides management services for computing resources in a distributed computing system. Control plane functions may include provisioning and de provisioning of computing resources monitoring of computing resources workflow monitoring and creation security policy actions and application programming interface API access to management functionality. For example a control plane may provide an API for a client to use that causes a new block storage volume to be created and attached to an existing virtual machine.

Volumes may be virtualized as well as physical. For example a virtual volume may be expanded or contracted as needed such that the virtual volume may span several physical volumes. A physical volume may be a set of space allocated on physical media. For example a physical volume may be a percentage of a hard drive such as a platter based or solid state drive.

Turning now to an illustrative example of a volume recovery system in accordance with at least one embodiment is shown. A storage server may recover from going offline by recovering a volume that is identified by a control plane as an authoritative source of data for a customer such as used a customer application . More specifically a program execution service allows a customer to provision and control various computing resources. These computing resources may include virtual storage such as a block storage service that may include a volume hosted by a storage server . Other program execution services may include virtual machine provisioning to support an application and configurable networking to support I O requests between the application and the volume . These computing resources and services may be managed by a control plane .

If an event occurs that causes disruption to the storage server also referred to as a recovery event or recovery triggering event such as a power outage or network failure the storage server may transition into a recovery state. During the recovery state a storage server may scan local volumes . In one embodiment the scanning is used to determine volume identifiers and volume health. The storage server may periodically attempt contact with the control plane to report the results of the scan as event information and receive instruction regarding the volumes . The control plane may receive a report from a storage server and determine if the storage server hosts a volume in need of recovery. In one embodiment a copy of the volume may exist on another storage server that was not affected by the event. Therefore the volume may be outdated and redundant. The control plane may safely instruct the storage server to delete the volume . On the other hand if the volume is the only authoritative copy in existence the control plane may instruct the storage server through a recovery request to recover the volume . I O requests to the volume may be paused during the recovery process.

During and or after recovery of the volume the storage server may continue normal operations of receiving and servicing provisioned volumes. The recovered volume may be placed in a read only state such that normal I O requests are paused. In one embodiment the volume may be re enabled to process I O requests if it is determined that there was no loss of data. In other embodiments the volume may remain paused until a customer makes a final decision about the volume. A customer may for example choose to verify perform a snapshot of re enable and or delete the volume. These requests from the customer may be received through various channels such as an application programming interface API and or a web interface. The requests may also be made by the customer directly through interaction with a computing resource or programmatically through use of an instruction channel such as an API.

For example a network outage may occur at a data center because of a hardware bug in multiple routers hardware. The storage server may discover the network is down and transition into a recovery state. Because of the down network I O requests to the storage server may be paused. The storage server may scan its local hard drives to determine which volumes it hosts and the health of the volumes . Periodically the storage server may attempt to send failure event information to the control plane that may include information on the volumes and the reason for the transition into recovery mode such as the network outage. In some embodiments the storage server may simply wait to detect the network and then send the event information message . When the network becomes available the control plane may receive the event information and determine if the volume should be recovered as an authoritative source of data. The control plane may send instructions to recover volumes containing authoritative sources of data and delete duplicate and or outdated volumes. During or after the recovery of the recovered volumes the storage server may resume normal servicing and provisioning of volumes while hosting the recovered volumes . The status of recovered volumes may be communicated to a customer such that the customer may verify the data on the recovered volume and re enable the volume for use including unpausing the I O requests .

It should be recognized that while specific examples described herein use block storage services and or high durability data stores other storage services and media may be applicable and or used. For example caches relational databases NoSQL databases object storage block storage and other storage paradigms may also be used.

Turning now to an illustrative example of an alternate embodiment of a volume recovery system in accordance with at least one embodiment is shown. Two storage servers responding to a failure contact a control plane and receive instructions as to which volume is to be restored using snapshot data from a high durability data store and which volumes are to be deleted. More specifically a program execution service provides computing resources and services that include application servers to support application block storage services to service volumes A B C through storage servers and high durability data storage for snapshots. Each volume A B C may actually consist of a primary volume and slave volume to improve durability. A slave volume may intercept write commands for the primary volume such that the slave volume mirrors the primary volume . Snapshots of a volume may be periodically taken and placed in a high durability data store such that data in volume A B C is further protected from failures. An application may communicate with volume A through I O requests that are placed in an I O queue that services volume A.

However an event may occur that interrupts storage servers and such as a power outage that prevents I O queue from delivering I O requests to volume A. In contrast storage server may not have been interrupted. Slave volume B and slave volume C may revert to primary volumes as primary volume B and primary volume C become unreachable. Slave volumes for volumes and may then be provisioned on other active servers to maintain durability of the volumes. Upon the ending of the event such as the return of power storage servers and may periodically attempt to contact the control plane . Here storage server is the first to contact the control plane . Storage server sends event information such as the detected problem and the volumes hosted by the storage server . The control plane may then determine that primary volume C is already hosted by storage server and not need of restore. However the control plane may also determine that slave volume A does not have an active counterpart as storage server is also offline with the primary volume A is the current authoritative volume A and should be restored. In other embodiments the control plane may wait for the primary volume A before proceeding with a restore operation. A restore request may be returned to storage server with a request to restore slave volume A and delete primary volume C . Storage server may come back online and make available the space formerly occupied by primary volume C . In some embodiments a snapshot of restored slave volume A may be taken to ensure survival of the data. In other embodiments slave volume A is transitioned into a primary volume and a slave volume provisioned to mirror volume such that volume A durability is improved.

Restored slave volume A may be held in a read only state until a customer or the control plane act to change the state of restored slave volume A . In an embodiment a customer may receive notification from the control plane of a problem through a communication channel such as email text message instant message or phone call. The customer may communicate with the control plane through a customer device such as a laptop or mobile device. The control plane may grant access to the customer device to the recovered slave volume A to review and determine the outcome of the volume. The customer may take such actions as verifying the data re enabling the volume performing a snapshot of the volume and or deleting the volume. If the customer re enables the restored slave volume A the slave volume A may be transitioned into a primary volume and a slave volume provisioned to complete volume A. Application may then resume communication with volume A and paused I O requests may be released to volume A. In some embodiments the control plane may determine that no data loss occurred such as a match with a recent snapshot or a match between a primary volume A and slave volume A . The control plane may then automatically re enable the slave volume A and unpause the I O requests for the volume A by releasing the I O requests from the I O queue . In one embodiment data from a snapshot may be used as a base of clean data for the restore. Dirty data from the restored slave volume A may then be applied such that any corruption of the dirty data may not affect the clean data. In another embodiment if restored volume A does not receive interaction within a certain amount of time the restored volume A may be stored in the high durability data store as a snapshot.

Regarding storage server coming back online several choices may be made by the control plane. In one embodiment the storage server sends event information that includes the hosted volumes . As are duplicates of existing volumes the control plane may send instructions to delete the volumes. Storage server may then delete the volumes and resume offering storage services. In another embodiment primary volume A may be used to stitch together data to remedy any corruption restore missing data and or ensure the integrity of slave volume A . The storage server may then resume normal operations providing storage services.

In at least one embodiment one or more aspects of the environment and may incorporate and or be incorporated into a distributed program execution service. depicts aspects of an example distributed program execution service in accordance with at least one embodiment. The distributed program execution service provides virtualized computing services including a virtual computer system service and a virtual data store service with a wide variety of computing resources interlinked by a relatively high speed data network. Such computing resources may include processors such as central processing units CPUs volatile storage devices such as random access memory RAM nonvolatile storage devices such as flash memory hard drives and optical drives servers such as the storage server and the application server described above with reference to one or more data stores such as the volume of as well as communication bandwidth in the interlinking network. The computing resources managed by the distributed program execution service are not shown explicitly in because it is an aspect of the distributed program execution service to emphasize an independence of the virtualized computing services from the computing resources that implement them.

The distributed program execution service may utilize the computing resources to implement the virtualized computing services at least in part by executing one or more programs program modules program components and or programmatic objects collectively program components including and or compiled from instructions and or code specified with any suitable machine and or programming language. For example the computing resources may be allocated and reallocated as necessary to facilitate execution of the program components and or the program components may be assigned and reassigned as necessary to the computing resources. Such assignment may include physical relocation of program components for example to enhance execution efficiency. From a perspective of a user of the virtualized computing services the distributed program execution service may supply computing resources elastically and or on demand for example associated with a per resource unit commodity style pricing plan.

The distributed program execution service may further utilize the computing resources to implement a service control plane configured at least to control the virtualized computing services. The service control plane may include a service administration interface . The service administration interface may include a Web based user interface configured at least to enable users and or administrators of the virtualized computing services to provision de provision configure and or reconfigure collectively provision suitable aspects of the virtualized computing services. For example a user of the virtual computer system service may provision one or more virtual computer system instances . The user may then configure the provisioned virtual computer system instances to execute the user s application programs. The ellipsis between the virtual computer system instances and indicates that the virtual computer system service may support any suitable number e.g. thousands millions and more of virtual computer system instances although for clarity only two are shown.

The service administration interface may further enable users and or administrators to specify and or re specify virtualized computing service policies. Such policies may be maintained and enforced by a service policy enforcement component of the service control plane . For example a storage administration interface portion of the service administration interface may be utilized by users and or administrators of the virtual data store service to specify virtual data store service policies to be maintained and enforced by a storage policy enforcement component of the service policy enforcement component . Various aspects and or facilities of the virtual computer system service and the virtual data store service including the virtual computer system instances the low latency data store the high durability data store and or the underlying computing resources may be controlled with interfaces such as application programming interfaces APIs and or Web based service interfaces. In at least one embodiment the control plane further includes a workflow component configured at least to interact with and or guide interaction with the interfaces of the various aspects and or facilities of the virtual computer system service and the virtual data store service in accordance with one or more workflows.

In at least one embodiment service administration interface and or the service policy enforcement component may create and or cause the workflow component to create one or more workflows that are then maintained by the workflow component . Workflows such as provisioning workflows and policy enforcement workflows may include one or more sequences of tasks to be executed to perform a job such as provisioning or policy enforcement. A workflow as the term is used herein is not the tasks themselves but a task control structure that may control flow of information to and from tasks as well as the order of execution of the tasks it controls. For example a workflow may be considered a state machine that can manage and return the state of a process at any time during execution. Workflows may be created from workflow templates. For example a provisioning workflow may be created from a provisioning workflow template configured with parameters by the service administration interface . As another example a policy enforcement workflow may be created from a policy enforcement workflow template configured with parameters by the service policy enforcement component .

The workflow component may modify further specify and or further configure established workflows. For example the workflow component may select particular computing resources of the distributed program execution service to execute and or be assigned to particular tasks. Such selection may be based at least in part on the computing resource needs of the particular task as assessed by the workflow component . As another example the workflow component may add additional and or duplicate tasks to an established workflow and or reconfigure information flow between tasks in the established workflow. Such modification of established workflows may be based at least in part on an execution efficiency analysis by the workflow component . For example some tasks may be efficiently performed in parallel while other tasks depend on the successful completion of previous tasks.

The virtual data store service may include multiple types of virtual data store such as a low latency data store and a high durability data store . For example the low latency data store may maintain one or more data sets which may be read and or written collectively accessed by the virtual computer system instances with relatively low latency. The ellipsis between the data sets and indicates that the low latency data store may support any suitable number e.g. thousands millions and more of data sets although for clarity only two are shown. For each data set maintained by the low latency data store the high durability data store may maintain a set of captures . Each set of captures may maintain any suitable number of captures and of its associated data set respectively as indicated by the ellipses. Each capture and may provide a representation of the respective data set and at particular moment in time. Such captures and may be utilized for later inspection including restoration of the respective data set and to its state at the captured moment in time. Although each component of the distributed program execution service may communicate utilizing the underlying network data transfer between the low latency data store and the high durability data store is highlighted in because the contribution to utilization load on the underlying network by such data transfer can be significant.

For example the data sets of the low latency data store may be virtual disk files i.e. file s that can contain sequences of bytes that represents disk partitions and file systems or other logical volumes. The low latency data store may include a low overhead virtualization layer providing access to underlying data storage hardware. For example the virtualization layer of the low latency data store may be low overhead relative to an equivalent layer of the high durability data store . Systems and methods for establishing and maintaining low latency data stores and high durability data stores in accordance with at least one embodiment are known to those of skill in the art so only some of their features are highlighted herein. In at least one embodiment the sets of underlying computing resources allocated to the low latency data store and the high durability data store respectively are substantially disjoint. In a specific embodiment the low latency data store could be a Storage Area Network SAN target or the like. In this exemplary embodiment the physical computer system that hosts the virtual computer system instance can send read write requests to the SAN target.

The low latency data store and or the high durability data store may be considered non local and or independent with respect to the virtual computer system instances . For example physical servers implementing the virtual computer system service may include local storage facilities such as hard drives. Such local storage facilities may be relatively low latency but limited in other ways for example with respect to reliability durability size throughput and or availability. Furthermore data in local storage allocated to particular virtual computer system instances may have a validity lifetime corresponding to the virtual computer system instance so that if the virtual computer system instance fails or is de provisioned the local data is lost and or becomes invalid. In at least one embodiment data sets in non local storage may be efficiently shared by multiple virtual computer system instances . For example the data sets may be mounted by the virtual computer system instances as virtual storage volumes.

Data stores in the virtual data store service including the low latency data store and or the high durability data store may be facilitated by and or implemented with a block data storage BDS service at least in part. The BDS service may facilitate the creation reading updating and or deletion of one or more block data storage volumes such as virtual storage volumes with a set of allocated computing resources including multiple block data storage servers. A block data storage volume and or the data blocks thereof may be distributed and or replicated across multiple block data storage servers to enhance volume reliability latency durability and or availability. As one example the multiple server block data storage systems that store block data may in some embodiments be organized into one or more pools or other groups that each have multiple physical server storage systems co located at a geographical location such as in each of one or more geographically distributed data centers and the program s that use a block data volume stored on a server block data storage system in a data center may execute on one or more other physical computing systems at that data center.

The BDS service may facilitate and or implement local caching of data blocks as they are transferred through the underlying computing resources of the distributed program execution service including local caching at data store servers implementing the low latency data store and or the high durability data store and local caching at virtual computer system servers implementing the virtual computer system service . In at least one embodiment the high durability data store is an archive quality data store implemented independent of the BDS service . The high durability data store may work with sets of data that are large relative to the data blocks manipulated by the BDS service . The high durability data store may be implemented independent of the BDS service . For example with distinct interfaces protocols and or storage formats.

Each data set may have a distinct pattern of change over time. For example the data set may have a higher rate of change than the data set . However in at least one embodiment bulk average rates of change insufficiently characterize data set change. For example the rate of change of the data set may itself have a pattern that varies with respect to time of day day of week seasonally including expected bursts correlated with holidays and or special events and annually. Different portions of the data set may be associated with different rates of change and each rate of change signal may itself be composed of independent signal sources for example detectable with Fourier analysis techniques. Any suitable statistical analysis techniques may be utilized to model data set change patterns including Markov modeling and Bayesian modeling.

As described above an initial capture of the data set may involve a substantially full copy of the data set and transfer through the network to the high durability data store may be a full capture . In a specific example this may include taking a snapshot of the blocks that make up a virtual storage volume. Data transferred between the low latency data store and high durability data store may be orchestrated by one or more processes of the BDS service . As another example a virtual disk storage volume may be transferred to a physical computer hosting a virtual computer system instance . A hypervisor may generate a write log that describes the data and location where the virtual computer system instance writes the data. The write log may then be stored by the high durability data store along with an image of the virtual disk when it was sent to the physical computer.

The data set may be associated with various kinds of metadata. Some none or all of such metadata may be included in a capture of the data set depending on the type of the data set . For example the low latency data store may specify metadata to be included in a capture depending on its cost of reconstruction in a failure recovery scenario. Captures beyond the initial capture may be incremental for example involving a copy of changes to the data set since one or more previous captures. Changes to a data set may also be recorded by a group of differencing virtual disks which each comprise a set of data blocks. Each differencing virtual disk may be a parent and or child differencing disk. A child differencing disk may contain data blocks that are changed relative to a parent differencing disk. Captures may be arranged in a hierarchy of classes so that a particular capture may be incremental with respect to a sub hierarchy of capture classes e.g. a capture scheduled weekly may be redundant with respect to daily captures of the past week but incremental with respect to the previous weekly capture . Depending on the frequency of subsequent captures utilization load on the underlying computing resources can be significantly less for incremental captures compared to full captures.

For example a capture of the data set may include read access of a set of servers and or storage devices implementing the low latency data store as well as write access to update metadata for example to update a data structure tracking dirty data blocks of the data set . For the purposes of this description data blocks of the data set are dirty with respect to a particular class and or type of capture if they have been changed since the most recent capture of the same class and or type . Prior to being transferred from the low latency data store to the high durability data store capture data may be compressed and or encrypted by the set of servers. At the high durability data store received capture data may again be written to an underlying set of servers and or storage devices. Thus each capture involves a load on finite underlying computing resources including server load and network load. It should be noted that while illustrative embodiments of the present disclosure discuss storage of captures in the high durability data store captures may be stored in numerous ways. Captures may be stored in any data store capable of storing captures including but not limited to low latency data stores and the same data stores that store the data being captured.

Captures of the data set may be manually requested for example utilizing the storage administration interface . In at least one embodiment the captures may be automatically scheduled in accordance with a data set capture policy. Data set capture policies in accordance with at least one embodiment may be specified with the storage administration interface as well as associated with one or more particular data sets . The data set capture policy may specify a fixed or flexible schedule for data set capture. Fixed data set capture schedules may specify captures at particular times of day days of the week months of the year and or any suitable time and date. Fixed data set capture schedules may include recurring captures e.g. every weekday at midnight every Friday at 2 am 4 am every first of the month as well as one off captures.

A data center may be viewed as a collection of shared computing resources and or shared infrastructure. For example as shown in a data center may include virtual machine slots physical hosts power supplies routers isolation zones and geographical locations . A physical host may be shared by multiple virtual machine slots each slot capable of holding a guest operating system. Multiple physical hosts may share a power supply such as a power supply provided on a server rack. A router may service multiple physical hosts across several power supplies to route network traffic. An isolation zone may service many routers the isolation zone being a group of computing resources that are serviced by redundancies such as backup generators. Multiple isolation zones may reside at a geographical location such as a data center . A provisioning server may include a memory and processor configured with instructions to analyze user data and rank available implementation resources using determined roles of computing resources and shared resources infrastructure in the calculation. The provisioning server may also manage workflows for provisioning and deprovisioning computing resources as well as detecting health and or failure of computing resources.

In some embodiments a tradeoff between data durability and data latency is determined. In a low latency data store as seen in data durability may be improved by provisioning a slave volume as remotely from a primary volume as possible that also maintains acceptable latency. In some embodiments a slave volume is a volume that mirrors a primary volume by receiving and performing all writes intended for the primary volume. For example a primary volume may share a router but not a power supply with a slave volume. If infrastructure supporting a primary volume fails such as a power supply a slave volume may become the new primary volume and a new slave may be provisioned. If infrastructure supporting a slave volume fails such as a physical host a new slave volume may be provisioned. However if supporting infrastructure of both the primary volume and slave volume fails such as a router isolation zone and or geographical location such as is possible by a major weather event the volume may be lost unless recovered. In situations a prior snapshot in the high durability data store may be adequate. However in other situations a customer may wish to recover the data on the failed volume. describe such processes for recovery.

Inside the data center may be internal servers internal networking a control plane a gateway and other resources such as a monitoring system. An internal server may be connected to other internal servers through internal networking . The internal servers may also be connected with a control plane . The control plane may receive requests to manipulate computing resources including provisioning resources and changing routing. The internal servers may also be connected with a gateway . The gateway may filter and route untrusted traffic to internal servers such as HTTP traffic to Web servers. The application server may communicate with computing resources in the data center such as low latency data stores serviced by internal servers .

Outside the data center may be any of a number of different components or environments as may include the Internet and various untrusted computing systems as may include desktops laptops and mobile devices such as electronic book readers mobile phones tablet computing devices etc. The systems may be untrusted because the systems may not be administered by a trusted administrator. Further the communication channels such as the Internet are not controlled by a trusted administrator. Thus a message from an untrusted computing system may be intercepted counterfeited and or exploited.

In some cases and for protective reasons internal servers on a secure internal network may only be given the Internet access required to operate if any at all. For example an application server in a data center may only receive outside traffic on port because a gateway provides access controls to the secure internal network that prevents all other Internet traffic from directly reaching the Web server. In another example a database server on a secure internal network may not be connected to the Internet because it is only queried by a local Web server over the secure internal network. In other embodiments an internal server may be behind a load balancer which may occasionally direct Internet requests to the internal server . In another example a low latency data store server may be configured to act as a block storage device to an application server but have no Internet access.

There may also be other reasons to prevent direct access to servers within the data center . Some data center operators and clients consider how the internal data center operates a trade secret. Knowledge of machines configurations or interfaces may be seen as a competitive advantage. By keeping that information only within the trusted network the trade secrets may be kept from others. However by preventing access to internal servers within the data center from outside sources any communication to internal servers must come from within. Thus it may be difficult to reach an internal server or interact with internal services in other than defined ways.

Turning now to an illustrative chart of a process in phases that may be used to recover a volume in accordance with at least one embodiment is shown. These phases may be performed by systems such as those shown in including control plane volume and storage server . The recovery process may include four phases notice investigation recovery and configuration . In the notice phase an event occurs that causes a storage server to report the event information to a control plane . For example a power outage event may cause a block storage server to reboot. Upon reboot the block storage server may report the reboot event information including a list of local volumes to the control plane and request instructions on what to do with the volumes.

In the investigation phase the control plane may use the event information to determine if the volumes hosted by the storage server are already hosted by other storage servers . For example the control plane may check to see if both a primary and slave volume were affected by the event . If only one of the primary or slave volumes were affected the system would have already created a new slave volume to replace the volume affected by the event . No recovery would be needed. If recovery has already occurred through a mirrored volume then no recovery may be necessary. If no recovery occurs then the storage server may drop the volumes and resume offering storage services. However if both the primary and slave volume were affected and no recovery has occurred or is incomplete the control plane may determine that the recovery phase should occur.

In the recovery phase the control plane may send a recovery request to the storage server identifying a volume to recover. The recovery request may also include instructions to drop volumes that do not need to be recovered. In some embodiments recovery may include only the volume itself such as error correction and or marking the volume as read only. In other embodiments the control plane and or storage server may use multiple volumes to form a recovered volume. For example a primary and slave volume may be examined together to verify that the data matches and or remove corruption from one or both volumes. In another example a snapshot may be restored and a comparison performed with the snapshot to determine any changes that may be applied to the snapshot in view of the restored data. The restored volume may then be ready for a configuration phase . In some embodiments if the control panel discovers the volume has no data loss the control panel may re enable the volume without any user intervention and the configuration phase may be skipped.

In the configuration phase a customer may review and or verify the data contained in the restored volume and configure the use of the restored volume through the control plane . For example a customer may be notified of the event and the restored volume awaiting review. The customer may access the volume through a computing device such as laptop . The customer may use the read access to the volume to verify the contents of the volume . Depending on the customer need the customer may configure the volume through the control plane. Configuration may include re enabling the volume for use transferring the volume to another data center archiving the volume and or deleting the volume

Turning now to an illustrative example of a process that may be used to recover a volume in accordance with at least one embodiment is shown. The process may be performed by systems such as those shown in including control plane volume and storage server . A recovery event may occur that causes the input output requests to a volume to be paused . A server and or control plane may determine that a recovery event has occurred on the server. For example a recovery event could include an unexpected shutdown or power loss detected at boot time. In another example a loss of networking services may be a recovery event that is detected by the lack of network presence such as electrical or packet loss . The server hosted volumes may then be determined . A volume may be determined to have no healthy volumes available. The volume may be recovered from the server and or secondary sources. A customer that manages the data may be provided read access to the volume. The control plane may then receive instructions from the customer on configuring the volume.

For example a power outage may be noted by a server. The server may then provide a list of hosted volumes to a control plane. The control plane may determine that the volume is not available through other redundancies and pause input output requests to the volume. The control plane may then request the server restore the volume in a read only state. The control panel may then grant read only access to the customer that is not affected by the pausing of input output requests. The customer may then access the volume and configure the fate of the volume by re enabling the volume for use archiving the volume deleting the volume or doing nothing. In some embodiments in the case of doing nothing the control plane may automatically cause the volume to be archived. In other embodiments the server may also resume hosting volumes during the recovery process reducing downtime of the server.

Some or all of the process or any other processes described herein or variations and or combinations thereof may be performed under the control of one or more computer systems configured with executable instructions and may be implemented as code e.g. executable instructions one or more computer programs or one or more applications executing collectively on one or more processors by hardware or combinations thereof. The code may be stored on a computer readable storage medium for example in the form of a computer program comprising a plurality of instructions executable by one or more processors. The computer readable storage medium may be non transitory.

Turning now to an illustrative example of a server process that may be used to recover a volume in accordance with at least one embodiment is shown. The process may be performed by systems such as those shown in including control plane volume and storage server . The storage server may determine that a recovery event has occurred such as a network outage. The storage server may review local volumes to verify volume identification and or volume health such as an amount of corruption or other problems . The storage server may periodically attempt to communicate the local volume information to a control plane. While waiting for a determination on recovering volumes the storage server may determine other non identified local volumes may be deleted provide the recently acquired space as available for new volumes and resume providing storage services. Upon successful communication about recovery the storage server may receive a request to recover one or more local volumes. The server may recover local volumes. In some cases recovery may be simply retaining the volume. In parallel the storage server may delete non retained volumes resume providing storage services and service the restored volume . The storage server may also resume providing storage services. In addition the storage server may provide read only access to the recovered volume and eventually enable or delete the recovered volume depending on customer input or lack thereof .

Turning now to an illustrative example of a control plane process that may be used to recover a volume in accordance with at least one embodiment is shown. The process may be performed by systems such as those shown in including control plane volume and storage server . The control plane may receive a report from a server identifying a recovery event and associated volumes. The control plane may determine if active copies of the volume exist or if instead the volumes are unique. If unique the control plane may request the unique volumes be recovered and pause input output requests to the unique volumes. For each recovered volume the control plane may determine if data loss has occurred. If no data loss the control plane may re enable the volume unpause the input output requests to the volume and review remaining volumes . If there was data loss the control plane may arrange for access to the volume be provided to the customer administering the volume and review further volumes . Volumes not identified as unique may be considered duplicates and a request to delete the volumes may be given. The storage server may also be enabled to continue to provide storage services. By enabling the storage system to only quarantine the recovered volumes data centers with massive events may become operational at a much quicker pace rather than waiting for individual intervention on each server. It should also be noted that operations and may be performed in series with the other operations as shown or may be performed in parallel with other operations depending on the embodiment.

The illustrative environment includes at least one application server and a data store . It should be understood that there can be several application servers layers or other elements processes or components which may be chained or otherwise configured which can interact to perform tasks such as obtaining data from an appropriate data store. As used herein the term data store refers to any device or combination of devices capable of storing accessing and retrieving data which may include any combination and number of data servers databases data storage devices and data storage media in any standard distributed or clustered environment. The application server can include any appropriate hardware and software for integrating with the data store as needed to execute aspects of one or more applications for the client device handling a majority of the data access and business logic for an application. The application server provides access control services in cooperation with the data store and is able to generate content such as text graphics audio and or video to be transferred to the user which may be served to the user by the Web server in the form of HTML XML or another appropriate structured language in this example. The handling of all requests and responses as well as the delivery of content between the client device and the application server can be handled by the Web server. It should be understood that the Web and application servers are not required and are merely example components as structured code discussed herein can be executed on any appropriate device or host machine as discussed elsewhere herein.

The data store can include several separate data tables databases or other data storage mechanisms and media for storing data relating to a particular aspect. For example the data store illustrated includes mechanisms for storing production data and user information which can be used to serve content for the production side. The data store also is shown to include a mechanism for storing log data which can be used for reporting analysis or other such purposes. It should be understood that there can be many other aspects that may need to be stored in the data store such as for page image information and to access right information which can be stored in any of the above listed mechanisms as appropriate or in additional mechanisms in the data store . The data store is operable through logic associated therewith to receive instructions from the application server and obtain update or otherwise process data in response thereto. In one example a user might submit a search request for a certain type of item. In this case the data store might access the user information to verify the identity of the user and can access the catalog detail information to obtain information about items of that type. The information then can be returned to the user such as in a results listing on a Web page that the user is able to view via a browser on the user device . Information for a particular item of interest can be viewed in a dedicated page or window of the browser.

Each server typically will include an operating system that provides executable program instructions for the general administration and operation of that server and typically will include a computer readable storage medium e.g. a hard disk random access memory read only memory etc. storing instructions that when executed by a processor of the server allow the server to perform its intended functions. Suitable implementations for the operating system and general functionality of the servers are known or commercially available and are readily implemented by persons having ordinary skill in the art particularly in light of the disclosure herein.

The environment in one embodiment is a distributed computing environment utilizing several computer systems and components that are interconnected via communication links using one or more computer networks or direct connections. However it will be appreciated by those of ordinary skill in the art that such a system could operate equally well in a system having fewer or a greater number of components than are illustrated in . Thus the depiction of the system in should be taken as being illustrative in nature and not limiting to the scope of the disclosure.

The various embodiments further can be implemented in a wide variety of operating environments which in some cases can include one or more user computers computing devices or processing devices which can be used to operate any of a number of applications. User or client devices can include any of a number of general purpose personal computers such as desktop or laptop computers running a standard operating system as well as cellular wireless and handheld devices running mobile software and capable of supporting a number of networking and messaging protocols. Such a system also can include a number of workstations running any of a variety of commercially available operating systems and other known applications for purposes such as development and database management. These devices also can include other electronic devices such as dummy terminals thin clients gaming systems and other devices capable of communicating via a network.

Most embodiments utilize at least one network that would be familiar to those skilled in the art for supporting communications using any of a variety of commercially available protocols such as TCP IP OSI FTP UPnP NFS CIFS and AppleTalk. The network can be for example a local area network a wide area network a virtual private network the Internet an intranet an extranet a public switched telephone network an infrared network a wireless network and any combination thereof.

In embodiments utilizing a Web server the Web server can run any of a variety of server or mid tier applications including HTTP servers FTP servers CGI servers data servers Java servers and business application servers. The server s also may be capable of executing programs or scripts in response requests from user devices such as by executing one or more Web applications that may be implemented as one or more scripts or programs written in any programming language such as Java C C or C or any scripting language such as Perl Python or TCL as well as combinations thereof. The server s may also include database servers including without limitation those commercially available from Oracle Microsoft Sybase and IBM .

The environment can include a variety of data stores and other memory and storage media as discussed above. These can reside in a variety of locations such as on a storage medium local to and or resident in one or more of the computers or remote from any or all of the computers across the network. In a particular set of embodiments the information may reside in a storage area network SAN familiar to those skilled in the art. Similarly any necessary files for performing the functions attributed to the computers servers or other network devices may be stored locally and or remotely as appropriate. Where a system includes computerized devices each such device can include hardware elements that may be electrically coupled via a bus the elements including for example at least one central processing unit CPU at least one input device e.g. a mouse keyboard controller touch screen or keypad and at least one output device e.g. a display device printer or speaker . Such a system may also include one or more storage devices such as disk drives optical storage devices and solid state storage devices such as random access memory RAM or read only memory ROM as well as removable media devices memory cards flash cards etc.

Such devices also can include a computer readable storage media reader a communications device e.g. a modem a network card wireless or wired an infrared communication device etc. and working memory as described above. The computer readable storage media reader can be connected with or configured to receive a computer readable storage medium representing remote local fixed and or removable storage devices as well as storage media for temporarily and or more permanently containing storing transmitting and retrieving computer readable information. The system and various devices also typically will include a number of software applications modules services or other elements located within at least one working memory device including an operating system and application programs such as a client application or Web browser. It should be appreciated that alternate embodiments may have numerous variations from that described above. For example customized hardware might also be used and or particular elements might be implemented in hardware software including portable software such as applets or both. Further connection to other computing devices such as network input output devices may be employed.

Storage media and computer readable media for containing code or portions of code can include any appropriate media known or used in the art including storage media and communication media such as but not limited to volatile and non volatile removable and non removable media implemented in any method or technology for storage and or transmission of information such as computer readable instructions data structures program modules or other data including RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disk DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by the a system device. Based on the disclosure and teachings provided herein a person of ordinary skill in the art will appreciate other ways and or methods to implement the various embodiments.

The specification and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense. It will however be evident that various modifications and changes may be made thereunto without departing from the broader spirit and scope of the invention as set forth in the claims.

Other variations are within the spirit of the present disclosure. Thus while the disclosed techniques are susceptible to various modifications and alternative constructions certain illustrated embodiments thereof are shown in the drawings and have been described above in detail. It should be understood however that there is no intention to limit the invention to the specific form or forms disclosed but on the contrary the intention is to cover all modifications alternative constructions and equivalents falling within the spirit and scope of the invention as defined in the appended claims.

The use of the terms a and an and the and similar referents in the context of describing the disclosed embodiments especially in the context of the following claims are to be construed to cover both the singular and the plural unless otherwise indicated herein or clearly contradicted by context. The terms comprising having including and containing are to be construed as open ended terms i.e. meaning including but not limited to unless otherwise noted. The term connected is to be construed as partly or wholly contained within attached to or joined together even if there is something intervening. Recitation of ranges of values herein are merely intended to serve as a shorthand method of referring individually to each separate value falling within the range unless otherwise indicated herein and each separate value is incorporated into the specification as if it were individually recited herein. All methods described herein can be performed in any suitable order unless otherwise indicated herein or otherwise clearly contradicted by context. The use of any and all examples or exemplary language e.g. such as provided herein is intended merely to better illuminate embodiments of the invention and does not pose a limitation on the scope of the invention unless otherwise claimed. No language in the specification should be construed as indicating any non claimed element as essential to the practice of the invention.

Preferred embodiments of this disclosure are described herein including the best mode known to the inventors for carrying out the invention. Variations of those preferred embodiments may become apparent to those of ordinary skill in the art upon reading the foregoing description. The inventors expect skilled artisans to employ such variations as appropriate and the inventors intend for the invention to be practiced otherwise than as specifically described herein. Accordingly this invention includes all modifications and equivalents of the subject matter recited in the claims appended hereto as permitted by applicable law. Moreover any combination of the above described elements in all possible variations thereof is encompassed by the invention unless otherwise indicated herein or otherwise clearly contradicted by context.

All references including publications patent applications and patents cited herein are hereby incorporated by reference to the same extent as if each reference were individually and specifically indicated to be incorporated by reference and were set forth in its entirety herein.

