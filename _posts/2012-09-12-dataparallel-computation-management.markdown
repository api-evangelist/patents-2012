---

title: Data-parallel computation management
abstract: Data-parallel computation programs may be improved by, for example, determining the functional properties user defined functions (UDFs), eliminating unnecessary data-shuffling stages, and/or changing data-partition properties to cause desired data properties to appear after one or more user defined functions are applied.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09383982&OS=09383982&RS=09383982
owner: Microsoft Technology Licensing, LLC
number: 09383982
owner_city: Redmond
owner_country: US
publication_date: 20120912
---
Data parallel computation processes or jobs typically involve multiple parallel computation phases that are defined by user defined functions UDFs . One factor in data parallel computation is the creation of data partitions with appropriate properties to facilitate independent parallel computation on separate machines or partitions in each phase. For example often before a reducer UDF may be applied in a reduce phase data partitions are clustered with respect to a reduce key so that all data entries with the same reduce key are mapped to and are contiguous in the same partition.

To achieve desirable data partition properties data shuffling stages are often introduced to prepare data for parallel processing in future phases. A data shuffling stage may re organize and re distribute data into appropriate data partitions. For example before applying a reducer UDF a data shuffling stage might perform a local sort on each partition re partition the data on each source machine for re distribution to destination machines and do a multi way merge on redistributed sorted data streams from source machines all based on the reduce key. However data shuffling tends to incur expensive network and disk input and output operations I O because it involves all of the data.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

This disclosure describes techniques to relate data partition properties to data shuffling in a process function e.g. user defined function UDF centric data parallel computation model. Additionally or alternatively this disclosure discusses defining how a set of functional properties for UDFs change the data partition properties when the UDFs are applied. Additionally or alternatively this disclosure discusses a program analysis framework to identify functional properties for UDFs as well as an optimization framework to reason about and leverage data partition properties functional properties and data shuffling.

This disclosure describes techniques to relate data partition properties to data shuffling in a user defined function UDF centric data parallel computation model. Additionally or alternatively this disclosure discusses defining how a set of functional properties for UDFs change the data partition properties when the UDFs are applied. Additionally or alternatively this disclosure discusses a program analysis framework to identify functional properties for UDFs as well as an optimization framework to reason about and leverage data partition properties functional properties and data shuffling.

Often a data parallel process treats process functions e.g. UDFs as black boxes meaning for purposes of this disclosure that it may be assumed that functionality of the black box is unknown and or a data structure of output data may or may not depend on a structure of the input data. For example it may be assumed that after each application of a UDF the output data does not retain data configuration or organizational properties that the data may have had before the UDF was applied. This may lead to unnecessary data shuffling steps that consume unnecessary input output operations and or processor cycles since in some instances the data may retain some or all configuration or organizational properties after application of the UDF.

This disclosure discusses determining functional properties of UDFs and the affect on data properties caused by application of the UDF. Understanding what data properties may be retained after application of a given UDF may allow elimination of unnecessary and expensive data shuffling stages and improve efficiency of the resulting execution plan. Additionally this disclosure discusses changing a partition key to create cause data properties to exist at subsequent UDFs. This may allow the elimination of a data shuffling stage altogether the replacement of an expensive data shuffling stage with a relatively lower cost stage or both.

Often a typical data parallel process or job performs one or more transformations on large datasets which usually include a list of records each with a list of columns A transformation uses a key that comprises one or more columns. For parallel computation a dataset is often divided into data partitions that may be operated on independently in parallel by separate processors or machines. A data parallel process may involve multiple parallel computation phases whose computations are defined by user defined functions UDFs . As an illustrative non limiting example one of the possible embodiments follows a Map Reduce Merge scheme that contains three types of UDFs mappers reducers and mergers. By selecting a low level assembly language like computation model illustrative techniques and embodiments may be applied broadly. For example programs written in various high level data parallel languages including but not limited to Structured Computations Optimized for Parallel Execution SCOPE developed by Microsoft Hadoop InteractiVE HIVE developed by Apache PigLatin developed by Yahoo and DryadLINQ developed by Microsoft may be compiled into processes in the illustrative model.

Data shuffling stages may be introduced to achieve appropriate data partition properties by re arranging data records without modifying them. A typical data shuffling stage may comprise three steps 1 a local sort step that sorts records in a partition with respect to a key 2 a re partition step that re distributes records to partitions via hash or range partitioning and 3 a multi way merge step that clusters re distributed records based on the key. Any one step or combinations of those steps may be used to achieve certain properties. Achievement of some of the certain properties may use all three steps while others may use fewer depending on the data partition properties before and after data shuffling.

Additionally if the data shuffling step of Local Sort is applied to data with Disjoint properties data with PSorted properties may result. Similarly if the data shuffling step of Local Sort is applied to data with Clustered properties data with PSorted properties may result. However if any of the data shuffling steps of Repartition hash and Merge collectively are applied to data with LSorted properties data with PSorted properties may result. Further if any of the data shuffling steps of Repartition range and Merge collectively are applied to data with LSorted properties data with GSorted properties may result. Similarly if any of the data shuffling steps of Repartition range and Merge collectively are applied to data with PSorted properties data with GSorted properties may result. As discussed elsewhere in this disclosure knowing the resulting data partition properties after a first data shuffling step has been applied may allow for the elimination of a or selection of a less expensive resource intensive second data shuffling step since the resulting data partition properties after the first data shuffling step may have some or all of the data partition properties sought by application of the second data shuffling step.

Additionally or alternative in one embodiment Clustered might not be able to be generated precisely through data shuffling steps if the implementation for the merge step uses merge sort. Therefore in at least that embodiment PSorted is generated instead to satisfy Clustered . As a non limiting example and for clarity and brevity in the rest of this disclosure a data parallel process may be represented as a directed acyclic graph DAG with three types of vertices 1 data vertices that correspond to input output data each with an associated data partition property 2 compute vertices that correspond to computation phases each with a type mapper reducer or merger and a UDF and 3 shuffle vertices that correspond to data shuffling stages each indicating the steps in that stage. The repartitioning stages in shuffle vertices also specify whether hash or range partitioning is used. This DAG may be created manually or generated automatically by a compiler from a program in a high level language and allows flexibly to define an optimization scope. For example the same framework may be used to analyze a pipeline of processes or a segment of a process.

One embodiment contemplates increasing the efficiency and execution speed collectively optimize or optimization of data shuffling by finding a valid execution plan with the lowest cost for a process J . The execution plan may satisfy the following conditions 1 the execution plan differs from process J at data shuffling stages 2 for each computation phase the input may have the expected data partition properties e.g. data partitions are Clustered for a reducer and PSorted for a merger 3 for a merger the input vertices may have the same data partitioning e.g. PSorted or GSorted may be on the same merge key and 4 the execution plan may preserve all data partition properties of an output vertex.

Data shuffling stages in data parallel models tend to be expensive as they often involve heavy disk and network I O. The data shuffling stages are often added to satisfy data partition properties for subsequent computation phases and to satisfy user expectations on output data. Although a preceding data shuffling stage may result in certain data partition properties a computation phase with a UDF often is not guaranteed to preserve those properties because traditionally UDFs are considered proverbial black boxes. 

One illustrative embodiment contemplates turning UDFs into proverbial grey boxes by defining appropriate functional properties that expose how data partition properties propagate across phases. For purposes of this disclosure a grey box may mean that some functionality and some data structure of the output data based at least in part on the input data may be known or determined. This may be in contrast to a proverbial black box where it may be assumed that functionality of the black box is unknown and or data structure of the output data may or may not depend on the structure of the input data. This may facilitate the identification and elimination of unnecessary data shuffling steps.

A functional property may describe an effect on an organizational relationship within input data and an associated organizational relationship within output data. For example a functional property may describe how an output column that is computed by a UDF is dependent upon the input columns of the UDF. Functional properties that preserve or transform data partition properties may be identified. In various embodiments those functional properties may be identified through automatic program analysis. For clarity and brevity this discussion focuses on deterministic functions that compute a single output column from a single input column in one single record however it is understood that this disclosure is not limited to this illustrative example. In various embodiments a UDF might exhibit one functional property on one output column and another functional property on another column. As a non limiting example this discussion is directed to columns that are used as a reduce key merge key or re partition key as well as those used to compute those keys.

Various embodiments may utilize functional properties exhibited by UDFs. For example a pass through function may be an identity function where the output column is the same as the corresponding input column. Often a reducer merger is a pass through function for the reduce merge key. A pass through function likely preserves all data partition properties.

Additionally or alternatively function may be strictly monotonic if and only if for any inputs x and x x

Additionally or alternatively function may be one to one if and only if for any inputs x and x x x implies x x . Examples of one to one UDFs include but are not limited to reversing uniform resource locators URLs e.g. www.acm.org org.acm.www and encryption algorithms e.g. MD5 Message Digest Algorithm calculation assuming no conflicts . One to one functions may not preserve sort order but may preserve contiguity within a partition and the partitioned property across partitions. As a result it may preserve data partition properties such as Disjoint and Clustered but may downgrade GSorted and PSorted to Clustered.

In various embodiments a UDF with Pass Through or Strictly Monotonic properties collectively applied to data with PSorted data properties may be sufficient to preserve PSorted data properties. Further a UDF with Monotonic properties applied to data with PSorted data properties may cause LSorted data properties in the data. Similarly a UDF with One to One properties applied to data with PSorted data properties may cause Clustered data properties in the data. However a UDF with other functional properties applied to data with PSorted data properties may cause AdHoc data properties in the data.

In various embodiments a UDF with Pass Through Strictly Monotonic or One to One properties collectively applied to data with Clustered data properties may be sufficient to preserve Clustered data properties. However a UDF with other functional properties or Monotonic properties applied to data with Clustered data properties may cause AdHoc data properties in the data.

In various embodiments a UDF with Pass Through Strictly Monotonic or One to One properties collectively applied to data with Disjoint data properties may be sufficient to preserve Di sj oint data properties. However a UDF with other functional properties or Monotonic properties applied to data with Disjoint data properties may cause AdHoc data properties in the data.

In various embodiments a UDF with Pass Through Strictly Monotonic or Monotonic properties collectively applied to data with LSorted data properties may be sufficient to preserve LSorted data properties. However a UDF with other functional properties or One to One properties applied to data with LSorted data properties may cause AdHoc data properties in the data.

Additionally or alternatively a monotonic UDF functionality may be sufficient for preserving LSorted one to one UDF functionality may be sufficient for preserving Clustered and strictly monotonic UDF functionality may be sufficient for preserving GSorted .

In various embodiments UDFs may be annotated with appropriate functional properties. Additionally or alternatively program analysis techniques may infer properties automatically when possible in a bottom up approach. Often functional properties focus on the dependency relationship between an output column and its relevant input columns allowing program slicing to extract a UDF s core function to infer its functional property with respect to each output column of interest. For example the input columns may comprise facts to which deduction rules may be applied to the low level instructions as well as third party library calls to infer functional properties recursively until a fixed point is reached. The process may return the final functional properties associated with the UDFs upon termination.

For purposes of this disclosure deduction rules may infer the functional property of an instruction s output operand from the functional properties of its input operands. Deduction rules may include but are not limited to the examples shown in lines . For example a first exemplary deduction rule line may state that for ASSIGN instructions the functional property of the input operand x may be propagated to the output operand y the   symbol may represent any functional property . The second and third exemplary deduction rules may state that for ADD instructions the output operand is strictly increasing as long as one of the input operands is strictly increasing while the other is increasing.

In various embodiments UDFs may also call into functions in third party libraries. Various embodiments contemplate applying deduction rules directly to the instructions in the library calls or treating these function calls simply as instructions and providing deduction rules manually. Various embodiments contemplate accumulating a knowledge base with manually provided deduction rules for commonly used library calls. Those manual annotations may be useful for cases where the automatic inference runs into its limitation. For example the functional property for MD5 shown in line may not be inferred automatically. Additionally or alternatively various embodiments contemplate automatically assigning annotations using the accumulated knowledge base.

Various embodiments contemplate rules that encode the relations among functional properties where one functional property might imply another. Examples of such rules include but are not limited to those shown in lines . Various embodiments contemplate defining Func as the weakest functional property that satisfies no specific constraints and also introduce Constant as a pseudo functional property for constant values which may be both increasing Inc and decreasing Dec .

Various embodiments contemplate determining the expectation on each data shuffling stage for validity as part of a valid execution plan. An embodiment may apply a backward WP analysis to compute the weakest pre condition before each computation phase and the weakest post condition after each data shuffling stage that maintains correctness of expected data partition properties.

For example data A has the data shuffling stage S applied while data B has the data shuffling stage S applied. The results of S and S are merged at . The results of the merger at are mapped at . The data shuffling stage S is then applied to the results of the mapper at the results of which are reduced at . The data shuffling stage S is applied to the results of the reducer at the results of which comprise data C . Additionally the results of S are reduced at . The data shuffling stage S is then applied to the results of the reducer at the results of which comprise data D .

In this illustrative case both the merger and the reducers are pass through functions and the mapper is a strictly monotonic function using log . This illustrative embodiment contemplates applying a backward WP analysis to determine the weakest post condition for the data shuffling stages. In this example the backward WP analysis results in GSorted for S and S Clustered for S PSorted for S and max PSorted Clustered PSorted for S.

Various embodiments contemplate after the backward WP analysis is complete finding valid execution plans through forward data partition property propagation. This process may track output data partition property in a CurrentPostDP field for each vertex and discover valid query plans along the way.

Additionally or alternatively revisiting the example shown in various embodiments contemplate creating a different execution plan. For example applying the illustrative approach shown in the CurrentPostDP may be set to AdHoc for the input data vertices and PSorted for S and S. In this example since the Merger may be a pass through function its CurrentPostDP may be set to PSorted. The CurrentPostDP may also be set to PSorted after the Mapper because it is strictly monotonic. Because PSorted may imply Clustered which is the weakest post condition for S all steps of S may be removed to produce an alternative valid plan.

In various embodiments all valid execution plans may be enumerated. Additionally or alternatively heuristics may be applied to limit the candidates if there are too many valid execution plans to evaluate. Regardless the valid executions may be evaluated based on a cost model where the valid execution plan with the lowest cost may be chosen.

Not all UDFs have the desirable functional properties for preserving data partition properties especially when it is conservatively assumed that the input data may be arbitrary. Various embodiments contemplate leveraging the ability to re define a partitioning key to apply some constraint to the input data so as to preserve certain data partition properties e.g. Disjoint for optimizing data shuffling. These mechanisms may further increase the coverage of an optimization framework described above. Various embodiments described herein contemplate re defining partitioning keys to allow preservation modification and or manipulation of data partition properties.

For example consider the case shown in where data A has the data shuffling stage S performed on it by hashing m rather than on x directly as is done at . The output of S may be reduced at and then mapped at . The mapped data at now may need only a Local Sort to be applied at for the output to be reduced at resulting in data B .

Various embodiments contemplate that although this approach may reduce the amount of total network I O by eliminating a later re partitioning step the re partitioning in S may be slightly more expensive in time and resources as it may have to invoke m on the input records. For example if the number of input records far exceeds the number records in the later mapper phase with m because the reducer on x at and may reduce the number of records at the later data shuffling stage. To reduce this extra overhead various embodiments contemplate applying program slicing to get a simpler function similar to the approach discussed with respect to the analysis of functional properties. Additionally or alternatively the partitioning might lead to data skew that does not exist in the original plan because of the different partitioning key used. However various embodiments contemplate using a cost model to assess whether or not to apply this candidate optimization.

Various embodiments contemplate generalizing this type of optimization to a chain of data shuffling stages. For example S S . . . SN may be a chain of data shuffling with hash re partitioning where before each Si except for S there may be a mapper with UDF mi i 2 . . . N . To allow for a single re partitioning to cause the keys in later phases to be partitioned appropriately a partition function in S may be constructed as a hash on mN . . . m where x is the initial input key.

Additional examples of data shuffling patterns include but are not limited to joined shuffling and forked shuffling. Joined shuffling is often used to implement a JOIN to correlate records in data parallel programs while forked shuffling is often used allow an input to be consumed by more than one thread.

In this example shown in data shuffling stages S and S are retained and inserted by the merger phase at . This pattern may be considered as the merge of multiple chained data shuffling meaning multiple data shuffling steps are aligned in a chain whose results are merged. For example one chained data shuffling may be formed by S and S while another may be formed by S and S. Various embodiments contemplate separately using a hash function on for S and a hash function on for S allowing for the removal of the re partitioning in S and S. In this example due to merger the two chains may be aligned for example at in that the same key may be mapped to the same partition. This may be achievable when the same hashing is applied to and .

Using the discussed techniques as well as others various embodiments contemplate creating new execution options with re defined partitioning keys. Various embodiments contemplate integrating techniques as mechanisms that may preserve certain data partition properties. These options may be integrated into the framework described above creating additional valid execution plans which may be evaluated and considered using a cost model.

Various embodiments contemplate implementing the optimization framework discussed above in existing and forthcoming data parallel computation systems. For example one possible embodiment has been implemented and integrated into the SCOPE compiler and optimizer. SCOPE is a SQL like scripting language for data parallel computation. The SCOPE optimizer uses a transformation based optimizer to generate efficient execution plans. The optimizer leverages existing work on relational query optimization and performs rich and non trivial query rewritings that consider the input script in a holistic manner. One of the embodiments implemented with SCOPE added the capability to reason about functional properties data partition properties and data shuffling into the current optimizer. Without understanding UDFs the system may be unable to derive any structural properties and thus potentially miss optimization opportunities. However the systems and process described herein are able to identify functional properties between input and output columns and then integrate those functional properties into the optimization framework. This enables efficient property derivation and allows the optimizer to optimize query plans with UDFs effectively.

One of the possible embodiments implements a UDF analyzer similar to UDF analyzer at a high level IR HIR of a framework e.g. a Phoenix framework shipped with Microsoft Visual Studio 2010 together with a database engine e.g. the BDD Based Deductive DataBase bddbddb engine . Phoenix is a framework for building compiler related tools for program analysis and optimizations. It allows external modules to be plugged in with full access to the internal Phoenix Intermediate Representation IR . With Phoenix the analyzer may feed the instructions and the library calls for a given UDF to the bddbddb engine together with the deduction rules. The engine then applies the deduction process.

In one of the embodiments the top eight 8 unary operators and seven 7 binary operators excluding the operator opcode CALL are selected based at least in part on frequency of use. These operators account for a majority of operator uses excluding CALL . In this embodiment various rules for those operators may be selected using several heuristics including but not limited to i some instruction type for example ASSIGN and BOX which belong to the same equivalent class as they share the same set of deduction rules ii binary operators for example the ADD operator shown in lines and that often include more than one rule. Several additional pseudo operators may be added to convert some operators to other operators. For example NEGATE and RECIPROCAL may be added to convert SUBSTRACT and DIVIDE to ADD and MULTIPLY respectively thereby reducing the total number of rules.

Additionally or alternatively constraints are often used in the rules for precision. The constraints may be on some aspects of the operands for example the types and value ranges of the operands. For example the CONVERT operator may be used to convert numbers between different types. Converting a number from a type with a smaller byte size to one with a larger size e.g. from int to double may preserve its value. This conversion may be considered a pass through function. However this may not be the case for the opposite direction. Various embodiments extract operand types and may make the rules type sensitive with the type constraints embedded to handle these cases.

Additionally or alternatively the UDFs may contain loops and branches. For example the value of an operand may come from any one of its input operands defined in any of the branches. Various embodiments introduce the INTERSECT operator that may have the rule stating that the output operand may have a certain property if both its input operands have the same functional property.

One of the possible embodiments implemented contemplates that the rewriter generates valid execution plans using the algorithms discussed above. For example one of the possible embodiments contemplates that the rewriter may work at the physical level while the SCOPE optimizer may start with logical relational operators. For ease of implementation in various embodiments the rewriter may take as input the best physical execution plan from the SCOPE optimizer. The results from this embodiment of the rewriter may then be assessed based on the internal cost model of the SCOPE optimizer. This embodiment of integration might lead to sub optimal results as two optimization phases are carried out separately. Other embodiments contemplate integrating the rewriter into the SCOPE optimizer to reason about functional properties and structured data properties in a single uniform framework. This may further provide seamless generation and optimization of both serial and parallel query plans.

Various embodiments may be implemented in web search related SCOPE processes. In this illustrative implementation the processes may run on the same cluster as a process that was previously collected. The number of machines that may be used in each process may depend on the size of the input data.

One illustrative embodiment may relate to anchor data. For example hyperlinks in web pages may form a web graph. Anchor texts associated with hyperlinks together with the web graph may be valuable for evaluating the quality of web pages and other search relevance related metrics. One of the anchor data pre processing processes may be to put the anchors that point to the same page together using a data shuffling stage and de duplicate the anchors with the same text. The process may further output the reversed URL and the anchor text pairs e.g. org.acm.www sigs anchor text instead of www.acm.org sigs anchor text . Since this reversed URL format may be the de facto representation of a URL URLs of the same domain may be laid out contiguously in this format to enable simple domain level aggregations.

In an illustrative embodiment of an optimization and with respect to the UDF analyzer may analyze the reversing UDF determining it to be a one to one function. The pre condition and post condition analyzer may determine that the condition of data properties expected for the reduction at may be the Clustered property. The data partition property propagation at may determine that the reversing performed at may preserve the Clustered property. The plan where stage S may be eliminated from the second process may be formulated as a valid plan at . The plan may be selected as the optimized plan based on the associated cost determined at .

An illustrative embodiment may relate to trend analysis. For example trend analysis may be a way to understand how things change over time which may be useful for many search related applications as well as being a stand alone service. One kind of trend analysis process may collect the term time aspect tuples in the search query logs where term may be the search keyword time may correspond to a time when the query is submitted and aspect may represent one of the search query s property e.g. its market and may aggregate various aspects occurrences at different time scales such as hours days and months. For instance the top three markets over the years for a specific brand may be determined using trend analysis.

Before optimization each process and may include data shuffling stages S and S . However after analysis the illustrative embodiment may identify a valid plan where the three shuffling stages are merged into one S and the partition key in the first process is re defined. The function may ensure that the seconds within the same week are on the same partition which may provide the Disjoint property even after the two mapper functions convert seconds to days and weeks respectively. Additionally since the time conversion function may be increasing LSorted may be preserved and the local sort operations in the second process and third process may be eliminated. Together the optimization may eliminate two shuffling stages and may ensure PSorted property before each reducer function in the three processes.

One illustrative embodiment may relate to query anchor relevance. For example search queries and anchors may be two term sets that may be bridged by URLs. Analyzing the joined data set may allow for the improvement of the search quality. If a query happens to result in a URL that an anchor points to then the query and the anchor may very likely be relevant. For instance if the word China appears frequently in query with result URL example.org a.html and the word Emerging Market appears in the anchor that points the same example.org a.html then China and Emerging Market may be relevant. Furthermore if these two words appear in example.org many times and their pointing to URLs also overlap frequently they may have a higher relevance.

The three illustrative embodiments discussed above with respect to may increase the performance of the respective processes. For example between the original versions and the optimized ones a read I O reduction may be realized.

Various embodiments contemplate context sensitive and path sensitive analysis. By being context sensitive an embodiment s analysis may be able to differentiate the cases where a function is invoked by different callers with different parameters. By being path sensitive an embodiment s analysis may take branching conditions into account. Various embodiments contemplate incorporating the value range information to handle operators such as MULTIPLY whose functional properties may depend on the value ranges of the input operands.

In at least one configuration the computing device includes at least one processor and system memory . The processor s may execute one or more modules and or processes to cause the computing device to perform a variety of functions. In some embodiments the processor s may include a central processing unit CPU a graphics processing unit GPU both CPU and GPU or other processing units or components known in the art. Additionally each of the processor s may possess its own local memory which also may store program modules program data and or one or more operating systems.

Depending on the exact configuration and type of the computing device the system memory may be volatile such as RAM non volatile such as ROM flash memory miniature hard drive memory card or the like or some combination thereof. The system memory may include an operating system one or more program modules and may include program data . The operating system includes a component based framework that supports components including properties and events objects inheritance polymorphism reflection and provides an object oriented component based application programming interface API . The computing device is of a very basic illustrative configuration demarcated by a dashed line . Again a terminal may have fewer components but may interact with a computing device that may have such a basic configuration.

Program modules may include but are not limited to an analyzer a rewriter a selector and or other components .

The computing device may have additional features and or functionality. For example the computing device may also include additional data storage devices removable and or non removable such as for example magnetic disks optical disks or tape. Such additional storage is illustrated in by removable storage and non removable storage .

The storage devices and any associated computer readable media may provide storage of computer readable instructions data structures program modules and other data. Computer readable media includes at least two types of computer readable media namely computer storage media and communication media.

Computer storage media includes volatile and non volatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data.

Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other non transmission medium that may be used to store information for access by a computing device.

In contrast communication media may embody computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transmission mechanism. As defined herein computer storage media does not include communication media.

Moreover the computer readable media may include computer executable instructions that when executed by the processor s perform various functions and or operations described herein. The computing device may also have input device s such as a keyboard a mouse a pen a voice input device a touch input device etc. Output device s such as a display speakers a printer etc. may also be included.

The computing device may also contain communication connections that allow the device to communicate with other computing devices such as over a network. By way of example and not limitation communication media and communication connections include wired media such as a wired network or direct wired connections and wireless media such as acoustic radio frequency RF infrared and other wireless media. The communication connections are some examples of communication media. Communication media may typically be embodied by computer readable instructions data structures program modules etc.

The illustrated computing device is only one example of a suitable device and is not intended to suggest any limitation as to the scope of use or functionality of the various embodiments described. Other well known computing devices systems environments and or configurations that may be suitable for use with the embodiments include but are not limited to personal computers server computers hand held or laptop devices multiprocessor systems microprocessor based systems set top boxes game consoles programmable consumer electronics network PCs minicomputers mainframe computers distributed computing environments that include any of the above systems or devices implementations using field programmable gate arrays FPGAs and application specific integrated circuits ASICs and or the like.

The implementation and administration of a shared resource computing environment on a single computing device may enable multiple computer users to concurrently collaborate on the same computing task or share in the same computing experience without reliance on networking hardware such as but not limited to network interface cards hubs routers servers bridges switches and other components commonly associated with communications over the Internet as well without reliance on the software applications and protocols for communication over the Internet.

For ease of understanding the processes discussed in this disclosure are delineated as separate operations represented as independent blocks. However these separately delineated operations should not be construed as necessarily order dependent in their performance. The order in which the processes are described is not intended to be construed as a limitation and any number of the described process blocks may be combined in any order to implement the process or an alternate process. Moreover it is also possible that one or more of the provided operations may be modified or omitted. The processes are illustrated as a collection of blocks in logical flowcharts which represent a sequence of operations that may be implemented in hardware software or a combination of hardware and software. For discussion purposes the processes are described with reference to the system shown in . However the processes may be performed using different architectures and devices.

At functional properties of the UDFs may be extracted and determined. For example the functional properties may comprise one or more of pass through one to one monotonic or strictly monotonic.

At data conditions expected for each process phase may be identified. For example the data conditions may comprise one or more of AdHoc LSorted Disjoint Clustered PSorted or GSorted. Additionally or alternatively a minimum data condition expected for each process phase may be found.

At a modified computation process plan may be generated. The modified computation process plan may improve performance over the received computation process at . For example the modified computation process plan may comprise eliminating a data shuffling step contained in the computation process. Additionally or alternatively the modified computation process plan may comprise a modified partition key to allow the elimination of a data shuffling step contained in the computation process received at . Additionally or alternatively the modified computation process plan may comprise a modified partition key to allow the replacement of a data shuffling step contained in the computation process with a step causing less computational cost than a data shuffling step contained in the computation process received at . Additionally or alternatively a plurality of modified computation process plans may be generated as candidates to replace the computation process received at . Here heuristics may be applied to eliminate one or more candidates. For example the heuristics may be applied before a candidate is evaluated through the cost model.

At a rewritten computation process may be selected from a plurality of modified computation process plans based at least in part on a respective cost set by the cost mode.

The subject matter described above can be implemented in hardware software or in both hardware and software. Although implementations have been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts are disclosed as example forms of implementing the claims. For example the methodological acts need not be performed in the order or combinations described herein and may be performed in any combination of one or more acts.

