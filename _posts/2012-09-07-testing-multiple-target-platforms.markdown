---

title: Testing multiple target platforms
abstract: An automated REpresentational State Transfer (REST) testing tool receives a file representing a set of tests to run on a target test platform and identifies a type of the file. Then the testing tool parses the file based on the type to extract test parameters, and performs test actions on the target test platform based on the test parameters.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09218269&OS=09218269&RS=09218269
owner: Red Hat Israel, Ltd.
number: 09218269
owner_city: Raanana
owner_country: IL
publication_date: 20120907
---
Embodiments of the present invention relate to testing Representational State Transfer REST Application Programming Interfaces APIs and more specifically to automated testing of different products with different REST API formats.

Many software applications and computing platforms make use of Representational State Transfer REST Application Programming Interfaces APIs . REST refers to a particular architectural style and particular interfaces for clients and servers. Clients and servers have a uniform interface in which individual clients and servers are replaceable as long as the interface remains unchanged. Further communication between REST clients and servers is stateless and cacheable. The REST style of client and server architecture can include other attributes as well. Automated testing tools can perform functional testing and regression testing on such software applications via REST APIs. However different products while each conforming to the REST architectural style can be incompatible between each other such as due to different APIs different available states or different communication protocols.

Thus in order to perform functional or regression testing to such different RESTful platforms completely separate tests are created manually for each RESTful platform.

Described herein is a system a method and apparatus for a generic automated testing tool which allows testing of multiple different products via their respective REST APIs. The example generic automated testing tool can accept input in multiple formats such as XML a Python script structured text or a spreadsheet in a format such as OpenDocument Spreadsheet ODS format. The automated testing tool can parse and import data from these various input types to determine which tests to run for a particular target test product. Then the automated testing tool can execute the tests for the desired target test products and validate responses from the target test product. The automated testing tool can also generate XML and JUnit test results. The automated testing tool can perform simultaneous parallel testing of multiple target test products or of multiple aspects of a single target test product. Further the automated testing tool can incorporate plug ins via a plug in management subsystem for providing additional functionality such as importing different input file types or executing tests on different platforms via their respective REST APIs. Besides providing the ability to import and export to different platforms plug ins can also provide other functionality for the automated testing tool such as cleanup after running a test interfacing with a bug tracker logging events occurring while running tests and so forth.

The spreadsheet also illustrates that multiple actions can be grouped such as rows 3 13 which are grouped to create a data center for testing. So a user can easily create a new test case by adding a new row to the spreadsheet and filling in each cell corresponding to that row. illustrates an example test scenario represented as an XML representing input test file . Instead of rows and columns like the spreadsheet the XML file represents information in XML tags. illustrates an example test scenario represented as an input Python script file. The same information can be conveyed to the test runner in any of these file formats or in other file formats. These three file formats are provided as illustrative examples.

Returning to the configuration file can provide for the test runner various system wide settings that are beyond the scope of the input test file. For example the configuration file can specify for the test runner whether and what to log parameters for connecting to a command line interface login credentials for performing the tests cryptographic tokens host names local or network paths to specific resources used for testing debugging information flags or conditions for skipping certain tests addresses of host machines associated with performing tests and so forth. The configuration file can include instructions for various plug ins in a plug in manager as well. Plug ins provide instructions for adapting communications with for different platforms using a common testing engine. In this way the testing engine or other components disclosed herein can be used with multiple different target testing platforms or according to different communication or markup standards. The plug in manager manages the plug ins such as determining which plug ins are available installing new plug ins removing plug ins selecting which plug in or plug ins to use for a particular test or test action and so forth. The plug in manager can maintain multiple plug ins for different versions of a same communication protocol to increase cross version compatibility. The configuration file can provide multiple different sets of configurations for simultaneous use.

The configuration file can further specify a test engine to use for a particular test. For example if several REST based test engines are supported and available the test runner can switch between the engines by loading an appropriate configuration file with a different test engine parameter. The configuration file can indicate specific portions of an input file to process when performing a test such as specific sheets or cell ranges in a spreadsheet specific line numbers or tags in an XML file or a specific group of tests. In this way a single input file can contain a listing of test actions for multiple different scenarios and only a desired subset of the test actions in the input file can be performed. The configuration file can indicate whether to enable a debug mode whether to perform certain tests in parallel a path to an XML schema document and so forth. In some embodiments the configuration file provides some configuration information which is overridden by other configuration information in the input test file .

The configuration file can include multiple sections such as a RUN section a REST CONNECTION section a PARAMETERS section and a REPORT section. The RUN section provides configuration data for how to execute tests. An example RUN section of a simple configuration file is provided below 

An example of the RUN section in the same XML file running parallel with different configuration files is provided below 

The REST CONNECTION section can include parameters such as scheme for defining whether to communicate with the test platform via HTTP or HTTPS host which indicates the user s REST client machine port which defines which port the REST API communicates on entry point which defines a main URL suffix for the REST API such as api entry point api user which defines a user name for logging in to the REST API client user domain and other parameters relating to establishing the REST connection.

The PARAMETERS section defines global parameters used by the test. These global parameters are optional and can be removed or extended depending on a specific test scenario definition. Any of these parameters can be a single value or a list of values. In one embodiment the PARAMETERS section includes customized sections for parallel testing. The test runner can run the same test file in parallel with different parameters from one configuration file. The test runner can use new configuration sections with required parameters for each of the parallel threads for parallel tests. A simplified configuration file for parallel tests is provided below 

The REPORT section of the configuration file defines parameters required for tests results reporting. For example the REPORT section can include a has sub tests parameter marked yes if each test should be reported as a sub test of its group testing module and marked no if each test should be reported independently. The REPORT section can include an add report nodes parameter listing node names if additional nodes besides the default ones as defined in input XML nodes should be added to report or marked no otherwise. For example additional nodes may be added for benchmark tests including but not limited to response time for an action a corresponding load on the CPU and memory or any other applicable benchmark criteria. The configuration file can indicate added nodes by add report nodes version name. If the user wants to add additional nodes to the report the function used in the test scenario should return these properties in its output dictionary.

Returning again to the test runner processes the input test file according to the configuration file and optionally using a plug in from the plug in manager to generate test actions . The input test file defines the test scenario. Depending on the file format of the input test file each test case in the input file contains certain attributes. For example an XML input test file includes XML nodes an ODS input test file includes columns and rows and a Python script input test file includes TestCase object attributes.

A test name attribute in the input file indicates the name of the test which will appear in reports. A test action attribute in the input file indicates an action which will run to implement the test. Each action can be mapped to a function defined in the configuration file for example. Parameters attributes read from the input test file are used to construct or execute the test. Parameter names should correspond to the names of test function parameters. The user can use parameter names from the configuration file as place holders. For example if the user puts the parameter name in brackets in the configuration file the test runner will replace the parameter name with the relevant value at run time. If the user desires product constants from the configuration file the user can enter the string eparam name. When the test runner encounters a list such as a comma separated list in the configuration file the test runner can fetch the corresponding value via array indexing such as name of param param index . The test runner can obtain the list as a string by using name of param such as when a user requests to view all or part of the list. When using a loop in the run column see below the user can concatenate the loop iteration index to any of the parameters. Some example parameter values are provided below 

The user can set positive mandatory to TRUE if the test should succeed FALSE if the test should not succeed or NONE if the function should not use this parameter at all. The user can set run mandatory to TRUE if the test should be run or no if the test should not be run. In one embodiment the user can specify a Python condition here such as if . However the user can also incorporate more complex Python conditions here as well. For example the user can specify ifaction in which the test will run if the action returns TRUE. On the other hand the user can specify not ifaction in which the test will run if the action returns FALSE.

In addition to conditions the user can include loops and forks in the input test file . A loop can be used to make zero or more of a same operation. A loop performs iterations one after another but a fork executes at least some of the iterations simultaneously. Both loops and forks can be used with groups so the contents of the group is executed several times and in a particular order. Examples of the syntax for specifying loops and forks are provided below 

The input test file can iterate over several parameters at once which can be useful to speed up host installation for example. As a specific example if the user desires to install several hosts which all have different passwords the user can define the following parameters in the configuration file 

The test will run three times and each time the required action will be run with the hostname and password relevant to the current iteration.

The input test file can include a report parameter with a default value of TRUE. If the report parameter is TRUE then the results reporter generates test results such as in a results.xml file. If the report parameter is FALSE then the results reporter does nothing for the particular test or generates results but does not report them. The results reporter can write events in a log file such as a start time and end time of the event a test status an iteration number such as if the event occurs in a loop as well as all parameters from the input test file associated with the event.

If the test function returns some value such as a value that will be used as input for further tests the user can store it using a fetch output parameter. The function can return additional values besides status in dictionary format. The user can specify the key name related to the desired output value and a parameter name for where to store the desired output value. Thus the format of this value should be the following 

The input test file can group tests into test sets. The input test file can group tests according to their functionality flows testing purpose permissions execution time dependencies on output from other tests and so forth. To define a custom group in input file a user can add a new test case before the tests that should be grouped as shown below 

The run parameter can instruct the test runner to run the whole group of tests in a loop or by certain condition. To mark where the test group is finished the user can add a new test case after the last test of the group as shown below 

The user can insert a START GROUP cell and an END GROUP cell that define the boundaries of the group so that the test runner treats tests between the START GROUP cell and the END GROUP cell as a group. The test runner executes or causes to be executed test actions on the target test platform via a REST based engine such as an API a command line interface CLI or a software development kit SDK . The test runner can send all types of REST API requests such as GET PUT POST DELETE TRACE or CONNECT. The test runner can receive feedback regarding the progress success failure or other status of tests which a results reporter can output to a user. The results reporter can report results via active messages such as a pop up in a graphical user interface via email or text message via an entry in a log and so forth. The results reporter can generate nested reports containing results of sub tests. The results reporter can also generate a single report summarizing the results of the test actions for a particular product product category feature test suite unit test and so forth. The test runner can simultaneously execute multiple sets of test actions which can be for the same API engine or a set of different test interfaces.

Before executing the test actions indicated by the input test file the test runner can compile the input test file to validate the input test file or to check for errors. If the test runner encounters any errors the test runner can generate an error message for display to the user such as via a debugging console.

While the test runner can simultaneously execute test actions some test actions can depend on the output of other test actions . Thus the test runner can fetch output from one test action or group of test actions for use when starting another test action. The test runner can execute test actions in groups or test sets and can run test actions in loops and using conditional statements and branching.

In one embodiment the test runner builds Python data structures based on an XML schema document. The data structures represent the elements described by the XML schema document. The test runner can incorporate a parser that loads data in an XML document into data structures for use with testing and can transform data from the data structures back into an XML document. The test runner can map all test functions to a known action in a configuration file. Then the test runner can access the actions from the input test file .

Returning to the test composer illustrate various example user interfaces for the test composer . The test composer provides a user interface for simple and accurate creation and management of automated tests. Users can create edit and otherwise manage parts of automated testing suites via a graphical user interface instead of manually creating or editing an XML or other type of automated testing input file. The typical automated test platform accepts very specific input providing details of what types of tests to run what input data to use which commands to execute an order of commands and any other testing parameters. The input to the automated test platform may also specify which testing outputs to track for logging purposes. A test composer allows users who may be unfamiliar with the specific automated test platform language form or commands to create and manage automated tests in a more familiar graphical environment and without knowledge of the language used to control the test platform such as XML. Further the graphical environment can provide suggestions such as lists of available testing options so that users can easily access functionality of the test platform. A graphical user interface approach to managing automated testing allows for increased efficiency and accuracy of testing and makes automated testing accessible to a wider audience of users beyond those who are technically knowledgeable in the specific language and details of the test platform.

The example test composer can provide a graphical user interface in which the user provides keyboard and mouse input or a voice interface in which the user provides spoken input. The test composer can include a file I O module for retrieving and storing files such as other configuration or preferences files for the test composer. The test composer can use test templates to populate tests in the user interface using common settings or common lists of available options. The test composer can further include an XML formatting database that specifies XML formats for target test platforms. The test composer can also include a test automator interface for communicating with test platforms directly or for communicating with a test automator which performs tests on the test platform. The test automator interface can communicate with test platforms such as by sending commands to the test platform via a network connection by sending XML files to the test platform or by publishing commands for a test platform on a test feed. The test automator interface can communicate with different test platforms using different protocols or different communication channels.

After the file is opened and loaded into the interface the user can extend modify or update the file within the user interface provided with the test composer as explained below. The user can use the keyboard to enter information in the user interface for certain types of columns such as the parameters column can select from a set of available options such as via a pull down list or other contextual menu in the test action column or can select by checking a check box for columns having two states such as the positive column or the report column. After the test scenario is ready the user can save the file with Save button . The test composer can save the file in XML format or in some other intermediate file format. The XML file can then be passed as an input file to a testing framework for executing the tests on the target test platform.

In one example the first column of the test composer interface shows a test name Create Data Center NFS a test action addDataCenter and parameters name DataCenterTest storage type estorage type . An example XML representation of this information is provided below 

The test composer can provide for several different operations related to creating and updating test scenario files. For example the user can upload a template file from a testing framework folder or some other file. The user can export or save test composer content to an XML file. The test composer can prompt the user to save the file in a local or network based storage such as on a server in the provided location. The user can also insert a row or column to the test scenario depicted in the test composer . When adding a column for example the user can enter or edit a column header name. The test composer can update the format of the column and data in the column based on a column type indicated by the name or selected by the user. The user can duplicate one or more selected rows or columns or one or more individual cells. The user can move rows up and down to rearrange the order the tests will be performed. The user can delete rows or columns. The user can also select a maximum number of rows to display per page. The user can resize columns and rows. Many operations and interac tions which are associated with spreadsheets can be applied to the user interface of the test composer .

The user interface of the test composer can provide tips for assisting users to understand how to input data for controlling tests. For example when a user clicks on a column header the user interface can display a wiki or other resource which describes the meaning and possible syntax for data entered in fields under the column. As another example when a user selects a desired action and hovers the mouse cursor over it the user interface can display the full documentation supplied with this action if it exists. When hovering the cursor over a cell the user interface can display the full cell content which can be useful when the input text is very long and the cell length is too short to view its full content.

The test composer can provide auto complete for certain fields. For example in the run column a user can enter the first character or first few characters of a command such as i for if l for loop y for yes n for no . Then the system can display all possible values supported for this statement that start with that character and the user can select from one of the possible values.

The system parses the file based on the type to extract test parameters . The system can identify a plug in associated with at least one of the type of the file the set of tests or the target test platform and parse the file using the plug in. The system performs test actions on the target test platform based on the test parameters . The test actions can be performed via a REST API accessed via a command line interface or a software development kit interface. Then the system can receive results of the test actions from the target test platform and validate the results.

The system can use the same input file to perform test actions on multiple different platform types. A same input file can be adapted via various plug ins to cause tests to be performed on different platform types such as a computing device running a Linux operating system a smartphone or other device running Android and its custom Java virtual machine or a custom embedded platform based on an ARM CPU. The input file specifies high level test actions and parameters which the system converts to platform specific commands or groups of commands for each different platform type. For example the system can identify a second target test platform communicating via a different interface standard from the target test platform and adapt the test parameters for the different interface standard. Then the system can perform the test actions on the second target test platform based on the test parameters and the different interface standard. For example the test runner of can convert the input test file to different sets of test actions each set targeted to a different platform. Then the test runner can cause each set of test actions to be executed on the appropriate target platform via the respective API engine or other interface. For example instead of carrying out the test actions via the API engine the test runner can cause commands to be executed via remote shell commands or via some other network based command protocol. While the test actions indicated in the input test file are the same the actual test actions for each platform may be different based on the abilities and limitations of each target test platform.

In another embodiment the system receives an input file representing a set of tests to run on a target test platform wherein the input file is in a format that is not compatible with the target test platform. The input file can be in a format that is generic and not directly compatible with any target test platform but that is readable by the test runner . The test runner can read and extract data from platform specific input test files via the appropriate plug ins libraries or translation routines. The system identifies a type of the input file and parses the input file based on the type to extract test parameters. The system adapts the test parameters to an output file that is compatible with the target test platform and performs test actions on the target test platform using the output file and via a communication channel that is appropriate for the target test platform. The system can optionally receive a configuration file that is separate from the input file and extract additional test parameters from the configuration file. Then the system can adapt the additional test parameters to the output file.

The example computer system includes a processing device a main memory e.g. read only memory ROM flash memory dynamic random access memory DRAM such as synchronous DRAM SDRAM or Rambus DRAM RDRAM etc. a static memory e.g. flash memory static random access memory SRAM etc. and a secondary memory e.g. a data storage device which communicate with each other via a bus .

Processing device represents one or more general purpose processing devices such as a microprocessor central processing unit or the like. More particularly the processing device may be a complex instruction set computing CISC microprocessor reduced instruction set computing RISC microprocessor very long instruction word VLIW microprocessor processor implementing other instruction sets or pro cessors implementing a combination of instruction sets. Processing device may also be one or more special purpose processing devices such as an application specific integrated circuit ASIC a field programmable gate array FPGA a digital signal processor DSP network processor or the like. Processing device is configured to execute processing logic e.g. instructions for a test runner for performing the operations and steps discussed herein.

The computer system may further include a network interface device . The computer system also may include a video display unit e.g. a liquid crystal display LCD or a cathode ray tube CRT an alphanumeric input device e.g. a keyboard a cursor control device e.g. a mouse other user input device such as a touch screen or a microphone and a signal generation device e.g. a speaker .

The secondary memory may include a machine readable storage medium or more specifically a computer readable storage medium on which is stored one or more sets of instructions for the test runner embodying any one or more of the methodologies or functions described herein. The instructions may also reside completely or at least partially within the main memory or within the processing device during execution thereof by the computer system the main memory and the processing device also constituting machine readable storage media.

The computer readable storage medium may also be used to store a problem resolution manager which may correspond to the test runner of or a software library containing methods that call a test runner . While the computer readable storage medium is shown in an example embodiment to be a single medium the term computer readable storage medium should be taken to include a single medium or multiple media e.g. a centralized or distributed database or associated caches and servers that store the one or more sets of instructions. The term computer readable storage medium shall also be taken to include any medium that is capable of storing or encoding a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present invention. The term computer readable storage medium shall accordingly be taken to include but not be limited to solid state memories and optical and magnetic media.

It is to be understood that the above description is intended to be illustrative and not restrictive. Many other embodiments will be apparent to those of skill in the art upon reading and understanding the above description. Although the present invention has been described with reference to specific example embodiments it will be recognized that the invention is not limited to the embodiments described but can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense. The scope of the invention should therefore be determined with reference to the appended claims along with the full scope of equivalents to which such claims are entitled.

