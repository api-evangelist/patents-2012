---

title: Arithmetic logic unit architecture
abstract: Embodiments of the present invention include an apparatus, method, and system for acoustic modeling. In an embodiment, an arithmetic logic unit for computing a one-dimensional score between a feature vector and a Gaussian probability distribution vector is provided. The arithmetic logic unit includes a computational logic unit configured to compute a first value based on a mean value and a variance value associated with a dimension of the Gaussian probability distribution vector and a dimension of a feature vector, a look up table module configured to output a second value based on the variance value, and a combination module configured to combine the first value and the second value to generate the one-dimensional score.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08924453&OS=08924453&RS=08924453
owner: Spansion LLC
number: 08924453
owner_city: Sunnyvale
owner_country: US
publication_date: 20120606
---
This application claims the benefit of U.S. Provisional Patent Application No. 61 577 595 filed Dec. 19 2011 titled Senone Scoring Unit and U.S. Provisional Patent Application No. 61 589 113 filed Jan. 20 2012 titled HW SW Architecture for Speech Recognition both of which are incorporated herein by reference in their entireties.

Embodiments of the present invention generally relate to speech recognition. More particular embodiments of the present invention relate to the implementation of an acoustic modeling process on a dedicated processing unit.

Real time data pattern recognition is increasingly used to analyze data streams in electronic systems. On a vocabulary with over tens of thousands of words speech recognition systems have achieved improved accuracy making it an attractive feature for electronic systems. For example speech recognition systems are increasingly common in consumer markets targeted to data pattern recognition applications such as for example the mobile device server automobile and PC markets.

Despite the improved accuracy in speech recognition systems significant computing resources are dedicated to the speech recognition process in turn placing a significant load on computing systems such as for example multiuser multiprogramming environments. Multiprogramming computing systems concurrently process data from various applications and as a result the load placed on these computing systems by the speech recognition process affects the speed at which the computing systems can process incoming voice signals as well as data from other applications. Further for handheld devices that typically include limited memory resources as compared to desktop computing systems speech recognition applications not only place significant load on the handheld device s computing resources but also consume a significant portion of the handheld device s memory resources. The above speech recognition system issues of processing capability speed and memory resources are further exacerbated by the need to process incoming voice signals in real time or substantially close to real time.

Therefore there is a need to improve the load that speech recognition systems place on the processing capability speed and memory resources of computing systems.

In an embodiment an arithmetic logic unit for computing a one dimensional score between a feature vector and a Gaussian probability distribution vector is provided. The arithmetic logic unit includes a computational logic unit configured to compute a first value based on a mean value and a variance value associated with a dimension of the Gaussian probability distribution vector and a dimension of a feature vector a look up table module configured to output a second value based on the variance value and a combination module configured to combine the first value and the second value to generate the one dimensional score.

Another embodiment of the present invention includes a method of computing a one dimensional distance score between a feature vector and a Gaussian probability distribution vector. The method includes computing a first value based on a mean value and a variance value associated with a dimension of the Gaussian probability distribution vector and a dimension of a feature vector retrieving a second value based on the variance value from a look up table module and combining the first value and the second value to generate the one dimensional score.

Further features and advantages of embodiments of the invention as well as the structure and operation of various embodiments of the present invention are described in detail below with reference to the accompanying drawings. It is noted that the invention is not limited to the specific embodiments described herein. Such embodiments are presented herein for illustrative purposes only. Additional embodiments will be apparent to persons skilled in the relevant art based on the teachings contained herein.

The following detailed description refers to the accompanying drawings that illustrate exemplary embodiments consistent with this invention. Other embodiments are possible and modifications can be made to the embodiments within the spirit and scope of the invention. Therefore the detailed description is not meant to limit the scope of the invention. Rather the scope of the invention is defined by the appended claims.

It would be apparent to a person skilled in the relevant art that the present invention as described below can be implemented in many different embodiments of software hardware firmware and or the entities illustrated in the figures. Thus the operational behavior of embodiments of the present invention will be described with the understanding that modifications and variations of the embodiments are possible given the level of detail presented herein.

This specification discloses one or more embodiments that incorporate the features of this invention. The disclosed embodiments merely exemplify the invention. The scope of the invention is not limited to the disclosed embodiments. The invention is defined by the claims appended hereto.

The embodiments described and references in the specification to one embodiment an embodiment an example embodiment etc. indicate that the embodiments described may include a particular feature structure or characteristic but every embodiment may not necessarily include the particular feature structure or characteristic. Moreover such phrases are not necessarily referring to the same embodiment. Further when a particular feature structure or characteristic is described in connection with an embodiment it is understood that it is within the knowledge of one skilled in the art to effect such feature structure or characteristic in connection with other embodiments whether or not explicitly described.

In signal processing stage an analog signal representation of an incoming voice signal can be filtered to eliminate high frequency components of the signal that lie outside the range of frequencies that the human ear can hear. The filtered signal is then digitized using sampling and quantization techniques well known to a person skilled in the relevant art. One or more parametric digital representations also referred to herein as feature vectors can be extracted from the digitized waveform using techniques such as for example linear predictive coding and fast fourier transforms. This extraction can occur at regular time intervals or frames of approximately 10 ms for example.

In acoustic modeling stage feature vectors from signal processing stage are compared to one or more multivariate Gaussian probability distributions also referred to herein as Gaussian probability distributions stored in memory. The one or more Gaussian probability distributions stored in memory can be part of an acoustic library in which the Gaussian probability distributions represent senones. A senone refers to a sub phonetic unit for a language of interest as would be understood by a person skilled in the relevant art. An individual senone can be made up of for example 8 components in which each of the components can represent a 39 dimension Gaussian probability distribution.

Acoustic modeling stage can process over 1000 senones for example. As a result the comparison of feature vectors to the one or more Gaussian probability distributions can be a computationally intensive task as thousands of Gaussian probability distributions for example can be compared to feature vectors every time interval or frame e.g. 10 ms . A set of scores for each of the senones represented in the acoustic library also referred to herein as senone scores results from the comparison of each of feature vectors to each of the one or more Gaussian probability distributions. Acoustic modeling stage provides senone scores to phoneme evaluation stage .

In phoneme evaluation stage Hidden Markov Models HMMs can be used to characterize a phoneme as a set of states and an a priori set of transition probabilities between each of the states where a state is associated with a senone. For a given observed sequence of senones there is a most likely sequence of states in a corresponding HMM. This corresponding HMM can be associated with an observed phoneme. A Viterbi algorithm can be used to find the likelihood of each HMM corresponding to a phoneme.

The Viterbi algorithm performs a computation that starts with a first frame and then proceeds to subsequent frames one at a time in a time synchronous manner. A probability score is computed for each senone in the HMMs being considered. Therefore a cumulative probability score can be successively computed for each of the possible senone sequences as the Viterbi algorithm analyzes sequential frames. Phoneme evaluation stage provides the phoneme likelihoods or probabilities also referred to herein as a phoneme score to word modeling stage .

In word modeling stage searching techniques are used to determine a most likely string of phonemes and subsequent words over time. Searching techniques such as for example tree based algorithms can be used to determine the most likely string of phonemes.

Input device is configured to receive an incoming voice signal e.g. incoming voice signal of and convert acoustical vibrations associated with the incoming voice signal to an analog signal. The analog signal is digitized using an analog to digital converter not shown in and the resulting digital signal is transferred to processing unit over data bus . Input device can be for example a microphone.

Processing unit is configured to process the digital input signal in accordance with the signal processing stage acoustic modeling stage phoneme evaluation stage and word modeler stage described above with respect to . is an illustration of speech recognition system with speech recognition modules performed by processing unit . Processing unit includes signal processing module acoustic modeling module phoneme evaluation module and word modeling module which operate in a similar manner as signal processing stage acoustic modeling stage phoneme evaluation stage and word modeler stage of respectively.

In reference to signal processing module can convert a digital input signal representation of incoming voice signal e.g. from input device into one or more feature vectors . Acoustic modeling module compares one or more feature vectors to one or more Gaussian probability distributions stored in an acoustic library in memory device . That is for each of the comparisons of one or more feature vectors to the one or more Gaussian probability distributions processing unit accesses memory device via data bus . For an acoustic library with thousands of senones in which each of the senones is composed of a plurality of Gaussian probability distributions not only are the comparisons performed by acoustic modeling module computationally intensive but the thousands of accesses to memory device via data bus by acoustic modeling module are also computationally intensive and time consuming. The thousands of accesses to memory device is further exacerbated by the bus width of data bus e.g. typically 8 to 32 bits in which multiple accesses to memory device may be required by acoustic modeling module to access each Gaussian probability distribution. Further interconnect parasitics associated with data bus may corrupt data transfer between memory device and acoustic modeling module .

Phoneme evaluation module receives senone scores from acoustic modeling module . As discussed above with respect to speech recognition process of HMMs can be used to characterize a phoneme as a set of states and an a priori set of transition probabilities between each of the states where a state is composed of a sequence of senones. The sets of states and a priori sets of transition probabilities used by phoneme evaluation module can be stored in memory device . Phoneme evaluation module provides phoneme scores to word modeling module .

Word modeling module uses searching techniques such as for example tree based algorithms to determine a most likely string of phonemes e.g. most likely phoneme and subsequent words over time.

An issue with conventional speech recognition system of among others is the significant load on processing unit due to the acoustic modeling process. For example for each comparison of one or more feature vectors to the one or more Gaussian probability distributions stored in memory device memory device is accessed by processing unit . As a result significant computing resources are dedicated to the acoustic modeling process in turn placing a significant load on processing unit . The load placed on processing unit by the acoustic modeling process affects the speed at which processing unit can process digital signals from input device as well as data from other applications e.g. where processing unit can operate in a multiuser multiprogramming environment that concurrently processes data from a plurality of applications . Further for computing systems with limited memory resources e.g. handheld devices the acoustic modeling process not only places a significant load on processing unit but also consumes a significant portion of memory device and bandwidth of data bus . These issues among others with processing capabilities speed and memory resources are further exacerbated by the need to process incoming voice signals in real time or substantially close to real time in many applications.

Embodiments of the present invention address the issues discussed above with respect to conventional speech recognition systems and of respectively. In an embodiment the acoustic modeling process is performed by a dedicated processing unit also referred to herein as an Acoustic Processing Unit or APU . The APU operates in conjunction with processing unit of also referred to herein as a Central Processing Unit or CPU . For example the APU receives one or more feature vectors e.g. feature vectors of from the CPU calculates a senone score e.g. senone score of based on one or more Gaussian probability distributions and outputs the senone score to the CPU. In an embodiment the one or more Gaussian probability distributions can be stored in the APU. Alternatively in another embodiment the one or more Gaussian probability distributions can be stored externally to the APU in which the APU receives the one or more Gaussian probability distributions from an external memory device. Based on the architecture of the APU which is described in further detail below an accelerated calculation for the senone score is achieved.

Although portions of the present disclosure is described in the context of a speech recognition system a person skilled in the relevant art will recognize that the embodiments described herein are applicable to any data pattern recognition applications based on the description herein. These other data pattern recognition applications include but are not limited to image processing audio processing and handwriting recognition. These other data pattern recognition applications are within the spirit and scope of the embodiments disclosed herein.

In reference to the embodiment of feedback is an optional feature of speech recognition process in which phoneme evaluation process can provide an active senone list to acoustic modeling process according to an embodiment of the present invention. The APU can compare one or more feature vectors to one or more senones indicated in the active senone list. Such feedback is further discussed below.

In another embodiment acoustic modeling process can compare the one or more feature vectors to all of the senones associated with an acoustic library. In this case feedback is not required as phoneme evaluation process receives an entire set of senone scores e.g. score all function from the APU for further processing.

In an embodiment the APU and CPU can be in communication with one another over a Serial Peripheral Interface SPI bus a Peripheral Controller Interface PCI bus an Application Programming Interface API bus an Advanced Microcontroller Bus Architecture High Performance Bus AHB an Advanced Peripheral Bus APB a memory bus or any other type of bus. Example non limiting embodiments of system bus architectures for speech recognition process of are described in further detail below.

As illustrated in APU is communicatively coupled to I O bus through PCI bridge . I O bus can be for example a PCI bus. Through PCI bridge and I O bus APU is communicatively coupled to system controller and CPU . In another embodiment not illustrated in APU can be directly coupled to processor memory bus and in turn communicatively coupled to CPU .

As illustrated in APU is communicatively coupled to system controller through APB bridge and APB . System controller is also communicatively coupled to CPU through AHB . In turn system controller is communicatively coupled to CPU through AHB .

As illustrated in APU and main memory are communicatively coupled to LPDDR interface via LPDDR memory bus . APU is also communicatively coupled to system controller through LPDDR memory bus and LPDDR interface . In turn system controller is communicatively coupled to CPU via AHB .

Non volatile memory device can store an acoustic library to be used in a speech recognition process in which the acoustic library can include over 1000 senones according to an embodiment of the present invention. In an embodiment when a senone request is received by speech recognition system memory controller copies the acoustic library from non volatile memory device to volatile memory device via bus . The acoustic library transfer process between the non volatile and volatile memory devices can be implemented using for example a direct memory access DMA operation.

In an embodiment speech recognition system can be powered on in anticipation of a senone scoring request. After power up the acoustic library from non volatile memory device is immediately copied to volatile memory device . Once volatile memory device has received the acoustic library APU is ready to begin processing senone scoring requests e.g. acoustic modeling process of using the acoustic library stored in volatile memory device .

When the senone scoring request is received by APU a selected senone from the acoustic library is copied from volatile memory device to APU via memory controller . APU calculates a senone score based on the selected senone and a data stream received by APU e.g. one or more feature vectors of . After completing the calculation APU transfers the senone score to the requesting system e.g. the CPU .

In an embodiment after a predetermined time of inactivity e.g. senone scoring inactivity by APU volatile memory device can be powered down. As a result power efficiency in speech recognition system can be improved as a periodic refresh of memory cells in volatile memory device will not be required. Here the acoustic library is still stored in non volatile memory device such that the acoustic library can be retained when volatile memory device is powered down. As would be understood by a person skilled in the art when volatile memory device is powered down the contents stored therein e.g. the acoustic library will be lost. In an embodiment when volatile memory device is powered down the other components of speech recognition system can be powered down as well.

In step a plurality of data patterns is copied from a non volatile memory device e.g. non volatile memory device of to a volatile memory device e.g. volatile memory device of . In an embodiment the plurality of data patterns can be one or more senones associated with an acoustic library.

In step a data pattern from the volatile memory device is requested by a computational unit e.g. APU of and transferred to the computational unit via a memory controller and bus e.g. memory controller and bus respectively of . In an embodiment the requested data pattern is a senone from an acoustic library stored in the volatile memory device.

In step after receiving the requested data pattern the computational unit e.g. APU of performs a data pattern analysis on a data stream received by the computational unit. In an embodiment the data pattern analysis is a senone score calculation based on a selected senone and the data stream received by the computational unit e.g. one or more feature vectors of . After completing the data pattern analysis the computational unit transfers the data pattern analysis result to the requesting system e.g. the CPU .

In step the volatile memory device powers down. In an embodiment the volatile memory device powers down after a predetermined time of inactivity e.g. inactivity in the data pattern analysis by the computational unit . As a result power efficiency can be improved as a periodic refresh of memory cells in the volatile memory device will not be required. In an embodiment when the volatile memory device is powered down the other components of the system e.g. other components of speech recognition system can be powered down as well.

Non volatile memory device can be configured to store an acoustic model that is copied to volatile memory device upon power up of APU according to an embodiment of the present invention. In an embodiment non volatile memory device can be a Flash memory device and volatile memory device can be a DRAM device. Further ASIC can be configured to perform an acoustic modeling process e.g. acoustic modeling process of according to an embodiment of the present invention.

Memory device can be configured to store an acoustic library and to transfer one or more senones to ASIC via an I O bus according to an embodiment of the present invention. In an embodiment memory device can be a DRAM device or a Flash memory device. In another embodiment the acoustic library can be stored in a memory device located within ASIC not shown in rather than memory device . In yet another embodiment the acoustic library can be stored in system memory for SOC e.g. DRAM device .

APU is in communication with a CPU not shown in via I O signals in which APU is configured to perform an acoustic modeling process e.g. acoustic modeling process of according to an embodiment of the present invention. In an embodiment I O signals can include an input feature vector data line for feature vector information an input clock signal an input APU enable signal an output senone score data line for senone score information and other I O control signals for APU . APU can be configured to receive one or more feature vectors calculated by the CPU via the feature vector data line from the CPU and to transmit a senone score via the senone score data line to the CPU for further processing according to an embodiment of the present invention. In an embodiment I O signals can be implemented as for example an SPI bus a PCI bus an API bus an AHB an APB a memory bus or any other type of bus to provide a communication path between APU and the CPU see e.g. and associated description . An interface between APU and the CPU as well as control signals for the interface are described in further detail below.

In an embodiment memory module and SSU can operate in two different clock domains. Memory module can operate at the clock frequency associated with the input clock signal to APU e.g. from I O signals and SSU can operate at a faster clock frequency based on the input clock signal according to an embodiment of the present invention. For example if the clock frequency associated with the input clock signal is 12 MHz then SSU can operate at a clock divided frequency of 60 MHz five times faster than the clock frequency associated with the input clock signal. Techniques and methods for implementing clock dividers are known to a person skilled in the relevant art. As will be described in further detail below the architecture of SSU can be based on the clock domain at which it operates.

In reference to memory module includes a bus controller a memory controller a memory device and a bridge controller . Memory device is configured to store an acoustic model to be used in a speech recognition process. In an embodiment memory device can be a non volatile memory device such as for example a Flash memory device. The acoustic library can be pre loaded into the non volatile memory device prior to operation of APU e.g. during manufacturing and or testing of APU .

In another embodiment memory device can be a volatile memory device such as for example a DRAM device. In an embodiment when a senone request is received by APU memory controller can copy the acoustic library from a non volatile memory device either integrated on the same chip as APU or located external to APU to the volatile memory device. The acoustic library transfer process between the non volatile and volatile memory devices can be implemented using for example a DMA operation.

Bus controller is configured to control data transfer between APU and an external CPU. In an embodiment bus controller can control the receipt of feature vectors from the CPU and the transmission of senone scores from APU to the CPU. In an embodiment bus controller is configured to transfer one or more feature vectors from the CPU to bridge controller which serves as an interface between memory module and SSU . In turn bridge controller transfers the one or more feature vectors to SSU for further processing. Upon calculation of a senone score the senone score is transferred from SSU to memory module via bridge controller according to an embodiment of the present invention.

In an embodiment bus controller can receive a control signal via I O signals that provides an active senone list. In an embodiment the active senone list can be transferred to APU as a result of the phoneme evaluation process performed by the CPU e.g. phoneme evaluation process of . That is in an embodiment a feedback process can occur between the acoustic modeling process performed by APU and the phoneme evaluation process performed by the CPU e.g. feedback of . The active senone list can be used in senone score calculations for incoming feature vectors into APU according to an embodiment of the present invention.

The active senone list indicates one or more senones stored in memory device to be used in a senone score calculation. In an embodiment the active senone list can include a base address associated with an address space of memory device and a list of indices related to the base address at which the one or more senones are located in memory device . Bus controller can send the active senone list to SSU via bridge controller in which SSU is in communication with memory device via memory controller to access the one or more senones associated with the active senone list.

In another embodiment bus controller can receive a control signal via I O signals that instructs APU to perform the senone score calculation using all of the senones contained in the acoustic library e.g. score all function . Bus controller sends the score all instruction to SSU via bridge controller in which SSU is in communications with memory device via memory controller to access all of the senones associated with the acoustic library.

Conventional speech recognition systems typically incorporate a feedback loop between acoustic modeling and phoneme evaluation modules e.g. acoustic modeling module and phoneme evaluation module of within the CPU to limit the number of senones used in senone score calculations. This is because as discussed above with respect to speech recognition system of significant computing resources are dedicated to the acoustic modeling process where thousands of senones can be compared to a feature vector. This places a significant load on the CPU and the bandwidth of the data bus e.g. data of transferring the senones from the memory device e.g. memory device of to the CPU. Thus for conventional speech recognition systems active senone lists are used to limit the impact of the acoustic modeling process on the CPU. However the use active senone lists by the CPU can place limitations on the need to process incoming voice signals in real time or substantially close to real time.

The score all function of APU not only alleviates the load on the CPU and the bandwidth of the data bus but also provides processing of incoming voice signals in real time or substantially close to real time. As discussed in further detail below features of APU such as for example the bus width of data bus and the architecture of distance calculator of provides a system for real time or substantially close to real time speech recognition.

In reference to SSU includes an output buffer an SSU control module a feature vector matrix module a distance calculator and an addition module . SSU is configured to calculate a Mahalanobis distance between one or more feature vectors and one or more senones stored in memory device according to an embodiment of the present invention. Each of the one or more feature vectors can be composed of N dimensions where N can equal for example 39. In an embodiment each of the N dimensions in the one or more feature vectors can be a 16 bit mean value.

Further each of the one or more senones stored in memory device is composed of one or more Gaussian probability distributions where each of the one or more Gaussian probability distributions has the same number of dimensions as each of the one or more feature vectors e.g. N dimensions . Each of the one or more senones stored in memory device can have for example 32 Gaussian probability distributions.

As discussed above memory module and SSU can operate in two different clock domains. In an embodiment SSU control module is configured to receive a clock signal from memory module via bridge controller . The frequency of the clock signal received by SSU control module can be the same or substantially the same as the clock frequency associated with the input clock signal to APU e.g. input clock signal from I O signals according to an embodiment of the present invention.

In an embodiment SSU control module can divide the frequency of its incoming clock signal and distribute that divided clock signal to other components of SSU e.g. output buffer feature vector matrix module distance calculator and addition module such that these other components operate at the clock divided frequency. For example if the clock frequency associated with the input clock signal e.g. from I O signals is 12 MHz then SSU control module can receive the same or substantially the same clock signal from bridge controller and divide that clock frequency using known clock dividing techniques and methods to a frequency of for example 60 MHz. SSU control module can distribute this clock divided signal to the other components of SSU such that these other components operate at for example 60 MHz five times faster than the clock frequency associated with the input clock signal.

For simplicity purposes the clock signals distributed from SSU control module to the other components of SSU are not illustrated in . For ease of reference the frequency associated with this clock signal is also referred to herein as the SSU clock frequency. Further for ease of reference the frequency associated with the input clock signal to SSU control module is also referred to herein as the memory module clock frequency. 

In reference to in an embodiment when bus controller receives an active senone list via I O signals the base address associated with the address space of memory device and list of indices related to the base address at which the one or more senones are located in memory device can be stored in input buffer of . Control unit is in communication with input buffer to monitor the list of the senones to be applied by distance calculator of in the senone score calculation.

For example the active senone list can contain a base address associated with an address space of memory device and 100 indices pointing to 100 senones stored in memory device . As would be understood by a person skilled in the relevant art the indices can refer to pointers or memory address offsets in reference to the base address associated with the address space of memory device . Further as discussed above a senone can be composed of one or more Gaussian probability distributions where each of the one or more Gaussian probability distributions has the same number of dimensions as each of one or more feature vectors e.g. N dimensions received by APU . For explanation purposes this example will assume that each senone stored in memory device is composed of 32 Gaussian probability distributions. Based on the description herein a person skilled in the relevant art will understand that each of the senones can be composed of more or less than 32 Gaussian probability distributions.

In an embodiment for the first senone in the active senone list control unit communicates with memory controller of to access the first senone in memory device based on the base address and the first index information contained in the active senone list. The senone associated with the first index can include memory address information of the first 2 Gaussian probability distributions associated with that senone according to an embodiment of the present invention. In turn memory device accesses two Gaussian probability distributions associated with the first senone in for example a sequential manner. For example memory device accesses the first Gaussian probability distribution and outputs this Gaussian probability distribution to distance calculator via data bus . As memory device outputs the first Gaussian probability distribution memory device can also access the second Gaussian probability distribution.

In an embodiment the second Gaussian probability distribution can include memory address information for a third Gaussian probability distribution to be accessed by memory device . Memory device can communicate this memory address information to control unit of via bridge controller of . Control unit in turn communicates with memory controller of to access the third Gaussian probability distribution. In an embodiment as the third Gaussian probability distribution is being accessed by memory device the second Gaussian probability distribution can be outputted to distance calculator via data bus . This iterative overlapping process of accessing a subsequent Gaussian probability distribution while outputting a current Gaussian probability distribution is performed for all of the Gaussian probability distributions associated with the senone e.g. for all of the 32 Gaussian probability distributions associated with the senone . A benefit among others of the iterative overlapping or parallel processing is faster performance in senone score calculations.

Control unit of monitors the transfer process of Gaussian probability distributions from memory device to distance calculator such that the memory access and transfer process occurs in a pipeline manner according to an embodiment of the present invention. After the 32 Gaussian probability distributions associated with the first senone is outputted to distance calculator of control unit repeats the above process for the one or more remaining senones in the active senone list.

After the senones in the active senone list are used in the senone score calculations for a current feature vector memory module can receive a control signal via I O signals that indicates that the active senone list from the current feature vector is to be used in senone score calculations for a subsequent feature vector according to an embodiment of the present invention. Upon receipt of the control signal from memory module via bridge controller SSU control module uses the same active senone list from the current feature vector in the senone score calculations for the subsequent feature vector. In particular control unit of applies the same base address and list of indices related to the base address stored in input buffer to the subsequent feature vector. Control unit of monitors the transfer process of Gaussian probability distributions from memory device to distance calculator for the subsequent feature vector in a similar manner as described above with respect to the active senone list example.

In another embodiment memory module can receive a control signal via I O signals that indicates a score all operation. As discussed above the score all function refers to an operation where a feature vector is compared to all of the senones contained in an acoustic library stored in memory device . In an embodiment control unit of communicates with memory controller of to access a first senone in memory device . The first senone can be for example located at a beginning memory address associated with an address space of memory device . Similar to the active senone list example above the first senone in memory device can include memory address information of the first 2 Gaussian probability distributions associated with that senone according to an embodiment of the present invention. In turn memory device accesses two Gaussian probability distributions associated with the first senone in for example a sequential manner.

In an embodiment similar to the active senone list example above the second Gaussian probability distribution can include memory address information on a third Gaussian probability distribution to be accessed by memory device . Memory device can communicate this memory address information to control unit of via bridge controller of . Control unit in turn communicates with memory controller of to access the third Gaussian probability distribution. In an embodiment as the third Gaussian probability distribution is being accessed by memory device the second Gaussian probability distribution can be outputted to distance calculator via data bus . This iterative overlapping process of accessing a subsequent Gaussian probability distribution while outputting a current Gaussian probability distribution is performed for all of the Gaussian probability distributions associated with the senone e.g. for all of the 32 Gaussian probability distributions associated with the senone .

Control unit of monitors the transfer process of Gaussian probability distributions from memory device to distance calculator such that the memory access and transfer process occurs in a pipeline manner according to an embodiment of the present invention. After the Gaussian probability distributions associated with the first senone are outputted to distance calculator of control unit repeats the above process for the one or more remaining senones in the acoustic library.

In reference to feature vector matrix module is used for speaker adaptation in APU . In an embodiment feature vector matrix module receives a feature vector transform matrix FVTM from the CPU via I O signals . The FVTM can be loaded into feature vector matrix module periodically such as for example once per utterance. In an embodiment the FVTM can be stored in a Static Random Access Memory SRAM device located within feature vector matrix module .

Along with mean and variance values stored for each senone in memory device an index can also be stored for each senone in which the index points to a row in the FVTM according to an embodiment of the present invention. The number of rows in the FVTM can vary e.g. 10 50 or 100 rows and can be specific to a voice recognition system implementing APU . Each row in the FVTM can have an equal number of entries as the N number of dimensions for a feature vector e.g. 39 where each of the entries is a scaling factor that is multiplied to its corresponding feature vector dimension to produce a new feature vector according to an embodiment of the present invention. The selected row from the FVTM e.g. row of 39 scaling factors is transferred to distance calculator via data bus in which distance calculator performs the multiplication operation to generate the new feature vector as will be described in further detail below.

In an embodiment SSU control module provides a feature vector received from the CPU and an index associated with a senone to feature vector matrix module . The index indicates a particular row in the FVTM for scaling the feature vector. For example the FVTM can have 100 rows and the index can be equal to 10. Here for a feature vector with 39 dimensions the 10th row of the FVTM contains 39 scaling factors in which the row of scaling factors is transferred to distance calculator to generate the new feature vector.

In reference to distance calculator is configured to calculate a distance between one or more dimensions of a senone stored in memory device and a corresponding one or more dimensions of a feature vector. is an illustration of an embodiment of distance calculator . Distance calculator includes a datapath multiplexer MUX a feature vector buffer arithmetic logic units ALUs and an accumulator .

Datapath MUX is configured to receive a Gaussian probability distribution from memory device of via data bus . In an embodiment the width of data bus is equal to the number of bits associated with one Gaussian probability distribution. For example if one Gaussian probability distribution is 768 bits then the width of data bus is also 768 bits. Over a plurality of Gaussian probability distribution dimensions the 768 bits associated with the Gaussian probability distribution can be allocated to a 16 bit mean value a 16 bit variance value and other attributes per Gaussian probability distribution dimension. As discussed above the Gaussian probability distribution can have the same number of dimensions as a feature vector e.g. 39 dimensions. In another embodiment the width of data bus can be greater than 256 bits.

Further in an embodiment memory device and distance calculator can be integrated on the same chip where data bus is a wide bus of the width discussed above integrated on the chip to provide data transfer of the Gaussian probability distribution from memory device to distance calculator . In another embodiment memory device and distance calculator can be integrated on two separate chips where data bus is a wide bus of the width discussed above that is tightly coupled between the two chips such that degradation of data due to noise and interconnect parasitic effects are minimized. As will be discussed below a benefit of a wide data bus of the width discussed above among others is to increase performance of APU in the calculation of senone scores.

Datapath MUX is also configured to receive one or more control signals and a feature vector from SSU control module via data bus as well as feature vector scaling factors from feature vector buffer . In an embodiment feature vector buffer can be configured to store scaling factors associated with a selected row of the FVTM transferred from feature vector matrix module via data bus . In another embodiment feature vector buffer can be configured to store the FVTM. Here one or more control signals from SSU control module via data bus can be used to select the FVTM row. Datapath MUX outputs the feature vector selected feature vector scaling factors from the FVTM and Gaussian probability distribution information to ALUs via data bus for further processing.

In an embodiment datapath MUX is also configured to receive a Gaussian weighting factor from the one or more controls signals from SSU control module via data bus . Datapath MUX is configured to output the Gaussian weighting factor to accumulator for further processing.

In reference to each of ALUs is configured per SSU clock cycle to calculate a distance score between a dimension of a Gaussian probability distribution received from datapath MUX and a corresponding dimension of a feature vector according to an embodiment of the present invention. In an embodiment ALUs can operate at the SSU clock frequency e.g. 5 times faster than the memory module clock frequency such that for every read operation from memory device of e.g. to transfer a Gaussian probability distribution to distance calculator a distance score associated a Gaussian probability distribution also referred to herein as Gaussian distance score is outputted from distance calculator to addition module .

In an embodiment datapath MUX is configured to distribute feature vector information associated with one dimension a mean value associated with a corresponding dimension of a Gaussian probability distribution a variance value associated with the corresponding dimension of the Gaussian probability and feature vector scaling factors to each of ALU . Based on the feature vector information and the feature vector scaling factors allocated to a respective ALU each of ALUs is configured to generate a new feature vector by multiplying dimensions of the feature vector by respective scaling factors.

In an embodiment the multiplication of the feature vector dimensions by the corresponding scaling factors is performed on the fly meaning that the multiplication operation is performed during the calculation of the distance score. This is in contrast to the multiplication operation being performed for each of the rows in a FVTM and the results of the multiplication operation being stored in memory to be later accessed by each of ALUs . A benefit of the on the fly multiplication operation among others is that memory storage is not required for the results of the multiplication operation associated with non indexed or non selected rows of the FVTM. This in turn results in a faster generation of the new feature vector since additional clock cycles are not required to store the feature vector scaling results associated with the non indexed rows in memory and also results in a smaller die size area for ALUs .

Based on the new feature vector the mean value and the variance value for a respective ALU each of ALUs is configured to calculate a distance score based on a feature vector dimension and a corresponding Gaussian probability distribution dimension per SSU clock cycle according to an embodiment of the present invention. Cumulatively in one clock cycle ALUs generate distance scores for 8 dimensions i.e. 1 dimension calculation per ALU . The architecture and operation of the ALU is described in further detail below.

The number of ALUs in distance calculator can be dependent on the SSU clock frequency and the memory module clock frequency discussed above such that distance calculator outputs a distance score for one Gaussian probability distribution for every read access to memory device according to an embodiment of the present invention. For example the memory module clock frequency can have an operating frequency of 12 MHz where memory device also operates at 12 MHz e.g. for a read access of approximately 83 ns . SSU can have an SSU clock frequency of for example 60 MHz to operate five times faster than the memory module cock frequency. With a feature vector of 39 dimensions and 8 ALUs a Gaussian distance score for one Gaussian probability distribution can be calculated in 5 SSU clock cycles or 1 memory module clock cycle. Therefore by design the 5 SSU clock cycles is a predetermined number of clock cycles that corresponds to 1 memory module clock cycle where as one Gaussian probability distribution is read from memory device at 1 memory module clock cycle a Gaussian distance score for another Gaussian probability distribution is calculated by accumulator .

In an embodiment a portion of ALUs can be activated on a rising edge of an SSU clock cycle while the remaining portion of ALUs can be activated on a falling edge of the SSU clock cycle. For example ALUs can be activated on the rising edge of the SSU clock cycle and ALUs can be activated on the falling edge of the SSU clock cycle. As a result of staggering the activation of ALUs the peak current and peak power generated by distance calculator can be minimized thus decreasing the susceptibility of reliability issues in distance calculator .

Based on the description herein a person skilled in the relevant art will recognize that the architecture of distance calculator is not limited to the above example. Rather as would be understood by a person skilled in the relevant art distance calculator can operate at a faster or slower clock frequency of 60 MHz and that distance calculator can include more or less than 8 ALUs.

In reference to accumulator is configured to receive the outputs from each of ALUs and the Gaussian weighting factor from datapath MUX via data bus . As discussed above in an embodiment for every SSU clock cycle a distance score for a Gaussian probability distribution dimension is outputted by each of ALUs . These distance scores from each of ALUs are stored and accumulated by accumulator to generate a distance score for the Gaussian probability distribution dimension or Gaussian distance score e.g. accumulator adds respective distance scores calculated by ALUs per SSU clock cycle.

After the Gaussian distance scores associated with all of the Gaussian probability distribution dimensions are accumulated in accumulator e.g. 39 dimensions accumulator multiplies the total sum by the Gaussian weighting factor to generate a weighted Gaussian distance score. In an embodiment the Gaussian weighting factor is optional where accumulator outputs the Gaussian distance score. In another embodiment the Gaussian weighting factor is specific to each Gaussian and is stored in memory device .

Addition module is configured to add one or more Gaussian distance scores or weighted Gaussian distance scores to generate a senone score. As discussed above each senone can be composed of one or more Gaussian probability distributions in which each Gaussian probability distribution can be associated with a Gaussian distance score. For a senone with a plurality of Gaussian probability distributions e.g. 32 Gaussian probability distributions addition module sums the Gaussian distance scores associated with all of the Gaussian probability distributions to generate the senone score. In an embodiment addition module is configured to perform the summation operation in the log domain to generate the senone score.

Output buffer is configured to receive a senone score from addition module and transfer the senone score to bridge controller . Bridge controller in turn transfers the senone score to the external CPU via bus controller . In an embodiment output buffer can include a plurality of memory buffers such that as a first senone score in a first memory buffer is being transferred to bridge controller a second senone score generated by addition module can be transferred to a second memory buffer for a subsequent transfer to bridge controller .

In step a plurality of Gaussian probability distributions is received via a data bus having a width of at least one Gaussian probability distribution and a feature vector from an external computing device. The Gaussian probability distribution can be composed of for example 768 bits where the width of the data bus is at least 768 bits. Further APU of can receive the feature vector from the external computing device e.g. a CPU in communication with APU via I O signals of .

In an embodiment information associated with a plurality of dimensions of the feature vector a plurality of mean values associated with the corresponding plurality of dimensions of the at least one Gaussian probability distribution and a plurality of variance values associated with the corresponding plurality of dimensions of the at least one Gaussian probability distribution are distributed to for example arithmetic logic units e.g. ALUs of .

In step a plurality of dimension distance scores is calculated based on a plurality of dimensions of the feature vector and a corresponding plurality of dimensions of the at least one Gaussian probability distribution. In an embodiment the distance score calculations are based on at least one senone from an active senone list. The active senone list can include a base address associated with an address space of a memory device and one or more indices related to the base address at which the at least one senone is located in the memory device. Further a plurality of scaling factors for the plurality of dimensions of the feature vector are stored where the plurality of scaling factors are applied to the plurality of dimensions of the feature vector during the calculation of the plurality of dimension distance scores. Step can be performed by for example distance calculator of .

In step the plurality of dimension distance scores are summed to generate a Gaussian distance score for the at least one Gaussian probability distribution. In an embodiment the Gaussian distance score is generated over a predetermined number of senone scoring unit SSU clock cycles. The predetermined number of SSU clock cycles can equate to a read access time of the at least one Gaussian probability distribution from a memory device. Step can be performed by for example distance calculator of .

In step a plurality of Gaussian distance scores corresponding to the plurality of Gaussian probability distributions is summed to generate a senone score. Step can be performed by for example distance calculator of .

Embodiments of the present invention address and solve the issues discussed above with respect to conventional speech recognition system of . In summary the acoustic modeling process is performed by for example APU of . The APU operates in conjunction with a CPU in which the APU can receive one or more feature vectors e.g. feature vectors of from the CPU calculate a senone score e.g. senone score of based on one or more Gaussian probability distributions and output the senone score to the CPU. In an embodiment the one or more Gaussian probability distributions can be stored in the APU. Alternatively in another embodiment the one or more Gaussian probability distributions can be stored externally to the APU in which the APU receives the one or more Gaussian probability distributions from an external memory device. Based on embodiments of the APU architecture described above an accelerated calculation for the senone score is achieved.

Thus in an embodiment for a given dimension and a given Gaussian probability distribution the one dimensional distance score output by ALU is dependent on three variables x and var. One technique for implementing this equation in software is to generate a look up table LUT that is indexed with these three variables. Moreover because the score does not specifically depend on the values of xand but rather the difference between them this LUT can be further simplified into a two dimensional LUT indexed by the and var. Thus a two dimensional LUT could be used to implement ALUs .

A two dimensional LUT however could have substantial drawbacks if used to implement ALUs in the hardware implementation of . In particular for example because there are eight ALUs that each compute a respective one dimensional distance score there would have to be eight copies of this two dimensional LUT. In one embodiment such a two dimensional LUT is approximately 32 Kbytes although other embodiments and applications may require larger LUTs. Thus in such an embodiment eight copies of a 32 Kbyte LUT would be needed. If implemented in such a manner a large amount of the total board space for the SSU would be allocated to only the eight two dimensional LUTs. This problem would be exacerbated if larger LUTs were required or desired.

In an embodiment ALU overcomes this drawback of two dimensional LUTs by implementing a scoring function using a combination of computational logic and a one dimensional LUT. Importantly Equation 1 can be split into two parts an alupart and a LUTpart with each specified below.

Thus ALU computes aluand in parallel with the computing retrieves LUT. The aluand LUTare then combined to form the distance score. In particular as shown in ALU includes a computational logic unit and a LUT module . As described in further detail below computational logic unit can compute value aluand LUT module can be used to retrieve value LUT. Moreover ALU additionally includes a combination module . Combination module combines the outputs of computational unit and LUT module and outputs the distance score.

Computational logic unit and LUT module only receive the inputs that are needed to determine their respective value. Specifically as described above aludepends on three variables x and var. Thus as shown in computational logic unit receives these three values as inputs. Moreover the values retrieved from LUT module are indexed using value varalone. Thus as shown in LUT module only receives value var.

As shown in LUT module includes a LUT and a formatting module . LUT stores values corresponding to LUT as expressed in Equation 3 and is indexed using var. The value retrieved from LUT is received by formatting module . Formatting module formats the output of LUT so that it can be effectively combined with the output of computational logic unit .

The outputs from computational unit and LUT module are received at combination module . Combination module includes an adder a shift module a rounding module and a saturation module . Adder computes the sum of the two received values and outputs the sum. Shift module is configured to remove the fractional portion of the sum output by adder . Rounding module is configured to round down the output of shift module . Saturation module is configured to receive the rounded sum and saturate the value to a specific number of bits. Thus the output of saturation module is a value having a specific number of bits that represents the one dimensional distance score.

Transform module includes a multiplier a scale bit module and a saturation module . As described above values of feature vector can be transformed by respective entries in a feature vector transform matrix to for example account for learned characteristics of a speaker. In an embodiment transform module can be configured to scale individual feature vector values xby corresponding transform values . Specifically multiplier computes a product of the feature vector value xand the corresponding transform value and outputs a value to scale bit module . Scale bit module shifts to the right and outputs the resulting integer to saturation module . Saturation module is similar to saturation module described with reference to saturates the received value to a specific number of bits. Thus the output of saturation module is a value that represents the scaled feature vector value.

Exception handling module and multiplexer are configured to address specific errors present in LUT . For example in an effort to save space the size of LUT can be reduced. This reduction in size can cause specific values of LUT to have an error. In such an embodiment exception handling module can recognize if the output of LUT will be one of those values and output the correct value. Put another way exception handling module can act as a LUT that includes an entry for each entry of LUT that may have an error due to size restrictions. Because LUT is indexed based on var exception handling module can recognize whether the output of LUT needs to be corrected based on the value of var.

In a further embodiment exception handling module can act as a two dimensional LUT that also receives . In such an embodiment exception handling module can output specific values of alu e.g. as opposed to the corresponding entry from LUT . Because the number of these possible errors in LUT is relatively small exception handling module does not occupy a significant amount of space as would other larger two dimensional LUTs. Furthermore by controlling multiplexer to output the output of exception handling module instead of the output of sign bit module exception handling module can ensure that the stored value for alurather than the value of alucalculated using the incorrect output of LUT is finally output to combination module .

Formatting module receives the product computed by multiplier . In an embodiment formatting module is configured to reduce the number of bits in the result. While not necessary this operation can save space and power by reducing the number of bits on the output.

Moreover the embodiment of shows subtraction module as including multiplexers and comparison module and a subtractor . In an embodiment squaring module may be configured to square specifically positive values. Thus the output of subtraction module in such an embodiment must be positive. To achieve this result the two operands i.e. the feature vector value optionally scaled with transform value and the mean value can be compared by comparison module . Comparison module then outputs a control signal to multiplexers and to ensure that the first operand into subtractor is at least as large as the than the second operand.

APU includes an acoustic model memory a first bus a memory buffer a second bus and a senone scoring unit . Acoustic model memory can be configured to store a plurality of senones that together form one or more acoustic models. First bus is a wide bus that is configured to allow acoustic model memory to output an entire Gaussian probability distribution vector to memory buffer . Senone scoring unit scores a senone score against a feature vector received from CPU . Senone scoring unit can be implemented as described above. For example senone scoring unit can be implemented as shown in . For more information on senone scoring unit see Section 4 above.

Memory buffer can hold a Gaussian probability distribution vector until senone scoring unit is ready to compute a Gaussian distance score for it. That is if senone scoring unit is scoring a feature vector received from CPU against a Gaussian probability distribution vector q memory buffer can hold the next Gaussian probability distribution vector to be scored i.e. vector q 1.

As shown in the inputs to APU include a reference to a specific senone senone and the feature vector. The senone input addresses the stored vector information corresponding to that particular senone in the acoustic model memory. The output of APU is the senone score which represents the probability that the referenced senone emits the feature vector in a given time frame. In an embodiment acoustic model memory utilizes a parallel read architecture and a very large internal bandwidth bus . The number of bits read in parallel is greater than 256 e.g. 768 bits wide sufficient to load an entire Gaussian probability distribution vector at once . The values read from the acoustic model memory are then latched into memory buffer using very large bandwidth bus . Both of the output from memory buffer and the observation vector information are input into senone scoring unit which performs the multiplications and additions required to compute the senone score. Bus over which memory buffer communicates with senone scoring unit is substantially similar to bus .

As noted above the senone score is computed by calculating the scores of the J Gaussian probability distribution vectors of dimension N and by then summing them together to get the total score. Some scoring algorithms however use only the most significant Gaussians in the calculation to increase the speed of the computation. When utilizing algorithms based on a partial set of Gaussians only those bits associated with the required Gaussians need to be transferred from the acoustic model memory to senone scoring unit . In other words the largest number of contiguous bits in memory which will always be required by senone scoring unit is equal to the number of bits used to store a single Gaussian probability distribution vector. The bandwidth requirements of the memory bus as well as the number of bits that need to be read in parallel with be minimized by transferring only those bits comprising a single Gaussian probability distribution vector in each transfer. Using this number of bits per transfer the power requirements of APU can be reduced and the transfer rate of the necessary data to senone scoring unit will be increased resulting in an improvement of the overall system performance. Put another way by reducing the number of bits per transfer the power requirements of APU can be reduced and the transfer rate of the necessary data to senone scoring unit can also be increased resulting in an improvement of the overall system performance.

As discussed above acoustic modeling is one of the major bottlenecks in many types of speech recognition system i.e. keyword recognition or large vocabulary continuous speech recognition . Because of the large number of comparisons and calculations high performance and or parallel microprocessors are commonly used and a high bandwidth bus between the memory storing the acoustic models and the processors is required. In the embodiment of the acoustic model memory can be incorporated into APU which is integrated into a single die with senone scoring unit with both of them connected using a wide high bandwidth internal buses and to improve the data transfer rate. However while increasing the number of bits per transfer does improve the data transfer rate it does not always improve the overall system performance.

The number of bits per transfer can also a function of the algorithms used for acoustic modeling. When scoring algorithms based on a partial set of Gaussians are used i.e. Gaussian Selection then the number of bits per transfer can be equal to the size of the Gaussian used by the algorithm. Fewer number of bits per transfer requires multiple cycles to transfer the data comprising the Gaussian while greater numbers of bits per transfer is inefficient due to data non locality.

In an embodiment an architecture is used for acoustic modeling hardware accelerators when scoring algorithms are used is at least partially based on a partial set of Gaussians i.e. Gaussian Selection . This optimized architecture can result in a significant improvement in the overall system performance compared to other architectures.

In an embodiment dedicated DRAM module is dedicated to senone scoring unit to for example store senones. Thus memory interface can couple senone scoring unit to dedicated DRAM .

SPI interface module can provide an interface to an SPI bus which in turn can couple hardware accelerator to a CPU. Memory interface couples senone scoring unit to dedicated DRAM module . In an embodiment a voice recognition system can be implemented in a cloud based solution in which the senone scoring and processing necessary for voice recognition is performed in a cloud based voice recognition application.

In software stack application communicates with voice recognition engine which in turn communicates with Generic DCA . In an embodiment voice recognition engine is coupled to the Generic DCA via a DCA API. Generic DCA can be coupled to LLD via a LLD API. LLD can be coupled to HAL via an HAL API. HAL is communicatively coupled to SPI Bus Controller which is communicatively coupled to SPI bus . APU is communicatively coupled to SPI bus and is communicatively coupled to the HAL via bus controller and SPI bus .

In an embodiment software stack provides a software interface between APU and application e.g. an application that employs voice recognition . In particular application and voice recognition engine can be hardware agnostic. That is the application and voice recognition engine can complete their respective operations without detailed knowledge about how the distance or senone scoring is taking place.

Generic DCA LLD layer and HAL layer include hardware specific API calls. In an embodiment the API calls of HAL depend on the type of controller to which it is connected. In an embodiment the bus interface for APU can be a different bus and controller combination requiring a different HAL with different API calls .

Generic DCA is a distance computational API. The DCA can be defined by a software developer. In an embodiment the DCA API is specifically defined to support a voice recognition engine such as voice recognition engine . Also Generic DCA can be implemented specifically for APU . Moreover LLD can be a functional abstraction of the senone scoring unit commands and can be a one to one mapping to the senone scoring unit commands. As shown in low level driver is coupled to HAL .

The DCA API can include the following five functions Create Close Set Feature Compute Distance Score and Fill Scores. In an embodiment the Create function specifies which acoustic model is to be used. There can be one or more acoustic models stored in memory e.g. one or more acoustic models for each language . For example as discussed above with reference to dedicated acoustic model memory of APU can store the acoustic model e.g. senone library s . Moreover given an acoustic model e.g. a library of senones that stores the Gaussian distribution of the sound corresponding to the various senones and a feature vector the Create function can specify the number of dimensions in the feature vector. In an embodiment for English the feature vector can have 39 dimensions. In another embodiment for other languages the feature vector can have another number of dimensions. More generally the number of dimensions can vary depending on the specific spoken language selected for voice recognition processing. Thus the Create function specifies the acoustic model selected number of dimensions and number of senones. The Close function is a function that ends delivery of feature vectors audio sample portions and senone scoring requests to the hardware accelerator e.g. APU .

In an embodiment the Set Feature function is used to set the senone scoring requests into their respective frames by passing a specific frameID a passID and the feature vector. As noted above the input audio signal can be broken up into frames e.g. by voice recognition engine . An exemplary frame comprises spectral characteristics of a portion of the audio input signal. In an embodiment a frame can be 12 milliseconds ms long. The Set Feature function can convert each frame into 39 dimensions e.g. 39 8 bit values . The Set Feature function can specify a particular frame s ID and the associated feature vector.

In an embodiment the Distance Compute Score function calculates the senone score e.g. Gaussian probability which as noted above can be implemented as a distance calculation. This function can be used to begin and prepare the senone scoring. For example the feature vector can be input into APU and APU will score against all the senones stored in the acoustic model or at least a selected portion of the senones. This score will then be given back to the upper layer. In an embodiment the Distance Compute Score function can specify that a portion or the complete acoustic model will be used for the senone scoring.

In an embodiment the Fill Scores function takes the senone scoring result and returns it to the upper software layers including application and voice recognition engine .

In an embodiment voice recognition engine can be used for any form of pattern recognition e.g. pattern recognition forms that use a Hidden Markov model for pattern recognition. In another embodiment another form of pattern recognition also uses Gaussian calculations. Examples of pattern recognition can include but are not limited to the above described senone scoring for speech recognition image processing and handwritten recognition.

As noted above application and voice recognition engine are agnostic to any hardware used to determine the senone score. In an embodiment a particular APU can be swapped out for different hardware without application and voice recognition engine knowing or being effected. When application and voice recognition engine are agnostic to any type of hardware used for the senone scoring a first hardware accelerator can be replaced with a second hardware accelerator of a different design without requiring any redesign of application and voice recognition engine . In other words as discussed herein while the APU Library of calls are specific to the type and design of hardware accelerator used the Generic DCA Library calls are not hardware specific.

In an embodiment a software architecture as illustrated in can be described by describing a data and control flow through the software stack illustrated in . Application can be any application that uses the voice recognition engine. In an embodiment voice recognition engine is the Vocon Engine provided by Nuance Inc. In alternate embodiments other speech recognition engines or pattern recognition engines that make use of a Gaussian Mixture Model GMM for probability estimation may be used.

In an embodiment APU computes senone scores using the Gaussian Mixture Model. APU can compute these scores much faster e.g. by an order of magnitude than an embedded processor e.g. a cortex A8 embedded processor making speech recognition more practical in on board speech recognition systems with APU . Offloading the senone scoring or distance computation to APU not only improves the user experience by reducing the computational latency but also allows CPU to attend to other tasks in the system. The software architecture plays an important role in reducing the CPU load and the latency.

In an embodiment voice recognition engine is not directly aware of APU . For example voice recognition engine can use Generic DCA API to compute the distances also referred to as senone scores . The specific implementation of the Generic DCA library discussed here has been designed specifically to use APU with a plurality of function calls to the APU discussed below. This differs from a fully software implementation of the Generic DCA library. This specific implementation translates the Generic DCA library calls to a sequence of APU library calls. The details of the implementation are described below. The definition and implementation of the APU library is specific to the current implementation of the APU and is also described below.

In an embodiment Generic DCA operates as an interface layer between the voice recognition engine and APU . For example voice recognition engine can utilize generic API calls to the Generic DCA to request senone scoring. Generic DCA then utilizes an APU specific library of API calls described further below to direct the APU hardware accelerator to perform the requested senone scoring. Because voice recognition engine is not aware of APU voice recognition engine can take advantage of the following benefits. For example voice recognition engine may only need to know the message passing formats of APU . Voice recognition engine also does not need to know the tasks to be performed by APU . Moreover there is a swap out benefit. That is APU can be replaced or redesigned without requiring any redesign of voice recognition engine . Only the interface in this embodiment Generic DCA needs to have the hardware specific API calls to ensure the required interoperability between voice recognition engine and APU .

The distance computation setfeaturematrix function is called between utterances to adapt the recognition to the specific speaker. The APU uses this matrix when computing the senone scores for the next utterance.

In an embodiment distance computation computescores and distance computation fillscores can be implemented such that the computational latency and the CPU load are minimized. For example these functions can be implemented so as to achieve the concurrent operation embodied in .

In an embodiment the APU can be used for scoring the senones for each frame of a given utterance. The acoustic model of choice is communicated to the APU at the beginning as part of the function distance computation create. The feature vector for a given frame is passed to the APU via the function distance computation setfeature. The senones to be scored for a given frame are passed to the APU via the function distance computation computescores. The actual scores computed by the APU can be passed back to the Voice Recognition Engine engine via the function distance computation fillscores.

The control flows from top to bottom of stack illustrated in . All the functions are synchronous and they complete before returning except for the function distance computation computescores. As noted below the scoring can be implemented as a separate thread to maximize the concurrency of distance computation and the search as described above. This thread yields the CPU to the rest of voice recognition engine whenever it is waiting for APU to complete the distance computation. This asynchronous computation is important to minimize the latency as well as the CPU load.

In one embodiment a thread e.g. an executable process separate from a thread that is being executed by application or voice recognition engine can be created for APU . For there to be separate threads there must be no dependency that a further action of a first actor is dependent upon the actions of a second actor . Breaking any dependency between application and voice recognition engine and APU allows application and voice recognition engine to operate in parallel with APU . In one exemplary embodiment a dependency between application and voice recognition engine on one hand and APU on the other can be avoided through the use of frames e.g. lasting approximately 10 12 ms although the invention is not limited to this embodiment . For example while the application is using the senone score for frame n APU can be performing a senone score for frame n 1.

More specifically a voice recognition operation requires two discrete operations scoring and searching. As described above the scoring operation involves a comparison between Gaussian probability distribution vectors of a senone with the feature vector corresponding to a specific frame. In an embodiment software stack can be configured such that these two operations occur in parallel. In particular as shown in voice recognition engine can include search thread and distance thread . Distance thread can manage distance calculations completed on APU and search thread can use the results of the distance calculations to determine which sound was received e.g. by searching a library of senone scores to determine the best match . By setting distance thread to a higher priority than search thread distance thread can perform the operations needed to start the scoring operation on APU . The distance thread can then be put to sleep. While asleep search thread can be activated and can search using the results of the last distance operation. Because the length of time needed to complete a distance computation is relatively predictable distance thread can be put to sleep for a predetermined amount of time. In alternative embodiments distance thread can be put to sleep indefinitely and an interrupt from APU can instead be used to wake up distance thread . In doing so APU can be used to compute a distance score for a frame n 1 while CPU performs a searching operation using the previously calculated score for frame n.

For any given frame the search can follow the distance computation as illustrated in . In particular the distance computation for frame i 1 can be performed while the search for frame i is being conducted. Thus as shown in the distance computation performed by the APU can be performed concurrently with the search function performed by the CPU. In an embodiment a call sequence to the DCA library is arranged to effect this operation. In a further embodiment the Generic DCA is implemented so that the concurrency of the search computation and the distance computation is maximized. In an embodiment an implementation of the Generic DCA library uses the API proved by the APU library.

In step the received audio signal is divided into frames. For example in voice recognition engine can divide a received audio signal into frames that are for example 10 12 ms in length.

In step a search thread and a distance computation thread are created. For example in voice recognition engine can create search thread and distance thread .

In step a distance score is computed using an APU. For example in at the direction of distance thread senone scoring unit of APU can compute a distance score between a feature vector corresponding to a frame and a Gaussian probability distribution vector.

In step a search operation is performed using the computed score for the frame. For example in search thread can use the distance score computed in step to search different senones to determine which sound was included in the frame.

In step it is determined whether the frame was the last frame of the audio signal. If so method ends. If not method proceeds to step .

In step concurrently with the search operation of step a distance score for the next frame is computing using the APU. For example in APU can be used to compute a distance score for a frame i 1 concurrently with search thread performing a search operation using the distance score for frame i.

Various aspects of the present invention may be implemented in software firmware hardware or a combination thereof is an illustration of an example computer system in which embodiments of the present invention or portions thereof can be implemented as computer readable code. For example the method illustrated by flowchart of the method illustrated by flowchart of the method illustrated by flowchart of software stack illustrated in and or the method illustrated by flowchart of can be implemented in system . Various embodiments of the present invention are described in terms of this example computer system . After reading this description it will become apparent to a person skilled in the relevant art how to implement embodiments of the present invention using other computer systems and or computer architectures.

It should be noted that the simulation synthesis and or manufacture of various embodiments of this invention may be accomplished in part through the use of computer readable code including general programming languages such as C or C hardware description languages HDL such as for example Verilog HDL VHDL Altera HDL AHDL or other available programming and or schematic capture tools such as circuit capture tools . This computer readable code can be disposed in any known computer usable medium including a semiconductor magnetic disk optical disk such as CD ROM DVD ROM . As such the code can be transmitted over communication networks including the Internet. It is understood that the functions accomplished and or structure provided by the systems and techniques described above can be represented in a core e.g. an APU core that is embodied in program code and can be transformed to hardware as part of the production of integrated circuits.

Computer system includes one or more processors such as processor . Processor may be a special purpose or a general purpose processor such as for example the APU and CPU of respectively. Processor is connected to a communication infrastructure e.g. a bus or network .

Computer system also includes a main memory preferably random access memory RAM and may also include a secondary memory . Secondary memory can include for example a hard disk drive a removable storage drive and or a memory stick. Removable storage drive can include a floppy disk drive a magnetic tape drive an optical disk drive a flash memory or the like. The removable storage drive reads from and or writes to a removable storage unit in a well known manner. Removable storage unit can comprise a floppy disk magnetic tape optical disk etc. which is read by and written to by removable storage drive . As will be appreciated by persons skilled in the relevant art removable storage unit includes a computer usable storage medium having stored therein computer software and or data.

Computer system optionally includes a display interface which can include input and output devices such as keyboards mice etc. that forwards graphics text and other data from communication infrastructure or from a frame buffer not shown for display on display unit .

In alternative implementations secondary memory can include other similar devices for allowing computer programs or other instructions to be loaded into computer system . Such devices can include for example a removable storage unit and an interface . Examples of such devices can include a program cartridge and cartridge interface such as those found in video game devices a removable memory chip e.g. EPROM or PROM and associated socket and other removable storage units and interfaces which allow software and data to be transferred from the removable storage unit to computer system .

Computer system can also include a communications interface . Communications interface allows software and data to be transferred between computer system and external devices. Communications interface can include a modem a network interface such as an Ethernet card a communications port a PCMCIA slot and card or the like. Software and data transferred via communications interface are in the form of signals which may be electronic electromagnetic optical or other signals capable of being received by communications interface . These signals are provided to communications interface via a communications path . Communications path carries signals and can be implemented using wire or cable fiber optics a phone line a cellular phone link a RF link or other communications channels.

In this document the terms computer program medium and computer usable medium are used to generally refer to media such as removable storage unit removable storage unit and a hard disk installed in hard disk drive . Computer program medium and computer usable medium can also refer to memories such as main memory and secondary memory which can be memory semiconductors e.g. DRAMs etc. . These computer program products provide software to computer system .

Computer programs also called computer control logic are stored in main memory and or secondary memory . Computer programs may also be received via communications interface . Such computer programs when executed enable computer system to implement embodiments of the present invention as discussed herein. In particular the computer programs when executed enable processor to implement processes of embodiments of the present invention such as the steps in the method illustrated by flowchart of and flowchart of the method illustrated by flowchart of the method illustrated by flowchart of and or the functions in software stack illustrated in can be implemented in system discussed above. Accordingly such computer programs represent controllers of the computer system . Where embodiments of the present invention are implemented using software the software can be stored in a computer program product and loaded into computer system using removable storage drive interface hard drive or communications interface .

Embodiments of the present invention are also directed to computer program products including software stored on any computer usable medium. Such software when executed in one or more data processing device causes a data processing device s to operate as described herein. Embodiments of the present invention employ any computer usable or readable medium known now or in the future. Examples of computer usable mediums include but are not limited to primary storage devices e.g. any type of random access memory secondary storage devices e.g. hard drives floppy disks CD ROMS ZIP disks tapes magnetic storage devices optical storage devices MEMS nanotechnological storage devices etc. and communication mediums e.g. wired and wireless communications networks local area networks wide area networks intranets etc. .

It is to be appreciated that the Detailed Description section and not the Summary and Abstract sections is intended to be used to interpret the claims. The Summary and Abstract sections may set forth one or more but not all exemplary embodiments of the present invention as contemplated by the inventors and thus are not intended to limit the present invention and the appended claims in any way.

Embodiments of the present invention have been described above with the aid of functional building blocks illustrating the implementation of specified functions and relationships thereof The boundaries of these functional building blocks have been arbitrarily defined herein for the convenience of the description. Alternate boundaries can be defined so long as the specified functions and relationships thereof are appropriately performed.

The foregoing description of the specific embodiments will so fully reveal the general nature of the invention that others can by applying knowledge within the skill of the relevant art readily modify and or adapt for various applications such specific embodiments without undue experimentation without departing from the general concept of the present invention. Therefore such adaptations and modifications are intended to be within the meaning and range of equivalents of the disclosed embodiments based on the teaching and guidance presented herein. It is to be understood that the phraseology or terminology herein is for the purpose of description and not of limitation such that the terminology or phraseology of the present specification is to be interpreted by the skilled artisan in light of the teachings and guidance.

The breadth and scope of the present invention should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents.

