---

title: Cache data migration in a multicore processing system
abstract: A method of transferring data between two caches comprises sending a first message from a first processor to a second processor indicating that data is available for transfer from a first cache associated with the first processor, requesting, from the second processor, a data transfer of the data from the first cache to a second cache associated with the second processor, transferring the data from the first cache to the second cache in response to the request, and sending a second message from the second processor to the first processor indicating that the data transfer is complete.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09135172&OS=09135172&RS=09135172
owner: QUALCOMM Incorporated
number: 09135172
owner_city: San Diego
owner_country: US
publication_date: 20120802
---
This disclosure relates to techniques for cache data migration and more specifically to techniques and systems for transferring data between caches in a multicore processing system.

Heterogeneous multicore systems are processing systems that contain two or more types of processors. For example a heterogeneous multicore system may include two or more of a central processing unit CPU graphics processing unit GPU digital signal processor DSP or other types of processing cores. In such systems each processor may have its own local cache for storing frequently used data. In addition a given processor may have multiple parallel cores.

In some instances it is desirable to share data used by one type of processor in its local cache with another type of processor. In such instances a data handoff technique may be used to transfer the data. One such data handoff technique involves flushing the data from a local cache of one processor and transferring the data to a system memory. A second processor may then copy the data from system memory into its local cache.

As another example some techniques may utilize a cache coherency mechanism to handle data transfers between processing cores. A cache coherency mechanism keeps track of what processing core s are allowed to access certain data stored in a central system memory and to copy that data to their local cache. This mechanism ensures the integrity of data that is frequently used by many different processing cores. However such systems are often very complex and may involve undesirable latency when accessing data.

Both techniques described above require substantial performance and power resources. This leads to significant drawbacks for devices with limited power and performance potential e.g. wireless devices .

In general this disclosure describes techniques for cache data migration and more specifically to techniques and systems for transferring data between caches in a multicore processing system. In particular this disclosure proposes the use of handshaking signals between two or more heterogeneous processors so that a cache associated with a processor of a first type may be used as tightly coupled memory for a processor of a second type.

In one example of the disclosure a method of transferring data between two caches comprises initiating with a first message a transfer of data between a first cache associated with a first processor and a second cache associated with a second processor. In one example the first message is sent from a first processor to a second processor indicating that data is available for transfer from a first cache associated with the first processor. The method may further include requesting from the second processor a data transfer of the data from the first cache to a second cache associated with the second processor transferring the data from the first cache to the second cache in response to the request and sending a second message from the second processor to the first processor indicating that the data transfer is complete.

In another example of the disclosure an apparatus configured to transfer data between two caches comprising a first processor a first cache associated with the first processor a second processor and a second cache associated with the second processor wherein the first cache includes at least one partition that is configured to be a tightly coupled memory for the second cache and wherein the first processor and the second processor are configured to transfer data directly from the first cache to the second cache through the use of handshaking signals.

In another example of the disclosure an apparatus configured to transfer data between two caches comprises means for sending a first message from a first processor to a second processor indicating that data is available for transfer from a first cache associated with the first processor means for requesting from the second processor a data transfer of the data from the first cache to a second cache associated with the second processor means for transferring the data from the first cache to the second cache in response to the request and means for sending a second message from the second processor to the first processor indicating that the data transfer is complete.

In another example of the disclosure a computer readable storage medium storing instructions that when executed cause one or more processors to send a first message from a first processor to a second processor indicating that data is available for transfer from a first cache associated with the first processor request from the second processor a data transfer of the data from the first cache to a second cache associated with the second processor transfer the data from the first cache to the second cache in response to the request and send a second message from the second processor to the first processor indicating that the data transfer is complete

The techniques of this disclosure are also described in terms of an apparatus and a computer readable storage medium storing instructions for causing a processor to perform the techniques. The details of one or more examples are set forth in the accompanying drawings and the description below. Other features objects and advantages will be apparent from the description and drawings and from the claims.

In general this disclosure describes techniques for cache data migration and more specifically to techniques and systems for transferring data between caches in a multicore processing system. In particular this disclosure proposes the use of handshaking signals between two or heterogeneous processors so that a cache associated with a processor of a first type may be used as tightly coupled memory for a processor of a second type.

In particular computing device may be a multi processor system having two or more processing cores where at least two processor cores have separate dedicated memory caches. In some example computing device may be a heterogeneous multicore system have two or processing cores of different types. For example computing device may include two or more of central processing unit CPU cores graphics processing unit GPU cores digital signal processing DSP cores or any other type of processing core. Typically processing cores of different types have separate dedicate memory caches for each processor type that are not directly accessible to processing cores of different types. In addition a given processor such as CPU or GPU may itself include multiple processor cores e.g. on the same integrated circuit chip.

In the example of computing device may include a user input interface a central processing unit CPU a memory controller a system memory a graphics processing unit GPU a graphics memory a CPU cache a display interface a display and buses and . Graphics memory may be on chip with GPU and CPU cache may be on chip with CPU . Graphics memory and CPU are examples of memories that may include one more partitions that are tightly coupled memories for their associated processing cores. A tightly coupled memory TCM is low latency memory that provides a processing core with fast access to data without the unpredictably of other types of memory cache. As opposed to other partitions in a cache a TCM partition is part of the physical memory map of a system and no writes to external memory e.g. system memory having the same physical memory address need to be stored. A TCM may be configured to store either data or instructions and may store data and or instructions in parallel with other data caches and instructions caches not designated as TCM. The size of TCM partitions is flexible and may be for example specified from 4 kB to 256 kB. However any size TCM may be utilized.

User input interface CPU memory controller GPU and display interface may communicate with each other using bus . Memory controller and system memory may also communicate with each other using bus . Buses may be any of a variety of bus structures such as a third generation bus e.g. a HyperTransport bus or an InfiniBand bus a second generation bus e.g. an Advanced Graphics Port bus a Peripheral Component Interconnect PCI Express bus or an Advanced eXentisible Interface AXI bus or another type of bus or device interconnect. It should be noted that the specific configuration of buses and communication interfaces between the different components shown in is merely exemplary and other configurations of computing devices and or other graphics processing systems with the same or different components may be used to implement the techniques of this disclosure.

CPU may comprise a general purpose or a special purpose processor that controls operation of computing device . A user may provide input to computing device to cause CPU to execute one or more software applications. The software applications that execute on CPU may include for example an operating system a word processor application an email application a spread sheet application a media player application a video game application a graphical user interface application a telecommunication program a storage utility program or another program. Additionally CPU may execute a GPU driver for controlling the operation of GPU . The user may provide input to computing device via one or more input devices not shown such as a keyboard a mouse a microphone a touch pad or another input device that is coupled to computing device via user input interface .

The software applications that execute on CPU may include one or more graphics rendering instructions that instruct CPU to cause the rendering of graphics data to display . In some examples the software instructions may conform to a graphics application programming interface API such as e.g. an Open Graphics Library OpenGL API an Open Graphics Library Embedded Systems OpenGL ES API a Direct3D API an X3D API a RenderMan API a WebGL API or any other public or proprietary standard graphics API. In order to process the graphics rendering instructions CPU may issue one or more graphics rendering commands to GPU e.g. through a GPU driver to cause GPU to perform some or all of the rendering of the graphics data. In some examples the graphics data to be rendered may include a list of graphics primitives e.g. points lines triangles quadrilaterals triangle strips etc.

Memory controller facilitates the transfer of data going into and out of system memory . For example memory controller may receive memory read and write commands and service such commands with respect to memory system in order to provide memory services for the components in computing device . Memory controller is communicatively coupled to system memory via memory bus . Although memory controller is illustrated in as being a processing module that is separate from both CPU and system memory in other examples some or all of the functionality of memory controller may be implemented on one or both of CPU and system memory .

System memory may store program modules and or instructions that are accessible for execution by CPU and or data for use by the programs executing on CPU . For example system memory may store a window manager application that is used by CPU to present a graphical user interface GUI on display . In addition system memory may store user applications and application surface data associated with the applications. System memory may additionally store information for use by and or generated by other components of computing device . For example system memory may act as a device memory for GPU and may store data to be operated on by GPU as well as data resulting from operations performed by GPU . For example system memory may store any combination of texture buffers depth buffers stencil buffers vertex buffers frame buffers or the like. System memory may include one or more volatile or non volatile memories or storage devices such as for example random access memory RAM static RAM SRAM dynamic RAM DRAM read only memory ROM erasable programmable ROM EPROM electrically erasable programmable ROM EEPROM Flash memory a magnetic data media or an optical storage media.

GPU may be configured to perform graphics operations to render one or more graphics primitives to display . Thus when one of the software applications executing on CPU requires graphics processing CPU may provide graphics commands and graphics data to GPU for rendering to display . The graphics data may include e.g. drawing commands state information primitive information texture information etc. GPU may in some instances be built with a highly parallel structure that provides more efficient processing of complex graphic related operations than CPU . For example GPU may include a plurality of processing elements that are configured to operate on multiple vertices or pixels in a parallel manner. The highly parallel nature of GPU may in some instances allow GPU to draw graphics images e.g. GUIs and two dimensional 2D and or three dimensional 3D graphics scenes onto display more quickly than drawing the scenes directly to display using CPU .

GPU may in some instances be integrated into a motherboard of computing device . In other instances GPU may be present on a graphics card that is installed in a port in the motherboard of computing device or may be otherwise incorporated within a peripheral device configured to interoperate with computing device . In other instances GPU and CPU may be integrated together with other components in a system on chip SoC package. GPU may include one or more processors such as one or more microprocessors application specific integrated circuits ASICs field programmable gate arrays FPGAs digital signal processors DSPs or other equivalent integrated or discrete logic circuitry. GPU may also contain programmable shader units and or fixed function logic circuitry.

GPU may be directly coupled to graphics memory . Thus GPU may read data from and write data to graphics memory without using bus . Likewise CPU may read data from and write data to CPU cache without using bus . In other words GPU and CPU may process data locally using a local storage instead of off chip memory. This allows GPU and CPU to operate in a more efficient manner by eliminating the need to read and write data via system bus which may experience heavy bus traffic. In some instances however GPU and CPU may also utilize system memory via system bus . Graphics memory and CPU cache may include one or more volatile or non volatile memories or storage devices such as e.g. random access memory RAM static RAM SRAM dynamic RAM DRAM erasable programmable ROM EPROM electrically erasable programmable ROM EEPROM or a Flash memory.

CPU and or GPU may store rendered image data in a frame buffer . Frame buffer may be an independent memory or may be is allocated within system memory . Display interface may retrieve the data from frame buffer and configure display to display the image represented by the rendered image data. In some examples display interface may include a digital to analog converter DAC that is configured to convert the digital values retrieved from the frame buffer into an analog signal consumable by display . In other examples display interface may pass the digital values directly to display for processing. Display may include a monitor a television a projection device a liquid crystal display LCD a plasma display panel a light emitting diode LED array a cathode ray tube CRT display electronic paper a surface conduction electron emitted display SED a laser television display a nanocrystal display or another type of display unit. Display may be integrated within computing device . For instance display may be a screen of a mobile telephone. Alternatively display may be a stand alone device coupled to computer device via a wired or wireless communications link. For instance display may be a computer monitor or flat panel display connected to a personal computer via a cable or wireless link.

In some instances it is desirable to share data used by one type of processor e.g. CPU in its local cache e.g. CPU cache with another type of processor GPU . In such instances a data handoff technique may be used to transfer the data. One such data handoff technique involves flushing the data from a local cache of one processor and transferring the data to a system memory. A second processor may then copy the data from system memory into its local cache. When the data is flushed from the local cache the locations in the local cache storing the data may be overwritten. The first processor would need to access system memory again to regain use of the data from the flushed cache.

As another example some techniques may utilize a cache coherency mechanism to handle data transfers between processing cores. A cache coherency mechanism by a memory controller e.g. memory controller keeps track of what processing core s are allowed to access certain data stored in a central system memory and to copy that data to their local cache. This mechanism ensures the integrity of data that is frequently used by many different processing cores. However such systems are often very complex and may involve undesirable latency when accessing data. Both techniques described above require substantial performance and power resources. This leads to significant drawbacks for devices with limited power and performance potential e.g. wireless devices .

In view of these drawbacks the present disclosure presents techniques for transferring data between the local caches of two different processors. In particular this disclosure describes techniques that allow for the transfer of data from cache to cache without use of the system memory or a cache coherency mechanism. The techniques of this disclosure may be particularly useful in applications where only a few types of processing cores are used. In particular in systems where parallel processors interact using the same data. One example of such an application is data handoff between a CPU and a GPU in general purpose GPU GPGPU applications. Another typical usage case is block transfer BLIT operations which are often used in graphics applications e.g. for user interfaces . When rendering a user interface a CPU may transfer the bitmaps to a GPU. The typical size of bitmaps is particularly applicable to the size of TCM cache partitions. As such by utilizing the techniques of this disclosure for CPU to GPU BLIT operations fast responding times and power saving can be achieved.

First cache may include a partition that is configured to be accessible by a remote processor e.g. second processor as tightly coupled memory TCM . In some examples partition may be pre mapped and assigned as TCM for second processor . In other examples partition or other partitions of first cache may be mapped as TCM for second processor on an as needed basis. That is mapping a first cache of a first processor to a second cache of a second processor is performed in response to a request for data transfer. Mapping of partition s may be done by a memory controller within a cache or by another hardware unit of a computing device. A TCM is a memory connected to a processor so as to provide for dedicated access. TCMs generally perform more quickly than regular cache and are more suitable for real time applications.

Data transfers between the first cache and the second cache may be initiated with a first message. In one example first processor may be configured to signal a first message to second processor to indicate that data in first cache is available for transfer. The data available for transfer is represented as the shaded area in partition of first cache . The first message may be triggered by an application executing on the first processor in situations where the first processor no longer needs the data in first cache and further processing of the data is to be performed by second processor . First message may be signaled to second processor through any communication fabric available in the computing device. As one example first message may be sent through system bus as shown in . In other examples signaling between processors may be over other buses or communication lines configured for such communication or may affected through a hardware interrupt through direct memory access DMA or through independent control logic between two or more caches without intervention of any processors.

In another example data transfer may also be initiated by a request for data sent from the second processor to the first processor. That is the first message initiating the data transfer comes from the second processor as a request rather than from the first processor as an indication that data is available for transfer. In this example first processor may wait until any processing by first process on the data in first cache is completed before sending the data to second cache . First message may be signaled in response to a request for data from second processor or in response to an instruction e.g. a computer program being executed on first processor .

First processor and second processor may be any type of processors where their associated caches e.g. first cache and second cache are not directly accessible by one another. As such in some examples first processor and second processor may be of the same type e.g. two different CPU cores . In another example of the disclosure first processor and second processor are of different types e.g. CPU GPU DSP etc. . In particular first processor and second processor may be part of a heterogeneous multicore system.

In some examples partitions in the first cache and the second cache may be pre mapped to each other. In some examples this mapping will consist of designating a partition of the first cache as tightly coupled memory for the second processor. In other examples the computing device may be configured to map the first cache to the second cache before transferring the data . Next computing device may transfer the data from the first cache to the second cache in response to the request . Computing device may then send a second message from the second processor to the first processor indicating that the data transfer is complete . In some examples computing device may be further configured to clear the data from the first cache in response to the second message . In this context clearing the data from the first cache may involve marking the memory addresses storing the data as available to be overwritten.

In some example of the disclosure the first processor and the second processor are of different types. For example the first processor and the second processor form a heterogeneous multicore system. The heterogeneous multicore system may be part of a mobile device such as a smartphone or tablet computer.

In some examples the first processor is a central processing unit CPU and the second processor is a graphics processing unit GPU . In other examples the first processor is a graphics processing unit GPU and the second processor is a central processing unit CPU . In still other examples the first processor is a central processing unit CPU and the second processor is a digital signal processor DSP .

In one or more examples the functions described above may be implemented in hardware software firmware or any combination thereof. If implemented in software the functions may be stored as one or more instructions or code on an article of manufacture comprising a non transitory computer readable medium. Computer readable media may include computer data storage media. Data storage media may be any available media that can be accessed by one or more computers or one or more processors to retrieve instructions code and or data structures for implementation of the techniques described in this disclosure. By way of example and not limitation such computer readable media can comprise RAM ROM EEPROM CD ROM or other optical disk storage magnetic disk storage or other magnetic storage devices flash memory or any other medium that can be used to carry or store desired program code in the form of instructions or data structures and that can be accessed by a computer. Disk and disc as used herein includes compact disc CD laser disc optical disc digital versatile disc DVD floppy disk and Blu ray disc where disks usually reproduce data magnetically while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer readable media.

The code may be executed by one or more processors such as one or more DSPs general purpose microprocessors ASICs FPGAs or other equivalent integrated or discrete logic circuitry. In addition in some aspects the functionality described herein may be provided within dedicated hardware and or software modules. Also the techniques could be fully implemented in one or more circuits or logic elements.

The techniques of this disclosure may be implemented in a wide variety of devices or apparatuses including a wireless handset an integrated circuit IC or a set of ICs e.g. a chip set . Various components modules or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques but do not necessarily require realization by different hardware units. Rather as described above various units may be combined in a codec hardware unit or provided by a collection of interoperative hardware units including one or more processors as described above in conjunction with suitable software and or firmware.

Various examples have been described. These and other examples are within the scope of the following claims.

