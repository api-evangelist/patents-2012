---

title: Load balancing on hetrogenous processing cluster based on exceeded load imbalance factor threshold determined by total completion time of multiple processing phases
abstract: Methods and systems for managing data loads on a cluster of processors that implement an iterative procedure through parallel processing of data for the procedure are disclosed. One method includes monitoring, for at least one iteration of the procedure, completion times of a plurality of different processing phases that are undergone by each of the processors in a given iteration. The method further includes determining whether a load imbalance factor threshold is exceeded in the given iteration based on the completion times for the given iteration. In addition, the data is repartitioned by reassigning the data to the processors based on predicted dependencies between assigned data units of the data and completion times of a plurality of the processers for at least two of the phases. Further, the parallel processing is implemented on the cluster of processors in accordance with the reassignment.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09038088&OS=09038088&RS=09038088
owner: NEC Laboratories America, Inc.
number: 09038088
owner_city: Princeton
owner_country: US
publication_date: 20120301
---
This application claims priority to provisional application Ser. No. 61 451 327 filed on Mar. 10 2011 incorporated herein by reference.

The present invention relates to processing on heterogeneous clusters and more particularly to parallel execution on heterogeneous processing clusters.

Iterative methods are used in a wide variety of scientific and high performance computing HPC applications. An iterative method is essentially a computational procedure that generates a sequence of improving solutions for a problem. When the sequence converges to a final solution the method is referred to as an iterative convergence method. Algorithms such as gradient descent simulated annealing and numerous others are based on iterative convergence.

Typically scientific and HPC applications that employ iterative methods are parallelized and deployed on high end compute clusters. An example of such an application is the Open Field Operation and Manipulation OpenFOAM application which is an open source Computational Fluid Dynamic CFD toolbox for simulating fluid flow problems. The numerical solvers in OpenFOAM are available as parallelized implementations for homogeneous central processing unit CPU based clusters. It can be used to build applications for simulating fluid flow and has an extensive range of features to solve complex problems ranging from chemical reactions turbulence and heat transfer to solid dynamics. Generally CFD simulations in such application domains are carried out on high performance compute clusters because they need large computational and memory capabilities and possibly large amounts of storage as well.

One embodiment is directed to a method for managing data loads on a cluster of processors that implement an iterative procedure through parallel processing of data for the procedure. The method includes monitoring for at least one iteration of the procedure completion times of a plurality of different processing phases that are undergone by each of the processors in a given iteration. The method further includes determining whether a load imbalance factor threshold is exceeded in the given iteration based on the completion times for the given iteration. In addition the data is repartitioned by reassigning the data to the processors based on predicted dependencies between assigned data units of the data and completion times of a plurality of the processers for at least two of the phases. Further the parallel processing is implemented on the cluster of processors in accordance with the reassignment.

An alternative embodiment is directed to a computer readable storage medium comprising a computer readable program. The computer readable program when executed on a computer causes the computer to perform a method for managing data loads on a cluster of processors that implement an iterative procedure through parallel processing of data for the procedure. The method includes monitoring for at least one iteration of the procedure completion times of a plurality of different processing phases that are undergone by each of the processors in a given iteration. The method further includes determining whether a load imbalance factor threshold is exceeded in the given iteration based on the completion times for the given iteration. In addition the data is repartitioned by reassigning the data to the processors based on predicted dependencies between assigned data units of the data and completion times of a plurality of the processers for at least two of the phases.

Another embodiment is directed to a system for managing data loads. The system includes a cluster of processors a data repartitioner module and a balancer module. The cluster of processors is configured to implement an iterative procedure through parallel processing of data for the procedure. The data repartitioner module is configured to partition and assign the data to the processors for the parallel processing. Further the balancer module is configured to for at least one iteration of the procedure monitor completion times of a plurality of different processing phases that are undergone by each of the processors in a given iteration of the at least one iteration. The balancer module is also configured to determine whether a load imbalance factor threshold is exceeded in the given iteration based on the completion times for the given iteration. Moreover the balancer module is further configured to direct the data repartitioner module to repartition the data by reassigning the data to the processors based on predicted dependencies between assigned data units of the data and completion times of a plurality of the processers for at least two of the phases.

These and other features and advantages will become apparent from the following detailed description of illustrative embodiments thereof which is to be read in connection with the accompanying drawings.

As indicated above applications that employ iterative methods are often parallelized and deployed on high end compute clusters. However even with a high end CPU based cluster application performance can scale poorly with problem size. For example for modest to large sized problems it has been observed that an OpenFOAM based application that tracks the interface between two incompressible fluids took several hours to a few days to complete. This performance issue can be addressed by utilizing graphics processing units GPUs as co processors or accelerators in high performance compute clusters. When an iterative convergence process is parallelized it creates parallel tasks that communicate with each other. The performance of such a parallelized job is optimal when the running times of all parallel tasks i.e. computation and communication are roughly equal. However heterogeneous clusters have compute nodes of varying capabilities due to the use of accelerators on only some nodes or the use of processors from different generations among other reasons. This causes an imbalance resulting in some parallelized tasks finishing faster and waiting for other tasks to complete.

In accordance with aspects of the present principles the efficiency of parallelized processing on heterogeneous processing clusters can be improved by employing novel load balancing methods and systems. In particular embodiments perform automatic load balancing for parallelized iterative convergence processes implemented on a heterogeneous cluster for example a cluster with an imbalance in computation and communication capabilities.

When iterative convergence is parallelized each parallel task consists of computation communication and waiting phases. More specifically a task performs a computation sends messages to other nodes waits and receives messages from other nodes computes local convergence criteria broadcasts convergence information waits and receives convergence information from other nodes and then encounters a synchronization barrier. To ensure that the waiting phases are minimized embodiments employ a runtime strategy to continuously measure the computation communication and wait times on each node as well as the amount of communication. An analytical function of the measurements can be applied to determine how to repartition input data so that waiting time for the different parallel iterations is minimized. The process iterates until a stable balance is achieved.

In accordance with exemplary aspects the embodiments described herein can apply an analysis of different processing phases undergone by processing nodes to optimize the repartitioning of the data. In addition to further improve the repartition decisions an imbalance factor can be employed that is based on wait times within iterations and is normalized by a total iteration time that accounts for the different processing phases. Moreover the imbalance factor can include a multiplier that ensures the detection of cases in which the processing nodes appear to be balanced but the wait times themselves are excessively high. Features also include the use of a time estimation model that predicts dependencies between data units processed and completion for the different processing phases. The use of the phase specific dependencies permits the determination of a minimal amount of data to be reassigned from one processor to another to achieve a substantial balancing effect per data unit transferred. This is desirable as the reassignment of data itself may be time consuming and the iterative method can converge in a shorter amount of time with minimized data reassignment.

It should be understood that embodiments described herein may be entirely hardware entirely software or including both hardware and software elements. In a preferred embodiment the present invention is implemented in hardware and software which includes but is not limited to firmware resident software microcode etc.

Embodiments may include a computer program product accessible from a computer usable or computer readable medium providing program code for use by or in connection with a computer or any instruction execution system. A computer usable or computer readable medium may include any apparatus that stores communicates propagates or transports the program for use by or in connection with the instruction execution system apparatus or device. The medium can be magnetic optical electronic electromagnetic infrared or semiconductor system or apparatus or device or a propagation medium. The medium may include a computer readable storage medium such as a semiconductor or solid state memory magnetic tape a removable computer diskette a random access memory RAM a read only memory ROM a rigid magnetic disk and an optical disk etc.

A data processing system suitable for storing and or executing program code may include at least one processor coupled directly or indirectly to memory elements through a system bus. The memory elements can include local memory employed during actual execution of the program code bulk storage and cache memories which provide temporary storage of at least some program code to reduce the number of times code is retrieved from bulk storage during execution. Input output I O devices including but not limited to keyboards displays pointing devices etc. may be coupled to the system either directly or through intervening I O controllers.

Network adapters may also be coupled to the system to enable the data processing system to become coupled to other data processing systems or remote printers or storage devices through intervening private or public networks. Modems cable modem and Ethernet cards are just a few of the currently available types of network adapters.

As indicated above OpenFOAM is an example of an application that implements parallel execution and is used here to illustrate aspects of the present principles. As such prior to discussing features of the present principles in detail a brief overview of OpenFOAM is provided for expository purposes.

The OpenFOAM CFD Toolbox is an open source CFD software package. It is utilized in most areas of engineering and science and supports an extensive range of features to solve problems related to complex fluid flows involving chemical reactions turbulence heat transfer solid dynamics and electromagnetics. The core technology of OpenFOAM is a flexible set of efficient C modules which can be used to build solvers to simulate specific problems in engineering mechanics. It is also possible to develop utilities to perform pre and post processing tasks ranging from simple data manipulations to visualization and mesh processing. OpenFOAM includes pre configured solvers utilities and libraries. Due to its flexible structure and hierarchical design OpenFOAM solvers utilities and libraries are fully extensible. The application uses finite volume numerics to solve systems of partial differential equations ascribed on any three dimensional unstructured mesh of polyhedral cells. The fluid flow solvers are developed within a pressure velocity iterative solution framework although alternative techniques are applied to continuum mechanics solvers.

The broad physical modeling capabilities of OpenFOAM have been used by the aerospace automotive biomedical energy and processing industries. OpenFOAM comprises a suit of solvers for physical modeling laplacianFoam solves a simple Laplace equation e.g. for thermal diffusion in a solid icoFoam is a transient solver for incompressible and laminar flow of Newtonian fluids interFoam is a solver for two incompressible and isothermal immiscible fluids using a VOF volume of fluid phase fraction based interface capturing approach. There are a number of solvers related to combustion heat transfer buoyancy driven flows particle tracking flows and electromagnetism.

Many of the different solvers in OpenFOAM follow a similar computation and communication structure characterized by sparse linear algebra domain decomposition and iterative convergence. These solvers share a set of basic modules and InterFoam is utilized here a representative solver. InterFoam is a solver for two incompressible fluids which tracks the interface between the fluids and includes the option of mesh motion. It is an unsteady solver and is based on a PISO Pressure Implicit with Splitting of Operators algorithm. The PISO algorithm is an efficient method to solve the Navier Stokes equations in unsteady problems.

In order to identify potential candidates for offloading processing on GPUs implemented as co processors the InterFoam solver can be profiled with the callgrind profiling tool. Profiling of the application gives useful insight about computational intensive methods in the application. Further analysis is applied to determine communication overheads and identification of data parallelism. Based on the analysis it is believed that the granularity of parallel execution methods should change in some circumstances. Instead of parallelizing low level methods it is at times beneficial to parallelize higher level functions in the call graph in order to minimize communication between CPUs and GPUs.

The preconditioned conjugate gradient method is an iterative algorithm for solving linear systems. It is used to compute a numerical solution of particular systems of linear equations such as those whose matrix is symmetric and positive definite. The preconditioning leads to a faster propagation of information through the computational mesh. Because the conjugate gradient method is an iterative method it can be applied to sparse systems that are too large to be handled by direct methods such as the Cholesky decomposition. Such systems often arise when numerically solving partial differential equations. The matrix is constrained to be symmetric and positive definite for the conjugate gradient method to converge. With given input vectors xand b the solution of Ax b is obtained with the PCG algorithm and a diagonal preconditioner as shown below in Table 1. The residual vector r is defined as r b Ax. In each iteration the vectors x and r are computed and the residual vector r is compared with convergence criterion to indicate whether the iteration has produced a result that is closer to the actual solution x.

The model of parallel computation used by OpenFOAM is referred to as domain decomposition in which the mesh and associated fields are partitioned and allocated to separate tasks which are then assigned to different processing units using a Message Passing Interface MPI system. OpenFOAM provides multiple options for domain decomposition. In one exemplary implementation Scotch partitioning is employed as it attempts to minimize the number of processor boundaries i.e sync points between processors and provides the capability of assigning weights to processors based on their computation capabilities.

For the algorithm shown above in Table 1 each processor computes vector p line and then sends part of p to a predefined neighbor s based on the partitioning scheme. Simultaneously the processor computes vector q based on its locally computed value of p. Once all processors receive portions of vector p from other processors they incrementally update their computed value of q. Once p and q are determined the intermediate solution x and the residual r are computed. Each processor then communicates its residual r to other processors and global convergence is decided. Table 2 below presents the computations time for InterFoam on a homogeneous cluster. Each node in the cluster contains a Quad core Xeon E5620 2.4 GHz processor and 48 GB RAM. The problem is decomposed into smaller sub domains each MPI process performs computations on a sub domain and communicates intermediate results with neighboring nodes. As evident from the table there is a sharp increase in computation time with increasing problem size i.e input data size . As indicated above computation on a large data set can take days to complete.

As indicated above GPUs can be employed to accelerate applications such as applications characterized by iterative convergence sparse matrix computations and domain decomposition. Methods for porting InterFoam to GPU based clusters are described herein below.

While dense linear algebra computations benefit significantly from GPUs sparse linear algebra computations within the context of a large application present more challenges such as irregular data accesses. An important feature pertaining to OpenFOAM is the presence of sparse matrix computations within the context of the iterative convergence method which is how solvers are implemented. Not only are there irregular accesses within an iteration leading to possibly limited parallelism but there may be successive iterations that cannot be run in parallel. Nonetheless for sufficiently large problem sizes the GPU based cluster does show overall speedups.

Peripheral controller interface PCI related additional data communication overheads pertaining to the GPU specifically when iterative convergence is parallelized on a GPU cluster should also be considered. Parallelizing iterative convergence results in local tasks on GPU nodes that communicate with each other on every iteration to correctly achieve global convergence. With the GPU this communication not only has to traverse the network stack of each node but also the PCI bus.

The problem of domain decomposition i.e. data partitioning is exacerbated in a cluster composed of heterogeneous processing units including GPUs with varying compute capabilities. As discussed in further detail herein below load balancing methods and systems in accordance with the present principles can be employed to enable efficient implementation of data partitioning.

As mentioned above each MPI process performs computations on a sub domain of the complete mesh. To utilize the GPU on the machine an MPI process can offload parts of the computation to the GPU. This can be done by identifying code sections that can benefit from executing on the GPU and rewriting them using Compute Unified Device Architecture CUDA kernels. After profiling and experimentation it can be shown that for the InterFoam application it is beneficial to offload the entire PCG solver to the GPU. Offloading only data parallel parts of the PCG solver had led to poor performance due to high communication overheads between the CPU and GPU. The CUDA Sparse Matrix CUSPARSE library which provides fast implementation of sparse matrix operations can be used for implementing the PCG solver in CUDA.

OpenFOAM solvers use the LDU matrix format to store sparse matrices produced from the mesh while the CUSPARSE library uses a compressed sparse row CSR format. In order to be able to use CUSPARSE an LDU to CSR format converter can be developed and employed.

First each MPI process sets an appropriate CUDA device. Then all LDU matrices are converted to CSR format. Thereafter the algorithm proceeds as follows in Table 3.

It can be shown that for large data sets the GPU based cluster outperforms CPU based cluster while for smaller data sets the CPU based cluster provides better performance. This is because for relatively large data sets the communication overhead between CPU and GPU portions is more than compensated for by the compute acceleration obtained with the GPU. In turn for smaller data sets the communication overhead dominates.

As mentioned above OpenFOAM employs task parallelism where the input data is partitioned and assigned to different MPI processes. For a homogeneous cluster of CPUs and GPUs a uniform data partitioning scheme would suffice. However a uniform data partitioning scheme will not suffice for clusters where some nodes do not have GPUs or the GPUs have different compute capabilities. In such heterogeneous clusters a uniform domain decomposition and data partitioning scheme can lead to imbalance and suboptimal performance.

Due to the difference in computation capabilities processors and complete the computations on data sets and assigned to them and wait for processors and to finish their computations resulting in suboptimal performance.

For applications such as OpenFOAM where compute time is dependent on the data size the above mentioned problem can be solved if the workload data is divided based on the compute capabilities of the processing units involved. One way to accomplish this is to characterize the cluster by profiling it statically and generating a map of relative computation power for the different nodes involved and then using this information for generating data partitions. However this simplistic approach has several limitations. First a cluster profile generated without running the application under consideration is likely going to be inaccurate in predicting optimal data partitioning for the application more so for a cluster of heterogeneous CPUs with different memory bandwidths cache levels and processing elements. Second in the case of multi tenancy where applications share resources in the cluster it would be difficult to predict the execution time of an application statically. Third a data aware scheduling scheme where the selection of computation to be offloaded to the GPU is done at runtime if employed would add to the complexity of estimating data partitions statically.

As such in accordance with aspects of the present principles a dynamic data partitioning scheme can be employed where a run time balancer analyzes the discrepancy in the computation and communication patterns of different MPI processes and directs the repartition of the data set accordingly. For example assume P is a master process with P P and P as slave processes. After running for a fixed number of iterations the slave processes send their own timing profiles computation and communication time to the master process. The master process observes the computation and communication pattern for each process and suggests a new partitioning ratio to balance the computation across the processes to achieve optimal performance. illustrates an example of such a repartitioning.

With reference to the repartition can assign larger data blocks and as compared to blocks and to processor nodes and respectively and can assign smaller data blocks and as compared to blocks and to processor nodes and respectively. The total processing time for all of the nodes T denoted by element is less than time . This is due to the relative parity of the computation times of the nodes and respectively.

To perform dynamic repartitioning the runtime analyzer or balancer can observe the computation and communication pattern for each process for a fixed number of iterations and can suggest a new partitioning ratio. It can then direct the repartitioning of the workload and the application is made to run with the repartitioned workload.

To implement the dynamic repartitioning described above system and method embodiments for managing data loads on a cluster of processors that implement an iterative procedure through parallel processing can be employed. The systems and methods described herein below can be applied to any cluster with heterogeneous processing and or communication capabilities that is utilized for iterative procedures. The OpenFOAM CPU GPU heterogeneous example is only one illustrative environment in which the systems and methods can be employed.

As indicated above iterative methods are those that produce a sequence of improving approximate solutions to a problem. They start with an initial data set model and continuously refine it over the course of several iterations. Examples of widely used algorithms that employ iterative methods are gradient descent simulated annealing k means and many others.

For illustrative purposes the methods and systems discussed herein below are described with respect to stationary iterative methods that perform the same operations every iteration. Here an iteration is comprised of two phases a COMPUTE phase during which the model is transformed using computing operations performed by the processor and a CONVERGENCE CHECK phase which determines if the refinements performed on the model are sufficient to terminate the algorithm.

The systems and methods described herein can be applied to systems that parallelize iterative methods across N distributed processing nodes. The processing nodes can be processing nodes with no shared memory such as those found in a compute cluster. In order to enable such a parallelization the initial model data is split across the processing nodes. An iteration on each processing node now performs the COMPUTE phase on its local model data but communicates and synchronizes with other processing nodes to correctly evaluate the global termination condition. For example consider the k means algorithm which starts with an initial model consisting of n points and k means. During each iteration for each mean m the sequential algorithm finds the subset of the n points for which mis the closest mean. It then averages these points to produce a new value for m. When k means is parallelized each processing node only sees a portion of the n points and computes k means based on that local data but all processing nodes have to communicate their locally computed means to each other to determine if the algorithm has globally converged.

Inter node communication need not occur only during the global convergence check. Depending on the type of operations parallelizing the COMPUTE phase itself may involve communication between processing nodes. In general each iteration on one processing node within a parallelized iterative framework consists of processing phases which may be classified as one of the following classes of phases a COMPUTE CMPT phases in which local computation is performed on the processing node b DATA TRANSFER XFER phases in which data transfer is performed between this processing node and other processing nodes c SYNCHRONIZATION WAIT phases during which this processing node waits for other processing nodes and CONVERGENCE CHECK CONY phase during which this processing node checks its local convergence criteria. Where processing nodes are configured to offload computations to an accelerator these phases inherently incorporate the communication time between the host processor node and the accelerator.

The convergence check and the parallelization enforces a global barrier at the end of every iteration. is a diagram that shows the different phases in a parallelized iterative method where a processing node J denoted by block refines model data DATA J K during iteration K and produces refined model DATA J K 1 during iteration K 1 . The diagram also illustrates a processing node J 1 denoted by block that refines model data DATA J 1 K during iteration K and produces refined model DATA J 1 K 1 during iteration K 1. The processing nodes to J 1 and J 2 to N similarly undergo the processing phases.

A parallelized iterative method is balanced when each processing node s iterations involve similar amounts of CMPT and XFER thereby reaching the global barrier at roughly the same time as any other processing node s iterations. Imbalance occurs when one of the processing node s CMPT phases complete faster than another processing node s CMPT phases or if one processing node spends more time in communicating data XFER phase than another processing node. Due to the global barrier enforced by the convergence check after every iteration imbalance results in different SYNCHRONIZATION or wait times across processing nodes. That is the faster processing node or the processing node performing less data transfer spends more time waiting for the other processing nodes to reach the barrier.

Imbalance in parallelized iterative methods is caused primarily by two factors hardware heterogeneity and data irregularity. With regard to hardware heterogeneity nodes in the cluster have different processing memory and I O capabilities. For example some processor nodes may have faster CPUs more memory or many core accelerators like GPUs while others may not. Accelerators themselves can be of different types and speeds such as the NVIDIA Tesla and Fermi devices and the Intel MIC. With regard to data irregularity although the amount of local model data for two iterations may be the same the application s access patterns may be irregular or sparse resulting in different processing times for the same amount of data.

The systems and methods described herein below provide a mechanism to automatically identify and rectify imbalance in parallelized iterative methods on heterogeneous clusters. The methods do not require a priori knowledge of either the application or the hardware used in the clusters.

In accordance with exemplary aspects of the present principles imbalance can be reduced by minimizing the WAIT phase time for each processing node in a cluster. Reducing the wait time automatically adjusts the CMPT and XFER phases so that all processing nodes will reach the global barrier at the end of each iteration at roughly the same time thereby improving the overall balance. Here the imbalance is quantified by defining an imbalance factor IF for the cluster during iteration K 

With reference now to an exemplary system for managing data loads on a cluster of processors that implement an iterative procedure through parallel processing is illustratively depicted. The system includes a balancer module a data repartitioner module and a cluster of a set of processors . Each of the balancer module and the data repartitioner can be implemented on hardware processors and can be configured to execute program code that is stored on one or more storage mediums and that implements resource management of the cluster of processors in accordance with the methods described herein. The cluster of processors can include processors with heterogenous processing capability and or speed and or with heterogenous communication capability and or speed. For example as noted above the processors can include CPUs and CPUs that offload computations to GPUs. In addition some of the processors in the cluster can be GPUs themselves. The data repartitioner here is part of the application that runs the iterative method on the cluster of processors. In alternative embodiments the data repartitioner can be implemented in the balancer .

Each processor of the cluster can be configured to perform a computation such that the processor operates on its share of data. It then communicates with one or more other processors. This communication involves sending messages and waiting to receive messages from other processors as noted above. Once this communication phase completes each processor performs a local convergence test and communicates this again to other processors. Once global convergence is achieved all processors stop and the solution is considered acceptable.

For iterative convergence two types of data can be identified by the data repartitioner the original input data which is the initial model data and the model data which is continuously refined. The data partitioner can implement a given data partitioning. For example data partitioner can dispatch partitioned input data to all processors and can direct the processors to exchange and reparation their intermediate model data.

The balancer can be configured to request CMPT XFER WAIT and CONV time measurements as well as the model data size from the data repartitioner which is configured to partition and assign the data to the processors . The balancer can be further configured to issue a repartitioning directive that instructs the application or its data partitioner on how best to repartition the model data.

The balancer can include a time estimation model or module a history table comprised of phase completion times and a repartition block . The history table tracks the CMPT time CMPT TIME and XFER time XFER TIME for each iteration ITER on each processing node for a given data size DATA SIZE . Processing nodes on the history table are identified as PROC NODE . Time measurements may be collected by instrumenting the application itself or by other known means for example by intercepting iteration end barriers. The data size is also expected to be provided by the application on request by the balancer . For example many applications use or can be configured to use a data repartitioner which can be used to provide this information.

The time estimation model or module predicts the CMPT and XFER times for another data size based on CMPT and XFER historical data. A simple linear model can be employed where for a given processing node the CMPT and XFER times vary linearly with data size. However other models can be obtained and utilized by curve fitting the history table data to for example a polynomial model of degree 2. Other models are also possible and may be used.

The repartition or repartitioning block can be configured to implement a method for managing data loads on a cluster of processors that implement an iterative procedure through parallel processing of data for the procedure. The method is illustratively depicted in . Further a more specific implementation of the method is illustrated in . Reference to both methods is made herein below. Each of the iterations of the methods and can correspond to one or more given iterations of the iterative procedure implemented by the cluster of processors . In addition the static partitioning method described above can be used to initialize the cluster of processors for the iterative procedure prior to the implementation of the methods and . Further it should also be noted that according to one exemplary aspect data partitioning in accordance with methods and described herein below can identify the imbalance shortly after an application starts and can restart the application with a better data partitioning. For long running applications the restarting overhead is minimal. However the methods can be extended to the case in which the iterative convergence model intermediate data is also repartitioned so that the overall computation is not restarted when the data is repartitioned.

The method can begin at step at which the repartition block can for at least one iteration of the iterative procedure monitor completion times of a plurality of different processing phases that are undergone by each of the processors in a given iteration. For example as indicated above the plurality of different processing phases are successive phases and include a computation phase a data transfer phase a synchronization phase and a convergence check phase. In addition step can be implemented by step where the balancer can request from the data repartitioner of the application the time measurements for the completion of each of the phases CMPT XFER WAIT and CONY for each of the processors . In addition at steps and the repartition block can also obtain the data size of the data processed by each respective processor .

At steps and the repartition block of the balancer can update a completion time history with the received completion times and can predict dependencies between assigned data units of the data processed by the cluster and completion times of a plurality of the processers in the cluster for at least two of the phases based on the completion times monitored for a plurality of iterations of the procedure. For example the balancer can update the table with the completion times received at steps or step . The table can be configured to record completion time histories of several previous runs of the application and or one or more previous runs of different applications on the cluster . Further the balancer can update the time estimation model with the new completion times. As noted above the time estimation model can be a simple linear model such that for a given processing node the CMPT and XFER times may vary linearly with data size. The time estimation model may track these dependencies for each of the processors so that each processor of the cluster has its own set of dependencies for the various phases of WAIT CMPT and XFER. Further the data size can be obtained at steps and as stated above.

At step the repartition block of the balancer can determine whether a load imbalance factor threshold is exceeded in the given iteration based on the completion times for the given iteration. Here step can be implemented by performing steps and of the method . For example the repartition block can at step compute the imbalance factor IFusing the WAIT times where

At step which can be implemented by steps of the method the repartition block can direct the data repartioner to repartition and reassign the data based on the predicted dependencies. For example at step the repartition block can identify the processing nodes in the cluster with the largest and smallest WAIT times i and j. It is very likely that the processing node with the largest WAIT time i spends less time computing and transferring data than the processing node with the largest WAIT time j that is CMPT i and XFER i will likely be smaller than CMPT j and XFER j respectively. At step the repartition block can reference the time estimation model and its corresponding dependencies to compute D which is the number of data units to be moved from processor j of the cluster to processor i of the cluster to make their CMPT times roughly equal. Similarly at step repartition block can reference the time estimation model and its corresponding dependencies to calculate D the number of data units to be moved from processor j of the cluster to processor i of the cluster to make their to make the XFER times roughly equal. At step the repartition block sets the number of data units D to be moved from processor j of the cluster to processor i of the cluster as the minimum of Dand D. As indicated above reassigning a minimal amount of data is desirable as the reassignment and load transfer may be time consuming and the iterative method can converge faster with a smaller load transfer. Further reconfiguring the data so that the fastest processor s and the slowest processor s complete any one of the compute and transfer phases at the same time provides substantial benefits in balancing the overall system thereby ensuring that the reassignment achieves a significant balancing effect per data unit transferred. At step the reparation block can direct the application s data repartitioner to repartition and reassign data of the iterative procedure to the cluster such that the minimum of Dand Ddata units is moved from processor j of the cluster to processor i of the cluster and the cluster of processors can implement the parallel processing of the data in accordance with the repartitioning and reassignment. Here the assignment to the processors of the cluster other than processor j and processor i can remain the same during the parallel processing of the data. The data can include both input and model data.

At step the cluster of processors can implement the parallel processing in accordance with the repartition and reassignment for example as stated above with respect to step . Thereafter the methods and can repeat.

It should be noted that in accordance with one exemplary aspect multiple repartitions can be made simultaneously in one iteration of the methods and . For example at steps and steps can be performed as described above. However at step and also step the repartition block can identify multiple 2n processing nodes in the cluster with the n largest and n smallest WAIT times. For example if n 3 then the repartition block identifies three processing nodes i i i with the top three wait times W Wand W from the highest wait time to the next two lower wait times and identifies three processing nodes j j j with the bottom three wait times W Wand W from the lowest wait time to the next two higher wait times . At step and also step the repartition block can reference the time estimation model and its corresponding dependencies to compute D which is the number of data units to be moved from processor jof the cluster to processor iof the cluster to make their CMPT times roughly equal. Also at step and the repartition block can reference the time estimation model and its corresponding dependencies to compute Dand D which are the number of data units to be moved from processors jand jof the cluster to processors iand iof the cluster respectively to make the corresponding CMPT times of jand iroughly equal to each other. In turn at step and repartition block can reference the time estimation model and its corresponding dependencies to calculate D D D the number of data units to be moved from processors j j jof the cluster to processors i i iof the cluster respectively to make the corresponding XFER times of jand iroughly equal to each other. At step and the repartition block sets the number of data units D D D to be moved from processors j j jof the cluster to processors i i iof the cluster respectively as the minimum of the corresponding Dand D. For example Dis set to min D D Dis set to min D D and Dis set to min D D . At step and the reparation block can direct the application s data repartitioner to repartition and reassign data of the iterative procedure to the cluster such that Ddata units are moved from processor jto processor i Ddata units are moved from processor jto processor i and Ddata units are moved from processor jto processor i. At step and step the cluster of processors can implement the parallel processing of the data in accordance with the repartitioning and reassignment as described above. Thereafter the methods and can repeat.

In addition in each of the implementations described above if at step and multiple processing nodes in the cluster are found to have the highest wait time s Wor W i.e. there is a tie between two or more processing nodes then one of these multiple processing nodes is selected at random to be node j or j. Similarly if at step and multiple processing nodes in the cluster are found to have the lowest wait time s Wor W i.e. there is a tie between two or more processing nodes then one of the multiple processing nodes is selected at random to be node i or i.

It should be further noted that as the cluster scales out balancing a highly parallelized iterative method itself could possibly be time consuming. To achieve faster balancing the balancer may be scaled out to larger clusters using either a hierarchical strategy or a fully distributed peer to peer strategy.

A fully distributed peer to peer strategy involves each processing node computing the imbalance factor between itself and a few closely located target nodes. The processing node then orchestrates data movement to or from the target node using the method of .

Systems and methods for automatically balancing parallelized iterative methods using data repartitioning are disclosed. The strategy addresses both the computation and data transfer phases of such methods and attempt to minimize the wait time for each processing node. Important features of the scheme include the definition of an imbalance factor based on wait times within the iterations and the use of a history table to track both computation and data transfer performance of individual iterations. Other important aspects include the use of a estimator model to predict the performance and data transfer times and the determination of a repartitioning of data that substantially improves the performance of the system.

Having described preferred embodiments of systems and methods for load balancing on heterogeneous processing cluster that implement parallel execution which are intended to be illustrative and not limiting it is noted that modifications and variations can be made by persons skilled in the art in light of the above teachings. It is therefore to be understood that changes may be made in the particular embodiments disclosed which are within the scope of the invention as outlined by the appended claims. Having thus described aspects of the invention with the details and particularity required by the patent laws what is claimed and desired protected by Letters Patent is set forth in the appended claims.

