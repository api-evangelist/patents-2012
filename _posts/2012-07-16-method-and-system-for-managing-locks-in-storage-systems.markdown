---

title: Method and system for managing locks in storage systems
abstract: Machine implemented method and system for a networked storage system having a first storage system node providing storage space to a client computing system at a first storage device and a second storage system node managing a second storage device are provided. A lock is granted by the first storage system node to the client computing system for accessing a data container stored at the first storage device. The first storage system node updates a lock data structure for managing the lock granted to the client computing system. Information regarding the lock is then replicated at the second storage system node, such that the second storage system node can recover the lock, when the first storage system node becomes unavailable and the second storage system node takes over the first storage device to interface with the client computing system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08868530&OS=08868530&RS=08868530
owner: NetApp, Inc.
number: 08868530
owner_city: Sunnyvale
owner_country: US
publication_date: 20120716
---
The present disclosure relates to storage systems and more particularly to managing locks in storage systems.

A storage system typically comprises one or more storage devices where information may be stored and from where information may be retrieved as desired. The storage system may be implemented in accordance with a variety of storage architectures including but not limited to a network attached storage NAS environment a storage area network SAN and a storage device assembly directly attached to a client or host computer.

The storage system typically includes a storage operating system that may implement a high level module such as a file system to logically organize information stored at storage volumes as a hierarchical structure of data containers such as files and logical units. For example stored files may be implemented as set of data structures i.e. storage device blocks configured to store information such as the actual data for the file. These data blocks are typically organized within a volume block number vbn space that is maintained by the file system. The file system typically organizes the data blocks within the vbn space as a logical volume each logical volume may be although is not necessarily associated with its own file system.

The storage system may be configured to operate according to a client server model of information delivery to thereby allow many clients to access data containers stored on the system. In this model the client may comprise an application such as a database application executing in a computer that communicates with the storage system. Each client may send input output I O requests to read and write data containers.

A plurality of storage systems may be interconnected to service numerous client requests. The plurality of storage systems provide redundancy to clients which means that if one storage system becomes unavailable then another storage system takes over the storage space provided to the client. In such an environment managing rights associated with I O operations is a challenge. Rights may be managed by issuing locks to one or more client application may also be referred to as a client on a data container residing at a storage volume. The lock provides certain rights to the client to perform read and write operations with respect the data container. Continuous efforts are being made to better manage locks in networked storage systems.

In one embodiment a machine implemented method and system for a networked storage system having a first storage system node providing storage space to a client computing system at a first storage device and a second storage system node managing a second storage device are provided. A lock is granted by the first storage system node to the client computing system for accessing a data container stored at the first storage device. The first storage system node updates a lock data structure for managing the lock granted to the client computing system. Information regarding the lock is then replicated mirrored at the second storage system node such that the second storage system node can recover the lock when the first storage system node becomes unavailable and the second storage system node takes over the first storage device to interface with the client computing system.

In another embodiment a networked storage system is provided. The system includes a first storage system node configured to grant a lock to a client computing system for accessing a data container stored at a first storage device and a second storage system node for replicating information regarding the lock at a second storage device managed by the second storage system node such that the second storage system node can recover the lock when the first storage system node becomes unavailable and the second storage system node takes over the first storage device to interface with the client computing system.

The mirrored information regarding the lock includes an identifier identifying the client computing system a reconnect key that allows the client computing system to reconnect and obtain the lock from the second storage system node when the first storage system node becomes unavailable an identifier identifying the data container and an identifier identifying a storage volume associated with the storage device used for storing the data container.

In yet another embodiment a machine implemented method for a networked storage system having a first storage system node providing storage space to a client computing system at a first storage device and a second storage system node managing a second storage device. The method includes mirroring information regarding any lock granted by the first storage system to the client computing system at the second storage system node such that the second storage system node can recover the lock when the first storage system node becomes unavailable and the second storage system node takes over the first storage device to interface with the client computing system and notifying by the second storage system node to the first storage system node that the information regarding any granted lock has been successfully mirrored at the second storage system node.

In another embodiment a machine implemented method for a networked storage system having a first storage system node providing storage space to a client computing system at a first storage device and a second storage system node managing a second storage device is provided. The method includes granting a lock by the first storage system node to the client computing system for accessing a data container stored at the first storage device and mirroring information regarding the lock at the second storage system node such that the second storage system node can recover the lock when the first storage system node becomes unavailable and the second storage system node takes over the first storage device to interface with the client computing system.

This brief summary has been provided so that the nature of this disclosure may be understood quickly. A more complete understanding of the disclosure can be obtained by reference to the following detailed description of the various embodiments thereof in connection with the attached drawings.

As preliminary note the terms component module system and the like as used herein are intended to refer to a computer related entity either software executing general purpose processor hardware firmware and a combination thereof. For example a component may be but is not limited to being a process running on a processor a processor an object an executable a thread of execution a program and or a computer.

By way of illustration both an application running on a server and the server can be a component. One or more components may reside within a process and or thread of execution and a component may be localized on one computer and or distributed between two or more computers. Also these components can execute from various computer readable media having various data structures stored thereon. The components may communicate via local and or remote processes such as in accordance with a signal having one or more data packets e.g. data from one component interacting with another component in a local system distributed system and or across a network such as the Internet with other systems via the signal .

Computer executable components can be stored for example on non transitory computer readable media including but not limited to an ASIC application specific integrated circuit CD compact disc DVD digital video disk ROM read only memory floppy disk hard disk EEPROM electrically erasable programmable read only memory memory stick or any other storage device in accordance with the claimed subject matter.

In one embodiment a machine implemented method and system for a networked storage system having a first storage system node providing storage space to a client computing system at a first storage device and a second storage system node managing a second storage device is provided. A lock is granted by the first storage system node to the client computing system for accessing a data container stored at the first storage device. The first storage system node updates a lock data structure managing the lock granted to the client computing system. Information regarding the lock is then replicated or mirrored at the second storage system node such that the second storage system node can recover or construct the lock when the first storage system node becomes unavailable and the second storage system node takes over the first storage device to interface with the client computing system.

To facilitate an understanding of the various embodiments of the present disclosure the general architecture and operation of a networked clustered storage system will now be described.

As described below in detail each node maintains a lock data structure for managing locks that are granted to client computing systems may also be referred to as clients for accessing a data container managed by a node. A lock as used herein means a mechanism used by node to limit access to a data container. There are various rights associated with the locks issued or revoked with respect to a data container. The term rights as used herein means a privilege that is granted to an entity for example a client application executed at a computing device with respect to any input output I O operation for example read and write operations. The term data containers as used throughout this specification mean a file a logical unit or any other information. The term file is used interchangeably with data container throughout this specification.

Nodes comprise various functional components that cooperate to provide distributed storage system architecture of cluster . Each node is generally organized as a network element N module and a storage device element D module . N module includes functionality that enables node to connect to client computing systems over a network connection while each D module connects to one or more storage devices such as or a storage array . Illustratively network may be embodied as an Ethernet network a Fibre Channel FC network or any other network type. Nodes may be interconnected by a cluster switching fabric which in the illustrative embodiment may be embodied as a Gigabit Ethernet switch or any other switch type.

It should be noted that while there is shown an equal number of N and D modules in the illustrative cluster there may be differing numbers of N and or D modules in accordance with various embodiments of the present disclosure. For example there may be a plurality of N modules and or D modules interconnected in a cluster configuration that does not reflect a one to one correspondence between the N and D modules. As such the description of node comprising one N module and one D module should be taken as illustrative only.

Clients may be general purpose computers having a plurality of components as described below in detail with respect to . These components may include a central processing unit CPU main memory I O devices and storage devices for example flash memory hard drives and others . The main memory may be coupled to the CPU via a system bus or a local memory bus. The main memory may be used to provide the CPU access to data and or program information that is stored in main memory at execution time. Typically the main memory is composed of random access memory RAM circuits. A computer system with the CPU and main memory is often referred to as a host system.

Clients may be configured to interact with a node in accordance with a client server model of information delivery. That is each client may request the services of the node and node may return the results of the services requested by the client over network .

Clients may be configured to execute processor executable instructions shown as application for reading and writing information at storage devices . Such application may include a database application a financial management system an electronic mail application or any other application type.

Client may issue packets using application including file based access protocols such as the Common Internet File System CIFS protocol or the Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information in the form of certain data containers. Alternatively the client may issue packets using application including block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP when accessing information in the form of other data containers such as blocks.

Processors A B may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such hardware based devices. The bus system may include for example a system bus a Peripheral Component Interconnect PCI bus a HyperTransport or industry standard architecture ISA bus a small computer system interface SCSI bus a universal serial bus USB or an Institute of Electrical and Electronics Engineers IEEE standard 1394 bus sometimes referred to as Firewire or any other interconnect type.

The cluster access adapter comprises a plurality of ports adapted to couple node to other nodes of cluster . In the illustrative embodiment Ethernet may be used as the clustering protocol and interconnect media although it will be apparent to those skilled in the that other types of protocols and interconnects may be utilized within the cluster architecture described herein. In alternate embodiments where the N modules and D modules are implemented on separate storage systems or computers the cluster access adapter is utilized by the N D module for communicating with other N D modules in the cluster .

The network adapter comprises a plurality of ports adapted to couple the node to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network.

The storage adapter cooperates with a storage operating system executing on the node to access information requested by the clients. The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored on the storage devices of array . The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the storage devices over an I O interconnect arrangement such as a conventional high performance FC link topology.

It is noteworthy that although various adapters and have been shown as separate hardware based components the embodiments disclosed herein are not limited to separate components. The embodiments disclosed herein may be implemented using a converged network adapter CAN that is capable of handling both network and storage protocols for example a Fibre Channel over Ethernet FCoE adapter.

Each node is illustratively embodied as a multiple processor system executing the storage operating system that preferably implements a high level module such as a file system to logically organize the information as a hierarchical structure of named directories files and special types of files called virtual disks hereinafter generally blocks on storage devices . However it will be apparent to those of ordinary skill in the art that the node may alternatively comprise a single or more than two processor systems. Illustratively one processor A executes the functions of the N module on the node while the other processor B executes the functions of the D module .

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing programmable instructions and data structures. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the programmable instructions and manipulate the data structures. It will be apparent to those skilled in the art that other processing and memory means including various non transitory computer readable media may be used for storing and executing program instructions pertaining to the disclosure described herein.

The storage operating system portions of which is typically resident in memory and executed by the processing elements functionally organizes the node by inter alia invoking storage operations in support of the storage service implemented by the node and maintaining a lock data structure for managing various lock types that are issued to client applications. An example of operating system is the DATA ONTAP Registered trademark of NetApp Inc. operating system available from NetApp Inc. that implements a Write Anywhere File Layout WAFL Registered trademark of NetApp Inc. file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such where the term ONTAP is employed it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings disclosed herein.

Storage of information on each storage array is preferably implemented as one or more storage volumes that comprise a collection of physical storage devices cooperating to define an overall logical arrangement of volume block number vbn space on the volume s . Each logical volume is generally although not necessarily associated with its own file system. The storage devices within a logical volume file system are typically organized as one or more groups wherein each group may be operated as a Redundant Array of Independent or Inexpensive Disks RAID .

Storage operating system maintains a plurality of lock types in responding to client requests for reading a data container writing a data container or modifying a data container. The following provides a brief description of the various lock types that may be used by storage operating system for managing access to data containers.

 Opportunistic Lock may also be referred to as OpLock means a lock that is placed by one or more client application may also be referred to as a client on a data container residing at a storage volume. The OpLock information may be embedded in a data container attribute for example meta data for the data container. OpLock based on client requests coordinates data caching and coherency between clients and storage systems. Coherent data in this context means data that is the same across a network i.e. data stored by the storage system and locally by the clients is synchronized. OpLocks are defined by the CIFS protocol and there are different types of OpLocks for example Level 1 Level 2 and other types. The rights associated with an OpLock depend on the OpLock type.

A Shared Lock is typically granted to a client application by storage operating system . The shared lock allows more than one application to access a data container.

 Byte lock is a lock type that limits access to a portion of a data container. A client may request to open a data container and request a lock for a byte range of the file.

 Persistent Open is a feature that may be used for various locks issued by storage operating system . The persistent open feature provides an exclusive right to a data container. When a node grants a persistent open lock to a data container it also provides a reconnect key to the client. If the node becomes unavailable for any reason the persistent open lock stays open for a duration t and within that duration the client may use the reconnect key to obtain the lock and access to the data container.

To manage the various locks storage operating system includes a lock manager that maintains one or more lock data structures for managing the locks. Details regarding the lock manager and the lock data structures are provided below.

Operating system may also include a protocol layer and an associated network access layer to allow node to communicate over a network with other systems such as clients . Protocol layer may implement one or more of various higher level network protocols such as NFS CIFS Hypertext Transfer Protocol HTTP TCP IP and others as described below.

Network access layer may include one or more drivers which implement one or more lower level protocols to communicate over the network such as Ethernet. Interactions between clients and mass storage devices are illustrated schematically as a path which illustrates the flow of data through operating system .

The operating system may also include a storage access layer and an associated storage driver layer to allow D module to communicate with a storage device. The storage access layer may implement a higher level disk storage protocol such as RAID while the storage driver layer may implement a lower level storage device access protocol such as FC Fibre Channel or SCSI.

A file system protocol layer provides multi pro file access and to that end includes support for the Direct Access File System DAFS protocol the protocol the CIFS protocol and the HTTP protocol .

A virtual interface VI layer implements the VI architecture to provide direct access transport DAT capabilities such as remote direct memory access RDMA as required by the OAFS protocol . An iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a FC driver layer receives and transmits block access requests and responses to and from the node. The FC and iSCSI drivers provide FC specific and iSCSI specific access control to the blocks and thus manage exports of luns where luns are represented as blocks to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing the blocks on the node .

In addition the storage operating system includes a series of processor executable layers organized to form a storage server that provides data paths for accessing information stored on the storage devices of the node . To that end the storage server includes the file system module in cooperating relation with a volume stripped module VSM a RAID system module and a storage device driver system module . The VSM cooperates with the file system to enable storage server to service a volume.

The RAID system manages the storage and retrieval of information to and from the volumes storage devices in accordance with I O operations while the storage device driver system implements a storage device access protocol such as e.g. the SCSI protocol.

The file system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustratively embodied as e.g. a virtual disk vdisk module not shown and a SCSI target module . The SCSI target module is generally disposed between the FC and iSCSI drivers and the file system to provide a translation layer of the virtualization system between the block lun space and the file system space where luns are represented as blocks.

The file system is illustratively a message based system that provides logical volume management capabilities for use in access to the information stored on the storage devices such as disks. That is in addition to providing file system semantics the file system provides functions normally associated with a volume manager. These functions include i aggregation of the storage devices ii aggregation of storage bandwidth of the storage devices and iii reliability guarantees such as mirroring and or parity RAID .

The file system illustratively may implement the write anywhere file system having an on disk format representation that is block based using e.g. 4 kilobyte KB blocks and using index nodes inodes to identify data containers and data container attributes such as creation time access permissions size and block location and lock information . The file system uses data containers to store meta data describing the layout of its file system these meta data data containers include among others an inode data container. A data container handle i.e. an identifier that includes an inode number inum may be used to retrieve an inode from storage device.

Broadly stated all inodes of the write anywhere file system are organized into the inode data container. A file system fs info block specifies the layout of information in the file system and includes an inode of a data container that includes all other inodes of the file system. Each logical volume file system has an fsinfo block that is preferably stored at a fixed location within e.g. a RAID group. The inode of the inode data container may directly reference point to data blocks of the inode data container or may reference indirect blocks of the inode data container that in turn reference data blocks of the inode data container. Within each data block of the inode data container are embedded modes each of which may reference indirect blocks that in turn reference data blocks of a data container.

Operationally a request from the client is forwarded as a packet over the computer network and onto the node where it is received at the network adapter . A network driver processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the writ anywhere file system . Here the file system generates operations to load retrieve the requested data from storage device if it is not resident in core i.e. in memory .

If the information is not in memory the file system indexes into the inode data container using the inode number inum to access an appropriate entry and retrieve a logical vbn. The file system then passes a message structure including the logical vbn to the RAID system the logical vbn is mapped to a storage device identifier and storage device block number storage device dbn and sent to an appropriate driver e.g. SCSI of the storage device driver system . The storage device driver accesses the dbn from the specified storage device and loads the requested data block s in memory for processing by the node. Upon completion of the request the node and operating system returns a reply to the client .

It should be noted that the software path through the operating system layers described above needed to perform data storage access for a client request received at node may alternatively be implemented in hardware. That is in an alternate embodiment of the disclosure the storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an ASIC. This type of hardware implementation increases the performance of the file service provided by node in response to a file system request issued by client .

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a node implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows XPO or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the disclosure herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this disclosure can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and a disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system the teachings of the present disclosure may be utilized with any suitable file system including a write in place file system.

In the illustrative embodiment the storage server is embodied as D Module to service one or more volumes of array . In addition the multi protocol engine is embodied as N Module to i perform protocol termination with respect to a client issuing incoming data access request packets over the network as well as ii redirect those data access requests to any storage server of the cluster . Moreover the N Module and D Module cooperate to provide a highly scalable distributed storage system architecture of the cluster . To that end each module includes a cluster fabric CF interface module A and B adapted to implement intra cluster communication among the modules including D Module to D Module communication for implementing the embodiments described herein.

The protocol layers e.g. the NFS CIFS layers and the iSCSI FC layers of the N Module function as protocol servers that translate file based and block based data access requests from clients into CF protocol messages used for communication with the D Module . That is the N Module servers convert the incoming data access requests into file system primitive operations commands that are embedded within CF messages by the CF interface module for transmission to the D Modules of the cluster . Notably the CF interface modules A and B cooperate to provide a single file system image across all D Modules in the cluster . Thus any network port of an N Module that receives a client request can access any data container within the single file system image located on any D Module of the cluster.

Further to the illustrative embodiment the N Module and D Module are implemented as separately scheduled processes of storage operating system however in an alternate embodiment the modules may be implemented as pieces of code within a single operating system process. Communication between an N Module and D Module is thus illustratively effected through the use of message passing between the modules although in the case of remote communication between an N Module and D Module of different nodes such message passing occurs over the cluster switching fabric .

A known message passing mechanism provided by the storage operating system to transfer information between modules processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection methods functions constituting a CF application programming interface API . Examples of such an agnostic protocol are the SpinFS and SpinNP protocols available from NetApp Inc.

The CF interface module implements the CF protocol for communicating file system commands among the modules of cluster . Communication is illustratively effected by the D Module exposing the CF API to which an N Module or another D Module issues calls. To that end the CF interface module is organized as a CF encoder and CF decoder. The CF encoder of e.g. CF interface A on N Module encapsulates a CF message as i a local procedure call LPC when communicating a file system command to a D Module residing on the same node or ii a remote procedure call RPC when communicating the command to a D Module residing on a remote node of the cluster . In either case the CF decoder of CF interface B on D Module de encapsulates the CF message and processes the file system command.

As mentioned above file system includes the lock manager that maintains locks for clients for providing access to data containers. The lock manager maintains the lock data structure that is used for recovering locks when a node that interfaces with a client system becomes unavailable and another node takes over the storage space managed by the node that became unavailable. The failover approach and handling locks are now described below in detail.

When node A comes back on line and becomes available again then node B provides storage back to node A such that node A may provide access to client A. This process is referred to as give back .

The lock manager A for node A maintains the lock data structure A while the lock manager B for node B maintains its lock data structure B. Lock data structure A includes information for all the locks that are granted by node A and may also include lock state information for locks that are granted by node B to client B. Similarly lock data structure B includes information for the locks granted by node B and may also include lock state information for locks that are granted by node A to client A.

As an example lock data structure A may include various fields A E that may be used to recover a lock and may be referred to as lock state . A client identifier A identifies a client system that has been granted a lock. A reconnect key B is used by a client to recover a lock for a persistent open lock type. The data container for which the lock is granted is identified by C. The storage volume associated with the data container is identified by D. A reserved field E may be used for any other information.

In block B node A prepares the lock state at the local memory of node A for example i.e. lock state information is attached based on the lock type. The lock state would depend on the lock type as described below in detail.

In block B the lock manager A determines if the lock needs to be mirrored to the partner node B. The term mirrored as used herein means being replicated. A lock that has a persistent open feature will need to be mirrored according to one embodiment. The lock manager A determines by evaluating the lock request whether it includes a request for a persistent open. If the request includes a request for a persistent open then the lock manager A decides to mirror the lock state to lock manager B so that lock data structure B can be updated as described below. If the lock is not to be mirrored in block B then the process moves to block B that is described below.

If the lock is to be mirrored then in block B the lock manager A determines if local or partner mirroring is needed. If local or partner mirroring is not needed then the process moves to block B. The term local mirroring as used herein means lock state mirrored at the local node for example A for a partner node as described below in detail. The term partner mirroring means that the lock is replicated at a partner node for example B which is a partner node of node A.

If local or partner mirroring is needed then mirrored lock state information is prepared in block B. The mirrored lock state information would depend on the lock type. For example for a shared lock the lock state information would include client identifier information A reconnect key B data container identifier C and volume identifier D where the data container is stored or any other information. The reconnect key B is typically provided to client A for a persistent open lock. The client A uses the reconnect key when node A becomes unavailable and client A needs to use the same lock either when node A becomes available again or if node B takes over.

In block B node A determines if node B is available. Node A may determine this by sending a status message to node B using connection . If the partner B is unavailable then the process moves to block B described below.

If partner B is available then in block B the lock manager A determines if the lock state may already have been mirrored. If the lock state has already been mirrored then in block B lock manager A determines if the mirroring was successful. This is typically determined by receiving a status messages from node B. If the mirroring succeeded then the process moves to block B. If the mirroring had not succeeded then in block B the process ends and an error message is sent to client A.

If the mirroring has not occurred in block B then in block B the lock state i.e. mirrored lock state information i.e. fields A E is sent by node A to node B. The lock state information includes minimal information that can be used by node B to generate a lock if node A became unavailable.

In block B node B creates or updates the mirrored lock information at lock data structure B. Node B then sends a status to node A in block B.

Referring to block B the lock state is committed to storage device of node A from memory . Thereafter in block B determines if mirroring of any lock states is needed at node A. This may happen if node A has taken over B and lock state on B s storage is changed. Node A may take over node B if node B becomes unavailable for some reason. Local mirroring may be needed when locks are created at a partner node or are changed at partner node while the node is in the takeover stage.

If local mirroring is not needed then the process ends in block B. If local mirroring is needed then in block B node A creates the lock state information at lock data structure A.

In block B node B sends a message to node A indicating that node A has valid lock state and node B needs mirrored lock state.

In block B as part of a give back procedure node A sends mirrored lock state information for all locks that it has to node B. The lock states that are sent to node B may include the locks that are owned by node A and the locks that were owned by node B before the rebooting in block B.

In block B node B generates locks based on the mirrored lock state information. These include locks that it owned before the reboot.

In block B node A indicates to node B that it owns valid locks for node A and B. Thereafter in block B node B marks the lock state information for node A and B locks as valid in lock data structure B maintained by lock manager B.

The embodiments disclosed herein have advantages because when a node becomes unavailable and another node takes over client access to data containers and locks is maintained because the partner node can construct the locks that were issued by the node that became unavailable. This is especially helpful in an environment where multiple clients are accessing data containers and using locks.

The CF message includes a media access layer an IP layer a UDP layer a reliable connection RC layer and a CF protocol layer . As noted the CF protocol is a generic file system protocol that conveys file system commands related to operations contained within client requests to access data containers stored on the cluster the CF protocol layer is that portion of message that carries the file system commands. Illustratively the CF protocol is datagram based and as such involves transmission of messages or envelopes in a reliable manner from a source e.g. an N Module to a destination e.g. a D Module . The RC layer implements a reliable transport protocol that is adapted to process such envelopes in accordance with a connectionless protocol such as UDP .

The processing system includes one or more processors and memory coupled to a bus system . The bus system shown in is an abstraction that represents any one or more separate physical buses and or point to point connections connected by appropriate bridges adapters and or controllers. The bus system therefore may include for example a system bus a Peripheral Component Interconnect PCI bus a HyperTransport or industry standard architecture ISA bus a small computer system interface SCSI bus a universal serial bus USB or an Institute of Electrical and Electronics Engineers IEEE standard 1394 bus sometimes referred to as Firewire .

The processors are the central processing units CPUs of the processing system and thus control its overall operation. In certain embodiments the processors accomplish this by executing programmable instructions stored in memory . A processor may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such devices.

Memory represents any form of random access memory RAM read only memory ROM flash memory or the like or a combination of such devices. Memory includes the main memory of the processing system . Instructions which implements techniques introduced above may reside in and may be executed by processors from memory .

Also connected to the processors through the bus system are one or more internal mass storage devices and a network adapter . Internal mass storage devices may be or may include any conventional medium for storing large volumes of data in a non volatile manner such as one or more magnetic or optical based disks. The network adapter provides the processing system with the ability to communicate with remote devices e.g. storage servers over a network and may be for example an Ethernet adapter a FC adapter or the like. The processing system also includes one or more input output I O devices coupled to the bus system . The I O devices may include for example a display device a keyboard a mouse etc.

The system and techniques described above are applicable and useful in the upcoming cloud computing environment. Cloud computing means computing capability that provides an abstraction between the computing resource and its underlying technical architecture e.g. servers storage networks enabling convenient on demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or service provider interaction. The term cloud is intended to refer to the Internet and cloud computing allows shared resources for example software and information to be available on demand like a public utility.

Typical cloud computing providers deliver common business applications online which are accessed from another web service or software like a web browser while the software and data are stored remotely on servers. The cloud computing architecture uses a layered approach for providing application services. A first layer is an application layer that is executed at client computers. In this example the application allows a client to access storage via a cloud.

After the application layer is a cloud platform and cloud infrastructure followed by a server layer that includes hardware and computer software designed for cloud specific services. Details regarding these layers are not germane to the inventive embodiments.

Thus a method and apparatus for managing locks have been described. Note that references throughout this specification to one embodiment or an embodiment mean that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the present disclosure. Therefore it is emphasized and should be appreciated that two or more references to an embodiment or one embodiment or an alternative embodiment in various portions of this specification are not necessarily all referring to the same embodiment. Furthermore the particular features structures or characteristics being referred to may be combined as suitable in one or more embodiments of the disclosure as will be recognized by those of ordinary skill in the art.

While the present disclosure is described above with respect to what is currently considered its preferred embodiments it is to be understood that the disclosure is not limited to that described above. To the contrary the disclosure is intended to cover various modifications and equivalent arrangements within the spirit and scope of the appended claims.

