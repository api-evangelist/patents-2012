---

title: Replication in distributed caching cluster
abstract: A cache cluster is configuration-aware such that client initialization, access to replicated cached data and changes to the underlying structure of the cache cluster can be dynamically updated. For example, a management system monitoring a cache cluster notices a large number of requests for a key that causes a significant load on a first memory caching node. To reduce the load on the first memory caching node, the management system may cause cached data related to the key to be replicated to a second memory caching node. A configuration stored in one or more of the memory caching nodes may be updated by the management system to allow both memory caching nodes to serve the requests for the key to clients.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09262323&OS=09262323&RS=09262323
owner: Amazon Technologies, Inc.
number: 09262323
owner_city: Reno
owner_country: US
publication_date: 20121126
---
This application is related to and incorporates by reference for all purposes the full disclosure of co pending U.S. patent application Ser. No. 13 685 596 filed concurrently herewith entitled DISTRIBUTED CACHING CLUSTER CONFIGURATION co pending U.S. patent application Ser. No. 13 685 607 filed concurrently herewith entitled DISTRIBUTED CACHING CLUSTER CLIENT CONFIGURATION and co pending U.S. patent application Ser. No. 13 685 615 filed concurrently herewith entitled DISTRIBUTED CACHING CLUSTER MANAGEMENT .

Data centers provide computing resources for use by one or more clients. These services may include computing storage and networking services. For example a data center may provide a machine to host an application storage to store application data cache to quickly respond to repeated data requests and networking to enable communication between resources. By making use of the data center services a customer may pay for computing and or resource use rather than purchasing anticipated hardware needs. This enables a customer to expand and contract use of computing services according to demand. For example an application may be configured to request more storage as needed rather than a developer or administrator monitoring and anticipating use.

On demand systems may be used to reduce the number of times a database must be read by caching data and objects from the database. For example one implementation uses a client centered architecture where a client knows the servers but the servers are not known to each other. To read or set a key the client uses a hash to determine which server to contact and contacts that server. The server then calculates a second hash to determine where to store or read the corresponding value. Additions or subtractions to the group of servers are managed by the client.

In the following description various embodiments will be described. For purposes of explanation specific configurations and details are set forth in order to provide a thorough understanding of the embodiments. However it will also be apparent to one skilled in the art that the embodiments may be practiced without the specific details. Furthermore well known features may be omitted or simplified in order not to obscure the embodiment being described.

Techniques described and suggested herein include enabling a cache cluster to be configuration aware such that initialization access to replicated cached data and changes to the underlying structure of the cache cluster can be dynamically updated. For example a cache cluster may comprise a set of memory caching nodes that cache data retrievable via keys. For example a cache cluster may upon receipt of a key by a node in the cluster return a value i.e. data corresponding to the key. A management system may monitor the memory caching nodes and key value pairs stored within for reasons to improve performance and or durability of the cached data. Frequently accessed cached data in one embodiment may be replicated to other memory caching nodes to improve performance and reduce the risk of frequently cached data being lost if a memory caching node fails. Data that causes significant effort to produce in another embodiment may also be replicated to improve durability and therefore faster availability and reduce the change of incurring the effort if a memory caching node fails. The management system may then record this replication of cached data in a configuration. A configuration may be a collection of information that comprises an indication of the number of memory caching nodes in the cache cluster the assignment of key value pairs to memory caching nodes the network location of the memory caching nodes and or other aspects of the cache cluster. The configuration may then be stored in one or memory caching nodes to notify clients about the new locations for replicated cached data in memory caching nodes. Clients in some embodiments through use of a client driver may retrieve and update their configuration from a memory caching node containing the configuration. The client may then use either or both of the original memory caching node and memory caching node storing the replicated data to access the cached data.

The use of replicated data may be done for many varied purposes. In one embodiment cached data is replicated to a memory caching node to serve as a backup of the cached data that is used if one or memory caching nodes fails that hosts at least part of the cached data. Otherwise in this embodiment the backup memory cached node is used during normal operation. The backup memory caching nodes may be noted as such in the configuration such that the backup memory caching nodes may be accessed in place of a failed memory caching node. In another embodiment one or memory caching nodes may be allocated to serve frequently accessed keys. Keys surpassing a threshold frequency may be replicated as part of a key value pair to one or more memory caching nodes that serve frequently accessed keys. In some embodiments a client is notified of access to a frequently accessed key as part of the response to a request for cached data such as a frequently accessed flag. In other embodiments the frequently accessed keys and replicated memory caching node locations are disclosed in the configuration. Clients of the cache cluster may then retrieve the frequently accessed keys from either of the original location or the assigned one or more memory caching nodes that serve the frequently accessed keys. In some embodiments once the cached data is replicated an original server may refuse and or forward requests for frequently accessed cached data to a memory caching node hosting the replicated data to avoid too high of a server load.

Requests for replication may be made by clients of the cache cluster such as through a client driver a management system or memory caching nodes. For example a client driver may detect that a memory caching node returns a flag indicating that a key is frequently accessed. The client driver may request a second memory caching node or even a new memory caching node be provisioned also host the frequently accessed key value pair associated with the key. The client may update a configuration and send it to one or more memory caching nodes in a cache cluster to insure that other client drivers may make use of the replicated key value pair. In another example a management system may monitor the memory caching node cause the key value pair replication update the configuration and send the configuration to memory caching nodes in the cache cluster to be stored. In one example a memory caching node may determine that access to a key is beyond a threshold amount and request that a second memory caching node or a new node be provisioned also cache the key value pair associated with the key. The memory caching node may update a configuration and propagate the configuration to other memory caching nodes.

A consistent hashing algorithm may be used to distribute keys among the memory caching nodes and form a key space based on the configuration received. In one embodiment a memory caching node is assigned one or more angles or points along a circle. The assignment of angles or points along the circle may be determined by multiple factors including size of the memory caching node frequency of requests for keys in the assigned portion of the key space and other factors. A hashing algorithm may be used to determine where a key falls along the circle. The point or angle where the key falls may then be traced in a predefined direction e.g. clockwise around the circle until a first memory caching node assignment is encountered. The first memory caching node assignment encountered determines which memory caching node services the key. By using a consistent hashing algorithm such as described cache invalidation may be minimized and caching node placement may be targeted. Furthermore if each node is allowed to service multiple points or angles a load of a lost memory caching node may be distributed among multiple remaining memory caching nodes if each assigned point or angle of a lost memory caching node is followed by an assignment of a different memory caching node. In some embodiments divisions of the circle may be pre computed to associate memory caching nodes with certain arcs of the circle. With replication multiple memory caching nodes may be assigned one or more portions of a circle arc. For example new nodes added to the cluster may be given one or more assignments of the available key space. This assignment may include replicating a portion of a key space assigned to another memory caching node. The new node may then be added to a configuration and sent to be stored in some or all of the memory caching nodes in the cache cluster. In some embodiments a client driver determines which replicate of cached data to request. In one example any memory caching node assigned replicated data may be used. In other embodiments a preferred memory caching node assigned replicated data may be used. In one embodiment an algorithmic choice such as round robin or random may be used to select which memory caching node to use to access the replicated data.

In some embodiments a caching system may comprise a cache cluster and a management system that manages the cache cluster for example by may determining storing and or maintaining configurations of one or more cache clusters including a distribution of keys stored by the cache clusters. The cache cluster may comprise a set of one or more memory caching nodes that store key value pairs and a configuration. The configuration may be maintained in one embodiment at each of the set of one or more memory caching nodes. As such each memory caching node by having the configuration for the cache cluster have information indicating the relationship of every other memory caching node in the cache cluster. When a management system in detects a large number of requests for a key that causes a significant load on a first memory caching node the management system may act to reduce the load. To reduce the load on the first memory caching node in one embodiment the management system may cause cached data related to the key to be replicated to a second memory caching node. A configuration stored in at least some of the memory caching nodes may be updated by the management system to allow both memory caching nodes to serve the requests for the key to clients.

In one illustrative example a cache cluster is managed by a management system. The management system may determine store and or maintain configurations of one or more cache clusters including a distribution of keys stored by the cache clusters. In addition the management system may be logically connected to distributed computing resources via a network. The management system may be configured to provision the computing resources as memory caching nodes. The management system may also be configured to associate the provisioned memory caching nodes with any of the one or more cache clusters. In an embodiment the management system may also be configured to wind down a memory caching node to replicate keys stored by the memory caching node and to remove the computing resources from the cache cluster to a general pool of distributed computing resources. When a cache cluster is updated such as a change to key distribution among the memory caching nodes the configuration of the cache cluster may be updated in the management system and sent to one or more of the memory caching nodes. As such each memory caching node may have a current version of the configuration of the cache cluster.

In another embodiment a cache cluster may cache data and objects to reduce accesses to a data source. The cache cluster may include one or more memory caching nodes. Each node may store a portion of a set of cached data. The cached data may be split between memory caching nodes based on keys that are used to retrieve an element of data from the cache cluster. Data sources may include a database application programming interface API or other data store or data source. The memory caching nodes may use computer memory as storage for cache such as RAM to speed up responses to requests.

In one example a cache cluster has three memory caching nodes servicing an application making cache requests to a database. Because of a large number of requests for a range of keys a set of key value pairs associated with the keys may be duplicated on a second of the three memory caching nodes to aid in the servicing of the large number of requests for the range of keys. After duplicating the set of key value pairs on the second memory caching node a configuration for the cache cluster may be updated to incorporate the availability of the keys on the second memory caching node. The updated configuration may be distributed to clients of the cache cluster to minimize caching disturbances. In one embodiment the clients may receive the updated configuration from a memory caching node without contacting the configuration endpoint. This may include storing the configuration in the three memory caching nodes for clients to retrieve.

A configuration to a client may be delivered in multiple ways. In one embodiment a pre defined configuration endpoint can include an alias to a memory caching node that makes available a configuration used to connect to the cache cluster and retrieve cached data such as using keys to retrieve values of the key value pairs. The configuration endpoint may be statically declared such that support software including a client driver for the client system may be able to resolve an alias provided by the configuration endpoint to a memory caching node from which a configuration may be obtained. An example of a configuration endpoint may be a static hostname that resolves to an active memory caching node through domain name services. The configuration can describe how to access caching resources of the cache cluster including the key value pair distribution among memory caching nodes and the distribution of replicated data. This static alias to a memory caching node allows for new clients to start and self configure to current cache cluster conditions rather than manually configure a new client before starting a new client. In another embodiment the memory caching instances can also contain reserved space for storing configuration describing the structure of the cache cluster. While normal caching of data such as the key value pairs can cause data to expire a section of storage may be partitioned such that configuration may be stored in such section as reserved configuration space without worry of deletion. This internal configuration stored by a memory caching instance allows a client already in communication with a memory caching instance to also request a current configuration if newer than the current configuration of the client. By providing the current configuration caching resources may be altered with the expectation that clients will update upon access of a memory caching node.

For example when a client is first attempting to connect to the cache cluster the client may resolve an alias from a static configuration endpoint to a memory caching node in a cache cluster. The client may request initial configuration from the memory caching node. As additional requests are made to the cache cluster to retrieve key value pairs a first memory caching node may change in one or more ways such as being added or removed from the cache cluster . As such key value distribution may change including replication of one or more keys between memory caching nodes. The configuration may be updated and provided to each of the set of one or more memory caching nodes in the cache cluster. The client may then obtain the updated configuration from a memory caching node that has the current configuration of the cache cluster. As such updated configurations are easily obtained by a client without having to resolve a memory caching node via the configuration endpoint after a node has been resolved.

A client already in communication with a memory caching node may request a current configuration from a memory caching node. In one embodiment the request is made if the configuration stored in the memory caching node has been updated from the current configuration that the client has. In another embodiment the client may request configurations on a periodic random or other time basis from one or more memory caching nodes. In another embodiment instead of responding to a request from a client new configurations may be pushed to the client by the memory caching node upon the event of a configuration change or any other change to the cache cluster. Further still the configuration of the cache cluster may be sent out to the client by the memory caching node based on time intervals such as a periodic heartbeat expiration or random time basis. In an additional embodiment configurations may be pushed to a client when a client makes a request to access the cache cluster. By providing the current configuration memory caching nodes may be altered with the expectation that clients will update upon access of a memory caching node.

The term provisioning is meant to be read broadly to include the assignment of a computing resource to a use. In some embodiments this includes preparing the computing resource for use. In one embodiment a provisioning of a memory caching node would include the assignment of a server installation of an operating system installation and configuration of the memory caching node and enabling the memory caching node for use. For example a provisioning system may select a server to use as a memory caching node. The provisioning system may then create a workflow that prepares the server for use as a memory caching node. As part of the workflow a machine image may be loaded on the server. The machine image may include operation system memory caching software and or settings. After loading the machine image the server may be caused to boot into the operating system and receive any further software and or settings. Such settings may include cache cluster configuration. After provisioning is complete the server may be turned over to a management system for use as a memory caching node.

Turning now to an illustrative example of a caching system in accordance with at least one embodiment is shown. A memory caching node may manage cached key value pairs respond to requests to provide cached values and provide a configuration identifying how to communicate with the cache cluster. The caching system may include a configuration endpoint that comprises an alias for one or more memory caching nodes that form a cache cluster. The cache cluster may comprise a plurality of memory caching nodes . One or more of the memory caching nodes may serve cached data from a cache space and configuration from a reserved configuration space . In one embodiment a cache cluster may include multiple memory caching nodes . The memory caching nodes may each comprise a virtual machine executing on a computing device such as a server. In another embodiment a cache cluster may include multiple memory caching nodes executing directly on hardware. As such it should be recognized that the programs and processes referenced herein may be run on actual hardware virtual machines or combinations thereof.

A management system may cause a key value pair to be duplicated between two or more memory caching nodes . Based on the duplication the control plane may provide a corresponding configuration that configures a client driver to retrieve the duplicated key value pair from any memory caching node having the duplicated key value pair. It should be recognized that the computing resources shown may be run on actual hardware or virtual machines. For example a monitoring system of a management system responsible for the management and observation of memory caching nodes and configuration endpoint determines that a request load of a first memory caching node is too high. To distribute the load the management system through a provisioning system may cause the key value pair to be copied to a second memory caching node such that both memory caching nodes service the key value pair . The provisioning system may update the configuration and distribute the updated configuration to the memory caching nodes . A client driver may retrieve and install the updated configuration . Using the configuration the client driver may service requests from an application for data found in the memory caching nodes . For duplicated key value pairs the client driver may select any memory caching node identified in the configuration that services the key value pair . The selection may be based on algorithmic random memory caching node load or other criteria.

The memory caching instance may receive key values to store within the cache space . The key values may have an expiration time as well as early expiration depending on if the memory caching instance runs out of cache space . In some embodiments a least frequently used algorithm is used to determine which items are expired early when the cache is full. The memory caching instance may also provide storage for a configuration detailing communication parameters with the distributed memory caching system. In some embodiments this configuration may be stored in a reserved configuration space that is not subject to expiration. In one embodiment the configuration is stored in the cache space but a client driver or management system ensures frequent enough requests and or updates to ensure the configuration is available. The configuration may be overwritten and or updated to keep current with changes to the distributed memory caching system .

A management system may be a set of one or more computing resources that manage the memory caching nodes and or the configuration endpoint . The management system may monitor the memory caching nodes and compute indicators that describe the status and or interaction of resources managed by the management system such as the memory caching nodes . Examples of indicators include request load cache load available bandwidth server health and durability. These indicators may be based on measurements including requests over time amount of cached used data transfer hardware data and number of duplicated data sets. The management system may take actions based on the indicators from the monitoring system such as through a provisioning system. The provisioning system may take actions that act on memory caching nodes such as provision de provision move and replace memory caching nodes . The provisioning system may also be responsible for other resources such as key value pair placement on memory caching nodes including the duplication of key value pairs and a configuration endpoint . A configuration describing how to access the resources may be created by the provisioning system in response to changes made by the provisioning system. A client driver such as a code library may use the configuration to dynamically adjust to changes in the cache cluster such that the client driver can field requests from application without notifying the application that the configuration has changed.

A configuration endpoint may also be provided to aid in configuration retrieval. In some embodiments a configuration endpoint is a static resource that may be directly referenced for an alias to a memory caching node to use in configuration retrieval. For example new client drivers may be initialized with a hostname of the configuration endpoint . Upon instantiation of a new client the client may connect with the configuration endpoint resolve an alias to a memory caching node and retrieve the configuration from the memory caching node . By providing the configuration endpoint clients may self configure rather than start with a list of memory caching nodes that may become obsolete and require maintenance. In some embodiments a client may continue to retrieve a more current configuration by requesting the configuration from the memory caching instance . In one embodiment the configuration endpoint is implemented by domain name system DNS services. A client may request a static hostname from the domain name server and receive an IP address that resolves to a memory caching node.

In some embodiments a caching node may store data from a neighboring node such that if the neighboring node is lost the cache from the neighboring node is not completely lost. In one embodiment space in each caching node may be reserved for replication of key value pairs and of a neighboring caching node as seen in . In the embodiment shown caching nodes and comprise a cache space reserved configuration space and reserved replication space . The cache space may be used by the caching node and to store key value pairs and assigned to the memory caching node and . The reserved configuration space may be used to store and retrieve configuration of the cache cluster as previously described. The reserved replication space may be used to store key value pairs and that have been replicated from another memory caching node and . In an embodiment a portion such as a percentage of a memory caching node and may be reserved for replication of key value pairs and of another memory caching node and .

For example a replication circle may be constructed where each memory caching node replicates a portion of its neighbor. Memory caching node may be assigned a set of key value pairs that includes a subset of key value pairs . The subset of key value pairs may be identified to replicate to memory caching node B . The identification may include measurements of frequency of access of key value pairs cost of recomputing of key value pairs cost of loss of key value pairs or other measures of value of the key value pairs. After identification the key value pairs may be replicated from memory caching space of caching node A to reserved replication space of caching node B . The total number of key value pairs replicated may be based on space available in reserved replication space a percentage of key value pairs in caching node A accesses to key value pairs value of key value pairs or other determinations. Key value pairs from caching node B may also be identified and replicated to caching node C . Key value pairs from caching node C may be identified and replicated to caching node A . A further example of this cache circle is discussed as related to consistent hashing in the description of .

Using this replication system if a memory caching node fails a portion of the cache of the memory caching node remains in the reserved replication space . For example if memory caching node B fails the replicated key value pairs continue to exist in the reserved replication space of memory caching node C . This replication allows for a fail over scenario where memory caching node C may serve at least a portion of memory caching node B s cache space . In one embodiment a replacement node for memory caching node B may be provisioned by a management system . The cache space of the replacement memory caching node may then be populated by using the key value pairs from the reserved replication space of memory caching node C .

Turning now to an illustrative example of a distributed memory caching system in accordance with at least one embodiment is shown. A client system uses a client driver to retrieve information from a cache cluster managed by a management system . In the embodiment shown a client system includes an application that retrieves data from the cache cluster . The client system uses a client driver to manage the interface between the application and the cache cluster . For example the application may be a shopping website and the client driver may be a library that exposes the caching functionality through function calls and or an application programming interface API .

The client driver may manage the communication with the cache cluster . In one embodiment the client driver supports automatic configuration. An initial configuration of the client driver may be small such as a hostname of a configuration endpoint that provides an alias at which configuration may be obtained. In one embodiment the alias is provided as part of the configuration endpoint acting as a domain name server. The configuration may include information needed for the client driver to connect to and use the cache cluster . For example an application may provide a hostname and or address of a configuration endpoint to the client driver as part of initializing the client driver . Using the given hostname and or address the client driver contacts the configuration endpoint to resolve an alias to a memory caching node and requests a configuration from the memory caching node . In one embodiment this configuration is stored in a reserved memory space of the memory caching node that is not subject to certain cache rules of a protocol implemented by the cache node such as the memcached protocol and variations thereof such as eviction. Information in the reserved memory space may be accessed according to an extension to a standard caching protocol such as memcached. Upon receiving the configuration the client driver may load the configuration . Once loaded the client driver may verify the configuration . In one embodiment the client driver contacts one or more memory caching nodes and verifies the version of the configuration against a configuration version contained in a second memory caching node . The client driver may use the most recent configuration discovered. The client driver may then act upon requests for data from the application by requesting the data from one or more memory caching nodes that store the data in their cache . Periodically the client driver may check the loaded configuration version against configuration versions stored by the memory caching nodes . The client driver may elect to use the newest configuration discovered which may be the current version loaded in the client driver . By loading the configuration the client driver can react to dynamic changes in the cache cluster . For example the configuration may identify memory caching nodes that are added or removed from the cache cluster . By loading the configuration the client driver may react to any changes in the cache cluster infrastructure without instruction from the application .

Loading the configuration may synchronize with other computing systems a client driver s information about the cache cluster. In one embodiment several client drivers exist at the same time to serve multiple instances of a web application each on its own server. Synchronizing a configuration of the cache cluster allows each client driver to properly populate and request information from memory caching node that form the cache cluster . For examples of populating and cache requests of memory caching nodes see and the associated description.

The client driver and caching nodes may communicate using a standard protocol such as a memcached protocol and extensions to the protocol. For example caching operations may use the standard protocol while configuration operations may use extensions to the protocol such as additions to a command set of the protocol. In some embodiments the extension operations operable on the reserved configuration storage may include create retrieve update and destroy operations. Other extension operations may include a get configuration version operation other metadata manipulation operations and a propagate configuration request.

A management system may be one or more computing resources responsible for management of other systems. In the management system is responsible for the distributed memory caching system including the provisioning and monitoring of memory caching nodes in the cache cluster . The management system may also receive instructions from customers and or administrators such that the management of the management system fits the needs of the customer and or administrator. For example a management system may be responsible for a set of memory caching nodes that form the cache cluster . The management system through a provisioning system as an example may cause new memory caching nodes to be instantiated or current memory caching nodes to be stopped. The management system may also be responsible for monitoring the cache cluster which may include monitoring the set of memory caching nodes for indicators. The indicators may include usage failure or other information about the use and or underlying systems. A configuration endpoint may also be maintained by the management system to ensure that an alias to an active memory caching node that can provide configuration is always available.

In one embodiment the management system may use a monitoring system and react to perceived problems with the caching service . For example if a failure occurs in a failed memory caching node the failed memory caching node may be de provisioned and removed from the cache cluster . A new memory caching node may be provisioned to replace the failed memory caching node and recover from loss of the failed memory caching node. In other examples the failed memory caching node may be repaired by replacing reinitializing and recovering the memory caching node. Using the changes made to the cache cluster the management system may update the configuration and cause the updated configuration to be stored in each memory caching node . If needed the alias provided by the configuration endpoint may also be updated. In another example the management system provisions a new memory caching node due to an increase in demand for cached data. The management system may update the configuration with the connection information to the new memory caching node and cause the configuration to be stored in memory caching nodes .

A configuration may include information necessary to connect to the cache cluster . In some embodiments that use a direct connecting configuration this may include information to directly connect to each memory caching node . In other embodiments using a request forwarding configuration the configuration may identify a memory caching node responsible for the forwarding of requests to a memory caching node that holds the data in cache . In one embodiment a hybrid approach may be taken where direct connection and request forwarding are both available.

Turning now to an illustrative example of a distributed memory caching environment in accordance with at least one embodiment is shown. The client application in management system in and configuration endpoint in may exist in the context of a data center. The computers may be divided into trusted computing resources within the data center and untrusted external computing systems sometimes referred to as application clients outside the data center . Inside the data center computing resources and networking may be under the domain and control of known administrators and thus have trusted internal connections. Outside of the data center may be beyond the control of administrators and therefore untrusted such as the Internet .

Inside the data center may be memory caching nodes internal networking a management system a gateway a configuration endpoint and a client application . An memory caching node may be connected to other memory caching nodes through internal networking . The memory caching nodes may also be connected with a management system . The management system may receive requests to manipulate computing resources including provisioning resources and changing routing. The memory caching nodes and management system may also be connected with a gateway . The gateway may filter and route external traffic to a client application such as HTTP traffic to Web servers. For example a client application may communicate with external systems but memory caching nodes are not allowed external communications.

Outside the data center may be any of a number of different components or environments and may include the Internet and various external computing systems such as desktops laptops and mobile devices such as electronic book readers mobile phones tablet computing devices etc. The systems may be viewed as untrusted because the systems may not be administered by a trusted administrator. Further the communication channels such as the Internet are not controlled by a trusted administrator. Thus a message from an external computing system may be intercepted counterfeited and or exploited.

In some cases and for protective reasons client applications on a secure internal network may only be given the Internet access required to operate if any at all. For example a Web server in a data center may only receive outside traffic on port because a gateway provides access controls to the secure internal network that prevent all other Internet traffic from directly reaching the Web server. In another example a memory caching node on a secure internal network may not be connected to the Internet because it is only queried by a local Web server over the secure internal network. In other embodiments a client application may be behind a load balancer which may occasionally direct Internet requests to the client application .

Turning now to an illustrative example of a distributed memory caching management environment in accordance with at least one embodiment is shown. A management system may monitor and or manage memory caching node . Memory caching node may manage cached key value pairs respond to requests to provide cached values from the key value pairs and provide a configuration identifying how to communicate with the cache cluster and or each memory caching node . Key value pairs may be inserted into a cache of the memory caching node when read and or changed from a data store . The cache cluster allows potentially quicker responses to frequently accessed and or high access cost data than requesting data directly from the data store .

A memory caching node may be provided that includes cache space and reserved memory space . The memory caching node may be serviced by virtual and or physical hardware including a virtual machine. The memory caching node may receive key values pairs to store within the cache space . The key values may have an expiration time as well as early expiration depending on whether the memory caching node runs out of cache space . The memory caching node may use an algorithm to determine which key value pairs may be expired early. In some embodiments a least frequently used algorithm is used to determine which items are expired early when a cache is full. In other embodiments a cost of querying the data store may be factored in. In one embodiment the expiration may be based on which key value pairs are not expected to be frequently accessed in the future. The memory caching node may also provide storage for a configuration detailing communication parameters with the cache cluster . In some embodiments this configuration may be stored in a reserved memory space that is not subject to expiration. In one embodiment the configuration is stored in the cache space but a client or management system ensures frequent enough requests and or updates to ensure the configuration is available. The configuration may be overwritten and or updated to keep current with changes to the cache cluster .

A configuration endpoint may also be provided to aid in configuration retrieval. In some embodiments a configuration endpoint is a static resource that may be directly referenced as an alias to a memory caching node for configuration retrieval. For example new clients may be initialized with a hostname of the configuration endpoint . Upon instantiation of a new client the client may connect with the configuration endpoint resolve an alias to a memory caching node and retrieve the configuration from the memory caching node . By providing the configuration endpoint clients may self configure rather than start with a list of memory caching nodes that may become obsolete and require maintenance. In some embodiments a client may continue to retrieve a more current configuration by requesting the configuration from a memory caching node referenced in the alias provided by the configuration endpoint or a memory caching node directly.

In one embodiment the management system assumes responsibility for the configuration . In another embodiment memory caching nodes may be cluster aware such that as new memory caching nodes are detected they may be added to the configuration . In another embodiment the management system may store an updated configuration in an identified memory caching node such as a memory caching node identified by an alias maintained by the configuration endpoint . Each memory caching nodes may then monitor the identified memory caching node for changes and download the configuration when it is determined that the configuration has changed. In some embodiments the identified memory caching node may distribute and or notify other memory caching node in the cache cluster of changes to configuration . By obtaining an updated configuration a client may adapt to dynamically changing memory caching nodes within the caching cluster .

A memory caching node may follow a protocol that includes rules governing cached data. In one embodiment the rules specify cache eviction upon a last recently used basis when the cache space is full. In another embodiment the rules allow cached data such as the key value pair to be associated with a time to live after which the data will no longer be available. In some embodiments the protocol governing cached data has been extended such that configuration stored in the reserved configuration space is not subject to the rules governing cache eviction and or time to live.

Turning now to an illustrative example of logical connections between components in accordance with at least one embodiment is shown. A purpose of the cache clusters using memory caching nodes may be to prevent a load on and or slow response from an API or data store such as a relational database NoSQL database and key value store. In the embodiment shown an application may cause a client driver such as through a library API call to retrieve a configuration from a memory caching node identified by an alias retrieved from a predetermined configuration endpoint . The configuration may include information to enable communication with data store and memory caching nodes . Upon configuring the communication the client driver may field requests from the application for data within the data store . The client driver may determine a memory caching node to contact to see if the data is in the cache of the memory caching node . If so the client driver may return the data to the application . If not the client driver may request the information from the data store directly. Because of the request the data store and or the client driver may cause the data to be stored in a cache of a memory caching node for future retrieval. In some embodiments during a request to a memory caching node the client driver may check and or be notified that a configuration change has occurred.

In some embodiments request forwarding may occur. For example a client driver may make a request for data to a first memory caching node that may forward the request to a second memory caching node . If the second memory caching node does not have the data requested in cache the second memory caching node may forward the request to the data store . The data store may return the requested data either through the same path or directly to the client driver . An advantage of request forwarding is that the client driver need not have a current configuration . However the delays may be more significant than direct communication with a memory caching node .

Turning now to an illustrative chart of distributed memory caching configuration process in accordance with at least one embodiment is shown. This process may be accomplished in one embodiment by computing resources such as those seen in including application client driver configuration endpoint and memory caching nodes . The configuration process may include three phases initialization use and reconfiguration . During initialization a client driver prepares to receive data from a cache on behalf of an application by obtaining a configuration . In the application gives the client driver a configuration endpoint identifier that identifies a configuration endpoint such as by hostname address or other identifying information. The client driver uses this configuration endpoint identifier to resolve an alias identifying a memory caching node comprising the configuration . The client driver requests the configuration from the memory caching node . The memory caching node may send the configuration to the client driver . The client driver may then load the configuration to enable communications with a cache cluster having memory caching nodes .

In some embodiments a configuration endpoint is ensured for high availability as new applications rely on the availability of the configuration endpoint alias. The configuration endpoint may be access restricted based on the request or an identifier associated with the request such as requesting IP address destination IP address and or credentials.

In the use phase the client driver may act as an interface between the application and the cache cluster . In some embodiments this interface may be done with an API and or code library. The application may send a request for data that is analyzed by the client driver to determine which memory caching node may have the requested data in its cache. The client driver may then send the request for data in a format recognized by the memory caching node . If the data is found within the memory caching node the memory caching node returns the data to the client driver . The client driver may then return the data to the application . However if the data is not found within the memory caching node the client driver s request may fail and or be redirected to the data store.

In some embodiments the data in a request may be serviced by more than one memory caching node in a cache cluster . In one embodiment this redundancy may be due to cached data that is expensive to recreate. In other embodiments this redundancy may be due to reducing a server load due to a collection of frequently accessed data. The client driver may use configuration information information from a management system regarding cache cluster request latency from a memory caching node and or other information or indicators to determine which memory caching node should be contacted for redundant information. In another embodiment a memory caching node is randomly selected if the data is available from two or more memory caching nodes .

In the reconfiguration phase the client driver ensures that its configuration is up to date by comparing its version with a version known to one or more of the memory caching nodes . In one embodiment a client driver may periodically send a request for configuration to one or more memory caching nodes . The contacted memory caching nodes may return a stored configuration which may be compared against a configuration used by the client driver . In another embodiment the client driver may request version information of the configuration from the memory caching node . The client driver may compare the version information retrieved against version information of a local configuration. If the retrieved information is a newer version the client driver may request the new version of the configuration . For example version information may be a combination of a serially incremented number and a timestamp. In some embodiments the client driver may receive an indicator from a memory caching node that a configuration has changed during the use phase such as a secondary return value.

Some or all of the process or any other processes described herein or variations and or combinations thereof may be performed under the control of one or more computer systems configured with executable instructions and may be implemented as code e.g. executable instructions one or more computer programs or one or more applications executing collectively on one or more processors by hardware or combinations thereof. The code may be stored on a computer readable storage medium for example in the form of a computer program comprising a plurality of instructions executable by one or more processors. The computer readable storage medium may be non transitory.

Turning now to an illustrative example of a process that may be used to manage memory caching node behavior in accordance with at least one embodiment is shown. This process may be accomplished by computing resources such as those seen in including application client driver configuration endpoint and memory caching nodes . During the monitoring of memory caching nodes one or more key value pairs are identified to make redundant. This redundancy may be used to distribute a high load among memory caching nodes and or increase durability of a cached key value pair. A second memory caching node may be selected and caused to service the identified key value pair. The second memory caching node may be identified by load use patterns durability or other attributes that make the memory caching node desirable. In some embodiments important key value pairs are stored on three or more memory caching nodes. Using the changes a new configuration may be determined . Using the new configuration memory caching nodes may be updated by receiving and storing the configuration. The configuration endpoint may also be updated by updating its alias.

For example durability of key value pairs that are costly to re calculate may be factored in a decision to make the key value pair redundant. An algorithm running on a monitoring system within the management system measures frequency of access to determine which key value pairs will be made redundant. Using the result of the algorithm the management system may cause a provisioning system to distribute the key value pairs across two or more memory caching nodes. After distributing the key value pairs the management system may then update a configuration and cause the configuration to be stored by memory caching nodes that form part of a cache cluster. In some embodiments this algorithm may be modified to weigh the costs of key value pairs such that costly and frequently accessed key value pairs may be made redundant. In some embodiments that require further redundancy multiple configuration endpoints may be used to increase durability.

In some embodiments the key value pair may be transferred rather than made redundant. For example a memory caching node under load may select a range of keys to offload to a second memory caching node. In some cases the memory caching node under load may have to continue servicing the range of keys until all or most of clients update their configuration.

In some embodiments configurations may be propagated between memory caching nodes. For example once a memory caching node receives a configuration the memory caching node may attempt to distribute the configuration to other memory caching nodes in the configuration. In this way the memory caching nodes may work in using peer to peer communication to propagate configuration to each memory caching node. In one embodiment memory caching nodes in a cache cluster may track nodes within the cache cluster such that changes to the cache cluster are monitored by the memory caching nodes themselves. A memory caching node that notices an addition or subtraction of a memory caching node or is the subject of the addition or subtraction may create a new configuration to distribute to the other memory caching nodes.

It should be recognized that the use of the term client driver does not necessarily refer to software that directly supports hardware. The client driver is code executed by a computing resource that at least manages communication between an application and a distributed cache cluster. In some embodiments this is accomplished by a library. For example a developer may call functions within a library to perform the phases seen and discussed in relation to .

It should be recognized that the use of the term memory caching node is used as a broad term that covers more than just the specific examples above. Other caching types are included in this term. Other examples of memory caching nodes include persistent caching systems and disk caching systems. In one embodiment a persistent caching system is used such that a cache state is saved to avoid losing the cache. In another embodiment a disk caching system may be used.

Keys Z Y X and W correspond to caching angles measured from a reference angle such as for example angles and shown on circle . The keys may be input into a hashing function that returns a corresponding caching angle. A memory caching node may be assigned at least one caching angle along circle . Larger memory caching nodes may be assigned more caching angles which may grant a larger coverage over the circle . It is understood that the number of hashes can differ per angle. For example memory caching node assignments to memory caching node A include caching angle and caching angle . A key is assigned to a memory caching node first encountered travelling clockwise around the circle from a caching angle corresponding to the key. For example caching angle determined from a hash of key Z is followed clockwise to the caching angle assignment of memory caching node A.

In a caching angle is shown to be measured clockwise from the reference angle . For example caching angle may have a smaller angle than caching angle as measured from the reference angle. To determine which memory caching node is responsible for a key the key is first processed through a hash function to determine a caching angle. The caching angle may then be followed clockwise until the first memory caching node assignment occurs. For example key X resolves to the caching angle shown. The caching angle is then swept clockwise along line until the first memory caching node assignment occurs at caching angle which is assignment number for memory caching node A. Therefore key X is assigned to memory caching node A. Similarly the caching angle of key Z sweeps to caching angle which is assignment of memory caching node A. For the same reasons Key Y is assigned to memory caching node A assignment because of the sweep to the assignment of caching angle . Key W is assigned to memory caching node B because sweeping clockwise arrives at the assignment of a caching angle assigned to assignment memory caching node B.

Memory caching node assignments may be accomplished by several different methods. In one embodiment the client driver comprises code configured to assign memory caching nodes within the mapping. In such an embodiment the client may be aware of the angles on the mapping that are hot and as such require an additional node to offload one or more requests. In another embodiment a management system may aid a client driver in assigning caching angles. For example a management system monitors the access of keys and determines an optimum placement of assignments to reduce server loads on memory caching nodes. The management system may be aware of one or more aspects of the cache cluster in general as well as added nodes that it may provide as hints to the client driver.

In another embodiment one or more clients monitor usage of the memory caching nodes. If needed a client may request provisioning of a new memory caching node to add to the cache cluster. For example a client may determine that a latency of a response from a memory caching node has increased beyond an acceptable threshold. As another example the client may query the memory caching node using a protocol extension or reviewing an access log the client determines that one or more keys are accessed with a frequency above a threshold. The client may then request that a provisioning system provision a new memory caching node. The client may then assign the memory caching node one or more caching angles.

In one example shown in a memory caching node C is added to the cache cluster illustrated in . Memory caching node C is able to support three caching angles caching angle assignment caching angle assignment and caching angle assignment . This ability to support three caching angles may be due to the size processing ability and or placement of memory caching node C. Further as illustrated nodes in the cluster may not necessarily be equidistant from each other in terms of the angular distances between them. Some nodes may be closer to each other than others in terms of angular distance in the representation in due to various factors in connection with the key spaces served by the nodes. In the example shown Key X and Key Z may be hot keys that are frequently accessed i.e. accessed with a frequency in a range designated as hot therefore causing the utilization of caching angle to be responsible for the high utilization of memory caching node A. A management system may cause a new memory caching node C to receive a caching angle assignment . Due to the new assignment of caching angle Key X may now be serviced by memory caching node C as a sweep of caching angles along line leads to caching angle that is assigned to memory caching node C assignment . Key Z may remain with memory caching node A due to an assignment of caching angle .

In another example an access history of Key W may be such that Key W should be serviced by more than one memory caching node. This replication of the key space may be due to load difficulty of calculation of the underlying cached value or other replication need. As shown in memory caching node C assignment has been assigned the same caching angle assignment as the caching angle assignment of memory caching node B assignment . Thus memory caching node B and memory caching node C share responsibility for the same key space. In some embodiments only a portion of the key space is noted as replicated.

In yet another example a management system may also determine that the key space covered by caching angle assigned to memory caching node B assignment should be smaller. A caching angle assignment to memory caching node C is added between assigned caching angles and . As may be noted in the range responsibility between caching angles and need not be symmetric. In some cases memory caching node C s range may be smaller than memory caching node A s range but may be more frequently accessed. Considerations such as range and frequency of access may be used to determine the assignment of caching angle assignments. It should be recognized that in each of the above embodiments the client driver may be in control of the determination of the location of the nodes and as such the management system may provide information which may be used by the client driver in making the determinations.

It should be recognized that while the memory caching angle assignments have been discussed in terms of three or less assignments actual use may be higher including hundreds thousands millions or more of caching assignments. The few assignments shown are for simplifying discussion.

While memory caching nodes have been used for illustration of various aspects of the present disclosure it should be recognized that the structures and processes described may also be more broadly applied to storage nodes and clusters of computing resources in general. For example a storage node may include a memory caching node databases and read replicas. In one embodiment membership information of a cluster of nodes is shared with clients of the nodes. For example the processes and structures may be used in database scaling. Configuration of read replicas may be stored in a configuration space on a database server. Clients of the database server may detect changes such as additions or subtractions to the read replicas by requesting the configuration from a read replica using client configuration update techniques described above. In another example the processes and structures may be used in database clustering. A cluster configuration may be stored in the database itself alongside the data that makes up the distributed data store of the cluster which may be retrieved by clients of the database. This allows the client initialization to be decoupled from server resources.

In some embodiments a node comprises memory caching space for keys assigned to the node configuration space for storing a configuration of the cache cluster and a replication caching space for storing replicated keys. The replication caching space may be cache key value pairs from other nodes such that loss of a node does not result in a loss of the cache of that node. In one embodiment replication may occur between adjacent nodes. Keys from a first memory caching node may be replicated to replication space of an adjacent memory caching node such that a memory caching angle may be followed to a second memory caching node to determine an adjacent node assigned to replication also seen in . For example in Key X may be serviced by a memory caching space of Node C Assignment as seen by the assignment arrow and following the arrow direction further leads to a second node of Node A Assignment which may store Key X in replication caching space. For a similar reason Key Z may be served by memory caching space of Node A assignment and continuing to follow the circle to the next memory caching angle would lead to the replication responsibility of Node B Assignment which may store Key Z in replication caching space.

The illustrative environment includes at least one application server and a data store . It should be understood that there can be several application servers layers or other elements processes or components which may be chained or otherwise configured which can interact to perform tasks such as obtaining data from an appropriate data store. As used herein the term data store refers to any device or combination of devices capable of storing accessing and retrieving data which may include any combination and number of data servers databases data storage devices and data storage media in any standard distributed or clustered environment. The application server can include any appropriate hardware and software for integrating with the data store as needed to execute aspects of one or more applications for the client device handling a majority of the data access and business logic for an application. The application server provides access control services in cooperation with the data store and is able to generate content such as text graphics audio and or video to be transferred to the user which may be served to the user by the Web server in the form of HTML XML or another appropriate structured language in this example. The handling of all requests and responses as well as the delivery of content between the client device and the application server can be handled by the Web server. It should be understood that the Web and application servers are not required and are merely example components as structured code discussed herein can be executed on any appropriate device or host machine as discussed elsewhere herein.

The data store can include several separate data tables databases or other data storage mechanisms and media for storing data relating to a particular aspect. For example the data store illustrated includes mechanisms for storing production data and user information which can be used to serve content for the production side. The data store also is shown to include a mechanism for storing log data which can be used for reporting analysis or other such purposes. It should be understood that there can be many other aspects that may need to be stored in the data store such as for page image information and to access right information which can be stored in any of the above listed mechanisms as appropriate or in additional mechanisms in the data store . The data store is operable through logic associated therewith to receive instructions from the application server and obtain update or otherwise process data in response thereto. In one example a user might submit a search request for a certain type of item. In this case the data store might access the user information to verify the identity of the user and can access the catalog detail information to obtain information about items of that type. The information then can be returned to the user such as in a results listing on a Web page that the user is able to view via a browser on the user device . Information for a particular item of interest can be viewed in a dedicated page or window of the browser.

Each server typically will include an operating system that provides executable program instructions for the general administration and operation of that server and typically will include a computer readable storage medium e.g. a hard disk random access memory read only memory etc. storing instructions that when executed by a processor of the server allow the server to perform its intended functions. Suitable implementations for the operating system and general functionality of the servers are known or commercially available and are readily implemented by persons having ordinary skill in the art particularly in light of the disclosure herein.

The environment in one embodiment is a distributed computing environment utilizing several computer systems and components that are interconnected via communication links using one or more computer networks or direct connections. However it will be appreciated by those of ordinary skill in the art that such a system could operate equally well in a system having fewer or a greater number of components than are illustrated in . Thus the depiction of the system in should be taken as being illustrative in nature and not limiting to the scope of the disclosure.

The various embodiments further can be implemented in a wide variety of operating environments which in some cases can include one or more user computers computing devices or processing devices which can be used to operate any of a number of applications. User or client devices can include any of a number of general purpose personal computers such as desktop or laptop computers running a standard operating system as well as cellular wireless and handheld devices running mobile software and capable of supporting a number of networking and messaging protocols. Such a system also can include a number of workstations running any of a variety of commercially available operating systems and other known applications for purposes such as development and database management. These devices also can include other electronic devices such as dummy terminals thin clients gaming systems and other devices capable of communicating via a network.

Most embodiments utilize at least one network that would be familiar to those skilled in the art for supporting communications using any of a variety of commercially available protocols such as TCP IP OSI FTP UPnP NFS CIFS and AppleTalk. The network can be for example a local area network a wide area network a virtual private network the Internet an intranet an extranet a public switched telephone network an infrared network a wireless network and any combination thereof.

In embodiments utilizing a Web server the Web server can run any of a variety of server or mid tier applications including HTTP servers FTP servers CGI servers data servers Java servers and business application servers. The server s also may be capable of executing programs or scripts in response requests from user devices such as by executing one or more Web applications that may be implemented as one or more scripts or programs written in any programming language such as Java C C or C or any scripting language such as Perl Python or TCL as well as combinations thereof. The server s may also include database servers including without limitation those commercially available from Oracle Microsoft Sybase and IBM .

The environment can include a variety of data stores and other memory and storage media as discussed above. These can reside in a variety of locations such as on a storage medium local to and or resident in one or more of the computers or remote from any or all of the computers across the network. In a particular set of embodiments the information may reside in a storage area network SAN familiar to those skilled in the art. Similarly any necessary files for performing the functions attributed to the computers servers or other network devices may be stored locally and or remotely as appropriate. Where a system includes computerized devices each such device can include hardware elements that may be electrically coupled via a bus the elements including for example at least one central processing unit CPU at least one input device e.g. a mouse keyboard controller touch screen or keypad and at least one output device e.g. a display device printer or speaker . Such a system may also include one or more storage devices such as disk drives optical storage devices and solid state storage devices such as random access memory RAM or read only memory ROM as well as removable media devices memory cards flash cards etc.

Such devices also can include a computer readable storage media reader a communications device e.g. a modem a network card wireless or wired an infrared communication device etc. and working memory as described above. The computer readable storage media reader can be connected with or configured to receive a computer readable storage medium representing remote local fixed and or removable storage devices as well as storage media for temporarily and or more permanently containing storing transmitting and retrieving computer readable information. The system and various devices also typically will include a number of software applications modules services or other elements located within at least one working memory device including an operating system and application programs such as a client application or Web browser. It should be appreciated that alternate embodiments may have numerous variations from that described above. For example customized hardware might also be used and or particular elements might be implemented in hardware software including portable software such as applets or both. Further connection to other computing devices such as network input output devices may be employed.

Storage media and computer readable media for containing code or portions of code can include any appropriate media known or used in the art including storage media and communication media such as but not limited to volatile and non volatile removable and non removable media implemented in any method or technology for storage and or transmission of information such as computer readable instructions data structures program modules or other data including RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disk DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by the a system device. Based on the disclosure and teachings provided herein a person of ordinary skill in the art will appreciate other ways and or methods to implement the various embodiments.

The specification and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense. It will however be evident that various modifications and changes may be made thereunto without departing from the broader spirit and scope of the invention as set forth in the claims.

Other variations are within the spirit of the present disclosure. Thus while the disclosed techniques are susceptible to various modifications and alternative constructions certain illustrated embodiments thereof are shown in the drawings and have been described above in detail. It should be understood however that there is no intention to limit the invention to the specific form or forms disclosed but on the contrary the intention is to cover all modifications alternative constructions and equivalents falling within the spirit and scope of the invention as defined in the appended claims.

The use of the terms a and an and the and similar referents in the context of describing the disclosed embodiments especially in the context of the following claims are to be construed to cover both the singular and the plural unless otherwise indicated herein or clearly contradicted by context. The terms comprising having including and containing are to be construed as open ended terms i.e. meaning including but not limited to unless otherwise noted. The term connected is to be construed as partly or wholly contained within attached to or joined together even if there is something intervening. Recitation of ranges of values herein are merely intended to serve as a shorthand method of referring individually to each separate value falling within the range unless otherwise indicated herein and each separate value is incorporated into the specification as if it were individually recited herein. All methods described herein can be performed in any suitable order unless otherwise indicated herein or otherwise clearly contradicted by context. The use of any and all examples or exemplary language e.g. such as provided herein is intended merely to better illuminate embodiments of the invention and does not pose a limitation on the scope of the invention unless otherwise claimed. No language in the specification should be construed as indicating any non claimed element as essential to the practice of the invention.

Preferred embodiments of this disclosure are described herein including the best mode known to the inventors for carrying out the invention. Variations of those preferred embodiments may become apparent to those of ordinary skill in the art upon reading the foregoing description. The inventors expect skilled artisans to employ such variations as appropriate and the inventors intend for the invention to be practiced otherwise than as specifically described herein. Accordingly this invention includes all modifications and equivalents of the subject matter recited in the claims appended hereto as permitted by applicable law. Moreover any combination of the above described elements in all possible variations thereof is encompassed by the invention unless otherwise indicated herein or otherwise clearly contradicted by context.

All references including publications patent applications and patents cited herein are hereby incorporated by reference to the same extent as if each reference were individually and specifically indicated to be incorporated by reference and were set forth in its entirety herein.

