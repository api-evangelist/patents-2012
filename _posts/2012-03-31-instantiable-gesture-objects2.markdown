---

title: Instantiable gesture objects
abstract: Instantiable gesture object techniques are described in which native gesture functionality is abstracted to applications using a script-based recognition interface. Gesture objects may be instantiated for different interaction contexts at the direction of applications programmed using dynamic scripting languages. Gesture objects can be configured to designate particular touch contacts and/or other inputs to consider for gesture recognition and a target element of content to which corresponding recognized gestures are applicable. After creation, gesture objects manage gesture processing operations on behalf of the applications including creating recognizers with the native gesture system, feeding input data for processing, and transforming raw gesture data into formats appropriate for the application and/or a target element. Accordingly, script-based applications may use the gesture objects to offload processing tasks associated with gesture recognition and take advantage of native gesture functionality.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09575652&OS=09575652&RS=09575652
owner: Microsoft Technology Licensing, LLC
number: 09575652
owner_city: Redmond
owner_country: US
publication_date: 20120331
---
The number and types of gestures that are supported by computing devices as a way for users to interact with content is ever increasing. Users may expect supported gestures to be available for various different applications and content interactions throughout the user experience. Traditionally though natively supported gesture functionality may require complex configuration which makes it difficult or cost prohibitive for some developers to incorporate gestures in their applications. Additionally native gesture functionality in existing systems may be limited or entirely unavailable for modern applications and or content that employ dynamic scripting languages such as JavaScript and HTML.

Instantiable gesture object techniques are described in which native gesture functionality is abstracted to applications using a script based recognition interface. Gesture objects may be instantiated for different interaction contexts at the direction of applications programmed using dynamic scripting languages. Gesture objects can be configured to designate particular touch contacts and or other inputs to consider for gesture recognition and a target element of content to which corresponding recognized gestures are applicable. After creation gesture objects manage gesture processing operations on behalf of the applications including creating recognizers with the native gesture system feeding input data for processing and transforming raw gesture data into formats appropriate for the application and or a target element. Accordingly script based applications may use the gesture objects to offload processing tasks associated with gesture recognition and take advantage of native gesture functionality.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter.

Traditionally natively supported gesture functionality provided by a platform may require complex configuration which makes it difficult or cost prohibitive for some developers to incorporate gestures in their applications. Additionally native gesture functionality in existing platforms may be unavailable for applications that employ dynamic scripting languages such as JavaScript and HTML.

Instantiable gesture object techniques are described in which native gesture functionality is abstracted to applications using a script based recognition interface. Gesture objects may be instantiated for different interaction contexts at the direction of applications programmed using dynamic scripting languages. Gesture objects can be configured to designate particular touch contacts and or other inputs to consider for gesture recognition and a target element of content to which corresponding recognized gestures are applicable. After creation gesture objects manage gesture processing operations on behalf of the applications including creating recognizers with the native gesture system feeding input data for processing and transforming raw gesture data into formats appropriate for the application and or a target element. Accordingly script based applications may use the gesture objects to offload processing tasks associated with gesture recognition and take advantage of native gesture functionality.

In the following discussion an example environment is first described that is operable to employ the instantiable gesture object techniques described herein. Example scenarios and procedures are then described which may be employed in the example environment as well as in other environments. Accordingly the example scenarios and procedures are not limited to the example environment and the example environment may incorporate scenarios and procedures in addition to the examples described herein. Lastly an example computing system is described that can be employed to implement instantiable gesture object techniques in one or more embodiments.

For example a computing device may be configured as a computer that is capable of communicating over the network such as a desktop computer a mobile station an entertainment appliance a tablet or slate device a surface computing device a set top box communicatively coupled to a display device a mobile communication device e.g. a wireless phone a game console and so forth. The computing device may be configured as any suitable computing system and or device that employ various processing systems some additional examples of which are discussed in relation to the example system of .

The computing device is further illustrated as including an operating system . Generally speaking the operating system is configured to abstract underlying functionality of the computing device to applications that are executable on the computing device . For example the operating system may abstract processing memory network and or display functionality of the computing device such that the applications may be written without knowing how this underlying functionality is implemented. The application for instance may provide data to the operating system to be rendered and displayed by the display device without understanding how this rendering will be performed. The operating system may provide various services interfaces and functionality that the applications may invoke to take advantage of system features. A variety of applications to provide a wide range of functionality to the computing device are contemplated including but not limited to a browser to access and render webpages and or other content an office productivity application an email client games a multi media management program device management software and social networking applications to name a few examples.

The operating system may further provide services interfaces and functionality for different kinds of applications including legacy applications that may be written using statically compiled languages such as C C and modern applications that may be written using dynamic scripting languages that may be dynamically compiled at runtime such as JavaScript hypertext markup language revision 5 and or cascading style sheets HTML5 CSS and extensible application mark up language XAML . Modern applications may operate through one or more runtime platforms supported by the operating system that are configured to provide respective execution environments for corresponding applications. Runtime platforms provide a common set of features routines and functions for compatible applications thereby offloading coding of common tasks from application development. Thus runtime platforms can facilitate portability of applications to different kinds of systems with little or no change to the dynamic script for the applications and or without recompiling. Examples of runtime platforms include JAVA runtime environment JRE Adobe Flash Microsoft .NET framework Microsoft Silverlight and WinRT to name a few examples.

In the depicted example the computing device includes or makes use of a display device that may be configured to include a touchscreen touch digitizer to enable touchscreen and gesture functionality. The computing device may therefore include a display driver gesture module and or other modules operable to provide touchscreen and gesture functionality enabled through touch capabilities of the display device . Accordingly the computing device may be configured to recognize input and gestures that cause corresponding operations to be performed.

For example a gesture module may be implemented as component of the operating system as depicted in . The gesture module represents native functionality for support and recognition of gestures e.g. a native gesture system that may be made accessible to applications via the operating system . For instance the operating system may provide one or more application programming interfaces APIs operable to invoke gesture recognition functionality. In particular the gesture module may be configured to recognize a touch input such as a finger of a user s hand as on or proximate to the display device of the computing device using touchscreen functionality. A variety of different types of gestures may be recognized by the computing device including by way of example and not limitation gestures that are recognized from a single type of input e.g. touch gestures as well as gestures involving multiple types of inputs. For example can be utilized to recognize single finger gestures and bezel gestures multiple finger same hand gestures and bezel gestures and or multiple finger different hand gestures and bezel gestures. Further the gesture module may be configured to detect and differentiate between gestures touch inputs camera based input a stylus input and other different types of inputs. Moreover various kinds of inputs obtained from different sources including inputs obtained through a touchscreen a mouse touchpad a camera software or hardware keyboard and or hardware keys of a device e.g. input devices may be used in combination to cause corresponding device operations.

The computing device is further depicted as including a rendering engine . The rendering engine represents a component of the computing device operable to handle various content rendering operations for display of content. For instance the rendering engine may process corresponding mark up language content and output formatted content for display on the display device . In at least some embodiments the rendering engine is configured as a component of a browser that facilitates rendering of webpages and other resources that may be obtained over the network from the web service . Resources can include any suitable combination of content and or services typically made available over a network by one or more service providers. The rendering engine may also be implemented as a standalone application or operating system component. The browser and other applications of the computing device may invoke the rendering engine to handle rendering of corresponding content items for the applications. For example various kinds of modern applications that employ dynamic scripting languages may use functionality of the rendering engine for processing and display of corresponding pages documents media images forms user interfaces and or other types of application content.

To implement instantiable gesture object techniques the rendering engine may be configured to include or otherwise make use of a recognition interface . The recognition interface represent functionality that may be exposed to applications via the rendering engine to enable interaction with native functionality for support and recognition of gestures that is represented by the gesture module . Traditionally direct configuration of gestures via the gesture module may be complex and resource intensive. Moreover native gesture functionality in existing systems may be limited or entirely unavailable for modern applications that employ dynamic scripting languages. Thus in order to produce applications that take advantage of native gesture functionality application developers traditionally maintain detailed knowledge of complex configuration management options provided via the gesture module and or may have to perform complex and or static programming to invoke the functionally directly from application code. As described in detail herein the recognition interface may simplify application development by performing various gesture configuration handling and processing operations on behalf of applications. Thus the recognition interface offloads various processing and handling operations for gesture recognition from the applications which relieves application developers from having to write code for these operations and or have detailed knowledge of gesture configuration options.

In one approach applications may invoke the recognition interface to instantiate gesture objects for various interactions. Gesture objects then automatically handle gesture recognition operations via the gesture module on behalf of the applications. For example the gesture objects may be configured to communicate with the gesture module on behalf of applications to create appropriate recognizers for different interactions and to feed input data to the recognizers for gesture processing. The gesture objects generally speaking are script based representations of and interfaces for manipulation of the underlying recognizers and native gesture functionality. A gesture object and corresponding recognizer may be created for each particular interaction context. An interaction context as used herein refers to a particular input or group of inputs e.g. touch contacts stylus input pointer input camera input etc. that is tracked by the gesture module for gesture recognition.

The recognizers are components objects created for the native gesture system that are configured to track inputs for different interaction contexts map the inputs to a library of supported gestures detect when particular gestures occur and fire appropriate events for gesture recognition. Thus the recognizers represent functionality to handle processing at the system level for gesture recognition in different contexts. The recognizers understand how to invoke the native gesture functionality for a set of inputs using designated system formats and protocols. Recognizers may be configured to detect a variety of supported gestures examples of which include but are not limited to tap hold rotate scale zoom pan and translate gestures.

The gesture object facilitates processing of raw gesture events generated by the recognizers on behalf of the application. The gesture object may facilitate translation of information between system formats used by the recognizers and script based formats understandable by applications programmed using dynamic scripting languages. This may include detecting events fired by the recognizers formatting gesture information events for a particular application transforming raw gesture data into a coordinate space of an application and or element associated with an interaction context gesture object and so forth details of which can be found in the following discussion.

Having described an example operating environment consider now a discussion of some example implementation details regarding instantiable gesture objects in one or more embodiments.

To further illustrate consider now which depicts generally at an example scenario in which a recognition interface is invoked to instantiate gesture objects to handle gesture recognition of behalf of an application . Content that is rendered for an application by the rendering engine or otherwise may be represented according to a corresponding content model . The content model is a hierarchal representation of the structure of a document page or other application content. The content model enables applications that consume content to reference and manipulate various elements contained in the content. Elements may include for example images controls text menus graphics and so forth included with the content. In one particular example a document object model DOM may be employed by an application. The DOM is a standard convention for representing and interacting with elements in pages for a browser or other application . The DOM of a page can be built by a browser and used to reference modify apply effects and or otherwise manipulate elements contained in a webpage. Content for other applications may also be configured in accordance with the DOM to represent the elements of corresponding pages files and or documents. Other kinds of content models may also be employed in one or more embodiments.

As represented in an application may interact with the recognition interface to direct the recognition interface to create gesture objects for one or more interaction contexts. Gesture objects then operate to create and wrap corresponding recognizers associated with native gesture functionality of the computing device . A gesture object feeds input data to the wrapped recognizer and receives raw gesture recognition data events back from the recognizer based on processing performed via the recognizer and or gesture module . Gesture objects may also be mapped to particular elements based on the corresponding content model . In this way an application is able to explicitly specify an element to which results of gesture processing for a gesture object will be applied. Moreover an application may be able to specify a collection of distinct items as an element for the purposes of gesture application. This is contrast to traditional techniques in which elements to which a gesture applies are inferred heuristically using hit testing or other comparable selection techniques on individual items.

In particular gesture objects are illustrated in as including interaction inputs target elements and custom properties that may be set by an application to generate an object for a given interaction context. The interaction inputs also referred to as pointers are inputs such as touch contacts pointer positions movements stylus input mouse inputs and or other inputs that are tracked for the interaction context. A gesture object may be configured to include one or more individual inputs contact e.g. pointers. Gesture objects may include multiple pointers of the same type from the same input source and or combinations of different types of pointers from different input sources. Inclusion of inputs contacts in a gesture object causes these inputs contacts to be considered for gesture detection. The target elements represent elements within the content model to which gesture objects are mapped. For instance elements in a DOM representation of content may be specified as targets for gesture objects. An individual element may be defined to contain one or more individual components items of a user interface page or document that is being rendered. Designating a target element for a gesture object causes gesture events generated in relation to the gesture object to be explicitly directed to the designated element. A single target element or multiple target elements may be designated for a gesture object to enable a variety of different interaction scenarios gestures. Moreover because the element or elements are known in advance raw gesture data obtained via the gesture module and or recognizers can be formatted for particular elements. Thus the gesture object may supply formatted gesture data to an application that may be applied to a corresponding element with little or no additional processing at the application level. Additionally custom properties may be provided to enable extensions and customization of gesture object behaviors and to facilitate lifetime management details of which are discussed later in this document.

In at least some embodiments the gesture objects are implemented in a script based programming language such as JavaScript as a built in object type available to developers. The gesture objects include the interaction inputs and target elements as properties of the object. Developers may use methods on the object to set the properties and cause the recognition interface to instantiate an appropriate object for a given interaction context. These methods enable the developer application to add touch contacts and or other inputs to an object and to specify target elements in the DOM or another content model. Developers may also set custom properties via exposed methods in a comparable manner. In one approach the recognition interface may be configured as an application programming interface that exposes methods used to define and create gesture objects for gesture recognition.

By way of example and not limitation the following pseudo code represents but one script based implementation of gesture objects and methods suitable for the described techniques 

Per the foregoing a gesture objects may be created using a built in object type that is supported by the recognition interface . Elements and interaction inputs e.g. contacts inputs may be specified as properties of the created object. Contacts inputs may be removed by the remove method so that the contacts inputs are no longer considered as part of the gesture processing. The stop method enables termination of processing on demand. The stop method removes contacts inputs from the object terminates processing fires an end event for the gesture and resets the object recognizer to prepare for another gesture. Otherwise an object may persist and automatically clean it self up in accordance with lifetime management techniques described below.

Thus to start gesture events for a given interaction context an application may invoke the recognition interface and specify a target element and one or more interaction inputs. This causes creation a corresponding gesture object through the recognition interface . The bulk of processing for gesture recognition is then handled via the gesture object on behalf of the application including supplying input data to a recognizer processing gesture event messages transforming raw gesture data and so forth. In effect the application registers with the recognition interface for gesture updates on particular inputs contacts relative to a selected target element e.g. the interaction context and may then just wait for the recognition interface to return pertinent information for the interaction context. For instance the recognition interface may supply gesture events data for elements that is already transformed relative to a corresponding element as DOM events in the DOM for application content or as comparable events appropriate to another content model employed by an application.

Consider an example in which a user selects an image with a touch of their index finger and drags the image across the display. Upon the initial selection of the image the application may operate to form a corresponding gesture object through the recognition interface with the image as the target element and the index finger touch as the interaction input . Now the application itself no longer needs to monitor the changes in finger position e.g. the drag or supply such information to the recognition system. Instead the gesture object instantiated in this scenario automatically tracks the interaction input on behalf of the application and feeds information regarding the changes in contact position to a recognizer which may detect the corresponding drag gesture. Further the gesture object and or recognition interface also processes gesture event messages generated by the recognizer and may supply these event messages to the application in accordance with a content model employed by the application.

As briefly discussed above results of gesture processing may be pre processed to transform the results specifically for a targeted element. The designation of target elements for a gesture object effectively makes the system element aware. As such the recognition interface is configured to transform a gesture detection result into the coordinate space associated with the element to which the gesture is targeted. Typically a gesture result may be expressed as a delta between a previous state and a new state or as cumulative values relative to an initial state. If the raw delta or cumulative data for a gesture is supplied to the application the application must do the work to transform the raw delta for a particular element. For instance if the element has previously been rotated the application must determine how to apply the gesture relative to the pre existing rotation. In traditional systems that are not element aware the developer has no choice but to handle these situations in their application code which complicates development. On the other hand explicitly defining target elements for a gesture object as per the techniques described herein enables the recognition interface to track the state of an element. The element state may be updated based on applied gestures and the updated state is used to compute a delta for a subsequent recognized gesture or gesture change in the coordinate space defined for the element. Thus each time a gesture is detected the raw delta information provided by the gesture module may be transformed into the appropriate coordinate space on behalf of the application. The application may therefore be able to directly apply the result to the corresponding element with little or no additional processing.

In at least some embodiments applications may also explicitly specify a particular coordinate space into which gesture data is transformed. To do so the application may set the target element and also set another designated and or custom property of a gesture object configured to specify a coordinate space that may or may not correspond to the target element. This enable the application to select a coordinate space that may be different form the coordinate space of the target element to which the gesture events are fired. For instance gesture data may be transformed into the global coordinate space of the page document rather than the target element s. Even further the application can define an arbitrary or global coordinate space transform for a gesture object such that corresponding gesture data is transformed and reported as being in the defined coordinate space selected by the application.

The recognition interface and gesture objects may be further configured to implement a managed lifetime scheme that enables reuse of the objects preserves the ability to access reference objects having active interaction inputs and controls automatic collection to clean up the objects at appropriate times. Generally speaking JavaScript and other script based objects are created having a scope e.g. functional or global and may persist for the duration of the associated scope. The scope may relate to a particular function module or for the lifetime of the page or application instance. The object may then recognize when it is no longer being used and automatically collect itself e.g. removes itself and or otherwise allows itself to be deleted .

To prevent gesture objects that still have active interaction inputs e.g. contacts inputs from automatically collecting removing themselves the interaction inputs for an object may be configured as pinning references. The pinning references keep the object alive so long as the interaction inputs remain active. This may be the case even if the scope of the object is no longer valid e.g. a function module that defines the scope of an object has terminated . The target elements for objects also may act as pinning references that keep respective objects alive until the target elements are cleared. Thus the gesture objects may persist as long as the interaction inputs driving a gesture persist and gesture events may continue to be supplied via the gesture objects . When interaction inputs are removed e.g. a user removes their fingers from the display a gesture object may proceed to recognize that they are no longer in use and automatically collect itself. This occurs without the application having to manage the gesture objects that are created on its behalf.

To enable continued access to gesture objects during the lifetime gesture events supplied by the recognition interface may include information that identifies and provides access to a corresponding gesture object. Thus even if reference is lost to an object the object identifying property contained in the events may be used to access the corresponding gesture object. Additionally custom properties may be set to carry custom information and even functions. For instance a particular function to call each time an event is fired by an object may be specified via custom properties . In another example a property may be configured to convey a list of contacts input currently associated with a given object to the application. A variety of other examples are also contemplated.

As mentioned gesture objects may also be reused. Once an object is created and associated with interaction inputs and target element the object may persist as discussed above. While interaction inputs associated with an object are active the target element remains the same. Additional interaction inputs may be added to the object. Interaction inputs may also be removed explicitly using a remove method or automatically when the contacts input driving a gesture are complete e.g. finger is lifted . Thus the particular contacts inputs considered for a gesture object may change over the lifetime of the object. Once all the interaction inputs are removed the object may automatically reset. An object may also reset in response to an explicit stop as described above. Subsequently different interaction inputs that are applied to the same element may be added to the gesture object to reuse the object. The gesture object can also be associated with a different element so long as the object is not actively being used for a different element e.g. a gesture is in progress .

Another feature of gesture objects is the ability for applications to seamlessly handle inertia processing. Inertia processing is used to simulate inertia when user interface elements are manipulated through gestures to provide natural interaction effects after a user lifts their fingers or otherwise concludes input driving a gesture. Thus for a period of time after user interaction concludes a manipulated element may coast to a stop bounce off of a border continue to scroll and so forth. Inertia may be dependent upon the velocity of user interactions that trigger gestures e.g. how fast or slow a user pans zooms or rotates . The inertia processing continues for some time based upon the velocity and then may gradually stop. The inertia processing may be accomplished by setting a timer when the last contact input used for the interaction concludes. While the timer is running gesture events generated through inertia processing may continue to be supplied to the application as though the event were generated by direct interaction. Gesture events generated through inertia processing may be directed to the application and or target elements in the same manner as gesture events generated through direct interaction.

Applications may be configured to detect when a mode change occurs from direct interaction to inertia processing. For example a mode identifier may be included in gesture events and or a notification message may be sent when contacts for a gesture conclude. Accordingly applications may selectively use or ignore gesture events that are generated through inertia processing. For example an application may simply ignore events associated with inertia processing so that an element stops moving reacting when direct user manipulation concludes. In addition or alternatively an application can call the stop method g.stop discussed above to reset the gesture which cuts off inertia processing and delivery of gesture events to the application.

As depicted in a gesture object corresponding to the user s left hand is configured to track contacts A and B and is associated with the first element as a target element. Another gesture object corresponding to the user s right hand is configured to track contacts C and D and is associated with the second element as a target element. The different gesture objects are concurrently active and may be managed processed independently of one another. For instance a pinch of the contacts A and B may be recognized as a zoom gesture that causes a zoom in on the first element while at the same time contact D is recognized as a select and hold gesture applied to the second element . In this example contact C may be considered in connection with the second element but may not contribute to the recognized select and hold gesture.

The use of multiple different gesture objects to control different interactions concurrently enables a variety of different multi touch input and multi gesture scenarios. A variety of different contexts gestures tracked by way of corresponding gesture objects may be active at the same time. Each object may be used to track one or multiple individual contacts inputs. Additionally an individual contact may be added to multiple gesture objects at the same time and therefore may be considered concurrently for detection of multiple gestures.

Although objects associated with different elements are depicted in multiple different gesture objects that track different gestures may be associated with the same element at the same time. For example consider an example in which both hands depicted in are applied to the first element . Different gesture objects may still be produced in this example. However here the target element for the different objects may be the same element. The different gesture objects may be configured to drive different actions on the same element. For example the left hand may perform a zoom gesture while the right hand performs a rotate gesture on the same element.

In another example the left hand may be configured to rotate the element coarsely while the right hand performs fine rotation of the element. These different gestures may be detected and applied concurrently to the first element via respective gesture objects . A variety of other examples of multi touch and multi gesture scenarios enabled through the use of one or more gesture objects are also contemplated including scenarios in which different input sources e.g. touch stylus camera mouse etc. and or combinations of multiple input sources e.g. touch and mouse stylus and camera touch and camera etc. are employed.

Having described some details regarding instantiable gesture objects consider now some example procedures in accordance with one or more embodiments.

The following discussion describes instantiable gesture object techniques that may be implemented utilizing the previously described systems and devices. Aspects of each of the procedures may be implemented in hardware firmware software or a combination thereof. The procedures are shown as a set of blocks that specify operations performed by one or more devices and are not necessarily limited to the orders shown for performing the operations by the respective blocks. In portions of the following discussion reference may be made to the operating environment and example scenarios and of respectively.

A gesture object is instantiated for an application block . For example a rendering engine may provide an interface that enables script based applications that rely on the rendering engine to invoke native gesture functionality of a computing device and or operating system . The interface enables applications written in dynamic scripting languages such as JavaScript to offload processing for gestures from the applications. To do so the rendering engine may instantiate gesture objects as directed by the application. The gesture objects are configured to handle operations for gesture recognition on behalf of the applications as described previously.

The gesture object that is instantiated is associated with interaction inputs and target elements specified by the application block . For example properties of the gesture object may be set to designated particular interaction inputs and a target element associated the object as discussed above in relation to . The interaction inputs added to an object may then be considered together for gesture recognition and any corresponding gestures recognized by the gesture module may be directed explicitly to a target element that is selected. Different interaction inputs may be added and removed from an object at various times and the object itself may be reused for different interaction contexts.

A recognizer is then created on behalf of the application to facilitate gesture recognition block . Interaction input data is then fed to the recognizer for recognition handling block . As discussed previously a recognizer is a system level object abstraction that facilitates gesture recognition via the native gesture system. Gesture objects may be implemented to use a script based format for interactions with modern applications and each object may create a corresponding recognizer that employs a native system format to invoke native gesture functionality. Gesture objects then feed input data regarding interaction inputs to corresponding recognizers for processing. Given a set of interaction inputs for an interaction context the recognizer manages tracking of these inputs and recognition of gestures through a gesture module . For instance the interaction inputs may be mapped to a library of gestures supported by the gesture module as discussed previously.

Gesture event messages are obtained from the recognizer that are indicative of recognized gestures block and raw gesture data in the gesture event messages obtained from the recognizer is processed on behalf of the application block . For example in response to recognition of gestures using inputs supplied by a corresponding gesture object a recognizer may send gesture event messages for receipt by the gesture object . The gesture object may obtain corresponding gesture event messages which generally contain raw gesture data computed via the native gesture system. In accordance with techniques described herein the gesture object may cause processing upon the raw data to transform the data specifically for a particular target element and or application. For example raw gesture data may be transformed into a coordinate space corresponding to the target element defined by the gesture object in the manner previously discussed. Thus the application is relieved of having to perform such computations.

Gesture events are fired to the target elements in accordance with a content model for the application block . The gesture events fired to the target elements contain processed gesture data that has been transformed for the application and or target elements. Thus the application may simply apply the gestures actions that are conveyed via the gesture events to appropriate elements.

An application may direct creation of a gesture object for a selected target element of a corresponding content item to register for gesture recognition via a script based interface block . For example an application programmed using a dynamic scripting language may interact with a recognition interface exposed by a rendering engine used to render content for the application. The interaction may cause the recognition interface to instantiate a gesture object for a given interaction context in the manner described previously. To define the interaction context the application may use methods on the object to configure the object by setting various properties of the object. In general this involves specifying at least interaction inputs and a target element as described in relation to . The application may also set custom properties to control behaviors and or extend functionality of the gesture object . Examples of such customizations include but are not limited to triggering functions to call in response to particular gestures designating particular kinds of gestures that are applicable to the context and or that may be ignored returning specified data properties regarding gestures and or inputs used to compute the gestures providing configuration parameters and thresholds to control gestures and so forth.

Gesture events are then obtained that are fired on the target element and are indicative of gestures recognized based upon user interaction with the target element block and the gesture events are applied to the target element to manipulate display of the target elements in accordance with the recognized gestures block . As mentioned previously the gesture objects enable applications to offload much of the workload for processing of gestures. Thus after directing creation of objects per block an application automatically begins to obtain gesture events for corresponding interaction contexts. This may occur without the application performing additional work to recognize the gestures. The application may simply incorporate handlers to listen for or otherwise obtain gesture events and apply them to appropriate elements. Moreover the gesture data described by gesture events fired to the elements may be pre processed so that the gesture data is in a script format understandable by the application and is already transformed for corresponding target elements.

Having considered example procedures consider now an example system that can be employed in one or more embodiments to implement aspects of instantiable gesture object techniques described herein.

The example computing device includes a processing system that may incorporate one or more processors or processing devices one or more computer readable media which may include one or more memory and or storage components and one or more input output I O interfaces for input output I O devices. Computer readable media and or one or more I O devices may be included as part of or alternatively may be coupled to the computing device . As illustrated the processing system may also include one or more hardware elements representative of functionality to implement at least some aspects of the procedures and techniques described herein in hardware. Although not shown the computing device may further include a system bus or data transfer system that couples the various components one to another. A system bus can include any one or combination of different bus structures such as a memory bus or memory controller a peripheral bus a universal serial bus and or a processor or local bus that utilizes any of a variety of bus architectures.

The processing system processors and hardware elements are not limited by the materials from which they are formed or the processing mechanisms employed therein. For example processors may be comprised of semiconductor s and or transistors e.g. electronic integrated circuits ICs . In such a context processor executable instructions may be electronically executable instructions. The memory storage component represents memory storage capacity associated with one or more computer readable media. The memory storage component may include volatile media such as random access memory RAM and or nonvolatile media such as read only memory ROM Flash memory optical disks magnetic disks and so forth . The memory storage component may include fixed media e.g. RAM ROM a fixed hard drive etc. as well as removable media e.g. a Flash memory drive a removable hard drive an optical disk and so forth .

Input output interface s allow a user to enter commands and information to computing device and also allow information to be presented to the user and or other components or devices using various input output devices. Examples of input devices include a keyboard a cursor control device e.g. a mouse a microphone for audio voice input a scanner a camera and so forth. Examples of output devices include a display device e.g. a monitor or projector speakers a printer a network card and so forth.

Various techniques may be described herein in the general context of software hardware or program modules. Generally such modules include routines programs objects elements components data structures and so forth that perform particular tasks or implement particular abstract data types. The terms module functionality and component as used herein generally represent software firmware hardware or a combination thereof. The features of the techniques described herein are platform independent meaning that the techniques may be implemented on a variety of commercial computing platforms having a variety of processing systems hardware elements computer readable media and or memory storage components.

An implementation of the described modules and techniques may be stored on or transmitted across some form of computer readable media. The computer readable media may include a variety of available medium or media that may be accessed by a computing device. By way of example and not limitation computer readable media may include computer readable storage media and communication media. 

 Computer readable storage media may refer to media and or devices that enable persistent and or non transitory storage of information in contrast to mere signal transmission carrier waves or signals per se. Thus computer readable storage media refers to non signal bearing media. The computer readable storage media includes volatile and non volatile removable and non removable media and or storage devices implemented in a method or technology suitable for storage of information such as computer readable instructions data structures program modules logic elements circuits or other data. Examples of computer readable storage media may include but are not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage hard disks magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or other storage device tangible media or article of manufacture suitable to store the desired information and which may be accessed by a computer.

 Communication media may refer to a signal bearing medium that is configured to transmit instructions to the hardware of the computing device such as via a network. Communication media typically may embody computer readable instructions data structures program modules or other data in a modulated data signal such as carrier waves data signals or other transport mechanism. Communication media also include any information delivery media. The term modulated data signal means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media include wired media such as a wired network or direct wired connection and wireless media such as acoustic RF infrared and other wireless media.

Hardware elements are representative of instructions modules programmable device logic and or fixed device logic implemented in a hardware form that may be employed in some embodiments to implement at least some aspects of the described techniques. Hardware elements may include components of an integrated circuit or on chip system an application specific integrated circuit ASIC a field programmable gate array FPGA a complex programmable logic device CPLD and so forth. In this context a hardware element may operate as a processing device that performs program tasks defined by instructions modules and or logic embodied by the hardware element.

Combinations of the foregoing may also be employed to implement various techniques and modules described herein. Accordingly software hardware or program modules including the gesture module rendering engine recognition interface applications operating system and other program modules may be implemented as one or more instructions and or logic embodied on some form of computer readable media and or by one or more hardware elements . The computing device may be configured to implement particular instructions and or functions corresponding to the software and or hardware modules. The instructions and or functions may be executable operable by one or more articles of manufacture for example one or more computing devices and or processing systems to implement techniques modules and example procedures for described herein.

As further illustrated in the example system enables ubiquitous environments for a seamless user experience when running applications on a personal computer PC a television device and or a mobile device. Services and applications run substantially similar in all three environments for a common user experience when transitioning from one device to the next while utilizing an application playing a video game watching a video and so on.

In the example system multiple devices are interconnected through a central computing device. The central computing device may be local to the multiple devices or may be located remotely from the multiple devices. In one embodiment the central computing device may be a cloud of one or more server computers that are connected to the multiple devices through a network the Internet or other data communication link. In one embodiment this interconnection architecture enables functionality to be delivered across multiple devices to provide a common and seamless experience to a user of the multiple devices. Each of the multiple devices may have different physical requirements and capabilities and the central computing device uses a platform to enable the delivery of an experience to the device that is both tailored to the device and yet common to all devices. In one embodiment a class of target devices is created and experiences are tailored to the generic class of devices. A class of devices may be defined by physical features types of usage or other common characteristics of the devices.

In various implementations the computing device may assume a variety of different configurations such as for computer mobile and television uses. Each of these configurations includes devices that may have generally different constructs and capabilities and thus the computing device may be configured according to one or more of the different device classes. For instance the computing device may be implemented as the computer class of a device that includes a personal computer desktop computer a multi screen computer laptop computer netbook and so on.

The computing device may also be implemented as the mobile class of device that includes mobile devices such as a mobile phone portable music player portable gaming device a tablet computer a multi screen computer and so on. The computing device may also be implemented as the television class of device that includes devices having or connected to generally larger screens in casual viewing environments. These devices include televisions set top boxes gaming consoles and so on. The techniques described herein may be supported by these various configurations of the computing device and are not limited to the specific examples the techniques described herein. This is illustrated through inclusion of the recognition interface on the computing device . Functionality of the recognition interface and or other applications modules may also be implemented all or in part through use of a distributed system such as over a cloud via a platform .

The cloud includes and or is representative of a platform for resources . The platform abstracts underlying functionality of hardware e.g. servers and software resources of the cloud . The resources may include applications and or data that can be utilized while computer processing is executed on servers that are remote from the computing device . Resources can also include services provided over the Internet and or through a subscriber network such as a cellular or Wi Fi network.

The platform may abstract resources and functions to connect the computing device with other computing devices. The platform may also serve to abstract scaling of resources to provide a corresponding level of scale to encountered demand for the resources that are implemented via the platform . Accordingly in an interconnected device embodiment implementation of the functionality described herein may be distributed throughout the system . For example the functionality may be implemented in part on the computing device as well as via the platform that abstracts the functionality of the cloud .

Although the invention has been described in language specific to structural features and or methodological acts it is to be understood that the invention defined in the appended claims is not necessarily limited to the specific features or acts described. Rather the specific features and acts are disclosed as example forms of implementing the claimed invention.

