---

title: Systems and methods for management of virtualization data
abstract: Described in detail herein is a method of copying data of one or more virtual machines being hosted by one or more non-virtual machines. The method includes receiving an indication that specifies how to perform a copy of data of one or more virtual machines hosted by one or more virtual machine hosts. The method may include determining whether the one or more virtual machines are managed by a virtual machine manager that manages or facilitates management of the virtual machines. If so, the virtual machine manager is dynamically queried to automatically determine the virtual machines that it manages or that it facilitates management of. If not, a virtual machine host is dynamically queried to automatically determine the virtual machines that it hosts. The data of each virtual machine is then copied according to the specifications of the received indication.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08725973&OS=08725973&RS=08725973
owner: CommVault Systems, Inc.
number: 08725973
owner_city: Oceanport
owner_country: US
publication_date: 20121102
---
This application is a divisional of U.S. patent application Ser. No. 12 553 294 filed Sep. 3 2009 entitled SYSTEMS AND METHODS FOR MANAGEMENT OF VIRTUALIZATION DATA which claims priority to U.S. Provisional Patent Application No. 61 094 753 filed Sep. 5 2008 entitled SYSTEMS AND METHODS FOR MANAGEMENT OF VIRTUALIZATION DATA U.S. Provisional Patent Application No. 61 121 383 filed Dec. 10 2008 entitled SYSTEMS AND METHODS FOR MANAGEMENT OF VIRTUALIZATION DATA and U.S. Provisional Patent Application No. 61 169 515 filed Apr. 15 2009 entitled SYSTEMS AND METHODS FOR MANAGEMENT OF VIRTUALIZATION DATA each of which is incorporated by reference herein in its entirety.

In general virtualization refers to the simultaneous hosting of one or more operating systems on a physical computer. Such virtual operating systems and their associated virtual resources are called virtual machines. Virtualization software sits between the virtual machines and the hardware of the physical computer. One example of virtualization software is ESX Server by VMware Inc. of Palo Alto Calif. Other examples include Microsoft Virtual Server and Microsoft Windows Server Hyper V both by Microsoft Corporation of Redmond Wash. and Sun xVM by Sun Microsystems Inc. of Santa Clara Calif.

Virtualization software provides to each virtual operating system virtual resources such as a virtual processor virtual memory a virtual network device and a virtual disk. Each virtual machine has one or more virtual disks. Virtualization software typically stores the data of virtual disks in files on the filesystem of the physical computer called virtual machine disk files in the case of VMware virtual servers or virtual hard disk image files in the case of Microsoft virtual servers . For example VMware s ESX Server provides the Virtual Machine File System VMFS for the storage of virtual machine disk files. A virtual machine reads data from and writes data to its virtual disk much the same way that an actual physical machine reads data from and writes data to an actual disk.

Traditionally virtualization software vendors have enabled the backup of virtual machine data in one of two ways. A first method requires the installation of backup software on each virtual machine having data to be backed up and typically uses the same methods used to back up the data of physical computers to back up the virtual machine data. A second method backs up the files that store the virtual disks of the virtual machines and may or may not require the installation of backup software on each virtual machine for which the data is to be backed up.

As an example of the second method VMware Consolidated Backup VCB also by VMware Inc. enables the backup of the data of virtual machines on ESX Server without having to install backup software on the virtual machines. VCB consists of a set of utilities and scripts that work in conjunction with third party backup software to backup virtual machine data. VCB and the third party backup software are typically installed on a backup proxy server that uses the Microsoft Windows Server 2003 operating system by Microsoft Corporation. VCB supports file level backups backups at the level of files and directories for virtual machines using Microsoft Windows operating systems. In a file level backup the granularity of the backup is at the level of individual files and or directories of the virtual machine. A file level backup allows copies of individual files on virtual disks to be made. File level backups can be full backups differential backups or incremental backups.

VCB also supports image level backups for virtual machines using any operating system e.g. Microsoft Windows operating systems Linux operating systems or other operating systems that may be installed upon ESX Server . In an image level backup the granularity of the backup is at the level of a virtual machine i.e. the entire virtual machine including its current state is backed up . For an image level backup typically the virtual machine is suspended and all virtual disk and configuration files associated with the virtual machine are backed up and then the virtual machine is resumed.

An administrator would typically choose to perform a file level backup of a Microsoft Windows virtual machine because of the potential need to restore individual files or directories from the backed up Microsoft virtual machine. However VCB may not perform a file level backup of a Microsoft Windows virtual machine as quickly as an image level backup. Accordingly a system that enables a backup of a Microsoft Windows virtual machine to be performed at least as quickly as a file level backup and enables granular restoration of any data e.g. individual files or directories from the backed up Microsoft virtual machine would have significant utility.

Because VCB only supports file level backups for virtual machines using Microsoft Windows operating systems a file level backup cannot be performed using VCB for virtual machines using operating systems other than Microsoft Windows e.g. Linux operating systems . An administrator must back up a non Microsoft Windows virtual machine using an image level backup. Therefore in order to granularly restore data e.g. an individual file or directory from the backed up non Microsoft Windows virtual machine the entire non Microsoft Windows virtual machine must be restored. This may require overwriting the original virtual machine with the backed up virtual machine or re creating the original virtual machine on a different physical machine. This may be a laborious and time intensive process and may result in loss of virtual machine data. Accordingly a system that enables the granular restoration of any data e.g. individual files or directories within a virtual machine using any type of operating system would have significant utility.

Another challenge posed by the use of VCB to perform backups of virtual machines is that such backups require an administrator to manually identify or specify the virtual machines that are to be backed up typically via a script created in advance of the backup operation. However because virtual machines may be easily set up and torn down virtual machines may be less permanent in nature than actual physical machines. Due to this potential transience of virtual machines it may be more difficult for the administrator to identify all of the virtual machines which are to be backed up in advance of the backup operation. Accordingly a system that provides automatic identification of virtual machines that are to be backed up at the time of the backup operation would have significant utility.

The need exists for a system that overcomes the above problems as well as one that provides additional benefits. Overall the examples herein of some prior or related systems and their associated limitations are intended to be illustrative and not exclusive. Other limitations of existing or prior systems will become apparent to those of skill in the art upon reading the following Detailed Description.

The headings provided herein are for convenience only and do not necessarily affect the scope or meaning of the claimed invention.

Described in detail herein is a method of copying data of one or more virtual machines being hosted by one or more non virtual machines. The method includes receiving an indication that specifies how to perform a copy of data of one or more virtual machines hosted by one or more virtual machine hosts. The method further includes determining whether the one or more virtual machines are managed by a virtual machine manager that manages or facilitates management of the virtual machines. If so the virtual machine manager is dynamically queried to automatically determine the virtual machines that it manages or that it facilitates management of. If not a virtual machine host is dynamically queried to automatically determine the virtual machines that it hosts. The data of each virtual machine is then copied according to the specifications of the received indication.

Under one example of the method a file level volume level or disk level copy of a virtual machine is performed. Performing a file level copy involves determining volumes of the virtual machine mounting the volumes on a proxy server and copying files from the volumes mounted on the proxy server to a secondary storage data store. Performing a volume level copy involves determining volumes of the virtual machine mounting the volumes on a proxy server and copying the volumes mounted on the proxy server to the secondary storage data store. Performing a disk level copy involves determining virtual disk and configuration files of the virtual machine copying the virtual disk and configuration files to the proxy server extracting metadata from the virtual disk and configuration files and copying the virtual disk and configuration files and the extracted metadata to the secondary storage data store.

Various examples of aspects of the invention will now be described. The following description provides specific details for a thorough understanding and enabling description of these examples. One skilled in the relevant art will understand however that aspects of the invention may be practiced without many of these details. Likewise one skilled in the relevant art will also understand that aspects of the invention may have many other obvious features not described in detail herein. Additionally some well known structures or functions may not be shown or described in detail below so as to avoid unnecessarily obscuring the relevant description.

The terminology used below is to be interpreted in its broadest reasonable manner even though it is being used in conjunction with a detailed description of certain specific examples of aspects of the invention. Indeed certain terms may even be emphasized below however any terminology intended to be interpreted in any restricted manner will be overtly and specifically defined as such in this Detailed Description section.

Unless described otherwise below aspects of the invention may be practiced with conventional data processing systems. Thus the construction and operation of the various blocks shown in B and may be of conventional design and need not be described in further detail herein to make and use aspects of the invention because such blocks will be understood by those skilled in the relevant art. One skilled in the relevant art can readily make any modifications necessary to the blocks in B and or other embodiments or figures based on the detailed description provided herein.

Aspects of the invention will now be described in detail with respect to . B and are block diagrams illustrating various environments in which aspects of the invention may be configured to operate. illustrates aspects of the invention interacting with virtual machines e.g. VMware virtual machines or Microsoft virtual machines storing data on a storage device connected to the virtual machine via a Storage Area Network SAN and illustrates aspects of the invention interacting with virtual machines storing data locally. illustrates aspects of the invention interacting with a virtual machine manager e.g. a VMware Virtual Center server or a Microsoft System Center Virtual Machine Manager which manages virtual machines. is a flow diagram illustrating a process for discovering one or more virtual machines in one or more of the environments illustrated in B and or in other environments .

Aspects of the invention can be embodied in a special purpose computer or data processor that is specifically programmed configured or constructed to perform one or more of the computer executable instructions explained in detail herein. Aspects of the invention can also be practiced in distributed computing environments where tasks or modules are performed by remote processing devices which are linked through a communications network such as a Local Area Network LAN a Wide Area Network WAN a SAN a Fibre Channel network or the Internet. In a distributed computing environment program modules may be located in both local and remote memory storage devices.

Aspects of the invention may be stored or distributed on tangible computer readable media including magnetically or optically readable computer discs hard wired or preprogrammed chips e.g. EEPROM semiconductor chips nanotechnology memory biological memory or other tangible or physical data storage media. In some aspects of the system computer implemented instructions data structures screen displays and other data under aspects of the invention may be distributed over the Internet or over other networks including wireless networks on a propagated signal on a propagation medium e.g. an electromagnetic wave s a sound wave etc. over a period of time or they may be provided on any analog or digital network packet switched circuit switched or other scheme . Those skilled in the relevant art will recognize that portions of aspects of the invention may reside on a server computer while corresponding portions reside on a client computer such as a mobile or portable device and thus while certain hardware platforms are described herein aspects of the invention are equally applicable to nodes on a network.

The virtual machine host hosts one or more virtual machines shown individually as virtual machines and . Each virtual machine has its own operating system shown individually as operating systems and and one or more applications executing on the operating system or loaded on the operating system shown individually as applications and . The operating systems may be any type of operating system e.g. Microsoft Windows 95 98 NT 2000 XP 2003 2008 Linux operating systems Sun Solaris operating systems UNIX operating systems etc. that can be hosted by the virtual machine host . The applications may be any applications e.g. database applications file server applications mail server applications web server applications transaction processing applications etc. that may run on the operating systems . The virtual machines are also connected to the network .

The computing device is connected to the primary storage data store via the SAN which may be any type of SAN e.g. a Fibre Channel SAN an iSCSI SAN or any other type of SAN . The primary storage data store stores the virtual disks shown individually as virtual disks and of the virtual machines hosted by the virtual machine host . Virtual disk is used by virtual machine and virtual disk is used by virtual machine . Although each virtual machine is shown with only one virtual disk each virtual machine may have more than one virtual disk in the primary storage data store . As described in more detail herein a virtual disk corresponds to one or more files e.g. one or more .vmdk or .vhd files on the primary storage data store . The primary storage data store stores a primary copy of the data of the virtual machines . Additionally or alternatively the virtual disks may be stored by other storage devices in the environment .

A primary copy of data generally includes a production copy or other live version of the data that is used by a software application and is generally in the native format of that application. Primary copy data may be maintained in a local memory or other high speed storage device e.g. on the virtual disks located in the primary storage data store that allows for relatively fast data access if necessary. Such primary copy data may be intended for short term retention e.g. several hours or days before some or all of the data is stored as one or more secondary copies for example to prevent loss of data in the event a problem occurs with the data stored in primary storage.

In contrast secondary copies include point in time data and are typically intended for long term retention e.g. weeks months or years depending on retention criteria for example as specified in a storage or retention policy before some or all of the data is moved to other storage or discarded. Secondary copies may be indexed so users can browse and restore the data at another point in time. After certain primary copy data is backed up a pointer or other location indicia such as a stub may be placed in the primary copy to indicate the current location of that data. The secondary storage data store stores one or more secondary copies of the data of the virtual machines .

The virtual machine storage manager includes a virtual machine storage operation component which includes a Virtual Logical Unit Number VLUN driver for accessing virtual disks described in more detail herein and a virtual machine mount component for mounting virtual machines described in more detail herein . The virtual machine storage manager also includes a data agent . The data agent includes an integration component that provides functionality for the virtual machine storage operation component . The data agent also includes a virtual disk analyzer component that examines the virtual disk and configuration files corresponding to the virtual disks and extracts metadata from the virtual disk and configuration files. For example the integration component may include a set of scripts that the data agent causes to be run prior to during and or following a copy of virtual machine data. As another example the integration component may be a component that encapsulates or wraps the virtual machine mount component and provides an Application Programming Interface API with functions for accessing the virtual machine mount component . The virtual machine storage manager also includes a data store that maintains data used by the virtual machine storage manager such as data used during storage operations and configuration data.

The secondary storage data store is connected to the computing device . The secondary storage data store may be any type of storage suitable for storing one or more secondary copies of data such as Directly Attached Storage DAS such as hard disks storage devices connected via another SAN e.g. a Fibre Channel SAN an iSCSI SAN or any other type of SAN Network Attached Storage NAS a tape library optical storage or any other type of storage. The secondary storage data store stores virtual machine data that is copied by the virtual machine storage manager . Accordingly the secondary storage data store stores one or more secondary copies of the data of the virtual machines . A secondary copy can be in one or more various formats e.g. a copy set a backup set an archival set a migration set etc. .

The environment also includes a virtual machine manager operating on a computing device e.g. a server . The virtual machine manager includes a virtual machine management component which enables administrators or other users with the appropriate permissions the term administrator is used herein for brevity to manage the virtual machines . The virtual machine manager also includes an Application Programming Interface API component which provides functions that enable the data agent to programmatically interact with the virtual machine manager and the virtual machines . The virtual machine hosts may also each include an API component. The virtual machine manager and or the virtual machine hosts may expose or provide other APIs not illustrated in B or such as an API for accessing and manipulating virtual disks and APIs for performing other functions related to management of virtual machines .

The environments and may include components other than those illustrated in B and respectively and the components illustrated may perform functions other than or in addition to those described herein. For example the virtual machine storage manager may include a public key certificate e.g. an X.509 public key certificate that the virtual machine storage manager provides to the virtual machine host or the virtual machine manager . The virtual machine host or the virtual machine manager can then use the X.509 public key of the certificate to encrypt data that is to be transmitted to the virtual machine storage manager . As another example the network may include a firewall that sits between the virtual machine host and the virtual machine storage manager and data being copied may have to pass through the firewall. If this is the case the virtual machine storage manager may use the systems and methods described in commonly assigned U.S. patent application Ser. No. 10 818 747 entitled SYSTEM AND METHOD FOR PERFORMING STORAGE OPERATIONS THROUGH A FIREWALL the entirety of which is incorporated by reference herein.

As another example a secondary storage computing device which is described in more detail herein e.g. with reference to may be connected to the virtual machine storage manager and to the secondary storage data store . The secondary storage computing device may assist in the transfer of copy data from the virtual machine storage manager to the secondary storage data store . The secondary storage computing device may perform functions such as encrypting compressing single or variable instancing and or indexing data that is transferred to the secondary storage data store . As another example one or more agents e.g. a file system agent and or a proxy host agent as well as a set of utilities e.g. VMware Tools if the virtual machines are VMware virtual machines may reside on each virtual machine to provide functionality associated with copying and restoring virtual machine data. As another example the environments and may include components or agents that perform various functions on virtual machine and other data such as classifying data indexing data and single or variable instancing or deduplicating data at different phases of storage operations performed on virtual machine and other data.

As another example the secondary storage data store may include one or more single instance storage devices that store only a single instance of multiple instances of data e.g. only a single instance of multiple instances of identical files or data objects stored on one or more computing devices . If this is the case the secondary storage data store may include one or more single instance storage devices as described in one or more of the following commonly assigned U.S. patent applications 1 U.S. patent application Ser. No. 11 269 512 entitled SYSTEM AND METHOD TO SUPPORT SINGLE INSTANCE STORAGE OPERATIONS 2 U.S. patent application Ser. No. 12 145 347 entitled APPLICATION AWARE AND REMOTE SINGLE INSTANCE DATA MANAGEMENT or 3 U.S. patent application Ser. No. 12 145 342 entitled APPLICATION AWARE AND REMOTE SINGLE INSTANCE DATA MANAGEMENT 4 U.S. patent application Ser. No. 11 963 623 entitled SYSTEM AND METHOD FOR STORING REDUNDANT INFORMATION 5 U.S. patent application Ser. No. 11 950 376 entitled SYSTEMS AND METHODS FOR CREATING COPIES OF DATA SUCH AS ARCHIVE COPIES or 6 U.S. Pat. App. No. 61 100 686 entitled SYSTEMS AND METHODS FOR MANAGING SINGLE INSTANCING DATA each of which is incorporated by reference herein in its entirety.

As a further example the secondary storage data store may include one or more variable instance storage devices that store a variable number of instances of data e.g. a variable number of instances of identical files or data objects stored on one or more computing devices . If this is the case the secondary storage data store may include one or more variable instance storage devices as described in the following commonly assigned U.S. Pat. App. No. 61 164 803 entitled STORING A VARIABLE NUMBER OF INSTANCES OF DATA OBJECTS .

Virtual disks as used in the systems described in B and may have various configurations. As previously described a virtual disk corresponds to one or more virtual disk files e.g. one or more .vmdk or .vhd files on the primary storage datastore . A virtual machine host may support several types of virtual disks . For example a virtual disk may be either 1 a growable virtual disk contained in a single virtual disk file that can grow in size e.g. a monolithic sparse virtual disk that starts at 2 GB and grows larger 2 a growable virtual disk split into multiple virtual disk files e.g. a split sparse virtual disk comprising multiple 2 GB virtual disk files the aggregation of which can grow in size by adding new virtual disk files 3 a preallocated virtual disk contained in a single virtual disk file e.g. a monolithic flat virtual disk the size of which does not change or 4 a preallocated virtual disk split into multiple virtual disk files e.g. a split flat virtual disk comprising multiple 2 GB virtual disk files the number of which and the size of each of which does not change . Where a virtual disk is split into multiple virtual disk files each individual virtual disk file is called an extent. A virtual machine host may also support types of virtual disks other than these types. Those of skill in the art will understand that a virtual disk can be structured in a wide variety of configurations and that virtual disks are not limited to the configurations described herein.

A virtual machine host may support snapshotting or taking a snapshot of a virtual machine . The virtual machine host can snapshot a virtual machine in a linear fashion in which there is only one branch of snapshots from the original state of the virtual machine and each snapshot in the branch linearly progresses from prior snapshots or in a process tree in which there are multiple branches of snapshots from the original state of the virtual machine and two snapshots may or may not be in the same branch from the original state of the virtual machine . When a snapshot is taken of a virtual machine the virtual machine stops writing to its virtual disks e.g. stops writing to the one or more .vmdk files . The virtual machine writes future writes to a delta disk file e.g. a delta.vmdk file using for example a copy on write COW semantic. As the virtual machine host can snapshot a virtual machine repeatedly there can be multiple delta disk files. The virtual disk and delta disk files can be analogized to links in a chain. Using this analogy the original disk file is a first link in the chain. A first child delta disk file is a second link in the chain and a second child delta disk file is a third link in the chain and so forth.

Also as previously described a virtual machine generally has associated configuration files that a virtual machine host uses to store configuration data about the virtual machine . These configuration files may include a .vmx file which stores data about the parent child relationships created between virtual disk files and delta disk files when a snapshot of a virtual machine is taken. These configuration files may also include a disk descriptor file e.g. a .vmdk file . In some embodiments instead of using a disk descriptor file the disk descriptor is embedded into a virtual disk file e.g. embedded in a .vmdk file .

The disk descriptor file generally stores data about the virtual disk files that make up a virtual disk . This data includes information about the type of the virtual disk . For example the virtual disk may be a monolithic flat virtual disk a monolithic sparse virtual disk a split flat virtual disk a split sparse virtual disk or another type of a virtual disk. This data also includes an identifier of the parent of the virtual disk file if it has one if the virtual machine has been snapshotted its original virtual disk file will have a child virtual disk file a disk database describing geometry values for the virtual disk e.g. cylinders heads and sectors and information describing the extents that make up the virtual disk . Each extent may be described by a line in the disk descriptor file having the following format 

This line describes an extent for which read write access is allowed of size 16777216 sectors of type VMFS e.g. for use on a primary storage data store and the filename of the virtual disk file test flat.vmdk. 

A virtual machine host provides an abstraction layer such that the one or more virtual disks files and any delta disk files of the virtual disks appear as one or more actual disks e.g. one or more hard disk drives to a virtual machine . Because the virtual machine host abstracts the virtual disk so that it appears as an actual disk to an operating system executing on the virtual machine the operating system can generally use its standard file system for storing data on a virtual disk . The various structures used by the file system and the operating system e.g. the partition table s the volume manager database s and the file allocation table s are stored in the one or more virtual disk files that make up a virtual disk .

For example a virtual machine host may store a single virtual disk file e.g. a single .vmdk file that is a preallocated virtual disk a monolithic flat virtual disk for each virtual disk used by a virtual machine operating on the virtual machine host . The single virtual disk file may be named flat.vmdk. There would also be a disk descriptor file for the single virtual disk file that would typically be named .vmdk. A snapshot taken of the virtual machine would result in an additional delta disk file being created that is a single virtual disk file e.g. a single .vmdk file which is a growable virtual disk a monolithic sparse virtual disk . The delta disk file would typically be named delta.vmdk where is a number indicating the sequence of the snapshot. There would also be a disk descriptor file for the single virtual disk file that would typically be named .vmdk again where is a number indicating the sequence of the snapshot.

The process begins at step when the data agent receives an indication specifying that the data agent is to perform a copy operation and how to perform the copy operation. The indication may be received from the administrator e.g. a manually specified indication to perform a copy operation or be triggered automatically e.g. by an automated schedule . The indication may be received as a result of a storage policy that specifies how and or when to copy data from one or more virtual machines to the secondary storage data store .

A storage policy is generally a data structure or other information source that includes a set of preferences and other storage criteria associated with performing a storage operation. The preferences and storage criteria may include but are not limited to a storage location relationships between system components network pathways to utilize in a storage operation retention policies data characteristics compression or encryption requirements preferred system components to utilize in a storage operation a single instancing or variable instancing policy to apply to the data and other criteria relating to a storage operation. For example a storage policy may indicate that certain data is to be stored in the secondary storage data store retained for a specified period of time before being aged to another tier of secondary storage copied to the secondary storage data store using a specified number of data streams etc. A storage policy may be stored in a database of a storage manager see e.g. and accompanying description to archive media as metadata for use in restore operations or other storage operations or to other locations or components of the system. The storage manager may include a jobs agent that monitors the status of some or all storage operations previously performed currently being performed or scheduled to be performed.

For example an administrator may create a storage policy for copying data of virtual machines and perform the copying of their data to the secondary storage data store according to the storage policy. This storage policy may specify that the virtual machine storage manager is to perform a file level copy of certain files e.g. all files in a specific directory or satisfying selection criteria on multiple virtual machines . As yet another example the storage policy may specify that the virtual machine storage manager is to perform a volume level copy of all virtual machines on multiple virtual machine hosts . As another example the storage policy may specify that the virtual machine storage manager is to perform a disk level copy of all virtual machines on all virtual machine hosts associated with a virtual machine manager . File level volume level and disk level copying is discussed in more detail herein for example with reference to .

At decision step the data agent determines e.g. by reading a stored indication of the virtual machine manager or by scanning a network for a virtual machine manager whether there is a virtual machine manager managing the virtual machine hosts and associated virtual machines . If there is a virtual machine manager the process continues at step where the data agent queries the virtual machine manager to determine the virtual machines that it manages and to receive an ordered or unordered list of virtual machines . The data agent may call a function of the API component to determine the virtual machines managed by the virtual machine manager and receive an ordered or unordered list of virtual machines .

If there is not a virtual machine manager the process continues at step where the data agent selects the next virtual machine host which on the first loop is the first determined virtual machine host . The virtual machine hosts may be dynamically determined e.g. by scanning the network or determined statically e.g. by reading a stored indication of the virtual machine hosts . More details as to the detection of virtual machine hosts and virtual machines are described herein for example with reference to . The steps and are not to be understood as mutually exclusive. For example the data agent may determine a first set of virtual machines by accessing the virtual machine manager and a second set of virtual machines by accessing one or more virtual machine hosts .

At step the data agent queries the virtual machine host to determine the virtual machines that it hosts. The data agent may call a function of the API component to determine the virtual machines hosted by the virtual machine host and to receive an ordered or unordered list of virtual machines . At step the data agent begins looping through the list of virtual machines that it determined in either or both of steps or and selects the next virtual machine on the list which on the first loop is the first determined virtual machine . At step the data agent copies the data of the virtual machine for example according to the indication received in step or according to a storage policy. This process is described in more detail herein for example with reference to .

At step other processing of virtual machine data may be performed. For example the data agent or another agent such as a data classification agent may analyze and classify the virtual machine data. To do so the data agent may use techniques such as those described in commonly assigned U.S. patent application Ser. No. 11 564 119 entitled SYSTEMS AND METHODS FOR CLASSIFYING AND TRANSFERRING INFORMATION IN A STORAGE NETWORK the entirety of which is incorporated by reference herein. As another example the data agent or another agent such as an indexing agent may create an index of the virtual machine data. To do so the data agent may use techniques such as those described in commonly assigned U.S. patent application Ser. No. 11 694 869 entitled METHOD AND SYSTEM FOR OFFLINE INDEXING OF CONTENT AND CLASSIFYING STORED DATA the entirety of which is incorporated herein. As a final example the data agent may single or variable instance or de duplicate the virtual machine data. To do so the data agent may use techniques described in one or more of previously referenced U.S. patent application Ser. Nos. 11 269 512 12 145 347 12 145 342 11 963 623 11 950 376 61 100 686 and 61 164 803. At decision step the data agent determines whether there are more virtual machines for which the data is to be copied. If so the data agent returns to step where the next virtual machine is selected.

If there are no more virtual machines for which the data is to be copied e.g. if the data agent has looped through the list of all the virtual machines determined in either or both of steps or the process continues at step . At decision step if there is not a virtual machine manager e.g. as determined in decision step the data agent determines whether there are more virtual machine hosts e.g. if more than one virtual machine hosts was specified in the indication received in step . If there are more virtual machine hosts the data agent returns to step . If not the process concludes.

Referring to representative computer displays or web pages for configuring storage operations to be performed for virtual machine data will now be described. The screens of may be implemented in any of various ways such as in C or as web pages in XML Extensible Markup Language HTML Hyper Text Markup Language or any other scripts or methods of creating displayable data such as the Wireless Access Protocol WAP . The screens or web pages provide facilities to present information and receive input data such as a form or page with fields to be filled in pull down menus or entries allowing one or more of several options to be selected buttons sliders hypertext links or other known user interface tools for receiving user input. While certain ways of displaying information to users is shown and described with respect to certain Figures those skilled in the relevant art will recognize that various other alternatives may be employed. The terms screen web page and page are generally used interchangeably herein.

When implemented as web pages the screens are stored as display descriptions graphical user interfaces or other methods of depicting information on a computer screen e.g. commands links fonts colors layout sizes and relative positions and the like where the layout and information or content to be displayed on the page is stored in a database typically connected to a server. In general a link refers to any resource locator identifying a resource on a network such as a display description provided by an organization having a site or node on the network. A display description as generally used herein refers to any method of automatically displaying information on a computer screen in any of the above noted formats as well as other formats such as email or character code based formats algorithm based formats e.g. vector generated or matrix or bit mapped formats. While aspects of the invention are described herein using a networked environment some or all features may be implemented within a single computer environment.

Buttons enable the administrator to confirm or cancel the selections and or view help regarding the interface . The interface may enable discovery of virtual machines by both regular expression matching and by association with one or more specified virtual machine hosts . For example the interface could be configured to discover all virtual machines associated with a specific virtual machine host as well as an additional number of virtual machines having names that match a regular expression e.g. virtual A .

An administrator can also establish additional sub clients to provide a further level of protection of virtual machine data. For example for a virtual machine upon which is loaded a mail application e.g. a Microsoft Exchange mail server and a database application e.g. an Oracle database application the administrator could establish one sub client for protection of the data of the mail application e.g. user mailboxes and one sub client for protection of the data of the database application e.g. databases datafiles and or tablespaces . As another example the administrator could establish sub clients for organizational groupings e.g. a sub client for a marketing group a sub client for a sales group etc. and or for virtual machines based upon their purpose e.g. a sub client for virtual machines used in production settings a sub client for virtual machines used in test and or development settings etc. . Those of skill in the art will understand that an administrator may establish sub clients according to various groupings.

An administrator can specify that any newly discovered virtual machines that do not qualify for membership in an established sub client group are to be added to the default sub client by selecting check box . The administrator can also select that the data agent is to discover virtual machines and add them to particular sub clients based upon rules by selecting check box . Since check box is not selected the options below it may not be selectable. However selecting check box allows the administrator to select radio buttons shown individually as radio buttons and and buttons shown individually as buttons and which enable functionality similar to that discussed with reference to the interface of . For example selection of the radio button and the button can enable the administrator to specify that all virtual machines that match a regular expression e.g. the regular expression a g could be used to match any virtual machines or any virtual machine hosts beginning with names for which the first character begins with any character in the range of a to g are to be added to a particular sub client. As another example selection of the radio button and the button can enable the administrator to specify that all virtual machines that are associated with a particular virtual machine host that is identified by e.g. name IP address and or other identifier are to be added to a particular sub client. Buttons enable the administrator to confirm or cancel the selections and or view help regarding the interface .

Other virtual machines that are part of the groups or are shown as being part of other sub clients such as the virtual machine named VM2 that is part of a sub client named Database SC which may be a sub client directed toward protecting data of a database application and the virtual machine named VM3 that is part of a sub client named Filesrv SC which may be a sub client directed toward protecting data on a file server. Similarly the virtual machines named SG111 and SG3 1 are both part of a sub client named Marketing Sales SC which may be a sub client directed toward protecting data of marketing and sales organizations. The virtual machine named W2K8 SC is also part of the sub client . Accordingly two different virtual machines on two different virtual machine hosts may be part of the same sub client.

The sub client may also include other non virtual machines. Non virtual machines can be defined broadly to include operating systems on computing devices that are not virtualized. For example the operating systems of the virtual machine hosts the virtual machine manager and the virtual machine storage manager can be considered to be non virtual machines. In this case the same storage policy would be applied to protect data of both the associated virtual machines and the non virtual machines. An administrator can select one or more virtual machine hosts and select a sub client using the listbox and then select the button labeled Apply to change all of the selected virtual machine hosts to a selected sub client. When the administrator selects the button labeled Discover an automated process for discovering virtual machine hosts and or virtual machines is started. When it concludes the interface displays any virtual machine hosts and or virtual machines discovered by the process. Buttons enable the administrator to confirm or cancel the selections and or view help regarding the interface .

As previously described the integration component encapsulates the virtual machine mount component and provides an API for accessing the virtual machine mount component . For example if the virtual machines are VMware virtual machines the virtual machine mount component may be VMware s vcbMounter command line tool and the integration component may encapsulate the functionality provided by vcbMounter into an API and redirect the output of the vcbMounter tool. At step the data agent calls an API function of the integration component to quiesce the file systems of the virtual machine . Quiescing the file systems ensures that no file system writes are pending at the time a snapshot of a virtual machine is taken thereby allowing the creation of filesystem consistent copies. The data agent may prior to quiescing the file systems in step also quiesce applications that are executing on the virtual machine or are loaded on the virtual machine .

At step the data agent calls an API function of the integration component to put the virtual machine into snapshot mode. Alternatively the data agent may call a function of the API component to put the virtual machine into snapshot mode. When the virtual machine is put into snapshot mode the virtual machine stops writing to its virtual disks e.g. stops writing to the one or more .vmdk files or .vhd files on the primary storage data store . The virtual machine writes future writes to a delta disk file e.g. a delta.vmdk file on the primary storage data store . Putting the virtual machine into snapshot mode enables the virtual machine to continue operating during the process . At step the data agent calls an API function of the integration component to unquiesce the file systems of the virtual machine . The data agent may subsequent to unquiescing the file systems in step also unquiesce any applications that were previously quiesced.

At step the data agent determines e.g. based upon the indication received in step of the process of how to copy the data of the virtual machine . For example the data agent may copy the data of the virtual machine in one of three ways 1 a file level copy 2 an image level copy or 3 a disk level copy.

If the indication specifies that the data agent is to perform a file level copy the process branches to the file level copy branch. For example an administrator may provide that a file level copy is to be performed if the administrator wishes to copy only certain files on a volume of a virtual disk e.g. only files within a certain directory or files that satisfy certain criteria . At step the data agent determines a mount point of the data store on the virtual machine storage manager e.g. by dynamically determining an available mount point or by reading a stored indication of a mount point to use . For example the mount point may be C mount on the data store . At step the data agent determines the volumes of the virtual machine e.g. by calling an API function of the integration component or by calling a function of the API component . For example a virtual machine using a Microsoft Windows operating system may have a C volume a D volume and so forth. At step the data agent mounts the determined volumes containing files at the determined mount point of the data store e.g. by again calling an API function of the integration component or by calling a function of the API component .

As previously described a virtual disk corresponds to one or more files e.g. one or more .vmdk or .vhd files called virtual disk files on the primary storage datastore . A volume may span one or more virtual disks or one or more volumes may be contained within a virtual disk . When the data agent mounts the determined volumes the primary storage data store sends to the VLUN driver a block list of the virtual disk files corresponding to the virtual disks of the determined volumes. The VLUN driver uses the block list information to present the determined volumes e.g. as read only volumes or as read write volumes to the operating system of the virtual machine storage manager . The data agent communicates with the VLUN driver to mount the determined volumes at the mount point of the virtual machine storage manager . Using the previous examples of a virtual machine with a C volume and a D volume the data agent would mount these volumes at the following respective locations 

After mounting the determined volumes the data agent can present to an administrator an interface displaying the mounted volumes and the directories and files on the mounted volumes to enable the administrator to select which files and or directories are to be copied. Alternatively files and or directories can be automatically selected in accordance with a storage policy determined by the virtual machine s membership in a sub client or in accordance with a set of criteria or rules. At step the data agent copies the selected files and or directories on the determined volumes to the secondary storage data store e.g. via a secondary storage computing device . The data agent does so by providing an indication of a file and or directory that is to be copied to the VLUN driver which requests the blocks corresponding to the selected file and or directory in the virtual disk files on the primary storage datastore . The mapping between blocks and files directories may be maintained by the primary storage data store e.g. in a table or other data structure .

After completing the copy the data agent at step unmounts the determined volumes from the virtual machine storage manager e.g. by calling an API function of the integration component or by calling a function of the API component . At step the data agent calls an API function of the integration component to take the virtual machine out of snapshot mode. Alternatively the data agent may call a function of the API component to take the virtual machine out of snapshot mode. Taking the virtual machine out of snapshot mode consolidates the writes from the delta disk file e.g. any intervening write operations to the virtual disk between the time the virtual machine was put into snapshot mode and the time it was taken out of snapshot mode to the virtual disk file of the virtual disk . In this way performing a copy operation on a primary copy of virtual machine data does not affect the virtual machine s use of the data. Rather operations can pick up at the point where they left off. The process then concludes.

If the indication specifies that the data agent is to perform a volume level copy the process branches to the volume level copy branch. The process for performing a volume level copy is similar to that for performing a file level copy and steps through are effectively the same for this second branch of the process . At step the data agent analyzes the virtual volume and extracts metadata from the virtual volume. This process is described in more detail herein e.g. with reference to .

After mounting the determined volumes step of the volume level copy branch the data agent can present to an administrator an interface displaying the mounted volumes optionally the files and or directories on the mounted volumes can also be displayed to enable the administrator to select which volumes are to be copied. Alternatively volumes can be automatically selected in accordance with a storage policy determined by the virtual machine s membership in a sub client or in accordance with a set of criteria or rules. At step the data agent copies the selected volumes at the mount point on the virtual machine storage manager to the secondary storage data store e.g. via a secondary storage computing device . The data agent does so by providing an indication of a volume that is to be copied to the VLUN driver which requests the blocks corresponding to the selected volumes in the virtual disk files on the primary storage datastore . The mapping between blocks and volumes may be maintained by the primary storage data store e.g. in a table or other data structure .

After copying the data agent at step unmounts the determined volumes from the virtual machine storage manager e.g. by calling an API function of the integration component or by calling a function of the API component . At step the data agent calls an API function of the integration component to take the virtual machine out of snapshot mode. Alternatively the data agent may call a function of the API component to take the virtual machine out of snapshot mode. Taking the virtual machine out of snapshot mode consolidates the writes to the delta disk file to the virtual disk file of the virtual disk . The process then concludes.

One advantage of performing copy operations at the file level or the volume level is that the data agent can copy the virtual machine data from the primary storage datastore to the secondary storage data store without having to copy it to the datastore on the virtual machine storage manager . Stated another way the data agent can obtain the virtual machine data from the primary storage datastore perform any specified operations upon it e.g. compress it single or variable instance it encrypt it etc. and stream the virtual machine data to the secondary storage data store e.g. via a secondary storage computing device without staging or caching the data at the virtual machine storage manager . This allows the data agent to copy the data directly to the secondary storage data store without first copying it to an intermediate location. Accordingly the data agent can quickly and efficiently perform file level and volume level copies of data of virtual machines .

The file level copy and the volume level copy can be thought of as operating at the virtual level. In other words the data agent may have to utilize data structures functions or other information or aspects exposed or provided by a virtual machine or the virtual machine host in order to copy the data of the virtual machine to the secondary storage data store . For example in order to perform a file level or volume level copy of the data of a virtual machine the data agent utilizes some information or aspect of the virtual machine to determine its files and directories and or volumes. The data agent does so in order to present the determined files and directories or volumes for their selection by an administrator or in order to apply implement or execute storage operations according to a storage policy. In contrast a disk level copy can be thought of as operating at a non virtual level e.g. at a level of the physical computer hosting the virtual machine and or the physical storage media upon which the virtual machine data is stored . In other words the data agent can directly access the physical storage media storing the data of the virtual machine e.g. the primary storage data store connected via the SAN to copy the virtual disks of the virtual machine . Because the data agent is copying the virtual disks without necessarily determining files and directories or volumes of the virtual machine the data agent does not necessarily have to utilize information or aspects of the virtual machine or the virtual machine host .

If the indication specifies that the data agent is to perform a disk level copy the process branches to the disk level copy branch. At step the data agent determines a copy point on the data store . For example the copy point may be C copy copyvirtualmachine on the data store . At step the data agent determines the virtual disk and any associated configuration files e.g. the .vmx file and or the disk descriptor files of the virtual machine e.g. by calling an API function of the integration component or by calling a function of the API component . The primary storage data store sends to the VLUN driver a block list of the virtual disk and configuration files. At step the data agent copies these files to the copy point on the datastore on the virtual machine storage manager . The data agent does so by providing an indication of the virtual disk and configuration files to the VLUN driver which requests the blocks corresponding to the virtual disk and configuration files from the primary storage datastore . The mapping between blocks and files directories may be maintained by the primary storage data store e.g. in a table or other data structure .

At step the data agent calls an API function of the integration component to take the virtual machine out of snapshot mode or calls a function of the API component . At step the data agent analyzes the virtual disk and configuration files and extracts metadata from the virtual disk and configuration files. This process is described in more detail herein e.g. with reference to . At step the data agent copies the virtual disk and configuration files to the secondary storage data store . At step the data agent removes the copied virtual disk and configuration files from the data store on the virtual machine storage manager . The process then concludes.

Because a disk level copy operates essentially at a non virtual level it may not have to utilize information or aspects of the virtual machine or the virtual machine host in order to copy its data to the secondary storage data store . Therefore a disk level copy may not necessarily involve much of the overhead involved in a file level copy or a volume level copy. Rather a disk level copy can directly access the physical storage media storing the data of the virtual machine e.g. the primary storage data store to copy the virtual disks of the virtual machine . Because a disk level copy can directly access the primary storage data store the volumes on the virtual disks do not need to be mounted. Accordingly a disk level copy may be performed faster and more efficiently than a file level copy or a volume level copy.

Certain steps in the following process for extracting metadata from the virtual volumes and or the virtual disk and configuration files are described below using a configuration of a virtual machine having a single virtual disk comprised in a single virtual disk file. Those of skill in the art will understand that the process is not limited in any way to this configuration. Rather the following process may be used to extract metadata from virtual disk and configuration files that are arranged or structured in a wide variety of configurations such as multiple virtual disks spanning multiple virtual disk files. Generally metadata refers to data or information about data. Metadata may include for example data relating to relationships between virtual disk files data relating to how volumes are structured on virtual disks and data relating to a location of a file allocation table or a master file table. Metadata may also include data describing files and data objects e.g. names of files or data objects timestamps of files or data objects ACL entries and file or data object summary author source or other information . Metadata may also include data relating to storage operations or storage management such as data locations storage management components associated with data storage devices used in performing storage operations index data data application type or other data. Those of skill in the art will understand that metadata may include data or information about data other than the examples given herein.

For example for a VMware virtual machine the virtual disk analyzer component may read and analyze the .vmx configuration files and or the .vmdk disk descriptor files. In this example the parent virtual disk may be named basedisk.vmdk. The parent virtual disk may have a .vmdk disk descriptor file with an entry that uniquely identifies the parent virtual disk having the following syntax 

For example the entry CID daf6cf10 comports with this syntax. A first child virtual disk e.g. a first snapshot may be named basedisk 000001.vmdk. The first child virtual disk may have a .vmdk disk descriptor file with an entry that uniquely identifies its parent having the following syntax 

For example the entry parentCID daf6cf10 comports with this syntax. The virtual disk analyzer component may identify parent child relationships between virtual disk files in other ways such as by observing access to virtual disk files inferring the relationships by such observations. At step the virtual disk analyzer component determines the relationships between virtual disk files the virtual disk analyzer component determines how the virtual disk is structured how many extents make up each link in the chain . The virtual disk analyzer component performs this step by reading and analyzing the disk descriptor file if it is a separate file or by reading the disk descriptor information if it is embedded into the virtual disk file. The virtual disk analyzer component may determine the relationships between virtual disk files in other ways such as by observing access to virtual disk files and inferring the relationships from such observations.

At step the virtual disk analyzer component determines how the partitions and volumes are structured on the virtual disks . The virtual disk analyzer component does this by reading the sectors of the virtual disks that contain the partition tables to determine how the virtual disk is structured e.g. whether it is a basic or a dynamic disk . The virtual disk analyzer component also reads the sectors of the virtual disks that contain the logical volume manager databases. Because the locations of these sectors e.g. the sectors of the partition tables and the logical volume manager databases are well known and or can be dynamically determined the virtual disk analyzer component can use techniques that are well known to those of skill in the art to read those sectors and extract the necessary data. The virtual disk analyzer component is thus able to determine how the virtual disks is partitioned by the operating system of the virtual machine and how volumes are laid out in the virtual disks e.g. if there are simple volumes spanned volumes striped volumes mirrored volumes and or RAID 5 volumes etc. 

At step the virtual disk analyzer component determines the location of the Master File Table MFT or similar file allocation table for each volume. As with the partition tables and the logical volume manager databases the locations of the sectors containing the MFT are well known and or can be dynamically determined. Therefore the virtual disk analyzer component can use techniques that are well known to those of skill in the art to determine the location of the MFT. At step the virtual disk analyzer component stores the determined parent child relationships relationships between virtual disk files the determined structure of volumes of the virtual disks and the determined location of the MFT in a data structure such as a table. For example a table having the following schema may be used to store this information 

The virtual disk analyzer component may use other data structures to store this information in addition or as an alternative to the preceding table. The virtual disk analyzer component may store this information in the secondary storage data store or in another data store. The virtual disk analyzer component may also collect other metadata such as metadata describing virtual disks and or metadata describing files and or data objects within virtual disks . For example instead of storing the determined location of the MFT the virtual disk analyzer component could store the locations of files or data objects within virtual disks . After storing this metadata the process then concludes.

The process begins at step where the secondary storage computing device receives an indication to restore data of one or more virtual machines . The indication can be to restore one or more files one or more volumes one or more virtual disks of a virtual machine or an entire virtual machine . At step the secondary storage computing device determines how e.g. by analyzing the index the data agent originally copied the virtual machine data either 1 a file level copy 2 an image level copy or 3 a disk level copy.

If the data agent originally performed a file level copy the process branches to the file level restore branch. At step the secondary storage computing device mounts a copy set corresponding to the files to be restored from the secondary storage data store . The copy set may be manually selected by an administrator or automatically selected based on an association between the copy set and the virtual machine from which the data in the copy set came. Additionally or alternatively the copy set may be automatically determined based upon the metadata extracted and stored described with reference to e.g. or based upon other metadata e.g. metadata stored in index .

Because the data agent originally performed a file level copy of selected files and or directories the secondary storage computing device generally restores files and or directories out of the copy set. At step the secondary storage computing device restores one or more files or directories e.g. a single file out of the copy set. For example the secondary storage computing device can call a function of an API exposed by a virtual machine or its hosting virtual machine host to restore the one or more files or directories to the virtual machine . As another example the secondary storage computing device can copy the one or more files or directories to the primary storage data store . The secondary storage computing device can restore the one or more files or directories to the original virtual machine from which they were originally copied to a different virtual machine to a non virtual machine and or to another storage device . The process then concludes.

If the data agent originally performed a volume level copy the process branches to the volume level restore branch. At step the secondary storage computing device mounts a copy set corresponding to the files or volumes to be restored from the secondary storage data store . The copy set may be manually selected by an administrator or automatically selected based on an association between the copy set and the virtual machine from which the data in the copy set came. Additionally or alternatively the copy set may be automatically determined based upon the metadata extracted and stored described with reference to e.g. or based upon other metadata e.g. metadata stored in index .

At step the secondary storage computing device accesses metadata corresponding to the data that is to be restored e.g. the determined location of the MFT . This is the metadata that was stored in step of the process . At step the secondary storage computing device uses the determined location of the MFT to access the MFT and use the entries in the MFT to determine where the files and directories on the virtual disk are located e.g. on which sectors of the virtual disk a particular file is located .

Because the data agent originally performed a volume level copy of selected volumes including files and or directories within the volumes the secondary storage computing device can generally restore both files and or directories and entire volumes e.g. an entire C volume an entire D volume etc. out of the copy set. If the secondary storage computing device is to restore a file the process branches to step . At this step the secondary storage computing device restores one or more files or directories out of the copy set e.g. a single file . The secondary storage computing device can restore the one or more files or directories to the original virtual machine from which they were originally copied to a different virtual machine to a non virtual machine and or to another storage device . For example if the original virtual machine no longer exists the one or more files or directories may be restored to its replacement.

If instead the secondary storage computing device is to restore a volume the process branches to step . At this step the secondary storage computing device restores one or more volumes out of the copy set. The data agent secondary storage computing device can restore the one or more volumes to the original virtual machine from which they were originally copied up to a different virtual machine or to a non virtual machine and or to another storage device . For example a C volume may be restored out of a copy set to the original virtual machine from which it was copied thus overwriting its existing C volume. As another example a D volume may be restored out of a copy set to another virtual machine thus replacing its current D volume.

The secondary storage computing device may restore the files directories and or volumes to various locations. For example the secondary storage computing device can copy the files directories and or volumes to the primary storage data store . The secondary storage computing device can restore the one or more volumes to the original virtual machine from which they were originally copied up to a different virtual machine to a non virtual machine e.g. to a physical machine and or to another storage device . For example an entire D volume from an original virtual machine may be restored to the original virtual machine to another virtual machine and or to a non virtual machine e.g. to a physical machine . As described in more detail herein a volume of a virtual machine may be restored in its original format e.g. if the volume came from a VMware virtual machine it can be restored as a volume in the VMware format such as a .vmdk file or converted to another format e.g. if the volume came from a VMware virtual machine it can be restored as a volume in the Microsoft format such as a .vhd file . The secondary storage computing device can also restore the volume as a container file from which the volume can be extracted. After steps and or the process then concludes.

If the data agent originally performed a disk level copy the process branches to the disk level restore branch. At step the secondary storage computing device mounts a copy set corresponding to the virtual disks files volumes and or virtual machines to be restored from the secondary storage data store . The copy set may be manually selected by an administrator or automatically selected based on an association between the copy set and the virtual machine from which the data in the copy set came. Additionally or alternatively the copy set may be automatically determined based upon the metadata extracted and stored described herein e.g. with reference to or based upon other metadata e.g. metadata stored in index .

At step the secondary storage computing device accesses metadata corresponding to the data that is to be restored e.g. the determined parent child relationships and relationships between virtual disk files the determined structure of volumes of the virtual disks and the determined location of the MFT . This is the metadata that was stored in step of the process . At step the secondary storage computing device uses the determined parent child relationships and relationships between virtual disk files to reconstruct the virtual disks . For example if a virtual disk is comprised of numerous virtual disk files the secondary storage computing device uses the determined relationships between them to link them together into a single virtual disk file. In so doing the secondary storage computing device may access grain directories and grain tables within virtual disk files. Grain directories and grain tables are data structures located within virtual disk files that specify the sectors blocks within virtual disks that have been allocated for data storage. The secondary storage computing device may access these data structures to locate data within virtual disks .

At step the secondary storage computing device uses the determined structure of volumes of the virtual disks to reconstruct the volumes. At step the secondary storage computing device uses the determined location of the MFT to access the MFT and uses the entries in the MFT to determine where the files and directories on the virtual disk are located e.g. on which sectors of the virtual disk a particular file is located .

Because the data agent originally performed a disk level copy of virtual disk and configuration files the secondary storage computing device can restore files or directories entire volumes e.g. an entire C volume an entire D volume etc. as well as an entire virtual machine out of the copy set. If an entire virtual machine is to be restored the process branches to step . The secondary storage computing device can copy all the virtual disk and configuration files to the location where the entire virtual machine is to be restored. This can be the original location of the virtual machine on the original virtual machine host or it can be a new location where the virtual machine had not originally been located e.g. on a new virtual machine host . If the virtual disk and configuration files are copied to the original virtual machine host the virtual machine host should be able to restart the virtual machine which can then recommence operating in the state it existed in when its virtual disk and configuration files were originally copied.

Similarly if the virtual disk and configuration files are copied to a new virtual machine host the new virtual machine host should be able to start the virtual machine which can then commence operating in the state it existed in when its virtual disk and configuration files were originally copied. The ability to restore a virtual machine to a new virtual machine host other than its original virtual machine host allows virtual machines to be moved or floated from one virtual machine host to another. The secondary storage computing device can also restore the entire virtual machine as a container file from which the entire virtual machine can be extracted. After step the process then concludes.

If instead of restoring an entire virtual machine the secondary storage computing device is to restore a volume the process branches to step . At this step the secondary storage computing device restores one or more volumes out of the copy set. After step the process then concludes.

If instead of restoring an entire virtual machine or a volume the secondary storage computing device is to restore a file the process branches to step . At this step the secondary storage computing device restores one or more files or directories out of the copy set e.g. a single file . The secondary storage computing device can restore the one or more files or directories to the original virtual machine from which they were originally copied to a different virtual machine to non virtual machine and or to another storage device . The process then concludes.

If instead of restoring an entire virtual machine a volume or a file the secondary storage computing device is to restore one or more virtual disks the process branches to step . At this step the secondary storage computing device restores the virtual disk and configuration files corresponding to the one or more virtual disks to be restored out of the copy set. The secondary storage computing device can restore the one or more virtual disks to the original virtual machine host from which they were originally copied. Additionally or alternatively the secondary storage computing device can restore the one or more virtual disks to the original virtual machine from which they were originally copied to a different virtual machine to a non virtual machine and or to another storage device . If the one or more virtual disks are to be restored to the virtual machine they may overwrite replace and or supplement the existing virtual disks of a virtual machine . The process then concludes.

Depending upon what the secondary storage computing device is to restore certain steps in the process may not need to be performed. For example if the secondary storage computing device is to restore an entire virtual machine out of a disk level copy the data agent may not need to access the stored metadata step or reconstruct the virtual disk volumes and files steps and . The data agent can simply mount the copy set and copy the virtual disk and configuration files to the appropriate location. As another example if the secondary storage computing device is to restore a volume out of a disk level copy the secondary storage computing device may not need to reconstruct files using the MFT as mentioned above. The secondary storage computing device can simply reconstruct the volumes and then copy the volumes to the appropriate location. Those of skill in the art will understand that more or fewer steps than those illustrated in the process may be used to restore data of virtual machines .

As previously described one advantage of performing a disk level copy is that it may be quicker and more efficient than file level or volume level copying. Also as previously described the process of extracting metadata from the virtual disk and configuration files enables the ability to restore individual files directories and or volumes to the virtual machine or to other locations e.g. to other virtual machines to non virtual machines and or to other storage devices . The combination of a disk level copy and the capability to restore individual files directories and or volumes of a virtual machine provides for a fast and efficient process for duplicating primary copies of data while still enabling granular access e.g. at the individual file or data object level to the duplicated primary data granular access to the secondary copies of data is enabled . This combination optimizes the aspect of virtual machine data management that is likely performed most frequently duplication of primary copies of data but not at the expense of the aspect that is likely performed less often restoration of secondary copies of data because granular access to duplicated primary copies of data is still enabled.

The system may generally include combinations of hardware and software components associated with performing storage operations on electronic data. Storage operations include copying backing up creating storing retrieving and or migrating primary storage data e.g. data stores and or and secondary storage data which may include for example snapshot copies backup copies HSM copies archive copies and other types of copies of electronic data stored on storage devices . The system may provide one or more integrated management consoles for users or system processes to interface with in order to perform certain storage operations on electronic data as further described herein. Such integrated management consoles may be displayed at a central control facility or several similar consoles distributed throughout multiple network locations to provide global or geographically specific network data storage information.

In one example storage operations may be performed according to various storage preferences for example as expressed by a user preference a storage policy a schedule policy and or a retention policy. A storage policy is generally a data structure or other information source that includes a set of preferences and other storage criteria associated with performing a storage operation. The preferences and storage criteria may include but are not limited to a storage location relationships between system components network pathways to utilize in a storage operation data characteristics compression or encryption requirements preferred system components to utilize in a storage operation a single instancing or variable instancing policy to apply to the data and or other criteria relating to a storage operation. For example a storage policy may indicate that certain data is to be stored in the storage device retained for a specified period of time before being aged to another tier of secondary storage copied to the storage device using a specified number of data streams etc.

A schedule policy may specify a frequency with which to perform storage operations and a window of time within which to perform them. For example a schedule policy may specify that a storage operation is to be performed every Saturday morning from 2 00 a.m. to 4 00 a.m. In some cases the storage policy includes information generally specified by the schedule policy. Put another way the storage policy includes the schedule policy. Storage policies and or schedule policies may be stored in a database of the storage manager to archive media as metadata for use in restore operations or other storage operations or to other locations or components of the system .

The system may comprise a storage operation cell that is one of multiple storage operation cells arranged in a hierarchy or other organization. Storage operation cells may be related to backup cells and provide some or all of the functionality of backup cells as described in the assignee s U.S. patent application Ser. No. 09 354 058 now U.S. Pat. No. 7 395 282 which is incorporated herein by reference in its entirety. However storage operation cells may also perform additional types of storage operations and other types of storage management functions that are not generally offered by backup cells.

Storage operation cells may contain not only physical devices but also may represent logical concepts organizations and hierarchies. For example a first storage operation cell may be configured to perform a first type of storage operations such as HSM operations which may include backup or other types of data migration and may include a variety of physical components including a storage manager or management agent a secondary storage computing device a client and other components as described herein. A second storage operation cell may contain the same or similar physical components however it may be configured to perform a second type of storage operations such as storage resource management SRM operations and may include monitoring a primary data copy or performing other known SRM operations.

Thus as can be seen from the above although the first and second storage operation cells are logically distinct entities configured to perform different management functions HSM and SRM respectively each storage operation cell may contain the same or similar physical devices. Alternatively different storage operation cells may contain some of the same physical devices and not others. For example a storage operation cell configured to perform SRM tasks may contain a secondary storage computing device client or other network device connected to a primary storage volume while a storage operation cell configured to perform HSM tasks may instead include a secondary storage computing device client or other network device connected to a secondary storage volume and not contain the elements or components associated with and including the primary storage volume. The term connected as used herein does not necessarily require a physical connection rather it could refer to two devices that are operably coupled to each other communicably coupled to each other in communication with each other or more generally refer to the capability of two devices to communicate with each other. These two storage operation cells however may each include a different storage manager that coordinates storage operations via the same secondary storage computing devices and storage devices . This overlapping configuration allows storage resources to be accessed by more than one storage manager such that multiple paths exist to each storage device facilitating failover load balancing and promoting robust data access via alternative routes.

Alternatively or additionally the same storage manager may control two or more storage operation cells whether or not each storage operation cell has its own dedicated storage manager . Moreover in certain embodiments the extent or type of overlap may be user defined through a control console or may be automatically configured to optimize data storage and or retrieval.

Data agent may be a software module or part of a software module that is generally responsible for performing storage operations on the data of the client stored in data store or other memory location. Each client may have at least one data agent and the system can support multiple clients . Data agent may be distributed between client and storage manager and any other intermediate components or it may be deployed from a remote location or its functions approximated by a remote process that performs some or all of the functions of data agent .

The overall system may employ multiple data agents each of which may perform storage operations on data associated with a different application. For example different individual data agents may be designed to handle Microsoft Exchange data Lotus Notes data Microsoft Windows 2000 file system data Microsoft Active Directory Objects data and other types of data known in the art. Other embodiments may employ one or more generic data agents that can handle and process multiple data types rather than using the specialized data agents described above.

If a client has two or more types of data one data agent may be required for each data type to perform storage operations on the data of the client . For example to back up migrate and restore all the data on a Microsoft Exchange 2000 server the client may use one Microsoft Exchange 2000 Mailbox data agent to back up the Exchange 2000 mailboxes one Microsoft Exchange 2000 Database data agent to back up the Exchange 2000 databases one Microsoft Exchange 2000 Public Folder data agent to back up the Exchange 2000 Public Folders and one Microsoft Windows 2000 File System data agent to back up the file system of the client . These data agents would be treated as four separate data agents by the system even though they reside on the same client .

Alternatively the overall system may use one or more generic data agents each of which may be capable of handling two or more data types. For example one generic data agent may be used to back up migrate and restore Microsoft Exchange 2000 Mailbox data and Microsoft Exchange 2000 Database data while another generic data agent may handle Microsoft Exchange 2000 Public Folder data and Microsoft Windows 2000 File System data etc.

Data agents may be responsible for arranging or packing data to be copied or migrated into a certain format such as an archive file. Nonetheless it will be understood that this represents only one example and any suitable packing or containerization technique or transfer methodology may be used if desired. Such an archive file may include metadata a list of files or data objects copied the file and data objects themselves. Moreover any data moved by the data agents may be tracked within the system by updating indexes associated with appropriate storage managers or secondary storage computing devices . As used herein a file or a data object refers to any collection or grouping of bytes of data that can be viewed as one or more logical units.

Generally speaking storage manager may be a software module or other application that coordinates and controls storage operations performed by the system . Storage manager may communicate with some or all elements of the system including clients data agents secondary storage computing devices and storage devices to initiate and manage storage operations e.g. backups migrations data recovery operations etc. .

Storage manager may include a jobs agent that monitors the status of some or all storage operations previously performed currently being performed or scheduled to be performed by the system . Jobs agent may be communicatively coupled to an interface agent e.g. a software module or application . Interface agent may include information processing and display software such as a graphical user interface GUI an application programming interface API or other interactive interface through which users and system processes can retrieve information about the status of storage operations. For example in an arrangement of multiple storage operations cell through interface agent users may optionally issue instructions to various storage operation cells regarding performance of the storage operations as described and contemplated herein. For example a user may modify a schedule concerning the number of pending snapshot copies or other types of copies scheduled as needed to suit particular needs or requirements. As another example a user may employ the GUI to view the status of pending storage operations in some or all of the storage operation cells in a given network or to monitor the status of certain components in a particular storage operation cell e.g. the amount of storage capacity left in a particular storage device .

Storage manager may also include a management agent that is typically implemented as a software module or application program. In general management agent provides an interface that allows various management agents in other storage operation cells to communicate with one another. For example assume a certain network configuration includes multiple storage operation cells hierarchically arranged or otherwise logically related in a WAN or LAN configuration. With this arrangement each storage operation cell may be connected to the other through each respective interface agent . This allows each storage operation cell to send and receive certain pertinent information from other storage operation cells including status information routing information information regarding capacity and utilization etc. These communications paths may also be used to convey information and instructions regarding storage operations.

For example a management agent in a first storage operation cell may communicate with a management agent in a second storage operation cell regarding the status of storage operations in the second storage operation cell. Another illustrative example includes the case where a management agent in a first storage operation cell communicates with a management agent in a second storage operation cell to control storage manager and other components of the second storage operation cell via management agent contained in storage manager .

Another illustrative example is the case where management agent in a first storage operation cell communicates directly with and controls the components in a second storage operation cell and bypasses the storage manager in the second storage operation cell. If desired storage operation cells can also be organized hierarchically such that hierarchically superior cells control or pass information to hierarchically subordinate cells or vice versa.

Storage manager may also maintain an index a database or other data structure . The data stored in database may be used to indicate logical associations between components of the system user preferences management tasks media containerization and data storage information or other useful data. For example the storage manager may use data from database to track logical associations between secondary storage computing device and storage devices or movement of data as containerized from primary to secondary storage .

Generally speaking the secondary storage computing device which may also be referred to as a media agent may be implemented as a software module that conveys data as directed by storage manager between a client and one or more storage devices such as a tape library a magnetic media storage device an optical media storage device or any other suitable storage device. In one embodiment secondary storage computing device may be communicatively coupled to and control a storage device . A secondary storage computing device may be considered to be associated with a particular storage device if that secondary storage computing device is capable of routing and storing data to that particular storage device .

In operation a secondary storage computing device associated with a particular storage device may instruct the storage device to use a robotic arm or other retrieval means to load or eject a certain storage media and to subsequently archive migrate or restore data to or from that media. Secondary storage computing device may communicate with a storage device via a suitable communications path such as a SCSI or Fibre Channel communications link. In some embodiments the storage device may be communicatively coupled to the storage manager via a SAN.

Each secondary storage computing device may maintain an index a database or other data structure that may store index data generated during storage operations for secondary storage SS as described herein including creating a metabase MB . For example performing storage operations on Microsoft Exchange data may generate index data. Such index data provides a secondary storage computing device or other external device with a fast and efficient mechanism for locating data stored or backed up. Thus a secondary storage computing device index or a database of a storage manager may store data associating a client with a particular secondary storage computing device or storage device for example as specified in a storage policy while a database or other data structure in secondary storage computing device may indicate where specifically the data of the client is stored in storage device what specific files were stored and other information associated with storage of the data of the client . In some embodiments such index data may be stored along with the data backed up in a storage device with an additional copy of the index data written to index cache in a secondary storage device. Thus the data is readily available for use in storage operations and other activities without having to be first retrieved from the storage device .

Generally speaking information stored in cache is typically recent information that reflects certain particulars about operations that have recently occurred. After a certain period of time this information is sent to secondary storage and tracked. This information may need to be retrieved and uploaded back into a cache or other memory in a secondary computing device before data can be retrieved from storage device . In some embodiments the cached information may include information regarding format or containerization of archives or other files stored on storage device .

One or more of the secondary storage computing devices may also maintain one or more single instance databases . Single instancing alternatively called data deduplication generally refers to storing in secondary storage only a single instance of each data object or data block in a set of data e.g. primary data . More details as to single instancing may be found in one or more of the following previously referenced U.S. patent application Ser. Nos. 11 269 512 12 145 347 12 145 342 11 963 623 11 950 376 and 61 100 686.

In some examples the secondary storage computing devices maintain one or more variable instance databases. Variable instancing generally refers to storing in secondary storage one or more instances but fewer than the total number of instances of each data object or data block in a set of data e.g. primary data . More details as to variable instancing may be found in the previously referenced U.S. Pat. App. No. 61 164 803.

In some embodiments certain components may reside and execute on the same computer. For example in some embodiments a client such as a data agent or a storage manager coordinates and directs local archiving migration and retrieval application functions as further described in the previously referenced U.S. patent application Ser. No. 09 610 738. This client can function independently or together with other similar clients .

As shown in secondary storage computing devices each has its own associated metabase . Each client may also have its own associated metabase . However in some embodiments each tier of storage such as primary storage secondary storage tertiary storage etc. may have multiple metabases or a centralized metabase as described herein. For example rather than a separate metabase or index associated with each client in the metabases on this storage tier may be centralized. Similarly second and other tiers of storage may have either centralized or distributed metabases. Moreover mixed architecture systems may be used if desired that may include a first tier centralized metabase system coupled to a second tier storage system having distributed metabases and vice versa etc.

Moreover in operation a storage manager or other management module may keep track of certain information that allows the storage manager to select designate or otherwise identify metabases to be searched in response to certain queries as further described herein. Movement of data between primary and secondary storage may also involve movement of associated metadata and other tracking information as further described herein.

In some examples primary data may be organized into one or more sub clients. A sub client is a portion of the data of one or more clients and can contain either all of the data of the clients or a designated subset thereof. As depicted in the data store includes two sub clients. For example an administrator or other user with the appropriate permissions the term administrator is used herein for brevity may find it preferable to separate email data from financial data using two different sub clients having different storage preferences retention criteria etc.

As previously noted because virtual machines may be easily set up and torn down they may be less permanent in nature than non virtual machines. Due to this potential transience of virtual machines it may be more difficult to detect them especially in a heterogeneous or otherwise disparate environment. For example a virtual machine host may host a number of different virtual machines . Virtual machines may be discovered using the techniques previously described herein. Alternatively or additionally virtual machines could be detected by periodically performing dynamic virtual resource detection routines to identify virtual machines in the network or some subset thereof such as a subnet . For example the data agent or other agent could analyze program behaviors corresponding to known virtual resource behaviors perform fingerprint hash or other characteristic based detection methods or routines query a system datastore e.g. the Windows registry or other data structure of the virtual machine host for keys or other identifiers associated with virtual resources. The data agent may use other methods and or combinations of these methods to detect virtual machines .

Once detected the data agent could maintain virtual machine identifiers in a database or other data structure and use associated program logic to track existing virtual machines in the network . Alternatively or additionally an administrator could manually populate the database or it could be populated as part of an install or virtual resource creation process or by an agent or other software module directed to detecting installation of virtual machines. The data agent could update the database to remove a virtual machine identifier upon receiving an affirmative indication that the corresponding virtual machine has been taken down or removed from its virtual machine host . Alternatively or additionally the data agent could periodically poll virtual machines to determine if the virtual machines are still functioning. If a virtual machine does not respond after a certain number of polling attempts the data agent may assume that the virtual machine is no longer functioning and thus remove its identifier from the database. Alternatively or additionally the virtual machines could periodically notify the data agent that they are still functioning e.g. by sending heartbeat messages to the data agent . Upon a failure to receive notifications from a virtual machine within a certain time period the data agent could remove its identifier from the database. The data agent may use other methods and or combinations of these methods to maintain an up to date listing of virtual machine identifiers in the database.

These techniques for detecting virtual machines and maintaining identifiers thereof may also be used to detect virtual resources of virtual machines and maintain identifiers thereof. For example a virtual machine may be coupled to a virtual storage device such as a virtual NAS device or a virtual optical drive. The data agent could detect these virtual resources and maintain identifiers for them in a database or other data structure. The virtual resources may then be addressed as if they were actual resources. Once detected or identified storage operations related to the virtual resources could be performed according to non virtualized storage policies or preferences according to storage policies or preferences directed specifically to virtual resources and or to combinations of non virtualized and virtualized storage policies and preferences. As another example a virtual machine may be coupled to a virtual tape library VTL . The data agent may perform additional analysis on the nature and structure of the virtual resource which underlies the VTL e.g. a virtual disk . This may allow the data agent to realize additional optimizations relating to storage operations associated with the data of the VTL. For example even though the virtual resource is a VTL necessitating sequential access storage operations might be able to be performed non linearly or in a random access fashion since the underlying virtual resource allows random access. Therefore rather than sequentially seeking through the VTL data to arrive at a particular point the data agent could simply go directly to the relevant data on the virtual disk that is the subject of the storage operation.

In traditional copy or backup of virtual machines an indexing agent is typically located at each virtual machine or is otherwise associated with each virtual machine . The indexing agent indexes data on the virtual machine . This results in the creation of one index per virtual machine . This facilitates searching of data on a per virtual machine basis but may make it difficult to search data across multiple virtual machines . Moreover the indexing is performed on the virtual machine and thus uses its resources which may not be desirable.

In contrast copying of data of virtual machines using the techniques described herein may use one indexing agent that is associated with multiple virtual machines . The sole indexing agent thus indexes multiple virtual machines . This results in the creation of one index for the multiple virtual machines . The one indexing agent can subdivide or logically separate the single index into multiple sub indexes for each virtual machine . This technique facilitates searching of data using one index across multiple virtual machines and also allows searching on a per virtual machine basis. The sole indexing agent may create the single index using secondary copies of virtual machine data so as not to impact the primary copies or utilize virtual machine resources. The indexed data may be tagged by users. More details as to indexing data are described in the previously referenced U.S. patent application Ser. No. 11 694 869.

As shown in clients and secondary storage computing devices may each have associated metabases and respectively . Each virtual machine may also have its own metabase containing metadata about virtual machine data. Alternatively one or more virtual machines may be associated with one or more metabases. A classification agent may analyze virtual machines to identify data objects or other files email or other information currently stored or present by the virtual machines and obtain certain information regarding the information such as any available metadata. Such metadata may include information about data objects or characteristics associated with data objects such as data owner e.g. the client or user that generates the data or other data manager last modified time e.g. the time of the most recent modification data size e.g. number of bytes of data information about the data content e.g. the application that generated the data the user that generated the data etc. to from information for email e.g. an email sender recipient or individual or group on an email distribution list creation date e.g. the date on which the data object was created file type e.g. the format or application type last accessed time e.g. the time the data object was most recently accessed or viewed application type e.g. the application that generated the data object location network e.g. a current past or future location of the data object and network pathways to from the data object frequency of change e.g. a period in which the data object is modified business unit e.g. a group or department that generates manages or is otherwise associated with the data object and aging information e.g. a schedule which may include a time period in which the data object is migrated to secondary or long term storage etc. The information obtained in this analyzing process may be used to initially create or populate the metabases.

Alternatively or additionally a journaling agent may populate the metabase with content by accessing virtual machines or by directly accessing virtual resources e.g. virtual disks . The journaling agent may include a virtual filter driver program and may be deployed on a virtual input output port or data stack and operate in conjunction with a virtual file management program to record a virtual machine s interactions with its virtual data. This may involve creating a data structure such as a record or journal of each interaction. The records may be stored in a journal data structure and may chronicle data interactions on an interaction by interaction basis. The journal may include information regarding the type of interaction that has occurred along with certain relevant properties of the data involved in the interaction. The classification agent may analyze and process entries within respective journals associated with journaling agents and report results to the metabase. More details as to techniques used in the classification of data and journaling of changes to data may be found in the previously referenced U.S. patent application Ser. No. 11 564 119.

Once virtual machine data has been indexed and or classified users can search for virtual machine data using techniques known to those of skill in the art. The system may provide a single interface directed to enabling the search for virtual machine data as well as non virtual machine data . A user can utilize the interface to provide a query which is used to search metabases and or indices of virtual machine data as well as non virtual machine data . The system can in return provide results from the metabases and or indices relevant to the query that may be segregated based upon their origin e.g. based upon whether they came from virtual machines or non virtual machines . The returned results may be optionally analyzed for relevance arranged and placed in a format suitable for subsequent use e.g. with another application or suitable for viewing by a user and reported. More details as to techniques for searching data and providing results may be found in commonly assigned U.S. patent application Ser. No. 11 931 034 entitled METHOD AND SYSTEM FOR SEARCHING STORED DATA the entirety of which is incorporated by reference herein.

Virtual machine data may be single or variable instanced or de duplicated in order to reduce the number of instances of stored data sometimes to as few as one. For example a virtual machine host may host numerous virtual machines configured identically or with slight variations e.g. the virtual machines have the same operating system files but different application data files . As another example a virtual machine may store substantially the same data in a virtual disk that a non virtual machine stores on its storage devices e.g. both a virtual machine and a non virtual machine may have a C Windows directory and corresponding system files and only one instance of each system file may need to be stored . If only a single instance of each data object in this data the data of both the virtual machines and the non virtual machines can be stored on a single instance storage device significant savings in storage space may be realized.

To single or variable instance virtual machine data an agent e.g. a media agent may generate a substantially unique identifier for example a hash value message digest checksum digital fingerprint digital signature or other sequence of bytes that substantially uniquely identifies the file or data object for each virtual data object. The word substantially is used to modify the term unique identifier because algorithms used to produce hash values may result in collisions where two different files or data objects result in the same hash value. However depending upon the algorithm or cryptographic hash function used collisions should be suitably rare and thus the identifier generated for a virtual file or data object should be unique throughout the system.

After generating the substantially unique identifier for the virtual data object the agent determines whether it should be stored on the single instance storage device. To determine this the agent accesses a single instance database to determine if a copy or instance of the data object has already been stored on the single instance storage device. The single instance database utilizes one or more tables or other data structures to store the substantially unique identifiers of the data objects that have already been stored on the single instance storage device. If a copy or instance of the data object has not already been stored on single instance storage device the agent sends the copy of the virtual data object to the single instance storage device for storage and adds its substantially unique identifier to the single instance database. If a copy or instance of the data object has already been stored the agent can avoid sending another copy to the single instance storage device. In this case the agent may add a reference e.g. to an index in the single instance database such as by incrementing a reference count in the index to the already stored instance of the data object. Adding a reference to the already stored instance of the data object enables storing only a single instance of the data object while still keeping track of other instances of the data object that do not need to be stored.

Redundant instances of data objects may be detected and reduced at several locations or times throughout the operation of the system. For example the agent may single or variable instance virtual machine data prior to performing any other storage operations. Alternatively or additionally the agent may single instance virtual machine data after it has been copied to the secondary storage data store . The agent may generate a substantially unique identifier and send it across the network to the single instance database to determine if the corresponding virtual data object should be stored or the agent may send the virtual data object to the single instance database which then may generate a substantially unique identifier for it. More details as to single instancing data may be found in one or more of the previously referenced described in one or more of previously referenced U.S. patent application Ser. Nos. 11 269 512 12 145 347 12 145 342 11 963 623 11 950 376 61 100 686 and 61 164 803.

The techniques described herein are applicable in both homogenous and heterogeneous environments. For example the techniques described herein can be used to copy and restore data from and to virtual machines operating solely on VMware virtual machine hosts e.g. VMware ESX servers or on solely Microsoft virtual machine hosts e.g. on a Microsoft Virtual Server or a Microsoft Windows Server Hyper V . As another example the techniques described herein can be used to copy and restore data from and to virtual machines that are operating in a mixed vendor environment e.g. virtual machines from VMware Microsoft and or other vendors . The data agent can perform file level volume level and or disk level copies of virtual machines operating on these Microsoft platforms and perform restores out of file level volume level and disk level copies.

For example virtual machines operating on these Microsoft platforms have their virtual disks in .vhd files. In performing a disk level copy of a virtual machine operating on a Microsoft platform the data agent copies the .vhd files extracts metadata e.g. file volume disk relationships metadata from the .vhd files and stores this metadata. In restoring out of a disk level copy the data agent uses the stored metadata to reconstruct the virtual disks volumes and files to allow the data agent to restore files volumes or entire virtual machines . The techniques described herein can also be used to copy and restore data from and to virtual machines operating on virtual machine hosts from other vendors.

In the context of a VMware virtual machine in restoring a volume of a virtual machine e.g. step of the process the secondary storage computing device restores the volume as a VMware volume e.g. to a virtual machine operating on a virtual machine host . However the secondary storage computing device can also restore the volume as a Microsoft volume e.g. to a virtual machine operating on Microsoft Virtual Server or Microsoft Windows Server Hyper V. The secondary storage computing device can thus convert data in the VMware .vmdk format to data in the Microsoft .vhd format. This conversion process can also be performed in the opposite direction e.g. from the Microsoft .vhd format to the VMware .vmdk format.

Similarly in restoring an entire virtual machine e.g. step of the process the secondary storage computing device can restore the entire virtual machine as a virtual machine operating on a Microsoft platform. The secondary storage computing device does so by converting the data in the .vmdk format to data in the .vhd format and associated configuration files . The secondary storage computing device can thus convert a virtual machine operating on an ESX Server to a virtual machine operating on Microsoft Virtual Server or Microsoft Windows Server Hyper V. This conversion process can also be performed in the opposite direction e.g. from the Microsoft .vhd format to the VMware .vmdk format. The conversion process enables virtual machine data originating on VMware platforms to be migrated to other platforms and for virtual machine data originating on non VMware platforms to be migrated to the VMware platform. Similar conversions can also be performed for virtual disks .

To perform the conversion the secondary storage computing device may use APIs or other programmatic techniques. For example to convert a .vhd file to a .vmdk file the secondary storage computing device may create the .vmdk file create necessary data structures e.g. grain directories and grain tables within the .vmdk file and copy sectors of the volume of the .vhd file to the .vmdk file going extent by extent and creating necessary entries in the data structures e.g. entries in the grain directories and grain tables along the way. The secondary storage computing device may perform a similar process to convert a .vmdk file to a .vhd file. As another example the secondary storage computing device may analyze a .vmdk file using an API function determine its sectors using another API function copy each sector of to a .vhd file using a third API function. As another example the secondary storage computing device may analyze a .vhd file using an API function determine its sectors using another API function and copy each sector of to a .vmdk file using a third API function. The secondary storage computing device may use other techniques e.g. third party toolkits to perform conversions between .vmdk and .vhd formats.

Conversion between other formats is also possible. For example the secondary storage computing device can convert data between the VMware format and an Open Virtual Machine Format OVF and vice versa. Those of skill in the art will understand that a wide variety of conversions are possible and the techniques are not limited to the conversions described herein.

As described herein a secondary storage computing device may maintain an index a database or other data structure that it uses to store index data generated during storage operations. The secondary storage computing device may use this index data to quickly and efficiently locate data that has been previously copied. This index data may be used for various purposes such as for browsing by an administrator and or for restoring the previously copied data.

During a storage operation involving multiple virtual machines the secondary storage computing device populates one index with metadata corresponding to all the multiple virtual machines e.g. a master index . For each of the virtual machines the secondary storage computing device also populates an index with metadata corresponding to that virtual machine e.g. a sub index . The master index points to or refers to the sub indices. When an operation to restore virtual machine data is to be performed the master index is accessed. Because the master index points to the sub indices these can be accessed and the indexed data is used so as to present the virtual machine data that is available to be restored. This available virtual machine data is displayed to an administrator segregated by individual virtual machines which is a logical distinction that is likely intuitive to the administrator. Accordingly accessing individual virtual machine index data involves two levels of indirection one for the master index and one for the sub indices.

Additionally or alternatively the secondary storage computing device can populate a single index that is subdivided or otherwise logically separated into multiple sub indexes one sub index for each virtual machine . When an operation to restore virtual machine data is to be performed the index data populated by the secondary storage computing device can be used to present the virtual machine data segregated by individual virtual machines . Other logical separations and or segregations of virtual machine data e.g. by file type by owner etc. are of course possible.

As described herein a virtual machine host may host multiple virtual machines . If a data agent is to perform simultaneous storage operations on a large number of the virtual machines their performance individually or collectively may be adversely affected. This potential adverse effect may be attributable to one or more reasons such as for example the snapshotting of virtual machines prior to copying their data see . There may not necessarily be a linear relationship between the number of storage operations that the data agent performs or the number of virtual machines upon which the data agent is performing storage operations and the reduction in performance. For example performance may decrease linearly with regards to a first number of concurrent storage operations e.g. ten concurrent storage operations and then may drastically decrease after surpassing that first number.

Accordingly it would be beneficial to be able to limit the number of concurrent storage operations being performed upon the virtual machines hosted by a virtual machine host . This could be done in one of several ways. First there could be a hard limit or threshold on the number of simultaneous storage operations performed. For example the data agent could be limited to performing ten simultaneous storage operations e.g. upon ten different virtual machines . The data agent could distribute the ten simultaneous storage operations across the sub clients corresponding to the virtual machines . For example if a single virtual machine host hosts virtual machines distributed across five sub clients the data agent could be limited to performing two simultaneous storage operations e.g. upon two virtual machines per sub client.

Second the number of concurrent storage operations could be limited based upon the performance of one or more individual virtual machines and or the performance of the virtual machine host . The data agent can measure performance using standard metrics e.g. number of disk writes and or reads per second central processing unit CPU usage memory usage etc. . If the data agent determines that the performances of the virtual machines are below a certain performance threshold the data agent could reduce the number of simultaneous storage operations that it performs. Alternatively if the data agent determines that the performances of the virtual machines exceed the certain performance threshold the data agent could increase the number of simultaneous storage operations that it performs.

Third the throughput of concurrent storage operations could be reduced so as to utilize less of the resources e.g. CPU disk memory network bandwidth etc. of the virtual machines and or the virtual machine host . This reduction in throughput may lessen the loads placed upon the virtual machines and or the virtual machine host by the simultaneous storage operations. However this may also necessitate lengthening the window of time in which the storage operations are performed. In each of these three approaches if the data agent is unable to perform a storage operation upon a virtual machine the data agent may flag the virtual machine for later performance of a storage operation and move to the next virtual machine . These three approaches are not mutually exclusive and combinations of two or more of the three may be used so as to optimally perform storage operations upon virtual machines .

The administrator can also specify the type of copy operation to be performed using options either file level volume level or disk level. The administrator can also select one or more virtual machine storage managers that are to perform the copy operations using list box . Generally the administrator has to select at least one virtual machine storage manager to perform the copy operation.

If the administrator selects two or more virtual machine storage managers in the list box this causes the copy operation when it commences to be performed by the selected virtual machine storage managers . This can assist in load balancing and provide other benefits. For example one or more sub clients could be configured to perform copy operations upon all the virtual machines associated with a specific virtual machine manager . This could be a large number of virtual machines and if only one virtual machine storage manager were to perform copy operations upon the one or more sub clients virtual machines it could take a lengthy period of time to conclude all the copy operations. Accordingly distributing copy operations across multiple virtual machine storage managers can shorten the amount of time it takes to conclude all the copy operations. This can be true even in the case of a single virtual machine for example when the single virtual machine contains a large amount of data . This workload balancing can provide significant benefits such as when copy operations need to be performed entirely within a specific window of time e.g. from 2 00 a.m. to 4 00 a.m. . Moreover such load balancing only requires a single virtual machine storage manager to coordinate the performance of the copy operations by the multiple virtual machine storage managers .

For example an administrator could select a first virtual machine storage manager that coordinates the copying of data of multiple virtual machines . The administrator could also select one or more second virtual machine storage managers to perform the copying of data of multiple virtual machines . The first data agent can allocate responsibility for the copying of the data amongst the second virtual machine storage managers such that the copying is more or less evenly distributed based upon selections previously made static load balancing .

Additionally or alternatively the first virtual machine storage manager can distribute the copy operations across the second virtual machine storage managers based upon various factors. Consider an example where ten copy operations of the data of ten virtual machines are to be performed and where two second virtual machine storage managers can be used to perform the copy operations. The first virtual machine storage manager can determine an availability of the second virtual machine storage managers as measured by percentage of CPU usage percentage of network utilization disk utilization average time spent performing storage operations and or other factors. For example if the first virtual machine storage manager determines that one of the second virtual machine storage managers have a percentage of CPU usage of 10 and that the other second virtual machine storage manager has a percentage of CPU usage of 50 the storage manager may allocate eight of the copy operations to the one second virtual machine storage manager and the remaining two copy operations to the other second virtual machine storage manager based upon this measurement of availability dynamic load balancing . The first virtual machine storage manager may also use other factors known to those of skill in the art to balance the workloads of the two virtual machine storage managers . Additionally or alternatively the storage manager may perform the load balancing amongst the multiple virtual machine storage managers .

As described herein the primary storage data store stores the data of virtual machines . The data is organized into multiple blocks of fixed size e.g. 64 kb 128 kb 256 kb 512 kb etc. . A data agent can perform full copies of data of virtual machines using the blocks of data. In some instances it may not be necessary to perform a second full backup of virtual machine data after a first full backup has been performed at least not until a set period of time has elapsed . Rather incremental and or differential backups of virtual machine data may suffice.

At step the data agent determines the blocks that have been allocated and or are being used within the virtual disks . At step the data agent accesses a block identifier data structure to make the determination of which blocks have changed since the last storage operation involving a full copy of the virtual machine data.

Returning to at step for each block that the data agent determines has been allocated and or is in use the data agent generates a substantially unique identifier. At step the data agent finds the row in the table for which the block identifier of column is the same as the block identifier of the block currently being processed. The data agent then looks up the substantially unique identifier in the column and compares it to the generated substantially unique identifier. If the two substantially unique identifiers do not match then the block currently being processed has changed. The process then continues at step where the data agent copies the block to a storage device. The data agent then updates the column of the table with the generated substantially unique identifier. At step the data agent determines whether there are more blocks to process. If so the process returns to step . If not the process concludes. If the block has not changed step the process continues at step . The next time the data agent performs a full copy of all of the data of the virtual machine the data agent can regenerate substantially unique identifiers for blocks of data and repopulate or recreate the table .

If at step the data agent cannot find a row in the table for which the block identifier of column is the same as the block identifier of the block currently being processed this generally indicates that the data agent is currently processing a block that has been allocated and or has been put to use since the time at which the last full copy operation was performed. If this is the case the data agent will copy the block to the storage device and at step the data agent will add a row to the table with the block identifier and the generated substantially unique identifier.

The process and the table thus enable copying of virtual machine data on an incremental basis. This can provide significant advantages in that it allows for only copying the data that has changed while still providing for protection of virtual machine data. Changes can be made to the process and or the table while still retaining the ability to perform storage operations on an incremental basis. For example a monitoring agent could monitor the blocks of the virtual disks and each time a block is changed e.g. due to a write operation the monitoring agent could set a flag or bit for the block in a data structure. When the data agent is to perform an incremental copy it can access the data structure containing the flags and only copy blocks that have been flagged. As another example the table could include a time copied column to store timestamps of when a block was last copied to a storage device. If the difference between the time of the incremental copy operation and the last time copied is greater than a threshold time the data agent could copy the block to the storage device regardless of whether the generated substantially unique identifier matches the stored substantially unique identifier.

From the foregoing it will be appreciated that specific embodiments of the storage system have been described herein for purposes of illustration but that various modifications may be made without deviating from the spirit and scope of the invention. For example although copy operations have been described the system may be used to perform many types of storage operations e.g. backup operations restore operations archival operations copy operations CDR operations recovery operations migration operations HSM operations etc. . Accordingly the invention is not limited except as by the appended claims.

Unless the context clearly requires otherwise throughout the description and the claims the words comprise comprising and the like are to be construed in an inclusive sense as opposed to an exclusive or exhaustive sense that is to say in the sense of including but not limited to. The word coupled as generally used herein refers to two or more elements that may be either directly connected or connected by way of one or more intermediate elements. Additionally the words herein above below and words of similar import when used in this application shall refer to this application as a whole and not to any particular portions of this application. Where the context permits words in the above Detailed Description using the singular or plural number may also include the plural or singular number respectively. The word or in reference to a list of two or more items that word covers all of the following interpretations of the word any of the items in the list all of the items in the list and any combination of the items in the list.

The above detailed description of embodiments of the invention is not intended to be exhaustive or to limit the invention to the precise form disclosed above. While specific embodiments of and examples for the invention are described above for illustrative purposes various equivalent modifications are possible within the scope of the invention as those skilled in the relevant art will recognize. For example while processes or blocks are presented in a given order alternative embodiments may perform routines having steps or employ systems having blocks in a different order and some processes or blocks may be deleted moved added subdivided combined and or modified. Each of these processes or blocks may be implemented in a variety of different ways. Also while processes or blocks are at times shown as being performed in series these processes or blocks may instead be performed in parallel or may be performed at different times.

The teachings of the invention provided herein can be applied to other systems not necessarily the system described above. The elements and acts of the various embodiments described above can be combined to provide further embodiments.

These and other changes can be made to the invention in light of the above Detailed Description. While the above description details certain embodiments of the invention and describes the best mode contemplated no matter how detailed the above appears in text the invention can be practiced in many ways. Details of the system may vary considerably in implementation details while still being encompassed by the invention disclosed herein. As noted above particular terminology used when describing certain features or aspects of the invention should not be taken to imply that the terminology is being redefined herein to be restricted to any specific characteristics features or aspects of the invention with which that terminology is associated. In general the terms used in the following claims should not be construed to limit the invention to the specific embodiments disclosed in the specification unless the above Detailed Description section explicitly defines such terms. Accordingly the actual scope of the invention encompasses not only the disclosed embodiments but also all equivalent ways of practicing or implementing the invention under the claims.

While certain aspects of the invention are presented below in certain claim forms the inventors contemplate the various aspects of the invention in any number of claim forms. For example while only one aspect of the invention is recited as embodied in a computer readable medium other aspects may likewise be embodied in a computer readable medium. As another example while only one aspect of the invention is recited as a means plus function claim under 35 U.S.C. 112 sixth paragraph other aspects may likewise be embodied as a means plus function claim or in other forms such as being embodied in a computer readable medium. Any claims intended to be treated under 35 U.S.C. 112 will begin with the words means for. Accordingly the inventors reserve the right to add additional claims after filing the application to pursue such additional claim forms for other aspects of the invention.

