---

title: Enterprise test system platform and associated method for interoperable test data management, test development, test libraries and test workflow management and automation
abstract: This Invention provides a system-level framework for an enterprise-level test environment. The environment provides test development, automation, and execution processes for product validation and verification, and allows the interoperability of test cases, data, equipment information, and results across various enterprise platforms. The system, with its processes and methods, provides various mechanisms for managing test configurations, developing test plans, managing test data, and developing test reports from one or more test-stations and/or from one or more test equipment(s) for one or more Device(s) Under Test (DUT). These mechanisms include, but are not limited to, functions such as data management and sharing, test library reusability, test station management, test configuration management, test execution, test report development, and data mapping/plotting. The system can be customized to support scalable enterprise requirements. The development of the standardized data handling and communication processes and methods allows inter-system communication and interoperability of test information across various platforms.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09087041&OS=09087041&RS=09087041
owner: 
number: 09087041
owner_city: 
owner_country: 
publication_date: 20120724
---
The field of invention relates to an enterprise level test environment which provides a method for test configuration developing and managing test data and test reports in interoperable ways across various testing platforms.

Generating collecting analyzing and consuming test data are keys to business intelligence for companies. Managing the creation of test plans test automation test data and then utilization of very expensive assets becomes a discipline and process of its own. The tests are often too complex to do manually and far too time consuming to be executed in a manufacturing environment. This testing process is not only time consuming but also resource intensive and thereby negatively impacting impacts the overall system design productively. The framework of database and test tools wrapped around this process is referred to as a test automation framework. Test automation is a key component of test processes and critical to enhancing the productivity of the overall team. Such infrastructure provides these companies with a competitive advantage in systematically defining test plans and then quickly achieving automation and generating large amounts of intelligent product critical data.

Automatic Testing Equipment ATE industry has pushed to develop a framework which would support the sharing of test information data and results across various enterprise platforms. Usually the test engineer develops a test plan comprising of design of the testbed equipment test specification and other testing requirements. Based on the test plan the test equipment is assembled and testing team orchestrates the automation requirements and further collects analyzes and reports the data upon test completion. Such a process of developing tool sets that automate the test activity is known as business process automation.

The test process for any given company can encompass test plan development equipment selection automation software development data collection and storage data analysis test report data sheet assembly and publication supply chain test monitoring contract manufacturing test station utilization test station reservation and scheduling test asset calibration and tracking and manufacturing execution system MES .

Technologically the testing system and their framework can be classified into three main groups of systems. The first one represents the multiple standalone station which do not share any data computing resources such as server data storage among others test data formats. These systems are highly fragmented and have a significantly different test data and process format from one test station to another. This configuration represents fragmentation to the extreme end where the test data format may vary from one test station to another test station within the same group resulting in an extremely inefficient system.

The second group of testing systems and framework include a system with its own dedicated file server where the test data and the test plans are centrally deposited and stored within an organization or a group. This obviously provides a more centralized approach to the standalone system as described above. In such configuration test plans typically begin with engineers developing text to spreadsheet plans with various individual formats and storage locations. These then translate into individual automation plans and automation solutions with minimal amounts of reusability. In both above cases once automation is developed and in place the production storage analysis and publications of results ranges from local text files to haphazardly organized file shares.

The third approach is a client server based system for configuring and managing the test station and test data and process. This involves a structured and organized approach to managing the data and its format for providing higher degrees of interoperability to users within the same group or organization. The structured approach gains efficiencies in structuring data however creates a very proprietary and investment intensive system. The system in such scenario may not be interoperable within geographically dispersed group organization and will definitely not be interoperable across various organizations.

Each of these approaches introduces several challenges and therefore inefficiencies within the organization. Each of these traditional methods means internal investment of proprietary systems which one at a time provide business process automation to the phases above. Companies have previously followed an evolutionary path to address this process. Such unstructured management of test plans and handling of test data and results leads to productivity losses due to issues related to data portability data organization and absence of a common platform for sharing the data within organizations and among other organizations. This creates confusion in organization of results as well as communication and publication challenges. Format of result publications and analysis reports vary from one organization to another and in many cases from one group within an organization to another group within the same organization. The absence of a unified standards and process for such test data sharing and management leads to a challenge in data management.

The inefficiencies and absence of a unified process have led to a need for an automated enterprise test system framework which allows various organizations to share data across multiple test platforms. To further leverage this unified environment a configurable test case library can be developed and shared across several platform s based on test equipment specifications and Design Under Test DUT requirements. Users may then use existing test cases and avoid developing the specific test cases for their design thereby adding efficiency to this overall process. Automatic Test Markup Language ATML has been recognized as a standard which provides XML based test information exchange among various ATEs making them interoperable.

This invention allows a more open and interoperable enterprise solution. In this case a test case library can be developed and shared across several platforms based on the test equipment specifications and the Design Under Test DUT requirements. Users can then use these existing test cases and avoid developing the specific test cases for their design thereby adding efficiency to this overall process. Further the invention provides a framework where the testing results can be shared and deciphered across multiple platforms and users. In order to ensure functionality such as interoperability of test data across multiple systems platforms a standard such as but not limited to ATML is followed for the overall system design.

The test station client side of the overall system is referred as the Agent which is executing on a Local Test Station connected to the Device Under Test. The agent maintains a constant communication with the backend system and overall application. The agent is responsible for data collection data assembly and transporting it to the backend server system comprising of database servers application servers configuration and user management system server and reporting system server among other hardware components.

The backend server system stores the data from all the connected test stations. The data organization is dependent upon a specific test plan. This data is then reorganized for efficient data analysis and report generation. Test equipment and availability can be tracked and linked to the test plan in support of project scheduling and management. This capability supports a single site or world wide capability. Assets can be tracked and the database capability of this product enables calibration and maintenance information to be monitored.

The system user is able to mine the data in order to perform engineering analysis and overall report generation. Engineering analysis workflow allows a user to select a part of the whole data assign mathematical analysis functions for issues such as performance judgment reliability analysis characterization and manufacturing test among others. As the product development lifecycle moves into the manufacturing and commercialization phase such automated tests will continue to be run on each product as it moves through the manufacturing workflow. Such analysis is the key component of business intelligence analysis phase for a product development and manufacturing organizations.

The test plan development component provides a standardized method of developing the test plan. This component supports plan development across multiple users and locations. The test development component would further allow sharing the test plan and obtaining its approval from other stakeholders. The test plans can be linked to equipment availability and test software and may be used as the basis for the actual test flow execution. The Test Software Library management component is interfaced with test plan development component. This automation system component maintains one or more reusable software libraries. Any user connected with the overall system can access any part of the library or add other test routines to the existing library.

The system further provides users the ability to create and manage the reports of test data in a consistent and simplified manner. Customizable reports developed using the tools within this system can be published stored and distributed from within the system. Reports can also be reusable to allow similar information to be reported over time or location using new data. This capability not only allows a user to monitor in house processes it also allows the user to monitor contract manufacturing or distant sites. These reports can further be published with various levels of access control with other users within a group entity or outside of entity.

The Device Under Test DUT is referenced as . may be either a physical device or a software component where test observations and measurements may be conducted. Examples of devices include but are not limited to electronics components and systems like semiconductor chips processors circuit board assemblies and components. Physical devices may also extend to mechanical items and mechanical products such as metal fasteners pressure tanks and engine components. A device may also be further extended to support measurement of physiological organisms in the bioscience domain. Data collection is not exclusively associated with physical objects. Software modules may also be tested and characteristics of the software could be measured and analyzed and may therefore also be considered as a unit under test. Data collected and managed by the system originates from observations and measurements of . Such measurements may be but are not limited to sensor measurements analog and digital voltages characteristic observations performance criteria and collection of software statistics. Specific part states or environmental stimuli like voltage temperature and device mode can be generated and used to stimulate . These measurements and stimuli are generated and imparted to the Device via interfaces and . are the interfacing signals connecting one or more test equipments with device . The interfacing signal may connect through wired or wireless logical media. Test equipment is used to generate stimuli and conduct measurements and observations on . may consist of but is not limited to oscilloscopes power supplies logic analyzers protocol analyzers or dynamometers. These may generally be but are not limited to off the shelf equipment which interface with device via interface item and with the test controller via . is the physical and logical protocol layer which is used to connect with test controller . comprises of a physical processing device such as a PC in combination with an Agent and Test Software . Layer enables commands and controls communication from test controller to test equipment . These control interfaces may be but are not limited to GPIB General Purpose Interface Bus serial USB wired and wireless Ethernet. On the test equipment measurement side may record measurements or may only be a data collection method from which data is passed back to test controller . In the latter case would processes the data and create a resultant measurement and judgment.

Test controller commands and communicates with via and to control states program modes load executable programs read back information and other tasks.

The Device Interface Equipment is referenced by . This is generally but not limited to customer interface circuit software test harnesses or physical fixture required for to have test and or command and control points made available for . Interface may contain items such as relays customer circuitry special mechanical adapters or anything else required to control and communicate with .

Physical and logical interfacing protocols such as JTAG USB Serial and I2S and other industry standard or proprietary protocols is referenced by and is employed for connecting test controller with Device Interface Equipment . This interface is used to pass bi directional information between the test controller and the device interface equipment .

Test Software comprises test routines and test sequencers. Test routines are algorithms and software implementations executing on the hardware PC that are responsible for implementing test and measurement procedures. Procedures are implemented using combination of instrument control device control device stimuli protocols sensor observations and processing routines to calculate measurements and observations. Test sequencers provide the test routine execution order control and results partitioning and organization.

Agent is responsible for detecting receiving notification of results from the test software then encrypting compressing and submitting packaged resultant data to data receiver module via an internet interface . Agent also maintains periodic communication with the configuration management subsystem via internet interface to verify eligibility and connectivity for data upload. further illustrate the agent status process and data upload process. An agent must be operating on each test controller participating in the system.

Hardware PC is any type of computer and operating system either industry standard or proprietary which can execute Test Software including test routines and analysis. It provides the logical and physical interfaces to connect with and and human interfaces to connect and interact with via .

Recovered data packets from are transmitted to Data Parser . There may be multiple Data Parser instances of executing on various data formats. stores the parsed datasets into an appropriate backend data repository . can be but is not limited to databases and file storage. Instances of and may be geographically collocated or distributed.

The data repository stores decrypted decompressed parsed validated data. There may be one or more instances of data repositories on any given site as well as geographically distant deployments. Furthermore provides data organization and logical and efficient access retrieval to other components of the overall system. Data in may be later annotated and commented on by other system participants.

Data from data repository is accessed by physical application servers containing the logical and physical items . These are servers with processors memory writable media power supplies and located in temperature controlled facilities such as a data center.

The configuration management module comprises Station Configuration and Management Module and User Configuration and Management Module . contains account configuration related information such as the entity owning the stations the station types the users assigned for managing a station equipment associated with a station among other station attributes. When a user initially attaches a Test Station to the system is invoked and deploys the agent to the test station . Together and record all the information associated with the actual deployed station and owner. Additionally is responsible for deploying updates to the agent as updates become available. can also identify and track test equipment and test software in the test station to support station hardware and software maintenance. also tracks the number of station licenses that have been activated. The details of process flow of module will be further explained in .

The payment processing engine is responsible for accepting and processing payments managing policies for various payment options and managing and displaying payment history. keeps track of the amount of data purchased the number of user and station licenses purchased and activated among other payment parameters.

The Data Analysis and Visualization Module comprises the Data Mining Module the Data Analysis Module the Report Building Generation and Publishing Module and the Data Visualization Module . The data mining interface allows system users to interact with data stored in the data repository by providing capabilities such as browsing search filtering and commenting on selected data. The user preference module provides the user level customized user interface for interaction with the stored data in . provides an example illustrating the real user interaction with for accessing the stored data in data repository . The Data Mining Module allows the user to proceed to the Analysis module or to jump directly to the QuickVIEW module. The QuickVIEW flow and process is described in further detail in .

The Data Analysis Module allows the user to apply standard analytical functions and calculations such as standard deviation process stability and others data selected from data repository . It also supports sending data to and receiving results from external analytic software such as Matlab in order to provide extremely advanced analysis capability. The module allows the user to associate data with user defined object s referred to as an output variable s . The user can further use the output variable as a system object and perform operations and analysis. illustrates the detailed data analysis flow.

The Report Building Generation and Publishing Module allows users to define or reuse their customized reports using data analysis of Data Analysis Module . Data analysis reports for a selected output variable or group of output variables from are then assigned to for publication and sharing with other system users based on user defined permissions. Processed reports and report templates are stored in a configuration management repository . is the Data Visualization Module which provides users the capability to interact with the mined and analyzed data against each output variable.

The Test Software Management Module comprises the Test Sequence definition and management module and Test Software Library management module . provides the capability to develop sequential test flows test sequences via the web interface which can then be downloaded to the test station s and . The Test Sequence development block is accessed via the web interface . From this tool the user defines a sequential test flow which is subsequently stored in the configuration management repository . The user may then download this test sequence via the station configuration and management tool to the test station . The test software within the test station accepts the sequence and is able to interpret and run the sequence. allows users to add test software to the configuration management repository from the web interface . Software that is added may be flagged as private in which case only the user or users group make access and use this software. The software may also be flagged as public which makes the software available to all systems users. The software may be selected from the web interface and downloaded to the test station via the station configuration and management tool . Once the software is downloaded to the test station the user of the test station may incorporate it into the test program.

Users can interact with the overall system for data visualization and analysis through the user interface which could be a web based program executing on any web browser on a physical computer or a mobile device such as a tablet or a client based interface. serves as a communication layer between web browser based front end and the backend system. provides user with capability for interaction with backend system for all functions related to and as discussed above. The system user interacts with the overall system for various tasks as discussed above.

Analyzing results consists of steps drive output variable name assign to output variable associate analysis with output variable and run analysis . Analysis of results could be carried out two possible operations after the data subset s are selected in step 1 by driving an output variable name 2 by assigning to the output variable name . This allows a user to use the data subset s names that exist in the database as output variables . This default case allows the user to keep the data subset s name and carry it forward to step and for inclusion in the final report . Analysis is performed on output variables so the data subset name s in this case are assigned to output variable s . allows a user to associate a name different than the data subset s name as in case of with an output variable. Such flexibility allows a user to rename data subset names for the final report and make it more customizable as per their requirements and needs. associates any possible analysis types such as mean standard deviation and maximum among others with any selected output variables from either or . Any combination of analysis types may be associated with any combination of output variables. This feeds to the next step of actual analysis . In addition to built in analysis types and may also be configured to send data to third party or proprietary analysis software to enhance the analysis capability of the system. During these steps of and all the selections are stored in the local memory of the system and when the user can save filter and selection options at the backend to save the workspace discussed in detail in . Up to this point only data pointers have been used to filter and select data subsets. However the actual data has not been processed. During analysis the backend servers will execute the assigned analysis types of output variables on the selected output variables of steps and or . Process is the assigning of analysis results obtained in for a report. The user will keep assigning results to a report using the iterative process until completing assigning all the variables to the report as per their requirements. This iterative mechanism of selecting various sets subsets of data and assigning them to the output variable executing analysis and assigning the analysis to a report could be executed multiple times until all the requirements for preparing a report is accomplished.

The final step in the data selection and reporting process is publishing the report . Publishing means the report is formatted into a downloadable human readable format and made visible to a predefined audience. This allows the user to select the group of people who can view a report as well as access permissions associated with the report such as read write and modify.

The user can also select the option of Fast Access of data from . Step allows the user to quickly view the data and apply some simple data manipulation and visualization techniques . This processed data can further be either downloaded to the local machine or onto a remote connected memory drive applied to a third party tool environment such as a matlab spotfire Simulink among others or exported to a third party interface either connected through a web based environment application programming interface or a client server application running on host machine . The QuickVIEW data process allows a fast path from data filter and select to visualization and export.

Upon selecting the system checks for the availability of a new license . If the entity has no more system licenses the system requests the user to add new licenses for the entity via their system administration . However if the entity has a new license the system allows the user to download the Agent . At that time the agent launches and extracts information regarding that particular station and gets installed on the test station . The agent the test station and other system information such as but not limited to machine name OS version system owner and hardware is sent back to the backend system and stored in the station management configuration module. The information collected in is posted back to the system. This becomes part of the permanent station record. The invention tracks the version of agent that is deployed to the test station . Once the information is received the backend system checks for the correct installation of the agent in case of a successful installation the process terminates otherwise the user is provided with an option to download the agent and install it again on the test station .

Similarly to adding a station the user can select to remove one or more station . The user is then requested to select a station or a group of stations to be removed from the entity station configuration . Upon confirmation of deleting the station the station s are removed from the system level station management configuration.

The station user can further modify the default or previously set station attributes such as station name user name or any other station attributes in . In such a scenario the user is prompted to select the station to modify and select attributes to modify . On providing new attributes the user can confirm the modification to save those attributes in the station management and configuration system at the backend.

