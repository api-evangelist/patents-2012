---

title: Detection of cabling error in communication network
abstract: In one embodiment, a method at a network device includes receiving a link layer advertisement, comparing information in the link layer advertisement with connectivity information stored at the network device, and based on the comparison, determining if there is a cabling error between the network device and a link peer transmitting the link layer advertisement. An apparatus and logic are also disclosed herein.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09083613&OS=09083613&RS=09083613
owner: Cisco Technology, Inc.
number: 09083613
owner_city: San Jose
owner_country: US
publication_date: 20121016
---
The present disclosure relates generally to communication networks and more particularly to detection of cabling errors in communication networks.

Data center networks are rapidly evolving. Cabling problems in highly meshed data center networks with a large number of switches and other smaller networks are often difficult to troubleshoot and may lead to network downtime and increased operational costs.

Corresponding reference characters indicate corresponding parts throughout the several views of the drawings.

In one embodiment a method at a network device generally comprises receiving a link layer advertisement comparing information in the link layer advertisement with connectivity information stored at the network device and based on the comparison determining if there is a cabling error between the network device and a link peer transmitting the link layer advertisement.

In another embodiment an apparatus generally comprises a processor for processing a link layer advertisement from a link peer comparing information in the link layer advertisement with connectivity information stored at the apparatus and based on the comparison determining if there is a cabling error between the apparatus and the link peer transmitting the link layer advertisement. The apparatus further comprises memory for storing the connectivity information.

The following description is presented to enable one of ordinary skill in the art to make and use the embodiments. Descriptions of specific embodiments and applications are provided only as examples and various modifications will be readily apparent to those skilled in the art. The general principles described herein may be applied to other applications without departing from the scope of the embodiments. Thus the embodiments are not to be limited to those shown but are to be accorded the widest scope consistent with the principles and features described herein. For purpose of clarity details relating to technical material that is known in the technical fields related to the embodiments have not been described in detail.

Many enterprise and service provider customers are building private or public clouds. Cloud computing enables network access to a shared pool of configurable resources that can be rapidly provisioned and released with minimum management effort. Due to the success of cloud deployments many network requirements are changing. For example Clos based network designs are changing to address cloud requirements for use in next generation fabric architectures. Clos networks are multistage switching networks which provide benefits such as the availability of equal cost multipath based switching fabric use of simplified and lower port density core network switches and a fully utilized link bandwidth on each network node. The Clos design also allows the networks to scale and grow incrementally on demand.

The network switches in a Clos network fabric are organized into two or more stages. The lowest level stage referred to as a leaf ingress or egress stage provides network connectivity to the end hosts and implements layer 2 bridging and or layer 3 routing functions. The next higher level stage referred to as a spine or middle stage provides redundant paths and connectivity from an ingress stage switch in the network fabric to an egress stage switch. In accordance with Clos network design rules each switch in a stage is connected to all switches in the adjacent higher level stage and lower level stage only once.

In data centers customers often use network planning tools to prepare and generate cable plans and use them as a basis to interconnect devices in traditional or Clos based networks. Even though connections are made per the administrator prepared cable plan it is possible that cabling problems can occur due to human error since technicians are dealing with thousands of cable connections.

In highly meshed data center networks with thousands of switches miscabling can be a pragmatic problem leading to difficult troubleshooting without adequate support.

The embodiments described herein provide methods and apparatus to automatically detect cabling errors that may be caused by human error. The embodiments may be used for example to take preventive action upon detection of a cabling error in highly meshed data center networks. Two example embodiments that allow for automatic detection of miscabling between link peers in the network fabric are described below. One embodiment is based on tier level checks and another is based on a user defined cable plan. The embodiments provide for faster troubleshooting of network issues due to miscabling and therefore reduce network downtime and operational costs. In one or more embodiments a neighbor cache with link layer connectivity details may be maintained for use in monitoring of network topology by management applications.

Referring now to the drawings and first to an example of a network in which the embodiments described herein may be implemented is shown. The network may be configured for use as a data center or any other type of network. The example shown in illustrates a Clos network topology comprising a plurality of network devices . The network devices may be layer 2 L2 layer 3 L3 or L2 L3 switching devices or other network devices configured to perform forwarding functions. The network devices may be for example a NEXUS 3000 5000 or 7000 series switch available from Cisco Systems Inc. of San Jose Calif. It is to be understood that these are only examples of network devices that may be used to implement the embodiments described herein.

In one embodiment the network devices are assigned a Clos tier level. In the example shown in the switches are assigned tier level 1 2 or 3. The level 1 switches are edge switches that provide connectivity to data center servers not shown . The level 2 and 3 and above switches are spine switches which provide a hierarchical network fabric connectivity topology.

An inter switch link ISL is used to connect adjacent Clos tier level switches . Per Clos network design the inter switch link of a switch is only connected to its adjacent Clos stage switches. This means that a tier level 1 switch is only connected to tier level 2 switches a tier level 2 switch is only connected to tier level 3 or tier level 1 switches and so on. Adjacent peers neighbors connected by the inter switch link are referred to herein as link peers.

As described in detail below network device includes connectivity information and a physical connectivity manager PCM configured to check for cabling errors based on the connectivity information. For simplification the connectivity information and PCM are shown for only one node in . Any number of network devices e.g. two network devices group of network devices all stage devices may comprise connectivity information and physical connectivity manager for use in performing cabling checks.

In a first embodiment the connectivity information comprises tier level information for the local network device. The physical connectivity manager compares the local tier level to the link peer tier level received from the link peers to determine if a cabling error exists. In a second embodiment the connectivity information comprises link information e.g. local chassis identifier local port identifier remote chassis identifier remote port identifier representing physical connections between the network device and link peer according to a cable plan. The physical connectivity manager compares the cable plan to link information received from the link s peer device to determine if there is a cabling mismatch.

The tier level or link information received from the link peer may be transmitted in a link layer advertisement such as a Link Layer Discovery Protocol LLDP message Cisco Discovery Protocol CDP message or other suitable link layer protocol message. If a cabling error is identified by the physical connectivity manager appropriate action may be taken including for example logging a cabling error or not bringing up the adjacency as described further below.

Each network device may also maintain a cache e.g. neighbor cache of discovered link peers for each local port at the switch. The cache may include information such as local port id remote chassis id remote port id remote tier level and the cabling validation state. This information may be used for example by management applications for monitoring link peers and their connectivity states from the switch.

It is to be understood that the network shown in and described herein is only an example and that the embodiments described herein may be implemented in networks having different network topologies and network devices without departing from the scope of the embodiments. For example the second embodiment described herein is based on cross validating the discovered remote link information against an administrator prepared cable plan and is not specific to a Clos network. Therefore the second embodiment may be implemented in non Clos networks that are smaller and or non meshed topologies as well as Clos networks.

An example of a network device e.g. switch that may be used to implement embodiments described herein is shown in . In one embodiment network device is a programmable machine that may be implemented in hardware software or any combination thereof. The device includes one or more processor memory network interface and physical connectivity manager miscabling detection module .

Memory may be a volatile memory or non volatile storage which stores various applications modules and data for execution and use by the processor . For example memory may store connectivity information e.g. local tier level link information from cable plan . The connectivity information may also include remote link peer information received from link peers and validation states based on physical connectivity manager cabling checks. For example memory may maintain per local port id the connected link peer s chassis id port id tier level for first embodiment and the cabling validation state in a local database once link information is received from a link peer and a cabling check is performed. The connectivity information may be stored in cache or any other data structure. The cache may rely on link layer advertisements received from link peers to refresh connection entries. The entries may be purged if no new information is received from the link peer for a specified amount of time.

Logic may be encoded in one or more tangible computer readable media for execution by the processor . For example the processor may execute codes stored in a computer readable medium such as memory . The computer readable medium may be for example electronic e.g. RAM random access memory ROM read only memory EPROM erasable programmable read only memory magnetic optical e.g. CD DVD electromagnetic semiconductor technology or any other suitable medium.

The network interfaces may comprise any number of interfaces linecards ports for receiving data or transmitting data to other devices. The interface may include for example an Ethernet interface for connection to a computer or network.

The physical connectivity manager may comprise code stored in memory for example. As described in detail below the physical connectivity manager is used to compare tier level or link information received from link peers with connectivity information stored at the network device to identify cabling errors. The remote tier level remote chassis id and remote port id are extracted from the link layer advertisement and used by physical connectivity manager to identify cabling errors. The physical connectivity manager may be used for example to detect connectivity inconsistencies provide alerts and take corrective action.

The network device may also include an interface e.g. module plugin library application programming interface not shown interposed between a link layer service module not shown and the physical connectivity manager for extracting connectivity information from the link layer advertisements and providing the connectivity information to the physical connectivity manager.

It is to be understood that the network device shown in and described above is only an example and that network devices having different components and configurations may be used without departing from the scope of the embodiments. For example the network device may include any suitable combination of hardware software algorithms processors devices components or elements operable to facilitate the capabilities described herein.

The first embodiment which uses tier level based checks is described below followed by a description of the second embodiment which uses cable plan based checks for identifying cabling errors.

As previously described and shown in each stage of the Clos network fabric is assigned and represented by a sequential number known as a tier level number. Every switch in a stage is associated with the corresponding tier level number assigned to the stage that the switch is in. In one example all of the leaf switches in the lowest level Clos stage are provisioned with a tier level value of 1 the next higher level stage switches first stage of spine switches are provisioned with a tier level of 2 and the next higher stage switches second stage of spine switches are provisioned with a tier level of 3 and so on. It is to be understood that this numbering scheme is only an example and other numbering schemes may be used.

The switch may be assigned a default switch configuration downloaded on boot up. The tier level number of the switches may be provisioned for example using a global configuration command which can be made available via the switch startup configuration or via switch boot up configuration from power on auto provisioning for example.

Each switch in the network that is configured with a tier level advertises its tier level number to its link peers. In one embodiment the switch advertises its tier level as part of the link layer PDUs protocol data units along with the chassis identifier chassis id and physical port identifier port id . Examples of link layer advertisements that may be used to transmit tier level information are described further below. Adjacent switches may for example exchange their tier level numbers and switch identifiers chassis id when an inter switch link is coming up. The switches may also exchange tier levels periodically or upon the occurrence of an event e.g. change in switch or network configuration .

The local switch receiver of the link layer advertisement performs different checks to validate link connections with the link peer sender of the link layer advertisement based on whether the local switch is a leaf switch or a spine switch. If the local switch is a leaf switch e.g. local switch tier level is equal to 1 in the example shown in the PCM first checks to see if the local tier level is the same as the remote switch link peer tier level steps and . If the local and remote tier levels are the same there is an exception to the general rule that the leaf switches are connected only to spine switches in the next tier level for virtual port channels vPCs . Virtual port channels may be used for example to provide redundancy at the leaf level to servers. Therefore the vPC peer links also known as multi chassis trunks are allowed to exist between the leaf switches. The switch may use for example an MCECM Multi Chassis Ether Channel Manager or port channel manager to determine if a local port is part of a vPC peer link. If the receiving link is a virtual port channel peer link a physical connection with a peer link having a remote tier level of one same as local switch is allowed step . In this case the process returns to step and the switch awaits link information from a link peer.

The tier level advertisement and check is a link level check rather than a device level check. The vPC described above is an example of a case where those checks for links on leaf nodes are relaxed with respect to the tier level check. There may also be other cases in which checks are relaxed and a cabling error is not identified for predetermined specified types of links which are excluded from tier level checks. For example a leaf node at tier 1 may connect to tier 3 spine nodes. Thus there may be one or more specific exemptions as described above for the vPC case.

If the local tier level of the leaf switch and the remote link peer tier level are different a check is then performed at the leaf node to determine if the remote tier level is an adjacent tier level remote tier level local tier level 1 step . If the link peer passes this check the process returns to step . An entry in the neighbor cache table at the local switch may be updated based on the check or a new entry may be created if this was the first check at the local switch for this connection. For example for secure inter switch port connections that have passed cabling validation checks the cache preferably maintains for the local port id on which tier level advertisement is received the remote tier level remote chassis id and remote port id. The cache entries may be refreshed upon receipt of link layer advertisements from link peers that contain the tier level. If a new port index is received a new entry is created in the cache. The cache entries may also be purged if no link layer advertisement is received from a link peer before a user configured hold timeout expires or if the link peer stops sending the tier level.

If the tier level check at the leaf node performed at step does not pass remote tier level local tier level 1 a cabling error is identified step . In this case the link adjacency is not brought up and a cabling mismatch error is logged. The logged error may include for example details about the local switch and peer remote switch such as the local chassis id local port id local tier level peer chassis id peer port id and peer tier level. It is to be understood that this is only an example and that additional information less information or different information may be provided in the error log. Also other actions may be performed in response to a miscabling determination. For example a second check may be performed before disabling the inter switch link port and the port disabled only after the second check has identified a cabling error. A message may also be sent to a user to provide a chance for the user to correct a tier level misconfiguration for example before the port is disabled. Other action that may be taken when a cabling error is identified includes transmitting an SNMP Simple Network Management Protocol trap notification to the network management system or invocation of a local script to handle a customized reaction to the error.

If the local switch is a spine switch e.g. local switch tier level 1 in the example of step a check is performed to see if the link layer advertisement was received from a link peer in an adjacent tier remote tier level local tier level 1 or local tier level 1 step . For example if the switch is in tier level 2 link layer advertisements should only be received from switches in tier level 1 or tier level 3. If the tier level check passes the process returns to step . As described above the corresponding entry in the neighbor cache may be updated or a new entry created. If the tier level check fails a cabling error is identified step . As described above the link adjacency may not be brought up a cabling mismatch error may be logged or both of these actions or other actions may be performed in response to the miscabling detection.

The following provides an example of an extension to an existing link layer discovery protocol to allow for the exchange of tier level information for the first embodiment described above. It is to be understood that the following is only an example and that other extensions formats or fields may be used to exchange tier level information in any link layer control protocol exchange without departing from the scope of the embodiments.

In one example LLDP is used with extensions that allow for exchange of Clos tier level information. The protocol is designed to allow network devices to introduce new organizationally specific attributes and optionally transmit and receive them in addition to the mandatory standard TLVs as part of the LLDP PDUs. The LLDP PDU includes a chassis ID port ID time to live and end of LLDP PDU TLVs along with optional TLVs. One optional TLV is an organizationally specific TLV. In one example a new organizationally specific TLV subtype is used under existing LLDP ORG CISCO TLV OUI hex 00 01 42 Cisco s reserved OUI for organizationally specific TLVs to allow for exchange of the Clos tier level with adjacent link partners. The following is one example of an organizationally specific Clos tier level TLV 

Once the switch is assigned a tier level the switch can create TLV data structures in LLDP memory for all LLDP interfaces and transmit the tier level TLV on the LLDP interfaces. The tier level TLV data structures in LLDP memory are updated following a tier level configuration change in which case the new tier level TLV is transmitted to link peers. The tier level configuration may also be disabled for system maintenance so that tier level checks are temporarily stopped for one or more switches and link peers in the network.

The first embodiment described above performs connectivity checks at a local switch by comparing the local switch tier level connectivity information stored at the local switch with link peer tier level information received in a link layer advertisement from the link peer. The second embodiment performs connectivity checks by comparing link information from a cable plan connectivity information stored at the local switch with link peer information received in a link layer advertisement from the link peer. Details of the second embodiment are described below.

Network planning tools are often used to prepare and generate cable plans which are used to interconnect devices in traditional or Clos based data centers. The cable plan specifies the ports at a switch that should be connected to remote switches and the ports at the remote switches e.g. port of local switch connected to port of spine switch port of local switch connected to port of spine switch etc. . When the inter switch link comes up the switch uses information received from the link peers to validate that the link is connected as specified in the cable plan and identify any cabling errors. The following describes an example of the second embodiment that can be used to detect cabling errors in runtime raise alerts and take preventive action.

Link information from the cable plan may be imported on the network device and stored in a local cache e.g. connectivity information in . The imported cable plan may be stored as a table with each entry representing a physical connection between a local switch port and a remote device port. In one example each entry includes a local chassis id local port id remote chassis id and remote port id.

The cabling error checks described above may be a one time check performed after the physical link comes up or a continuous check performed every time remote information is received from the peer for example.

As previously discussed the cable plan based miscabling detection embodiment is not specific to Clos networks and can also be used for non Clos network topologies in data centers or other networks.

It is to be understood that the processes illustrated in and described above are only examples and that steps may be modified added or combined without departing from the scope of the embodiments.

Although the method and apparatus have been described in accordance with the embodiments shown one of ordinary skill in the art will readily recognize that there could be variations made without departing from the scope of the embodiments. Accordingly it is intended that all matter contained in the above description and shown in the accompanying drawings shall be interpreted as illustrative and not in a limiting sense.

