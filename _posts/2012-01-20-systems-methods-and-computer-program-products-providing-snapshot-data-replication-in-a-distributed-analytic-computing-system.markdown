---

title: Systems, methods, and computer program products providing snapshot data replication in a distributed analytic computing system
abstract: A computer program product having a computer readable medium tangibly recording computer program logic for performing analytics on data at a data node, the computer program product including code to instruct a storage array to create a snapshot of the data, code to access the snapshot, by the data node, as an independent virtual volume, code to receive, at the data node, a command mapping a processing task to the data node, in which the processing task includes analysis on the data, and code to perform the processing task on the data by accessing the data through the snapshot.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08892833&OS=08892833&RS=08892833
owner: NetApp, Inc.
number: 08892833
owner_city: Sunnyvale
owner_country: US
publication_date: 20120120
---
The present description relates to distributed analytic computing systems and more specifically to using snapshot replication in such computing systems.

Various systems exist to perform analysis on very large data sets e.g. petabytes of data . One such example is a Map Reduce distributed computing system for large analytic jobs. In such a system a master node manages the storage of data blocks in one or more data nodes. The master node and data nodes are server computers with local storage. When the master node receives a processing task the master node partitions that task into smaller jobs where the jobs are assigned to the different subordinate data nodes. This is the mapping part of Map Reduce where the master node maps processing jobs to the subordinate nodes.

The subordinate nodes perform their assigned processing jobs and return their respective output to the master node. The master node then processes the different output to provide a result for the original processing task. This is the reducing part of Map Reduce where the master node reduces the output from multiple subordinate nodes into a result. Map Reduce is often used by search engines to parse through large amounts of data and return search results to a user quickly and efficiently. One example of a Map Reduce system is the Hadoop framework from Apache Software Foundation also called the Hadoop Distributed File System HDFS .

The HDFS framework relies on data replication to provide increased reliability. For example if one data node fails to operate the data can be accessed from another data node. The master node commands that multiple copies of the data be made and the data nodes comply by performing a server to server replication.

In one example server to server replication process a data node that has a copy of the data sends the data over a network e.g. a layer 2 connection such as Ethernet to another node that saves the data in its own local storage. However the amount of data to be copied can be quite large which consumes network bandwidth. Additionally conventional Von Neumann processor architecture passes the data through the processor so that large data transfers in systems with such processors can consume large amounts of computer processing cycles computer bus bandwidth and computer memory as well. Thus keeping additional copies of data may increase reliability but it also has a cost in bandwidth and processing power. Conventional distributed processing systems often incur too much cost in providing data replication.

Various embodiments include systems methods and computer program products that provide data replication to a distributed analytics system using snapshots. In one example a distributed analytic computing system receives data and sends that data to a storage array. The storage array then creates multiple snapshots of the data where each of the snapshots corresponds to a data node in the distributed analytic computing system. Each data node accesses its own separate snapshot as a virtual volume as it performs analytical tasks on the data.

One of the broader forms of the present disclosure involves a computer program product having a computer readable medium tangibly recording computer program logic for performing analytics on data at a data node the computer program product including code to instruct a storage array to create a snapshot of the data code to access the snapshot by the data node as an independent virtual volume code to receive at the data node a command mapping a processing task to the data node in which the processing task includes analysis on the data and code to perform the processing task on the data by accessing the data through the snapshot.

Another of the broader forms of the present disclosure involves a method performed in a distributed computer system with a master node and a plurality of data nodes the method including receiving data into the distributed computer system and saving the data to a storage array in communication with the distributed computer system sending commands to the storage array causing the storage array to create a plurality of virtual replicas of the data and accessing the data by the plurality of data nodes where each one of the plurality of data nodes mounts a respective one of the virtual replicas as an independent copy of the data.

Another of the broader forms of the present disclosure involves a distributed computing system including a master node a plurality of data nodes that carry out instructions from the master node a storage array in communication with the master node and the data nodes where the storage array is configured to receive data from the master node and to create virtual replicas of the data for access by the data nodes as independent virtual volumes and in which the master node includes a scheduling function to map processing tasks to each of the data nodes the processing tasks including analysis of the data accessed through the virtual replicas.

The following disclosure provides many different embodiments or examples for implementing different features of the invention. Specific examples of components and arrangements are described below to simplify the present disclosure. These are of course merely examples and are not intended to be limiting. In addition the present disclosure may repeat reference numerals and or letters in the various examples. This repetition is for the purpose of simplicity and clarity and does not in itself dictate a relationship between the various embodiments and or configurations discussed.

Various embodiments disclosed herein provide for using snapshots as independent virtual volumes at the data nodes of a distributed analytic computing system. In one example a method includes ingesting the data at the master node and then sending the data to a storage array. The data at the storage array is saved in at least one master copy and at the command of the master node and data nodes is virtualized to provide multiple snapshots of the data. In this example embodiment each of the snapshots acts as an interface to the underlying data and can be mounted by a data node as a volume with its own Logical Unit Number LUN . From the point of view of a data node its snapshot appears as an independent copy of the data. Thus data replication at the data nodes can be provided by snapshots incurring less cost than creating actual physical copies. One example embodiment includes adapting an HDFS implementation to use an external storage array and snapshots to provide data replication.

The method may further include mapping processing tasks to the various data nodes of the system. Each of the data nodes performs its mapped processing task on its portion of the data but rather than accessing an independent copy of the data each data node accesses its respective snapshot. The method may further include the data nodes sending results of the processing tasks back to the master node. The master node may then process the results and compute output.

The scope of embodiments is not limited to any particular framework. Various embodiments may find use with Hadoop analytics or any other multi server open source analytics such as NO SQL Cassandra Lexis Nexis and the like. Furthermore some embodiments may find use with any other multi client clustered file system such as Lustre Glustre StorNext StorageGrid or HDFS used without HBase or Hadoop . In fact the embodiments described above may be applied to scale out analytics problems involving dozens or hundreds of servers working on a large single dataset.

Each of the data nodes may include for example a personal computer PC server computer a workstation handheld computing communication device or tablet and or the like. The same is true for master node as well though in many embodiments each of master node and data nodes includes a server computer such as a commodity server with many processors running the Linux operating system. shows five data nodes but the scope of embodiments can include any appropriate number of data nodes.

Furthermore the amount of replication in this example system is five one replication per data node though the scope of embodiments may include any appropriate degree of replication. As described further below various embodiments may facilitate scaling with a high level of efficiency so that embodiments with any practical number of data nodes and degree of replication may be accommodated.

Master node data nodes and storage controller are in communication over network using switch . The network may include for example a local area network LAN wide area network WAN the Internet or any combination of such interconnects. Storage array is coupled to ingest servers through a Serial Attached SCSI SAS physical connection and protocol or similar physical connection and protocol though the scope of embodiments may include any appropriate interface to storage array . Ingest servers provide appropriate pre processing to received data and pass the received data to storage array .

The storage controller manages the storage of data in storage array so that master node and data nodes do not see the specific inner workings of storage array . Storage controller instead provides an interface to storage array which from the perspective of master node and data nodes appears as a set of logical storage objects referred to as virtual volumes that may be mounted e.g. accessed by a data node for the storage and retrieval of data in the virtual volume . Storage controller can be inside or outside of the enclosure that includes storage array .

Storage array is not limited to any particular storage technology and can use any storage technology now known or later developed. For example storage array has a number of nonvolatile mass storage devices not shown which may include conventional magnetic or optical disks or tape drives non volatile solid state memory such as flash memory or any combination thereof. In one particular example storage array may include one or more Redundant Arrays of Independent Disks RAID s.

Storage array may be configured to allow data access according to any appropriate protocol or storage environment configuration. In one example master node and data nodes utilize file level data access services as is conventionally performed in a NAS environment. In another example master node and data nodes utilize block level data access services as is conventionally performed in a SAN environment. In yet another example master node and data nodes utilize both file level and block level data access services.

Storage array stores master copy which serves as a base volume that is updated as write operations change the data or add to the data. Data is received by the system at ingest servers and passed to storage controller . Storage controller then saves data to master copy .

In some examples master copy is a single master copy though the various embodiments do not preclude physical replication altogether. Rather storage array may provide some form of physical data replication for purposes of reliability or failure prevention. However in contrast to some conventional systems such as HDFS the embodiment of does not perform a server to server copy function.

Storage array also includes snapshots which are virtual replicas of the data in master copy . Depending on the application snapshots may collectively be substantially complete with respect to the master copy and in other examples snapshots may only provide access to portions of the data in master copy . In the example of data nodes request storage array to make some or all of the data in master copy available as snapshots . For instance if the analytic tasks only analyze portions of the data stored to storage array then the snapshots may provide an interface only for those portions.

As mentioned above snapshots act as interfaces to the data in master copy . However it should be noted that as the data in master copy is updated the data in any given snapshot will not be updated and thus a snapshot may in some instances not provide an interface to the latest version of the data. Nevertheless snapshots may provide an acceptable alternative to accessing physical replications in many embodiments.

Various embodiments include new techniques using snapshots to replace server based replication. The scope of embodiments may include any appropriate technique for creating snapshots . In general a given snapshot volume represents a point in time image of another volume on a storage system. In this example snapshots are implemented as virtual copies of the volume they are duplicating in this case snapshots are duplicating master copy . Thus snapshots and master copy are exportable by storage array as separate logical units commonly referred to as LUNs .

The following example provides one way of creating snapshots though the scope of embodiments is not limited to any snapshot creating technique. Internally each of the individual snapshots is implemented as a redirection table not shown and a small repository volume not shown for storing the original contents of data blocks that are subsequently on the base original volume in this case master copy . The redirection table has an entry for each block in the master copy . The redirection table is initially empty but it is updated for each write to the master copy . When a write to the master copy occurs the storage controller will first copy the original contents of the block being written to the repository. Following the copy on write operation the redirection table is updated to indicate that these blocks are now updated and should be retrieved from the repository volume instead of the master copy . Storage controller updates the master copy by the write following the steps above and treats the master copy normally for reading data. A similar mechanism is used to support writes to a snapshot volume although snapshots are sometimes implemented and or used as read only volumes. Hence the write case with respect to snapshots does not apply to some embodiments.

The above description is a simplification of the copy on write approach for implementing snapshot volumes and it is provided for ease of explanation. Some implementations of the embodiment of include optimizations that limit overhead and increase performance when performing copy on write though such optimizations are beyond the scope of this example.

As described above the embodiment of provides an inexpensive creation of a separate LUN that contains the same data or nearly the same as the base volume. Standard file systems on Linux such as though used by HDFS are not designed to be shared. Hence the conventional approach to sharing data is to copy the data into separate file systems one per host participating in the sharing. By contrast in the present embodiment once a given snapshot is created on storage array the master copy remains mounted by one host e.g. master node or any one of data nodes and one of the snapshots can be mounted as a separate non shared copy by one of nodes without incurring the overhead of copying the data. Additional respective nodes mount other instances of the snapshots . In other words in this example embodiment each respective node accesses a snapshot that has its own redirection table and repository volume. Each respective instance of the snapshots is provided with a LUN in the same manner that any virtual volume of the storage array would be provided a LUN. Providing each of the snapshots with a separate LUN allows each of the snapshots to be accessed separately. Thus in this example each of the nodes accesses a separate snapshot with its own unique LUN in the same way that a node would access any other virtual volume. The creation and management of the snapshots is performed by storage controller .

In system data nodes inform via a checkin operation the master node of the blocks contained in its volume. Master node tracks the set of files contained in the cluster file system but it relies on each of the data nodes to disclose where the data blocks for each file are stored. A cluster file system in this example is a file system that allows multiple different computers to share a common data volume . When a snapshot volume is mounted as a separate LUN on a second one of the data nodes the data node owning the base volume master copy and the data node owning the snapshot volume both disclose to the master node the same block IDs for the blocks stored on the master copy . Thus it appears to master node that each of the data nodes has its own local copy of the data i.e. virtual locality exists in system for each of the data nodes .

System is shown as an example only. Other types of hardware and software configurations may be adapted for use according to the features described herein.

At block the master node calls the data nodes to replicate data. This is similar to actions in a conventional HDFS implementation where the master node instructs the data nodes to replicate the data. Thus in some embodiments functionality from conventional HDFS may be adapted for use at least as far as the call from the master node the data nodes described in block . In some examples the master node is not limited to a single node instead the functions of a master node may be implemented by two or more nodes. For instance in an embodiment that builds upon an HDFS implementation the master node may include both a name node and a job tracker node.

At block the data nodes create snapshots on the storage array. In one example each of the data nodes includes an Application Programming Interface API that communicates with a controller of the storage array to cause the storage array to make one or more snapshots. The storage array may use any appropriate technique to create the snapshots including the copy on write operation described above.

Further in this example the storage array may create writeable snapshots. In one example the writable snapshots use the copy on write technique to update the master copy as the data is changed. Not every embodiment may implement snapshot write functionality in the data nodes. But those embodiments that do implement write functionality may apply the updates to one or more base volumes of the data on the storage array.

At block the data nodes mount the snapshots. In one example implementation the data nodes have an API that receives a signal from the storage array that the snapshots are created. After receiving the signal a particular data node mounts its respective snapshot volume. In this example the storage array configures its disk drives into RAID groups and volumes with addresses and access privileges on the storage area network. The storage array mounts or allows access by one or more servers to one or more volumes using the storage area network address and access privileges.

As mentioned above the storage array is configured to present the snapshots as virtual volumes with their own separate LUNs. In block the data nodes mount the snapshots as they would any other virtual volumes provided by the storage array. Each of the snapshots has its own LUN and each of the data nodes mounts its own respective snapshot volume. After the data nodes have mounted the snapshot volumes the data nodes interact with the snapshot volumes from the point of view of the data nodes as they would with any other volumes.

Block may further include the data nodes signaling to the master node that the data replication is complete. This step may be the same as or similar to a signaling step in HDFS in which the data nodes signal that data replication is complete.

At block the master node sends analyze commands to the data nodes. In a system adapted from an HDFS implementation the master node may use a job scheduler function to map different analytic jobs to the different nodes according to task partitioning principles. Block may be performed in response to receiving a signal from the data nodes that the replication is complete

At block the data nodes perform the commanded analytics and send the results back to the master node. In some examples each of the data nodes performs a different analytic job processing subtask on the same underlying data and in other examples each of the data nodes performs a similar analytic job on different portions of the underlying data. The scope of embodiments is not limited to any particular analytic job or any particular way of mapping the analytic jobs to each of the data nodes. In an embodiment adapted from a HDFS implementation block may also include each of the data nodes sending signals to the master node to indicate that their respective analytic job is done.

At block the master node assembles the analysis results and generates output. The action of block may be performed according to task partitioning principles in which the results are reduced to create acceptable output to a human user or another application.

Some embodiments include repeating process multiple times. Thus in one example the distributed analytic system receives multiple processing requests from different users e.g. in a web search engine embodiment for processing the same data. In such an example the analysis steps may be performed multiple times for a given replication operation. However the data may be changed or replaced as often as appropriate and in some embodiments the replication operations may be performed as often or nearly as often as the data is changed or replaced. In some embodiments the data replication operation may be performed for each subsequent processing task received by the distributed analytic computing system.

At block the storage array receives data from the distributed analytic computing system. The data is ingested at the distributed analytic computing system which may or may not include pre processing of the data. In any event the storage controller causes the storage array to store the data as managed data for example as a master copy. The storage array stores the managed data according to any appropriate technique. For instance the storage array may distribute the managed data in one or more virtual volumes.

At block the storage array creates multiple snapshots for the data nodes. In one example the storage controller receives signals from APIs at the data nodes requesting that the storage array create one or more snapshots of the data. The snapshots may be read only or writable and may be created by any appropriate technique. As explained above in a RAID example the copy on write technique may be used to create one or more snapshots and to update the base volume when data is changed.

Further as explained above the snapshots are not complete physical copies of the underlying data. In some embodiments each snapshot may include a redirection table and a repository for storing data to be updated. In an embodiment with multiple snapshots of the same data each snapshot functions as an interface by pointing to the same underlying data so that each snapshot is not a separate copy.

In the example above each data node is associated with at least one snapshot. Each such snapshot appears as a mountable virtual volume by virtue of having its own LUN. Block may also include in some embodiments sending a signal from the storage array or storage controller to the data node to indicate when the snapshot creation is complete.

At block the storage array deletes the snapshots and manages the master copy of the data. Deleting the snapshots may be performed in response to a command from the master node after the analytic tasks are complete. Managing the master copy may include updating the data in the master copy to reflect any recent write operations whether the write operations were performed by a data node on a writable snapshot or performed directly on the master copy.

Just as with process of processes of is exemplary and the scope of embodiments is not limited to the specific configuration shown. Other embodiments may add omit rearrange or modify one or more actions. For instance process may be performed in conjunction with process so that blocks are performed as often as commanded by the distributed analytic computing system.

The embodiments described above may provide one or more advantages over conventional HDFS implementations. For instance whereas conventional HDFS implementations use actual physical replication to achieve the desired degree of data replication the embodiments described above use snapshots provided by an external storage array. Each snapshot is not a complete replication of the underlying data and therefore does not have the same cost in storage space. Furthermore elimination of server to server replication in favor of replication within an external storage array conserves bandwidth of the network and processing power at the nodes.

An additional advantage of snapshots in the context of is that a given snapshot may be generated and mounted in a matter of seconds. This is in contrast to a conventional HDFS implementation that may have multiple terabytes of data in a particular replication at a data node. Copying terabytes of data may with current technology take multiple hours to complete. Therefore replication during normal use as well as replication during failure recovery may benefit from the embodiments described herein by virtue of increased efficiency and lower down times. is a timeline that illustrates a failure recovery process according to one embodiment which takes advantage of reduced replication time in the system of . Specifically the timeline illustrates actions taken by the data nodes and the master node of . illustrates actions by only four data nodes and it is understood that the actions of can be applied to any appropriate number of data nodes.

Process begins at action in a safe mode where the master node establishes a cluster by initializing four data nodes. Each of the data nodes are represented by a column in the signal diagram. For abbreviation DN refers to a data node and S refers to a snapshot.

At actions each of the data nodes generates a heartbeat signal to indicate to the master node that the data node is responsive and operable. Although not shown explicitly in each of the data nodes periodically generates a heartbeat signal so that its respective status can be continually monitored by the master node.

After the cluster is established and operability of the data nodes is confirmed the distributed analytic computing system exits safe mode at action . At action data arrives at the distributed analytic computing system and the data is sent to the storage array to be stored as a master copy.

The master node sends a command to the data nodes directing the data nodes to perform replication. Such action is described in more detail above with respect to block of . At actions the data nodes comply with the master node s commands by sending commands to the storage array to create snapshot volumes. The storage array creates the snapshot volumes which are mapped to the specific data nodes. The data nodes then mount their respective snapshot volumes. These actions are described above in more detail with respect to blocks of and block of . Data node is not in use at this part of the timeline yet and is indicated as standby at actions and .

At action the master node commences an analytics job by mapping processing subtasks to the data nodes. The master node sends analyze commands to the data nodes as in block of .

At actions the data nodes run their respective processing subtasks as in block of . During this time each of the data nodes is continuing to generate heartbeat signals. However at action data node ceases to generate heartbeat signals and is assumed to be inoperative.

In response to the lack of a heartbeat signal from data node the master node takes data node off line and issues a command to data node to mount the snapshot volume currently mapped to data node . Data node mounts the snapshot volume at actions and so that the volume used by data node is remapped to data node . Alternatively action may include creating a new snapshot volume for data node rather than mounting the snapshot volume used by data node . During this time processing continues at data nodes and and the processing subtasks assigned to data node are rescheduled by the master node to data node .

It is worth noting that the actions and of may take only a few seconds or even less than a second in some applications . Thus data node replaces data node with very little downtime. This is in contrast to a conventional HDFS implementation which continues to process the task with fewer nodes two instead of three while it initiates a physical replication of the data from one of the working servers over the network to data node causing I O congestion in the network and at the servers and wasting processing power at the servers. Furthermore a physical server to server replication in a conventional DHFS implementation may take hours or days depending on the particular implementation. Therefore the various embodiments described above that use snapshot volumes provide significant advantages in reducing downtime.

Embodiments of the present disclosure can take the form of a computer program product accessible from a tangible computer usable or computer readable medium providing program code for use by or in connection with a computer or any instruction execution system. For the purposes of this description a tangible computer usable or computer readable medium can be any apparatus that can store the program for use by or in connection with the instruction execution system apparatus or device. The medium can be an electronic magnetic optical electromagnetic infrared or a semiconductor system or apparatus or device . In some embodiments one or more processors not shown running in one or more of master node and or data nodes execute code to implement the actions shown in . Also in some embodiments one or more processors not shown running in one or more of storage controller execute code to implement the actions shown in .

Various embodiments may include one or more advantages over conventional systems. Specifically advantages regarding time savings lower storage costs less network congestion and less processing power waste are explained above. In addition to those advantages various embodiments described herein may provide increased scalability compared to conventional DHFS implementations that use server to server physical replication. As explained above there is very little cost for creating snapshots in terms of time storage space and the like. Various embodiments may take advantage of this low cost by providing any arbitrary degree of replication desired for a particular application. In other words the storage array can be configured to provide almost any degree of replication because of the low cost of snapshot volumes. Thus while the typical degree of replication for a conventional DHFS implementation is three various embodiments described herein may provide replication at significantly higher degrees.

The foregoing outlines features of several embodiments so that those skilled in the art may better understand the aspects of the present disclosure. Those skilled in the art should appreciate that they may readily use the present disclosure as a basis for designing or modifying other processes and structures for carrying out the same purposes and or achieving the same advantages of the embodiments introduced herein. Those skilled in the art should also realize that such equivalent constructions do not depart from the spirit and scope of the present disclosure and that they may make various changes substitutions and alterations herein without departing from the spirit and scope of the present disclosure.

