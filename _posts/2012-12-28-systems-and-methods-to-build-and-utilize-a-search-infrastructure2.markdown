---

title: Systems and methods to build and utilize a search infrastructure
abstract: Methods and systems to build and utilize a search infrastructure are described. The system generates index information components in real-time based on a database that is time-stamped. The system updates index information at a plurality of query node servers based on the index information components. A query engine receives a search query from a client machine and identifies search results based on the query and the index information. The system communicates the search results, over the network, to the client machine.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09607049&OS=09607049&RS=09607049
owner: eBay Inc.
number: 09607049
owner_city: San Jose
owner_country: US
publication_date: 20121228
---
This application claims priority to U.S. Provisional Application No. 61 675 793 filed on Jul. 25 2012 and entitled SYSTEMS AND METHODS TO BUILD AND UTILIZE A SEARCH INFRASTRUCTURE which is hereby incorporated by reference in its entirety.

This disclosure relates to the technical field of data storage and retrieval. More particularly systems and methods to build and utilize a search infrastructure.

A search infrastructure supports the storage of data items in one or more databases and the retrieval of the data items from the one or more databases. Building and utilizing the search infrastructure may present many technical challenges. In particular the performance manageability and quality of service in storing and retrieving the data items may present many opportunities for innovation.

In the following description for purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of some example embodiments. It will be evident however to one of ordinary skill in the art that embodiments of the present disclosure may be practiced without these specific details.

As described further below according to various example embodiments of the disclosed subject matter described and claimed herein systems and methods to build and utilize a search infrastructure are provided. Various embodiments are described below in connection with the figures provided herein.

Illustrated on the top left is an operation A that describes a first user operating the client machine to interact with an application server to store or update a document in a database illustrated in the middle are operations B C D E that describe retrieving and transforming the contents of the database storing the transformed contents in a database that is time stamped retrieving the contents from the database to generate a full index and a set of mini indexes which are utilized to generate and continually update the index information in the database to be consumed and served by the query node servers and illustrated on the top right is an operation F that describes a second user who operates a client machine to enter a query that is received by one or more query node servers that in turn apply the query to the index information to identify and return search results that reference the document . The above operations to continually rebuild the index information are performed in real time and without interruption to service that is provided to the first and second users who continue to interact with the system .

The index information may include an inverted index and document information . An inverted index e.g. inverted index as is well known in the art is an index data structure storing a mapping from content e.g. content contained by the document such as words or numbers to its locations in a database file or in a document e.g. document or a set of documents. The documents e.g. document data column group data and or information contained by the documents may be stored in the document information .

Merely for example a document X may include the words apple orange and banana a document Y may include the words apple and orange and a document Z may include the word apple. An inverted index for the words in documents X Y and Z may be generated as follows 

The above inverted index may be utilized to identify the word apple as being positioned in the first word of documents X Y and Z the word orange as being positioned in the second word of the documents X and Y and the word banana as being positioned as the third word of the document X. Accordingly the above inverted index may be utilized to map a keyword apple contained in a query that is received from a client computer to the documents X Y and Z that are further referenced in search results that are returned to the client computer. It is appreciated by one skilled in the art that the inverted index corresponds to the underlying database that it describes. Accordingly any update to the underlying database is reflected in a corresponding update to the inverted index . Updates to the database may include the addition and deletion of documents in the document information as well as the update of any of the contents contained by the documents in the document information . In the present embodiment the index information may be updated in real time to respond to a query in real time with accurate search results that include the most recent document information . To this end the operations A F are now further described.

The information storage and retrieval platform includes multiple components including the application servers that may execute on one or more application server machines not shown the database a database an Hadoop distributed file system the database the query node servers that operate on query node server machines not shown an HBase Hadoop Cluster comprised of one or more HBase Hadoop machines not shown including an HBase Hadoop Node e.g HBase Hadoop machine an index distribution module executing on HBase Hadoop machine search front end servers that executes on search machines not shown and search back end servers that execute on search machines not shown as being communicatively coupled together. For example the multiple components may be communicatively coupled with any combination of a wide area network local area network wireless network or any other type of network utilizing various networking technologies.

At operation A the document or one or more elements of the document may be communicated from the client machine to the application servers and stored in the database e.g. Oracle database . The document may include multiple elements including elements a b c d e and f that may include strings of text numeric information scores or other discrete quantum of information that are positioned in different sections or fields of the document e.g. item information .

At operation B at the application servers event manager modules may identify updates to the database generate events that correspond to the respective updates prioritize the events according to the quality of the data in the event and communicate the prioritized events into event queues that are consumed by consumer modules that service the respective event queues . According to an embodiment the event manager modules and the consumer modules may utilize three event queues to process and prioritize event types. For example the update of the element a in the document in the database may be a price change to item information describing an item for sale that causes the generation of a corresponding event that is associated with a high priority that in turn is communicated into in a first event queue associated with high priority that in turn is received by a consumer module . Similarly the update of the element b in document in the database may be a change to a title of the item that causes the generation of an event that is associated with a medium priority that in turn is communicated into a second event queue associated with the medium priority that in turn is received by a consumer module . Finally the update of the element c in document in the database may be a change to a description of the item that causes the generation of an event that is communicated into a third event queue associated with a low priority that in turn is received by a consumer module . Accordingly the three event queues may be utilized to communicate events in high medium and low priorities to facilitate a preference for the update of high priority events e.g. price over medium priority events e.g. title over low priority events e.g. description . In some embodiments the priority for the respective event types may be configured. Other embodiments may include fewer or more event queues .

At operation C the consumer modules may transform the data in the events and communicate the transformed data via an HBase application programming interface to an HBase master server in an HBase Hadoop cluster that in turn stores the transformed data in one or more tables including an items table in the database e.g. HBase . The transformed data may be stored according to regions that are managed by region server processes . According to an embodiment the database may be embodied as an open source non relational distributed database e.g. HBase that runs on a Hadoop Distributed Filesystem HDFS . HDFS is an open source software framework that supports data intensive distributed applications known by those skilled in the art. The HBase Hadoop cluster may further includes the HBase master server that is utilized to manage the HBase HDFS environment a scheduler module and an HBase Hadoop node that includes multiple region server processes and a map reduce job module . Each region server process may further be associated with a column not shown that corresponds to a range of documents e.g. or items corresponding to item information in the items table and may be utilized to manage one or more regions not shown that respectively correspond to a range of the documents . For example the documents may be uniquely identified with document identifiers e.g. item identifiers that are numbered from to X where each column and region are dedicated to respective overlapping predetermined ranges of documents e.g. documents and documents as described further in this document. According to one embodiment the number of region server processes may be in the hundreds but scaling is not limited to any fixed number. HBase is a technology that provides a fault tolerant way of storing large quantities of sparse data featuring compression in memory operation and a space efficient probabilistic data structure e.g. Bloom filters on a per column basis as outlined in the original BigTable paper as is known by those skilled in the art. An items table in the database e.g. HBase may serve as the input and output for one or more map reduce jobs that are scheduled by the map reduce job module . The map reduce jobs may be embodied as a map jobs and reduce jobs that runs in HDFS. The items table in the database may further be accessed through the Java Application Programming Interface API but also through representational state transfer REST architecture and other APIs.

At operation D the scheduler module executing in the HBase Hadoop cluster may schedule two index generating sub operations that process in parallel to generate indexes that are subsequently distributed to the query node servers . The sub operations may execute for the generating of a full index and the generating of the mini indexes . The sub operations may further execute for the distribution of the indexes to the query node servers . The full index may be a snapshot of the contents of items table in the database and the mini indexes may respectively correspond to a series of consecutive snapshots where each snapshot captures one or more updates to the items table in the database that occurred within an associated time period of time. The distribution of the full indexes and the mini indexes to the query node servers may be over a network utilizing an index distribution module based on Bit Torrent a peer to peer file sharing protocol. In one embodiment the scheduler module may schedule the generation of the full index twice in a twenty four hour period and the generation of mini indexes every five minutes. The scheduler module may generate a full index that is associated with a start time by scheduling a map reduce job module . The map reduce job module may initiate a map step that divides the job into smaller sub jobs e.g. map tasks and multiple reduce steps that consume the output from the sub jobs and aggregates results to generate the index information . Similarly the scheduler module may generate a mini index by scheduling a map reduce job module for execution on the HBase Hadoop Node . The generation of the mini index may include a map step but not according to one embodiment a reduce step. Accordingly each mini index may be associated with events that arrive from the event queues during a particular period of time and is associated with one or more full indexes . Each index e.g. full and mini may include a bill of material BOM information which describes the content of the index including the index information . The full index may include full index BOM information and the mini index may include mini index BOM information . The index information may include the inverted index and document information as previously described.

At operation E each of the query node servers may receive the full index and the associated mini indexes . The query node servers may be comprised of a search grid that is arranged in columns of query node servers as described later in this document. Each column of query node servers may be utilized to manage a range of the documents as previously mentioned. The index information may be stored in memory of the query node servers and in the database connected to the query node servers . The index information may be updated with the full index responsive to its arrival at the query node servers . Further the index information may be updated with the mini index responsive to its arrival at the query node servers . The index information is generally updated in sequential order. For example the index information are generally updated at the query node server in the order in which the full index and the mini indexes are generated. To this end the full index may be associated with full index BOM information the mini index may be associated with mini index BOM information that are utilized by the query node server to manage the update of the index information . In one embodiment a map reduce job module may include sub jobs that execute on the HBase Hadoop node to generate inverted indices in the form of region sub indices not shown for part of the region associated with the region server HBase . The sub jobs may further merge or stitch the multiple region sub indices together for the region.

At operation F a second user who operates the client machine may enter a query that may be communicated over a network e.g. Internet via front end servers and back end servers to be received by the query node servers which may be divided into two layers. The two layers may include an aggregation layer and a query execution layer. The aggregation layer may include a query node server that includes a query engine e.g. query module that receives the query that in turn communicates the query to multiple query engines that respectively execute in the execution layer in multiple query node servers that correspond to the columns. The query engines in the query execution layer may in turn respectively apply the same query in parallel against respective the index information that were generated for a range of document identifiers e.g. column to identify search results e.g. document in parallel. Finally the query engines at each query node server in the query execution layer may communicate their respective partial search results to the query engine in the aggregation layer which aggregates the multiple sets of partial search results to form a search result for the entire index information and to communicate the search result over the network to the second user.

Callout corresponds to a full snapshot of the items table and callout corresponds to a full deployment of the full snapshot . The full snapshot may capture the entire contents of the items table at an instant in time. Further callout corresponds to a full snapshot that occurs later in time and callout corresponds to a full deployment of the full snapshot . The full snapshot and the full snapshot may be utilized to respectively generate the full index and the full index 

Callout corresponds to a start time of a delta snapshot of the items table and callout corresponds to an end time of the delta snapshot . The delta snapshot may capture the changes to the items table that are subsequent to the previous delta snapshot. For example subsequent to a prior delta snapshot an entry of item information may be added to the items table an entry of item information may be removed from the items table or an existing item information entry may be modified. These changes are capture with the delta snapshot. Sequential delta snapshots are illustrated including callout which corresponds to a start time of a delta snapshot of the items table and callout which corresponds to an end time of the delta snapshot . The successive delta snapshots may be may be utilized to generate the mini indexes e.g. mini index mini index mini index etc. 

The index information at the query node servers may be updated with the full indexes and the mini indexes in an order that is sequential. For example the index information may be updated based on the order in which the full index and the mini indexes are generated and communicated to the query node servers . Further the mini indexes may arrive out of sequence at the query node servers . Accordingly each of the query node servers may utilize current BOM information at the query node servers a full index BOM information associated with the full index and the mini index BOM information associated with the mini index to ensure the update is performed in sequential order. In some embodiments a delta snapshot may be skipped if explicitly identified. Further it will be appreciated that the same index information at the query node server may be generated by combining different full and delta snapshots. For example the index information may be generated based on the full snapshot associated with the full index and the delta snapshots respectively associated with the mini indexes or the full index and the delta snapshots respectively associated with the mini indexes . Other equivalent combinations may be formed. For example the index information may be generated based on the full snapshot associated with the full index and the delta snapshots respectively associated with the mini indexes or the full snapshot associated with the full index and the delta snapshots respectively associated with the mini indexes etc.

At operation the HBase Hadoop Cluster may include a scheduler module that periodically generates builds the index information components including the full index or the mini index . The scheduler module may periodically generate the index information component by scheduling a map reduce job module that initiates jobs that execute in a map reduce framework. The map reduce job module may schedule one set of jobs to generate the full index and another set of jobs to generate the mini index . The building of the full index and the mini index may be in real time while the information search and retrieval platform remains operational and in parallel. For example the scheduler module may schedule the generation of the full index twice in a twenty four hour period and the generation of mini indexes every five minutes. The scheduling and execution of jobs is described more fully in method of .

At operation the index distribution module may communicate the index information component to the appropriate query node servers . For example the index distribution module may communicate the full index to the appropriate column of query node servers in the grid of query node servers responsive to the build of the full index being completed. Also for example the index distribution module may communicate the mini index to the appropriate column of query node servers in the grid of query node servers responsive to the build of the mini index being completed.

At operation the query node servers in the query node column may update the index information responsive to receipt of the index information component . The query node server may update the index information with the full index by restarting the query node server as described more fully in method of . Also for example the query node server may update the index information with the mini index as described more fully in method of .

At operation the information storage and retrieval platform may receive a search query over a network from a client machine and utilize the index information in the grid of query node servers to identify search results that are communicated back to the client machine .

At decision operation the map reduce job module may identify whether a mini index is scheduled for generation build. If a mini index is scheduled for generation build then the map reduce job module may sequentially execute the mini index section job operation and the transport packing job operation . The transport packing job may communicate the mini index to the appropriate query node column of query node servers in the grid of query node servers . The execution of jobs is described more fully in a data flow of .

At decision operation the query engine may identify whether the received mini index is identified with a mini index identifier that identifies the next expected mini index . For example the query engine may identify whether the mini index identifier in the mini index BOM information is equal to the current mini index identifier plus 1. If the received mini index is the next in sequence then processing continues at operation . Otherwise processing continues at operation . At operation the query engine may identify whether mini indexes may be skipped. For example the query engine may read the skip information in the mini index BOM information included in the mini index . At operation the query engine may identify whether any mini indexes have been stored as mini index storage information . At decision operation the query engine may determine whether the update of the index information in the query node server may be performed based on the skip information and the identified stored mini indexes . If the update may be performed then processing continues at operation . Otherwise processing continues at operation . At operation the query engine may store the mini index that was most recently received as mini index storage information . At operation the query engine may update in sequential order the index information in the query node server with the mini indexes that were identified. For example the query engine may sequentially update the index information with the one or more mini indexes identified as stored as mini index storage information and the mini index that was most recently received while skipping any mini indexes that were identified in the skip information .

The full index section job may initiate map tasks e.g. M M M MN one for each of the regions of the items table as previously described. The map tasks may take full snapshots of the item information corresponding to item identifiers in the associated region . To this end the map tasks may read item information e.g. describing items from the items table according to regions and generate token information and other information both being utilized to generate the section information . The other information may be communicated directly to the reducers e.g. R R R RN . The token information may be communicated to a partitioned which in turn partitions the token information for consumption by reducers e.g. R R R RN . The partitioner may partition the token information not shown based on the contents of the token information including a token element not shown an item identifier and the column identifier. For example token information may be embodied as follows 

Responsive to receiving the token information the partitioner may identify a particular reducer e.g. R R R RN based on a hash value that is generated from the token element and the column identifier and send the token information to the identified reducer . The merger jobs may initiate the reducers and map tasks to process the token information and other information to generate the full index . The reducers and map tasks may execute on the HBase Hadoop nodes . It will be appreciated that processing time to produce the full index may be minimized by increasing the number of map tasks reducers map tasks and HBase Hadoop nodes . Further resources may be economized by decreasing the same. Each of the reducers may segregate the received token information according to columns e.g. COLUMN COLUMN COLUMN COLUMN N . For example the token information and other information for Column may be segregated as output for COLUMN . Other output may be segregated for other columns in a similar manner. Recall that the columns may correspond to a query node column of query node servers in a grid of query node servers not shown that utilize the full index once generated to process a query. The reducers may organize the token information and other information into output according to columns based on column identifiers and distributes the output in accordance with the columns to the map tasks . For example illustrates the reducer identified as R as receiving the token information for all columns generating output that is organized according to the columns C C C CN and distributing the output for C to the map task M. For clarity sake the other output e.g. C C and CN is not illustrated as being distributed to the other map tasks e.g. M M M and MN . Further the remaining reducers e.g. R R and RN are also illustrated as distributing the output for C to the map task R but again for clarity sake the full data flow is not illustrated. Broadly each reducer may generate output for all columns and distributes the output to the map tasks according to columns .

The map task may receive the output for a single column . The map task may utilize the output and the other information to generate the section information e.g. S S S and SN for the particular column .

The index packing job may execute to generate the full index . The index packing job may generate the full index by packing the sections of the section information together generating the full index BOM information and packing the full index . The index packing job may pack the full index by packing the section information the full index BOM information and the index properties information into the full index .

Finally the transport job may execute to distribute the full indexes according to columns to the grid of query node servers . For example the transport job may execute to transport the full index for column to each of the query node servers in column of the grid . In one embodiment the distribution of the full indexes to the query node servers may be over a network utilizing the index distribution module based on Bit Torrent a peer to peer file sharing protocol.

The mini index section job may initiate map tasks e.g. M M M and MN one for each column . The map tasks may further correspond to two regions of the items table according to an embodiment. Other embodiments may utilize a different ratio of regions to columns to map tasks . The map tasks may take a snapshot of changes to the items table that have occurred between a start time and an end time. For example the snapshot may record an addition of item information e.g. new item a deletion of item information and a modification to existing item information e.g. field addition field addition field modification . The map tasks may further generate the mini index . The map tasks may generate the mini index by packing the sections of the section information together generating the mini index BOM information and packing the mini index . The map tasks may pack the mini index by packing the section information the mini index BOM information and the index properties information into the mini index .

The transport job may execute to distribute the mini indexes according to columns to the query node column in the grid of query node servers . For example the transport job may execute to transport the mini index for column to the query node servers not shown in column of the grid not shown . In one embodiment the distribution of the mini indexes to the query node servers may be over a network utilizing the index distribution module based on Bit Torrent a peer to peer file sharing protocol.

At operation A the information storage and retrieval platform may utilize search front end servers to receive the query from the client machine . For example the query may include the keywords BLACK IPOD NANO ACCESSORIES. The search front end servers may parse the query to generate query information and store the query information in a query container . The query container may contain multiple entries of query information some being parsed from the same query BLACK IPOD NANO ACCESSORIES and others being parsed from other queries not shown . The query information that is illustrated is for the query expression AND IPOD NANO being parsed from the example query BLACK IPOD NANO ACCESSORIES. Other query information is not illustrated. The query information may include the query expression output field information sort field information and a primary input table . The query expression as described above may be comprised of keywords that are parsed by the front end server from the query that is received and operators that either appear in the query or are implied as being in the query . The output field information may identify output fields to be included the search results. For example the output field information may identify one or more fields of records e.g. items documents that are included in the search results. The sort field information may identify the one or more field s utilized to sort the search results and whether to sort in ascending or descending order. The primary input table may identify an input table from which data is retrieved based on the query expression . At least a portion of the data may be returned to the client machine as search results.

At operation B the search front end servers may communicate the query container to the search back end servers . The search back end servers may process the query information in the query container as described later in this document.

At operation C the search back end servers may communicate the query container to a query node server in an aggregation layer of query node servers . The query node server may respond to receipt of the query container by invoking a query engine not shown to generate a query expression tree based on the query expression and store the query expression tree in the query container . Further the query engine may identify a single query node server in each of the query node columns of the grid of query node servers and communicate the query container to the identified query node servers . Further recall that each of the query node columns is dedicated to a particular range of documents e.g. items in the index information . Accordingly the query node server in the aggregation layer communicates the query container to one query node server in each of the query node columns in the grid to retrieve search results for the entire index information .

At operation D the query node servers in respective query node columns may receive the query container and process the query information entries in the query container . For example the query node server may process each query information entry to build e.g. generate and execute a query plan . To this end one query engine from each query node column invokes a query plan builder to build the query plan and further executes the query plan . The query plan may include a cursor expression tree that include expression nodes not shown that correspond to cursor objects of the query expression tree not shown . The query plan builder may invoke expansion generators not shown that read the expression nodes of the query expression tree to generate the cursor objects of the cursor expression tree . The expansion generators may include a generic expansion generator not shown that executes in the execution layer to generate cursor objects and multiple specific expansion generators not shown that execute in the storage layer to generate storage cursor objects. The expression nodes directly correspond to the cursor objects e.g. one to one correspondence .

The query engine executes the query plan . For example the query engine may execute cursor objects not shown and storage cursor objects not shown in the query plan . The query engine may execute the storage cursor objects to retrieve data from a particular storage device . The query node server may store the data that was retrieved in a table container that may subsequently be communicated as search results via the aggregation layer to the search back end servers to the search front end servers to the client machine . Accordingly operations that are unique to a particular storage device are hidden within a storage layer that is accessible via an execution layer that is exposed to query processing clients e.g. query engine resulting in a unified storage interface.

In one embodiment the transformer may perform an expansion function for the query information in the query container . For example the query expression AND IPOD NANO may be expanded to capture plural forms synonyms idioms etc. In one embodiment the transformer may further perform a scatter gather function by iterating the search of the query expression and blending the results. The transformer may generate the desired search result by initiating two searches in parallel. The transformer may initiate the first search by communicating the query container to a first query node server in the aggregation layer of query node servers to request item information for items that are offered for sale with an auction process and the second search by communicating the query container to a second query node server in the aggregation layer of query node servers to request item information for items that are offered for sale with a purchase process as previously described. The search results may be blended into a single search result as previously described.

At operation the query node server in the aggregation layer may utilize the query engine to generate a query expression tree for each query information entry in the query container . For example the query engine may generate the query expression tree based on the query expression in the query information and store the query expression tree in the query container . The query expression tree may include nodes representing expressions in the query expression that are logically connected with edges. For example the query expression tree for the query expression AND IPOD NANO may include an operator expression node for AND a term expression node IPOD and a term expression node for NANO where two edges lead away from the AND operator expression node one leading to the IPOD term expression node and the other leading to the NANO term expression node . Further the query engine may identify a single query node server in each of the query node columns of a grid of query node servers and communicate the query container to the identified query node servers .

At operation the query node servers in respective query node columns may receive the query container and invoke the query engine to invoke the query plan builder . The query plan builder may process each query information entry in the query container to build e.g. generate an associated query plan . The query plan builder may build the query plan to include a cursor expression tree that includes cursor objects that correspond to expression nodes in the query expression tree as described more fully in method on .

At operation the query engine may execute the cursor expression tree to retrieve data from a storage device . For example the query engine may execute a method in the storage cursor object for NANO to retrieve records e.g. item information that include the string NANO from a storage device e.g. relational storage . Further the query engine may execute a method in the storage cursor object for IPOD to retrieve records e.g. item information that include the string IPOD from a storage device e.g. relational storage . Finally the query engine may execute a method in the cursor object for AND to AND the two sets of retrieved records and store the combined set in a table container as results.

At operation the query node server may communicate table container via the aggregation layer to the search back end servers that in turn communicate the table container to the search front end servers that in turn extract the search results from the table container and communicate the search results to the client machine .

An Application Program Interface API server and a web server are coupled to and provide programmatic and web interfaces respectively to one or more application servers . The application servers host one or more marketplace applications and payment applications . The application servers are in turn shown to be coupled to one or more database servers that facilitate access to one or more databases

The marketplace applications may provide a number of marketplace functions and services to users that access the network based marketplace . The payment applications may likewise provide a number of payment services and functions to users. The payment applications may allow users to accumulate value in accounts and then to later redeem the accumulated value for products e.g. goods or services that are made available via the marketplace applications . The value may be accumulated in a commercial currency such as the U.S. dollar or a proprietary currency such as points. While the marketplace applications and payment applications are shown in to both form part of the network based marketplace it will be appreciated that in alternative embodiments the payment applications may form part of a payment service that is separate and distinct from the network based marketplace .

Further while the networked system shown in employs client server architecture embodiments of the present disclosure are of course not limited to such an architecture and could equally well find application in a distributed or peer to peer architecture system for example. The various marketplace applications and payment applications could also be implemented as standalone software programs which do not necessarily have networking capabilities.

The web client and mobile web client access the various marketplace applications and payment applications via the web interface supported by the web server . Similarly the programmatic client accesses the various services and functions provided by the marketplace applications and payment applications via the programmatic interface provided by the API server . The programmatic client may for example be a seller application e.g. the TurboLister application developed by eBay Inc. of San Jose Calif. to enable sellers to author and manage listings on the network based marketplace in an off line manner and to perform batch mode communications between the programmatic client and the network based marketplace .

The mobile device may be embodied as a mobile phone a personal digital assistant PDA a cell phone or any other wireless device that is capable of communicating with the network based marketplace . For example the mobile device may be embodied as an iPhone mobile phone manufactured by Apple Inc. of Cupertino Calif. or as previously mentioned a Blackberry mobile phone manufactured by Research In Motion of Waterloo Ontario.

The network based marketplace of may provide a number of publishing listing and price setting mechanisms whereby a seller may list or publish information concerning goods or services for sale a buyer can express interest in or indicate a desire to purchase such goods or services and a price can be set for a transaction pertaining to the goods or services. To this end the marketplace applications are shown to include at least one publication application and one or more auction applications which support auction format listing and price setting mechanisms e.g. English Dutch Vickrey Chinese Double Reverse auctions etc. . The various auction applications may also provide a number of features in support of such auction format listings such as a reserve price feature whereby a seller may specify a reserve price in connection with a listing and a proxy bidding feature whereby a bidder may invoke automated proxy bidding.

A number of fixed price applications support fixed price listing formats e.g. the traditional classified advertisement type listing or a catalogue listing and buyout type listings. Specifically buyout type listings e.g. including the Buy It Now BIN technology developed by eBay Inc. of San Jose Calif. may be offered in conjunction with auction format listings and may allow a buyer to purchase goods or services which are also being offered for sale via an auction for a fixed price that is typically higher than the starting price of the auction.

Store application s allows a seller to group listings within a virtual store which may be branded and otherwise personalized by and for the seller. Such a virtual store may also offer promotions incentives and features that are specific and personalized to a relevant seller.

Reputation applications allow users that transact utilizing the network based marketplace to establish build and maintain reputations which may be made available and published to potential trading partners. Consider that where for example the network based marketplace supports person to person trading users may otherwise have no history or other reference information whereby the trustworthiness and credibility of potential trading partners may be assessed. The reputation applications allow a user to establish a reputation within the network based marketplace over time for example through feedback provided by other transaction partners and by the computation of a feedback score based on the feedback. For example the feedback score may be publicly displayed by the network based marketplace . Other potential trading partners may then reference such a feedback score for the purposes of assessing credibility and trustworthiness.

Personalization applications allow users of the network based marketplace to personalize various aspects of their interactions with the network based marketplace . For example a user may utilizing an appropriate personalization application create a personalized reference page at which information regarding transactions to which the user is or has been a party may be viewed. Further a personalization application may enable a user to personalize listings and other aspects of their interactions with the networked system and other parties.

The networked system may support a number of marketplaces that are customized for example for specific geographic regions. A version of the networked system may be customized for the United Kingdom whereas another version of the networked system may be customized for the United States. Some of these versions may operate as an independent marketplace or may be customized or internationalized presentations of a common underlying marketplace. The networked system may accordingly include a number of internationalization applications that customize information and or the presentation of information by the networked system according to predetermined criteria e.g. geographic demographic or marketplace criteria . For example the internationalization applications may be used to support the customization of information for a number of regional websites that are operated by the networked system and that are accessible via respective servers and both of .

Navigation of the network based marketplace may be facilitated by one or more navigation applications . Merely for example the navigation applications may receive search information in the form of a query to search for items on the network based marketplace and return search results responsive to the request. A browse application may allow users to browse various category catalogue or inventory data structures according to which listings may be classified within the networked system . Various other navigation applications may be provided to supplement the search and browsing applications. For example the navigation applications may include the event manager module the scheduler module the map reduce job module included in the system to build and utilize a search infrastructure. Further the navigation applications may include other modules in the system that are not presently mentioned. In order to make listings available via the networked system as visually informing and attractive as possible the marketplace applications may include one or more imaging applications with which users may upload images for inclusion within listings. An imaging application also operates to incorporate images within viewed listings. The imaging applications may also support one or more promotional features such as image galleries that are presented to potential buyers. For example sellers may pay an additional fee to have an image included within a gallery of images for promoted items.

Listing creation applications allow sellers to conveniently author listings pertaining to goods or services that they wish to transact via the network based marketplace while the listing management applications allow sellers to manage such listings. Specifically where a particular seller has authored and or published a large number of listings the management of such listings may present a challenge. The listing creation applications may further include a processing module communication module and listing module that facilitate a buyer watching for specific types of listings. The listing management applications provide a number of features e.g. auto relisting inventory level monitors etc. to assist the seller in managing such listings.

One or more post listing management applications may also assist sellers with a number of activities that may typically occur post listing. For example upon completion of an auction facilitated by one or more auction applications a seller may wish to leave feedback regarding a particular buyer. To this end a post listing management application may provide an interface to one or more reputation applications so as to allow the seller conveniently to provide feedback regarding multiple buyers to the reputation applications .

Dispute resolution applications provide mechanisms whereby disputes arising between transacting parties may be resolved. For example the dispute resolution applications may provide guided procedures whereby the parties are guided through a number of steps in an attempt to settle a dispute. In the event that the dispute cannot be settled via the guided procedures the dispute may be escalated to a third party mediator or arbitrator.

A number of fraud prevention applications implement fraud detection and prevention mechanisms to reduce the occurrence of fraud within the network based marketplace .

Messaging applications are responsible for the generation and delivery of messages to users of the network based marketplace with such messages for example advising users regarding the status of listings at the network based marketplace e.g. providing outbid notices to bidders during an auction process or to providing promotional and merchandising information to users . Respective messaging applications may utilize any one of a number of message delivery networks and platforms to deliver messages to users. For example messaging applications may deliver electronic mail e mail instant message IM Short Message Service SMS text facsimile or voice e.g. Voice over IP VoIP messages via the wired e.g. the Internet Plain Old Telephone Service POTS or wireless e.g. mobile cellular WiFi e.g. IEEE 802.11 technologies including 802.11n 802.11b 802.11g and 802.11a Worldwide Interoperability for Microwave Access e.g. WiMAX IEEE 802.16 networks.

Merchandising applications support various merchandising functions that are made available to sellers to enable sellers to increase sales via the network based marketplace . The merchandising applications also operate the various merchandising features that may be invoked by sellers and may monitor and track the success of merchandising strategies employed by sellers. The transaction incentivizing applications operate to provide incentives for buyers and sellers to enter into and complete transactions.

The tables also include an items table in which item records are maintained for goods and services that are available to be or have been transacted via the network based marketplace . Item records within the items table may furthermore be linked to one or more user records within the user table so as to associate a seller and one or more actual or potential buyers with an item record.

A transaction table contains a record for each transaction e.g. a purchase or sale transaction or auction pertaining to items for which records exist within the items table .

An order table is populated with order records with each order record being associated with an order. Each order in turn may be associated with one or more transactions for which records exist within the transaction table .

Bid records within a bids table relate to a bid received at the network based marketplace in connection with an auction format listing supported by an auction application of . A feedback table is utilized by one or more reputation applications of in one example embodiment to construct and maintain reputation information concerning users in the form of a feedback score. A history table maintains a history of transactions to which a user has been a party. One or more attributes tables record attribute information pertaining to items for which records exist within the items table . Considering only a single example of such an attribute the attributes tables may indicate a currency attribute associated with a particular item with the currency attribute identifying the currency of a price for the relevant item as specified in by a seller.

Search storage structures may store information that is utilized to search the items table and other tables. For example the search storage structures may be utilized by the system as illustrated n to build and utilize a search infrastructure according to an embodiment. A customization table may store customization records that may be utilized to customize the operation of the network based marketplace .

The example computer system includes a processor e.g. a central processing unit CPU a graphics processing unit GPU or both a main memory and a static memory which communicate with each other via a bus . The computer system may further include a video display unit e.g. a liquid crystal display LCD or a cathode ray tube CRT . The computer system also includes an input device e.g. a keyboard a cursor control device e.g. a mouse a disk drive unit a signal generation device e.g. a speaker and a network interface device .

The disk drive unit includes a machine readable medium on which is stored one or more sets of instructions e.g. software embodying any one or more of the methodologies or functions described herein. The instructions e.g. software may also reside completely or at least partially within the main memory the static memory and or within the processor during execution thereof by the computer system . The main memory and the processor also may constitute machine readable media. The instructions may further be transmitted or received over a network via the network interface device .

Applications that may include the apparatus and systems of various embodiments broadly include a variety of electronic and computer systems. Some embodiments implement functions in two or more specific interconnected hardware modules or devices with related control and data signals communicated between and through the modules or as portions of an application specific integrated circuit. Thus the example system is applicable to software firmware and hardware implementations. In example embodiments a computer system e.g. a standalone client or server computer system configured by an application may constitute a module that is configured and operates to perform certain operations as described herein. In other embodiments the module may be implemented mechanically or electronically. For example a module may comprise dedicated circuitry or logic that is permanently configured e.g. within a special purpose processor to perform certain operations. A module may also comprise programmable logic or circuitry e.g. as encompassed within a general purpose processor or other programmable processor that is temporarily configured by software to perform certain operations. It will be appreciated that the decision to implement a module mechanically in the dedicated and permanently configured circuitry or in temporarily configured circuitry e.g. configured by software may be driven by cost and time considerations. Accordingly the term module should be understood to encompass a tangible entity be that an entity that is physically constructed permanently configured e.g. hardwired or temporarily configured e.g. programmed to operate in a certain manner and or to perform certain operations described herein.

While the machine readable medium is shown in an example embodiment to be a single medium the term machine readable medium should be taken to include a single medium or multiple media e.g. a centralized or distributed database and or associated caches and servers that store the one or more sets of instructions. The term machine readable medium shall also be taken to include any medium that is capable of storing encoding or carrying a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present description. The term machine readable medium shall accordingly be taken to include but not be limited to solid state memories optical media and magnetic media. As noted the software may be transmitted over a network using a transmission medium. The term transmission medium shall be taken to include any medium that is capable of storing encoding or carrying instructions for transmission to and execution by the machine and includes digital or analogue communications signal or other intangible medium to facilitate transmission and communication of such software.

The illustrations of embodiments described herein are intended to provide a general understanding of the structure of various embodiments and they are not intended to serve as a complete description of all the elements and features of apparatus and systems that might make use of the structures described herein. Many other embodiments will be apparent to those of ordinary skill in the art upon reviewing the above description. Other embodiments may be utilized and derived therefrom such that structural and logical substitutions and changes may be made without departing from the scope of this disclosure. The figures provided herein are merely representational and may not be drawn to scale. Certain proportions thereof may be exaggerated while others may be minimized. Accordingly the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.

In some embodiments the methods described herein may be implemented in a distributed or non distributed software application designed under a three tier architecture paradigm whereby the various components of computer code that implement this method may be categorized as belonging to one or more of these three tiers. Some embodiments may include a first tier as an interface e.g. an interface tier that is relatively free of application processing. Further a second tier may be a logic tier that performs application processing in the form of logical mathematical manipulations of data inputted through the interface level and communicates the results of these logical mathematical manipulations to the interface tier and or to a backend or storage tier. These logical mathematical manipulations may relate to certain business rules or processes that govern the software application as a whole. A third storage tier may be a persistent storage medium or non persistent storage medium. In some cases one or more of these tiers may be collapsed into another resulting in a two tier architecture or even a one tier architecture. For example the interface and logic tiers may be consolidated or the logic and storage tiers may be consolidated as in the case of a software application with an embedded database. This three tier architecture may be implemented using one technology or as will be discussed below a variety of technologies. This three tier architecture and the technologies through which it is implemented may be executed on two or more computer systems organized in a server client peer to peer or so some other suitable configuration. Further these three tiers may be distributed between multiple computer systems as various software components.

Some example embodiments may include the above illustrated tiers and processes or operations that make them up as being written as one or more software components. Common to many of these components is the ability to generate use and manipulate data. These components and the functionality associated with each may be used by client server or peer computer systems. These various components may be implemented by a computer system on an as needed basis. These components may be written in an object oriented computer language such that a component oriented or object oriented programming technique can be implemented using a Visual Component Library VCL Component Library for Cross Platform CLX Java Beans JB Java Enterprise Beans EJB Component Object Model COM Distributed Component Object Model DCOM or other suitable technique. These components may be linked to other components via various APIs and then compiled into one complete server client and or peer software application. Further these APIs may be able to communicate through various distributed programming protocols as distributed computing components.

Some example embodiments may include remote procedure calls being used to implement one or more of the above illustrated components across a distributed programming environment as distributed computing components. For example an interface component e.g. an interface tier may reside on a first computer system that is remotely located from a second computer system containing a logic component e.g. a logic tier . These first and second computer systems may be configured in a server client peer to peer or some other suitable configuration. These various components may be written using the above illustrated object oriented programming techniques and can be written in the same programming language or a different programming language. Various protocols may be implemented to enable these various components to communicate regardless of the programming language used to write these components. For example a component written in C may be able to communicate with another component written in the Java programming language by using a distributed computing protocol such as a Common Object Request Broker Architecture CORBA a Simple Object Access Protocol SOAP or some other suitable protocol. Some embodiments may include the use of one or more of these protocols with the various protocols outlined in the Open Systems Interconnection OSI model or Transport Control Protocol Internet Protocol TCP IP protocol stack model for defining the protocols used by a network to transmit data.

Some embodiments may utilize the OSI model or TCP IP protocol stack model for defining the protocols used by a network to transmit data. In applying these models a system of data transmission between a server and client or between peer computer systems is illustrated as a series of roughly five layers comprising an application layer a transport layer a network layer a data link layer and a physical layer. In the case of software having a three tier architecture the various tiers e.g. the interface logic and storage tiers reside on the application layer of the TCP IP protocol stack. In an example implementation using the TCP IP protocol stack model data from an application residing at the application layer is loaded into the data load field of a TCP segment residing at the transport layer. This TCP segment also contains port information for a recipient software application residing remotely. This TCP segment is loaded into the data load field of an IP datagram residing at the network layer. Next this IP datagram is loaded into a frame residing at the data link layer. This frame is then encoded at the physical layer and the data transmitted over a network such as an internet Local Area Network LAN . WAN or some other suitable network. In some cases internet refers to a network of networks. These networks may use a variety of protocols for the exchange of data including the aforementioned TCP IP and additionally ATM SNA SDI or some other suitable protocol. These networks may be organized within a variety of topologies e.g. a star topology or structures.

The illustrations of embodiments described herein are intended to provide a general understanding of the structure of various embodiments and they are not intended to serve as a complete description of all the elements and features of apparatus and systems that might make use of the structures described herein. Many other embodiments will be apparent to those of ordinary skill in the art upon reviewing the above description. Other embodiments may be utilized and derived therefrom such that structural and logical substitutions and changes may be made without departing from the scope of this disclosure. The figures provided herein are merely representational and may not be drawn to scale. Certain proportions thereof may be exaggerated while others may be minimized. Accordingly the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.

Thus systems and methods to build and utilize a search infrastructure are disclosed. While the present disclosure has been described in terms of several example embodiments those of ordinary skill in the art will recognize that the present disclosure is not limited to the embodiments described but may be practiced with modification and alteration within the spirit and scope of the appended claims. The description herein is thus to be regarded as illustrative instead of limiting.

