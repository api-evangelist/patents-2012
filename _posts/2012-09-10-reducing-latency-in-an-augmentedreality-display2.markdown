---

title: Reducing latency in an augmented-reality display
abstract: Disclosed are methods and systems for generating display pixel data so as to reduce latency when rendering a representation of a graphic on a display, such as for augmented-reality applications. The method comprises: receiving a set of display pixel coordinate-pairs at the graphics processing unit; applying a transform matrix to the set of display pixel coordinate-pairs to obtain a set of graphic pixel coordinate-pairs, the transform matrix calculated using orientation data received from an external reference; retrieving a set of graphic pixel data associated with the set of graphic pixel coordinate-pairs; and, determining a set of display pixel data based on the retrieved set of graphic pixel data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09576397&OS=09576397&RS=09576397
owner: BlackBerry Limited
number: 09576397
owner_city: Waterloo, Ontario
owner_country: CA
publication_date: 20120910
---
The present disclosure relates generally to augmented reality systems and more particularly to generating display pixel data for rendering a representation of a graphic on a display in augmented reality systems.

Augmented reality systems can be used in connection with certain electronic devices. For example digital images can be superimposed over an environment shown on a display of an electronic device in order to provide additional information to a viewer. The digital images can be related to the objects in the environment by location or content for example.

Electronic devices can use sensor data to identify features of the objects in the environment in order to provide an augmented reality display. Augmented reality is a process wherein a live view of a physical real world environment which may be obtained via the camera for example may be augmented by computer generated images.

In one aspect the present disclosure describes a method implemented on a graphics processing unit of generating display pixel data for rendering a representation of a graphic on a display the method comprising receiving a set of display pixel coordinate pairs at the graphics processing unit applying a transform matrix to the set of display pixel coordinate pairs to obtain a set of graphic pixel coordinate pairs the transform matrix calculated using orientation data received from an external reference retrieving a set of graphic pixel data associated with the set of graphic pixel coordinate pairs and determining a set of display pixel data based on the retrieved set of graphic pixel data.

In another aspect the present disclosure describes a graphics processing unit associated with a memory the graphics processing unit comprising a controller for calculating a transform matrix based on orientation data received from an external reference the orientation data representing a three dimensional plane a warp unit in communication with the controller for applying the transform matrix to a set of display pixel coordinate pairs to calculate a set of graphic pixel coordinate pairs a pixel fetch module in communication with the warp unit for retrieving a set of graphic pixel data from a graphic stored in memory the set of graphic pixel data associated with the set of graphic pixel coordinate pairs and a rendering module for rendering the set of graphic pixel data on the display.

In another aspect the present disclosure describes a system for generating display pixel data for rendering a representation of a graphic on a display the system comprising a memory a processor for executing instructions stored on the memory a display and a graphics processing unit connected to the memory display and processor the graphics processing unit comprising a control processor for calculating a transform matrix based on orientation data received from an external reference identifying a three dimensional plane a warp unit in communication with the control processor for applying the transform matrix to a set of display pixel coordinate pairs to obtain a set of graphic pixel coordinate pairs a pixel fetch module in communication with the warp unit for retrieving a set of graphic pixel data from an image stored in memory the set of graphic pixel data associated with the set of graphic pixel coordinate pairs and a blending module for determining the set of display pixel data associated with the set of display pixel coordinate pairs for a rendering on the display.

It can be important to reduce the amount of latency when rendering a representation of a graphic on a display screen in an augmented reality setting. For example the augmented reality system may be a set of wearable glasses with the display screen consisting of one or both of the lenses of the glasses. The graphic to be rendered on the display screen may be associated with the environment or an element of the environment that is visible through the glasses. If the glasses are moved i.e. the user moves his or her head the view through the glasses changes and the rendered image or the rendered graphic may have to be regenerated or re rendered on the display screen to reflect this change in view. The time between the changing of the view and the regenerating or re rendering of the image to reflect this change is a latency that can be beneficial to minimize.

In existing augmented reality systems an image or graphic to be rendered on the display is first generated in its entirety by being stored into a frame buffer and then the frame buffer is sent to the display where the image is then rendered. This can cause a noticeable latency in situations when the display is a lens on wearable glasses.

The rendered image can be referred to as a rendered graphic. The rendered image can be a picture text video or another form digital image. One or more augmented reality rendered images can be rendered onto the display.

In accordance with one or more embodiments the target member can identify a location and or orientation on the display screen of the electronic device onto to which the augmented reality rendered images can be rendered or projected. In accordance with one or more embodiments the target member can identify or define a three dimensional planar region or a three dimensional plane onto which the one or more graphics can be projected for rendering as rendered images on the lens of the glasses.

The lenses of the glasses are an example of a display onto which one or more graphics can be represented as a rendered image . Other examples of displays onto which augmented reality images can be rendered include display screens of electronic devices such as computers mobile devices cellular telephones smart phones tablet computers and the like. Accordingly in one or more embodiments the display is not transparent or is partially transparent. In situations in which the display is transparent the environment can be the view through the display and the target member can be an object located in the environment. In situations in which the display is not transparent the electronic device which supports or contains the display can include a camera or other image sensing component. The camera can capture an image or video to output onto the display. The image or video captured by the camera can be the environment and can include the target member i.e. the target member can form part of the environment captured on camera or video . The rendered image can be such that it is superimposed over the environment on the display.

In one or more embodiments the position of the rendered image on the display can be related or associated to the position or orientation of the target member on the display. Further the orientation of the rendered image on the display can also be related to the position or orientation of the target member on the display. Similarly the content of the rendered image can be related to the position or orientation of the target member . By way of example the target member may transmit information or data to the electronic device indicating the position or orientation of the target member . The electronic device can interpret this information as a three dimensional planar region onto which the augmented reality image can be displayed. For example the target member can include one or more light emitting diodes LEDs which can emit light that can be received at the electronic device . The electronic device can calculate a three dimensional plane and or a three dimensional planar region based on the infrared light received from the LEDs. By way of further example the target member can transmit orientation data determined from a gyroscope accelerometer magnetometer and or one or more other types of sensors to the electronic device and the electronic device can use this transmitted orientation data along with orientation data representing the orientation of the electronic device in order to calculated the relative orientation of the target member and electronic device . Using the relative orientation the electronic device and the relative position of the target member e.g. from one or more LEDs on the target member the electronic device can calculate a three dimensional plane and or a three dimensional planar region. The rendered image can then be positioned or oriented onto the three dimensional planar region. In one or more embodiments the target member can transmit data to the electronic device identifying specific graphic including content text photographs or video for example to be displayed as a rendered image . The rendered image may also be referred to as a representation of a graphic on a display.

The LEDs are examples of data emitting objects or information emitting objects. Other types of data emitting objects can be supported by associated with or attached to the target member .

One or more of the LEDs can emit infrared light. The infrared light emitted from one or more of the LEDs can be received at the electronic device . For example the emitted infrared light can provide information to the electronic device such as the relative location of the LEDs .

In one or more embodiments the target member can itself be an electronic device such as a computer smartphone camera cellular phone tablet computer etc. By way of further example the target member such as an electronic device can transmit one or more graphics to be rendered as images in the augmented reality system on the display of the glasses. Such transmitted graphics can include email messages calendar reminders contact information etc.

Referring to an exemplary embodiment of an electronic device that implements an augmented reality system is illustrated in greater detail. The electronic device can be a wearable electronic device such as a pair of glasses. The components of the electronic device can be stored or supported in the arm of the glasses for example. In one or more embodiments the electronic device can be a computer with a display screen such as a digital camera a mobile communication device or a table computer.

The electronic device includes a processor which controls general operation of the electronic device . The processor can interact with additional device subsystems such as a display a memory a graphics processing unit GPU an Infrared receiver and any other device subsystems or peripheral devices generally designated at . The memory can include a random access memory RAM a read only memory ROM or flash memory for example. Other device subsystems may include auxiliary input output I O subsystems such as a keyboard trackball touchpad or optical sensor for example a data port a speaker a microphone a short range communications subsystem such as Bluetooth for example. The specific device subsystems included on the electronic device can depend on the nature of the electronic device . For example if the electronic device is a set of wearable glasses then it may not include auxiliary I O subsystems such as a keyboard or trackball.

In accordance with one or more embodiments the processor can interact with device subsystems such as a wireless communication subsystem for exchanging radio frequency signals with a wireless network to perform communication functions. Some of the subsystems shown in perform communication related functions whereas other subsystems may provide resident on device functions.

In one or more embodiments the electronic device may be equipped to send and or receive data across a communication network as shown at . In such embodiments the electronic device includes a communication subsystem which includes a receiver a transmitter and associated components such as one or more embedded or internal antenna elements and local oscillators LOs and a processing module such as a digital signal processor DSP . As will be apparent to those skilled in field of communications the particular design of the communication subsystem depends on the communication network in which the electronic device is intended to operate.

Operating system software used by the processor may be stored in memory which may include a persistent store such as flash memory which may be a ROM a ROM or similar storage element. The operating system specific device applications or parts thereof may be temporarily loaded into a volatile store such as RAM which may be included in memory .

The processor in addition to its operating system functions enables execution of software applications on the electronic device . A predetermined set of applications which control basic device operations can be installed on the electronic device during its manufacture. These basic operations can include data communication applications for example. Additionally applications may also be loaded onto the communication device through the network or other subsystems such as an auxiliary I O subsystem a serial port a short range communications module or any other suitable subsystem and installed by a user in memory for execution by the processor . Such flexibility in application installation increases the functionality of the electronic device and may provide enhanced on device features communication related features or both.

The infrared receiver can include an infrared sensing device and associated circuits and components. The infrared receiver can be configured to identify and locate the source of received infrared light. The received infrared light can be emitted from a source external to the electronic device . The Infrared receiver is one example of a short range communication module that can be included on the electronic device . Such short range communication modules can provide for communication between the electronic device and different systems or devices such as the target member which need not be similar devices. For example the short range communication module may include a wireless bus protocol compliant communication mechanism such as a Bluetooth communication module to provide for communication with similarly enabled systems and devices. In one or more embodiments the infrared receiver can be located in the GPU as shown in . However in one or more embodiments the infrared receiver can be separate and apart from the GPU as shown in .

The GPU is an electronic circuit that is adapted to configure or alter graphics for rendering on the display as rendered images . For example the GPU can obtain or receive a graphic stored in memory and alter the graphic before outputting it to the display .

The display is used to visually present rendered images to a user e.g. the person . Further the display may also be configured to visually present a view of the environment to a user. The display configuration or type of the display can depend on the type of electronic device . For example if the electronic device is a wearable electronic device such as set of glasses as shown in then the display may be at least partially transparent so that the person wearing the glasses can see the environment through the display in addition to a rendered image although the rendered image may not necessarily be present as in . By way of further example if the electronic device is a camera the display may be a screen on which an image captured by an image sensor in the camera is rendered. In yet a further example if the electronic device is a mobile computer containing a camera then an application s graphical user interface GUI can be rendered or presented on the display and or an image captured by the image sensor of the camera can be rendered on the display . The view of an image captured by the camera may be considered the environment on top of which a rendered image is displayed. The display can show an environment e.g. the view through a lens of a set of glasses or the view through a camera lens and can also render an image i.e. an augmented reality image on top of the environment.

In some example embodiments a device subsystem such as the I O subsystem may include an external communication link or interface for example an Ethernet connection. The electronic device may include other wireless communication interfaces for communicating with other types of wireless networks.

In some example embodiments the electronic device also includes a removable memory module typically including flash memory and a memory module interface . Network access may be associated with a subscriber or user of the electronic device via the memory module which may be a Subscriber Identity Module SIM card for use in a GSM network or other type of memory module for use in the relevant wireless network type. The memory module may be inserted in or connected to the memory module interface of the electronic device .

The electronic device may store data in memory which in one example embodiment is an erasable persistent memory. In various example embodiments the data may include service data having information required by the electronic device to establish and maintain communication with the wireless network . The data may also include user application data such text e.g. email messages address book and contact information calendar and schedule information notepad documents image files and other commonly stored user information stored on the electronic device by its user and other data.

In some example embodiments the electronic device is provided with a service routing application programming interface API which provides an application with the ability to route traffic through a serial data i.e. USB or Bluetooth Bluetooth is a registered trademark of Bluetooth SIG Inc. connection to the host computer system using standard connectivity protocols. When a user connects an electronic device to the host computer system via a USB cable or Bluetooth connection traffic that was destined for the wireless network is automatically routed to the electronic device using the USB cable or Bluetooth connection. Similarly any traffic destined for the wireless network is automatically sent over the USB cable Bluetooth connection to the host computer for processing.

The electronic device also includes a battery as a power source which is typically one or more rechargeable batteries that may be charged for example through charging circuitry coupled to a battery interface such as a serial data port. The battery provides electrical power to at least some of the electrical circuitry in the communication device and the battery interface provides a mechanical and electrical connection for the battery . The battery interface is coupled to a regulator not shown which provides power V to the circuitry of the electronic device .

A predetermined set of applications that control basic device operations including data and possibly voice communication applications may be installed on the electronic device during or after manufacture. Additional applications and or upgrades to an operating system or software applications may also be loaded onto the electronic device through the wireless network an auxiliary I O subsystem a data port a short range communication module or other suitable device subsystems . The downloaded programs or code modules may be permanently installed for example written into the program memory or written into and executed from a RAM for execution by the processor at runtime.

In some example embodiments the electronic device operates in a data communication mode in which it may receive a data signal such as a text message an email message an image file or a webpage. The data signal can be downloaded across the network for example processed by the communication subsystem and input to the processor for further processing. For example a downloaded webpage may be further processed by a web browser or an email message may be processed by the email messaging application and output to the display. The downloaded data can be then be rendered onto the display for example.

In some example embodiments the electronic device may also operate in a voice communication mode such as when the electronic device is a cellular phone or smart phone or otherwise provides telephony functions. The overall operation is similar to the data communication mode except that the received signals would be output to the speaker and signals for transmission would be generated by a transducer such as the microphone which could both be associated with the electronic device . The telephony functions are provided by a combination of software firmware i.e. a voice communication module and hardware i.e. the microphone the speaker and input devices . Alternative voice or audio I O subsystems such as a voice message recording subsystem may also be implemented on the electronic device . Although voice or audio signal output may be accomplished primarily through the speaker the display may also be used to provide an indication of the identity of a calling party duration of a voice call or other voice call related information.

The processor operates under stored program control and executes software modules such as applications stored in memory such as persistent memory. The software modules may include operating system software one or more additional applications or modules and data . The processor may also operate to process the data stored in memory associated with the electronic device .

The infrared receiver receives infrared light from an external source. For example the infrared receiver can receive infrared light from the one or more LEDs associated with the target member . The infrared receiver receives the infrared light and can identify the relative location of the LEDs e.g. based on the brightness and location of the received infrared light . In one or more embodiments the infrared receiver may be able to identify or determine the three dimensional location of the received infrared light in relation to the GPU . Using infrared light received at an infrared receiver is one example of how a three dimensional location of the target member can be calculated or determined from orientation data.

The infrared receiver is an example of a module of the GPU that can receive orientation data from an external reference. By way of further example a gyroscope or an accelerometer or another sensor on the target member and or on the electronic device can be used to receive or obtain orientation data instead of or in addition to the infrared receiver . A sensor other than an infrared receiver can obtain the orientation data from an external reference such as the target member . The orientation data can be used to calculate or determine the orientation of the target member relative to the electronic device .

The control processor can receive the orientation data from the infrared receiver or from another module or sensor on the electronic device . The control processor uses the orientation data to identify a three dimensional plane as defined by the orientation data. The three dimensional plane is a planar region in three dimensions associated with the target member and its location or orientation may be relative to the electronic device . For example in the embodiment in which the LEDs are located on a target member the orientation data may include enough information to identify a three dimensional plane defined by the target member . For example in the case when the infrared receivers receive infrared signals from multiple sources e.g. multiple LEDs the orientation data can be used to define or determine points or coordinates in three dimensional space. The coordinates of the points can be compared to reference coordinates in order to determine a three dimensional plane defined by the points. The reference coordinates can be predefined or can be represented by a location of the points that define a flat two dimensional plane such as that shown in discussed below . The three dimensional plane can be identified by an orientation matrix calculated based on the orientation data. For example the three dimensional plane may be identified by the orientation matrix as being relative to a two dimensional surface e.g. the display . Thus the orientation matrix may be able to project or transform a coordinate pair on a two dimensional surface onto a coordinate pair on the three dimensional plane. Alternatively the three dimensional plane can be stored in a different format in memory that is accessible by the processor and or GPU . The three dimensional plane as it is defined and stored in memory can be such that the processor and or GPU can determine whether a coordinate pair is on the three dimensional plane.

In one or more alternative embodiments the infrared receiver captures raw orientation data from the LEDs on the target member . The control processor analyzes the raw orientation data as received at the infrared receiver to identify bright spots and to compute the coordinate pairs e.g. x y coordinates or x y z coordinates of the bright spots on a reference plane defined by the control processor . The reference plane can be a plane defined by the control processor such as the plane defined by the display . The control processer can access information e.g. pre stored in memory that describes the distance between each of the LEDs on the target member . For example the memory may indicate that two LEDs on the target member are separated by a distance of 3 inches. The distance between the coordinate pairs of the LEDs bright spots on the reference plane can be compared to the distance between the LEDs on the target member which was previously stored in memory or which was communicated to the electronic device by the target member to determine an angle of the three dimensional plane defined by the LEDs . Similarly the relative angle of the coordinate pairs of two bright spots on the reference plane can be compared to a predefined angle e.g. horizontal or parallel to the Earth s surface to determine an angle of the three dimensional plane defined by the target member or by the LEDs on the target member . Further the relative brightness of the light received at the infrared receiver can indicate which of the LEDs is closer to the electronic device which can indicate the direction of the angle of the three dimensional plane defined by the LEDs .

In one or more embodiments the target member can include one or more sensors such as a gyroscope and an accelerometer. Further the target member can be an electronic device and can include a processor and memory such as described above in relation to . The target member can determine its orientation using the one or more sensors and can transmit the determined orientation to the electronic device which can also receive infrared light from the LEDs at the infrared receiver . The electronic device can receive the orientation from the target member or alternatively the electronic device can receive the raw sensor data from the target member and can calculate the orientation of the target member using the transmitted raw sensor data. Either one or all of the calculated orientation or the raw sensor data may be considered orientation data. The electronic device can use the orientation data received at the infrared receiver from one or more LEDs along with the orientation data of the target member in order to determine the three dimensional plane defined by the surface of the target member . For example the light received at the infrared receiver two LEDs can define the relative position or angle of two points on the target member or on the planar region as described above but it may not be able to define the rotation of the target member about the line defined by the two points. The sensor data can be used to determine the rotation of the target member or planar region about the line defined by the two points. Thus the sensor data together with two LEDs can be used to identify a three dimensional plane defined by a line e.g. connecting the two LEDs and a rotation about that line as determined using the sensor data . The electronic device may also contain one or more sensors e.g. gyroscope accelerometer magnetometer etc. in order to determine its orientation. The orientation of the electronic device can be used to calculate the relative orientation of the target member . Using the relative orientation of the target member and the relative location of the target member e.g. as determined from the LEDs the electronic device can define the three dimensional planar region of the surface of the target member .

In one or more embodiments the target member can contain 4 LEDs that identify a three dimensional plane defined by the target member . When the lights from the LEDs are received at the infrared receiver and processed at the control processor the brightness and the relative positioning of the received LEDs define a three dimensional plane. The three dimensional plane can be relative to the electronic device .

In accordance with one or more embodiments the electronic device can re orient a coordinate pair from its display to be on the three dimensional plane defined by the target member by translating the coordinate pair following the new location of the target member e.g. as determined from the infrared receiver and by rotating the coordinate pair using the relative orientation of the target member e.g. as determined from the sensors on the electronic device and or the target member .

In one or more embodiments the electronic device can impose a finite shape or finite boundary on the three dimensional plane thereby defining a three dimensional planar region. For example the finite shape can correspond to the shape of the target member or may be adjacent to the target member . By way of further example the finite shape can be a rectangle ellipse or other shape.

In one or more embodiments the three dimensional plane can be stored in memory as an orientation matrix. The orientation matrix can transform a two dimensional coordinate pair on a surface onto the three dimensional plane. In one or more embodiments the coordinate pairs represented by the orientation matrix and or transform matrix can be homogeneous coordinates e.g. x y w or x y z w .

In the above example T T Trepresents a translation using the centre of the target member as the origin and Vto Vrepresents a rotation. Thus applying the above exemplary orientation matrix to a coordinate pair on a two dimensional surface results in a coordinate triplet on a three dimensional plane. As understood the coordinate pair on the two dimensional surface may be represented in homogeneous coordinates as X Y 0 W with the z coordinate equal to 0.

The orientation matrix or the representation of the three dimensional plane as stored in memory can be used to calculate a transform matrix. Or in accordance with one or more embodiments the transform matrix may include the orientation matrix such as the above exemplary matrix and a perspective projection matrix. The transform matrix may also be identified as an inverse transform matrix. The transform matrix includes a projection or inverse projection of a three dimensional planar region onto a two dimensional flat surface. The three dimensional planar region may be the portion of the three dimensional plane that has a finite boundary imposed on it i.e. the portion of the three dimensional plane that is defined by the finite shape imposed on it . The transform matrix e.g. including the orientation matrix can be applied to a set of display pixel coordinate pairs e.g. pixel coordinate pairs or locations on a display to determine a set of graphic pixel coordinate pairs e.g. pixel coordinate pairs on a two dimensional plane or two dimensional planar region . A coordinate pair can be a location of a pixel on a display screen for example. For example coordinate pairs may be stored as x y pairs. By way of further example the coordinate pairs may be stored as x y 0 triplets or as x y 0 w homogeneous coordinates. The display pixel coordinate pairs represent coordinate pairs on the display . The graphic pixel coordinate pairs represent coordinate pairs on one or more graphics stored in memory e.g. in a digital file . When the transform matrix is applied to a set of display pixel coordinate pairs the result is a set of coordinate pairs from the graphic i.e. a set of graphic pixel coordinate pairs that have been projected onto the three dimensional planar region that is defined from the three dimensional plane. The transform matrix can be a combination of one or more of a rotation matrix a scaling matrix and a perspective projection matrix which can be determined based on the difference between the three dimensional planar region associated with the three dimensional plane as defined by the orientation data and the two dimensional surface of the display screen . A three dimensional planar region that is associated with a three dimensional plane can be a three dimensional planar region that results from the three dimensional plane having a boundary imposed on it . For example the transform matrix may include the orientation matrix together with a perspective projection matrix projecting the three dimensional planar region onto a two dimensional surface e.g. the display .

The boundary imposed on the three dimensional plane may be determined from the orientation data or otherwise from the target member . For example the boundary may be a pre defined shape such as a rectangle or triangle with the corners of the target member identified by LEDs . In one or more embodiments the boundary is defined by a projection matrix included as part of the transform matrix. Thus the three dimensional planar region may be defined by the target member .

By way of further example the transform matrix may include the orientation matrix shown above with a projection matrix that has the following format 

Where Pto Pare perspective transformation elements. Thus an example of the application of the transform matrix including the orientation matrix and perspective projection matrix to a coordinate pair is as follows 

In accordance with one or more embodiments the initial coordinate pairs represent coordinates on the display and the final coordinate pairs after application of the transform matrix including the orientation matrix represent corresponding coordinate pairs of the graphic as projected onto the three dimensional planar region represented on the display .

By way of further example the transform matrix can transform the set of display pixel coordinate pairs to a coordinate system representative of the three dimensional plane associated with the target member e.g. as defined by the orientation data where an origin of the coordinate system could be a point on the target member such as its center . The transform matrix can also include a projection of the three dimensional planar region associated with the target member and associated with the three dimensional plane onto a graphic pixel coordinate system. The graphic pixel coordinate system can be the flat or two dimensional coordinate system on which graphics are stored. The transform matrix may also include one or more translations or position offset from the origin and scaling for example. The result of the application of the transform matrix is a set of graphic pixel coordinate pairs that correspond to the display pixel coordinate pairs projected onto the three dimensional planar region associated with the target member .

Accordingly in one or more embodiments the transform matrix may be calculated using the orientation data. The transform matrix can be applied to orient the set of display pixel coordinate pairs on the three dimensional plane associated with the orientation data and then to identify the set of graphic pixel coordinate pairs projected onto the three dimensional planar region that corresponds to the set of display pixel coordinate pairs on the three dimensional plane.

Reference herein to the application of the transform matrix may include the application of the orientation matrix along with a perspective projection matrix. It is understood that perspective projection matrices may have other forms as the above example is only for illustrative purposes.

In one or more embodiments the control processor is not contained in the GPU and is instead a separate component on the electronic device .

The warp unit is connected to the control processor memory and the blending module . The warp unit receives the transform matrix and a set of display pixel coordinate pairs from the control processor . For example the set of display pixel coordinate pairs received at the warp unit can be a row of pixel coordinate pairs such as the top row on the display . A row may also be referred to as a scan line. By way of further example the set of display pixel coordinate pairs received at the warp unit can be a single pixel coordinate pair on the display . The warp unit applies the transform matrix to the set of display pixel coordinate pairs to determine a set of graphic pixel coordinate pairs that are projected onto the three dimensional planar region associated with the orientation data. The warp unit can then retrieve the set of graphic pixel data associated with the graphic pixel coordinate pairs. The graphic pixel data can be RGB values for example. By way of further example the graphic pixel data for a specific graphic pixel coordinate pair can be the RGB value for the pixel located at the specific graphic pixel coordinate pair.

In accordance with one or more embodiments the GPU may include more than one warp unit . Each warp unit may be associated with a graphic element e.g. a component or feature of a graphic file or a separate graphic stored in memory e.g. a separate graphic file . Each separate warp unit can receive the same display pixel coordinate pairs. Each separate warp unit can receive a separate transform matrix. For example the control processor may calculate transform matrices for each warp unit . The transform matrixes are applied to the display pixel coordinate pairs at the warp unit . The transform matrices may be the same or may be different for each warp unit . For example a second warp unit can apply a second transform matrix associated with a second three dimensional planar region such that the image component or image associated with the second warp unit will be projected onto the planar region associated with that warp unit . By way of further example the second transform matrix can be applied to the set of display pixel coordinate pairs to determine a second set of graphic pixel coordinate pairs which can in turn be used to fetch or retrieve a second set of graphic pixel data. A second set of display pixel data based on the retrieved second set of graphic pixel data can then be determined for rendering on the display. After the transform matrix is applied to the set of display pixel coordinate pairs the warp unit fetches the set of graphic pixel data that is associated with the set of graphic pixel coordinate pairs and based on the set of graphic pixel data determines the display pixel data to be rendered on the display pixel coordinate pairs. Thus multiple warp units can be used to represent multiple three dimensional planar regions through the application of multiple transform matrices . The warp unit can also include or be in communication with a pixel fetch module that can retrieve a set of graphic pixel data from a graphic stored in memory with the set of graphic pixel data being associated with the set of graphic pixel coordinate pairs.

In one or more embodiments more than one transform matrix will be applied to each set of display pixel coordinate pairs. For example a first transform matrix can transform a two dimensional graphic to a specific location and a second transform matrix can translate scale and or rotate the graphic. In one or more embodiment the infrared receiver is external to the GPU the first transform matrix is calculated at the GPU and applied at the warp unit in the GPU as described herein and the second transform matrix can be calculated at the processor and applied at the warp unit .

The calculation of the transform matrix at the control processor may be performed in parallel with the calculation of the set of graphic pixel coordinate pairs performed at the warp unit . For example while the warp unit is calculating the set of graphic pixel coordinate pairs e.g. while the transform matrix is being applied to the set of display pixel coordinate pairs the control processor may be calculating a subsequent transform matrix based on further orientation data subsequently obtained using an external reference. By way of further example the orientation data may be obtained at predefined intervals. In yet a further example rate of sampling or obtaining the orientation data can be increased in proportion to the rate of change of the orientation data.

The blending module is connected to the one or more warp units . In one or more embodiments the blending unit is in communication with a pixel fetch module which may itself form part of the warp unit which can retrieve the set of graphic pixel data associated with the determined set of graphic pixel coordinate pairs. The blending module can receive the set of graphic pixel data associated with the set of graphic pixel coordinate pairs from the one or more warp units or from the pixel fetch module associated with the warp unit as the case may be . The blending module determines the set display pixel data based on the set of graphic pixel data. The set of display pixel data can be associated with the set of display pixel coordinate pairs. There may be more than one graphic pixel data in the set of graphic pixel data. The blending module determines the display pixel data associated with a display pixel coordinate pair based on set of graphic pixel data which may include multiple graphic pixel data. Thus the multiple graphic pixel data associated with a single display pixel coordinate pair may be combined at the blending module . For example the set graphic pixel data received from the one or more warp matrices may be associated with one or more graphic pixel coordinate pairs form one or more graphics stored in memory or one or more graphic components.

The rendering module is for rendering the set of data pixel data on the display . The rendering module may be connected to the blending module . In one or more embodiments the rendering module may be in communication with the warp unit and may form part of the blending module . In one or more embodiments the rendering module receives the set of display pixel data from the blending module . The rendering module may also receive the corresponding set of display pixel coordinate pairs from the blending module or from the warp unit . The rendering module may cause the each display pixel datum in the set of display pixel data to be rendered at its associated display pixel coordinate pair e.g. from the set of display pixel coordinate pairs .

The pixel clock generator is connected to the one or more warp units . The pixel clock generator dictates or instructs the warp units when to fetch then set of graphic pixel data from memory thereby synchronizing the fetching of graphic pixel data.

The operation of the transform matrix will now be explained in more detail. is a representation of a graphic stored in memory . The graphic in is a two dimensional image with coordinate pairs representing locations on the graphic in memory. is the same graphic shown projected onto a three dimensional planar region on the display . Thus the graphic on the display in may be considered a rendered image . The coordinate pairs shown in represent the coordinates on the display . The transform matrix can be applied to a pixel from the display such as that identified at in and performs a reverse projection of that pixel from the planar region to the two dimensional graphic plane in order to obtain the coordinate pair from the graphic in memory e.g. the graphic shown in which can be identified at coordinate pair .

The matrix multiplier module receives a transform matrix from the control processor . The matrix multiplier module can also receive the set of display pixel coordinate pairs from the control processor . The timing of the receipt of the display pixel coordinate pairs may be controlled by the pixel clock generator . The matrix multiplier module applies the transform matrix to the set of display pixel coordinate pairs in order to obtain a set of graphic pixel coordinate pairs. The set of graphic pixel coordinate pairs may be a set of x y coordinate pairs for example. Each x y coordinate pair in the set may identify an x y position on a graphic e.g. on a graphic in a digital file as in for example. For example the matrix multiplier can apply the transform matrix to the coordinate pair identified at in in order to obtain the coordinate pair identified at in .

The texture memory address calculation module receives the set of graphic pixel coordinate pairs from the matrix multiplier module and determines the address es in memory of the set graphic pixel data associated with the set of graphic pixel coordinate pairs. For example the set of graphic pixel coordinate pairs which can be x y coordinate pairs of an image can be used to identify locations of data in memory which correspond to the graphic pixel coordinate pairs. The data can be RGB values.

The pixel fetch module is in communication with the matrix multiplier module and memory . The pixel fetch module receives the address es in memory associated with the set of graphic pixel coordinate pairs from the texture memory address calculation module . The pixel fetch module retrieves from memory the set of graphic pixel data associated with the set of graphic pixel coordinate pairs. For example the pixel fetch module retrieves the RGB values located at the address es in memory received from the texture memory address calculation. After retrieving the set of graphic pixel data associated with the set of graphic pixel coordinate pairs from memory the pixel fetch module can transmit the graphic pixel data to the blending module . The warp unit or the pixel fetch module for example can also transmit the set of display pixel coordinate pairs to the blending module .

In one or more embodiments the warp unit transmits the set of display pixel coordinate pairs along with the set of graphic pixel data retrieved by the pixel fetch module to the blending module . The warp unit may also associate specific display pixel coordinate pairs with graphic pixel data retrieved by the pixel fetch module . For example each time the pixel fetch module retrieves graphic pixel data for a graphic pixel coordinate pair e.g. from the set of graphic pixel coordinate pairs the pixel fetch module may associate the graphic pixel data with the graphic pixel coordinate pair. The association between the graphic pixel coordinate pair and the graphic pixel data may be stored in memory. Thus the blending module may also receive an identification of the graphic pixel data that is associated with each specific display pixel coordinate pair in the set of display pixel coordinate pairs.

In one or more exemplary embodiments the graphic can be generated based on an electronic file stored in memory such as a file in .tiff .bmp or .jpg formats. By way of further example the graphic can be generated based on a component of an electronic image file. A component of an electronic file can be delineated in the electronic file itself. For example an element of text in a graphic file e.g. in an electronic file can be delineated in that graphic file as being a separate graphic component. The graphic can be generated from this separate graphic component. By way of further example the graphic to be generated can be based on separate graphic components of a video file stored in memory. By way of further example the graphic can be generated from data received over a network . For example the graphic can be an email message a photograph or another type of file received over a communication network such as the Internet. The generated graphic can be rendered on the display as the rendered image for example.

The method can be implemented on the GPU . The GPU can be associated with or connected to or integral with a member that contains or supports the display . For example the display can be the first display portion of the glasses and the member that contains the display can be the wearable glasses. The GPU can be contained in an arm of the wearable glasses for example. By way of further example the display can be a display screen on a mobile electronic device e.g. a smart phone or tablet computer and the GPU can be contained within the mobile electronic device.

At a set of display pixel coordinate pairs is received at the GPU . In accordance with an embodiment the set of display pixel coordinate pairs is a row or scan line of pixel coordinate pairs on the display . For example the set of display pixel coordinate pairs can be the top row of the display . In accordance with an embodiment the set of display pixel coordinate pairs comprises a single pixel coordinate pair.

At a transform matrix is applied to the set of display pixel coordinate pairs to obtain a set of graphic pixel coordinate pairs. For example the transform matrix can be multiplied with a pixel coordinate pair from the set of display pixel coordinate pairs to calculate a further pixel coordinate pair. When the transform matrix is multiplied with each pixel coordinate pair from the set of display pixel coordinate pairs the result is a further set of pixel coordinate pairs. The further set of pixel coordinate pairs is the set of graphic pixel coordinate pairs identifying a set of pixel coordinate pairs of graphic stored in memory . In other words the further set of pixel coordinate pairs represents coordinate pairs for graphic stored in memory. The sets of pixel coordinate pairs can be x y coordinates on a two dimensional plane for example. In one or more example one or more of the obtained graphic pixel coordinate pairs may not be an integer values and may instead include fractions or real numbers. In such a situation obtaining a set of graphic pixel coordinate pairs can include rounding up or rounding down the fraction or real numbers in order to obtain integer values if necessary.

The transform matrix can be calculated using orientation data received from an external reference. As described above the transform matrix can be calculated at the control processor . The orientation data can be received at the infrared receiver from the target member . The target member is an example of an external reference. Additional data can be transmitted from the external reference. For example orientation data representing the orientation of the external reference can be transmitted from the external reference to the electronic device . The orientation data can be determined at the external reference by one or more of a gyroscope accelerometer and magnetometer for example. The orientation data may also include relative orientation of the electronic device e.g. relative to the target member as determined by sensors e.g. a gyroscope accelerometer and or magnetometer on the electronic device . The infrared receiver can be on the wearable glasses or otherwise attached to the electronic device that supports or contains the display . The raw orientation data can include the raw infrared input received from the LEDs . The control processor can use the raw orientation data from the infrared receiver along with the orientation data from the sensors on the external reference and or on the electronic device to define a three dimensional planar region and or a three dimensional plane. For example the control processer can use the relative positioning of two or more infrared light signals received at the infrared receiver as compared to a reference relative positioning along with the orientation data from the sensors on the external reference and or on the electronic device to determine the transformation of the three dimensional plane between the reference positioning and the positioning defined by the received infrared light signals and orientation data. The reference positioning may be representative of a certain planar region such as the two dimensional planar region of the display or a flat surface on the display . By way of further example the reference positioning may be such that two coordinate pairs corresponding to the locations of the LEDs when the electronic device is in a reference position are on a horizontal plane relative to the display at a distance of 3 cm apart whereas the received infrared light signals may be at an angle of 25 degrees and 2 cm apart. The control processor or other component of the electronic device can also determine the rotation of the external reference or target member relative to the electronic device using the orientation data received at the sensors on the external reference and transmitted from the external reference as compared to the orientation of the electronic device as determined by the sensors on the electronic device . From this relative positioning and from the relative rotation information the control processor can calculate an orientation matrix to identify the three dimensional plane of the received infrared light signals and to identify the transformation between the reference positioning and the new positioning. The orientation matrix can therefore identify the three dimensional plane defined by the external reference relative to the electronic device . The control processor can then calculate a transform matrix based on the three dimensional planar region associated with the three dimensional plane as defined by the orientation data.

The transform matrix can be used to calculate the original location i.e. the coordinate pair s on the two dimensional or flat image file of input coordinate pairs on a display if the image is to be projected onto the three dimensional planar region associated with the orientation data and rendered on the display on the three dimensional planar region. The three dimensional planar region may be a finite region such as a rectangle or other shape rather than an infinite plane . If the transform matrix identifies pixel coordinate pairs that are not on the image then such pixel coordinate pairs will be returned as a null value. Null values for pixel coordinate pairs may be treated as void of any pixel data and as such will not render any data on the display . Thus for example if the display is the first display portion e.g. a lens on a pair of glasses then any null values for display pixel coordinate pairs will remain transparent. The application of the transform matrix to a set of display pixel coordinate pairs can include the application of the orientation matrix. For example the orientation matrix may be applied to the set of display pixel coordinate pairs to determine the location of the coordinate pairs on the three dimensional plane and then the transform matrix may be applied to project in a perspective projection the three dimensional coordinate pairs onto two dimensional graphic pixel coordinate pairs. It is recognized that that the resulting graphic pixel coordinate pairs may be non integers or fractional values. The non integer or fractional values can be rounded up or rounded down for example to obtain integer values for the graphic pixel coordinate pairs if necessary.

In one or more embodiments there are multiple transform matrices on multiple warp units . For example there may be multiple warp units in the GPU associated with multiple graphic elements or graphic files with different warp units receiving different transform matrices. Each warp unit and each transform matrix may represent different three dimensional planar regions or different transformations or projections of the image stored in memory . For example different transform matrices can be applied to different graphic elements in order to determine the coordinate pairs of each graphic element that will be projected to the three dimensional planar region represented by a transform matrix. Different warp units can apply the different transform matrices.

In accordance with an exemplary embodiment each warp unit can apply one or more additional custom matrix to the set of display pixel data and or the set of display pixel coordinate pairs. For example the custom matrix can alter the display pixel data in order to implement one or more effects onto the graphic such as altering the colour of the display pixel data and hence the graphic . The custom matrix may also be applied so as to alter the appearance of an graphic element. The custom matrix can be a predetermined custom matrix.

In accordance with one or more embodiments different warp units can apply the same transform matrices in parallel in addition to applying different custom matrices.

At a set of graphic pixel data associated with the set of graphic pixel coordinate pairs is retrieved. For example set of graphic pixel coordinate pairs can be a set of coordinate pairs e.g. x y coordinates for an graphic stored in memory . Each graphic pixel coordinate pair can have graphic pixel data associated with it. The graphic pixel data can be RGB values for example. The RGB values can represent the red green and blue colour components of the data rendered at the associated graphic pixel coordinate pair when the graphic is displayed. The displayed graphic e.g. the graphic rendered on the display may consist of a portion of or may be an entire rendered image . The set of graphic pixel data can therefore include all of the graphic pixel data that is associated with all of the graphic pixel coordinate pairs in the set of graphic pixel coordinate pairs.

The transform matrix applied to the set of display pixel coordinate pairs may identify certain graphic pixel coordinate pairs that are not on the three dimensional planar region on the display . In such a situation the graphic pixel coordinate pairs may be identified as null values and the corresponding graphic pixel data may be identified as null. A null graphic pixel coordinate pair may be rendered as a transparent pixel or may not be rendered at all for example. Thus if the display is the first display portion and the second display portion e.g. lenses of the wearable glasses then no RGB values will be superimposed over locations of null valued display pixel coordinate pairs.

At a set of display pixel data is determined based on the retrieved set of graphic pixel data for rendering on the display . In accordance with an embodiment the set of display pixel coordinate pairs comprise a single coordinate pair and the set of display pixel data comprise data associated with the single coordinate pair. In such a situation the display pixel data can be the same as the graphic pixel data. In accordance with another embodiment the set of graphic pixel data can comprise more than one graphic pixel datum. Each retrieved graphic pixel data point or datum can be associated with a display pixel coordinate pair. For example more than one graphic pixel data point can be associated with the same display pixel coordinate pair. In such a situation the multiple graphic pixel data are transmitted to the blending module where a single display pixel data point for rendering on a single display pixel coordinate pair is determined based on the multiple graphic pixel data points. In one or more embodiments a normal alpha blending mode can be used. In one or more embodiments a premultiplied alpha blending mode where the graphic already has all colour values multiplied with alpha values can be used. In one or more embodiments fragments or graphic pixel data are sorted by depth e.g. of the corresponding graphic pixel coordinate pair with the fragment furthest away from the display being first blended at the blending module .

In accordance with another embodiment the multiple graphic pixel data points associated with the display pixel coordinate pair can be retrieved from more than one graphic or from more than one graphic element of one or more graphics stored in memory .

After the graphic pixel data are blended thereby resulting in display pixel data for each coordinate pair in the set of display pixel coordinate pairs the display pixel data can be rendered on the display at the associated display pixel coordinate pair.

The method described in respect of can be repeated. For example the first set of display pixel coordinate pairs can be the top row of the display the second set of display pixel coordinate pairs can be the second from top row of the display the third set of display pixel coordinate pairs can be the third from top row of the display and so on. A new transform matrix can be calculated for each new display pixel coordinate pair. Similarly the new transform matrix can be calculated on newly obtained orientation data. The new transform matrix can be calculated while the previous set of display pixel data is being generated. In one or more embodiments the subsequent display pixel coordinate pairs can be exclusive of the previous display pixel coordinate pairs.

In one or more embodiments a further transform matrix can be applied to the set of display pixel coordinate pairs to obtain a further set of graphic pixel coordinate pairs. The further transform matrix can be calculated using orientation data received from the external reference e.g. LEDs on the target member . The further transform matrix can be associated with a second three dimensional planar region on the display. The second three dimensional planar region can be different from the three dimensional planar region. For example the further transform matrix can include a different perspective projection matrix from the transform matrix. The different perspective projection may be based on predetermined data such as a desired relative angle between the three dimensional planar region and the second three dimensional planar region . However in one or more embodiments the transform matrix and the further transform matrix may be based on or determined using the same orientation data. A further set of graphic pixel data associated with the further set of graphic pixel coordinate pairs can then be retrieved. For example the graphic pixel data can be retrieved from memory. A further set of display pixel data based on the retrieved further set of graphic pixel data can then be determined. This further display pixel data can be rendered on the display on the second three dimensional planar region and the display pixel data that was associated with the determined graphic pixel coordinate pairs can be rendered on the three dimensional planar region. Thus the display pixel data and the further display pixel data can be rendered on different three dimensional planar regions.

In accordance with an exemplary embodiment depicts a method implemented on a graphics processing unit of generating display pixel data for rendering a representation of a graphic on a display that is performed after the method of .

At a subsequent set of display pixel coordinate pairs is received after retrieving the set of graphic pixel data associated with the set of graphic pixel coordinate pairs. For example the set of display pixel coordinate pairs can be a row of coordinate pairs on the display and the subsequent set of display pixel coordinate pairs can be the subsequent row of coordinate pairs on the display .

At a subsequent transform matrix is applied to the subsequent set of display pixel coordinate pairs to obtain subsequent set of graphic pixel coordinate pairs. The subsequent transform matrix can be calculated using further orientation data. The further orientation data can be received from an external reference such as the target member . The further orientation data can be received or obtained more recently than the orientation data that was used to calculate the previous transform matrix. For example after the initial graphic pixel data is retrieved e.g. for rendering on the first row of the display then further orientation data can be obtained to calculate a new e.g. further or subsequent transform matrix. In one or more embodiments the further orientation data may indicate that the orientation or positioning of the target member relative to the electronic device is different than indicated by the orientation data previously obtained e.g. as obtained for the first row . In such a situation the subsequent transform matrix would represent a different projection of a graphic or the subsequent transform matrix would be associated with a different three dimensional planar region. This situation could occur for example if the glasses were in motion relative to the target member .

At the subsequent set of graphic pixel data associated with the subsequent set of graphic pixel coordinate pairs is retrieved. For example the subsequent set of graphic pixel data may be from a graphic or a graphic element stored in memory .

At a subsequent set of display pixel data is determined based on the retrieved subsequent set of graphic pixel data for rendering on the display . This may be performed using the blending module as described above.

In one or more embodiments the graphics processing unit is associated with wearable glasses and the display can include a first display portion viewable by a first eye of a wearer of the glasses and a second display portion viewable by a second eye of the wearer of the glasses. For example the first display portion can be a left lens and the second display portion can be the right lens. The display can be viewable from both the first display portion and the second display portion thereby providing a stereoscopic view of the rendered image .

At a second transform matrix is applied to the set of display pixel coordinate pairs to obtain a second set of graphic pixel coordinate pairs. The second transform matrix is representative of the view from the second display portion e.g. left lens of the glasses . The second transform matrix can be calculated using orientation data received from an external reference and orientation data obtained from sensors on the electronic device . For example the external reference is the target member that has a plurality of LEDs . The second transform matrix may include a different perspective projection matrix from the transform matrix for example.

At a second set of graphic pixel data associated with the second set of graphic pixel coordinate pairs is retrieved. For example the second set of graphic pixel data can be retrieved from a graphic or graphic element stored in memory .

At a second set of display pixel data is determined based on the retrieved second set of graphic pixel data. The second set of display pixel data can be calculated using the blending module for example.

In one or more embodiments the second set of display pixel data can be rendered on the second display portion of the glasses e.g. the second lens .

In accordance with an embodiment and with reference to the method described in the warp unit used to apply the second transform matrix to the set display pixel coordinate pairs is different from the warp unit used to apply the transform matrix to the set of display pixel coordinate pairs. In order to achieve the stereoscopic effect the second transform matrix represents a different three dimensional planar region from the transform matrix.

While the present disclosure is primarily described in terms of methods a person of ordinary skill in the art will understand that the present disclosure is also directed to various apparatus such as a handheld electronic device including components for performing at least some of the aspects and features of the described methods be it by way of hardware components software or any combination of the two or in any other manner. Moreover an article of manufacture for use with the apparatus such as a pre recorded storage device or other similar computer readable storage medium including program instructions recorded thereon which may for example cause a processor to perform one or more of the methods described herein or a computer data signal carrying computer readable program instructions may direct an apparatus to facilitate the practice of the described methods. It is understood that such apparatus articles of manufacture and computer data signals also come within the scope of the present disclosure.

The term computer readable storage medium as used herein means any medium which can store instructions for use by or execution by a computer or other computing device including but not limited to a portable computer diskette a hard disk drive HDD a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or flash memory an optical disc such as a Compact Disc CD Digital Versatile Video Disc DVD or Blu ray Disc and a solid state storage device e.g. NAND flash or synchronous dynamic RAM SDRAM .

The embodiments of the present disclosure described above are intended to be examples only. Those of skill in the art may effect alterations modifications and variations to the particular embodiments without departing from the intended scope of the present disclosure. In particular features from one or more of the above described embodiments may be selected to create alternate embodiments comprised of a sub combination of features which may not be explicitly described above. In addition features from one or more of the above described embodiments may be selected and combined to create alternate embodiments comprised of a combination of features which may not be explicitly described above. Features suitable for such combinations and sub combinations would be readily apparent to persons skilled in the art upon review of the present disclosure as a whole. The subject matter described herein and in the recited claims intends to cover and embrace all suitable changes in technology.

