---

title: Methods and nodes for enabling and producing input to an application
abstract: Methods and nodes for enabling and producing input generated by speech of a user, to an application. When the application has been activated (), an application node () detects () a current context of the user and selects (), from a set of predefined contexts (), a predefined context that matches the detected current context. The application node () then provides () keywords associated with the selected predefined context to a speech recognition node (). When receiving () speech from the user, the speech recognition node () is able to recognize () any of the keyword in the speech. The recognized keyword is then used () as input to the application.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09576572&OS=09576572&RS=09576572
owner: Telefonaktiebolaget LM Ericsson (Publ)
number: 09576572
owner_city: Stockholm
owner_country: SE
publication_date: 20120618
---
This application is a 35 U.S.C. 371 National Phase Entry Application from PCT SE2012 050672 Jun. 18 2012 the disclosure of which is incorporated by reference.

The present disclosure relates generally to methods and an application node and a speech recognition node for enabling and producing speech generated input to an application in a communication network.

In the field of telecommunication speech recognition is sometimes employed in various communication services meaning that a user is able to speak voice commands into a User Equipment UE for controlling some functionality therein or in a communication network rather than entering written commands and pressing buttons on a keyboard or the like. In some applications a speech recognition function in the UE or in the network is able to translate the entered voice command into a text such as a recognizable message or just a single word. A spoken voice command in the UE may also be sent in digitally encoded form to a speech recognition entity where the actual speech recognition is executed by analyzing and translating the speech into corresponding text. Recently speech recognition has been applied for smart phones e.g. the speech based service called Siri developed for Apple iPhones.

Possibly the entity may also utilize a function referred to as Artificial Intelligence AI to make a more or less elaborated interpretation of the spoken command as shown by a schematic action . In that case the AI function basically deduces the meaning of a spoken question or command once it has been converted to text by the speech recognition . As a result the speech recognition entity may issue a control message or command corresponding to the entered speech as shown in an action which somehow controls or otherwise interacts with a service function or apparatus . The service function or apparatus may then process the control message and operate accordingly such as providing a suitable response back to the UE as shown by a final action .

In general the speech recognition services known today include two parts the actual speech recognition and the interpretation thereof e.g. by means of an AI function or the like. In different typical implementations both of these parts may reside in the UE or partly or completely in nodes of the network. In the above mentioned service Siri for iPhones a simplified speech analysis and AI analysis is made by the phone which in parallel may send the speech in text form to an AI function in the network for obtaining a more advanced analysis and creation of a suitable response or other action.

Voice controlled applications are configured to operate according to different received speech input as commands or queries e.g. an electronic game application implemented in a game server in the network which may receive various spoken lines from game participants for controlling the ongoing game. One or more words in a received speech input are typically significant for the command or query and are therefore often called keywords in this field. The one or more keywords in a received speech input must therefore be recognized such that the application is able to act and operate upon the speech input in a proper manner. To support this process some kind of automatic speech analysis of the speech input needs to be made.

Computer implemented speech analysis may be executed according to some different techniques. A first example is generally referred to as speech recognition where all speech received in audio form is translated word by word into a text version of the entire speech input thus comprising a chain of words. It is then easy for a computer to identify any keywords occurring in the text.

A second example is referred to as keyword spotting which does not require translation of the entire speech input into text but the audio is searched only for specific words or phrases by recognizing their sound more or less and then translating them into text. In general keyword spotting requires less computing than speech recognition since only a limited word or phrase must be recognized for translation instead of an entire vocabulary.

A third example is referred to as phonetic based search which is similar to keyword spotting in that only certain words are searched and identified in the speech input although it does not require converting the speech input into text. In phonetic based search the process is divided into separate indexing and searching stages. In the indexing stage the speech input is indexed to produce a phonetic search track which is a phonetic representation of the speech rather than words in text form. Once the indexing has been completed the searching stage includes searching for a keyword in the form of phoneme i.e. sound based sequences in the phonetic search track.

Even though certain significant keywords can be recognized and identified in a received speech input e.g. using any of the above techniques some applications may need to act and operate upon received keywords in different ways depending on the current situation. For example a command may need certain actions when coming from one user and other actions when coming from another user. Further some keywords may be significant for the application to act upon in one situation while other keywords may be significant for the application in another situation. It is thus a problem in currently known solutions that the use of keywords in speech input for controlling applications is somewhat static or inflexible and not adaptable to different situations.

It is an object of embodiments described herein to address at least some of the problems and issues outlined above. It is possible to achieve these objects and others by using methods and nodes as defined in the attached independent claims.

According to one aspect a method is provided in an application node for enabling input to an application from speech made by a user. In this method the application node detects a current context of the user when the application has been activated and selects a predefined context that matches the detected current context. The application node then provides at least one keyword associated with the selected predefined context to a speech recognition node thereby enabling the speech recognition node to recognize any of the at least one keyword in the speech when made by the user. The recognized keyword is used as input to the application i.e. as a result of receiving the speech.

Thereby speech recognition can be employed in a flexible and efficient way by adapting the choice of valid keywords for use as input to an application in dependence of the current context of the user. It is also an advantage that the solution thus enables dynamic use of keywords in speech input for controlling applications which is adaptable to different situations.

According to another aspect an application node is provided which is configured to enable input from speech made by a user to an application. The application node comprises a detecting unit adapted to detect a current context of the user when the application has been activated and a selecting unit adapted to select a predefined context that matches the detected current context. The application node also comprises a providing unit adapted to provide at least one keyword associated with the selected predefined context to a speech recognition node thereby enabling the speech recognition node to recognize any of the at least one keyword in the speech wherein the recognized keyword is used as input to the application.

According to another aspect a method is provided in a speech recognition node for producing input to an application from speech made by a user. In this method the speech recognition node receives from an application node at least one keyword associated with a predefined context that matches a current context of the user. When receiving the speech made by the user the speech recognition node recognizes in the received speech a keyword out of the received at least one keyword and uses the recognized keyword as input to the application.

According to another aspect a speech recognition node is provided which is configured to produce input to an application from speech made by a user. The speech recognition node comprises a first receiving unit adapted to receive from an application node at least one keyword associated with a predefined context that matches a current context of the user. The speech recognition node also comprises a second receiving unit adapted to receive the speech made by the user a logic unit adapted to recognize in the received speech a keyword out of the received at least one keyword and a usage unit adapted to use the recognized keyword as input to the application.

Further possible features and benefits of this solution will become apparent from the detailed description below.

Briefly described a solution is provided for making the use of speech from a user as input to an application more flexible and adaptable to different situations by enabling recognition of keywords in the speech depending on the current context of the user. In this solution one or more predefined contexts and associated keywords have been configured in an application node for an application such that each predefined context is associated with a certain set of keywords which thus may at least partly vary between different contexts. When a current context of the user is detected which context is characterized by certain context parameters a predefined context is selected having context parameters that best matches the detected context and the keywords that are associated with the selected context are then valid as input to the application. Thus when any of the keywords of the selected context is recognized in speech from the user it is used as input to the application. For example when recognized in speech from the user the keywords of the selected context may be used as commands information or other input for controlling the application in some way.

The solution will now be explained further with reference to an example shown in the block diagram of which illustrates how an application can be controlled by means of speech made by a user. This scenario involves an application node in which a set of predefined contexts and associated keywords have been configured for the application such that one or more specific keywords are valid for each predefined context. This means basically that those keywords are valid and useful as input to the application whenever the context of those keywords has been detected to prevail for the user. Any number of such predefined contexts and associated keywords may have been configured in the application node for one or more applications including e.g. just a single context with one or more keywords valid for an application.

As indicated above each context can be described or defined in terms of context parameters. To mention some non limiting examples of context parameters in the predefined contexts a context may pertain to a current location of the user such as geographical location urban or rural indoor or outdoor etc. A predefined context may further pertain to a current status of the user characteristics of the user or the identity or role of the user. For example in an electronic game application the user s role in the game may dictate what keywords are valid or not as input to the application. Further a certain identity or role or a current high status may authorize the user to control the game in a certain manner such as add or remove game participants by means of certain keyword commands which may not be possible when having another role or a low status. A predefined context may further pertain to any of the language spoken by the user the type or current status of the activated application a current time and a current environment of the user.

The scenario of also involves a speech recognition node which is utilized to recognize any of the valid keywords in speech made by the user into a UE which is connected to a schematically shown communication network . The UE operated by the user may without limitation be a telephone computer smartphone or any other communication device capable of sending speech in audio form e.g. to the speech recognition node . In this description the term in audio form should be understood such that the speech is represented as digitized audio which is a well known technique. The application to be controlled by speech based input further resides in the application node in this example although it may alternatively or additionally reside in the UE or in any other node not shown which may be responsive or susceptible to valid keywords in the speech. It is also possible that the application node is integrated to coincide with the UE of which an example is shown in to be described later below.

A first action in illustrates that the application is activated e.g. by manual input such as when the user presses a button or the like e.g. on a computer game console control panel or similar. Activating the application may include registering or detecting the user as a controlling party which means that the user is basically authorized to give input to the application. For example the application may be configured to receive input from one or more specific users or from any user without requiring authorization depending on how the application has been designed.

In a next action the application node detects a current context of the user schematically illustrated by a dashed arrow to the UE of the user. For example the current context of the user may be defined by any of the above mentioned examples of context parameters that may be included in a predefined context. Context information about the user may be obtained from the UE and or from the network and or from various sensors associated with the user and his UE . Another action illustrates that the application node selects a predefined context here denoted A out of the set of predefined contexts that matches or corresponds to the detected current context by having one or more of the above described context parameters in common. This action may be performed by comparing the detected context with each predefined context and determining how much of the context parameters they have in common. The predefined context that best matches the detected one is thus selected in this action.

The keywords that are associated with the selected predefined context A are then provided to the speech recognition node in an action as a basis for speech recognition of speech made by the user. The same set of keywords A may also be provided to the UE shown by an optional action . This enables the speech recognition node and optionally also the UE to recognize and identify any of the received keywords in speech made by the user and any other words or phrases in the speech can basically be ignored at least with regard to the application. This will facilitate the process of speech recognition in the node and also make recognition of valid input to the application more accurate and effective.

Another action illustrates that the speech recognition node receives speech from the UE which speech has been uttered by the user into the UE e.g. when having activated a speech input function or the like. The speech recognition node then recognizes in the received speech in an action at least one of the keywords that were received in action above. The keyword may be recognized by using any of the above described techniques of speech recognition keyword spotting and phonetic based search although the solution is not limited to any particular procedure for recognizing specific words. More than one of the previously received keywords may be recognized in the speech in this action although only one is mentioned and discussed here for simplicity.

A final action illustrates that the speech recognition node somehow uses the recognized keyword as input to the application which may include that the recognized keyword is sent to at least one of the application in the node and the UE . As mentioned above the application may reside in one or more of the application node the UE and another node not shown here which may thus be responsive or susceptible to valid keywords when recognized in the speech. The speech recognition node may further translate the recognized keyword into a corresponding command used for controlling the application.

The flow chart of comprises actions performed by an application node configured according to this solution for enabling input to an application from speech made by a user. The application node may act basically as the application node in . Further the flow chart of is basically a continuation of and comprises actions performed by a speech recognition node configured according to this solution for producing input to an application from speech made by a user. The speech recognition node may act basically as the speech recognition node in . It is assumed that the application node is used to support the use of a speech controlled application which may be implemented in one or more of the application node itself a UE operated by the user and some other node than the above nodes. In a further example the application node may be implemented as a part in the UE.

An optional first action illustrates that a set of predefined contexts and associated keywords are configured in the application node for the application in preparation for the procedure to follow. This action may be performed in response to manual input from the user or an administrator of the application depending on the implementation.

When the application has been activated in some way the application node detects a current context of the user in an action basically corresponding to action in . In a further action the application node selects a predefined context that matches i.e. corresponds to the detected current context basically corresponding to action in . As described above the detected current context may be compared with all predefined contexts configured in action to determine which one of the latter best matches the current context e.g. by comparing various context parameters in the predefined contexts with those of the current context and selecting the context having the most context parameters that match the current context. In another action the application node provides at least one keyword associated with the selected predefined context to the speech recognition node basically corresponding to action in .

An action on the speech recognition node side illustrates that this node receives the at least one keyword from the application node which is are thus valid as input to the application under the current context of the user. At some point later the speech recognition node receives speech made by the user e.g. from a UE operated by the user as shown in an action basically corresponding to action in . The speech recognition node then recognizes in the received speech a keyword out of the received at least one keyword in another action basically corresponding to action in . It should be noted that this action does not exclude that more than one keyword is recognized in the speech. The recognized keyword is thus valid and can be taken as input to the application under the detected current context of the user. Finally the speech recognition node uses the recognized keyword as input to the application in another action basically corresponding to action in e.g. by sending the keyword as a command or other input to at least one of the application and the UE.

The above described procedure may be modified in different ways such as outlined below. For example the UE itself may have a speech recognition function that can be used to perform an initial rudimentary analysis which is sent to the speech recognition node which then makes a more advanced final determination of whether a valid keyword was present in the speech. To reduce computational cost the rudimentary analysis carried out by the UE may include a speech recognition using a limited dictionary or using a small scale artificial neural network classifier or a light weight version of any other algorithm used e.g. in the case of phonetic search.

In actions and the keywords are sent over a communication channel between the application node and the speech recognition node This channel may be a special purpose channel e.g. a bi directional Hyper Text Transfer Protocol HTTP eXtensible Messaging and Presence Protocol XMPP or Session Initiation Protocol SIP which channel is used exclusively for exchanging the keywords. Alternatively the keywords can be sent over a multi purpose channel e.g. a channel which is used for exchanging other information related to the application such as a game. If the UE and the speech recognition node use phonetic search they need to translate the keywords from text into their phonetic representation for instance using a pronunciation dictionary.

A more detailed example of implementing the solution in practice will now be described with reference to the block diagram in . In the figure the user operates a UE and an online gaming application is assumed. The online game is run on a game server located in the public Internet the server thus effectively acting as the application node in this example. A set of predefined contexts with associated keywords have been configured in the game server .

Further an IP Multimedia Subsystem IMS network is used to control voice communication within the game by means of a Media Resource Function MRF comprising an MRF Processor MRFP and an MRF Controller MRFC . The IMS network also provides an Automatic Speech Recognition ASR service for the game in the form of an ASR server comprising an ASR processor and an ASR controller . The ASR controller is a control plane node whereas the ASR processor is a media plane node. The ASR server thus effectively acts as the speech recognition node in this example.

It should be noted that the HTTP REST interfaces in the example above could also be interfaces using a different protocol e.g. XMPP SIP or any other suitable protocol.

A detailed but non limiting example of how an application node and a speech recognition node can be configured to accomplish the above described solution is illustrated by the block diagram in . The application node is configured to enable input from speech made by a user in a UE to an application while the speech recognition node is configured to produce input to an application from speech made by a user e.g. according to the procedures described above for any of respectively.

The application node and the speech recognition node will now be described one by one in terms of a possible example of employing the solution. It should be noted that the application node described here could be implemented within the UE used by the user e.g. as shown in or as a node separate from the UE e.g. as shown in and the solution is thus not limited in this respect. Although shown as a separate entity in this figure the application may be implemented together with the application node and or in the UE and or in some other node not shown.

The application node comprises a detecting unit adapted to detect a current context of the user when the application has been activated and a selecting unit adapted to select a predefined context that matches the detected current context. It may be assumed that a set of predefined context and associated keywords have already been configured in the application node where at least one valid keyword is associated with each predefined context . It is also possible that just one predefined context and at least one associated keyword have been configured in the application node .

The application node also comprises a providing unit adapted to provide at least one keyword associated with the selected predefined context to the speech recognition node . Thereby the speech recognition node is enabled to recognize any of the at least one keyword in the speech made by the user wherein the recognized keyword is used as input to the application e.g. as a command or the like for controlling the application.

The above application node and its functional units may be configured or adapted to operate according to various optional embodiments. In a possible embodiment the selecting unit may be further adapted to select the predefined context from the set of predefined contexts and associated keywords configured in the application node for the application. As also exemplified above the predefined contexts may pertain to at least one of current location of the user current status of the user characteristics of the user identity or role of the user language spoken by the user type or current status of the activated application current time and current environment of the user.

In another embodiment the providing unit may be further adapted to provide the at least one keyword associated with the selected predefined context to the user s UE thereby enabling the UE to translate any of the at least one keyword when occurring in the user s speech into a command as input to the application .

The speech recognition node comprises a first receiving unit adapted to receive from the application node at least one keyword associated with a predefined context that matches a current context of the user and a second receiving unit adapted to receive the speech made by the user. The speech recognition node also comprises a logic unit adapted to recognize in the received speech a keyword out of the received at least one keyword and a usage unit adapted to use the recognized keyword as input to the application .

The above speech recognition node and its functional units may also be configured or adapted to operate according to various optional embodiments. In a possible embodiment the usage unit may be further adapted to use the recognized keyword by sending the keyword to at least one of the application and the UE . Further the logic unit may be further adapted to recognize the keyword by using any of speech recognition keyword spotting and phonetic based search. The logic unit may also be further adapted to translate the recognized keyword into a command used for controlling the application .

It should be noted that illustrates various functional units in the application node and the speech recognition node in a logical sense and the skilled person is able to implement these functional units in practice using suitable software and hardware means. Thus this aspect of the solution is generally not limited to the shown structures of the application node and the speech recognition node and the functional units and may be configured to operate according to any of the features described in this disclosure where appropriate.

The functional units and described above can be implemented in the application node and the speech recognition node respectively by means of program modules of a respective computer program comprising code means which when run by processors P cause the application node and the speech recognition node to perform the above described actions. Each processor P may comprise a single Central Processing Unit CPU or could comprise two or more processing units. For example each processor P may include general purpose microprocessors instruction set processors and or related chips sets and or special purpose microprocessors such as Application Specific Integrated Circuits ASICs . Each processor P may also comprise a storage for caching purposes.

Each computer program may be carried by a computer program product M in the application node and the speech recognition node respectively in the form of a memory having a computer readable medium and being connected to the processor P. Each computer program product M or memory thus comprises a computer readable medium on which the computer program is stored e.g. in the form of computer program modules m . For example the memory M may be a flash memory a Random Access Memory RAM a Read Only Memory ROM or an Electrically Erasable Programmable ROM EEPROM and the program modules m could in alternative embodiments be distributed on different computer program products in the form of memories within the application node and the speech recognition node respectively.

By using any of the above described embodiments of the solution it is possible to employ speech recognition in a more flexible and efficient way by adapting the choice of valid keywords in dependence of the current context of the user for use as input to an application. It is an advantage that the solution thus enables dynamic use of keywords in speech input for controlling applications which is adaptable to different situations. No particular functionality is required in the user s terminal and no extra efforts are needed from the user either once the predefined contexts and associated keywords have been configured in the application node. Also the same speech recognition functionality can be used for different applications and for users in different contexts.

While the solution has been described with reference to specific exemplary embodiments the description is generally only intended to illustrate the inventive concept and should not be taken as limiting the scope of the solution. For example the terms application node speech recognition node keyword context application and context parameter have been used throughout this description although any other corresponding entities functions and or parameters could also be used having the features and characteristics described here. The solution is defined by the appended claims.

