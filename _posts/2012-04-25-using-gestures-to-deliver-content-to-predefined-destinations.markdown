---

title: Using gestures to deliver content to predefined destinations
abstract: Disclosed are various embodiments for using gestures to deliver content to predefined destinations. After a user designates a content item, a flick gesture is detected, where the flick gesture has a point of origin and crosses a trip line surrounding the point of origin. A destination associated with the flick gesture is identified from a plurality of predefined destinations. Each predefined destination is associated with a region in a predefined plurality of display regions. The content item is delivered to the determined destination.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09507512&OS=09507512&RS=09507512
owner: AMAZON TECHNOLOGIES, INC.
number: 09507512
owner_city: Seattle
owner_country: US
publication_date: 20120425
---
Many computing devices now include a gesture user interface that allows the user to interact with applications executing on the device through gestures. Gesturing can be particularly useful when the size of the device would make a keyboard inconvenient or impractical.

Various embodiments described herein relate to the use of gesturing to simplify the delivery of content. The embodiments may be implemented on a mobile or handheld computing device with a touchscreen although other types of computing devices displays and input devices may also be utilized. In some embodiments regions or areas of the touchscreen are associated with predefined destinations and a type of gesture referred to herein as a flick gesture is used to deliver selected items of content to one of these destinations. These destinations may include folders on the device email addresses social networking sites other computing devices and as any other type of destination which the computing device can access. The selected items may include files documents images video audio web pages messages and other types of content.

The flick gesture itself specifies the destination for the selected content. As one example flicking toward the left side of the display may create an email message to a particular contact and insert the selected image as an attachment. As another example flicking toward the right side of the display may post selected text to a Twitter account.

In other embodiments the display regions or areas are associated with different clipboards. Content items can be selected then aggregated or accumulated into a particular clipboard using a flick gesture. For example flicking toward the top edge of the display may add the selected item to a family clipboard while flicking toward the bottom edge of the display may add the selected item to a my blog clipboard. After one or more items have been aggregated the user can interact with application s to set up delivery of the aggregated content and then perform an insert gesture which instructs the application s to deliver the aggregated content. The insert gesture may be for example an off screen swipe from one of the edges toward an application.

As one example of this aggregation operation the user may navigate using the browser to a page on a photo sharing site that is visible to family members. The user may then perform an off screen swipe from the bottom clipboard region. The aggregated photos from the family clipboard are then posted to the photo sharing site on the family page. The user may then navigate using the browser to a page for creating a new blog post perform an off screen swipe from the top clipboard region. The aggregated files from the my blog folder are then copied into the new blog post.

Though specific examples of content destinations and gestures are discussed herein it will be understood that other types may be used as well. In the following discussion a general description of the system and its components is provided followed by a discussion of the operation of the same.

With reference to shown is a networked environment according to various embodiments. The networked environment includes one or more computing devices in data communication with one or more computing devices by way of a network . The network includes for example the Internet intranets extranets wide area networks WANs local area networks LANs wired networks wireless networks or other suitable networks or any combination of two or more such networks.

The computing device may comprise for example a processor based system such as a computer system. Such a computer system may be embodied in the form of a mobile phone a web pad a tablet computer system a notebook computer a personal digital assistant a game console an electronic book reader a set top box a television a music player a video player a media player a desktop computer or other devices with like capability. The computing device includes a display and a gesture input device . The display may take the form of a cathode ray tube display a liquid crystal display LCD screens a gas plasma based flat panel display or other types of display.

The gesture input device allows a user to input gestures such as drags drops taps tap and holds swipes slides scrolls flicks pinches etc. In some embodiments the gesture input device includes a motion sensing capability which allows the user to interact with and manipulate items on the screen via gesture recognition and pointing. The gesture input device may for example use an accelerometer and or optical sensors.

In some embodiments a touchscreen may combine the functions of the display and the gesture input device . The touchscreen may use for example capacitive technology resistive technology or some other technology. The computing device may also include other input devices such as a keyboard touch pad touch stick mouse joystick game controller button etc.

In some embodiments the computing device may incorporate three dimensional and or haptic technologies. For example the display may be a stereoscopic display a three dimensional composite display a holographic projection display etc. The gesture input device may be a haptic device having an output section which provides feedback to the user. Examples of such haptic feedback include vibration devices buzzing devices audio devices indicator lights seven segment display devices and so on.

The computing device may be configured to execute one or more applications an operating system a gesture based user interface and various other components. The applications may include but are not limited to productivity tools e.g. email calendar contact databases word processing and so on browsers media players games location based applications and electronic commerce applications. When the computing device is a mobile device such applications may be referred to as apps. The applications may access the network and may interoperate with other applications and or with computing devices . The applications may be downloadable from a computing device via the network . The applications may be purchased or available for free.

The gesture based user interface allows a user to interact with the applications through gestures such as drags drops taps tap and holds swipes slides scrolls flicks pinches etc. Multi touch gestures may also be supported. The user makes these gestures using the gesture input device . The gesture based user interface supports a flick gesture which allows a user to select an item of content e.g. file image etc. and deliver the selected item to one of a set of predefined destinations each associated with a region of the display . In other words the user can select an item and flick toward a region of the display and the gesture based user interface delivers the selected item to a destination that is associated with that region. Various types of destinations and delivery may be supported for example message delivery to email or instant message contacts saving of files to a folder stored on the computing device posting the item to a social networking site sending the item to another computing device and so on.

The gesture based user interface may make use of services provided by the operating system which may in turn use one or more device drivers to interface with the gesture input device and the display . For example the operating system may notify the gesture based user interface of touchscreen events mouse events keyboard events etc. The operating system may also provide services which allow components such as the gesture based user interface to render or draw on the display .

The computing device may also comprise a data store which stores various types of data including those used by the gesture based user interface . The data store includes for example gesture mapping data gesture definition data and configuration data . The gesture based user interface and or the operating system use the gesture definition data to detect various gestures and to distinguish between them e.g. a swipe vs. a scroll a slide vs. a flick etc. . The gesture based user interface uses the gesture mapping data to map between regions of the display and various destinations and or clipboards. The configuration data stores various settings which control the behavior of the gesture based user interface .

Having discussed the computing device the computing device will now be described. The computing device may comprise for example a server computer or any other system providing computing capability. Various applications and or other functionality may be executed in the computing device according to various embodiments. The computing device may also include a data store which stores various types of data for example user behavior data . The components executed on the computing device may include a network page server which may be implemented as a commercially available hypertext transfer protocol HTTP server such as for example Apache HTTP Server Microsoft Internet Information Services IIS and other servers. In some embodiments the components executed on the computing device may also include a gesture data collector .

In such embodiments the gesture data collector operates to collect data from the gesture based user interface that is executing on the computing device . As the user interacts with the user interface information about the user s behavior is provided to the collector . For example the collector may obtain data describing the flick gestures made by the user the destinations and or clipboards with which the user interacts the applications with which the user interacts and the network sites with which the user interacts. The collector may process this data to make recommendations as to which destinations clipboards and or applications the user may wish to target with a flick gesture. The collector may recommend based on the aggregate behavior data collected from multiple users. For example if multiple users are visiting a particular social networking site then collector may communicate that social networking site to the gesture based user interface along with a recommendation that the site be associated with the flick gesture destination. The collector may make a recommendation for a user based solely on the behavior of that particular user and in such embodiments the collector and user behavior data may be stored locally at the computing device rather than at the network accessible computing device .

A general description of the operation of the various components of the networked environment is provided. Each of depicts a user interface shown on the display of the computing device . As will be described herein the gesture based user interface allows the user to designate a particular content item to be sent to one of a set of predefined destinations such as folders contacts network sites etc. and then to indicate the particular destination by means of a flick gesture.

The gesture based user interface may provide a configuration option which allows the user to specify these destinations and the association between destinations and regions of the display . For example the user may define a destination in terms of an application e.g. file explorer email browser etc. and or a location e.g. folder email contact network site etc. . In some embodiments the user may view the destinations in a hierarchy e.g. Contacts Social Networking Sites Folders and Devices . Each of these top level groups may have groups underneath e.g. Twitter Facebook StumbleUpon etc. with and multiple levels may be supported e.g. Jane s Facebook Bob s Facebook etc. . The gesture based user interface may provide a configuration interface which allows the user to specify these various configuration options which are then stored as configuration data .

The gesturing described herein provides a simple mechanism for a user to designate a content item to identify a destination that corresponds to a display region and to easily deliver the designated content item to the destination. The user interfaces shown in illustrate an example of this process of designating a content item identifying a destination for the content item and causing the gesture based user interface to send the designated content item to the identified destination.

As shown in a user interacts with an application through the gesture based user interface to view browse navigate or otherwise interact with one or more content items . The content item may comprise for example a file a folder a document a network page an image a video an audio file an email message etc. In the example of the content items take the form of photos displayed by a photo gallery application . The user interface may visually represent the content items with icons. For some types of content items each icon is a thumbnail or smaller representation of the actual content. For content items the icon may instead represent the type of content. For example the icon for a text document may be shown as a sheet of paper the icon for a subdirectory in a file system may be shown as a folder etc. In the example of the content items are arranged in a grid. However other arrangements of multiple content items are possible.

Turning now to the user begins the process of sending a content item to a particular predefined destination by performing a gesture which designates a particular content item as one intended for a predefined destination. In the example user interface of the gesture used to designate is a tap and hold gesture at location on the display . For a tap and hold gesture the user touches one of the content items at a particular screen location and continues to touch for some predefined period of time. The touch may take the form of a physical touch or may take the form of clicking the mouse button for example in which case the hold might correspond to holding the mouse button for the predefined period.

The gesture based user interface may provide a visual and or audio indication that the content item has been designated. illustrates one example of such an indication in which the icon for the content item begins flashing suggested by circular region . As another example the icon for the content item may be animated in some way to show movement. As yet another example the gesture based user interface may use a tone or sound to alert the user that the content item has been designated. The gesture based user interface may support more than one type of indication and allow the user to choose between them via a configuration interface where the gesture based user interface stores the user s choices as configuration data . Finally although the designation of a content item described in connection with involves selecting one item from multiple items the user may also select a single content item when that item is the only content item shown.

Once the user has designated a content item or a portion of a content item to send to a destination the user then uses a specific gesture to identify which destination among of a set of predefined destinations. This gesture is referred to herein as a flick gesture. Destinations may correspond for example to folders email or phone contacts social networking sites blogs note taking sites media sharing sites etc.

Referring now to each predefined destination for the content item is associated with a particular region of the display . By gesturing toward one of these predefined regions the user instructs the gesture based user interface to send the designated content item to the destination associated with this region . Thus a user can quickly send a photo to his friend via an email message or can quickly save a document to a particular folder by performing a gesture that identifies the friend folder etc. The user may define these destinations and associate the destinations with regions via a configuration interface where the gesture based user interface stores these settings as configuration data .

User designation of a content item for sending to a destination causes the gesture based user interface to change state and look for a flick gesture that identifies one of a set of predefined destinations. As shown in a flick gesture involves a touch at a point of origin followed by movement toward one of a set of predefined regions of the display . Since a flick gesture involves a touch and move a flick gesture can be described by a point of origin and movement along a track toward a region .

The display may be divided into any number of regions . The regions may comprise only a subset of the display . For example in the example embodiment shown in the display is divided into four regions one for each edge of a rectangular display . In another embodiment the display is divided into four regions one for each corner of a rectangular display . In other embodiments the entire display may be divided into some number of rectangular regions which form a grid. In still other embodiments the regions have a shape that is other than rectangular.

In some embodiments that incorporate three dimensional display and or input technologies the regions are arranged in three dimensional space. For example rather than a two dimensional grid of regions on the screen the flick gesture destinations may correspond to a three dimensional grid of regions in space. While various embodiments described herein involve regions laid out in a linear grid non linear arrangements are also possible.

The gesture based user interface may use the distance from the point of origin to determine that the flick gesture has identified a particular delivery destination. That is the gesture based user interface may utilize a tripline at a predetermined distance from the point of origin and consider any movement past this tripline to be an indication that a particular region and thus a particular destination has been chosen. For example consider a scenario in which the predefined destinations are associated with the four edges of a rectangular display . The gesture based user interface may detect when the track of a flick gesture extends more than an inch in any direction and further determine which region lies in this direction. In such a scenario the gesture based user interface may determine that the user has selected this region and the predefined destination associated with this region as the destination for content delivery.

In addition to including a point of origin the flick gesture is also described in terms of velocity and or acceleration that is how quickly the user moves from the point of origin toward the destination region . The flick gesture may be distinguished in this respect from other gestures such as swipe or scroll in that the flick gesture involves a faster or jerkier movement than does a swipe or scroll. In some embodiments the flick gesture may therefore be detected by a touch and hold for a first predefined threshold period followed by a movement having a velocity or acceleration exceeding a second predefined threshold period.

In some embodiments the final destination of the flick gesture depends on the velocity and or acceleration of the flick. For example a relatively low speed flick gesture may indicate a destination that is relatively close to the center of origin of the flick gesture while a relatively high speed flick gesture may indicate a destination that is farther away from the center of origin. This behavior may be viewed as involving a frictional component such that a faster or jerkier motion overcomes the friction and thus lands farther away while a slower or smoother motion does not overcome the friction and thus lands closer to the point of origin.

Some forms of the flick gesture involve a tap at a point of origin followed by a move in a single direction. In other scenarios a user may tap at a point of origin and then move in a first direction then in a second direction and so on before finally completing the gesture by lifting the finger off the screen. Such a flick gesture thus involves more than a tap followed by a move toward a single destination or region of the screen as the user s finger moves toward several different screen regions before the flick gesture is complete. In such scenarios the gesture based user interface may derive movement toward a particular final screen region by computing a vector or trajectory based on the last direction or based on the last N points over which the user s finger traveled or some combination thereof.

The flick gesture may be distinguished in another respect from other gestures such as swipe or scroll in that the track of the flick gesture may be shorter than the track of a swipe or scroll. That is a user making a flick gesture may touch the display and then move his finger for a relatively small distance such as an inch or less whereas a swipe or scroll may be a much larger motion on the order of several inches or more. Therefore in some embodiments the flick gesture may be detected by a touch at a point of origin a hold at this point of origin for a first predefined threshold period followed by movement to a second point where the distance between the first and second points is under a second predefined threshold.

The flick gesture is also distinguishable from a drag and drop gesture in that a drag and drop requires the user to move all the way to the destination. For example inserting a file into a folder via a drag and drop requires the user to move all the way to the folder. In contrast the flick gesture is a move toward a destination and does not require the motion to complete at the destination.

Some embodiments of the gesture based user interface provide a visual representation of the set of predefined destinations and their association with the display regions . This visual indication may appear for example after the user has designated content so as to remind the user what his options are for the different predefined destinations. The gesture based user interface may support more than one type of indication and allow the user to choose between them via a configuration interface where the gesture based user interface stores the user s choices as configuration data .

An example of one such representation is shown in where the gesture based user interface displays a set of labels within each of the various regions . Each label includes text with the name of the destination. In the embodiment of the regions correspond to the edges of the screen and are labeled Email to Bob Jane s FaceBook page My Favorite Photos and My Tablet. These labels may appear when the gesture based user interface has designated a content item . The labels may be visually distinguished by a highlight transparency flashing or some other mechanism for drawing the user s attention to the labels . The user may define the text of the labels via a configuration interface or the gesture based user interface may derive the text from the corresponding destination.

In some embodiments the gesture based user interface provides a hierarchy of destinations. For example one corner of the display may be associated with email contacts and the user may specify a particular email contact after the flick gesture. In such an embodiment the gesture based user interface may display an additional set of labels once a particular region has been selected and the user may perform another flick gesture to choose a particular email address.

Through the process described in connection with the user sends a content item to a predefined destination by first designating or selecting a content item then performing a flick gesture that is associated with a predefined display region . The predefined regions are in turn associated with predefined destinations. In some embodiments the gesture based user interface may provide a confirmation when the designated content item has been sent to the destination.

In the embodiments described above the user designates an entire content item for delivery to a predefined destination. In other embodiments the user may as part of the designation process select a portion of a content item for delivery to a predefined destination. For example the user may select a portion of a document or network page or a portion of an image or a segment of a video or audio file. The partial designation process allows a user to easily share a few lines from an article on a network page to his friends via a post on a social networking site. The features described in connection with also apply to partial selection embodiments.

Turning now to shown is a flowchart that provides one example of the operation of portion s of the gesture based user interface according to various embodiments. It is understood that the flowchart of provides merely an example of the many different types of functional arrangements that may be employed to implement the operation of portion s of the gesture based user interface as described herein. As an alternative the flowchart of may be viewed as depicting an example of steps of a method implemented in the computing device according to one or more embodiments.

Beginning at box the gesture based user interface detects a user gesture designating a content item for delivery to a predefined destination. The designation may correspond to a generic selection of an item for any purpose or may be a designation that is specific to delivery to a predefined destination. For example if the gesture based user interface uses a single tap for generic selection a double tap may be used to designate delivery to a designation. The gesture based user interface may utilize any type of gesture for this designation as may be appreciated.

In detecting the designation gesture the gesture based user interface may receive various events from the operating system and then process those events to detect different types of gestures. The events may be low level events such as touches from which the gesture based user interface can detect types of gestures or may be higher level events which represent entire gestures.

At box the gesture based user interface displays a visual representation of the predefined destinations in conjunction with the regions of the display . Several examples of such a visual representation were discussed earlier including labels appearing within the regions and labels appearing around the point of origin .

Next at box the gesture based user interface detects a flick gesture described by a point of origin and movement . The movement may be represented as a track or series of points a vector an endpoint or some combination thereof as well as other types of representations as should be appreciated. The description of the flick gesture may also include a velocity of the gesture movement or acceleration of the gesture movement.

At box the gesture based user interface processes the movement information contained in the flick gesture to determine which region of the display that the user has gestured toward. For example if the regions correspond to the corners of the display the gesture based user interface may correlate any movement toward the top left quadrant of the display with the top left corner region . Similarly if the regions correspond to the edges of the display the gesture based user interface may correlate any movement toward the top left quadrant of the display with the top edge region .

Next at box the gesture based user interface determines which of the set of predefined destinations is associated with the region identified in box . This association may be retrieved from the gesture mapping data . As noted above the user may configure the gesture based user interface to specify these destinations and the association between destinations and regions .

Once a destination is identified at box the gesture based user interface takes action to deliver the designated content item to the identified destination. The gesture based user interface may include code to deliver the content to some types of destinations but may also invoke an application executing on the computing device to deliver other types of content. For example the gesture based user interface may deliver a file to a locally stored folder or deliver a text message to a phone number contact while relying on an email application to send a message to a destination that is specified as an email address.

The specifics of the action depend on the type of destination. For example if the destination is an email contact the gesture based user interface may communicate with an email application executing on the computing device in order to create an email message addressed to the contact insert the content item into the message and send the message. Depending on the type of content item the item may be inserted inline into the message or may be attached to the message. As another example if the destination is a social networking site the gesture based user interface may log in to the site using information stored in the gesture mapping data and create a post on the site that includes the content item . As yet another example if the destination is another computing device the gesture based user interface may initiate communications with the other computing device and then transfer the content item using a network protocol. During this communication process the gesture based user interface may provide the other computing device with credentials that identify the user where these credentials may be stored in the gesture mapping data .

The embodiments discussed above involve delivering a single content item to a predefined destination. Other embodiments of the gesture based user interface allow a user through gesturing to accumulate or aggregate content items in a clipboard associated with a region of the display . The user may label these clipboards and associate the clipboards with regions via a configuration interface where the gesture based user interface stores these settings as configuration data .

After the accumulation is complete the user interacts with an application to set up delivery of the accumulated content to a destination. For example the user may execute a browser application to log in to a friend s social networking page. At this point the user performs a gesture which instructs the gesture based user interface to insert all of the aggregated content items from the clipboard into the social networking page.

With reference now to the user then selects content items and or portions of content items . In this example the user has selected a few lines of text and an image . Having selected one or more items the user performs a flick gesture in a region which includes selected content items . The flick gesture instructs the gesture based user interface to add the selected content items to a clipboard associated with the flick gesture. As discussed above the flick gesture includes a point of origin and movement in a direction of a region of the display . The gesture based user interface manages one or more clipboards each associated with a region . A flick gesture in the direction of a region instructs the gesture based user interface to add the selected content item to the clipboard associated with that region .

The gesture based user interface may provide a visual or audio indication that content items and or portions of content items are being accumulated at a particular region . For example the gesture based user interface may use color transparency highlighting or other types of cues in the region . The visual indication may include a label which distinguishes the clipboards and the regions for example a name for the clipboard given by the user.

Turning now to having accumulated one or more content items the user then interacts with an application to set up an object to deliver content to a destination. This application may be the same as the one from which content was accumulated or may be a different one. In this example the application used to deliver the accumulated content is an email application . The user interacts with the application to create a message and address it to a contact. At this point the user performs an insert gesture which instructs the gesture based user interface to insert all of the aggregated content items from a particular clipboard into the message . As one example the insert gesture may be a swipe from the region associated with the clipboard toward a location of the application . In some embodiments the insert gesture also causes the gesture based user interface to stop accumulating. In other embodiments a separate action or gesture is used to stop accumulation.

As shown in in response to an insert gesture the gesture based user interface interoperates with the application to insert into the email message all of the aggregated content items from the clipboard associated with the insert gesture . The gesture based user interface may utilize for example an application programming interface API provided by the application or by the operating system e.g. an Insert From Clipboard function . In this example the selected lines of text are inserted in the body of the message and the selected image is inserted as an attachment . The insertion type may be a configuration option may depend on the location of the insert gesture or may depend on the type of the content items or some combination thereof.

The embodiment shown in an insert gesture operates to insert all the content items from a particular clipboard. Other embodiments allow the user to select one or more particular content items from a clipboard after performing an insert gesture . An example of such an embodiment is shown in .

As can be seen in the user interacts with the application to create a message and address it to a contact then performs an insert gesture . In this embodiment the gesture based user interface causes a clipboard viewer to be displayed. The clipboard viewer includes a list of the content items that are currently stored in the clipboard associated with the insert gesture . The user then selects one or more of the content items from the clipboard viewer and in response the gesture based user interface interoperates with the application to insert into the email message the selected content item s from the clipboard.

In some embodiments multiple clipboards may be in use at one time. For example a user may use the right side of the display as a clipboard for the user s own Twitter account and the left side of the display as a clipboard for a friend s Facebook page. As the user interacts with content by performing activities such as browsing network pages reading email and viewing locally stored documents the user may accumulate content items to the two clipboards. The user may then navigate to his account on the Twitter site and swipe from the right off screen area to the Twitter network page which will insert accumulated content items stored in the Twitter clipboard. Similarly the user may navigate to his friend s account on the Facebook site and swipe from the right off screen area to the Facebook network page which will insert accumulated content items stored in the Facebook clipboard.

Turning now to shown is a flowchart that provides one example of the operation of portion s of the gesture based user interface according to various embodiments. It is understood that the flowchart of provides merely an example of the many different types of functional arrangements that may be employed to implement the operation of portion s of the gesture based user interface as described herein. As an alternative the flowchart of may be viewed as depicting an example of steps of a method implemented in the computing device according to one or more embodiments.

Beginning at box the gesture based user interface detects a user gesture selecting a content item for accumulation to a particular clipboard. In detecting the selection gesture the gesture based user interface may receive various events from the operating system and then process those events to detect different types of gestures. The events may be low level events such as touches from which the gesture based user interface can detect types of gestures or may be higher level events which represent entire gestures.

Next at box the gesture based user interface detects a flick gesture described by a point of origin and movement . The movement may be represented as a track or series of points a vector an endpoint or some combination thereof as well as other types of representations as should be appreciated. The description of the flick gesture may also include a velocity of the gesture movement or acceleration of the gesture movement.

At box the gesture based user interface processes the movement information contained in the flick gesture to determine which region of the display that the user has gestured toward. For example if the regions correspond to the corners of the display the gesture based user interface may correlate any movement toward the top left quadrant of the display with the top left corner region . Similarly if the regions correspond to the edges of the display the gesture based user interface may correlate any movement toward the top left quadrant of the display with the top edge region . Having determined the region the gesture based user interface determines which of the set of predefined clipboards is associated with the region . This association may be retrieved from the gesture mapping data . As noted above the user may configure the gesture based user interface to associate clipboards with regions and to label or name the clipboards.

Having identified a clipboard at box the gesture based user interface adds the selected content item to the clipboard. Next at box the user interacts with one more applications and objects maintained by those applications in order to set up delivery of the accumulated content items to the identified destination. For example the user may open up a document in a word processing application use an email application to create and address an email message or may use a browser application to navigate through pages on a network site.

At box the gesture based user interface looks for an insert gesture . If no insert gesture is detected then processing continues at box . Thus this process of selecting content items flicking the items to a clipboard for accumulation and interacting with applications continues until an insert gesture is detected. Upon such detection processing continues at box .

At box the gesture based user interface determines from the insert gesture which clipboard the user intends to use as a source of accumulated content and which application the user intends as a destination for the accumulated content. The gesture based user interface may use for example the origin of the gesture to determine the clipboard and the direction of the gesture to determine the application . Then gesture based user interface then interacts with this application to cause the accumulated content items from the particular clipboard to be inserted into the particular application object. The nature of such interactions will depend on the type of application and content as may be appreciated. The process of is then complete.

In the embodiment described in connection with the gesture based user interface responds to an insert gesture at box by inserting all aggregated content items into the application object. For some embodiments which allow the user to select particular content items for insertion the functionality of box is expanded as will now be described in connection with .

Turning now to shown is a flowchart that provides one example of the operation of portion s of the gesture based user interface according to various embodiments. In particular the flowchart of describes in more detail the functionality of box in . It is understood that the flowchart of provides merely an example of the many different types of functional arrangements that may be employed to implement the operation of portion s of the gesture based user interface as described herein. As an alternative the flowchart of may be viewed as depicting an example of steps of a method implemented in the computing device according to one or more embodiments.

The process of begins after a flick gesture and then an insert gesture are detected. Beginning at box the gesture based user interface displays in a clipboard viewer the accumulated contents of the clipboard that is associated with the insert gesture . At box the gesture based user interface receives a user selection of one or more content items from the clipboard. Next at box the user interface waits for another flick gesture. At box the user interface determines from the direction of the flick gesture a particular application object for insertion. For example a flick toward an email message indicates insertion into that email message while a flick toward a folder indications insertion into that folder. Finally at box the gesture based user interface interacts with this application to cause the selected content items from the clipboard to be inserted into the application object specified by the flick gesture that was detected at box .

Turning now to shown is a schematic block diagram of the computing device according to an embodiment of the present disclosure. The computing device includes at least one processor circuit for example having a processor and a memory both of which are coupled to a local interface . The local interface may comprise for example a data bus with an accompanying address control bus or other bus structure as can be appreciated. Also coupled to the local interface are the gesture input device and the display discussed earlier in connection with . A display and a gesture input device are also attached to coupled to or integrated with the computing device . In some embodiments the computing device also includes a network interface not shown .

Stored in the memory are both data and several components that are executable by the processor . In particular stored in the memory and executable by the processor are the applications the operating system and the gesture based user interface . Other components may also be stored in the memory and executable by the processor . While not illustrated the computing device also includes components like those shown in .

It is understood that there may be other applications that are stored in the memory and are executable by the processor as can be appreciated. Where any component discussed herein is implemented in the form of software any one of a number of programming languages may be employed such as for example C C C Objective C Java JavaScript Perl PHP Visual Basic Python Ruby Delphi Flash or other programming languages.

A number of software components are stored in the memory and are executable by the processor . In this respect the term executable means a program file that is in a form that can ultimately be run by the processor . Examples of executable programs may be for example a compiled program that can be translated into machine code in a format that can be loaded into a random access portion of the memory and executed by the processor source code that may be expressed in proper format such as object code that is capable of being loaded into a random access portion of the memory and executed by the processor or source code that may be interpreted by another executable program to generate instructions in a random access portion of the memory and executed by the processor etc. An executable program may be stored in any portion or component of the memory including for example random access memory RAM read only memory ROM hard drive solid state drive USB flash drive memory card optical disc such as compact disc CD or digital versatile disc DVD floppy disk magnetic tape or other memory components.

The memory is defined herein as including both volatile and nonvolatile memory and data storage components. Volatile components are those that do not retain data values upon loss of power. Nonvolatile components are those that retain data upon a loss of power. Thus the memory may comprise for example random access memory RAM read only memory ROM hard disk drives solid state drives USB flash drives memory cards accessed via a memory card reader floppy disks accessed via an associated floppy disk drive optical discs accessed via an optical disc drive magnetic tapes accessed via an appropriate tape drive and or other memory components or a combination of any two or more of these memory components. In addition the RAM may comprise for example static random access memory SRAM dynamic random access memory DRAM or magnetic random access memory MRAM and other such devices. The ROM may comprise for example a programmable read only memory PROM an erasable programmable read only memory EPROM an electrically erasable programmable read only memory EEPROM or other like memory device.

Also the processor may represent multiple processors and the memory may represent multiple memories that operate in parallel processing circuits respectively. In such a case the local interface may be an appropriate network that facilitates communication between any two of the multiple processors between any of the processors and any of the memories or between any two of the memories etc. The local interface may comprise additional systems designed to coordinate this communication including for example performing load balancing. The processor may be of electrical or of some other available construction.

Although the applications the operating system and the gesture based user interface and other various systems described herein may be embodied in software or code executed by general purpose hardware as discussed above as an alternative the same may also be embodied in dedicated hardware or a combination of software general purpose hardware and dedicated hardware. If embodied in dedicated hardware each can be implemented as a circuit or state machine that employs any one of or a combination of a number of technologies. These technologies may include but are not limited to discrete logic circuits having logic gates for implementing various logic functions upon an application of one or more data signals application specific integrated circuits having appropriate logic gates or other components etc. Such technologies are generally well known by those skilled in the art and consequently are not described in detail herein.

The flowcharts of and show the functionality and operation of an implementation of portions of the gesture based user interface . If embodied in software each block may represent a module segment or portion of code that comprises program instructions to implement the specified logical function s . The program instructions may be embodied in the form of source code that comprises human readable statements written in a programming language or machine code that comprises numerical instructions recognizable by a suitable execution system such as one of the processors in a computer system or other system. The machine code may be converted from the source code etc. If embodied in hardware each block may represent a circuit or a number of interconnected circuits to implement the specified logical function s .

Although the flowcharts of and show a specific order of execution it is understood that the order of execution may differ from that which is depicted. For example the order of execution of two or more blocks may be scrambled relative to the order shown. Also two or more blocks shown in succession in the flowcharts of and may be executed concurrently or with partial concurrence. Further in some embodiments one or more of the blocks shown in and may be skipped or omitted. In addition any number of counters state variables warning semaphores or messages might be added to the logical flow described herein for purposes of enhanced utility accounting performance measurement or providing troubleshooting aids etc. It is understood that all such variations are within the scope of the present disclosure.

Also any logic or application described herein including the applications the operating system and the gesture based user interface that comprises software or code can be embodied in any non transitory computer readable medium for use by or in connection with an instruction execution system such as for example the processor in a computer system or other system. In this sense the logic may comprise for example statements including instructions and declarations that can be fetched from the computer readable medium and executed by the instruction execution system. In the context of the present disclosure a computer readable medium can be any medium that can contain store or maintain the logic or application described herein for use by or in connection with the instruction execution system. The computer readable medium can comprise any one of many physical media such as for example magnetic optical or semiconductor media. More specific examples of a suitable computer readable medium would include but are not limited to magnetic tapes magnetic floppy diskettes magnetic hard drives memory cards solid state drives USB flash drives or optical discs. Also the computer readable medium may be a random access memory RAM including for example static random access memory SRAM and dynamic random access memory DRAM or magnetic random access memory MRAM . In addition the computer readable medium may be a read only memory ROM a programmable read only memory PROM an erasable programmable read only memory EPROM an electrically erasable programmable read only memory EEPROM or other type of memory device.

It should be emphasized that the above described embodiments of the present disclosure are merely possible examples of implementations set forth for a clear understanding of the principles of the disclosure. Many variations and modifications may be made to the above described embodiment s without departing substantially from the spirit and principles of the disclosure. All such modifications and variations are intended to be included herein within the scope of this disclosure and protected by the following claims.

