---

title: Dynamic techniques for optimizing soft real-time task performance in virtual machine
abstract: Methods to dynamically improve soft real-time task performance in virtualized computing environments under the management of an enhanced hypervisor comprising a credit scheduler. The enhanced hypervisor analyzes the on-going performance of the domains of interest and of the virtualized data-processing system. Based on the performance metrics disclosed herein, some of the governing parameters of the credit scheduler are adjusted. Adjustments are typically performed cyclically, wherein the performance metrics of an execution cycle are analyzed and adjustments may be applied in a later execution cycle. In alternative embodiments, some of the analysis and tuning functions are in a separate application that resides outside the hypervisor. The performance metrics disclosed herein include: a “total-time” metric; a “timeslice” metric; a number of “latency” metrics; and a “count” metric. In contrast to prior art, the present invention enables on-going monitoring of a virtualized data-processing system accompanied by dynamic adjustments based on objective metrics.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08499303&OS=08499303&RS=08499303
owner: Avaya Inc.
number: 08499303
owner_city: Basking Ridge
owner_country: US
publication_date: 20120419
---
The present application is a continuation of U.S. patent application Ser. No. 12 849 921 filed Aug. 4 2010 now U.S. Pat. No. 8 166 485 entitled Dynamic Techniques for Optimizing Real Time Task Performance in Virtual Machines which claims the benefit of U.S. Provisional Patent Application No. 61 232 542 filed Aug. 10 2009 entitled Supporting Soft Real Time Virtual Machines. The concepts but not necessarily the nomenclature of this provisional application are hereby incorporated by reference.

Furthermore this application claims the benefit of U.S. provisional application No. 61 254 019 filed Oct. 22 2009 entitled Supporting Soft Real Time Virtual Machines. The concepts but not necessarily the nomenclature of this provisional application are hereby incorporated by reference.

If there are any contradictions or inconsistencies between this application and one or more of the cases that have been incorporated by reference the claims in the present case should be interpreted to be consistent with the language in this case.

U.S. patent application Ser. No. 12 686 719 filed Jan. 13 2010 now U.S. Pat. No. 8 245 234 entitled Credit Scheduler for Ordering the Execution of Tasks is related to the present application.

U.S. patent application Ser. No. 12 768 458 filed Apr. 27 2010 now U.S. Pat. No. 8 161 491 entitled Soft Real Time Load Balancer is related to the present application.

The present invention relates to data processing systems in general and more particularly to credit schedulers for ordering the execution of tasks on a processor.

In a data processing system when two or more operating systems operate on one piece of hardware the tasks associated with each operating system contend for the hardware that is available. Without something to mediate their access one operating system s tasks could monopolize or over use the hardware to the detriment of the other operating system s tasks. When the hardware comprises more than one processor that is available to execute the various tasks one processor can become over utilized while another remains under used. Therefore a program that sits between the operating systems and the hardware acts as a mediator. This program is commonly known as a hypervisor. 

One of the jobs performed by the hypervisor is to choose a processor that is to execute one or more tasks. Another job of the hypervisor is to schedule the order of execution of tasks. Another job of the hypervisor is to allocate the share of time that the data processing system is to spend executing certain types of tasks relative to other types. These are not easy jobs. Some tasks are time sensitive e.g. tasks associated with input or output speech processing video processing transmission or reception of signals etc. and some tasks are non time sensitive or are less time sensitive. Some tasks require relatively long times on a processor and other tasks requires less time. Whatever the mix the respective operating systems are always presenting to the hypervisor tasks to be executed. If the hypervisor does not properly balance and allocate resources the performance of the entire system can degrade. For voice and media related applications the degradation is evidenced by poor quality of voice or video.

A need therefore exists for a hypervisor that can properly manage a complex mix of contending tasks including time sensitive and non time sensitive tasks.

The present invention enables the scheduling and execution of tasks without some of the costs and disadvantages associated with hypervisors in the prior art. The present disclosure describes techniques that dynamically improve soft real time task performance in virtualized computing environments that are under the management of an enhanced hypervisor.

Tasks generally known in the art as soft real time tasks are time sensitive tasks that have somewhat flexible or soft deadlines. Tasks that perform general computing typically are non time sensitive tasks. Soft real time tasks are often associated with media servers and IP PBX applications or with other voice processing or media call processing applications. Some of the illustrative embodiments of the present invention serve the special needs of soft real time tasks and therefore whether a task is time sensitive or non time sensitive is material to the operation of some of the illustrative embodiments. However the present invention is not so limited and the techniques and systems disclosed herein can be applied to other types of tasks that are not necessarily time sensitive but which have special performance needs that prior art hypervisors do not address.

For purposes of this specification a domain is defined as software that is i an operating system or ii an application using the operating system and that comprises tasks each of which is to be executed by a processor in the data processing system that is under the management of a hypervisor.

The present disclosure describes techniques that measure and monitor the performance of one or more domains of interest running in a virtualized data processing system. The disclosure introduces a plurality of performance metrics that characterize the domain s of interest which typically are time sensitive domains. Based on the performance metrics an enhanced hypervisor generates adjusted parameters to improve the performance of the domain s of interest.

In the aggregate the enhanced hypervisor receives tasks to be executed and analyzes them. The enhanced hypervisor in the illustrative embodiments comprises a credit scheduler. The credit scheduler is the system component that actually queues each task and gives it a priority level that affects when the task will be executed on a processor. The priority and queue placement is governed by parameters in the credit scheduler. The enhanced hypervisor analyzes the on going performance of the domains of interest and of the virtualized data processing system that runs the domains. Based on the performance metrics that are disclosed herein some of the governing parameters of the credit scheduler are adjusted. Adjustments are typically performed cyclically wherein the performance metrics of an execution cycle are analyzed and if need be adjustments are applied in a later execution cycle although a different time interval can be used.

In alternative embodiments some of the analysis and tuning functions are implemented in a separate application that resides outside the hypervisor and communicates with the hypervisor through appropriate interfaces. Although the illustrative embodiments operate on a multi processor system the invention is not so limited and some of the disclosed techniques apply to a single processor system.

In alternative embodiments the disclosed metrics are based on a time interval that does not equate to an execution cycle such as a plurality of execution cycles or some other time interval.

In some illustrative embodiments comprising voice related applications one of the metrics of performance quality is the voice quality as measured by the perceptual evaluation of speech quality PESQ standard measure specified by ITU T Recommendation P.862. In general if the voice quality of the voice related domains is not acceptable the performance of the data processing system requires adjustment. Adjustments can be based on benchmarks other than PESQ.

The disclosed techniques evaluate the above mentioned metrics and if necessary calculate adjustments. Based on the calculations the scheduler then adjusts its parameters going forward. In contrast to prior art systems the present invention enables on going monitoring of a virtualized data processing system accompanied by dynamic adjustments based on objective metrics.

Some illustrative embodiments comprise A method comprising receiving by a hypervisor in a data processing system a first metric that is associated with a first domain wherein i the data processing system comprises a first processor and a second processor ii the first domain comprises a first plurality of tasks that were executed on the data processing system and iii the first metric is based on the amount of time during an earlier time interval that was spent executing tasks of the first domain that had an over priority when the value of the first metric is above a first threshold increasing an amount of time to be spent in a later time interval to execute tasks of the first domain that have an over priority and executing by the data processing system in the later time interval the tasks of the first domain based on the increased amount of time.

For the purposes of this specification the term processor is defined as a tangible computing resource that is capable of executing a task. In the present disclosure data processing system comprises two processors. It will be clear to those skilled in the art that alternative names for a processor include but are not limited to computer core computing core processing core central processing unit CPU computing resource or processing resource. 

For the purposes of this specification the term task is defined as at least one operation performed by a processor. A task is any entity that can be scheduled on a processor e.g. a process or a virtual CPU VCPU .

In some illustrative embodiments a domain is sometimes referred to as a virtual machine VM and can comprise one or more VCPUs. In some alternative embodiments a VM comprises a plurality of domains.

For purposes of this specification a time sensitive task is a task that has an execution deadline. A task that is generally known in the art as a soft real time task is a time sensitive task with a somewhat flexible or soft deadline. Some tasks are time sensitive in accordance with the function they perform in data processing system . A domain that is designated as time sensitive comprises tasks that are all or in substantial part time sensitive. In some illustrative embodiments when a domain is designated as time sensitive the tasks that it comprises are treated as time sensitive tasks.

For purposes of this specification a non time sensitive task is a task that lacks an execution deadline or has a generous deadline that is well beyond the flexible deadline of a time sensitive task. Tasks that perform general computing typically are non time sensitive tasks. A domain that is designated as non time sensitive comprises tasks that are all or in substantial part non time sensitive. In some illustrative embodiments the tasks of a domain that is designated as non time sensitive are treated as non time sensitive tasks.

For purposes of this specification a priority attribute of a task governs at least in part i where the task is placed within a queue to await execution and ii how soon after being queued the task will be executed by a processor. Priority is described more below and in regards to .

As noted the present invention enables monitoring a virtualized data processing system and dynamically applying adjustments to compensate for inadequate performance. In some embodiments the particular concern is with the performance of time sensitive tasks and domains. For example one concern is whether time sensitive tasks are being starved for processing resources which may be evidenced by too much time spent in the over priority region of a queue. A second concern is whether time sensitive tasks are too often pre empted from a processor even when they have plenty of allocated credits which may be evidenced by too much time or too many visits to the under priority region of a queue. A third concern is whether time sensitive tasks with high input output needs would benefit from moving to the boost priority region of a queue to speed up their execution. The system techniques and metrics described herein enable the monitoring and accompanying adjustments to improve the performance of time sensitive tasks and domains in a system according to the present invention.

Data processing system is a hardware and software system that comprises in salient part receiver hypervisor queue processor transmitter queue processor and transmitter . It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments in which some of the components of data processing system are instantiated in software or in a combination of software and hardware and wherein any components instantiated in software themselves are executed on a processor in data processing system . It will be clear to those having ordinary skill in the art after reading the present disclosure that any disclosure herein in respect to a component n applies equally to other components of the same kind.

Although depicts one receiver it will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments of the present invention that comprise any number of receivers e.g. two receivers three receivers etc. For example a receiver might be dedicated to each queue.

Although depicts one hypervisor it will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments of the present invention that comprise any number of hypervisors e.g. two hypervisors three hypervisors etc.

Although depicts two queues and it will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments of the present invention that comprise any number of queues e.g. one queue three queues four queues etc.

Although depicts two processors and meaning that data processing system is instantiated as a multi processor system it will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments of the present invention that comprise any number of processors e.g. one processor three processors four processors etc. It will be further clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments of the present invention in which a multi core processor platform comprises some or all of the processors in data processing system . It will be further clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments of the present invention in which a processor in data processing system comprises multiple cores.

Although depicts two transmitters and it will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments of the present invention that comprise any number of transmitters e.g. one transmitter three transmitters etc.

It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments of the present invention in which the components are differently connected than the depiction of . For example each queue might receive tasks from a receiver before task scheduling occurs at hypervisor .

Although depicts the components of data processing system as being separate from one another it will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments of the present invention in which a single physical device comprises one or more of these components. For example a multi core processor platform could comprise some or all of the processors some or all of the queues some or all of the receivers and some or all of the transmitters in data processing system . For example hypervisor could comprise queues and . It will be further clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments of the present invention in which multiple physical devices are logically associated to comprise one or more of these components. For example multiple processors that are distinct physical components could be logically associated in a virtualized computing environment. Likewise a virtualized computing environment could comprise a combination of distinct and combined components in accordance with the present invention such as a single core processor and a multi core processor.

Although depicts certain ratios of one type of component to another type of component it will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments of the present invention in which the ratios differ. For example a single queue could be logically subdivided to serve more than one processor. For example a single transmitter might transmit output from more than one processor.

Receiver is hardware that receives a temporal succession of tasks to be executed by data processing system and presents those tasks to hypervisor . For purposes of the present disclosure each task is identified by T wherein k is an integer that represents the relative order of arrival of the task at receiver with respect to other tasks. For example task Tarrived at receiver immediately before task T wherein k is an integer. It will be clear to those skilled in the art after reading this disclosure how to make and use receiver .

Hypervisor is software that is capable of performing the functionality described in this disclosure and in the accompanying figures. It will be clear to those having ordinary skill in the art after reading the present disclosure that a software based scheduler is itself executed by a processor but not necessarily by processor or processor . It will be further clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments of the present invention in which scheduler is exclusively hardware or a combination of hardware and software.

Hypervisor virtualizes the tangible physical resources of data processing system such as processors memory and direct access storage devices so that domains can smoothly run on data processing system . Hypervisor enables multiple domains to run concurrently on data processing system by presenting the guest domains with a virtual platform while the hypervisor monitors the performance of the domains allocates tasks to the available processors and takes other appropriate actions. A hypervisor is also known in the art as a virtual machine monitor or VMM. 

In accordance with the illustrative embodiments hypervisor is an enhanced hypervisor that is based on a credit scheduling scheme because it comprises a credit scheduler which is depicted in more detail in . The XEN hypervisor is an example of a hypervisor that comprises a credit scheduler. It will be clear to those skilled in the art after reading this disclosure how to make and use alternative embodiments of the present invention in which hypervisor does not comprise a credit scheduler or a credit scheduling scheme. It will be clear to those skilled in the art after reading this disclosure how to make and use hypervisor .

Queue is hardware that holds each task and its accompanying parameters while the task awaits execution by processor .

Queue is hardware that holds each task and its accompanying parameters while the task awaits execution by processor . It will be clear to those skilled in the art after reading this disclosure how to make and use queues and .

Processor is hardware that is a processor that executes tasks in the order determined by hypervisor . In accordance with the illustrative embodiments of the present invention processor comprises one core but it will be clear to those skilled in the art after reading this disclosure how to make and use alternative embodiments of the present invention in which processor comprises multiple cores.

Processor is hardware that executes tasks in the order determined by hypervisor . In the illustrative embodiments processor is a processor identical to processor but it will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments in which processors and are not identical.

In the illustrative embodiments processor selects the task at the head of queue to execute next. In the illustrative embodiments processor selects the task at the head of queue to execute next. This is depicted in more detail in . It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments in which a processor executes its next task from a source other than its corresponding queue or from other than the head of the corresponding queue. It will be clear to those skilled in the art how to make and use processor and processor .

Transmitter is well known hardware that transmits the results of each task execution from processor . Transmitter is well known hardware that transmits the results of each task execution from processor . It will be clear to those having ordinary skill in the art how to make and use transmitters and .

Hypervisor comprises in salient part scheduler monitor and parameter tuner . Scheduler comprises in salient part core scheduler engine and metrics hooks . Monitor comprises in salient part events recorder metrics analyzer and user interface .

It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments in which monitor or parameter tuner or both or portions thereof reside s outside of hypervisor . For example a separate application could comprise monitor and parameter tuner and could communicate with hypervisor through appropriate interfaces such as via an application programming interface API . Likewise metrics analyzer could reside outside monitor and outside hypervisor as a separate application with appropriate interfaces such as an API.

In some embodiments comprising an API interface scheduler collects natively available trace data and transmits them to the separate application via the API. Additionally the separate application supplies adjustments to hypervisor via the same API. Scheduler receives the adjustments and translates them into appropriate parameters that govern the operations of scheduler .

It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments in which hypervisor also comprises queues and .

Scheduler according to the illustrative embodiments is a credit scheduler. It will be clear to those skilled in the art after reading this disclosure how to make and use alternative embodiments of the present invention in which the scheduler is not a credit scheduler.

Core scheduler engine represents the core of the virtual machine monitor scheduler functions in hypervisor including but not limited to setting and applying of the principal native parameters of scheduler . For example in accordance with the credit scheduler in the illustrative embodiment the native parameters include but are not limited to weights caps credits and pinning. The core scheduler engine sets and applies the credits and caps that are allocated to each of the guest domains. Credits are distributed periodically. For example credits are tuned through weight settings wherein legal weights range from 1 to 65535 and the default weight is 256. A weight is a proportional share of a processor allocated to a guest domain and a weight can be changed by user input. A cap represents the maximum processing time that a domain can use in a given execution cycle even if excess processor capacity is available. According to the illustrative embodiments core scheduler engine manages the weights credits and caps and the resultant queuing of tasks that are to be executed on the available processors of data processing system .

Additionally core scheduler engine is capable of setting a processor pinning policy meaning that tasks associated with a given domain can be scheduled to be executed exclusively on a given processor i.e. the domain is pinned to the selected processor. Typically the other domains are pinned away from the selected processor but in some illustrative embodiments other domains may be permitted to run on the selected processor.

On a task by task basis core scheduler engine is responsible for evaluating each task from receiver and placing the task into an appropriate queue to await processing by the corresponding processor. The actions performed task by task by core scheduler engine are described in more detail below and in the accompanying figures.

Metrics hooks captures trace data from core scheduler engine . Examples of trace data include but are not limited to hypervisor calls domain interactions debugging features and default events. Metrics hooks further enables submitting changes to the parameters in core scheduler engine .

Events recorder receives from scheduler event notifications and associated information including parameters that are associated with a task records the received information and provides the information to metrics analyzer .

Metrics analyzer processes and evaluates a plurality of metrics that are collected and recorded and additionally calculates other metrics that are relevant to the present invention. These metrics are described in more detail below and in the accompanying figures.

User interface provides at least one interface to users of hypervisor in a manner well known in the art. User interface enables users to view results from the metrics analyzer and to submit modifications to tune the parameters of scheduler . For example a user can change the weight allocated to a given domain. In some embodiments user interface is provided via an API to a separate application.

Parameter tuner calculates modified parameters for scheduler so that the performance of scheduler can be tuned to improve the performance of one or more domains running in data processing system . Parameter tuner transmits the modified parameters to scheduler . The parameters and resulting adjustments are described in more detail below and in the accompanying figures.

In the aggregate hypervisor depicted in is an enhanced hypervisor that receives tasks to be executed analyzes them applies the currently governing parameters of scheduler and places each task in a queue to await execution by a corresponding processor. Furthermore by way of monitor and parameter tuner hypervisor analyzes the on going performance of data processing system and of the domains that run on it. Based on the performance metrics that are disclosed herein at least some of the governing parameters of scheduler are adjusted. Adjustments are typically performed cyclically wherein the performance metrics of an execution cycle are analyzed and if need be adjustments are applied in a later execution cycle.

In prior art hypervisors tasks have or are assigned a priority attribute that governs at least in part where the task is placed within a queue. Typically prior art credit schedulers queue an incoming task or some other scheduler related event at the end of the list of tasks that have the same priority because credit scheduler queues have a region in the queue for each priority level. The priority levels and their relative meanings are specific to the scheduler platform being used by the implementers. For the XEN credit scheduler for example tasks have boost priority under priority over priority or idle priority in decreasing priority value. Thus a task with boost priority is of a higher priority than a task with under priority. Each of these priority levels has its own region within the queue that the XEN credit scheduler manages. In the preferred embodiments of the present invention task priority levels are the same as in the XEN credit scheduler. Queuing of incoming tasks to the appropriate priority region of the queue is generally the same in the preferred embodiments as in the XEN credit scheduler except as disclosed otherwise herein.

A task that is waiting an input output I O event is considered to be idle while it awaits the event. Such a task is placed by hypervisor in idle priority region . Such a task is not executed until it receives the expected event. Thus a processor does not select tasks from this region of the queue as it does from the other regions described below.

A task that has over used its credit allocation when it executed on a processor during the previous execution cycle is placed by hypervisor in over priority region . Such a task took longer to execute than its proportionally allotted credit.

A task that has under used its credit allocation when it executed on a processor during the previous execution cycle is placed by hypervisor in under priority region . Such a task took less time to execute than its proportionally allotted credit.

A task that receives an I O event is placed in boost priority region under certain conditions. Generally that is because a task that previously awaited an external event such as the arrival of a packet should be executed quickly once the packet arrives. Thus a task in idle priority region is boosted to boost priority region and therefore comes up for execution sooner than tasks in the under and over regions.

It should be noted that in the default XEN scheduler only tasks that were in the idle priority region are boosted upon receiving an event. However enhanced hypervisor also boosts other tasks to the boost priority region based on conditions even if such tasks were in the under priority region or in the over priority region when receiving an event. This policy is referred to herein as boost with event and generally applies to time sensitive tasks. This is at least in part because time sensitive domains such as voice processing applications tend to be I O intensive. The boost priority enhances the probability that time sensitive tasks will be rapidly executed regardless of their credit history. The boost with event policy is described in more detail below in regards to step .

It is to be understood that depending upon the conditions of data processing system and the domains executing on it any or all of regions can be empty of resident tasks at any point in time. Furthermore any number of tasks can reside in each and every one of regions .

The head of the queue is where processor selects the next task to execute according to the illustrative embodiment. It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments in which processor selects a task to execute from other than the head of the queue .

At step hypervisor analyzes metrics and if appropriate tunes or adjusts the parameters that are to govern scheduler during the present execution cycle. Step and the conditions under which adjustments are made if at all are described in more detail below and in the accompanying figures.

In the illustrative embodiment the adjustments are based at least in part on performance metrics from an earlier execution cycle but it will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments that rely on other conditions such as time of day day of year other parameters etc. It will be further clear that in some alternative embodiments the disclosed metrics are based on a time interval that does not equate to an execution cycle such as a plurality of execution cycles or some other time interval.

At step receiver receives one or more tasks to be executed by data processing system and presents each task to hypervisor .

At step scheduler in hypervisor schedules each task according to the governing parameters established in step by placing the task in an appropriate queue to be executed by the corresponding processor.

At step a processor to which the task is scheduled selects the task from the corresponding queue and executes the task and transmits the results to corresponding transmitter 

At step hypervisor monitors the performance of the present execution cycle so that it can generate performance metrics for subsequent use. So long as tasks remain to be executed control passes from step to step . Furthermore so long as a new execution cycle is to begin control passes from step to step . Step is described in more detail below and in the accompanying figures.

It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use the tasks associated with data processing system . It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments of the present invention in which steps and their constituent steps are executed in a different sequence are sub divided into other steps are selectively skipped are executed with different timing or are differently grouped or are executed by a different component or sub component of data processing system or use different data structures than described herein.

At step enhanced hypervisor selects an earlier execution cycle designated herein as C wherein i is an integer. It is to be understood that execution cycle Cneed not be the execution cycle that immediately precedes the present execution cycle designated herein as C wherein j is an integer and j i.

At step a decision point determines whether during execution cycle C the quality of performance of one or more domains of interest was acceptable. In the illustrative embodiments the domain of interest is a media call processing application that is responsible for processing incoming media streams and playing them as they arrive and transcoding at least some of them for retransmission. Media streams can originate from any live source such as an Internet protocol phone a media server audio from a radio station a video camera feed etc. In the illustrative embodiments the quality of performance of this domain for purposes of step is defined to be the perceived voice quality. In the illustrative embodiment the voice quality is measured by the perceptual evaluation of speech quality PESQ standard measure as specified by the ITU T in 862. In general PESQ is based on comparing the original waveform from the caller against the final one received by the callee ranging in value from 0 bad quality to 4.5 best quality . A PESQ value of 4 and above is generally considered to be good voice quality.

When the quality of performance is deemed acceptable control passes to step . For example a PESQ value of 4 or above for at least 90 of the tasks in the domain of interest could be considered to be acceptable performance. Average task response time below a certain threshold may also be considered to be acceptable performance. It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments in which the domain s of interest is of another type or that use other metrics of performance quality or combinations of metrics in step or that use another level of acceptability.

At step hypervisor calculates a plurality of metrics for the earlier execution cycle C. The metrics and step are described in more detail below and in the accompanying figures.

At step hypervisor establishes a threshold value TH that is associated with each of the metrics calculated at step . The threshold is established based on the characteristics of the domains that run in data processing system . In the preferred embodiments the relevant thresholds are developed through averaging past results from a configuration when the domain of interest runs by itself with acceptable quality of performance e.g. voice quality. The threshold value TH that is associated with each metric is then set to be 15 higher or lower depending on the type of metric from the prior run average which represents an idealized model. It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments that determine thresholds in a different manner such as by trial and error by a figure other than 15 or by other heuristics.

At step hypervisor tunes the parameters of scheduler for use in the present execution cycle C wherein j i based on the values of relevant metrics and their respective thresholds. Step is described in more detail below and in the accompanying figures.

At step under specified conditions of acceptable quality of performance in execution cycle C hypervisor rolls back some or even all of the parameters of scheduler that were previously tuned. The specified conditions are at the discretion of the implementers of data processing system . For example when traffic from a domain of interest has dropped significantly e.g. voice traffic has dwindled the tuned parameters may no longer be necessary to achieve acceptable quality of performance. For example a system administrator may be observing other changed conditions that warrant a roll back. It is to be understood that acceptable quality of performance as determined in step does not force a roll back of tuned parameters unless specified conditions of step are satisfied. When the specified conditions of acceptable quality are not satisfied hypervisor does not modify the parameters and settings that were utilized by scheduler in the earlier execution cycle C and retains those same parameters and settings for the present execution cycle C.

At step a per domain do loop begins for each domain d running on data processing system during execution cycle C.

At step a nested per priority do loop begins for each priority region p across all queues in data processing system during execution cycle C. Thus in regards to a priority p for a given domain d the relevant metrics are cumulative across all the queues in data processing system .

At step hypervisor calculates a total time metric that is the total amount of time during execution cycle C that was spent executing all tasks of domain d with a priority p.

At step hypervisor calculates a timeslice metric that is the average amount of time during execution cycle C that was spent executing a task of domain d with a priority p. This metric is calculated across all processors in data processing system .

At step hypervisor calculates a number of latency metrics for tasks of domain d with priority p during execution cycle C which are calculated across all processors in data processing system . A total latency metric is the total amount of time during execution cycle C that a task of domain d with a priority p waited in queue before being executed on a processor. A credit latency metric is the average amount of time during execution cycle C that a task of domain d with a priority p waited in queue before being allocated more credit by scheduler . An average latency metric is the average amount of time during execution cycle C that a task of domain d with a priority p waited in queue before being executed on a processor.

At step hypervisor calculates a count metric that is the number of times during cycle C that a task of domain d with a priority p entered queue . After step is executed the per priority nested do loop continues with step when this do loop is exhausted the per domain do loop continues with step when this do loop is exhausted control passes to step .

As noted earlier it will be clear to those having ordinary skill in the art how to make and use alternative embodiments in which hypervisor is not generating the metrics calculations and in which the metrics are calculated outside of hypervisor . It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments that calculate a subset of these metrics or other metrics that are consistent with the characteristics of the domain s of interest.

At step the total time metric of over priority tasks of the domain of interest d designated Total Time is compared to its corresponding threshold. When the value of the metric exceeds its corresponding threshold value the credit parameter associated with domain d is adjusted for the present execution cycle C. The credit allocated to domain d is increased meaning that domain d will get a larger share of execution time.

The reason for this adjustment is that the relatively high value of the total time metric for over priority tasks of domain d means that too many of domain d s tasks entered a queue with over priority i.e. having over spent their allocated credits. To alleviate the over spending more credits are allocated to domain d in the present execution cycle C.

At step the total latency metric of over priority tasks of the domain of interest d designated Total Latency is compared to its corresponding threshold. When the value of the metric exceeds its corresponding threshold value the credit parameter associated with domain d is adjusted for the present execution cycle C. The credit allocated to domain d is increased meaning that domain d will get a larger share of execution time.

The reason for this adjustment is that the relatively high value of the latency metric for over priority tasks of domain d means that altogether domain d s tasks waited too long in the over priority region of a queue i.e. after having over spent their allocated credits. To alleviate the over spending and too long wait time more credits are allocated to domain d in the present execution cycle C. It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments in which one of the other latency metrics is applied here e.g. Credit Latencyor Average Latencyor a combination of latency metrics.

At step a combination of Countand one or more of the disclosed latency over priority metrics i.e. Total Latency Credit Latency and Average Latency are considered and compared to their respective thresholds. When one or more thresholds are exceeded the credit parameter associated with domain d is adjusted for the present execution cycle C. The credit allocated to domain d is increased meaning that domain d will get a larger share of execution time. The combination of metrics to be considered in this step is at the discretion of the implementers of data processing system . It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments in which other additional metrics are brought to bear at this point to determine whether and by how much to increase the credit parameter associated with domain d.

At step the timeslice metric of under priority tasks of the domain of interest d designated Timeslice is compared to its corresponding threshold. When the value of the metric is below its corresponding threshold value processor pinning is established for domain d in the present execution cycle C. In the preferred embodiments processor pinning means that i the tasks of domain d are scheduled exclusively to a first processor and ii the tasks of the non d domains are scheduled to other processor s of data processing system i.e. away from the first processor. It should be noted that pinning is generally not a default setting in prior art credit schedulers because it is not work conserving i.e. pinning may leave the processor under utilized. However pinning is desirable for assuring sufficient processor resources for the domain of interest d.

The reason for pinning under the conditions of step is that the relatively low value of the timeslice metric for under priority tasks of domain d means that on average domain d s tasks spent too short a time being executed on a processor even though they under spent their credits probably because they were pre empted by other tasks such as boost priority tasks possibly from a non d domain. To give domain d s tasks sufficient processing time domain d is pinned to a first processor. To assure exclusivity to domain d tasks from non d domains are scheduled to other processors but not to the first processor.

It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments with a non exclusive pinning policy in which some tasks of non d domains are scheduled to the first processor even when all tasks of domain d are scheduled to the first processor. In some embodiments non exclusive pinning is achieved through bin packing as shown at step .

At step when there is more than one domain of interest to consider that might benefit from pinning bin packing is employed. The illustrative domains under consideration are d and d. The timeslice metric of under priority tasks of domain d designated Timeslice hd d under is compared to its corresponding threshold. The timeslice metric of under priority tasks of domain d designated Timeslice is compared to its corresponding threshold. When both metrics are below their respective thresholds both domains would benefit from some form of pinning. However in the illustrative embodiment exclusive pinning of both domains would leave no processors available for any other domains in the system. Therefore non exclusive pinning is applied. Bin packing enables some load balancing among the available processors to prevent non exclusive pinning from over using one processor.

For each of the tasks of domains d and d arriving at hypervisor in temporal order T. . . T hypervisor determines a runtime parameter that is associated with task T. In the illustrative embodiment accompanies each task Twhen it arrives but it will be clear to those skilled in the art after reading the present disclosure how to make and use alternative embodiments in another manner of the implementers choosing. For example can be calculated upon the arrival of each task or can be re calculated based on other considerations.

Hypervisor then bin packs the tasks of domains d and d. Bin packing techniques are well known in the art. In regards to the particular problem of multi processor scheduling one way of phrasing the multi processor scheduling problem in terms of bin packing concepts is Given a set J of jobs where job jhas length land a number of processors m what is the minimum possible time required to schedule all jobs in J on m processors such that none overlap In some illustrative embodiments the bin packing is directed at minimizing the difference in expected utilization among processors for executing the tasks of domains d and d i.e. load balancing the processing of these domains. Thus the bin packing in accordance with some illustrative embodiments aims to equalize the percentage of time in an execution cycle that each processor in data processing system is to spend executing domains d and d. For purposes of this disclosure the percentage of time in an execution cycle that a processor m is to spend executing time sensitive tasks in an execution cycle is defined as wherein m is an integer. Accordingly in some embodiments based on a data processing system comprising two processors i.e. processor and processor the bin packing in step is directed at creating at least one bin pack for each of the two processors. Thus in some embodiments for processor is the sum of the runtime parameters of the tasks in a first bin pack taken as a percentage of the total processing capacity of processor . Likewise in some embodiments for processor is the sum of the runtime parameters of the tasks in a second bin pack taken as a percentage of the total processing capacity of processor .

It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments in which the bin packing in step is based on other considerations such as minimizing the number of processors in data processing system that are to be used for processing time sensitive tasks or minimizing the expected execution time of the tasks in each bin pack or a combination of considerations or other considerations. For example when the objective is to minimize the number of processors allocated to bin packed tasks one way of stating the problem is that objects of different volumes must be packed into a finite number of bins of capacity V in a way that minimizes the number of bins used. Accordingly in some embodiments based on a data processing system comprising two processors i.e. processor and processor the bin packing is directed at bin packing as many domains as need non exclusive pinning such that at most m but as few as possible processors receive bin packed tasks to execute. It will be clear to those having ordinary skill in the art how to implement bin packing in data processing system in accordance with the bin packing optimization objectives sought to be achieved.

It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments in which tasks are grouped together other than by bin packing.

At step the boost with event policy is executed meaning that a queued task that receives an I O event is immediately moved to the boost region of a queue regardless of what region the task occupied when it received the I O event. It should be noted that prior art credit schedulers do not boost tasks that are outside the idle priority region . By contrast to the prior art the enhanced hypervisor according to the illustrative embodiments applies boost with event to tasks in under priority region . In other embodiments enhanced hypervisor also applies boost with event to tasks in over priority region . In other embodiments enhanced hypervisor applies boost with event to tasks in over priority region but not to tasks in under priority region .

 Boost with event is applied in the illustrative embodiments to time sensitive tasks and to tasks from a domain that is designated as time sensitive but the invention is not so limited. The rationale of the boost with event policy is that time sensitive domains such as voice processing applications tend to be I O intensive. The boost priority increases the probability that time sensitive tasks will be rapidly executed regardless of their credit history. Thus the boost with event policy as implemented in the enhanced hypervisor seeks to decrease the average latency of the boosted tasks.

In some illustrative embodiments the timeslice metric for boost priority tasks of the domain of interest d designated Timeslice is compared to its corresponding threshold. When the value of the metric is below its corresponding threshold value the boost with event policy is established for execution cycle C. It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments that apply boost with event to time sensitive tasks regardless of the value of Timeslice. In such embodiments boost with event does not require the calculation of Timesliceand accordingly conserves some computing resources.

It will be clear to those having ordinary skill in the art after reading the present disclosure how to make and use alternative embodiments in which the credits are allocated indirectly by adjusting the weight associated with domain d. In general in alternative embodiments wherein parameter tuner resides outside hypervisor the credit adjustments are achieved through weight adjustments submitted by parameter tuner to scheduler which translates the weight s into the appropriate adjustments to the credit allocations. For example a privileged domain running in data processing system designated Dom could transmit the weight adjustments to scheduler . Likewise in regards to the pinning and boost with event policies when parameter tuner resides outside hypervisor the policy modifications according to the present invention are transmitted to scheduler indirectly such as by the privileged domain Dom and are translated by scheduler into appropriate local parameters and settings.

At step absent the conditions set forth in steps hypervisor does not modify the parameters and settings that were utilized by scheduler in the earlier execution cycle C and retains those same parameters and settings for the present execution cycle C.

It will be clear to those having ordinary skills in the art after reading the current disclosure how to make and use alternative embodiments in which the metrics disclosed herein are differently defined and calculated based on other considerations that are relevant to the domains of interest to the implementers. For example a timeslice calculation could be based on a running average over an extended time interval rather than a single execution cycle.

At step monitor receives from scheduler a notification of an event that occurred in data processing system . The event notification is accompanied by relevant parameters associated with the event. For example every time a task enters a queue an event is reported by scheduler to monitor . The event information is accompanied by attributes such as the domain that comprises the task the priority region where the task was placed the time that the task entered the queue etc. For example when a task is executed an event is reported to monitor . The event information is accompanied by attributes such as the domain that comprises the task the processor identifier the priority region from which the task was selected for execution the duration of execution etc.

At step monitor records each event and the associated information via events recorder . The recorded data is available for data analysis at metrics analyzer and can be made available to users via user interface .

At step the monitoring steps described above are repeated for every task of every domain throughout the present execution cycle.

Some illustrative embodiments comprise a media server running on a credit scheduler based platform that hosts a virtualized enterprise Internet Protocol telephony system with multiple virtual machines including virtual machines performing call signaling media processing and CPU intensive tasks. Such applications are highly I O intensive and at the same time they also need CPU cycles to process media packets. An exemplary media server operates with a traffic load of 4 calls per second using the well known G.711 codec with a call hold time of 30 seconds for a maximum of 240 streams 120 callers and 120 callees incoming into the media server. The voice quality is measured by PESQ. When the exemplary media server operates with a sampling rate of 1 in 4 calls significant performance benefits were observed using the techniques of the present invention. In particular from a situation when more than 50 of the media streams had poor PESQ quality operating on a prior art system the methods according to the present invention resulted in a situation where almost no sampled media stream had a PESQ below 4.0.

It is to be understood that the disclosure teaches just some examples of the illustrative embodiments and that many variations of the invention can easily be devised by those skilled in the art after reading this disclosure and that the scope of the present invention is to be determined by the following claims.

