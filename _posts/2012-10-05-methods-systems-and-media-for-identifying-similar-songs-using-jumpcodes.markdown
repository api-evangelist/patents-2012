---

title: Methods, systems, and media for identifying similar songs using jumpcodes
abstract: Methods, systems, and media for identifying similar songs using jumpcodes are provided. In some embodiments, methods for a cover song from a query song are provided, the methods comprising: identifying a query song jumpcode for the query song, wherein the query song jumpcode is indicative of changes in prominent pitch over a portion of the query song; identifying a plurality of reference song jumpcodes for a reference song, wherein each of the reference song jumpcodes is indicative of changes in prominent pitch over a portion of the reference song; determining if the query song jumpcode matches any of the plurality of reference song jumpcodes; and upon determining that the query song jumpcode matches at least one of the plurality of reference song jumpcodes, generating an indication that the reference song is a cover song of the query song.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09384272&OS=09384272&RS=09384272
owner: The Trustees of Columbia University in the City of New York
number: 09384272
owner_city: New York
owner_country: US
publication_date: 20121005
---
This application claims the benefit of U.S. Provisional Patent Application No. 61 543 739 filed Oct. 5 2011 which is hereby incorporated by reference herein in its entirety.

The disclosed subject matter relates to methods systems and media for identifying similar songs using jumpcodes.

The capability to automatically identity similar songs is a capability with many applications. For example a music lover may desire to identify cover versions of a song in order to enjoy other interpretations of that song. As another example copyright holders may want to be able to identify different versions of their songs copies of their songs etc. in order to insure proper copyright license revenue. As yet another example users may want to be able to identify songs with similar sound to a particular song. As still another example a user listening to a particular song may desire to know the identity of the song or artist performing the song.

While it is generally easy for a human to identify two songs that are similar automatically doing so with a machine is much more difficult. For example the two songs can be played in a different key such that conventional fingerprinting is not accurate. As another example the two songs can be played at different tempos. As yet another example a performer playing a cover version may add remove or rearrange parts of the song. All of this can make it hard to identify a cover version of a song. With millions of songs readily available having humans compare songs manually is practically impossible. Therefore there is a need for mechanisms that can automatically identify similar songs.

Methods systems and media for identifying similar songs using jumpcodes are provided. In accordance with some embodiments methods for identifying a cover song from a query song the methods comprising identifying using a hardware processor a query song jumpcode for the query song wherein the query song jumpcode is indicative of changes in prominent pitch over a portion of the query song identifying using the hardware processor a plurality of reference song jumpcodes for a reference song wherein each of the reference song jumpcodes is indicative of changes in prominent pitch over a portion of the reference song determining using the hardware processor if the query song jumpcode matches any of the plurality of reference song jumpcodes and upon determining that the query song jumpcode matches at least one of the plurality of reference song jumpcodes generating using the hardware processor an indication that the reference song is a cover song of the query song.

In accordance with some embodiments systems for identifying a cover song from a query song the systems comprising a hardware processor that identities a query song jumpcode for the query song wherein the query song jumpcode is indicative of changes in prominent pitch over a portion of the query song identifies a plurality of reference song jumpcodes for a reference song wherein each of the reference song jumpcodes is indicative of changes in prominent pitch over a portion of the reference song determines if the query song jumpcode matches any of the plurality of reference song jumpcodes and upon determining that the query song jumpcode matches at least one of the plurality of reference song jumpcodes generates an indication that the reference song is a cover song of the query song.

In some embodiments non transitory computer readable media containing computer executable instructions that when executed by a processor cause the processor to perform a method for identifying a cover song from a query song the method comprising identifying a query song jumpcode for the query song wherein the query song jumpcode is indicative of changes in prominent pitch over a portion of the query song identifying a plurality of reference song jumpcodes for a reference song wherein each of the reference song jumpcodes is indicative of changes in prominent pitch over a portion of the reference song determining if the query song jumpcode matches any of the plurality of reference song jumpcodes and upon determining that the query song jumpcode matches at least one of the plurality of reference song jumpcodes generating an indication that the reference song is a cover song of the query song.

In accordance with various embodiments mechanisms for identifying similar songs using jumpcodes are provided. These mechanisms can be used in a variety of applications. For example cover songs of a query song can be identified. A cover song can include a song performed by one artist that is a version of a song performed by another artist or the same artist at a different time. As another example similar songs e g. two songs with similar sounds whether unintentional e.g. due to coincidence or intentional e.g. in the case of sampling copying or through the creation of a derivative work such as a song parody can be identified. As yet another example different songs with common distinctive features can be identified e.g. songs from a similar performer the same performer a similar style etc. for recommending songs to a user by identifying features of a query song. As a still further example a song being played can be identified e.g. the mechanisms described herein can allow a user to identify the name of a song on the radio or the name of a song being played live by the original performer another performer such as a cover band .

In some embodiments these mechanisms can receive a song or a portion of a song. For example songs can be received from a storage device from a microphone or from any other suitable device or interface. Beats in the song can then be identified. By identifying beats in the song variations in tempo between two songs e.g. between an original recording and a cover can be normalized. Beat level descriptors in the song can then be generated using any suitable techniques as described below in connection with for example. It should be noted that references to song herein are intended to encompasses a full song as well as portions of a song.

In some embodiments chroma vectors can be extracted from the song in accordance with musical segments of the song or based on the time periods. A chroma vector can be characterized as having a bin that corresponds to each of twelve semitones e.g. piano keys within an octave formed by folding all octaves together e.g. putting the intensity of semitone A across all octaves in the same semitone bin I putting the intensity of semitone B across all octaves in the same semitone bin putting the intensity of semitone C across all octaves in the same semitone bin etc. . The semitone bins of a chroma matrix can be numbered from one to twelve such that the lowest pitched semitone can be labeled as bin and the highest pitched semitone can be labeled as bin . These chroma vectors can then be averaged over each beat to create a beat level feature array of beat synchronized chroma vectors.

Chroma vectors can be extracted from a song from a portion of a song or from any portion of audio in any suitable way. For example in some embodiments an application such as The Echo Nest analyzer API available at the web page of The Echo Nest e.g. the.echonest.com can be used to extract chroma vectors among other information from a portion of audio such as a song or a portion of a song. In some embodiments the processes described below in connection with can be used to extract beat synchronized chroma vectors from a portion of audio.

In some embodiments the beat synchronized chroma vectors can be normalized. As described below any suitable technique can be used to normalize chroma vectors.

In some embodiments normalized beat synchronized chroma vectors can be averaged over two successive beats to reduce the amount of information to be processed by reducing the number of chroma vectors used to represent a song by half.

In some embodiments landmarks can be found from an array of normalized and or averaged beat synchronized chroma vectors e.g. a normalized beat synchronized chroma matrix . Landmarks can represent prominent pitch information from a chroma vector. For example if a semitone corresponding to bin is a prominent semitone in a particular chroma vector this can be set as a landmark. In some embodiments there can be more than one landmark allowed at each time frame e.g. at each beat averaged over two beats etc. . In some embodiments the position of a landmark can be specified by a time chroma coordinate time chroma . For example a landmark located in bin at time frame can be specified as a landmark at . An example of a process for finding landmarks from a beat synchronized chroma matrix is described below in connection with .

In some embodiments the extracted landmarks can be used to create jumpcodes that are indicative of changes in landmarks over a window of time frames and or beats in a portion of audio such as a song or a portion of a song. In some embodiments jumpcodes from a query song can be compared to jumpcodes from one or more reference songs to determine a similarity between the query song and the reference song. For example a number of jumpcodes common to both a query song and a reference song can be calculated and a song that contains common jumpcodes can be presented as a similar song.

As used herein a jumpcode can specify changes in prominent pitch of a song over a small portion of the song e.g. two beats four beats five beats etc. of the song from an initial prominent pitch at the beginning of the small portion of the song.

Turning to an example of a process for comparing a query song or portion of a query song to one or more reference songs in accordance with some embodiments is shown. As shown process can include extracting a chroma matrix from a song at generating a beat synchronized chroma matrix from the extracted chroma matrix at setting and storing landmarks based on the beat synchronized chroma matrix at calculating and storing jumpcodes in a song database at at comparing jumpcodes calculated at to jumpcodes stored at and outputting results of the comparison that indicate whether reference songs stored in database are similar to the query song.

As shown at a chroma matrix can be extracted from a song . The chroma matrix can be extracted using any suitable technique such as by using The Echo Nest analyzer API using the processes described in connection with or any other suitable technique.

At a vat synchronized chroma matrix can be generated from the chroma matrix extracted at . In some embodiments generating a beat synchronized chroma matrix can include averaging chroma vectors over each beat to create beat synchronized chroma vectors. Additionally or alternatively generating a beat synchronized chroma vectors can include normalizing beat synchronized chroma vectors using any suitable technique. As one example the techniques described in connection with can be used to normalize the beat synchronized chroma vectors.

In some embodiments at successive beat synchronized chroma vectors can be averaged together. This can allow for the amount of information used to represent a song to be decreased by reducing the number of chroma vectors used to represent the song.

At landmarks can be set and stored based on the chroma vectors in the beat synchronized chroma matrix generated at in some embodiments landmarks can be indicative of a prominent chroma bin at a specific point in time. For example a threshold can be set based on prominent chroma bins in some portion of a song and this threshold can be used in determining other prominent chroma bins in other portions of the song.

In some embodiments landmarks can be set on both a forward pass through a song e.g. from the beginning of the song toward the end and on a backward pass through the song e.g. from the end of the song toward the beginning . Additionally the intersection of the landmarks set on the forward pass and landmarks set on a backward pass can be stored as landmarks. In such an embodiment landmarks that are not common to both the forward pass can be ignored discarded etc.

At jumpcodes for the song can be calculated using the landmarks stored at . In some embodiments jumpcodes can e calculated by determining a difference between successive landmarks within a time window. The time window can then be moved to a different portion of the song e.g. moved forward through the song and additional jumpcodes can be calculated for the landmarks within the moved time window. This can be repeated until the time window is moved through the entire song and jumpcodes are calculated for each position of the time window.

In some embodiments when jumpcodes are calculated for a song and there are duplicate jumpcodes e.g. more than one of the same jumpcode is calculated for different portions of the song a weight can be calculated fin the duplicate jumpcodes based on the number of duplicate jumpcodes in the song.

In some embodiments the extracted jumpcodes or the weighted extracted jumpcodes for a reference song and or a query song can be stored in a database . In some embodiments the position of specific jumpcodes in a song may not be preserved when the jumpcodes are stored in database . Additionally or alternatively the key of the stored jumpcodes can be transposed by rotating the jumpcodes about the chroma axis. The jumpcodes for each song can be stored in database in the original key of the song and in each of the eleven other keys represented in the twelve se In tones of the chroma vectors after transposition of the jumpcodes into those eleven other keys. Any suitable technique can be used to transpose the jumpcodes into other keys such as the techniques described in connection with .

At extracted jumpcodes from song or a portion of song can be compared to jumpcodes of other songs or portions of songs such as jumpcodes previously stored in database . The results of the comparison can be presented at in any suitable fashion such as presentation on a display of a computing device.

In some embodiments the mechanism described herein can include a process shown in that takes a chroma matrix for a song as an input and either adds jumpcodes calculated from the chroma matrix to database and or compares jumpcodes calculated from the chroma matrix to jumpcodes stored in database .

At a beat synchronized chroma matrix can be venerated for the chroma matrix from a reference song . In some embodiments the beat synchronized chroma matrix can be generated in accordance with the techniques described in connection with .

In some embodiments chroma matrix can be received as a beat synchronous chroma matrix. In such an embodiments can be omitted and process can proceed to .

At landmarks can be set and stored based on a reference song beat synchronized chroma matrix. In some embodiments the landmarks can be set and stored based on the reference song beat synchronized chroma matrix generated at . In some embodiments landmarks can be set and stored in accordance with the techniques described in connection with . In some embodiments the landmarks can be stored in a database in association with the song that the landmarks are derived from. Additionally or alternatively the landmarks can be stored in memory for use in generating jumpcodes. In some embodiments landmarks can be set and stored in accordance with the techniques described in connection with .

At jumpcodes can be calculated and weighted from landmarks for a reference song being analyzed. In some embodiments jumpcodes can be calculated and or weighted in accordance with the techniques described herein. More particularly jumpcodes can be calculated from landmarks and or can be weighted in accordance with the techniques described in connection with and .

At jumpcodes and weights associated with the jumpcodes can be stored in a database as reference song jumpcodes. In some embodiments the jumpcodes that are calculated at can be transposed into other keys using techniques described herein. For example the jumpcodes can be transposed using the techniques described in connection with . These transposed jumpcodes can also be stored in a database in association with the reference song jumpcodes from which they were derived. As described below in connection with each jumpcode stored in the database can be stored as a single value calculated using a hash function with the jumpcode as an input. This can allow for direct comparison between different jumpcodes by comparing a single value rather than a list of values. Further it can allow for the jumpcodes to be stored in the database efficiently.

In some embodiments weighted jumpcodes can be stored in a database in association with identification information of the audio from a song that the jumpcodes correspond to. For example the weighted jumpcodes can be stored in a database along with a corresponding identifier. In a more particular example in the case of a known song the artist title song writer etc. can be stored in association with the weighted jumpcodes. In another example a URL of a video of an unknown song can be stored in association with the weighted jumpcodes. In yet another example identifying information about an unknown song such as a source and or a location of the unknown song can be stored in association with the jumpcodes corresponding to the unknown song.

In some embodiments weighted jumpcodes can be extracted from a collection of known songs. For example a content owner e.g. a record company a performing rights organization etc. can extract and weight jumpcodes from songs in the content owner s collection of songs and store re the weighted jumpcodes in a database. In such an example information e.g. title artist song writer etc. identifying the songs can be associated with the weighted jumpcodes.

In some embodiments the order in time of the jumpcodes in a song is not preserved. Because the specific order of the jumpcodes is not preserved an identification of songs that contain similar portions to a query song based on a comparison of jumpcodes can be independent of the order in which the jumpcodes are generated from the query song and the reference song. For example the identification can recognize a query song that is a medley containing a part of a reference song as being a version of the reference song. In another example a reference song that contains parts of a query song arranged in a different order than those parts are arranged in the query song can be recognized as being a version of the query song.

Process can begin by calculating query song jumpcodes from a chroma matrix for a query song at which can be performed as described above in connection with of process for calculating jumpcodes of a reference song.

At weighted jumpcodes calculated at can be compared to the reference song jumpcodes that were stored in the database at using process . Additionally the jumpcodes of the query song can be compared to transposed jumpcodes stored in the database at in some embodiments.

In some embodiments all jumpcodes in the database that match at least one jumpcode from the query song can be determined along with the weight corresponding to each of the matching reference jumpcodes.

At reference songs that are considered similar o the query song can be determined based on the number of reference song jumpcodes from each reference song that match one of the jumpcodes from the query song. In some embodiments a weighted jumpcode from a reference song can be determined to match a jumpcode from the query song if the weight of the reference jumpcode is within a window around the weight of the query song jumpcode. For example a reference jumpcode can be considered to match a query jumpcode if the weight of the reference jumpcode w meets the following conditions 1 w

In some embodiments all matching jumpcodes can be determined and the number of jumpcodes associated with a particular reference song that match jumpcodes in the query song can be calculated. A total number of matching jumpcodes from a reference song can be used to rank the reference songs according to how similar they are to the query song. For example a reference song with more matching jumpcodes can be considered more similar to the query song and therefore can be ranked higher than a reference song with less matching jumpcodes. In some embodiments the mechanisms described herein can identify a reference song as matching when the reference song has a threshold number of matching jumpcodes. For example if a reference song shares fifty percent or 75 or any other suitable threshold or more of the same jumpcodes as the query song the mechanisms described herein can identify the reference song as being a matching reference song.

When reference song jumpcodes corresponding to some portion of reference songs stored in the database including all of the reference songs have been compared at to the query song jumpcodes the results of the comparison can be output at .

In some embodiments the processes of can be used together to identify similar songs using process by comparing query song jumpcodes to jumpcodes stored in a database in accordance with process . For example a content distributor e.g. a radio station a music retailer etc. can extract and weight jumpcodes from songs made available by the content distributor. In such an example information e.g. title artist song writer etc. identifying the songs can be associated with the weighted jumpcodes. Further a query song can be input and songs similar to the query song that are available from the content distributor can be recommended to a user.

In some embodiments weighted jumpcodes can be extracted from a collection of unknown songs. For example a user can calculate and weight jumpcodes from soundtracks to videos uploaded by the user and or other users to a video sharing Web site YOUTUBE . In such an example information identifying the source of the soundtrack. e.g. a URL a username a reference number etc. can be associated with the weighted jumpcodes. The information identifying the source of the soundtracks and associated soundtrack jumpcodes can be used to create a collection of unknown songs. A user can then input a query song and search for different versions of the query song by comparing query song jump odes to the soundtrack jumpcodes associated with the collection of unknown songs.

At chroma vectors corresponding to each music event can be generated or received and the song can be partititioned into beats. An example of a musical event can include each time there is a change in pitch in the song. In some embodiments whenever there is a musical event a chroma vector can be calculated. Musical events can happen within a beat or can span beats. In some embodiments the chroma matrix can already be partitioned into beats for example by The Echo Nest analyzer API. Other techniques for partitioning a chroma matrix into beats are described below with reference to .

At chroma vectors received at can be averaged over each beat to obtain beat synchronized chroma vectors. Any suitable technique can be used to average chroma vectors over a beat including techniques for averaging chroma vectors described below in connection with .

At the beat synchronized chroma vectors can be normalized. In some embodiments the value in each chroma bin of a beat synchronized chroma vector can be divided by the value in the chroma bin having a maximum value. For example if chroma bin of a particular chroma vector has a maximum value of chroma bins through then the value of each of chroma bins through can be divided by the value of chroma bin . This can result in the maximum value in a chroma bin being equal to one for the normalized beat synchronous chroma vectors. At normalized beat synchronous chroma vectors can be averaged over pairs of successive beats to obtain a beat synchronized chroma matrix for the chroma matrix and a beat synchronized chroma matrix can be output for use by the identification application.

Turning to an example of a process for generating landmarks for a song from a beat synchronized chroma matrix in accordance with sonic embodiments is illustrated. At an initial threshold vector T where i 1 . . . 12 can be found. In some embodiments the initial value for each value of the vector Tcan be the maximum in each chroma dimension e.g. in each chroma bin over the first n time frames e.g. over the first ten time frames where a time frame is representative of one or more beats. For example the initial threshold vector Tcan take the maximum value in each chroma dimension i over the first ten time frames where each time frame is two beats long. In accordance with some embodiments process can be used to set and store landmarks at and or .

At a first time frame of the beat synchronized chroma matrix can be set as a current time frame t. At a chroma bin c can be set as a landmark if the chroma value v in bin c is above threshold T and the landmark at the location of chroma bin c at time t can be stored. For example in a first time frame a chroma bin can be set as a landmark if the value v in chroma bin is over threshold T.

In some embodiments the landmarks identified at can be stored and the location of the stored landmarks within the song can be identified with a set of coordinates e.g. time chroma where time is the time frame location of the landmark and chroma is the chroma bin of the landmark. Additionally in some embodiments a value v can be stored with the landmark for use in deciding which landmarks to use in a case where over a maximum number of landmarks were identified at any one time frame.

At the threshold vector can be updated based on a landmark set at time t. More particularly the threshold vector can be updated such that T v where v is the value in bin c at time frame t. Returning to the preceding example if chroma bin is set as a landmark at time frame one the threshold vector can be updated by setting the value at dimension equal to the value of chroma bin at time frame one as follows T v. The threshold vector value for any chroma dimension that is not set as a landmark can be left unchanged.

At the current time frame t can be moved to time frame t 1 and the threshold vector can be set for time frame t 1 as follows T T where is a decay factor. In some embodiments can be set any suitable value e.g. 0.95 0.995 etc. where a decay value closer to one may result in less chroma bins being set as landmarks.

At it can be determined if the current time frame t is the final time frame of the pass. For example if the current pass is a forward pass that started at the first time frame at it can be determined whether the current time frame is the last time frame of the beat synchronized chroma matrix . If the current time frame is not the last time frame NO at process can return to . Otherwise if the current time frame is the last time frame YES at process can proceed to .

At it can be determined whether landmarks have been stored for a forward pass and a backward pass of the beat synchronized chroma matrix . If landmarks have not been stored for both a forward pass and a backward pass NO at the identification application can proceed to . Otherwise if landmarks have been stored for both a forward and backward pass YES at the identification application can proceed to .

At last frame of the beat synchronized chroma matrix can be set as a current frame and a backward pass of the beat synchronized chroma matrix can be initiated to set landmarks for a backward pass. Process can return to to complete the backward pass and process can proceed to when landmarks have been stored for both a forward and a backward pass as determined at .

At the landmarks set on the forward pass and the landmarks set on the backward pass can be compared and the intersection can be kept. More particularly landmarks that were stored on both the forward pass and the backward pass can be stored as landmarks. In some embodiments landmarks that were stored on either the forward pass or the backward pass but were not set for both can be ignored or discarded. In some embodiments landmarks can be generated for the beat synchronized chroma matrix to be used in generating jumpcodes.

In some embodiments a maximum number of landmarks e.g. one two three etc. can be set such that no more than the maximum number of landmarks are kept for any one time frame. More particularly the most prominent landmarks up to the maximum number of landmarks at a particular time frame from among the landmarks identified in process can be kept. For example if the maximum number of landmarks is set as two and three landmarks are set at a particular time frame the two most prominent landmarks can be chosen and kept. In such an example the value v corresponding to each of the landmarks can be checked and the landmarks with the highest corresponding values can be kept as landmarks.

In some embodiments a jumpcode in a time window of size W can correspond to time differences where time can be measured synchronously in units of time e.g. in seconds milliseconds etc. or can be measured asynchronously in units of beats and semitone differences between landmarks in the time window of size W. The difference in semitones between two landmarks can be found by subtracting a chroma bin for a first landmark from a chroma bin for a second landmark. Because this can result in a negative number the result of the subtraction modulo can be used to ensure that the difference will end up in one of the chroma bins being used.

At it can be determined whether the time window of size W includes the last time frame of the song being analyzed. If the time window of size W does not include the last time frame NO at the time window of size W can be advanced toward the end of the song. Otherwise if the time window of size W includes the last time frame YES at process can proceed to where the jumpcodes can be hashed to create a single value corresponding to each jumpcode. In some embodiments the time window of size W can be advanced by a specified number of time frames at e.g. one time frame two time frames etc. between finding jumpcodes at .

Jumpcodes calculated at can be encoded using the difference in time and the difference in chroma bin of successive landmarks in a time window W. For example if there are three landmarks located at and in a time window IV where W in this example is at least five time frames a jumpcode for time window W can be calculated by finding the differences in location between the successive landmarks. In such an example a difference between landmark one and landmark two can be found by subtracting the time coordinate of landmark one from the time coordinate of landmark two as follows 201 200 1. As described above a difference in chroma bins can be found as follows 7 2 5. The time difference between the second landmark and the third landmark can be found similarly with the time difference being equal to three and the chroma bin being equal to eight as follows 3 7 8. Finally a jumpcode for the current position of the time window of size W can be specified by the initial chroma bin and the differences between the successive pairs of landmarks as follows 2 1 5 3 8 . Landmark pairs encoded as jumpcodes can provide information relating to musical objects such as chords and changes in the melody line.

In some embodiments where a single time frame in a time window of size W has more than one landmark one jumpcode can be calculated for each possible combination of landmarks involving each of the landmarks at each particular time frame within time window of size W. For example if size W is five and there are three time frames within time window of size W that each have one landmark with the other two time frames having zero landmarks then a single jumpcode can be calculated for the particular time frames within the time window of size W. In another example if size W is five and there are two time frames within time window of size W that have one landmark each and a single time frame that has two landmarks then two jumpcodes can be calculated for the particular time frames within the time window of size W.

At the jumpcodes calculated at for the landmarks can be hashed to create a single value H for each of the jumpcodes. Given a set of k time chroma landmarks t c t c . . . t c or using the notation above c t t c c . . . t t c c within a time window of size W arithmetic and delta coding can be used to store the jumpcode in a single value as follows 

At a value H for each of the jumpcodes calculated at and hashed at can be counted to determine how often each particular hash code appears in the song being analyzed. The counts of the each value of H can then be used to create a weight w for each value H where w is indicative of the frequency that the particular hash code appears in the song associated with the jumpcode in some embodiments w can be a number of times that the particular value H appears in the song divided by log base of the total number of jumpcodes in the song. Process can output weighted jumpcodes for the song being analyzed. Weighted jumpcodes can be stored in a database in association with an identification of the song that the jumpcodes were extracted from or can be used in a comparison with weighted jumpcodes stored in a database of known and or unknown songs.

Additionally each of the jumpcodes in a database can be transposed and the transposed jumpcodes can be stored in the database together with the original jumpcodes. This can allow for jumpcodes to be calculated as though the song was performed in another key. To transpose a jumpcode stored as a hash value into another key first the initial chroma value for the jumpcode cof equation 1 can be extracted using the modulo operation H. The result of the modulo operation to H can then be added to a rotation value between 0 and 11 modulo prior to recombination. More particularly the transposed value of H H can be found using the following formula 2 where T is the number of semitones that over which the song is to be transposed. As H c equation 2 can be rewritten as follows 3 In this way each song can be associated with jumpcodes in keys corresponding to each semitone. This can allow the identification application to identify songs that are a different version of the query song performed in a different key.

System can include one or more servers . Server can be any suitable server for providing access to or a copy of the application such as a processor a computer a data processing device or any suitable combination of such devices. For example the application can be distributed into multiple backend components and multiple frontend components or interfaces. In a more particular example backend components such as data collection and data distribution can be performed on one or more servers .

More particularly for example each of the computing devices and server can be any of a general purpose device such as a computer or a special purpose device such as a client a server etc. Any of these general or special purpose devices can include any suitable components such as a hardware processor which can be a microprocessor digital signal processor a controller etc. memory communication interfaces display controllers input devices etc. For example computing device can be implemented as a smartphone a tablet computer a personal data assistant PDA a personal computer a laptop computer a multimedia terminal a special purpose device a game console etc.

Referring back to communications network can be any suitable computer network including the Internet an intranet a wide area network WAN a local area network LAN a wireless network a digital subscriber line DSL network a frame relay network an asynchronous transfer mode ATM network a virtual private network VPN or any suitable combination of any of such networks. Communications links and can be any communications links suitable for communicating data between mobile devices and server such as network links dial up links wireless links hard wired links any other suitable communications links or any suitable combination of such links. Mobile devices can enable a user to execute the application that allows the features of the mechanisms to be used. Computing devices and server can be located at an suitable location.

System can also include a content owner server that can include hardware similar to server . Content owner server can be operated by for example a record company a copyright licensing organization etc. In some embodiments content owner server can use songs owned by the content owner or a party associated with the content owner such as an agent a copyright licensing organization etc. as query songs. Using the mechanisms described herein the content owner can automatically search for cover versions of the songs that are owned by the content owner. For example the content owner can search a database of songs available using content server . Content server can be a server or multiple servers that are part of a service e.g. YOUTUBE etc. allowing users to upload user generated content including content copied from another source by a user not only content created by a user . Using the mechanisms described herein can allow the content owner to search a database containing unknown songs for alternate versions of a song owned by the copyright owner.

In some embodiments a party providing a service associated with content saver can maintain a database of beat synchronized chroma matrices and or jumpcodes of songs uploaded to content server . Content server can then allow users to input a query song and the content server can identify different versions of the song and or similar songs to the user. This can be provided as part of a service to all users and or as a service to content owners and copyright licensing organizations such as BMI or ASCAP.

Hardware processor can use the computer program to present on display an interface that allows a user to interact with the application and to send and receive data through communication link . It should also be noted that data received through communications link or any other communications links can be received from any suitable source. In some embodiments hardware processor can send and receive data through communication link or any other communication links using for example a transmitter receiver transmitter receiver transceiver or any other suitable communication device. Input device can be a computer keyboard a cursor controller dial switchbank lever touchscreen or any other suitable input device as would be used by a designer of input systems or process control systems.

Server can include hardware processor display input device and memory which can be interconnected. In some embodiments memory can include a storage device for storing data received through communications link or through other links and also receives commands and values transmitted by one or more users. The storage device can further include a server program for controlling hardware processor .

In one particular embodiment the application can include client side software hardware or both. For example the application can encompass a computer program written in a programming language recognizable by the computing device executing the application e.g. a program written in a programming language such as Java C Objective C C C Javascript Visual Basic or any other suitable approaches .

In some embodiments the identification application with a user interface and mechanisms for identifying similar songs and other functions can be delivered to computing device and installed as illustrated in the example shown in . The computing device can be for example a mobile computing device such as a mobile phone or a tablet computer. At weighted jumpcodes for one or more reference songs can be stored in a database by server . In one example the extraction and weighting of the jumpcodes can be done by server . In another example the extraction and weighting of the jumpcodes can be performed using any suitable device and can be uploaded to server in any suitable manner. At the weighted jumpcodes stored at can be transmitted to computing device as part of the application for utilizing the mechanisms described herein. It should be noted that transmitting the application to the computing device can be done from any suitable device and is not limited to transmission from server . It should also be noted that transmitting the application to computing device can involve intermediate steps such as downloading the application to a personal computer or other device and or recording the application in memory or storage such as a hard drive a flash memory a SIM card a memory card or any other suitable device for temporarily or permanently storing an application.

Computing device can receive the application and weighted jumpcodes from server at . After the application is received at computing device the application can be installed and can be used to receive audio data for a query song at as described herein. The application executing on computing device can extract jumpcodes from the query song at in accordance with process and can compare the query jumpcodes to the reference jumpcodes in accordance with of process and determine if there is a match at in accordance with of process and generate and output results at in accordance with of process if matching songs are determined from the database YES at . If the identification application running on computing device determines that there are no matching jumpcodes e.g. the number of matching jumpcodes is not over a threshold the identification application can cause a message to be output that there were no similar songs in the database of reference songs.

In some embodiments the identification application with a user interface and mechanisms for receiving query song data e.g. audio data for a song or a portion of a song and transmitting query song data and other user interface functions can be transmitted to computing device e.g. a mobile computing device but the jumpcodes for the reference songs can be kept on server as illustrated in the example shown in . Similarly to the example in at extracted and weighted jumpcodes can be stored in a database in accordance with the mechanisms described herein. Server can transmit the identification application or at least a portion of the identification application to computing device at . Computing device can receive the application at and start receiving and transmitting query song data e.g. audio data for a query song to server at . In some embodiments audio data is transmitted to server . Additionally or alternatively chroma vectors a chroma matrix a beat synchronized chroma matrix weighted jumpcodes or any other suitable data about the query song can be received and or generated by computing device and can be transmitted to server at . Computing device can proceed to where computing device can receive a list of similar songs e.g. one or more songs identified as similar to the query song from server and proceed to .

At server can receive query song data e.g. audio weighted jumpcodes etc. from computing device extract weighted jumpcodes in accordance with of process if computing device has not already done so and compare the query song jumpcodes to reference song jumpcodes in accordance with of with process . Server can determine if there is a match between the query song jumpcodes and the reference song jumpcodes at in accordance with of process and if there are any acceptable matches e.g. number of weighted matching jumpcodes is over a threshold proceed to . If there is not a match at server can return to and continue to receive query song data transmitted from mobile device .

At server can generate an alert based on the presence of a match between the query song jumpcodes extracted at and reference song jumpcodes stored at and can transmit a list of similar reference songs to computing device this can include an indication that no similar songs were found among the reference songs . In some embodiments server can transmit audio and or video of the similar songs or a link to audio and or video of the similar songs at . After receiving and transmitting query song data at computing device can proceed to where it can be put into a state to receive a list of similar songs from the server and can move to to check if a list of songs has been received from server . If a list of similar songs has been received YES at computing device can proceed to where it can provide the list of similar songs to a user of the computing device in accordance with process and or process . If a list of similar songs has not been received NO at computing device can output a message to a user to inform the user that no similar songs were found and process can return to where it can receive and transmit query song data.

In some embodiments a hybrid process can combine conventional song fingerprinting and the mechanisms described herein. Such a hybrid process can be used to identify similar songs based on a query song. In such a hybrid process a user can record a portion of a song for identification using a mobile device for example. That portion can be used in a song fingerprinting process to attempt to find an close match to the query song among songs in a database. Song fingerprinting can include generating landmarks in a specific sequence e.g. preserving the timing of the landmarks and attempting to identify the song from the portion of the song being analyzed by matching the specific sequence of landmarks to a specific sequence of landmarks in a known song.

In a hybrid process a database of reference songs used for fingerprinting can include jumpcodes for each the reference songs that have been calculated in accordance with the processes described herein. In the hybrid process the portion of the recorded audio can be transmitted to a server for a fingerprinting analysis that can compare the query song to a database of songs. Alternatively a portion of the processing for fingerprinting analyses can be performed on a user s computing device to generate the landmarks used for fingerprinting.

The portion of audio can be used for fingerprinting and based on the fingerprinting it can be determined if any reference songs in the database match the query song. If a reference song matches the query song it can be determined if the matching song is a known song e.g. whether a title and artist for the song are known . If the title and artist for the matching song are unknown the jumpcodes stored in association with the song in the database can be used to search the database for similar songs using the mechanisms described herein.

In some embodiments if the song identified by fingerprinting is known and the song has previously been identified as a cover e.g. as a version of another song the original song can be associated in the database with the identified cover song.

In the hybrid process if there is no matching song in the database based on fingerprinting the mechanisms described herein can be used to calculate query song jumpcodes. These query song jumpcodes can then be compared the reference song jumpcodes as described herein. Using the hybrid system to first identify whether the jumpcodes for the query song have already been calculated and are available on the server the query song can be identified more quickly by eliminating the need to calculate jumpcodes for the query song.

In accordance with some embodiments in order to track beats for extracting and calculating chroma vectors all or a portion of a song can be converted into an onset strength envelope O t as illustrated in process in . As part of this process the song or portion of the song can be sampled or re sampled e.g. at 8 kHz or any other suitable rate at and then the spectrogram of the short term Fourier transform STFT calculated for time intervals in the song e.g. using 32 ms windows and 4 ms advance between frames or any other suitable window and advance at . An approximate auditory representation of the song can then be formed at by mapping to 40 Mel frequency bands or any other suitable number of bands to balance the perceptual importance of each frequency band. This can be accomplished for example by calculating each Mel bin as a weighted average of the FFT bins ranging from the center frequencies of the two adjacent Mel bins with linear weighting to give a triangular weighting window. The Mel spectrogram can then be converted to dB at and the first order difference along time is calculated for each band at . Then at negative values in the first order differences can be set to zero half wave rectification and the remaining positive differences can be summed across all of the frequency bands. The summed differences can then be passed through a high pass filter e.g. with a cutoff around 0.4 Hz and smoothed e.g. by convolving with a Gaussian envelope about 20 ms wide at . This gives a one dimensional onset strength envelope as a function of time e.g. O t that responds to proportional increase in energy summed across approximately auditory frequency bands.

In some embodiments the onset envelope for each musical excerpt can then be normalized by dividing by its standard deviation.

In some embodiments a tempo estimate for the song or portion of the song can next be calculated using process as illustrated in . Given an onset strength envelope O t autocorrelation can be used to reveal any regular periodic structure in the envelope. For example autocorrelation can be performed at to calculate the inner product of the envelope with delayed versions of itself. For delays that succeed in lining up many of the peaks a large correlation can occur. For example such an autocorrelation can be represented as 

Because there can be large correlations at various integer multiples of a basic period e.g. as the peaks line up with the peaks that occur two or more beats later it can be difficult to choose a single best peak among many correlation peaks of comparable magnitude. However human tempo perception as might be examined by asking subjects to tap along in time to a piece of music is known to have a bias towards 120 beats per minute BPM . Therefore in some embodiments a perceptual weighting window can be applied at to the raw autocorrelation to down weight periodicity peaks that are far from this bias. For example such a perceptual weighting window W can be expressed as a Gaussian weighting function on a log time axis such as 

By applying this perceptual weighting window W to the autocorrelation above a tempo period strength can be represented as 

Tempo period strength for any given period can be indicative of the likelihood of a human choosing that period as the underlying tempo of the input sound. A primary tempo period estimate can therefore be determined at by identifying the for which TPS is largest.

In some embodiments rather than simply choosing the largest peak in the base TPS a process of can be used to determine . As shown two further functions can be calculated at and by re sampling TPS to one half and one third respectively of its original length adding this to the original TPS then choosing the largest peak across both of these new sequences as shown below 2 0.5 2 0.25 2 1 0.25 2 1 7 3 0.33 3 0.33 3 1 0.33 3 1 8 

Whichever sequence 7 or 8 results in a larger peak value TPS or TPS determines at whether the tempo is considered duple or triple respectively. The value of or corresponding to the larger peak value is then treated as the faster target tempo metrical level at or with one half or one third of that value as the adjacent metrical level at or . TPS can then be calculated twice using the faster target tempo metrical level and adjacent metrical level using equation 6 at . In some embodiments at of 0.9 octaves or any other suitable value can be used instead of an of 1.4 octaves in performing the calculations of equation 6 . The larger value of these two TPS values can then be used at to indicate that the faster target tempo metrical level or the adjacent metrical level respectively is the primary tempo period estimate .

Using the onset strength envelope and the tempo estimate a sequence of beat times that correspond to perceived onsets in the audio signal and constitute a regular rhythmic pattern can be generated using process as illustrated in connection with using the following equation 

A property of the objective function C t is that the best scoring time sequence can be assembled recursively to calculate the best possible score C t of all sequences that end at time t. The recursive relation can be defined as 

This equation is based on the observation that the best score for time t is the local onset strength plus the best score to the preceding beat time that maximizes the sum of that best score and the transition cost from that time. While calculating C the actual preceding beat time that gave the best score can also be recorded as 

In some embodiments a limited range of can be searched instead of the full range because the rapidly growing penalty term F will make it unlikely that the best predecessor time lies far from t . Thus a search can be limited to t 2 . . . t 2 as follows 

To find the set of beat times that optimize the objective function for a given onset envelope. C t and P t can be calculated at for every time starting from the beginning of the range zero at via . The largest value of C which will typically be within of the end of the time range can be identified at . This largest value of C is the final beat instant t where N the total number of beats is still unknown at this point. The beats leading up to C can be identified by back tracing via P at finding the preceding beat time t P t and progressively working backwards via until the beginning of the song or portion of a song is reached. This produces the entire optimal beat sequence t .

In order to accommodate slowly varying tempos can be updated dynamically during the progressive calculation of C t and P t . For instance t can be set to a weighted average e.g. so that times further in the past have progressively less weight of the best inter beat intervals found in the max search for times around t. For example as C t and P t are calculated at t can be calculated as 1 13 where is a smoothing constant having a value between 0 and 1 e.g. 0.1 or any other suitable value that is based on how quickly the tempo can change. During the subsequent calculation of C t 1 the term F t can be replaced with F t to take into account the new local tempo estimate.

In order to accommodate several abrupt Changes in tempo several different values can be used in calculating C and P in some embodiments. In some of these embodiments a penalty factor can be included in the calculations of C and P to down weight calculations that favor frequent shifts between tempo. For example a number of different tempos can be used in parallel to add a second dimension to C and P to find the best sequence ending at time t and with a particular tempo . For example C and P can be represented as 

This approach is able to find an optimal spacing of beats even in intervals where there is no acoustic evidence of any beats. This filling in emerges naturally from the back trace and may be beneficial in cases in which music contains silence or long sustained notes.

Using the optimal beat sequence t the song or a portion of the song can next be used to generate a single feature vector per beat as beat level descriptors in accordance with of . These beat level descriptors can be used to represent both the dominant note typically melody and the broad harmonic accompaniment in the song or portion of the song e.g. when using chroma features as described below.

In some embodiments beat level descriptors are generated as the intensity associated with each of 12 semitones e.g. piano keys within an octave formed by folding all octaves together e.g. putting the intensity of semitone A across all octaves in the same semitone bin A putting the intensity of semitone across all octaves in the same semitone bin B putting the intensity of semitone C across all octaves in the same semitone bin C etc. .

In generating these beat level descriptors phase derivatives instantaneous frequencies of FFT bins can be used both to identify strong tonal components in the spectrum indicated by spectrally adjacent bins with close instantaneous frequencies and to get a higher resolution estimate of the underlying frequency. For example a 1024 point Fourier transform can be applied to 10 seconds of the song or the portion of the song sampled or re sampled at 11 kHz with 93 ms overlapping windows advanced by 10 ms. This results in 513 frequency bins per FFT window and 1000 FFT windows.

To reduce these 513 frequency bins over each of 1000 windows to 12 for example chroma bins per beat the 513 frequency bins can first be reduced to 12 chroma bins. This can be done by removing non tonal peaks by keeping only bins where the instantaneous frequency is within 25 or any other suitable value over three or any other suitable number adjacent bins estimating the frequency that each energy peak relates to from the energy peak s instantaneous frequency applying a perceptual weighting function to the frequency estimates so frequencies closest to a given frequency e.g. 400 Hz have the strongest contribution to the chroma vector and frequencies below a lower frequency e.g. 100 Hz 2 octaves below the given frequency or any other suitable value or above an upper frequency e.g. 1600 Hz 2 octaves above the given frequency or any other suitable value are strongly down weighted and sum up all the weighted frequency components by putting their resultant magnitude into the chroma bin with the nearest frequency.

As mentioned above in some embodiments each chroma bin can correspond to the same semitone in all octaves. Thus each chroma bin can correspond to multiple frequencies i.e. the particular semitones of the different octaves . In some embodiments the different frequencies f associated with each chroma bin i can be calculated by applying the following formula to different values of r 2 14 where r is an integer value representing the octave relative to ffor which the specific frequency fis to be determined r 1 indicates to determine ffor the octave immediately below 440 Hz N is the total number of chroma bins e.g. 12 in this example and fis the tuning center of the set of chroma bins e.g. 440 Hz or any other suitable value .

Once there are 12 chroma bins over 1000 windows in the example above the 1000 windows can be associated with corresponding beats and then each of the windows for a beat combined to provide a total of 12 chroma bins per beat. The windows for a beat can be combined in some embodiments by averaging each chroma bin i across all of the windows associated with a beat. In some embodiments the windows for a beat can be combined by taking the largest value or the median value of each chroma bin i across all of the windows associated with a beat. In some embodiments the windows for a beat can be combined by taking the N th root of the average of the values raised to the N th power for each chroma bin i across all of the windows associated with a beat.

In some embodiments the Fourier transform can be weighted e.g. using Gaussian weighting to emphasize energy a couple of octaves e.g. around two with a Gaussian half width of 1 octave above and below 400 Hz.

In some embodiments instead of using a phase derivative within FFT bins in order to generate beat level descriptors as chroma bins the STFT bins calculated in determining the onset strength envelope O t can be mapped directly to chroma bins by selecting spectral peaks. For example the magnitude of each FFT bin can be compared with the magnitudes of neighboring bins to determine if the bin is larger. The magnitudes of the non larger bins can be set to zero and a matrix containing the FFT bins multiplied by a matrix of weights that map each FFT bin to a corresponding chroma bin. This results in having 12 chroma bins per each of the FFT windows calculated in determining the onset strength envelope. These 12 bins per window can then be combined to provide 12 bins per beat in a similar manner as described above for the phase derivative within FFT bins approach to generating beat level descriptors.

In some embodiments the mapping of frequencies to chroma bins can be adjusted for each song or portion of a song by up to 0.5 semitones or any other suitable value by making the single strongest frequency peak from a long FFT window e.g. 10 seconds or any other suitable value of that song or portion of that song line up with a chroma bin center.

In some embodiments the magnitude of the chroma bins can be compressed by applying a square root function to the magnitude to improve performance of the correlation between songs.

In some embodiments each chroma bin can be normalized to have zero mean and unit variance within each dimension i.e. the chroma bin dimension and the beat dimension . In some embodiments the chroma bins are also high pass filtered in the time dimension to emphasize changes. For example a first order high pass filter with a 3 dB cutoff at around 0.1 radians sample can be used.

In some embodiments in addition to the beat level descriptors described above for each beat e.g. 12 chroma bins other beat level descriptors can additionally be generated and used in comparing songs or portions of songs . For example such other beat level descriptors can include the standard deviation across the windows of beat level descriptors within a beat and or the slope of a straight line approximation to the time sequence of values of beat level descriptors for each window within a beat. Note that if transposition of the chroma bins is performed as discussed below the mechanism for doing so can be modified to insure that the chroma dimension of any matrix in which the chroma bins are stored is symmetric or to account for any asymmetry in the chroma dimension.

In some of these embodiments only components of the song or portion of the song up to 1 kHz are used in forming the beat level descriptors. In other embodiments only components of the song or portion of the song up to 2 kHz are used in forming the beat level descriptors.

The lower two panes and of show beat level descriptors as chroma bins before and after averaging into beat length segments.

In some embodiments any suitable computer readable media can be used for storing instructions for performing the functions and or processes described herein. For example in some embodiments computer readable media can be transitory or non transitory. For example non transitory computer readable media can include media such as magnetic media such as hard disks floppy disks etc. optical media such as compact discs digital video discs Blu ray discs etc. semiconductor media such as flash memory electrically programmable read only memory EPROM electrically erasable programmable read only memory EEPROM etc. any suitable media that is not fleeting or devoid of any semblance of permanence during transmission and or any suitable tangible media. As another example transitory computer readable media can include signals on networks in wires conductors optical fibers circuits any suitable media that is fleeting and devoid of any semblance of permanence during transmission and or any suitable intangible media.

It should be noted that as used herein the term mechanism can encompass hardware software firmware or any suitable combination thereof.

It should be understood that the above described steps of the processes of and can be executed or performed in any order or sequence not limited to the order and sequence shown and described in the figures. Also some of the above steps of the processes of can be executed or performed substantially simultaneously where appropriate or in parallel to reduce latency and processing times.

Although the invention has been described and illustrated in the foregoing illustrative embodiments it is understood that the present disclosure has been made only by way of example and that numerous changes in the details of implementation of the invention can be made without departing from the spirit and scope of the invention which is limited only by the claims that follow. Features of the disclosed embodiments can be combined and rearranged in various ways.

