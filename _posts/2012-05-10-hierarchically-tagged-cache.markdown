---

title: Hierarchically tagged cache
abstract: A hierarchically tagged cache provides abstraction between access requests for data and the cached data. The cache is managed by hierarchical layers of indexes including reference to the data. Access requests indicate a header for the requested data, which is associated with one of the layers. Cache management determines what layer is associated with the header, and can traverse the indexes, beginning at the layer associated with the header, to access the data. If the header does not exist at the index of that layer, it can be created. Indexes can be appropriately updated with indexes at each layer being referenced by one or more indexes of the layer higher up in the hierarchy.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08874850&OS=08874850&RS=08874850
owner: NetApp, Inc.
number: 08874850
owner_city: Sunnyvale
owner_country: US
publication_date: 20120510
---
Embodiments described are related generally to management of data access and embodiments described are more particularly related to implementing a cache with hierarchical tags.

Portions of the disclosure of this patent document can contain material that is subject to copyright protection. The copyright owner has no objection to the reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever. The copyright notice applies to all data as described below and in the accompanying drawings hereto as well as to any software described below Copyright 2012 NetApp Inc. All Rights Reserved.

Data for companies or other organizations is commonly stored in networked storage. The resources for storage and access bandwidth are limited. However the amount of data and the desire to access data more quickly i.e. in terms of time for access and more efficiently i.e. in terms of power and processing load are all increasing. Thus a common goal for a data center that stores and manages the networked data is to improve utilization of the resources of the networked storage to improve storage utilization and access throughput.

The data access management of the data can implement service level objectives SLOs that define performance requirements for certain workloads or users and can also implement caching of frequently used data. The data access management manages access to the data through file layout techniques which define how the data is stored and accessed in a system. However traditional data access management uses a single common file layout for all data in the system. Traditionally the data representation by the file layout is closely coupled to the physical layout of the data in storage resources.

While a file layout can be made to efficiently share data in a networked storage system it will be understood that the complexity and implementation costs of including access optimizations in a single common file layout for access to multiple different data types are prohibitive. The complexity and implementation costs are especially high when considering that data of different types or the same type can have different SLOs. Thus traditional file layout for data access management of necessity works better for some data access scenarios than others. If the techniques of such a traditional file layout were used to manage data access for a cache the effectiveness of the caching may be significantly lower than desired.

A hierarchically tagged cache provides abstraction between access requests for data and the cached data. The cache is managed by hierarchical layers of indexes including reference to the data. Access requests indicate a header for the requested data which is associated with one of the layers. Cache management determines what layer is associated with the header and can traverse the indexes beginning at the layer associated with the header to access the data. If the header does not exist at the index of that layer it can be created. Indexes can be appropriately updated with indexes at each layer being referenced by one or more indexes of the layer higher up in the hierarchy. In one embodiment multiple indirection layers can be used. In one embodiment a generation count is used to identify valid versus stale indexes. In one embodiment the indirection supports delayed write where allocation of a reference to a lower layer is delayed.

Descriptions of certain details and embodiments follow including a description of the figures which can depict some or all of the embodiments described below as well as discussing other potential embodiments or implementations of the inventive concepts presented herein.

As described herein a networked storage system includes caching. The system includes cache management that manages data access to the cached data via a flexible data layout. The data layout includes hierarchical tagging which provides abstraction between access requests for data and the cached data itself. The representation of the data e.g. block at each layer of the hierarchy maps to or references the data directly or references a block that maps directly to the data. A filesystem layer organizes data representation based on a file layout or relationships between data elements. Examples include files and volumes. An abstraction layer organizes data by regions or extents of data which can be organized by type organization workload service level objective or other virtual representation of the data. The abstraction layer maps data logically over a contiguous range of blocks which may or may not correspond to a contiguous range of physical addresses. A physical mapping layer maps data to its physical address in the system. The physical mapping maps to disk or other storage for the data.

The cache management maintains indexes for the hierarchy of layers. Each index includes entries that match headers associated with the data buffers and each entry references the data directly for physical mapping or indirectly for the higher layers of the hierarchy by referencing an index entry that references a mapping or another index entry that eventually references a mapping of the data to its physical address. Access requests indicate a header or tag for the requested data the header is associated with one of the layers. Header is primarily used herein to refer to the metadata associated with hierarchically tagging the data however tag could also be used. Cache management determines what layer is associated with the header and can traverse the indexes beginning at the layer associated with the header to access the data. If the header does not exist at the index of that layer the controller can create an entry for it. Index entries can be appropriately updated from other layers of the hierarchy. Index entries at each layer are referenced by one or more index entries of the layer higher up in the hierarchy.

Physical mapping layer can also be referred to as a slab layer where data is stored in fixed size blocks. Indirection layer can also be referred to as an extent layer or region layer with extents or regions that are logically contiguous data. The logically contiguous data is not necessarily physically contiguous. The controller organizes data in indirection layer by logical relationship regardless of how it is organized in a file or volume and regardless of how it is stored in the physical layer. The independence of the indirection layer provides flexibility in how the data is organized and accessed in system .

In one embodiment the controller organizes data in filesystem layer as trees of data as shown . The tree of volume has block E as a root and further includes blocks E E E E and E. The tree of volume has block E as a root and further includes blocks E E E E and E. As shown region of indirection layer includes blocks E E. Region includes blocks E E. At indirection layer the controller organizes data as region or extent blocks which reference data blocks at physical mapping layer . Slab represents one possible grouping of disk resources labeled as A in RAID group . Slab represents one possible grouping of disk resources labeled as B in RAID group . Slab represents one possible grouping of disk resources labeled as C in RAID group . Resources A and B are shown as traditional spinning platter hard drive devices while resources C are shown as solid state drive devices. It will be understood that any combination of solid state and spinning platter drives or other nonvolatile storage could be used.

In system the controller represents the same data blocks with different names or references at different layers. It will be understood that block is used to refer simply to a unit of data. At physical mapping layer a block is fixed sized and a slab has a physical layer name which is the physical disk or location. At indirection layer in one embodiment the regions have an address range by region ID or extent ID and offset. The block size of a region is not fixed but instead indirection layer deals with variable size blocks of data. The block of data at indirection layer can be from a few physical blocks of data to thousands or more long. At filesystem layer each volume is a filesystem metadata construct that gives a file ID and an offset to data.

In one embodiment system supports lazy allocation which is a late binding of tags to a data buffer. The controller can delay a write for example at the filesystem layer to allow more time for additional requests to come in that are related to requests already received. Thus the controller can more efficiently use cache resources by storing related data in contiguous blocks.

In one embodiment the controller migrates regions to a new slab at indirection layer without any modification at filesystem layer or physical mapping layer . The controller can perform the migration for example as a background process. The references in the indexes of filesystem layer do not need to change but the data referenced at the filesystem layer will be mapped to different resources. By mapping to different resources the controller can reduce a processing load at filesystem layer .

Each of filesystem layer indirection layer and physical mapping layer includes an index structure to track headers or metadata related to data buffers stored in a cache. In one embodiment system includes a B tree index per header type or index type i.e. one for each of volume or file headers or indexes extent or region headers or indexes and slab headers or indexes . The controller can track indexes via B trees or hash tables or other management technique. The index entries map to index entries of the layer below in the hierarchy and thus provide a mapping from one layer to the other. Thus one or more volume headers can map to a region index entry one or more region headers can map to a slab index entry which in turn maps to physical address space or data buffers . Cache management which includes the cache controller can manage accessing the data buffers by traversing the indexes.

For example consider a buffer retrieved via a lookup request to the cache associated with system . The lookup includes a header that identifies an index. The lookup request can contain either a volume header a region header a slab header a volume and extent header an extent and slab header or a volume extent and slab header. In one embodiment the controller looks up the indexes in system in order from physical mapping layer up to filesystem layer depending on the header s received with the request. When the controller finds a match it returns the buffer to the requester. If a request does not pass in a lower layer header then a match in one of the higher indexes e.g. at the filesystem layer or indirection layer may require a lookup in one of the lower indexes using the newly found mapping. Thus in one embodiment a lookup with only a volume header can result in the discovery of an extent header which subsequently is looked up to find the slab header and data buffer mapped by the slab header.

In one embodiment if the controller cannot find a slab tag and data buffer but the request specifies that the data block exists on disk the controller can query the layer that received the request to resolve the next layer header based on the metadata discussed above that hierarchically maps the layers. In one embodiment the controller blocks the requests while header resolution is in process until the lookup is completed. Once the processing completes and the next level header is found the controller can update indexes with the new header mappings. In one embodiment an index at a higher layer can point directly to the data buffer being queried while the processing occurs to complete the lookup. In one embodiment creation of an index entry at the filesystem layer or region indirection layer can be followed by creation of a slab index entry to map to the data buffer instead of having the higher layer index directly map to the data buffer.

As shown there is a single indirection layer . It will be understood that system can include multiple indirection layers . Structurally there should only be one filesystem layer although multiple volumes can be stored within the layer and one slab layer. However there can be multiple indirection layers based on the design of the system. More indirection layers can provide additional flexibility in organizing and managing the data at the cost of additional metadata and another layer of indexing to traverse.

As shown region includes indexes for data of type L region includes indexes for data of type E and region includes indexes for data of type S . The blocks below the dashed lines in each of the regions represent slab blocks to which the region blocks are mapped. Thus for each region the controller keeps indexes as described in more detail below that map each region or extent to the slab indexes.

It will be understood that as suggested above the organization and mapping at indirection layer can be dynamic while the system is running. Thus a cache controller can identify data based on any type and create a region for data of that type. The index entries of the filesystem layer can continue to map to the region index entries without any change but the cache controller can manage the region blocks in a different way than when the filesystem was originally mapped to them.

In addition to the three layers identified system includes access request interface through which requests for data access would generally come from a user. The filesystem layer resides directly below the access request interface layer . The filesystem layer can communicate with the region or indirection layer via extent interface . The indirection layer can also pass index information back up to the filesystem layer to keep all headers and indexes up to date. Thus index information can be written at layers other than the layer receiving the request to maintain all index data up to date. It will be understood that in an implementation where multiple indirection layers are used additional interfaces would exist between the different indirection layers. The indirection layer communicates to storage slabs which could also be referred to as a physical mapping layer or slab allocation layer .

As shown system can have multiple instantiations of either the filesystem layer and or the indirection layer. These filesystem and indirection layer components are customizable based on the workload and the desired data layout. Cache resides to the side of the three layers and interacts with each of them. Cache stores buffers and each data buffer includes associated metadata . Cache controller retrieves buffers from storage and caches them. Metadata can include the various headers for the index entries at each layer. In one embodiment cache is local to the requesters attempting to access data and the storage where the data originates is remote over a network.

The filesystem layer is the core file system through which high level user access to data is managed. The indirection layer handles collections of contiguous related blocks. The slab layer or physical mapping layer is responsible for storing data blocks on physical disks using RAID . Each layer refers to a data block in a different manner. Thus each layer has a different name or key for the block. The different names are associated with the data buffers in a memory state such as through metadata . Thus a single data buffer has multiple header associated with it to be used by the different layers to reference the data. Additionally a shared block e.g. one that is deduplicated can have multiple headers of the same type e.g. multiple volume headers or multiple region headers associated with it.

It will be understood that cache does not store all data buffers indefinitely. Thus cache controller eventually evicts buffers from cache based on an eviction policy implemented by controller . In one embodiment controller uses a least recently used LRU policy to evict buffers. Other policies are possible. In one embodiment controller keeps a list of buffers rather than keeping a list of index entries for purposes of eviction. The indexes are the mechanisms used for management of the layers. Thus there can be index entries in the various layers that are associated with a data buffer that is no longer in cache . In one embodiment the slab header is kept with the buffer so that when the buffer gets evicted the slab header also gets evicted. Otherwise a slab block could invalidly map to a different data buffer than what is expected. The volume and indirection tags can be kept without the same risk of accessing incorrect data.

In one embodiment controller could also remove the indexes of the higher layers when a buffer is evicted. However index entry eviction would require more processing overhead including more communicating through the interfaces. As an alternative the controller could use an LRU mechanism for index entry eviction at each layer independent of the other layers to evict stale entries to allow placing new entries.

In one embodiment system supports delayed write allocation. Assume write requests are received via access request interface at the filesystem or volume layer. Controller can assign a volume ID but wait to assign a region ID or a slab ID. Thus initially there will be a volume ID but not a region ID or slab ID. The volume index entry can point directly to the buffer in cache . Delaying the write can allow the system to write larger blocks of data at the same time which can improve storage resource efficiency by writing related blocks together.

Storage access manager provides a file layout for storage access and can be a traditional system that provides direct mapping from the file layout to the physical storage resources. In contrast controller abstracts the mapping of the file layout to the physical storage resources with multiple layers as discussed above.

Access request interface can provide access to either the components of controller or to storage access manager . In one embodiment controller includes filesystem BLOB binary large object store LUN logical unit number and possibly other constructs. These components make up the filesystem layer. The filesystem layer communicates with an indirection layer via extent interface . In one embodiment the indirection layer includes sequential region random region and possibly other regions. In one embodiment regions are organized based on whether they have random data or contiguous data. As discussed above there are other ways to organize the indirection layer.

Controller also includes physical mapping layer slab allocation which is directly below the lowest level of indirection layer. Slab allocation maps blocks to physical addresses of data buffers. Slab allocation maps to physical address space via physical resource manager such as a RAID controller. Storage access manager directly maps its file layout to physical address space via physical resource manager . In one embodiment storage access manager includes write allocation manager to manage writes to the physical storage.

Physical resource manager can manage any of a number of different types of storage resources including but not limited to hard drive HDD solid state drive SSD logical unit number storage LUN raw flash or other storage. The storage can be local to a device that executes controller or can be accessible remotely over a network connection. In one embodiment controller is implemented as a virtual machine in hardware that is remote from storage resources .

Hierarchy is shown from the perspective of a logical structure for the data buffer. A cache caches data buffers which are single fixed sized blocks e.g. 4 KB blocks . Each block data buffer can be associated with a number of headers starting with physical slab header . Every data buffer in the cache should have a slab header associated with it. Each data buffer may or may not have one or multiple extent headers or volume headers associated with it as well. By allowing multiple volume headers and extent headers to point to the same slab header the cache controller allows for an easy representation of de duplicated data.

Briefly with de duplicated data the cache controller causes two volume headers or logically two volume blocks or extent headers or logically two region blocks to point to the same data buffer . Rather than storing the data buffer twice the cache controller allows multiple logical blocks via extent headers to map to a single physical block. Likewise multiple volume headers can map to a single indirection block.

Referring to the tags or header information used to index the blocks at each layer volume headers can include per file a FILE KEY with a volume ID file ID and offset. Extent headers can include a REGION KEY with an extent ID and an offset. Slab headers can include a SLAB KEY with a slab ID and a slab address. The controller uses volume headers in the filesystem layer volumes to locate data blocks given the volume fileID and file block number offset within the file. The controller uses extent headers in the filesystem layer and the indirection layer extents to locate a block within an extent by extentID and offset. The controller uses slab header in the indirection layer and the slab layer physical mapping layer to locate a block of physical storage.

It will be understood that as shown volume headers would only be able to index L0 data blocks or level 0 data blocks. Level 0 data blocks are the lowest level of indirection. To enable the use of multiple levels of indirection additional information should be added to the volume headers. In one embodiment volume headers include information in a key indicating what level of indirection the index points to. Alternatively if a file s metadata is stored in a different file a unique fileId can be used to identify the metadata.

In one embodiment SLOs are stored with the files. Thus hierarchy illustrates SLO associated with volume headers . As mentioned above the cache controller can rearrange the organization of the indirection layer s to meet the SLOs. Examples of rearrangement include moving processing of certain regions to lighter used resources. The controller can rearrange the indirection layer with having to make any changes in the filesystem layer or the physical mapping layer. Thus changes can be localized to one layer in hierarchy .

It is possible to associate slab header with multiple data buffers. Slab header can be associated with multiple buffers in a situation where a write request arrives for a buffer that is in the process of being write allocated or cleaned. It is understood that a dirty buffer is a buffer where the value of the data in the buffer has been changed in cache but the changed data has not yet been written to storage. Cleaning a buffer refers to a process or mechanism where the dirty buffer is written to storage. In one embodiment the controller marks data buffers in the process of being write allocated as copy on write COW and a new copy of the data buffer is made writeable. In such a case slab header can map to both buffers until the process is completed and the buffer marked as COW is released and evicted. In one embodiment write allocation at the filesystem layer involves the controller mapping a region to a range of space associated with a file. In one embodiment write allocation at the indirection layer involves the controller mapping a slab ID and address to an extent header.

In one embodiment there are times when the controller maps the volume or region indexes directly to data buffers rather to the next level of the hierarchy. One such time is if write allocation has not yet been performed and thus there is no available mapping to the next level of the hierarchy at the moment the controller creates the index. Another such time is if the controller is obtaining a next level of mapping for the data buffer such as for a first access to the data buffer.

There are different ways the controller of hierarchy can handle tracking the mappings when evicting a data buffer from cache. In a na ve implementation the controller could create a backpointer from each lower level index to the next higher level index. However since data buffers can be shared across multiple indexes at each level of the hierarchy the use of backpointers would add a large unbounded overhead in terms of in memory space consumed by the backpointers. As an alternative to using backpointers the controller can determine what mappings are outdated and so can be discarded on subsequent lookups.

In one embodiment the controller manages the indexes of each layer independently which means that eviction at one layer does not necessarily cause eviction at another layer. Additionally the controller can keep an eviction policy for each index e.g. an LRU scheme to evict mappings not used in some amount of time . In one embodiment the controller associates a generation count with each data buffer. The controller increments the generation count whenever a data buffer is evicted and copies the generation count into the index when the data buffer is inserted in the cache. If an index s generation number does not match the generation number of the buffer it references the controller determines the mapping to be stale and evicts the index.

As illustrated cache controller receives user requests at cache interface e.g. access request interface of access request interface of . Processes can make requests through cache interface as well as being able to make requests at any level of the hierarchy. Thus with the hierarchy each layer is separately accessible for lookup and insertion. The filesystem layer typically drives the data lookup as it is the layer through which most requests will be received. However the indirection layer can also drive the data lookup in cache .

In one embodiment cache includes index tables to index the headers or tags used to identify data blocks in system . In one embodiment cache controller maintains one table for each layer. Thus as shown index tables include filesystem table indirection table and physical mapping e.g. slab table . Index tables represents the tables of indexes that cache controller maintains. The dashed line blowup represents the hierarchical nature of the different layers.

The accessed requests received from users or processes include a tag to identify the data buffer s sought. Cache controller can extract an identifier from the tag to use as a key to traverse index tables . In one embodiment each tag is associated with a hierarchy layer identified in the tag. The hierarchy layer identified can indicate to cache controller what index layer is associated with the tag and where the cache controller should begin a lookup for the data buffer.

In one embodiment cache controller keeps generation count for each data buffer of cache . As mentioned above with respect to a generation count generation count enables cache controller to determine when an index is out of date. For example in one embodiment cache controller stores a generation count with a volume header e.g. see as well as storing a generation count with an associated or referenced data buffer . The cache controller increments the generation count every time the data buffer is allocated used to hold a different data block or every time the data buffer is evicted. If the volume header does not include the right generation count then cache controller identifies it as being stale and evicts the index.

The cache system receives a request to access data referenced by a header process block . While the highest level of the hierarchy the filesystem layer typically receives the requests any of the layers can receive the request. In one embodiment any of three different types of headers can be used in the request volume or file headers region or extent headers and slab or disk headers. The cache controller processes the request and identifies which layer is associated with the header process block . The layer can be a filesystem layer one of one or more layers of indirection or a physical mapping layer.

If there is an entry in an index for the layer determined to be associated with the header process block the controller can traverse the indexes of the various layers process block to obtain access to a data buffer to return the data buffer process block . If there is no entry in the index process block the controller can determine if there is an entry at another layer of the hierarchy process block . If there is an entry at another layer the controller can create an entry at the other layer of the hierarchy map the two entries to each other and traverse the indexes to return the data buffer process blocks and .

If the controller determines there is no entry at another layer process block the controller can determines if the request is a write request or a read request. If the controller determines the request is a write request process block the controller performs write allocation. In write allocation the controller allocates a new buffer and fills the buffer with data process block . In one embodiment there are circumstances under which the controller will delay the creation of an entry in the index. Thus in one embodiment the controller determines if the write should be delayed. If the write is to be delayed the controller continues processing and buffer access operations until the time to create the entry.

If the request is not a write request process block the request is a read request and the controller can determine if the header of the received request is a physical mapping process block . If the header is not a physical mapping then the controller determines that no buffer exists and can return an error process block . If the header is a physical mapping process block the controller reads the data from storage process block . Once the data is read the controller can allocate a new buffer and fill it process block . Alternatively if the request is a write request process block the controller can perform write allocation and allocate a new buffer and fill the new buffer process block . In one embodiment the controller creates a physical mapping at the time of write allocation. In one embodiment the controller creates a physical mapping only at the time of write allocation.

The controller can create an entry for the buffer in one or more layers of the hierarchy process block . The controller can return the data buffer after write allocation process block . In one embodiment write allocation will also include updating information in the index hierarchy. When a valid data buffer is returned the process ends. Process ends after the data buffer is returned process block or after an error is returned when no buffer exists process block .

It will be understood that having individual index structures per mapping layer allows the different storage system layers to independently lookup only the mappings required. Additionally the individual index structure allows for the delayed allocation of physical storage to data buffers. For example the controller can allocate a file at the filesystem layer without the physical space on the disk being reserved thus no disk slab mapping exists . At a later stage the controller can perform physical allocation layout decision and insert the appropriate mapping s into the appropriate index es .

Storage of data in storage units is managed by storage servers which receive and respond to various read and write requests from clients directed to data stored in or to be stored in storage units . Storage units constitute mass storage devices which can include for example flash memory magnetic or optical disks or tape drives illustrated as disks A B . Storage devices can further be organized into arrays not illustrated implementing a Redundant Array of Inexpensive Disks Devices RAID scheme whereby storage servers access storage units using one or more RAID protocols known in the art.

Storage servers can provide file level service such as used in a network attached storage NAS environment block level service such as used in a storage area network SAN environment a service which is capable of providing both file level and block level service or any other service capable of providing other data access services. Although storage servers are each illustrated as single units in a storage server can in other embodiments constitute a separate network element or module an N module and disk element or module a D module . In one embodiment the D module includes storage access components for servicing client requests. In contrast the N module includes functionality that enables client access to storage access components e.g. the D module and the N module can include protocol components such as Common Internet File System CIFS Network File System NFS or an Internet Protocol IP module for facilitating such connectivity. Details of a distributed architecture environment involving D modules and N modules are described further below with respect to and embodiments of a D module and an N module are described further below with respect to .

In one embodiment storage servers are referred to as network storage subsystems. A network storage subsystem provides networked storage services for a specific application or purpose and can be implemented with a collection of networked resources provided across multiple storage servers and or storage units.

In the embodiment of one of the storage servers e.g. storage server A functions as a primary provider of data storage services to client . Data storage requests from client are serviced using disks A organized as one or more storage objects. A secondary storage server e.g. storage server B takes a standby role in a mirror relationship with the primary storage server replicating storage objects from the primary storage server to storage objects organized on disks of the secondary storage server e.g. disks B . In operation the secondary storage server does not service requests from client until data in the primary storage object becomes inaccessible such as in a disaster with the primary storage server such event considered a failure at the primary storage server. Upon a failure at the primary storage server requests from client intended for the primary storage object are serviced using replicated data i.e. the secondary storage object at the secondary storage server.

It will be appreciated that in other embodiments network storage system can include more than two storage servers. In these cases protection relationships can be operative between various storage servers in system such that one or more primary storage objects from storage server A can be replicated to a storage server other than storage server B not shown in this figure . Secondary storage objects can further implement protection relationships with other storage objects such that the secondary storage objects are replicated e.g. to tertiary storage objects to protect against failures with secondary storage objects. Accordingly the description of a single tier protection relationship between primary and secondary storage objects of storage servers should be taken as illustrative only.

In one embodiment storage servers include cache controller components A B . Cache controllers enable storage servers to manage a cache in system with hierarchical headers. In one embodiment cache controllers are implemented at the client side rather than in the storage servers.

Nodes can be operative as multiple functional components that cooperate to provide a distributed architecture of system . To that end each node can be organized as a network element or module N module A B a disk element or module D module A B and a management element or module M host A B . In one embodiment each module includes a processor and memory for carrying out respective module operations. For example N module can include functionality that enables node to connect to client via network and can include protocol components such as a media access layer Internet Protocol IP layer Transport Control Protocol TCP layer User Datagram Protocol UDP layer and other protocols known in the art.

In contrast D module can connect to one or more storage devices via cluster switching fabric and can be operative to service access requests on devices . In one embodiment the D module includes storage access components such as a storage abstraction layer supporting multi protocol data access e.g. Common Internet File System protocol the Network File System protocol and the Hypertext Transfer Protocol a storage layer implementing storage protocols e.g. RAID protocol and a driver layer implementing storage device protocols e.g. Small Computer Systems Interface protocol for carrying out operations in support of storage access operations. In the embodiment shown in a storage abstraction layer e.g. file system of the D module divides the physical storage of devices into storage objects. Requests received by node e.g. via N module can thus include storage object identifiers to indicate a storage object on which to carry out the request.

Also operative in node is M host which provides cluster services for node by performing operations in support of a distributed storage system image for instance across system . M host provides cluster services by managing a data structure such as a relational database RDB RDB A RDB B which contains information used by N module to determine which D module owns services each storage object. The various instances of RDB across respective nodes can be updated regularly by M host using conventional protocols operative between each of the M hosts e.g. across network to bring them into synchronization with each other. A client request received by N module can then be routed to the appropriate D module for servicing to provide a distributed storage system image.

In one embodiment node A includes cache controller A and node B includes cache controller B. Alternatively cache controller A can be located at the client side and associated with node A. Similarly cache controller B can be located at the client side and associated with node B. Cache controllers as described above manage a cache with hierarchical levels of headers and indexes.

It will be noted that while shows an equal number of N and D modules constituting a node in the illustrative system there can be different number of N and D modules constituting a node in accordance with various embodiments. For example there can be a number of N modules and D modules of node A that does not reflect a one to one correspondence between the N and D modules of node B. As such the description of a node comprising one N module and one D module for each node should be taken as illustrative only.

Memory includes storage locations addressable by processor network adapter and storage adapter for storing processor executable instructions and data structures associated with a multi tiered cache with a virtual storage appliance. A storage operating system portions of which are typically resident in memory and executed by processor functionally organizes the storage server by invoking operations in support of the storage services provided by the storage server. It will be apparent to those skilled in the art that other processing means can be used for executing instructions and other memory means including various computer readable media can be used for storing program instructions pertaining to the inventive techniques described herein. It will also be apparent that some or all of the functionality of the processor and executable software can be implemented by hardware such as integrated currents configured as programmable logic arrays ASICs and the like.

Network adapter comprises one or more ports to couple the storage server to one or more clients over point to point links or a network. Thus network adapter includes the mechanical electrical and signaling circuitry needed to couple the storage server to one or more clients over a network. Each client can communicate with the storage server over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

Storage adapter includes a plurality of ports having input output I O interface circuitry to couple the storage devices e.g. disks to bus over an I O interconnect arrangement such as a conventional high performance FC or SAS Serial Attached SCSI Small Computer System Interface link topology. Storage adapter typically includes a device controller not illustrated comprising a processor and a memory for controlling the overall operation of the storage units in accordance with read and write commands received from storage operating system . As used herein data written by a device controller in response to a write command is referred to as write data whereas data read by device controller responsive to a read command is referred to as read data. 

User console enables an administrator to interface with the storage server to invoke operations and provide inputs to the storage server using a command line interface CLI or a graphical user interface GUI . In one embodiment user console is implemented using a monitor and keyboard.

In one embodiment computing device includes cache controller . While shown as a separate component in one embodiment cache controller is part of other components of computer . Cache controller enables computer to hierarchically manage a cache with layers of indexes and headers.

When implemented as a node of a cluster such as cluster of the storage server further includes a cluster access adapter shown in phantom having one or more ports to couple the node to other nodes in a cluster. In one embodiment Ethernet is used as the clustering protocol and interconnect media although it will be apparent to one of skill in the art that other types of protocols and interconnects can by utilized within the cluster architecture.

Multi protocol engine includes a media access layer of network drivers e.g. gigabit Ethernet drivers that interface with network protocol layers such as the IP layer and its supporting transport mechanisms the TCP layer and the User Datagram Protocol UDP layer . A file system protocol layer provides multi protocol file access and to that end includes support for the Direct Access File System DAFS protocol the NFS protocol the CIFS protocol and the Hypertext Transfer Protocol HTTP protocol . A VI virtual interface layer implements the VI architecture to provide direct access transport DAT capabilities such as RDMA as required by the DAFS protocol . An iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a FC driver layer receives and transmits block access requests and responses to and from the storage server. In certain cases a Fibre Channel over Ethernet FCoE layer not shown can also be operative in multi protocol engine to receive and transmit requests and responses to and from the storage server. The FC and iSCSI drivers provide respective FC and iSCSI specific access control to the blocks and thus manage exports of luns to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing blocks on the storage server.

The storage operating system also includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on storage devices. Information can include data received from a client in addition to data accessed by the storage operating system in support of storage server operations such as program application data or other system data. Preferably client data can be organized as one or more logical storage objects e.g. volumes that comprise a collection of storage devices cooperating to define an overall logical arrangement. In one embodiment the logical arrangement can involve logical volume block number vbn spaces wherein each volume is associated with a unique vbn.

File system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustrated as a SCSI target module . SCSI target module is generally disposed between drivers and file system to provide a translation layer between the block lun space and the file system space where luns are represented as blocks. In one embodiment file system implements a WAFL write anywhere file layout file system having an on disk format representation that is block based using e.g. 4 kilobyte KB blocks and using a data structure such as index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . File system uses files to store metadata describing the layout of its file system including an inode file which directly or indirectly references points to the underlying data blocks of a file.

Operationally a request from a client is forwarded as a packet over the network and onto the storage server where it is received at a network adapter. A network driver such as layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to file system . There file system generates operations to load retrieve the requested data from the disks if it is not resident in core i.e. in memory . If the information is not in memory file system accesses the inode file to retrieve a logical vbn and passes a message structure including the logical vbn to the RAID system . There the logical vbn is mapped to a disk identifier and device block number disk dbn and sent to an appropriate driver of disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the storage server. Upon completion of the request the node and operating system returns a reply to the client over the network.

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the storage server adaptable to the teachings of the invention can alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path can be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware embodiment increases the performance of the storage service provided by the storage server in response to a request issued by a client. Moreover in another alternate embodiment of the invention the processing elements of adapters can be configured to offload some or all of the packet processing and storage access operations respectively from processor to increase the performance of the storage service provided by the storage server. It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

When implemented in a cluster data access components of the storage operating system can be embodied as D module for accessing data stored on disk. In contrast multi protocol engine can be embodied as N module to perform protocol termination with respect to a client issuing incoming access over the network as well as to redirect the access requests to any other N module in the cluster. A cluster services system can further implement an M host e.g. M host to provide cluster services for generating information sharing operations to present a distributed file system image for the cluster. For instance media access layer can send and receive information packets between the various cluster services systems of the nodes to synchronize the replicated databases in each of the nodes.

In addition a cluster fabric CF interface module CF interface modules A B can facilitate intra cluster communication between N module and D module using a CF protocol . For instance D module can expose a CF application programming interface API to which N module or another D module not shown issues calls. To that end CF interface module can be organized as a CF encoder decoder using local procedure calls LPCs and remote procedure calls RPCs to communicate a file system command between D modules residing on the same node and remote nodes respectively.

In one embodiment cache access management is in parallel to storage operating system . In one embodiment cache access management includes CF interface C to facilitate inter cluster communication. Filesystem layer indirection layer and physical mapping layer provide the hierarchical layers for management of the cache. A logical representation of cache is provided to the side of the layers for reference purposes a physical cache storing data buffers is not implemented in the control layers of the system. Rather the cache control is implemented by the layers shown. With the storage operating system filesystem directly interfaces with RAID system . In the cache access management physical mapping layer directly interfaces with RAID system which in turn provides access to the disk drive system . The physical cache resources of cache can be separate from disk drive system .

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and can implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

Flow diagrams as illustrated herein provide examples of sequences of various process actions. Although shown in a particular sequence or order unless otherwise specified the order of the actions can be modified. Thus the illustrated embodiments should be understood only as an example and the process can be performed in a different order and some actions can be performed in parallel. Additionally one or more actions can be omitted in various embodiments thus not all actions are required in every embodiment. Other process flows are possible.

Various operations or functions are described herein which can be described or defined as software code instructions configuration and or data. The content can be directly executable object or executable form source code or difference code delta or patch code . The software content of the embodiments described herein can be provided via an article of manufacture with the content stored thereon or via a method of operating a communications interface to send data via the communications interface. A machine readable medium or computer readable medium can cause a machine to perform the functions or operations described and includes any mechanism that provides i.e. stores and or transmits information in a form accessible by a machine e.g. computing device electronic system or other device such as via recordable non recordable storage media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices or other storage media or via transmission media e.g. optical digital electrical acoustic signals or other propagated signal . A communication interface includes any mechanism that interfaces to any of a hardwired wireless optical or other medium to communicate to another device such as a memory bus interface a processor bus interface an Internet connection a disk controller. The communication interface can be configured by providing configuration parameters and or sending signals to prepare the communication interface to provide a data signal describing the software content.

Various components described herein can be a means for performing the operations or functions described. Each component described herein includes software hardware or a combination of these. The components can be implemented as software modules hardware modules special purpose hardware e.g. application specific hardware application specific integrated circuits ASICs digital signal processors DSPs etc. embedded controllers hardwired circuitry etc.

Besides what is described herein various modifications can be made to the disclosed embodiments and implementations without departing from their scope. Therefore the illustrations and examples herein should be construed in an illustrative and not a restrictive sense.

