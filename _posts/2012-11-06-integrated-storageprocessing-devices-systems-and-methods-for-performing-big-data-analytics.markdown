---

title: Integrated storage/processing devices, systems and methods for performing big data analytics
abstract: Architectures and methods for performing big data analytics by providing an integrated storage/processing system containing non-volatile memory devices that form a large, non-volatile memory array and a graphics processing unit (GPU) configured for general purpose (GPGPU) computing. The non-volatile memory array is directly functionally coupled (local) with the GPU and optionally mounted on the same board (on-board) as the GPU.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08996781&OS=08996781&RS=08996781
owner: OCZ Storage Solutions Inc.
number: 08996781
owner_city: San Jose
owner_country: US
publication_date: 20121106
---
The present invention generally relates to data processing systems for use in computer systems and more particularly to systems capable of performing big data analytics as well as devices therefor.

Big data analytics is a relatively new approach to managing large amounts of data. As used herein the term big data is used to describe unstructured and semi structured data in such large volumes for example petabytes or exabytes of data as to be immensely cumbersome to load into a relational database for analysis. Instead of the conventional approach of extracting information from data sets where an operator defines criteria that are used for data analysis big data analytics refers to a process by which the data themselves are used to generate their own search strategies based on commonalities of events for example recurrent data structures or abnormal events that is unique data structures that do not match the rest of the data set. One of the prerequisites for this kind of data driven analysis is to have data sets that are as large as possible which in turn means that they need to be processed in the most efficient way. In most cases the analysis involves massive parallel processing as done for example on a graphics processing unit GPU . The general purpose type of the work load performed by a GPU has led to the term general purpose graphics processing unit or GPGPU for the processor and GPGPU computing for this type of computational analysis with a GPU.

Big data analytics has become the method of choice in fields like astronomy where no experimental intervention can be applied to preselect data. Rather data are accumulated and analyzed essentially without applying any kind of filtering. Another exemplary case underscoring the importance of the emergence of big data analytics has been a study of breast cancer survivors with a somewhat surprising outcome of the study in that the phenotypical expression and configuration of non cancerous stromal cells was equally or even more deterministic for the survival rate of patients than the actual characteristics of the tumor cells. Interestingly attention had not been paid to the first until a big data analytics going far beyond the immediate focus of the study was applied in which without preselection by an operator all available data were loaded into the system for analysis. This example illustrates how seemingly unrelated data can hold clues to solving complex problems and underscores the need to feed the processing units with data sets that are as complete and all encompassing as possible without applying preselection or bias of any sort.

The lack of bias or preselection further underpins that the data sets used in big data analytics are exactly what the name describes meaning that data sets in excess of terabytes are not the exception but rather the norm. Conventional computer systems are not designed to digest data on massive scales for a number of reasons. General purpose central processing units CPUs are very good at performing a highly diverse workload but the limitation in the number of cores which determines the number of possible concurrent threads including Intel s HyperThreading prevents CPUs from being very good at massive parallel analytics of large data. For this reason GPUs characterized by a large array of special purpose processors have been adapted to perform general purpose computing leading to the evolution of GPGPUs. However even with the highest end GPGPU expansion cards currently available for example the Tesla series of graphics expansion cards commercially available from nVidia Corporation the on board local volatile memory referred to as a local frame buffer or LFB functionally integrated with the GPGPU on the graphics expansion card is limited to 6 GB which can only hold a fraction of the data designated to be analyzed in any given scenario. Moreover the data need to be loaded from a host system for example a personal computer or server through a PCIe peripheral component interconnect express or PCI Express root complex which typically involves access of the data through a hard disk drive or in a more advanced configuration through NAND flash based solid state drives SSDs which receive data from a larger storage array in the back end of a server array. Either type of drive will read the data out to the main system memory which in turn through a direct memory access DMA channel forwards the data to the LFB. While functional this process has drawbacks in the form of multiple protocol and data format conversions and many hops from one station to another within the computer system adding latencies and potential bus congestion. In other words the current challenge in systems used to perform big data analytics is that their performance is no longer defined by the computational resources but rather by the I O limitations of the systems.

Another difference compared to current mainstream computing is that the data made available to GPGPUs are often not modified. Instead they are loaded and the computational analysis generates a new set of data in the form of additional paradigms or parameters that can be applied against specific aspects or the whole of the original data set. However the original data are not changed since they are the reference and may be needed at any later time again. This changes the prerequisites for SSDs serving as last tier storage media before the data are loaded into a volatile memory buffer. Specifically with respect to loading the data into the SSD most of the transactions will be sequential writes of large files whereas small random access writes could be negligible. In the case of data reads to the LFB a mixed load of data comprising large sequential transfers and smaller transfers with a more random access pattern are probably the most realistic scenario.

As previously noted a particular characteristic of big data analytics is its unstructured or semi structured nature of information. Unlike structured information which as used herein refers to relational database ordered in records and arranged in a format that database software can easily process big data information is typically in the form of raw sets of mixed objects for example MRI images outputs of multiple sensors video clips and so on. Each object contains a data part e.g. a bitmap of the MRI image and a metadata part e.g. description of the MRI image information about the patient MRI type and diagnosis.

The massive amount of data gathered and subjected to analytics typically requires a distributed processing scheme. That is the data are stored in different nodes. However each node in the system can process data from any other node. In other words the storage is accumulated within the nodes capacity and the processing power is spread across all nodes forming a large space of parallel processing.

Funneling all data through the PCIe root complex of a host system may eventually result in bus contention and delays in data access. Specifically in most current approaches data are read from a solid state drive to the volatile system memory then copied to a second location in the system memory pinned to the GPU and finally transferred via the PCIe root complex to the graphics expansion card where the data are stored in the LFB. Alternatively a peer to peer data transfer can be used to transfer data directly from one device to another but it still has to pass through the PCIe root complex. Similar constraints are found in modern gaming applications where texture maps are pushing the boundaries of the LFB of gaming graphics expansion cards. US patent application 2011 0292058 discloses a non volatile memory space assigned to an Intel Larrabee LRB type graphics processor for fast access of texture data from the SSD as well as a method for detection whether the requested data are in the non volatile memory and then arbitrating the access accordingly.

Given the complexity and lack of optimization of the above discussed data transfer scheme between non volatile storage and the local on board volatile memory of a graphics expansion card including all latencies and possible contentions at any of the hops between the origin in the SSD and the final destination in the LFB it is clear that more efficient storage and processing systems are needed for performing big data analytics.

The current invention discloses highly efficient architectures and methods for performing big data analytics by providing an integrated storage processing system containing non volatile memory devices that form a large non volatile memory array and a graphics processing unit GPU configured for general purpose GPGPU computing. The non volatile memory array is local to the GPU which as used herein means that the array is directly functionally coupled with the GPU and optionally is mounted on the same board on board as the GPU. Non limiting examples of such direct functional coupling may include a flash controller with a DDR compatible interface a non volatile memory controller integrated into the GPU and working in parallel to the native DDR controller of the GPU or a PCIe based interface including a PCIe switch.

According to a first aspect of the invention the local non volatile memory array may be functionally equivalent to a large data queue functionally coupled to the GPU. The GPU may be a stand alone graphics processing unit GPU or a hybrid processing unit containing both CPU and GPU cores commonly referred to as an advanced processing unit APU for example containing CPU cores in combination with an array of GPU cores and an optional PCIe root complex. In either case the GPU is mounted on a processor expansion card for example a PCIe based processor expansion card which further includes an on board local volatile memory array of volatile memory devices preferably fast DRAM as a local frame buffer LFB that is functionally integrated with the GPU. In addition however the GPU is also functionally coupled to the aforementioned local non volatile memory array provided as a local array of the non volatile memory devices capable of storing large amounts of data and allowing direct low latency access thereof by the GPU without accessing a host computer system in which the processor expansion card is installed. The non volatile memory devices are solid state devices for example NAND flash integrated circuits or another nonvolatile solid state memory technology and access to the local non volatile memory array is through a non volatile memory controller for example a NAND flash controller which can be a direct PCIe based memory controller or a set of integrated circuits for example a PCIe based SATA host bus controller in combination with a SATA based flash controller.

In a first embodiment an integrated storage processing system includes the processor expansion card including the GPU and on board local volatile memory array as LFB and the processor expansion card is PCIe based compliant and functionally coupled to a PCIe based solid state drive SSD expansion card comprising the local non volatile memory array. The processor expansion card and SSD expansion card are functionally coupled by establishing a peer to peer connection via an I O input output hub on a motherboard of the host computer system to allow access of data stored in the non volatile memory devices by the GPU without accessing memory of the host computer system by peer to peer transfer of PCIe protocol based command address and data CAD packets.

In a second embodiment of the invention the processor expansion card may be one of possibly multiple PCIe based processor expansion cards each with a GPU and an on board local volatile memory array as LFB that are functionally integrated with the GPU. In addition one or more PCIe based SSD expansion cards comprise the non volatile memory devices that constitute one or more local non volatile memory arrays. The processor expansion card s and the SSD expansion card s are connected to a daughter board having PCIe expansion sockets to accept PCIe based expansion cards. Each PCIe expansion socket comprises a PCIe connector coupled to multiple parallel PCIe lanes each constituting a serial point to point connection comprising differential pairs for sending and receiving data. The PCIe lanes coupled to the PCIe connectors for the processor expansion cards are connected to a PCIe switch which is coupled by another set of PCIe lanes to one or more PCIe edge connectors adapted to be inserted into PCIe expansion slots of a motherboard of the host computer system. A technical effect of this approach is that by linking a processor expansion card and SSD expansion card via the switch faster throughput is achieved as compared to a link through a chipset input output hub IOH controller containing a PCIe root complex.

In a third embodiment of the invention in addition to the GPU functionally integrated with the on board local volatile memory array as LFB the processor expansion card comprises the local non volatile memory array and non volatile memory controller therefor in which case the local array can be referred to as an on board non volatile memory array with respect to the processor expansion card. The processor expansion card comprises a PCIe connector that defines multiple parallel PCIe lanes constituting an interface for the processor expansion card with the host computer system. Of the total number of PCIe lanes a first group of the PCIe lanes is directly connected to the GPU and a second group of the PCIe lanes is connected to the memory controller. The GPU is capable of executing virtual addressing of the non volatile memory devices of the on board non volatile memory array through a direct interface between the GPU and the memory controller.

An alternative option with the third embodiment is that of the PCIe lanes constituting the interface of the processor expansion card with the host computer system a first group of the PCIe lanes couples the GPU to the host computer system and a second group of the PCIe lanes is coupled to a PCIe switch connected to the non volatile memory controller and the GPU wherein the PCIe switch functions as a transparent bridge to route data from the host computer system to the non volatile memory controller or the GPU or from the non volatile memory controller to the GPU.

As another alternative option with the third embodiment of the invention of the PCIe lanes constituting the interface of the processor expansion card with the host computer system a functionally unified group of PCIe lanes is routed through a PCIe switch and then arbitrates across different modes of endpoint connections based on modes defined as address ranges and directionality of transfer. Such modes preferably include host to GPU host to SSD and SSD to GPU coupling.

Certain aspects of the invention include the ability of the processor expansion card to use a hybrid processing unit comprising CPU and GPU cores as well as an integrated PCIe root complex system logic and at least one integrated memory controller. The on board volatile memory array of volatile memory devices as LFB may use dual inline memory modules DIMMs and the local non volatile memory array of non volatile memory devices is addressed via the PCIe root complex integrated into the APU. The PCIe root complex may have two separate links of different width for example a wide link of sixteen PCIe lanes and a narrow link of four PCIe lanes. The CPU cores can also run virtual machines. The processor expansion card may use a non transparent bridge NTB to interface with the host computer system.

In the various embodiments discussed above in which the local non volatile memory array and memory controller are integrated onto the processor expansion card i.e. onboard with the GPU the memory controller can be dual ported and adapted to receive data directly from a host computer system as well as transfer data directly to the on board GPU of the processor expansion card. The local non volatile memory array acts as a queue or first in first out buffer for data transferred from the host computer system to the integrated storage processing system.

Also in the various embodiments discussed above the GPU may have a graphics port adapted to transfer data to a second host computer system.

In yet another specific aspect of the invention the memory controller implements the NVM Express standard NVMe formerly known as Enterprise non volatile memory host controller interface NVMHCI a specification for accessing SSDs over a PCIe channel. As NVM Express supports up to 64K queues it allows at least one queue and preferably more to be assigned to each GPU core of the GPU thus achieving true parallel processing of each core with its appropriate data. Alternatively the memory controller may implement an STA s SCSI Trade Association SCSI express standard for SCSI commands over a PCIe channel or may implement another proprietary or standard interface of flash or SCSI commands over a PCIe channel for use with flash based storage or may an object storage protocol OSD version 1 OSD version 2 or any proprietary object storage standard. Furthermore the memory controller may implement one of the above interfaces with additional non standard commands. Such commands can be key value commands for an associative array or hash table search as defined in a Memcached API application programming interface . Another example of such an API can be a cache API with Read Cache Write Cache and Invalidate directives.

According to another aspect the invention comprises a method for efficient big data analytics using a GPU and an on board local volatile memory array of volatile memory devices as a local frame buffer LFB integrated together with a local non volatile memory array on a PCIe based expansion card. Data are loaded from the non volatile memory array into the LFB without being intermittently stored in the system memory and processed by parallel execution units of the GPU. As with other embodiments of the invention the GPU may be a graphics processing unit GPU or a hybrid processing unit APU containing both CPU and GPU cores.

According to still another aspect of the invention a method is provided for distributed analytics of big data using a cluster of several client machines each client machine having a PCIe based expansion card with a GPU and a local non volatile memory array. Each client machine is attached to a network attached storage array via Ethernet fiber channel or any other suitable protocol for loading data into non volatile memory devices of the local non volatile memory array. The GPU performs big data analytics on data loaded into the non volatile memory devices and results of the analytics are output through a graphics port or media interface on the expansion card and transferred to a host computer system.

The present invention is targeted at solving the bottleneck shift from computational resources to the I O subsystem of computers used in big data analytics. Conventional solutions using GPGPU computing are data starved in most cases since the storage system cannot deliver data at a rate that makes use of the processing capabilities of massive parallel stream processors for example the CUDA compute unified device architecture parallel computing architecture developed by the nVidia Corporation. Though the adding of additional solid state drives SSDs to function as prefetch caches ameliorates the problems this approach is still slowed by latencies and a sub optimal implementation of a streamlined direct I O interface connected to graphics processing units with enough storage capacity to hold large data sets at a reasonable cost and power budget.

To overcome these problems the present invention provides integrated storage processing systems and devices that are configured to be capable of efficiently performing big data analytics. Such a system can combine a graphics processing unit GPU configured for general purpose GPGPU computing with a directly attached local array of non volatile memory devices that may be either integrated onto a device with a GPGPU on board or on a separate device that is directly functionally coupled with a GPGPU local but not on board via a dedicated micro architecture that may comprise an interposed daughter card. As used herein the term GPGPU is used to denote a stand alone graphics processing unit GPU configured for general purpose computing as well as hybrid processing units containing both CPU and GPU cores and commonly referred to as advanced processing units APUs . A nonlimiting embodiment of an integrated storage processing device equipped with an on board volatile memory array of volatile memory devices for example DRAM memory devices and an on board non volatile memory array of non volatile memory devices is schematically represented in . For illustrative purposes the integrated storage processing device illustrated in is based on an existing Nvidia Fermi memory hierarchy to which the non volatile memory array has been added though the invention is not limited to this configuration. The non volatile memory devices are solid state memory devices and as indicated in can be flash memory devices FLASH and preferably NAND flash memory devices though any other suitable high capacity non volatile memory technology may be used. The memory capacity of the non volatile memory array is preferably terabyte scale.

The following discussion will make reference to of which through depict various embodiments of integrated storage processing systems and devices that are within the scope of the invention. For convenience consistent reference numbers are used throughout the drawings to identify the same or functionally equivalent elements.

As a point of reference represent examples of existing system architectures used for GPGPU computing. Current system architectures of the type shown in are typically configured as a high end PCIe based graphics expansion card adapted to be installed in an expansion slot not shown on a motherboard mainboard or any other suitable printed circuit board of a host computer system for example a personal computer or server . The expansion card includes a GPU having a PCIe endpoint and execution units and a large DRAM based local frame buffer LFB . The expansion card is functionally coupled via a PCIe bus connector interface generally part of the expansion bus of the motherboard to interface with a PCIe root complex on the motherboard . A DMA Direct Memory Access channel allows for direct transfer of data from an array of DRAM based system memory on the motherboard to the LFB on the graphics expansion card through a central processing unit CPU on the motherboard . Local data storage is provided by a solid state drive SSD represented in as comprising a flash memory array and a SSD controller which interfaces with a SATA host bus adapter connected to the PCIe root complex for low latency access of data stored in flash memory devices of the memory array . Alternatively a hard disk drive using rotatable media can be used for local data storage with the inherent trade off between data capacity and access latency and bandwidth.

A more advanced system architecture known in the art and illustrated in uses a dedicated PCIe based SSD expansion card having a PCIe to SATA endpoint functionally coupled to a SATA SSD controller which in turn is coupled to a flash memory array . The expansion card interfaces with the host computer system motherboard through a first group of PCIe lanes via a first PCIe connector . This particular architecture has the advantage of bypassing the limitation of a single SATA interface with respect to bandwidth. However the data still need to be transferred from the SSD expansion card to the host system PCIe root complex . The graphics expansion card uses a second set of PCIe lanes via a second PCIe connector to interface with the PCIe root complex on the motherboard . With this configuration a peer to peer transfer would require copying the data to the system memory in which case they would then need to be copied again into a memory range pinned to the GPU before being transferred through a second group of PCIe lanes to the graphics expansion card .

The processor of the integrated expansion card is further represented as functionally coupled to a local on board non volatile memory array of non volatile memory devices capable of storing large amounts of data and allowing direct low latency access thereof by the processor without accessing the motherboard of the host computer system in which the integrated expansion card is installed. The non volatile memory array preferably contains solid state memory devices for example NAND flash memory devices though the use of other non volatile solid state memory technologies is also within the scope of the invention. The memory array is accessed through a memory controller having a PCIe endpoint for example a direct PCIe based memory controller or a set of integrated circuits for example a PCIe based SATA host bus controller in combination with a SATA based flash memory controller. If the processor is an APU the memory controller with its memory array can be addressed through the APU s PCIe root complex. If the processor is a standalone GPU the memory controller with its memory array can be addressed through a GPU Direct or a unified virtual addressing architecture. The memory controller can further set up a DMA channel not shown to the on board volatile memory array . Packets containing command address and data CAD are loaded from the motherboard into the memory array via the PCIe bus connector .

Conceptually one of the easiest implementations of the architecture discussed above can rely on discrete graphics and SSD expansion cards but use a direct device to device data transfer scheme. represents such a data transfer scheme between separate processor and SSD expansion cards and going through an I O hub IOH on a motherboard such that the processor expansion card equipped with an on board volatile memory array for example Graphic Double Data Rate GDDR memory communicates with the SSD expansion card equipped with an on board non volatile memory array for example NAND flash memory via peer to peer transfers through the IOH . However depending on the exact hardware and software device driver specifications and or licensing agreements between manufacturers of the motherboard chipset for example IOH processor expansion card and SSD expansion card this particular mode of operation may not be supported broadly enough to gain ubiquitous acceptance.

An alternative solution bypassing the aforementioned technical and logistical problems is to insert a PCIe expansion micro architecture as schematically represented in and represented by a possible physical embodiment in . Instead of relying on the IOH on the motherboard as done in the microarchitecture of further comprises an expansion or daughter board with a PCIe switch to allow direct communication between the PCIe based processor and SSD expansion cards and . The PCIe switch is preferably a transparent bridge that is functionally coupled to the IOH located on the motherboard however peer to peer traffic is routed though the PCIe switch which effectively doubles the bandwidth compared to traffic routed through the IOH in .

The daughter board has at least one PCIe edge connector to be inserted into a PCIe slot not shown on the motherboard . Each edge connector can establish a multi lane PCIe link to the PCIe switch mounted on the daughter board which also has two PCIe based expansion slots female connectors for insertion of the processor and SSD expansion cards and shown as full size expansion cards in the non limiting example of . The processor expansion card is a graphics expansion card featuring a processor GPGPU and a volatile memory array whereas the SSD expansion card contains a non volatile memory NVM array and memory controller . The PCIe switch allows peer to peer communication of the two expansion cards and or else communication of either expansion card with a host computer system through the PCIe edge connectors with the motherboard .

While the above discussed implementations may provide a relatively easy approach to combine existing hardware for a streamlined GPGPU SSD functional complex the following discussion will be directed to the combination of both devices on a single expansion card and example of which is the embodiment previously discussed in reference to .

In most cases PCIe slots are configured to support one group of PCIe lanes with a single target device. However the PCIe specifications also support multiple targets on a single physical slot i.e. a split PCIe bus connector an example of which is shown in . In the embodiment of the processor is represented as using one group of eight PCIe lanes of the connector for command address and data CAD signals as well as for the DMA channel to the DRAM of the volatile memory array . A second group of eight PCIe lanes is coupled to the memory controller in order to transfer data from the host computer system not shown to the non volatile memory array . The memory controller is configured to be recognized by the processor GPGPU as a compatible device through a group of PCIe lanes of a direct PCIe interface a GPU direct interface or any similar access scheme. The embodiment of can also make use of a DMA channel not shown from the memory controller to the DRAM of the volatile memory array . Other specific access schemes or protocols are also possible.

Instead of using direct point to point communication as discussed above the processor may also request data from the non volatile memory array by sending the request to the host computer system. The host computer system then issues a ReadFPDMA or equivalent NVMExpress request to the memory controller but sets up the target address range to be within the volatile memory array of the processor .

In a modified implementation shown in the processor uses a group of eight PCIe lanes of the split PCIe bus connector as a dedicated PCIe link to establish a permanent and direct interface between the processor and the PCIe root complex . A second group of eight PCIe lanes connects to a PCIe switch PCIe switch transparent bridge . The PCIe switch routes data and request signals PCIe packets over the PCIe lanes between the host computer system the processor and the memory controller for transfer of PCIe packets between the host computer system and the processor between the host computer system and the memory controller and between the memory controller and the processor . If the processor requests a specific set of data it sends a request to the host computer system which in turn translates the request into a read request which is transferred to the memory controller via the PCIe switch . As soon as the memory controller is ready to transfer the data to the processor and the DRAM of the volatile memory array the memory controller sets up a DMA channel through the PCIe switch and streams the requested data into the volatile memory array . The host computer system then waits for the processor to issue the next request or else speculatively transfers the next set of data to the non volatile memory array . In a more streamlined configuration the memory controller and processor can transfer data directly through peer to peer transfers based on the address range of the destination memory array using the switch to set up the correct routing based on the addresses.

The processor can return the result of the analytics directly to the host computer system via the PCIe bus connector . Alternatively the processor can also output the results of the data processing through any of the video ports such as DVI HDMI or DisplayPort as non limiting examples.

In a slightly simplified implementation shown in all PCIe lanes of a PCIe bus connector are used as a unified link and coupled to a PCIe switch that arbitrates the coupling between a host computer system memory controller and processor in a three way configuration. Arbitration of connections may be done according to the base address registers BAR defining the address range of individual target devices the processor or memory controller . Similar as discussed above the processor can access the non volatile memory controller through the PCIe switch using GPU Direct or a comparable protocol.

One particular embodiment of an expansion card as discussed in reference to is shown in . The expansion card has a PCIe compliant edge connector adapted to interface with the host computer system s PCIe bus connector not shown . The edge connector routes a first group of PCIe lanes identified as PCIe link to the processor GPGPU and a second group of PCIe lanes identified as PCIe link to the memory controller . The processor can directly access the memory controller through a third group of dedicated PCIe lanes identified as PCIe link between the processor GPGPU and memory controller . In practice the PCIe bus connector at the host level may be sixteen lanes wide of which eight PCIe lanes are dedicated to the processor and the remaining eight PCIe lanes connect directly to the memory controller to serve as an interface to the non volatile memory NVM devices of the non volatile memory array . The memory controller may have a built in PCIe bank switch not shown to select the eight PCIe lanes connected to the host PCIe bus connector via the PCIe link or else select a second set of PCIe lanes PCIe link that connect directly to the processor depending on the address or command information received. Alternatively the bank switch may also be controlled by the write vs. read command. That is if a write command is received the switch automatically connects to the host computer system whereas a read command will automatically connect to the processor . Another possible implementation of this design uses a memory controller with an eight PCIe lanes wide bus connector which is split into four PCIe lanes connecting to the host computer system and four PCIe lanes connecting to the processor .

One of the issues faced with integrating a GPU and a flash memory controller on the same device and establishing a direct functional coupling without the need to route data through the host computer system is that the GPU and memory controller typically are configured as PCIe endpoints. In most implementations PCIe endpoints require a PCIe switch or need to pass through a PCIe root complex in order to communicate with each other which as discussed above is feasible but adds complexity and cost to the device. A possible solution to this drawback is represented in as entailing the use of a hybrid processor comprising both CPU and GPU cores instead of a GPU configured for general purpose computing. As previously discussed such a processor is referred to in the industry as an APU a commercial example of which is manufactured by AMD. Similar offerings are available from Intel in their second and third generation core processors such as Sandy Bridge and Ivy Bridge. In addition to x86 x64 cores and graphics processors system logic such as PCIe root complex and DRAM controllers are integrated on the same die along with secondary system interfaces such as system agent Direct Media Interface DMI or United Media Interface UMI link. For convenience a processor of this type is identified in as an APU regardless of the specific configuration. The processor can run on the operating system of the host computer system as part of a symmetric multiprocessing architecture or can run a guest operating system including optional virtual machines and local file systems. The CPU x86 x64 cores may also locally run specific application programming interfaces APIs containing some of the analytics paradigms.

A variation of the embodiment of is illustrated in wherein the entire width all sixteen PCIe lanes of the PCIe interface is dedicated to establish a functional interface with the host computer system via the edge connector . In addition the UMI interface link of the APU comprising 4 PCIe lanes is used to directly communicate with the memory controller . As in the embodiment of dual channel DRAM controllers Dual DC interface with a volatile memory array comprising two DIMMs that may use as a nonlimiting example DDR3 SDRAM technology and the memory controller is coupled to a multi channel non volatile memory array made up of for example NAND flash memory devices or another non volatile memory technology as well as a read ahead and write buffer cache . The APU is again connected to an edge connector through an NTB for electrical and logical isolation of the PCIe and memory domains.

Yet another variation of the embodiment is shown in wherein the UMI interface comprising 4 PCIe lanes establishes functional connectivity between the APU and the host computer system whereas the 16 PCIe interface is used for communication between the APU and the memory controller . This particular arrangement may be particularly advantageous if for example the NVM Express standard SCSI express standard for SCSI commands over a PCIe channel or any other advanced flash addressing protocol is used. Similar as discussed above the PCIe link to the host computer system uses an NTB for electrical and logical isolation of the PCIe and memory domains.

As discussed earlier big data analytics are run in massively parallel configurations which also include clustering of processing devices or client computers as shown in . As noted above the non volatile memory array can be used as a queue for data that may be accessed via an Ethernet or fiber channel FC from a remote location such as a SAN device or a NAS device . Instead of transferring data back to each host computer system and using up costly bandwidth of a client computer s interconnect one possible implementation of the invention uses the video output of the GPU portion for example a display port of the APU to return data through a dedicated cable to either a centralized server or else even to communicate data from one expansion card to another not illustrated .

Likewise a second type of auxiliary connector for example nVidia s SLI or AMD s CrossfireX link may be used to communicate data between expansion cards through a bridge connection of a type known and currently used in SLI or CrossfireX. In the context of the invention this type of bridge connection could comprise a PCIe switch similar to what is represented for the daughter board in but with the bridge connection replacing the daughter board . This type of implementation would have the advantage of better ease of integration into existing form factors since no additional height of the PCIe based expansion cards is incurred through the interposed daughter board .

The above physical description of the invention applies to virtual environments as well. A hypervisor can emulate multiple expansion cards from a single physical expansion card and or such that each virtual machine will see an expansion card. Here the non volatile memory capacity is divided between the virtual machines and the processor s cores are divided virtual machines. The same functionality applies to each virtual expansion card as it applies to a physical expansion card with the non volatile memory and core allocation difference .

In the context of the present disclosure unless specifically indicated to the contrary the term coupled is used to refer to any type of relationship between the components which could be electrically mechanically or logically in a direct or indirect manner. Likewise the terms first second and similar are not meant to establish any hierarchical order or prevalence but merely serve to facilitate the understanding of the disclosure.

While the invention has been described in terms of specific embodiments it is apparent that other forms could be adopted by one skilled in the art. For example functionally equivalent memory technology may supersede the DDR3 GDDR5 MRAM and NAND flash memory noted in this disclosure. In addition other interface technologies may supersede the PCIe interconnect or bridge technology noted herein. Therefore the scope of the invention is to be limited only by the following claims.

