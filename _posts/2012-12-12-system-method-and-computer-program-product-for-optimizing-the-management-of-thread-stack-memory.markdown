---

title: System, method, and computer program product for optimizing the management of thread stack memory
abstract: A system, method, and computer program product for optimizing thread stack memory allocation is disclosed. The method includes the steps of receiving source code for a program, translating the source code into an intermediate representation, analyzing the intermediate representation to identify at least two objects that could use a first allocated memory space in a thread stack memory, and modifying the intermediate representation by replacing references to a first object of the at least two objects with a reference to a second object of the at least two objects.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09411715&OS=09411715&RS=09411715
owner: NVIDIA Corporation
number: 09411715
owner_city: Santa Clara
owner_country: US
publication_date: 20121212
---
The present invention relates to compilers and more particularly to optimizations in memory allocation.

Local memory available for execution of a program is an important resource in a system architecture. Proper management of that resource is crucial to efficient execution of the program. Ideally during execution of the program only memory that includes data for current instructions and future instructions is allocated while any memory containing data used in previously executed instructions is deallocated and returned to a free memory pool. Certain memory may be managed by a programmer such as by using the malloc and free instructions in a program written in C in order to allocate and free blocks of memory explicitly.

However in the case of a thread stack memory i.e. a portion of memory allocated to a program thread that acts as a last in first out LIFO queue the programmer typically does not manage the thread stack memory. Instead the thread stack memory is allocated dynamically as the program executes. Current compilers do not often address optimizations for efficient management of the thread stack memory. Thus there is a need for addressing this issue and or other issues associated with the prior art.

A system method and computer program product for optimizing thread stack memory allocation is disclosed. The method includes the steps of receiving source code for a program translating the source code into an intermediate representation analyzing the intermediate representation to identify at least two objects that could use a first allocated memory space in a thread stack memory and modifying the intermediate representation by replacing references to a first object of the at least two objects with a reference to a second object of the at least two objects.

Some conventional compilers optimize allocation of registers by performing data flow analyses and ordering program instructions in a manner such that certain registers can be reused to store different values at different times in the program execution. A compiler typically translates the source code e.g. a program written in a high level language such as C into an intermediate representation IR which is a data structure that represents the meaning including the execution order of the program. The IR may be an intermediate language for an abstract machine. The IR enables the compiler to perform data flow analysis and rearrange the order of the program before generating the machine code to be executed by a processor.

The optimizations described above in connection with register allocation cannot be applied in the same way to larger memory structures in the thread stack. The objects allocated in the thread stack may be variable in size. In contrast registers have a well defined size such as 32 bits. When a value is stored in a register by an instruction included in a program the value necessarily overwrites all data previously allocated to that register. Objects in the thread stack memory behave differently. When a value is stored in an element of an object such as one entry of an array the value overwrites the data that was previously stored in that entry. However the other entries of the object may still be valid and allocated to the data that was previously stored in the object. A data flow analysis that attempted to track every entry in the thread stack would become very complex and the optimizations that would be able to be achieved may not be effective.

The algorithm described below performs a different type of data flow analysis that tracks whether objects allocated to the thread stack memory i.e. stack allocated objects are live in different parts of the program. The following definitions are used throughout the present disclosure. In the context of the present description an object is live at a certain point i.e. instruction in the program if the data that is stored in the object is potentially needed by the current instruction or a future instruction. An object s def is an instruction that stores data in the object. Examples of an object s def include a store instruction an instruction having a variable on the left hand side LHS of the instruction that points to the object etc. The instruction does not need to overwrite the entire memory allocated to the object but merely needs to write data into a portion of the object. The allocation of memory for an object is not an object s def because the data in the object is not initialized . An object s use is an instruction that uses a variable that has access to the stack allocated object. Examples of an object s use include a load instruction a binary operation involving a pointer to the object etc.

At step the compiler modifies the IR by replacing references to a first object of the at least two objects with a reference to a second object of the at least two objects. In one embodiment when two stack allocated objects may be allocated to the same memory space the compiler will replace instructions that point to the smaller object i.e. in terms of memory size allocated to the object with new instructions that point to the larger object. It should be noted that while various optional features are set forth herein in connection with optimizing allocation of a thread stack memory such features are set forth for illustrative purposes only and should not be construed as limiting in any manner.

In one embodiment the program is a text file e.g. ASCII text that includes a plurality of instructions in a human readable format. The program may be written in a high level programming language such as C . The IR includes representations of the plurality of instructions from the program abstracted out into a plurality of instructions in the intermediate language. For example the intermediate language may be TAC and each instruction of the IR represents one fundamental operation of the target processor e.g. binary operations logical operations load store operations etc. . An instruction in program may not be able to be executed in a single clock cycle. For example a binary operation such a D A B C may not be able to be executed by the processor in a single clock cycle. Therefore in the intermediate language the instruction set forth above may be broken down into two instructions in the intermediate language e.g. a first instruction T1 A B and a second instruction D T1 C .

Once the compiler has generated the IR the compiler performs a data flow analysis on the IR . For example the compiler may rearrange the order of instructions in the IR . By changing the order of instructions the compiler may be able to allocate memory in the thread stack in a more efficient way reusing the same memory locations for different variables in the program . The compiler may analyze the rearranged instructions to find variables that can be assigned to the same memory object and modify the instructions in the IR such that different variables in the program can reuse the same memory object in the thread stack. Once the compiler has modified the IR and made certain optimizations to the code the compiler compiles the IR to generate the machine code for execution by the processor.

The main body of the function g comprises an If . . . Else statement. A first block i.e. lines 10 15 of the program of the If . . . Else statement is executed when parameter n is less than parameter m. A second block i.e. lines 17 25 of the program is executed when parameter n is greater than or equal to parameter m. Then the value of the variable result is returned by the function. Examining the If . . . Else statement in more detail one of skill in the art will notice that the array A is initialized in the first block within a first for loop i.e. lines 10 12 of the program and then a plurality of values of A are added to the variable result within a second for loop i.e. lines 13 15 of the program . The array B is initialized in a third for loop i.e. lines 17 19 of the program the array C is initialized in a fourth for loop i.e. lines 20 22 of the program and then every kvalue of B is multiplied by every kvalue of C and added to the variable result within a fifth for loop i.e. lines 23 25 of the program .

It will be appreciated that each time function g is called by a thread the function will execute either the first block or the second block of the If . . . Else statement but not the first block and the second block. In other words during execution either the array A will be defined and used in one or more instructions or the arrays B and C will be defined and used in one or more instructions based on the conditional statement n

At step for each stack allocated object the compiler analyzes the IR to identify all variables that have access to the stack allocated object. In other words as a result of an instruction a variable that represents the left hand side LHS of an instruction points to the object if one or more variables on the right hand side RHS of the instruction point to the object. For example if a handle a is copied to b b is considered to point to the object allocated to a and thus should be tracked in order to determine when the memory space for a is being used. The specific instructions for which the result may point to a stack allocated object are different depending on the intermediate language utilized by the IR . Types of instructions that point to a stack allocated object include but are not limited to copy instructions binary operations conversions e.g. a type cast PHI nodes i.e. a special instruction used to select a value depending on the predecessor of the current block store instructions and function calls. In one embodiment if a variable points to any location in the memory space allocated to the object the variable is considered to point to the entire object i.e. not just a particular element of the object . Table 1 illustrates an example pseudocode for identifying all variables that have access to a stack allocated object 

At step the compiler generates a liveness web for each stack allocated object. A liveness web for a stack allocated object is a collection of defs i.e. instructions assigning a value to the object and uses i.e. instructions using a value stored by the object as an operand for the stack allocated object that represent when the stack allocated object is live in the program flow. It will be appreciated that the allocation of memory for an object is not a def for that object. An object is live at a certain point in the program if the data that the object contains is potentially needed by the current instruction or by a future instruction. In one embodiment for each stack allocated object the compiler iterates through the instructions in IR and determines if the instruction is either a def or a use or both of the object. If the instruction is either a def or a use then the instruction is added to the liveness web for the object. Table 2 illustrates an example pseudocode for generating the liveness web for a stack allocated object 

At step the compiler tracks whether each stack allocated object is initialized or uninitialized at the beginning and end of each basic block. In one embodiment the compiler iterates through each basic block in the IR and records when defs are encountered for each stack allocated object. A basic block is a portion of code within a program that has one entry point and one exit point i.e. whenever the first instruction in a basic block is executed the rest of the instructions in the basic block are executed exactly once in order . For example in IR of a first basic block extends from line 1 i.e. A alloc in 2000 to line 6 i.e. if n m goto L5 a second basic block extends from line 7 i.e. i 0 to line 8 i.e. if i 2000 goto L2 a third basic block extends from line 9 i.e. t0 i 3 to line 14 i.e. goto L1 and so forth. The compiler may use the liveness web for each object which records each of the defs and uses for an object to determine whether each basic block includes a def for the object. If an object has been initialized on any path leading to the current block then the object is considered initialized in the current block. Table 3 illustrates an example pseudocode for tracking whether each stack allocated object is initialized or uninitialized at each basic block 

At step the compiler determines which stack allocated objects are live at the same points in the IR . After obtaining the liveness webs in step and tracking which objects are initialized and uninitialized at the beginning and end of each basic block in step the compiler maintains a set of stack allocated objects that are live at the beginning and end of each basic block. It will be appreciated that step only determines when an object has been initialized i.e. represented by the defs of the object but in step the compiler determines when the object is no longer needed based on the object s uses. The compiler updates the set of stack allocated objects that are live by iterating through the blocks and the instructions in the function in reverse order taking into account the defs and uses for the object. If at any point in the IR two stack allocated objects are live simultaneously the pair of objects may be marked as a conflict. Each stack allocated object may be associated with a list of the other stack allocated objects that conflict with that stack allocated object. Table 4 illustrates an example pseudocode for determining which stack allocated objects are live at the same time 

At step the compiler determines which objects can be allocated to the same memory space. If two objects do not conflict then the two objects can be allocated to the same memory space. At step the compiler determines how many allocations are actually needed the size of each allocation and which objects will use each allocated space. In one embodiment only objects of the same type and alignment are allocated to the same memory space. For example if there are two allocated arrays of integers with an alignment 4 one array having a size of 2000 integers and the other array having a size of 3000 integers and the two allocated arrays do not conflict then both objects can use a single allocated space of 3000 integers. However if the arrays are of different types such as an array of integers and an array of doubles or of different alignment then the objects are not allocated to the same memory space. In another embodiment objects of different types and alignment can be allocated to the same memory space.

In one embodiment the larger allocations are kept and smaller allocations of the same type use the memory space allocated to the larger object. For each type of object in the IR the compiler sorts the stack allocated objects of that type by size in decreasing order. A set of objects that will remain in the code is maintained. Each object maintains a pointer to the stack allocated object that is going to replace the object if the object is replaced and a set of stack allocated objects that will use the object s allocated memory space if the object remains in the IR . The larger objects will be allocated to the thread stack memory and the rest of the objects are checked to see if they conflict with the first object or with other objects that use that memory space. If the objects do not conflict the objects are assigned to use the same memory space or are put in a worklist. The largest object in the worklist is then allocated to the thread stack memory and the rest of the objects in the worklist are checked to see if they conflict with the first object or with other objects that use that memory space. If the objects do not conflict the objects are assigned to use the same memory space or are put in a new worklist which is again checked like the previous worklist and so forth until all of the objects are allocated to the thread stack memory space or are assigned to use the same memory space as a stack allocated object. Tables 5 and 6 illustrate example pseudocode for determining which objects can be allocated to the same memory space 

It will be appreciated that the framework set forth above may be implemented for a variety of different compilers. In one embodiment the framework may be implemented in a compiler of a parallel processing unit PPU that generates machine code in response to a program generated by an application executing on a central processing unit CPU . The following description illustrates one such architecture that could be used to implement at least a portion of the framework set forth above.

In one embodiment the PPU includes an input output I O unit configured to transmit and receive communications i.e. commands data etc. from a central processing unit CPU not shown over the system bus . The I O unit may implement a Peripheral Component Interconnect Express PCIe interface for communications over a PCIe bus. In alternative embodiments the I O unit may implement other types of well known bus interfaces.

The PPU also includes a host interface unit that decodes the commands and transmits the commands to the grid management unit or other units of the PPU e.g. memory interface as the commands may specify. The host interface unit is configured to route communications between and among the various logical units of the PPU .

In one embodiment a program encoded as a command stream is written to a buffer by the CPU. The buffer is a region in memory e.g. memory or system memory that is accessible i.e. read write by both the CPU and the PPU . The CPU writes the command stream to the buffer and then transmits a pointer to the start of the command stream to the PPU . The host interface unit provides the grid management unit GMU with pointers to one or more streams. The GMU selects one or more streams and is configured to organize the selected streams as a pool of pending grids. The pool of pending grids may include new grids that have not yet been selected for execution and grids that have been partially executed and have been suspended.

A work distribution unit that is coupled between the GMU and the SMs manages a pool of active grids selecting and dispatching active grids for execution by the SMs . Pending grids are transferred to the active grid pool by the GMU when a pending grid is eligible to execute i.e. has no unresolved data dependencies. An active grid is transferred to the pending pool when execution of the active grid is blocked by a dependency. When execution of a grid is completed the grid is removed from the active grid pool by the work distribution unit . In addition to receiving grids from the host interface unit and the work distribution unit the GMU also receives grids that are dynamically generated by the SMs during execution of a grid. These dynamically generated grids join the other pending grids in the pending grid pool.

In one embodiment the CPU executes a driver kernel that implements an application programming interface API that enables one or more applications executing on the CPU to schedule operations for execution on the PPU . An application may include instructions i.e. API calls that cause the driver kernel to generate one or more grids for execution. In one embodiment the PPU implements a SIMD Single Instruction Multiple Data architecture where each thread block i.e. warp in a grid is concurrently executed on a different data set by different threads in the thread block. The driver kernel defines thread blocks that are comprised of k related threads such that threads in the same thread block may exchange data through shared memory. In one embodiment a thread block comprises 32 related threads and a grid is an array of one or more thread blocks that execute the same stream and the different thread blocks may exchange data through global memory. In one embodiment the driver kernel implements a compiler that performs optimizations for thread stack memory allocation when generating threads for execution on PPU .

In one embodiment the PPU comprises X SMs X . For example the PPU may include 15 distinct SMs . Each SM is multi threaded and configured to execute a plurality of threads e.g. 32 threads from a particular thread block concurrently. Each of the SMs is connected to a level two L2 cache via a crossbar or other type of interconnect network . The L2 cache is connected to one or more memory interfaces . Memory interfaces implement 16 32 64 128 bit data buses or the like for high speed data transfer. In one embodiment the PPU comprises U memory interfaces U where each memory interface U is connected to a corresponding memory device U . For example PPU may be connected to up to 6 memory devices such as graphics double data rate version 5 synchronous dynamic random access memory GDDR5 SDRAM .

In one embodiment the PPU implements a multi level memory hierarchy. The memory is located off chip in SDRAM coupled to the PPU . Data from the memory may be fetched and stored in the L2 cache which is located on chip and is shared between the various SMs . In one embodiment each of the SMs also implements an L1 cache. The L1 cache is private memory that is dedicated to a particular SM . Each of the L1 caches is coupled to the shared L2 cache . Data from the L2 cache may be fetched and stored in each of the L1 caches for processing in the functional units of the SMs .

In one embodiment the PPU comprises a graphics processing unit GPU such as the GPU . The PPU is configured to receive commands that specify shader programs for processing graphics data. Graphics data may be defined as a set of primitives such as points lines triangles quads triangle strips and the like. Typically a primitive includes data that specifies a number of vertices for the primitive e.g. in a model space coordinate system as well as attributes associated with each vertex of the primitive. The PPU can be configured to process the graphics primitives to generate a frame buffer i.e. pixel data for each of the pixels of the display . The driver kernel implements a graphics processing pipeline such as the graphics processing pipeline defined by the OpenGL API.

An application writes model data for a scene i.e. a collection of vertices and attributes to memory. The model data defines each of the objects that may be visible on a display. The application then makes an API call to the driver kernel that requests the model data to be rendered and displayed. The driver kernel reads the model data and writes commands to the buffer to perform one or more operations to process the model data. The commands may encode different shader programs including one or more of a vertex shader hull shader geometry shader pixel shader etc. For example the GMU may configure one or more SMs to execute a vertex shader program that processes a number of vertices defined by the model data. In one embodiment the GMU may configure different SMs to execute different shader programs concurrently. For example a first subset of SMs may be configured to execute a vertex shader program while a second subset of SMs may be configured to execute a pixel shader program. The first subset of SMs processes vertex data to produce processed vertex data and writes the processed vertex data to the L2 cache and or the memory . After the processed vertex data is rasterized i.e. transformed from three dimensional data into two dimensional data in screen space to produce fragment data the second subset of SMs executes a pixel shader to produce processed fragment data which is then blended with other processed fragment data and written to the frame buffer in memory . The vertex shader program and pixel shader program may execute concurrently processing different data from the same scene in a pipelined fashion until all of the model data for the scene has been rendered to the frame buffer. Then the contents of the frame buffer are transmitted to a display controller for display on a display device.

The PPU may be included in a desktop computer a laptop computer a tablet computer a smart phone e.g. a wireless hand held device personal digital assistant PDA a digital camera a hand held electronic device and the like. In one embodiment the PPU is embodied on a single semiconductor substrate. In another embodiment the PPU is included in a system on a chip SoC along with one or more other logic units such as a reduced instruction set computer RISC CPU a memory management unit MMU a digital to analog converter DAC and the like.

In one embodiment the PPU may be included on a graphics card that includes one or more memory devices such as GDDR5 SDRAM. The graphics card may be configured to interface with a PCIe slot on a motherboard of a desktop computer that includes e.g. a northbridge chipset and a southbridge chipset. In yet another embodiment the PPU may be an integrated graphics processing unit iGPU included in the chipset i.e. Northbridge of the motherboard.

It will be appreciated that a master thread may be configured to execute on a first SM of PPU . In addition two or more child threads may be configured to execute on two or more additional SMs e.g. etc. . The master thread and child threads may access motion vector data stored in a memory by a hardware video encoder .

As described above the work distribution unit dispatches active grids for execution on one or more SMs of the PPU . The scheduler unit receives the grids from the work distribution unit and manages instruction scheduling for one or more thread blocks of each active grid. The scheduler unit schedules threads for execution in groups of parallel threads where each group is called a warp. In one embodiment each warp includes 32 threads. The scheduler unit may manage a plurality of different thread blocks allocating the thread blocks to warps for execution and then scheduling instructions from the plurality of different warps on the various functional units i.e. cores DPUs SFUs and LSUs during each clock cycle.

In one embodiment each scheduler unit includes one or more instruction dispatch units . Each dispatch unit is configured to transmit instructions to one or more of the functional units. In the embodiment shown in the scheduler unit includes two dispatch units that enable two different instructions from the same warp to be dispatched during each clock cycle. In alternative embodiments each scheduler unit may include a single dispatch unit or additional dispatch units .

Each SM includes a register file that provides a set of registers for the functional units of the SM . In one embodiment the register file is divided between each of the functional units such that each functional unit is allocated a dedicated portion of the register file . In another embodiment the register file is divided between the different warps being executed by the SM . The register file provides temporary storage for operands connected to the data paths of the functional units.

Each SM comprises L processing cores . In one embodiment the SM includes a large number e.g. 192 etc. of distinct processing cores . Each core is a fully pipelined single precision processing unit that includes a floating point arithmetic logic unit and an integer arithmetic logic unit. In one embodiment the floating point arithmetic logic units implement the IEEE 754 2008 standard for floating point arithmetic. Each SM also comprises M DPUs that implement double precision floating point arithmetic N SFUs that perform special functions e.g. copy rectangle pixel blending operations and the like and P LSUs that implement load and store operations between the shared memory L1 cache and the register file . In one embodiment the SM includes 64 DPUs 32 SFUs and 32 LSUs .

Each SM includes an interconnect network that connects each of the functional units to the register file and the shared memory L1 cache . In one embodiment the interconnect network is a crossbar that can be configured to connect any of the functional units to any of the registers in the register file or the memory locations in shared memory L1 cache .

In one embodiment the SM is implemented within a GPU. In such an embodiment the SM comprises J texture units . The texture units are configured to load texture maps i.e. a 2D array of texels from the memory and sample the texture maps to produce sampled texture values for use in shader programs. The texture units implement texture operations such as anti aliasing operations using mip maps i.e. texture maps of varying levels of detail . In one embodiment the SM includes 16 texture units .

The PPU described above may be configured to perform highly parallel computations much faster than conventional CPUs. Parallel computing has advantages in graphics processing data compression biometrics stream processing algorithms and the like.

The system also includes input devices a graphics processor and a display i.e. a conventional CRT cathode ray tube LCD liquid crystal display LED light emitting diode plasma display or the like. User input may be received from the input devices e.g. keyboard mouse touchpad microphone and the like. In one embodiment the graphics processor may include a plurality of shader modules a rasterization module etc. Each of the foregoing modules may even be situated on a single semiconductor platform to form a graphics processing unit GPU .

In the present description a single semiconductor platform may refer to a sole unitary semiconductor based integrated circuit or chip. It should be noted that the term single semiconductor platform may also refer to multi chip modules with increased connectivity which simulate on chip operation and make substantial improvements over utilizing a conventional central processing unit CPU and bus implementation. Of course the various modules may also be situated separately or in various combinations of semiconductor platforms per the desires of the user.

The system may also include a secondary storage . The secondary storage includes for example a hard disk drive and or a removable storage drive representing a floppy disk drive a magnetic tape drive a compact disk drive digital versatile disk DVD drive recording device universal serial bus USB flash memory. The removable storage drive reads from and or writes to a removable storage unit in a well known manner.

Computer programs or computer control logic algorithms may be stored in the main memory and or the secondary storage . Such computer programs when executed enable the system to perform various functions. The memory the storage and or any other storage are possible examples of computer readable media. Program IR IR machine code and compiler may be stored in the main memory and or the secondary storage . The compiler is then executed by processor to generate the optimized machine code .

In one embodiment the architecture and or functionality of the various previous figures may be implemented in the context of the central processor the graphics processor an integrated circuit not shown that is capable of at least a portion of the capabilities of both the central processor and the graphics processor a chipset i.e. a group of integrated circuits designed to work and sold as a unit for performing related functions etc. and or any other integrated circuit for that matter.

Still yet the architecture and or functionality of the various previous figures may be implemented in the context of a general computer system a circuit board system a game console system dedicated for entertainment purposes an application specific system and or any other desired system. For example the system may take the form of a desktop computer laptop computer server workstation game consoles embedded system and or any other type of logic. Still yet the system may take the form of various other devices including but not limited to a personal digital assistant PDA device a mobile phone device a television etc.

Further while not shown the system may be coupled to a network e.g. a telecommunications network local area network LAN wireless network wide area network WAN such as the Internet peer to peer network cable network or the like for communication purposes.

While various embodiments have been described above it should be understood that they have been presented by way of example only and not limitation. Thus the breadth and scope of a preferred embodiment should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents.

