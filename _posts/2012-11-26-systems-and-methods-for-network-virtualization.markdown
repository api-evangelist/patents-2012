---

title: Systems and methods for network virtualization
abstract: Message publish/subscribe systems are required to process high message volumes with reduced latency and performance bottlenecks. The end-to-end middleware architecture proposed by the present invention is designed for high-volume, low-latency messaging by, among other things, reducing intermediary hops with neighbor-based routing, introducing efficient native-to-external and external-to-native protocol conversions, monitoring system performance, including latency, in real time, employing topic-based and channel-based message communications, and dynamically optimizing system interconnect configurations and message transmission protocols.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09253243&OS=09253243&RS=09253243
owner: Tervela, Inc.
number: 09253243
owner_city: New York
owner_country: US
publication_date: 20121126
---
This application is a continuation of U.S. application Ser. No. 13 152 939 filed Jun. 3 2011 which claims the benefit of and priority to U.S. Provisional Patent Application No. 60 641 988 filed Jan. 6 2005 U.S. Provisional Patent Application No. 60 688 983 filed Jun. 8 2005 and U.S. patent application Ser. No. 11 316 778 filed Dec. 23 2005 the complete disclosures of all of which are incorporated by reference herein.

The present invention relates to data messaging and more particularly to middleware architecture of publish subscribe systems.

The increasing level of performance required by data messaging infrastructures provides a compelling rationale for advances in networking infrastructure and protocols. Fundamentally data distribution involves various sources and destinations of data as well as various types of interconnect architectures and modes of communications between the data sources and destinations. Examples of existing data messaging architectures include hub and spoke peer to peer and store and forward.

With the hub and spoke system configuration all communications are transported through the hub often creating performance bottlenecks when processing high volumes. Therefore this messaging system architecture produces latency. One way to work around this bottleneck is to deploy more servers and distribute the network load across these different servers. However such architecture presents scalability and operational problems. By comparison to a system with the hub and spoke configuration a system with a peer to peer configuration creates unnecessary stress on the applications to process and filter data and is only as fast as its slowest consumer or node. Then with a store and forward system configuration in order to provide persistence the system stores the data before forwarding it to the next node in the path. The storage operation is usually done by indexing and writing the messages to disk which potentially creates performance bottlenecks. Furthermore when message volumes increase the indexing and writing tasks can be even slower and thus can introduces additional latency.

Existing data messaging architectures share a number of deficiencies. One common deficiency is that data messaging in existing architectures relies on software that resides at the application level. This implies that the messaging infrastructure experiences OS operating system queuing and network I O input output which potentially create performance bottlenecks. Another common deficiency is that existing architectures use data transport protocols statically rather than dynamically even if other protocols might be more suitable under the circumstances. A few examples of common protocols include routable multicast broadcast or unicast. Indeed the application programming interface API in existing architectures is not designed to switch between transport protocols in real time.

Also network configuration decisions are usually made at deployment time and are usually defined to optimize one set of network and messaging conditions under specific assumptions. The limitations associated with static fixed configuration preclude real time dynamic network reconfiguration. In other words existing architectures are configured for a specific transport protocol which is not always suitable for all network data transport load conditions and therefore existing architectures are often incapable of dealing in real time with changes or increased load capacity requirements.

Furthermore when data messaging is targeted for particular recipients or groups of recipients existing messaging architectures use routable multicast for transporting data across networks. However in a system set up for multicast there is a limitation on the number of multicast groups that can be used to distribute the data and as a result the messaging system ends up sending data to destinations which are not subscribed to it i.e. consumers which are not subscribers . This increases consumers data processing load and discard rate due to data filtering. Then consumers that become overloaded for any reason and cannot keep up with the flow of data eventually drop incoming data and later ask for retransmissions. Retransmissions affect the entire system in that all consumers receive the repeat transmissions and all of them re process the incoming data. Therefore retransmissions can cause multicast storms and eventually bring the entire networked system down.

When the system is set up for unicast messaging as a way to reduce the discard rate the messaging system may experience bandwidth saturation because of data duplication. For instance if more than one consumer subscribes to a given topic of interest the messaging system has to deliver the data to each subscriber and in fact it sends a different copy of this data to each subscriber. And although this solves the problem of consumers filtering out non subscribed data unicast transmission is non scalable and thus not adaptable to substantially large groups of consumers subscribing to a particular data or to a significant overlap in consumption patterns.

One more common deficiency of existing architectures is their slow and often high number of protocol transformations. The reason for this is the IT information technology band aid strategy in the Enterprise Application Integration EIA domain where more and more new technologies are integrated with legacy systems.

Hence there is a need to improve data messaging systems performance in a number of areas. Examples where performance might need improvement are speed resource allocation latency and the like.

The present invention is based in part on the foregoing observations and on the idea that such deficiencies can be addressed with better results using a different approach. These observations gave rise to the end to end message publish subscribe architecture for high volume and low latency messaging. So therefore a data distribution system with an end to end message publish subscribe architecture in accordance with the principles of the present invention can advantageously route significantly higher message volumes with significantly lower latency by among other things reducing intermediary hops with neighbor based routing and network disintermediation introducing efficient native to external and external to native protocol conversions monitoring system performance including latency in real time employing topic based and channel based message communications and dynamically and intelligently optimizing system interconnect configurations and message transmission protocols. In addition such system can provide guaranteed delivery quality of service with data caching.

In connection with resource allocation a data distribution system in accordance with the present invention produces the advantage of dynamically allocating available resources in real time. To this end instead of the conventional static configuration approach the present invention contemplates a system with real time dynamic learned approach to resource allocation. Examples where resource allocation can be optimized in real time include network resources usage of bandwidth protocols paths routes and consumer system resources usage of CPU memory disk space .

In connection with monitoring system topology and performance a data distribution system in accordance with the present invention advantageously distinguishes between message level and frame level latency measurements. In certain cases the correlation between these measurements provides a competitive business advantage. In other words the nature and extent of latency may indicate best data and source of data which in turn may be useful in business processes and provide a competitive edge.

Thus in accordance with the purpose of the invention as shown and broadly described herein one exemplary system with a publish subscribe middleware architecture includes one or more than one messaging appliance configured for receiving and routing messages an interconnect and a provisioning and management system linked via the interconnect and configured for exchanging administrative messages with each messaging appliance. In such system the messaging appliance executes the routing of messages by dynamically selecting a message transmission protocol and a message routing path.

In addition the publish subscribe system can be centrally monitored and configured from the provisioning and management system. This provides a manageable and scalable single point configuration for entitlements user management digital rights management schemas etc.

Moreover the foregoing system can be implemented with one or more namespace domains and if indeed more than one space domain exists the system further includes a domain interconnect medium for connecting between the namespace domains. This domain interconnect medium can be for instance any networking infrastructure.

In yet another embodiment an enterprise system with a publish subscribe middleware architecture includes a market data delivery infrastructure having one or more messaging appliances for receiving and routing market data messages a market order routing infrastructure having one or more messaging appliances for receiving and routing transaction order messages and an intermediate infrastructure in communication link with the market data delivery and market order routing infrastructures respectively. In this system the intermediary infrastructure includes one or more than one messaging appliance configured for receiving and routing the market data and transaction order messages an interconnect and a provisioning and management system linked via the interconnect and configured for exchanging administrative messages with each messaging appliance including the messaging appliances in the market data delivery and market order routing infrastructures. Again each of the messaging appliances is further configured for executing the routing of messages it receives by dynamically selecting a message transmission protocol and a message routing path.

Moreover in such enterprise system there are market data sources for publishing the market data messages and market data consumers external data destinations for receiving the market data messages and for publishing the transaction order messages. The market data consumers include one or more applications. Therefore the intermediate infrastructure includes an application programming interface between each of the applications and one of the messaging appliances in the intermediate infrastructure to which such application programming interface is registered. The application programming interfaces are operative for delivering the market data messages to the applications and transaction order messages from the applications. This system further includes protocol transformation engines for translating between native and external message protocols of incoming and outgoing message respectively.

In sum these and other features aspects and advantages of the present invention will become better understood from the description herein appended claims and accompanying drawings as hereafter described.

Before outlining the details of various embodiments in accordance with aspects and principles of the present invention the following is a brief explanation of some terms that may be used throughout this description. It is noted that this explanation is intended to merely clarify and give the reader an understanding of how such terms might be used but without limiting these terms to the context in which they are used and without limiting the scope of the claims thereby.

The term middleware is used in the computer industry as a general term for any programming that mediates between two separate and often already existing programs. Typically middleware programs provide messaging services so that different applications can communicate. The systematic tying together of disparate applications often through the use of middleware is known as enterprise application integration EAI . In this context however middleware can be a broader term used in the context of messaging between source and destination and the facilities deployed to enable such messaging and thus middleware architecture covers the networking and computer hardware and software components that facilitate effective data messaging individually and in combination as will be described below. Moreover the terms messaging system or middleware system can be used in the context of publish subscribe systems in which messaging servers manage the routing of messages between publishers and subscribers. Indeed the paradigm of publish subscribe in messaging middleware is a scalable and thus powerful model.

The term consumer may be used in the context of client server applications and the like. In one instance a consumer is a system or an application that uses an application programming interface API to register to a middleware system to subscribe to information and to receive data delivered by the middleware system. An API inside the middleware architecture boundaries is a consumer and an external consumer is any publish subscribe system or external data destination that doesn t use the API and for communications with which messages go through protocol transformation as will be later explained .

The term external data source may be used in the context of data distribution and message publish subscribe systems. In one instance an external data source is regarded as a system or application located within or outside the enterprise private network which publishes messages in one of the common protocols or its own message protocol. An example of an external data source is a market data exchange that publishes stock market quotes which are distributed to traders via the middleware system. Another example of an external data source is transactional data. Note that in a typical implementation of the present invention as will be later described in more detail the middleware architecture adopts its unique native protocol to which data from external data sources is converted once it enters the middleware system domain thereby avoiding multiple protocol transformations typical of conventional systems.

The term external data destination is also used in the context of data distribution and message publish subscribe systems. An external data destination is for instance a system or application located within or outside the enterprise private network which is subscribing to information routed via a local global network. One example of an external data destination could be the aforementioned market data exchange that handles transaction orders published by the traders. Yet another embodiment of the external data destination is transactional data. Note that in the foregoing middleware architecture messages directed to an external data destination are translated from the native protocol to the external protocol associated with the external data destination.

As can be ascertained from the description herein the present invention can be practiced in various ways with various configurations each embodying a middleware architecture. An example of an end to end middleware architecture in accordance with the principles of the present invention is shown in .

This exemplary architecture combines a number of beneficial features which include messaging common concepts APIs fault tolerance provisioning and management P M quality of service QoS conflated best effort guaranteed while connected guaranteed while disconnected etc. persistent caching for guaranteed delivery QoS management of namespace and security service a publish subscribe ecosystem core ingress and egress components transport transparent messaging neighbor based messaging a model that is a hybrid between hub and spoke peer to peer and store and forward and which uses a subscription based routing protocol that can propagate the subscriptions to all neighbors as necessary late schema binding partial publishing publishing changed information only as opposed to the entire data and dynamic allocation of network and system resources. As will be later explained the publish subscribe system advantageously incorporates a fault tolerant design of the middleware architecture. Note that the core MAs portion of the publish subscribe ecosystem uses the aforementioned native messaging protocol native to the middleware system while the ingress and egress portions the edge MAs translate to and from this native protocol respectively.

In addition to the publish subscribe system components the diagram of shows the logical connections and communications between them. As can be seen the illustrated middleware architecture is that of a distributed system. In a system with this architecture a logical communication between two distinct physical components is established with a message stream and associated message protocol. The message stream contains one of two categories of messages administrative and data messages. The administrative messages are used for management and control of the different physical components management of subscriptions to data and more. The data messages are used for transporting data between sources and destinations and in a typical publish subscribe messaging there are multiple senders and multiple receivers of data messages.

With the structural configuration and logical communications as illustrated the distributed publish subscribe system with the middleware architecture is designed to perform a number of logical functions. One logical function is message protocol translation which is advantageously performed at an edge messaging appliance MA component. A second logical function is routing the messages from publishers to subscribers. Note that the messages are routed throughout the publish subscribe network. Thus the routing function is performed by each MA where messages are propagated say from an edge MA or API to a core MA or from one core MA to another core MA and eventually to an edge MA e.g. or API . The API communicates with applications via an inter process communication bus sockets shared memory etc. .

A third logical function is storing messages for different types of guaranteed delivery quality of service including for instance guaranteed while connected and guaranteed while disconnected. A fourth function is delivering these messages to the subscribers. As shown an API delivers messages to subscribing applications 

In this publish subscribe middleware architecture the system configuration function as well as other administrative and system performance monitoring functions are managed by the P M system. Additionally the MAs are deployed as edge MAs or core MAs depending on their role in the network. An edge MA is similar to a core MA in most respects except that it includes a protocol translation engine that transforms messages from external to native protocols and from native to external protocols. Thus in general the boundaries of the publish subscribe system middleware architecture are characterized by its edges at which there are edge MAs and APIs and within these boundaries there are core MAs 

In a typical system the core MAs route the published messages internally within the system towards the edge MAs or APIs e.g. APIs . The routing map particularly in the core MAs is designed for maximum volume low latency and efficient routing. Moreover the routing between the core MAs can change dynamically in real time. For a given messaging path that traverses a number of nodes core MAs a real time change of routing is based on one or more metrics including network utilization overall end to end latency communications volume network delay loss and jitter.

Alternatively instead of dynamically selecting the best performing path out of two or more diverse paths the MA can perform multi path routing based on message replication and thus send the same message across all paths. All the MAs located at convergence points of diverse paths will drop the duplicated messages and forward only the first arrived message. This routing approach has the advantage of optimizing the messaging infrastructure for low latency although the drawback of this routing method is that the infrastructure requires more network bandwidth to carry the duplicated traffic.

Note that the system architecture is not confined to a particular limited geographic area and in fact is designed to transcend regional or national boundaries and even span across continents. In such cases the edge MAs in one network can communicate with the edge MAs in another geographically distant network via existing networking infrastructures.

The edge MAs have the ability to convert any external message protocol of incoming messages to the middleware system s native message protocol and from native to external protocol for outgoing messages. That is an external protocol is converted to the native e.g. Tervela message protocol when messages are entering the publish subscribe network domain ingress and the native protocol is converted into the external protocol when messages exit the publish subscribe network domain egress . Another function of edge MAs is to deliver the published messages to the subscribing external data destinations.

Additionally both the edge and the core MAs and are capable of storing the messages before forwarding them. One way this can be done is with a caching engine CE . One or more CEs can be connected to the same MA. Theoretically the API is said not to have this store and forward capability although in reality an API could store messages before delivering them to the application and it can store messages received from applications before delivering them to a core MA edge MA or another API.

When an MA edge or core MA has an active connection to a CE it forwards all or a subset of the routed messages to the CE which writes them to a storage area for persistency. For a predetermined period of time these messages are then available for retransmission upon request. Examples where this feature is implemented are data replay partial publish and various quality of service levels. Partial publish is effective in reducing network and consumers load because it requires transmission only of updated information rather than of all information.

To illustrate how the routing maps might effect routing a few examples of the publish subscribe routing paths are shown in . In this illustration the middleware architecture of the publish subscribe network provides five or more different communication paths between publishers and subscribers.

The first communication path links an external data source to an external data destination. The published messages received from the external data source are translated into the native e.g. Tervela message protocol and then routed by the edge MA . One way the native protocol messages can be routed from the edge MA is to an external data destination . This path is called out as communication path . In this case the native protocol messages are converted into the external protocol messages suitable for the external data destination. Another way the native protocol messages can be routed from the edge MA is internally through a core MA . This path is called out as communication path . Along this path the core MAroutes the native messages to an edge MA . However before the edge MA routes the native protocol messages to the external data destination it converts them into an external message protocol suitable for this external data destination . As can be seen this communication path doesn t require the API to route the messages from the publishers to the subscribers. Therefore if the publish subscribe system is used for external source to destination communications the system need not include an API.

Another communication path called out as communications path links an external data source to an application using the API . Published messages received from the external data source are translated at the edge MA into the native message protocol and are then routed by the edge MA to a core MA . From the first core MA the messages are routed through another core MA to the API . From the API the messages are delivered to subscribing applications e.g. . Because the communication paths are bidirectional in another instance messages could follow a reverse path from the subscribing applications to the external data destination . In each instance core MAs receive and route native protocol messages while edge MAs receive external or native protocol messages and respectively route native or external protocol messages edge MAs translate to from such external message protocol to from the native message protocol . Each of the edge MAs can route an ingress message simultaneously to both native protocol channels and external protocol channels. As a result each edge MA can route an ingress message simultaneously to both external and internal consumers where internal consumers consume native protocol messages and external consumers consume external protocol messages. This capability enables the messaging infrastructure to seamlessly and smoothly integrate with legacy applications and systems.

Yet another communication path called out as communications path links two applications both using an API . At least one of the applications publishes messages or subscribes to messages. The delivery of published messages to or from subscribing or publishing applications is done via an API that sits on the edge of the publish subscribe network. When applications subscribe to messages one of the core or edge MAs routes the messages towards the API which in turn notifies the subscribing applications when the data is ready to be delivered to them. Messages published from an application are sent via the API to the core MA to which the API is registered .

Note that by registering logging in to an MA the API becomes logically connected to it. An API initiates the connection to the MA by sending a registration a log in request message to the MA. After registration the API can subscribe to particular topics of interest by sending its subscription messages to the MA. Topics are used for publish subscribe messaging to define shared access domains and the targets for a message and therefore a subscription to one or more topics permits reception and transmission of messages with such topic notations. The P M sends to the MAs in the network periodic entitlement updates and each MA updates its own table accordingly. Hence if the MA find the API to be entitled to subscribe to a particular topic the MA verifies the API s entitlements using the routing entitlements table the MA activates the logical connection to the API. Then if the API is properly registered with it the core MA routes the data to the second API as shown. In other instances this core MA may route the messages through additional one or more core MAs not shown which route the messages to the API that in turn delivers the messages to subscribing applications .

As can be seen communications path doesn t require the presence of an edge MA because it doesn t involve any external data message protocol. In one embodiment exemplifying this kind of communications path an enterprise system is configured with a news server that publishes to employees the latest news on various topics. To receive the news employees subscribe to their topics of interest via a news browser application using the API.

Note that the middleware architecture allows subscription to one or more topics. Moreover this architecture allows subscription to a group of related topics with a single subscription request by allowing wildcards in the topic notation.

Yet another path called out as communications path is one of the many paths associated with the P M system and with each of them linking the P M to one of the MAs in the publish subscribe network middleware architecture. The messages going back and forth between the P M system and each MA are administrative messages used to configure and monitor that MA. In one system configuration the P M system communicates directly with the MAs. In another system configuration the P M system communicates with MAs through other MAs. In yet another configuration the P M system can communicate with the MAs both directly or indirectly.

In a typical implementation the middleware architecture can be deployed over a network with switches router and other networking appliances and it employs channel based messaging capable of communications over any type of physical medium. One exemplary implementation of this fabric agnostic channel based messaging is an IP based network. In this environment all communications between all the publish subscribe physical components are performed over UDP User Datagram Protocol and the transport reliability is provided by the messaging layer. An overlay network according to this principle is illustrated in

As shown overlay communications and can occur between the three core MAs via switches a router and subnets . In other words these communication paths can be established on top of the underlying network which is composed of networking infrastructure such as subnets switches and routers and as mentioned this architecture can span over a large geographic area different countries and even different continents .

Notably the foregoing and other end to end middleware architectures according to the principles of the present invention can be implemented in various enterprise infrastructures in various business environments. One such implementation is illustrated on .

In this enterprise infrastructure a market data distribution plant is built on top of the publish subscribe network for routing stock market quotes from the various market data exchanges to the traders applications not shown . Such an overlay solution relies on the underlying network for providing interconnects for instance between the MAs as well as between such MAs and the P M system. Market data delivery to the APIs is based on applications subscription. With this infrastructure traders using the applications not shown can place transaction orders that are routed from the APIs through the publish subscribe network via core MAs and the edge MA back to the market data exchanges 

An example of the underlying physical deployment is illustrated on . As shown the MAs are directly connected to each other and plugged directly into the networks and subnets in which the consumers and publishers of messaging traffic are physically connected. In this case interconnects would be the direct connections say between the MAs as well as between them and the P M system. This enables a network backbone disintermediation and a physical separation of the messaging traffic from other enterprise applications traffic. Effectively the MAs can be used to remove the reliance on traditional routed network for the messaging traffic.

In this example of physical deployment the external data sources or destinations such as market data exchanges are directly connected to edge MAs for instane edge MA . The consuming or publishing applications of messaging traffic such as market trading applications are connected to the subnets . These applications have at least two ways to subscribe publish or communicate with other applications. The application could either use the enterprise backbone composed of multiple layers of redundant routers and switches which carries all enterprise application traffic such as messaging traffic or use the messaging backbone composed of edge and core MAs directly interconnected to each other via an integrated switch. Using an alternative backbone has the benefit of isolating the messaging traffic from other enterprise application traffic and thus better controlling the performance of the messaging traffic. In one implementation an application located in subnet logically or physically connected to the core MA subscribes to or publishes messaging traffic in the native protocol using the Tervela API. In another implementation an application located in subnet logically or physically connected to the edge MA subscribes to or publishes the messaging traffic in an external protocol where the MA performs the protocol transformation using the integrated protocol transformation engine module.

Logically the physical components of the publish subscribe network are built on a messaging transport layer akin to layers 1 to 4 of the Open Systems Interconnection OSI reference model. Layers 1 to 4 of the OSI model are respectively the Physical Data Link Network and Transport layers.

Thus in one embodiment of the invention the publish subscribe network can be directly deployed into the underlying network fabric by for instance inserting one or more messaging line card in all or a subset of the network switches and routers. In another embodiment of the invention the publish subscribe network can be deployed as a mesh overlay network in which all the physical components are connected to each other . For instance a fully meshed network of 4 MAs is a network in which each of the MAs is connected to each of its 3 peer MAs. In a typical implementation the publish subscribe network is a mesh network of one or more external data sources and or destinations one or more provisioning and management P M systems one or more messaging appliances MAs one or more optional caching engines CE and one or more optional application programming interfaces APIs .

As will be later explained in more detail reliability availability and consistency are often necessary in enterprise operations. For this purpose the publish subscribe system can be designed for fault tolerance with several of its components being deployed as fault tolerant systems. For instance MAs can be deployed as fault tolerant MA pairs where the first MA is called the primary MA and the second MA is called the secondary MA or fault tolerant MA FT MA . Again for store and forward operations the CE cache engine can be connected to a primary or secondary core edge MA. When a primary or secondary MA has an active connection to a CE it forwards all or a subset of the routed messages to that CE which writes them to a storage area for persistency. For a predetermined period of time these messages are then available for retransmission upon request.

Notably communications throughout the publish subscribe network are conducted using the native protocol messages independently from the underlying transport logic. This is why we refer to this architecture as a transport transparent channel based messaging architecture.

In other words a channel manages the OSI transport to physical layers . Optimization of channel resources is done on a per channel basis e.g. message density optimization for the physical medium based on consumption patterns including bandwidth message size distribution channel destination resources and channel health statistics . Then because the communication channels are fabric agnostic no particular type of fabric is required. Indeed any fabric medium will do e.g. ATM Infiniband or Ethernet.

Incidentally message fragmentation or re assembly may be needed when for instance a single message is split across multiple frames or multiple messages are packed in a single frame Message fragmentation or reassembly is done before delivering messages to the channel management layer.

In another implementation the channel is established over an Infiniband interconnect using a native Infiniband transport protocol where the Infiniband fabric is the physical medium. In this implementation the channel is node based and communications between the source and destination are node based using their respective node addresses. In yet another implementation the channel is memory based such as RDMA Remote Direct Memory Access and referred to here as direct connect DC . With this type of channel messages are sent from a source machine directly into the destination machine s memory thus bypassing the CPU processing to handle the message from the NIC to the application memory space and potentially bypassing the network overhead of encapsulating messages into network packets.

As to the native protocol one approach uses the aforementioned native Tervela message protocol. Conceptually the Tervela message protocol is similar to an IP based protocol. Each message contains a message header and a message payload. The message header contains a number of fields one of which is for the topic information. As mentioned a topic is used by consumers to subscribe to a shared domain of information.

In some embodiment the topic information in the message might be encoded or mapped to a key which can be one or more integer values. Then each topic would be mapped to a unique key and the mapping database between topics and keys would be maintained by the P M system and updated over the wire to all MAs. As a result when an API subscribes or publishes to one topic the MA is able to return the associated unique key that is used for the topic field of the message.

Preferably the subscription format will follow the same format as the message topic. However the subscription format also supports wildcards that match any topic substring or regular expression pattern match against the topic. Handling of wildcard mapping to actual topics may be dependant on the P M subsystem or handled by the MA depending on complexity of the wildcard or pattern match request.

Example 1 A string with a wildcard of T. .T.T would match T.T.T.T and T.T.T.T but would not match T.T.T.T.T

Example 2 A string with wildcards of T. .T.T. would not match T.T.T.T and T.T.T.T but it would match T.T.T.T.T

Example 3 A string with wildcards of T. .T.T. optional 5element would match T.T.T.T T.T.T.T and T.T.T.T.T but would not match T.T.T.T.T.T

Example 4 A string with a wildcard of T.T .T.T would match T.T.T.T and T.T.T.T but would not match T.T.T.T

Example 5 A string with wildcards of T. .T.T. any number of trailing elements would match T.T.T.T T.T.T.T T.T.T.T.T and T.T.T.T.T.T.

A tree includes nodes e.g. T . . . T connected by edges where each sub string of a topic subscription corresponds to a node in the tree. The channels mapped to a given subscription are stored on the leaf node of that subscription indicating for each leaf node the list of channels from where the topic subscription came i.e. through which subscription requests were received . This list indicates which channel should receive a copy of the message whose topic notation matches the subscription. As shown the message routing lookup takes a message topic as input and parse the tree using each substring of that topic to locate the different channels associated with the incoming message topic. For instance T T T Tand Tare directed to channels and T T and T are directed to channel T T T T and Tare directed to channels and T T T Tand Tare directed to channel and T T T T and Tare directed to channel .

Although selection of the routing table structure is intended to optimize the routing table lookup performance of the lookup depends also on the search algorithm for finding the one or more topic subscriptions that match an incoming message topic. Therefore the routing table structure should be able to accommodate such algorithm and vice versa. One way to reduce the size of the routing table is by allowing the routing algorithm to selectively propagate the subscriptions throughout the entire publish subscribe network. For example if a subscription appears to be a subset of another subscription e.g. a portion of the entire string that has already been propagated there is no need to propagate the subset subscription since the MAs already have the information for the superset of this subscription.

Based on the foregoing the preferred message routing protocol is a topic based routing protocol where entitlements are indicated in the mapping between subscribers and respective topics. Entitlements are designated per subscriber or groups classes of subscribers and indicate what messages the subscriber has a right to consume or which messages may be produced published by such producer publisher . These entitlements are defined in the P M system communicated to all MAs in the publish subscribe network and then used by the MA to create and update their routing tables.

Each MA updates its routing table by keeping track of who is interested in requesting subscription to what topic. However before adding a route to its routing table the MA has to check the subscription against the entitlements of the publish subscribe network. The MA verifies that a subscribing entity which can be a neighboring MA the P M system a CE or an API is authorized to do so. If the subscription is valid the route will be created and added to the routing table. Then because some entitlements may be known in advance the system can be deployed with predefined entitlements and these entitlements can be automatically loaded at boot time. For instance some specific administrative messages such as configuration updates or the like might be always forwarded throughout the network and therefore automatically loaded at startup time.

In addition to its role in the subscription process the P M system has a number of other management functions. These additional functions include publish subscribe system configuration and health monitoring and reporting. Configuration involves both physical and logical configuration of the publish subscribe system network and components. The monitoring and reporting involves monitoring the health of all network and system components and reporting the results automatically per demand or to a log.

The P M system performs its configuration monitoring and reporting functions via administrative messages which are obtained from the administrative message layer in the appliance message layer . Communications with other components in the network are done via the messaging transport layer with all the aforementioned channel management which is typical to components in a system configured in accordance with the principles of the present invention. However unlike the message transport layer in the MA which interacts directly with the physical medium interfaces the P M system is often implemented on top of the operating system OS through which the messaging transport layer communicates with the physical medium interfaces interfaces 1 . . . N . Hence in order to support the various types of channels the OS may require particular drivers for each physical medium that would not otherwise be made available with the OS. The OS might also require a particular interface card for the medium e.g. direct connect interface card or Infiniband interface card .

The P M might also use a network management stack to communicate with the network based management services. Examples of such network based services include SNMP simple network management protocol system logging HTTP HTTPS hypertext transfer protocol over Secure Socket Layer Telnet SSH Secure Shell Protocol .

The P M may be provided with a graphical user interface GUI built on top of a number of functional blocks. Examples of such functional blocks include the configuration manager the real time monitoring block the historical trending block and the business logic application reporting block . The configuration manager functional block handles the configuration of all the physical components involved in the publish subscribe network. The configuration of each of these components involves a number of aspects including for instance security authentication entitlements rights in terms of which users are allowed to subscribe to what topics and topology including communication paths between these different components .

The real time monitoring functional block listens to sniffs the various events occurring in the publish subscribe network. Examples of these events include new subscription requests from API new subscribers connected to the Publish subscribe network real time statistics on different hardware components in the networked publish subscribe system size of routing tables for all MAs and levels of resource utilization.

The historical trending block is preferably tightly linked to the real time monitoring subsystem because a trend can be established over time from events that are monitored in real time. To this end the historical trending block takes its input from the real time monitoring subsystem and stores each data point in a real time database. The historical trending block can then query the real time database and chart the events it retrieves as a function of time. This block can be further used to track the publish subscribe network behavioral patterns over time.

The business logic reporting block provides another level of reporting by correlating the raw data of event patterns over time in order to help in the business decision making process. In one implementation the business logic reporting block translates into business metrics the low level message and network metrics data typical raw data examples of which include message and frame rate network delay jitter and loss data.

Optionally also the real time monitoring and business logic reporting block is used to monitor service level agreements SLA and verify that a specific level of service is met over time. When an SLA is not met it allows understanding and legal proof of where the problem is and how it is observed assuming that all parties have agreed on the validity of such reports. Furthermore establishing trends of historical metrics might help understand the changes in messaging infrastructure and it might give an insight into long term messaging traffic patterns. As a result it becomes a very valuable input in the business decision process.

In addition the P M system allows the administrator to define a message namespace associated with each of the messages routed throughout a given publish subscribe network. Accordingly a publish subscribe network can be physically and or logically divided into namespace based sub networks. This namespace based topology is illustrated on . The namespace is unique for each publish subscribe sub network. Therefore in the combined publish subscribe network each publish subscribe sub network has a unique namespace assigned to it. In this example the publish subscribe network is composed of two publish subscribe sub networks the first one with namespace Namespace and the second one with namespace Namespace . Essentially the namespace management feature in items at provides the capacity to define different administrative domains and enable topic based message communications across these different administrative domains while avoiding topic collisions or duplications.

In one example a publish subscribe sub network A publishes news updates that are routed towards the publish subscribe sub network B and sub network C publishes news updates that are also routed towards the sub network B . However if sub networks A and C publish the same news updates on the same topic sub network B can differentiate between the news coming from A and those that came from C because of their associated namespace. In many instances these namespace domains will be different intra organizational domains. In other instances these domains will be different organization or legal entities domains. In other words the namespace feature may be used by an organization to limit entitlements to its data or content to certain users in or outside the organization. For users within the organization this is done by issuing a namespace license to these users and for users outside the organization this is done by issuing a namespace license to the organization provided it has an MA.

As mentioned both configuration and monitoring of the system components are performed by communications of administrative messages. Accordingly in order to communicate with the MAs the P M system uses the channel based messaging stack along with message layer message transport layer and channel management . is a diagram illustrating the communication between the P M system and one of the MAs.

Turning now to the Messaging Appliances MAs is a block diagram illustrating an MA configured in accordance with the principles of the present invention. In one configuration the MA is a standalone appliance. In another configuration the MA is an embedded component within any network physical component such as a router or a switch. In the shown embodiment of an MA it is divided into three different functional parts. The first part includes the aforementioned network management services e.g. NTP SNMP syslog telnet SSH HTTP HTTPS and the like . These services are built on top of a standard network stack such as TCP IP stack and they may use for instance a dedicated Ethernet network interface card NIC . In one embodiment the use of a dedicated NIC provides a way to physically isolate the management traffic from the data messaging traffic. The second part is defined as the messaging stack with a message layer e.g. Tervela message layer at the top and a transport message layer below it. The messaging stack handles any messaging traffic going in and out of the MA. The third part includes the internal services. These services are used inside the MA and don t have any direct external interface. Examples of these internal services include system management service such as local and remote management logging real time monitoring and historic trends services. The internal services can be requested via an internal communication bus with calls from any of the aforementioned first and second parts.

In other words these internal services communicate indirectly with the P M system through administrative messages generated by the native messaging layer or via the network management stack . They keep track of the general health of the system including particular performance metrics and statistics of the messaging layer and the underlying physical medium. These statistics can be stored on a per channel basis or they can be aggregated by computing moving weighted averages over time for the entire system.

In addition to the above mentioned internal services another internal service is the time stamping service TSS which can be used to request an accurate timestamp. In one configuration the TSS is based on a GPS signal received directly by the MA. Alternatively the MA s internal processor clock is used instead of a GPS signal. The clock needs to be periodically updated and synchronized with an external time source however. The network time protocol NTP or another comparable source are often used for this purpose and are suitable here as well. For a system with multiple MAs the internal TSS can be synchronized across the multiple MAs by using standard network time protocols such as the NTP. In particular one master MA would be synchronized to an external time source and the neighboring MAs in the publish subscribe network then synchronize themselves with the master MA. The time synchronization between MAs could be implemented by using a particular administrative message protocol to exchange time information. Alternatively time synchronization could be implemented with the time information embedded in each data message routed throughout the publish subscribe network.

In the illustrated MA the native e.g. Tervela messaging layer has a number of roles two of which are to route native protocol messages and to handle local administrative messages. An administrative message might be a registration request of an API a subscription request from an API a configuration update from the P M system and more. Administrative messages are typically standard messages with specific administrative topics. Therefore the MA will have to subscribe to the specific administrative messages to the administrative topics before any messages can be delivered locally in the MA. The initial administrative topic subscriptions can be inserted in the routing table as static fixed routes that are pre defined in the system for the delivery of administrative messages. These so called static routes map administrative subscriptions locally to the particular MA indicating to the message routing engine that it should deliver the matching administrative messages locally.

Whenever a message is generated locally in and routed by the MA or is forwarded via the MA the message routing engine MRE searches for channels that have subscriptions matching the topic of the message. The routing table lookup is expected to return a list of one or more channels that satisfy this criterion. However if the returned list of channels is empty the message will be dropped instead of being forwarded. Otherwise if the returned list of channels is not empty a copy of the message will be sent to each channel in this list. Preferably instead of sending a separate copy of the message to each channel the channel management module keeps only one copy of the message in memory and it additionally keeps track of how many channels are obtaining and transporting this message. When all channels are done sending the message over their own physical mediums the reference count goes back to zero and the channel management can then free the memory allocated for that message. This is one example of how to optimize allocation and use of the system resources in this case memory buffers in the messaging transport layer.

Note that before the MRE sends the message to the message transport layer for all the channels that require a copy of the message the MRE obtains and records statistics on consumption patterns over these channels and system resources allocated for these channels. This monitoring and statistics tracking task is performed by the protocol optimization service POS .

If the POS determines that the system and channel resources at the source destination or both are not optimally used it may adjust the channel configuration or even create a new channel that will make a better use of the system and channel resources. For example by monitoring metrics associated with the system and channel resources e.g. latency and discard rate the POS can change the channel communication protocol if these metrics go above or below predetermined thresholds. The discard rate is defined as the percentage of discarded messages from the total number of received messages. Messages are discarded when they are delivered to their destinations e.g. APIs via multicast channels and the destinations don t have the same subscription patterns. If the discard rate exceeds a percentage threshold the MA may decide to switch the channel communication from multicast to unicast or redistribute the subscriptions over the existing multicast channels.

As mentioned the publish subscribe network might be built on top of an IP based network. In that case the MA may have multiple unicast channels with different consumers which are subscribing to the same topics. All these channels might share the same medium bandwidth. If the message rate increases sharply the MA may no longer use the available bandwidth of the medium efficiently if the MA has to send multiple copies of the same message to all the consumers. Therefore the POS might decide to switch from unicast based channel protocols to a multicast based channel protocol that will send only one copy of the message to all consumers located on that same medium. To switch from one type of channel protocol to another the POS module running on the MA will notify the POS module on the API s that another channel needs to be created to optimize the channel resources. When the channel is created and ready the MA switches from the old channel to the new one.

Then for the particular case of an edge MA when a channel delivers an incoming message to the channel management module the first check is to verify whether the message protocol differs from the native message protocol. If it does the channel management module will request the protocol translation engine to convert the incoming messages to the native e.g. Tervela message protocol. When the message is converted it is handed off to the Tervela messaging Layer . Otherwise in the case of a core MA when a channel handles an incoming message the message is handed off to the native Tervela messaging layer assuming that all channels are using this native message protocol and therefore all messages already have the native message format.

As described earlier all messages that are routed in the publish subscribe network are received or sent on a particular channel see inside message transport layer . Using these channels the MA communicates with all other physical components in the publish subscribe network. These communication interfaces are represented in the diagrams where in the interface is shown for communications between the P M system and the MA in the interface is shown for communications between the MA and the CE caching engine and in the interface is shown for communications between the MA and the API.

There are times when these interfaces are interrupted or destinations can t keep up with the load. In these and other similar situations the messages may be recalled from storage and retransmitted. Hence Whenever message data storage such as store and forward functionality is needed the MAs can operatively associate with a caching engine CE . The CE is connected via a physical medium directly to the MA as shown on and it is designed to provide the feature of a store and forward architecture in a high volume and low latency messaging environment. is a block diagram illustrating a CE configured in accordance with one embodiment of the invention.

The CE performs a number of functions. For message data persistency one function involves receiving data messages forwarded by the MA indexing them using different message header fields and storing them in a storage area . Another function involves responding to message retrieve requests from the MA and retransmitting messages that have been lost or not received and thus requested again by consumers .

Generally the CE is built on the same logical layers as an MA. However its native e.g. Tervela messaging layer is considerably simplified. There is no need for routing engine logic because as opposed to being routed to another physical component in the publish subscribe network all the messages are handled and delivered locally at the CE to its administrative message layer or to its caching layer . As before the administrative messages are typically used for administrative purpose except the retrieve requests that are forwarded to the caching layer . All the data messages are forwarded to the caching layer which uses an indexing service to first index the messages and then a storage service for storing the messages in the storage area . All data messages are stored for a predefined period of time. The indexing service is responsible for garbage collection activity and notifies the storage service when expired data messages need to be discarded from the storage area.

In addition to CEs the MAs communicate with the aforementioned APIs. is a block diagram of an API configured in accordance with one embodiment of the present invention.

The illustrated API is a combination of an API communication engine and API stubs that are compiled and linked to all the applications that use the API. One implementation of the communication engine could be a daemon. Communications between the API stubs and the API communication engine are done via an inter process communication bus implemented using mechanisms such as sockets or shared memory. The API stubs are available in various programming languages including C C Java and .NET. In some instances the API itself might be available in multiple languages. The API runs on various operating system platforms three examples of which are Windows Linux and Solaris . Alternatively the API communication engine and stubs can be merged at compile time with the application as a monolithic API to eliminate the need for spawning an additional process on the application server.

Much like the CE the API communication engine is built on the logical layers found in the MA. In order to be able to communicate with the MA the API also has a messaging transport layer . However the messaging transport layer in the API and the MA differ from each other because unlike the MA which interacts directly with the physical medium interfaces the API sits in most implementations on top of an operating system as is the case with the P M system . In order to support different types of channels the OS may require specific drivers for each physical medium that is otherwise not supported by the OS by default. The OS might also require the user to insert a specific physical medium card. For instance physical mediums such as direct connect DC or Infiniband require a specific interface card and its associated OS driver to allow the messaging transport layer to send messages over the channel.

The messaging layer in an API is also somewhat similar to a messaging layer in an MA. The main difference however is that the incoming messages follow different paths in the API and MA respectively. In the API the data messages are sent to the application delivery routing engine less schema bindings and the administrative messages are sent to the administrative messages layer . The application delivery routing engine behaves similarly to the message routing engine except that instead of mapping channels to subscriptions it maps applications to subscriptions. Thus when an incoming message arrives the application delivery routing engine looks up for all subscribing applications and then sends a copy of this message or a reference to this message to all of them.

In some implementations the application delivery routing engine is responsible for the late schema binding feature. As mentioned earlier the native e.g. Tervela messaging protocol provides the information in a raw and compressed format that doesn t contain the structure and definition of the underlying data. As a result the messaging system beneficially reduces its bandwidth utilization and in turn allows increased message volume and throughput. When a data message is received by the API the API binds the raw data to its schema allowing the application to transparently access the information. The schema defines the content structure of the message by providing a mapping between field name type of field and its offset location in the message body. Therefore the application can ask for a specific field name without knowing its location in the message and the API uses the offset to locate and return that information to the application.

To a large extent outgoing messages follow the same outbound logic as in the MA. In this example the API has a protocol optimization service POS that tracks statistics about consumption patterns system and channel resource utilization as is done in the MA . However unlike the POS in the MA which makes its own decisions on when to change the channel configurations the POS in the API acts as a slave of the master POS in the MA to which it is linked. When the POS on the MA decides to change the channel configurations it remotely controls the slave POS at the API.

As mentioned above for availability and reliability of the system and consistency and persistency of the message data it is advantageous to configure the system as a fault tolerant system. Preferably the system is designed with session based fault tolerant configuration as shown in . Another possible configuration is full failover but in this instance we chose session based fault tolerance instead.

A session encompasses the communications between two MAs or between one MA and an API e.g. . A session can be active or passive. This configuration uses primary and secondary MAs e.g. and . If a failure occurs the MA or the API may decide to switch the session from the primary MA to the secondary MA . A failure occurs when a session experiences failures of connectivity and or system resources such as CPU memory interfaces and the like. Connectivity problems are defined in terms of the underlying channel. For instance an IP based channel would experience connectivity problems when loss delay and or jitter increase abnormally over time. For a memory based channel connectivity problems may be defined in terms of memory address collisions or the like.

Overall the session based fault tolerant design has the advantage of not affecting all the sessions when only one or a subset of all the sessions is experiencing problems. That is when a session experiences some performance issues this session is moved from the primary MA e.g. to the secondary fault tolerant FT MA without affecting the other sessions associated with that primary MA . So for instance APIare shown still having their respective active sessions with the primary MA as the active MA while APIhas an active session with the FT MA .

The primary and secondary MA may be seen as a single MA using some channel based logic to map logical to physical channel addresses. For instance for an IP based channel the API or the MA could redirect the problematic session towards the secondary MA by updating the ARP cache entry of the MA logical address to point at the physical MAC address of the secondary MA.

In sum the present invention provides a new approach to messaging and more specifically an end to end middleware architecture that improves the effectiveness of messaging systems. Although the present invention has been described in considerable detail with reference to certain preferred versions thereof other versions are possible. Therefore the spirit and scope of the appended claims should not be limited to the description of the preferred versions contained herein.

