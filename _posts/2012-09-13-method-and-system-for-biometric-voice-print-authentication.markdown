---

title: Method and system for bio-metric voice print authentication
abstract: A method () and system () for authenticating a user is provided. The method can include receiving one or more spoken utterances from a user (), recognizing a phrase corresponding to one or more spoken utterances (), identifying a biometric voice print of the user from one or more spoken utterances of the phrase (), determining a device identifier associated with the device (), and authenticating the user based on the phrase, the biometric voice print, and the device identifier (). A location of the handset or the user can be employed as criteria for granting access to one or more resources ().
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08571867&OS=08571867&RS=08571867
owner: Porticus Technology, Inc.
number: 08571867
owner_city: Needham
owner_country: US
publication_date: 20120913
---
The present application claims the benefit of priority to U.S. Provisional Application No. 60 685 427 filed on May 27 2005 Utility patent application Ser. No. 11 420 190 filed on May 24 2006 now U.S. Pat. No. 7 536 304 and is a Continuation of patent application Ser. No. 12 422 787 filed on Apr. 13 2009 all of which are hereby incorporated by reference in their entirety.

The present invention relates in general to speech recognition and more particularly to voice identification.

Advances in electronic technologies and software have enabled systems to more effectively recognize and identify people. For example image processing systems such as cameras can capture an image of a person and identify a person from the image. Fingerprint scanning systems can capture a fingerprint for identifying a person through touch. Voice processing systems can identify a person through their voice. These technologies provide for identification of a user prior to use for ensuring system security and delegating access to the system.

Voice Identification ID systems have been used in a variety of security related applications. Voice ID sometimes called voice authentication is a type of user authentication that uses voiceprints and pattern recognition software to verify a speaker. An adaptation of biometrics Voice ID relies on the premise that vocal characteristics like fingerprints and the patterns of people s irises are unique for each individual.

More people can interact together on line over the Internet through the coupling of mobile devices and computers. Mobile devices are capable of going on line and establishing connections with other communication systems. Identifying a user of the mobile device is an important aspect for providing secure access. However the identity of a user of the mobile device is not generally available. A need therefore exists for authenticating a user.

Embodiments of the invention concern a method for voice authentication on a device. The method can include receiving one or more spoken utterances from a user recognizing a phrase corresponding to the one or more spoken utterances identifying a biometric voice print from the one or more spoken utterances of the phrase determining a device identifier associated with the device and authenticating the user based on the phrase the biometric voice print and the device identifier. A variability of the one or more spoken utterances can be determined for creating the biometric voice print. The biometric voice print is a vocal tract configuration that is physically unique to a vocal tract of the user. Upon authenticating the user access can be granted to one or more resources having a communication with the device. A location of the device or the user can be determined for granting access.

Detailed embodiments of the present method and system are disclosed herein. However it is to be understood that the disclosed embodiments are merely exemplary and that the invention can be embodied in various forms. Therefore specific structural and functional details disclosed herein are not to be interpreted as limiting but merely as a basis for the claims and as a representative basis for teaching one skilled in the art to variously employ the embodiments of the present invention in virtually any appropriately detailed structure. Further the terms and phrases used herein are not intended to be limiting but rather to provide an understandable description of the embodiment herein.

The terms a or an as used herein are defined as one or more than one. The term plurality as used herein is defined as two or more than two. The term another as used herein is defined as at least a second or more. The terms including and or having as used herein are defined as comprising i.e. open language . The term coupled as used herein is defined as connected although not necessarily directly and not necessarily mechanically. The term suppressing can be defined as reducing or removing either partially or completely. The term processing can be defined as number of suitable processors controllers units or the like that carry out a pre programmed or programmed set of instructions.

The terms program software application and the like as used herein are defined as a sequence of instructions designed for execution on a computer system. A program computer program or software application may include a subroutine a function a procedure an object method an object implementation an executable application a source code an object code a shared library dynamic load library and or other sequence of instructions designed for execution on a computer system.

Embodiments of the invention concern a system and method for authenticating a user. The method can include receiving one or more spoken utterances from a user recognizing a phrase corresponding to one or more spoken utterances identifying a biometric voice print of the user from a variability of one or more spoken utterances of the phrase determining a device identifier associated with the device and authenticating the user based on the phrase the biometric voice print and the device identifier.

Embodiments of the invention also include an authentication system that can be based on a user s unique voice print a phrase the user speaks during a creation of the voice print and a user s handset s identifier for example an IMEI number. In one implementation a location of the handset or the user can be employed as an additional criteria for approving access to one or more resources. The system can replace for example the current social security number mother s maiden name model of user identification with a more robust method using a biometric characteristic namely the user s voice.

Referring to a mobile communication environment for voice authentication is shown. The mobile communication environment can include a voice authentication server a database and one or more mobile devices . User profiles can be stored on the database which can be used to identify a user of the mobile device . A user profile can include a pass phrase a biometric voice print and a device identifier. The server can compare a user s profile to other user profiles stored on the database for authorizing the user s voice. For example a user of the mobile device can speak into the mobile device for accessing one or more resources available to the mobile device. Upon authorizing the user s voice access can be granted to one or more resources. For example a resource can be a server a PBX or any other suitable communication system. The resource can provide a feature or service available to the device such as a music downloading on line gambling subscription gaming and the like. The resource can provide access to a secure or non secure website such as a personal information a remote server or a data store hosting financial data or business data but is not herein limited to these.

The server can acknowledge whether a pass phrase spoken by the user is a correct pass phrase and whether the biometric voice print associated with a pronunciation of the phrase is a correct match to a user profile in the database. In particular the biometric voice print is captured by analyzing one or more variabilities in the user s speaking style during one or more pronunciations of the pass phrase. For example the voice authentication server can determine whether characteristics of the user s voice captured during a pronunciation of the pass phrase match one or more biometric voice prints in the database for authenticating access to one or more resources. The server can also verify that the mobile device is a device authorized for use to access resources and is a device associated with the biometric voice print of the user. In particular the server can validate that the user speaking into the mobile device is associated with the mobile device. In one example the server can determine if the device is registered to the user through an IMEI number associated with the captured biometric voice print. The IMEI number is a device identifier that is unique to the mobile device. In another arrangement the server can determine a location of the device for authorizing access to one or more resources. For example the mobile device can include a global positioning system GPS for identifying a location of the device. Alternatively the server can authorize access to resources based on a location stated by the user. For example the user can speak their location and the server can determine if the spoken location corresponds with an authorized or accepted location of the device or the user. The user s voice can be processed on the mobile device or at the server for validating an identity of the user.

The mobile communication environment can provide wireless connectivity over a radio frequency RF communication network or link with one or more voice authentication servers on the system. The server can be a Gateway PBX or any other telecommunications network device capable of supporting voice and data delivery. Communication within the network can be established using a wireless copper wire and or fiber optic connection using any suitable protocol e.g. TCP IP HTTP HTTPS SIP etc. . In one arrangement the mobile device can communicate with a base receiver using a standard communication protocol such as CDMA TDMA OFDM GSM or the like. The base receiver in turn can connect the mobile device to the Internet over a packet switched link. The internet can support application services and service layers for providing media or content to the mobile device . Application service layers can include database access for financial or business based applications. The mobile device can also connect to other communication devices through the Internet using a wireless communication channel. The mobile device can establish connections with a server on the network and with other mobile devices for exchanging voice data and media. The server can host application services directly or over the internet which can be accessed through the mobile device .

The mobile device can send and receive data to the server or other remote servers on the mobile communication environment . For example the mobile device can also connect to the Internet over a WLAN. Wireless Local Access Networks WLANs provide wireless access to the mobile communication environment within a local geographical area. WLANs are typically composed of a cluster of Access Points also known as base stations. The mobile communication device can communicate with other WLAN stations such as the laptop within the base station area for exchanging voice data and media. In typical WLAN implementations the physical layer uses a variety of technologies such as 802.11b or 802.11g WLAN technologies. The physical layer may use infrared frequency hopping spread spectrum in the 2.4 GHz Band or direct sequence spread spectrum in the 2.4 GHz Band.

The mobile device can send and receive data to and from the server over a circuit switch RF connection or a packet based WLAN AP but is not herein limited to these. Notably the data can include the user s profile which can be shared amongst one or more voice authentication servers for granting the user access to one or more resources. Understandably voice can be represented as packets of voice which can be transmitted to and from the mobile devices to provide voice communication. For example a user of the mobile device can initiate a call to the server or the laptop for accessing one or more features available to the mobile device. Voice data can be transmitted over the mobile communications environment thereby providing voice communication. The mobile device can be a cell phone a personal digital assistant a portable music player or any other type of communication device.

Referring to an exemplary illustration of a voice authentication system deployed within the mobile communication environment is shown. The voice authentication system can include the voice authentication server an interface and the database . The server can access the database through the interface for retrieving user profiles. The interface can include a web layer a business layer and a database access layer . It should be noted that the interface is merely illustrative of the transport layers involved with data processing on a network. The interface may have more or less than the number of components shown and is not limited to those shown.

The database can include a plurality of user profiles for voice authentication. A user profile can be unique to the user and unique to the device. The user profile can include a biometric voice print a pass phrase and a mobile device identifier . The pass phrase can be one or more words specifically selected by the user to be recited during voice authentication. When the user speaks the pass phrase into the mobile device a voice print of the user s voice can be captured and stored in the user profile . The biometric voice print identifies characteristics of the user s speaking style that are unique to the user. In particular the biometric voice print represents a vocal tract configuration difference that is physically unique to a vocal tract of the user. That is the user s vocal tract is capable of undergoing physical changes which are dependent on the physical formations of the user s vocal tract. The biometric voice print captures the physical features associated with these characteristic changes of the vocal tract during the pronunciation of the pass phrase that are unique to the individual. A user s vocal tract configuration includes the esophagus the pharynx the larynx the mouth the tongue and the lips. These physical attributes can undergo a certain physical change during speech production during the articulation of a pass phrase which is characteristic of the user s vocalization and speaking style. In particular the amount of change these physical attributes undergo during one or more pronunciations of a spoken utterance can be measured for validating a user s identity.

Referring to an exemplary implementation of the voice authentication system is shown. The exemplary implementation includes a handset such as a mobile telephone or other mobile computing device and a voice authentication server in communication with the handset over the mobile communication environment . The server can be any suitable computing or networking server. The software running on the server can include the web layer See for communication with the handset a business layer and a database access layer for storing and retrieving data though are not limited to these. The server can also include a monitoring page which allows administrative access to the server. For example a user can update their profile through the monitoring page. The voice authentication server provides for user profile creation user profile maintenance and user authentication. For example a user profile can be generated from the biometric voice print the pass phrase and the device identifier and stored in the voice print database as described in . User profile maintenance entitles a user to update or change their profile details such as their biometric voiceprint and password and associated information. User authentication allows users to be authenticated against their previously created voiceprints. The authentication can be performed using the user s recorded voice and the handsets IMEI or the PIN provided to the user. For example in place of the IMEI a PIN can be assigned to the mobile device for associating the device with the user s profile.

In addition to the system components previously shown in the exemplary implementation can include a gateway inserted between the voice authentication server and the existing call processing mobile communication environment of . In one arrangement the server can support subscriber compliance LDAP and audit trails. In one arrangement the gateway can can verify a location of the caller using information through GPS positional data provided by the mobile device . The combination of the biometric voiceprint recognition with a location verification capability makes a particularly convenient solution for such applications as gambling which may for example only be allowed in some states or territories or commerce where sale of certain items may not be allowed to some jurisdictions . The gateway can identify a location of the device from the GPS data to establish a location of the caller.

The gateway can also perform call matching and routing in the mobile communication environment . For example as is known in the art the gateway can support ANI and DNIS for identifying a calling number and a called number associated with the user. A user can be identified by the number from which the user is calling or by the number which the user is calling. In one arrangement contemplated the calling information can be included as part of the user profile and used to verify an identity of a user. In practice the voice authentication server can authenticate a user speaking into the mobile device with reference to user profiles stored on the database by inquiring the gateway for caller identification information and location information.

Referring to an exemplary voice authentication system is shown. The voice authentication system can include the mobile device having a connection to the voice authentication server . The authentication server can include an authentication servlet a profile management module a verification module and a voice print database . The modules may reside on the server or at remote locations on other servers within the mobile communication environment . refers to a client server based architecture through aspects of the invention are not limited to this configuration. The principles of voice authentication can be equally applied in distributed networks and peer to peer networks.

It should be noted that some of the components are carried forward from and that the components are provided merely to illustrate one embodiment for integrating the voice authentication system within the mobile communications environment See . In practice the voice authentication system can grant a user of a mobile device access to one or more resources available to the device based on an authentication of the user s voice for accessing the resources or services. The voice authentication system is not limited to the program modules shown or the architecture of the program modules. The program modules are merely presented as one embodiment for deploying the inventive aspects of voice authentication described herein.

The voice authentication system can include an application running on the mobile device . The application can be a software program written in a programming language such as C C Java Voice XML Visual Basic and the like. For example the application can be a financial or business application for sending confidential or secure information to and from a secure website. The confidential information can be in the form of voice audio video or data. The application can acquire access to underlying communication protocols supported by the mobile device. For example the application can be a Java 2 Micro Edition J2ME applet having socket connections supporting HTTP to one or more servers communicatively connected to the mobile device . The communication protocols can be supported through a native C interface. For example the J2ME can access native C code on the mobile device for connecting to a server .

The application can communicate with an authentication servlet running on the voice authentication server See . The Authentication Servlet can act as a front end to the mobile device client and direct requests to the voice authentication server depending on request type. For example the request type may be of a user profile creation a user profile update or a user profile authentication as previously described. Based on the request type the authentication servlet can invoke an appropriate profile management function. That is upon determining the request type the profile management module can communicate with the application to perform the associated request.

In one arrangement the authentication servlet and the application can communicate over a secure HTTP connection . The authentication servlet can be communicatively coupled to a verification module for authorizing a user. In one arrangement the authentication servlet can communicate with the verification module over a Java Native Interface JNI . The JNI provides programming language translation between the program components. For example the authentication servlet can be written in Java whereas the verification module may be written in C. The JNI provides an interface to transport data from one format to another while preserving structural aspects of the code and data. The verification module can communicate information to and from the application . Notably the mobile device HTTPS authentication servlet and JNI establish a channel of communication between the verification module on the voice authentication server and the application on the mobile device .

In practice the mobile device can send a user profile See to the verification module . For example when a user desires access to one or more resources or services offered to the mobile device the mobile device can present the application . The mobile device can also present the application when the user creates a user profile. For example the application can be a J2ME application which asks the user to speak a password phrase. The application can also access a device identifier on the mobile device such as an IMEI number. The information can be used to create the user profile. In certain device an IMEI number extraction mechanism may not be supported through J2ME. Accordingly such devices may include a provision for user to key in a short PIN which the user can easily remember and use for authentication. If an IMEI number is not supported the user may be required to key in the PIN which is then used to approve sending a stored IMEI number.

In one arrangement the mobile device can include a speech recognition engine for validating a pass phrase. Understandably the voice recognition engine may only evaluate that a phrase was recognized and not an identity of the user. Accordingly a first aspect of the voice authentication can be performed on the mobile handset that is the verifying the pass phrase. The biometric voice print authentication and device identifier can be evaluated at the server. Thus a second aspect of voice authentication can be performed at the server.

Alternatively the entire voice authentication including the speech recognition can be conducted on the server . In this case the application can create a user profile See which includes the pass phrase the biometric voice print and the IDEI . Upon speaking the password phrase the J2ME application can send the user profile to the verification server. In one arrangement the J2ME application can perform voice processing on the spoken utterance i.e. pass phrase and encode one or more features of the biometric voice prior to create the user profile and sending it to the verification module . The encoding can compress the voice data to reduce the size of the voice packets required for transmitting the spoken utterance. For example the voice data can be compressed using a vocoder as is known in the art. In a second arrangement the spoken utterance can be transmitted in an uncompressed format to the verification module . For example the audio can be transmitted in pulse code modulation PCM format or Microsoft Wave Format WAV .

The profile management module can communicate with the authentication servlet for evaluating one or more user profiles stored in the voice print database . The profile management module can create update and delete user profiles. The profile management module can also synchronize with other profile management systems. For example the profile management module can expose an API for integration with external systems after successful authentication of a user. In one arrangement the Application Programming Interface API allows application developers to quickly integrate their applications in accordance with the aspects of voice authentication system herein discussed. For example referring back to the API can include a module for creating the biometric voice print a module for creating the pass phrase and a module for identifying the device . The API provides an interface to the authentication servlet for accessing voice print creation and authentication services.

The profile management module can communicate with the voice print database over a Java Database Connectivity JDBC interface. The JDBC can provide data access for retrieving and storing data from the voice print database . For example the voice print database can be a relational database composed of tables which can be indexed be row column formatting as is known in the art. The JDBC provides a structured query language locating data headers and fields within the voice print database . The profile management module can parse the user profile for the biometric voice print and compare the biometric voice print with other voice prints in the voice print database . In one arrangement biometric voiceprints can be stored using the mobile handsets IMEI number for indexing. Notably the voice print database includes one or more reference voice prints from multiple user s having a registered voice print. Upon determining a match with a voice print the profile management module can grant access to the user to one or more resource. For example the profile management module can allow a socket connection to one or more secure websites business databases financial centers and the like.

Referring to a flowchart for user profile creation is shown. The user profile creation may contain more or fewer than the number of steps shown. Reference will be made to for describing the steps. At step the user starts the application. For example referring to the user activates a J2ME application . Alternatively the user may be accessing a website voice mail or requesting a service that requires authentication such as a log in screen. In this case the device may automatically launch the J2ME application for authorizing the user. At step the user is prompted to record his voice for voice print creation. The user can submit a particular phrase that the user will recite during voice authorization. At step the user records their voice using the provided application . At step the user can enter in a PIN number. Again the PIN number may be required if the application cannot retrieve an IMEI number from the device. If the application can access the IMEI then the PIN number may not be required. At step the user is prompted to register his profile. For example the user can elect to store the newly created user profile on a voice print database for later retrieval. At step the registration details along with the recorded voice are sent to the authentication server. At The Authentication server creates the user s voiceprint. At step the Authentication server creates the user s profile using the user s voice print and IMEI or PIN . For example the user profile can be stored on the voice print database . At the Authentication server responds back with a positive confirmation to the user.

Referring to a flowchart for verifying a user through voice authentication is shown. The authentication may contain more or fewer than the number of steps shown. Reference will also be made to for describing components associated with practicing the steps. At step the user starts application. The application may also start automatically based on a user s action such as accessing a feature or service that requires authentication. At step the user is prompted to record his voice for voice print verification. This is the same phrase that was recorded during user profile creation . At step the user records his voice using the provided application . At step the user types in the PIN that was used to register with the authentication server during user profile creation . At step authentication details along with the recorded voice are sent to the authentication server . At step the authentication server retrieves the user s voiceprint using the user s PIN. At step the authentication server uses the Verification module to verify the user s recorded voice against one or more stored voiceprints. At step the authentication server responds back to the user. At step if the authentication is successful the user can proceed further with the service or application. At step If the authentication is unsuccessful the user is prompted about authentication failure and the application exits.

Referring to a method for voice authentication on a device is shown. The method can include receiving one or more spoken utterances from a user recognizing a phrase corresponding to the one or more spoken utterances identifying a biometric voice print of the user from a variability of the one or more spoken utterances of the phrase determining a device identifier associated with the device and authenticating the user based on the phrase the biometric voice print and the device identifier . In particular in one arrangement the user speaks the spoken utterance e.g. pass phrase multiple times. The variation in the user s voice can be evaluated to determine changes in the user s vocal tract configuration. In one arrangement a location of the device or the user can be determined for granting access as previously described in .

The vocal tract configuration changes can be captured in the biometric voice print and compared with a plurality of reference voice prints on a voice print database for identifying a match. That is a first voice print and at least a second voice print can be generated in response to a speaker s voice a difference between the first voice print and a second voice print can be identified and a determination can be made as to whether the difference corresponds to a natural change of the speaker s vocal tract. Notably the biometric voice print is a vocal tract configuration that is physically unique to a vocal tract of the user. Consequently the speaker can be authenticated if the difference is indicative of a natural change in the speaker s vocal tract.

For example referring back to the device implementing the voice authentication method can establish a connection to at least one authentication server send a user profile to at least one authentication server compare the user profile with a plurality of reference profiles stored on the at least one authentication server and determine if the user profile matches one of the plurality of reference profiles for authenticating the user. Upon recognizing the phrase the voice authentication server or the device can evaluate one or more vocal tract configuration differences between the spoken utterances. One or more vocal tract shapes from the plurality of reference profiles can be matched based on the vocal tract configuration difference.

In the foregoing a detailed description of the voice authentication system for practicing the methods steps is provided. In particular referring to an algorithm for the voice authentication aspect of the voice authentication system is presented. The algorithm is a high level description of the underlying voice processing methods employed for validating an identity of a user based on biometric voice print analysis. As such it should be noted that the algorithm can contain more than or fewer than the number of steps shown. In fact each step can contain further contain steps not shown in the drawings but herein set forth in the specification. Reference will be made to when describing method .

At step a speech utterance can be segmented into vocalized frames. For example referring to the pass phrase e.g. spoken utterance the user speaks into the mobile device can be partitioned into voiced and unvoiced segments. That is regions corresponding to periodic regions such as vowels can be classified as voiced and regions corresponding to non periodic regions such as consonants can be classified as unvoiced. At step Linear Prediction Coding LPC coefficients can be calculated from the voiced regions and at step transformed into Linear Spectrum Pairs LSP . LSP coefficients are suitable for compression and coding. At step formants can be calculated from the LSP coefficients. Formants are those portions of the speech spectrum that correspond to resonances and nulls formed by the vocalization process. In particular the physical structures of the human speech production system such as the throat tongue mouth and lips form cavities which create resonances in the pressure wave emanating from the lungs. The formants in the spectral domain represent characteristics of the users vocal tract formation during pronunciation of the voiced frames. At step information regarding the formant structure and features extracted during the LPC LSP analysis can be included in a feature matrix. At step the feature matrix can be normalized. One aspect of normalization can include removing background noise. A second aspect of normalization can include accounting for vocal tract configuration length and area. At step a voice print and threshold can be calculated from the feature matrix. The biometric voiceprint can include the features shown in Table 1.

In practice a user can present a spoken utterance corresponding to a pass phrase that was used during voice enrollment that is when the user registered their biometric voice print with a voice authorization server. For example during enrollment a user pronounces the same pass phrase three times. A feature matrix is calculated for each recording of the pass phrase. The feature matrix is a matrix of numeric values that represent features of the speaker s voice. In this case three feature matrices are used to create the biometric voice print. For example with reference to the enumerated voice print listed above in Table 1 various features including averages and bounds are used in the voice print. The features of Table 1 are used in conjunction with the three matrices to define the voice print. For example the feature matrices define the features of the voice and the attributes of Table 1 describe the variation of a vocal tract configuration. For instance the attributes of Table 1 represent a vocal tract shape. Notably the variation in the pronunciation of the pass phrase is captured by identifying bounds of the feature vector for each voice frame which are defined in the biometric voice print of Table 1. For example index 3 of the biometric voiceprint in Table 1 identifies a maximum and minimum value for each element of the one or more feature vectors. For instance the bounds can identify the naturally occurring change in amplitude of a formant the change in bandwidth of a formant of the change in location of a formant during pronunciation of the pass phrase which is particular to a user speaking the pass phrase.

During verification the user speaks the same spoken utterance corresponding to the pass phrase and a biometric voice print is generated. The biometric voice print is compared against previously stored voice prints for identifying a match. During the verification process a feature matrix is also calculated from the spoken phrase using the voice authentication algorithm as used in enrollment. This feature matrix is compared against one or more reference matrices store in a voiceprint database. A logarithmic distance can be calculated for each feature matrix of a biometric voice print. If the logarithmic distance is less than a predetermined threshold level a match can be determined and the speaker can be identified. One unique aspect of the verification process includes setting a comparison threshold level that depends on a threshold from a voiceprint. The threshold depends on intra speaker variability and can be adapted based on the user s voice. Alternatively the threshold can be set independently of the threshold and which is not adapted based on the user s voce.

In one implementation the method of generating the voice print can be performed by a handset and the method of authorizing a user can be performed by a server in communication with the handset. Referring to a diagram depicting various components of a voice authentication system for practicing the method of generating the voice print is shown. The voice authentication system can include a voice processor and a biometric voice analyzer . The voice processor can receive a spoken utterance and at least one repetition of the spoken utterance from a user. The biometric voice analyzer can calculate one or more vocal tract shapes from the spoken utterance and the at least one repetition and calculate a vocal tract configuration difference between the one or more vocal tract shapes based on a varying pronunciation of the spoken utterance and the at least one repetition. A vocal tract configuration difference corresponds to a bounded physical change of a user s vocal tract associated with one or more spoken utterances. For example a vocal tract configuration difference can be based on a momentum spectrum that accounts for a dynamic change of a speech spectrum over time. The momentum spectrum can include a lower bound and an upper bound for the one or more voice segments of speech such that variations in the speech spectrum between the lower bound and the upper bound correspond to a unique vocal tract configuration.

In one arrangement though not required the voice processor can include a speech recognizer . The speech recognizer can validate a phrase spoken by the user during voice authentication. In one aspect the speech recognizer can also identify voiced and unvoiced regions in the spoken utterance recognize one or more phonemes from the voiced regions and identify a location of the one or more phonemes in the vocalized frames e.g. voiced segments . The voice processor can segment a spoken utterance into one or more vocalized frames generate one or more feature vectors from the one or more vocalized frames calculate a feature matrix from the one or more feature vectors and normalize the feature matrix over the one or more vocalized frames. For example a feature matrix can be calculated for every spoken phrase. The speech utterance can be partitioned into one or more speech frames having time length between 5 and 20 ms.

The voice processor can identify an absolute minimum and maximum in the speech frames. The values can be compared against a predetermined threshold. If both maximum and minimum values are less than an amplitude level then the frame can be classified as having no voice component and the algorithm proceeds to the next frame. If the minimum and maximum are greater than the amplitude level then an autocorrelation function is calculated for the speech frame signal. If one or more pre specified autocorrelation terms are less than a predefined threshold then the frame is considered to lack a voiced signal and the algorithm processed to the next frame.

A Fast Fourier Transform FFT can be applied to the voiced windowed speech frame. The speech frame can be multiplied by a weighting window to account for discontinuities prior to frequency analysis. The FFT converts each frame of N samples from the time domain into the frequency domain. The result obtained after this step is an amplitude spectrum or spectrum.

Human perception of the frequency contents of sounds of speech signals does not follow a linear scale. Accordingly a Bark scale can be applied to the amplitude spectrum for converting from a linear frequency scale to a scale that approximates human hearing sensitivity. That is a perceptual filter bank analysis can be performed on the one or more vocalized frames. One approach to simulate the Bark frequency is to use filter bank one filter for each desired Mel frequency component. The filter bank can have a triangular band pass frequency response. The spacing as well as the bandwidth is determined by one bark frequency interval. The number of Bark spectrum coefficients IBR depends on frequency range. In telephone channel frequency range 3400 Hz matches 17 Bark. Therefore 0 3400 Hz frequency range matches 17 one bark bandwidth filters. Each filter band can have a triangular band pass frequency response and the spacing as well as the bandwidth can be determined by a constant bark frequency interval. The spectrum frequency shifted in accordance with the Bark scale can be called a Bark spectrum.

The Bark spectrum X n k can be multiplied by weighting factors on a bark scale frequency bank and the products for all weighting factors can be summed to get an energy of each frequency band. An energy matrix can be calculated for each speech frame of the spoken utterance. For example the spoken pass phrase can be represented as a matrix E m i . In order to remove some undesired impulse noise a three point median filter can be used for smoothing. The smoothed energy E m i can be normalized by removing the frequency energy of background noise to get the primary energy associated with the speech signal E m i . In one arrangement the background noise energy E m i . can be estimated by averaging the energy of the first 8 speech frames.

With the smoothed and normalized energy of the i th band of the m th frame E m i the total energy of the speech signal at the i th band can be calculated 

If T i 1.5 the band can be left intact as more speech can be considered present than noise. Conversely it the threshold is less the band can be considered too noisy and not used in further calculations. Accordingly higher speech content is reflected when more bands exceed the 1.5 threshold. The bands exceeding the threshold can be counted as the new band count. That is the perceptual filter bank analysis includes estimating speech energy and noise energy in one or more frequency bands along a Bark frequency scale. Background noise can be suppressed during the perceptual filter bank analysis by discarding filterbanks having a ratio of speech energy to noise energy that do not exceed a threshold of vocalization. The total signal energy can be calculated with the new band count 

A minimum and maximum value can be determined for each E m . An adaptive vocalized segmentation threshold can also be calculated based on the determined minimum and a root mean square term Min 0.3 standard deviation of 

Frames having E m Tv can be classified as vocalized and a new matrix can be computed using only the vocalized frames. Notably the aforementioned voice processing techniques are employed to identify voice segments of speech and calculate a feature matrix based on these voiced regions of speech. Voiced regions of speech can include phonemes which can be identified and located within the spoken utterance. For example referring to the speech recognizer can identify phonemes.

Following voiced activity analysis Linear Prediction Coefficients LPC can be calculated from the energy bands of the perceptual filter bank analysis. Pre emphasis can be applied to the E m i to reduce the dynamic range of the spectrum. This improves the numerical properties of the LPC analysis algorithms. The maximum of the amplitude spectrum is found and all points after the maximum can be multiplied by weighting coefficients. The LPC s can then be converted to Line Spectral Pair coefficients LSP s . Formants and anti formants can be calculated from the LSP s and a feature vector can be calculated from the formants and anti formants. Upon determining the formants and anti formants a feature vector for each speech frame can be calculated. A feature matrix can be created for the feature vectors representing voiced segments of the spoken utterance. The feature matrix can include formant locations formant amplitudes formant bandwidths anti formant locations anti formant amplitudes anti formant bandwidths phase information average amplitude information difference information and dynamic features. In particular the formant and anti formant information is represented along a Bark scale. Differences in the formant and anti formant information can be evaluated for characterizing one aspect of a natural change in a vocal tract configuration. That is a distortion can be evaluated for one or more feature vectors for identifying voice print matches generated from similar vocal tract configurations.

A vocal tract spectrum can be calculated from the feature matrix. In particular formants having similar characteristics between the one or more repetitions of the spoken utterance are used for creating the vocal tract spectrum. That is formants substantially contributing to a consistent representation of vocal structure are used for creating the vocal tract spectrum. The vocal tract spectrum can be calculated from the LPC s or from an autocorrelation function. Changes in the vocal tract shape which correspond to a vocal tract configuration can be identified from changes in the vocal tract spectrum. In particular the vocal tract configuration can be represented as one or more sections having a corresponding length and area that are characteristic to one or more sections of the user s vocal tract. A vocal tract configuration difference corresponds to a bounded physical change of a user s vocal tract associated with one or more spoken utterances. For example a vocal tract configuration difference can be based on a momentum spectrum that accounts for a dynamic change of a speech spectrum over time. The dynamic change can occur to an amplitude of the spectrum or a phase of the spectrum. The momentum spectrum can include a lower bound and an upper bound for the one or more voice segments of speech such that variations in the speech spectrum between the lower bound and the upper bound correspond to a unique vocal tract configuration. The upper and lower bounds for the feature matrix were presented in Table 1.

For example referring to the voice processor calculates a feature matrix from the feature vectors for multiple sections of the spoken utterance corresponding to the one or more vocalized frames wherein the feature matrix is a concatenation of feature vectors of the one or more vocalized frames. The voice processor also normalizes the feature matrix by removing vocalized frames shorter that a predetermined length and removing vocalized frames corresponding to vocal tract configurations that exceed an average vocal tract configuration. The vocal tract spectrum can be characterized or represented by a number of features in the feature matrix. The attributes of the features have been selected from statistical research of voice databases to minimize an intra speaker variability and that maximize an inter speaker variability.

Understandably during voice authentication the biometric voice analyzer See compares identification parameters of the feature vector against identification parameters of a stored feature vector of the speaker s voice. The parameters include the formant information and anti formant information captured in the biometric voice print of Table 1. Notably the biometric voice print includes the three feature matrices associated with the three repetitions of the phrase and the attributes of Table 1 that characterize the user s vocal tract shape. That is the vocal tract shape is characterized by and can be calculated from the feature matrix.

During calculation of the feature matrix for determining a vocal tract shape a first vocal tract shape will be generated from the first three formants specified in the feature matrix. The vocal tract shape curve can be calculated with 0.2 cm increments from the formant frequencies. A vocal tract length can also be calculated for the voiced frames. For example the biometric voice analyzer calculates a first vocal tract shape from lower formants of the first biometric voice print determines a vocal tract configuration difference based on the first vocal tract shape identifies a similar vocal tract shape providing the smallest vocal tract configuration difference and shapes the similar vocal tract shape from higher formants of the first biometric voice print. The higher formant frequencies are emphasized to characterize one aspect of a speaker s articulation style.

Referring again to the biometric voice analyzer determines one or more vocal tract cross section areas from the feature vector and determines one or more vocal tract lengths for the one or more vocal tract cross section areas. Also a communication bandwidth can be taken into account when determining vocal tract shape. For example formant frequencies can be adjusted for telephone bandwidth which is generally between 140 Hz to 4.6 KHz F1 640 F2 1730 F3 2860 and F4 3340. The cross section of the vocal tract can be updated based on the compensated formant frequency locations. An average of the vocal tract cross section can be determined for the vocal tract shape based on one or more vocalized frames of speech. For example the cross section can be determined for phoneme regions of voiced speech where change in the vocal tract shape are relatively constant.

Variation bounds can be created based on a variability of the vocal tract shape for producing variation vectors for the feature vectors in the feature matrix. For example the biometric voice analyzer calculates a logarithmic distance for the variation vectors and establishes a threshold based on the logarithmic distance. The threshold is used to determine whether a vocal tract configuration difference for authenticating a user is within a variation bounds. The variation bounds can be represented as an average and a standard deviation of the feature vectors such as that shown in Table 1. The biometric voice analyzer also calculates a histogram on the variation bounds determines a maximum for the histogram calculates a derivative vector based on the maximum and calculates a personal histogram and second variation bounds based on the derivative vector.

During verification biometric voice analyzer evaluates a personal histogram to determine whether a biometric voice print matches one of the said plurality of biometric voice prints for verifying an identity of the user. An identify is validated when a first plurality of bins of the personal histogram are filled and wherein the identity is invalidated when a second plurality of bins of the personal histogram are filled. Notably the feature information of Table 1 in the biometric voice print is used to generate a personal histogram for determining when a user s vocal tract shape matches the personal histogram. The histogram statistically identifies whether the features of the biometric voice print are characteristic of the person speaking. That is variations in the speakers vocal tract shape can evaluated and statistically compared to variations associated with a particular user s vocal tract configuration. Recall multiple presentation of the spoken utterance are provided for determining a vocal tract configuration difference that is a change in vocal tract shape. The personal histogram provides a practical detection method for classifying and authorizing a user. For example during verification the biometric voice analyzer calculates a logarithmic distance and evaluates a threshold for determining when the first plurality of bins of the personal histogram is filled. The threshold can also be adapted based on the user s voice.

Benefits other advantages and solutions to problems have been described above with regard to specific embodiments. However the benefits advantages solutions to problems and any element s that may cause any benefit advantage or solution to occur or become more pronounced are not to be construed as a critical required or essential feature or element of any or all the claims. As used herein the terms comprises comprising or any variation thereof are intended to cover a non exclusive inclusion such that a process method article or apparatus that comprises a list of elements does not include only those elements but may include other elements not expressly listed or inherent to such process method article or apparatus. It is further understood that the use of relational terms if any such as first and second top and bottom and the like are used solely to distinguish one from another entity or action without necessarily requiring or implying any actual such relationship or order between such entities or actions.

Where applicable the present embodiments of the invention can be realized in hardware software or a combination of hardware and software. Any kind of computer system or other apparatus adapted for carrying out the methods described herein are suitable. A typical combination of hardware and software can be a mobile communications device with a computer program that when being loaded and executed can control the mobile communications device such that it carries out the methods described herein. Portions of the present method and system may also be embedded in a computer program product which comprises all the features enabling the implementation of the methods described herein and which when loaded in a computer system is able to carry out these methods.

While the preferred embodiments of the invention have been illustrated and described it will be clear that the embodiments of the invention is not so limited. Numerous modifications changes variations substitutions and equivalents will occur to those skilled in the art without departing from the spirit and scope of the present embodiments of the invention as defined by the appended claims.

