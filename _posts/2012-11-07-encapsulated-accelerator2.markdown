---

title: Encapsulated accelerator
abstract: A data processing system comprising: a host computer system supporting a software entity and a receive queue for the software entity; a network interface device having a controller unit configured to provide a data port for receiving data packets from a network and a data bus interface for connection to a host computer system, the network interface device being connected to the host computer system by means of the data bus interface; and an accelerator module arranged between the controller unit and a network and having a first medium access controller for connection to the network and a second medium access controller coupled to the data port of the controller unit, the accelerator module being configured to: on behalf of the software entity, process incoming data packets received from the network in one or more streams associated with a first set of one or more network endpoints; encapsulate data resulting from said processing in network data packets directed to the software entity; and deliver the network data packets to the data port of the controller unit so as to cause the network data packets to be written to the receive queue of the software entity.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09600429&OS=09600429&RS=09600429
owner: SOLARFLARE COMMUNICATIONS, INC.
number: 09600429
owner_city: Irvine
owner_country: US
publication_date: 20121107
---
This application is a Continuation In Part of U.S. application Ser. No. 12 964 642 filed 9 Dec. 2010 incorporated herein by reference. This application also claims the benefit of U.S. Provisional Application No. 61 714 405 filed 16 Oct. 2012 incorporated herein by reference.

This invention relates to a network interface device including an accelerator unit and a data processing system comprising such a network interface device.

Achieving the right balance between the functionality performance of a network interface device and power cost considerations has long been the subject of debate particularly in terms of the choice as to which aspects of the communication and other protocols that might operate over the device should be accelerated in hardware at the network interface device. Such acceleration functions are referred to as offloads because they offload processing that would otherwise be performed at the CPU of the host system onto the network interface device.

Usually the offload is chosen to be a specific function of the network protocol stack that is amenable to hardware acceleration. Typically this includes the data integrity aspects of a protocol such as TCP IP checksums iSCSI CRC digests or hashing or lookup operations such as the parsing of data flows onto virtual interface endpoints. Whether or not a particular function of a network protocol is amenable to hardware acceleration depends on several factors which will now be discussed.

Whether or not a function may be performed based solely on the contents of an individual network packet. This property is termed stateless when applied to an offload. A stateless offload requires little local storage at the network interface for example TCP IP checksum insertion on transmission requires buffering of a single Ethernet frame. In contrast a statefull operation may require the interface to store state relative to a large number of network flows over a large number of network packets. For example an Ethernet device that performs reassembly of TCP IP flows into units which are larger than the MSS Maximum Segmentation Size would be required to track many thousands of packet headers. Statefull protocol offloads can therefore require the network interface to have significant amounts of fast memory which is both expensive and power hungry.

Whether or not a function may be directly implemented in parallel logic operating over a single or small number of passes of the data contained within the network packet. This property is termed tractable. For example the AES GCM cryptographic algorithm has been designed such that the internal feedback loop may be unrolled when implemented. This enables a hardware designer to scale an AES GCM engine s performance bandwidth by simply adding more gates in silicon which by Moore s Law can be readily accommodated as higher speeds are required. In contrast the Triple DES cryptographic algorithm may not be unrolled into parallel hardware. This requires an implementation to iterate repeatedly over the data. In order to improve the performance of an iterative algorithm the implementation must scale in clock frequency which is becoming increasingly difficult on silicon based processes. Being untractable iterative algorithms are more difficult to implement as hardware offloads.

Whether or not a protocol function has been designed for hardware execution. Generally the specification of a hardware protocol will be unambiguous and strictly versioned. For example Ethernet line encodings are negotiated at link bring up time and once settled upon are strictly adhered to. Changing encoding requires a re negotiation. By contrast the TCP protocol that has not been specifically designed for execution at hardware is specified by many 10s of RFCs Request For Comments . These specifications often present alternative behaviours and are sometimes conflicting but together define the behaviour of a TCP endpoint. A very basic TCP implementation could be made through adherence to a small number of the RFCs but such a basic implementation would not be expected to perform well under challenging network conditions. More advanced implementations of the TCP protocol require adherence to a much larger number of the RFCs some of which specify complex responses or algorithms that are to operate on the same wire protocol and that would be difficult to implement in hardware. Software oriented specifications are also often in a state of continued development which is sometimes achieved without strict versioning. As such software oriented specifications are usually best expressed in high level programming languages such as C which cannot be easily parallelized and converted to hardware logic representation.

Whether or not a function is well known and commonly used enough for it to be considered for implementation in a commercial network interface device. Often application specific functions such as normalisation of stock exchange data feeds are only known to practitioners of their field and are not widely used outside of a few companies or institutions. Since the cost of implementing a function in silicon is tremendously expensive it might not be commercially viable to implement in hardware those functions whose use is limited to a small field.

In summary features that are typically chosen to be implemented as offloads in hardware are those which are stateless tractable hardware oriented well known and commonly used.

Unfortunately there are number of functions which do not meet these criteria and yet being performance sensitive greatly benefit from being accelerated in hardware offloads. For example in the Financial Services sector it is often the case that large numbers of data feeds must be aggregated together and normalized into a unified data model. This normalisation process would typically unify the feed data into a database by for example time representation or stock symbol representation which would require hundreds of megabytes of data storage to implement in hardware. Other niche application spaces that greatly benefit from being accelerated in hardware offloads include event monitoring equipment in high energy particle colliders digital audio video processing applications and in line cryptographic applications.

Often the hardware suitable for accelerating protocol functions in such niche application spaces does not exist because it is simply not commercially viable to develop. In other cases bespoke network interface hardware has been developed which implement the application specific offloads required but at significant cost such as with the Netronome Network Flow Engine NFE 3240. Additionally many bespoke hardware platforms lag significantly behind the performance of commodity silicon. For instance 40 Gb s Ethernet NICs are now available and the shift to 100 Gb s commodity products is quickly approaching yet most bespoke NICs based upon an FPGA are only capable of 1 Gb s.

To give an example the hardware offloads for a normalisation process in the Financial Services sector would typically be implemented at a NIC based upon an FPGA Field Programmable Gate Array controller that includes the features of a regular network interface as well as the custom offloads. This requires the FPGA controller to define for instance the Ethernet MACs and PCIe core as well as the custom offload engines and would typically be provided with a set of bespoke drivers that provide a host system with access to the hardware offloads of the FPGA. This implementation strategy is problematic because the speed and quality of FPGA chips for NICs is not keeping pace with the innovation of commodity NICs that use application specific integrated circuits ASICs . In fact the design and implementation of the PCIe core is often the rate determining factor in bringing a custom controller to market and FPGA vendors typically lag the commodity silicon designs by a year.

Furthermore the problem is becoming more acute as systems become more integrated and demand that NICs offer more commodity features such as receive side scaling RSS support for multiple operating systems network boot functions sideband management and virtualisation acceleration such as the hardware virtualisation support offered by the PCI SIG I O Virtualisation standards . This is being driven by the increasing use of virtualisation in server environments and data centres and in particular the increasing use of highly modular blade servers.

A data processing system is shown in of the type that might be used in the Financial Services sector to provide hardware accelerated normalisation of certain data feds. The data processing system includes a bespoke network interface device NIC coupled to a host system over communications bus . NIC has two physical Ethernet ports and connected to networks and respectively networks and could be the same network . The bespoke NIC is based around an FPGA controller that provides offloads and in hardware. The offloads could for example perform normalisation of data feeds received at one or both of ports and . Typically the NIC will also include a large amount of high speed memory in which the data processed by the hardware offloads can be stored for querying by software entities running at host system .

Generally host system will have an operating system that includes a kernel mode driver for the bespoke NIC and a plurality of driver libraries by means of which other software at user level is configured to communicate with the NIC . The driver libraries could be in the kernel or at user level . In the case of a host system in the Financial Services sector software might be bank software that includes a set of proprietary trading algorithms that trade on the basis of data generated by the offloads and and stored at memory . For example memory could include a database of normalised stock values the normalisation having been performed by the offloads and in accordance with known database normalisation methods. Typically host system will also include management software by means of which the NIC can be managed.

Since NIC provides a customised function set the vendor of the NIC will provide the driver and driver libraries so as to allow the software to make use of the custom functions of the NIC. Any software running at user level on the host system must therefore trust the vendor and the integrity of the driver and driver libraries it provides. This can be a major risk if the software includes proprietary algorithms or data models that are valuable to the owner of the data processing system. For example the data processing system could be a server of a bank at which high frequency trading software is running that includes very valuable trading algorithms the trades being performed at an exchange remotely accessible to the software over network or by means of NIC . Since all data transmitted to and from the host system over the NIC traverses the kernel mode vendor driver and vendor libraries the software including its trading algorithms are accessible to malicious or buggy code provided by the NIC vendor. It would be an onerous job for the bank to check all the code provided by the NIC vendor particularly since the drivers are likely to be regularly updated as bugs are found and updates to the functionality of the NIC are implemented. Furthermore a NIC vendor may require that a network flow is established between the management software of the NIC to the NIC vendor s own data centres. For example this can be the case if the NIC is a specialised market data delivery accelerator and the market data is being aggregated from multiple exchanges at the vendor s data centers. With the structure shown in the bank would not be able to prevent or detect the NIC vendor receiving proprietary information associated with software .

Financial institutions and other users of bespoke NICs that need to make use of hardware offloads are therefore currently left with no choice but to operate NICs that offer a level of performance behind that available in a commodity NIC and to trust any privileged code provided by the NIC vendor that is required for operation of the NIC.

There have been efforts to arrange network interface devices to utilise the processing power of a GPGPU General Purpose GPU provided at a peripheral card of a data processing system. For example an Infiniband NIC can be configured to make peer to peer transfers with a GPGPU as announced in the press release found at 

http colon slash slash www dot mellanox dot com pdf whitepapers TB GPlJ Direct.pdf. Both of these documents are incorporated herein by reference for their teachings.

However despite offering acceleration for particular kinds of operations such as floating point calculations GPGPUs are not adapted for many kinds of operations for which hardware acceleration would be advantageous. For example a GPGPU would not be efficient at performing the normalisation operations described in the above example. Furthermore in order for a NIC to make use of a GPGPU the NIC typically requires an appropriately configured kernel mode driver and such an arrangement therefore suffers from the security problems identified above.

Other publications that relate to memory mapped data transfer between peripheral cards include Remoting Peripherals using Memory Mapped Networks by S. J. Hodges et al. of the Olivetti and Oracle Research Laboratory Cambridge University Engineering Department a copy of the paper is available at http colon slash slash www dot cl dot cam dot ac dot uk research dtg www publicatlons public files tr.9R.6.pdf and Enhancing Distributed Systems with Low Latency Networking by S. L. Pope et al. of the Olivetti and Oracle Research Laboratory Cambridge University Engineering Department a copy of the paper is available at http colon slash slash www dot cl dot cam dot ac dot uk research dtg www publications public files tr.98.7.pdt . Both of these documents are incorporated herein by reference for their teachings.

There is therefore a need for an improved network interface device that provides a high performance architecture for custom hardware offloads and an secure arrangement for a data processing system having a network interface device that includes custom hardware offloads.

According to a first aspect of the present invention there is provided a data processing system comprising a host computer system supporting a software entity and a receive queue for the software entity a network interface device having a controller unit configured to provide a data port for receiving data packets from a network and a data bus interface for connection to a host computer system the network interface device being connected to the host computer system by means of the data bus interface and an accelerator module arranged between the controller unit and a network and having a first medium access controller for connection to the network and a second medium access controller coupled to the data port of the controller unit the accelerator module being configured to on behalf of the software entity process incoming data packets received from the network in one or more streams associated with a first set of one or more network endpoints encapsulate data resulting from said processing in network data packets directed to the software entity and deliver the network data packets to the data port of the controller unit so as to cause the network data packets to be written to the receive queue of the software entity.

Preferably the software entity is configured to programme the first set of one or more network endpoints into the accelerator module.

Suitably said processing of incoming data packets by the accelerator module comprises parsing the incoming data packets so as to identify network messages carried therein that have one or more of a set of characteristics. Preferably the software entity is configured to programme said set of characteristics into the accelerator module.

Suitably the data resulting from the processing of the incoming data packets comprise said identified network messages.

Suitably the software entity is a financial application configured to trade on a remote electronic exchange accessible over the network the incoming data packets comprise financial messages and said processing of the incoming data packets by the accelerator module comprises processing the financial messages so as to generate normalised financial data. Suitably the said set of characteristics defines a set of security symbols. Suitably the data resulting from the processing of the incoming data packets comprises said normalised financial data.

Preferably the accelerator module is configured to forward onto the controller unit incoming data packets that are not associated with the first set of one or more network endpoints.

Said processing of incoming data packets by the accelerator module could comprise one or more of normalisation of financial information carried within financial messages of the incoming data packets serialisation of trades carried within financial messages of the incoming data packets and directed to an electronic exchange arbitration between financial message streams decompression or compression of data packet headers analysis of scientific data carried within the incoming data packets processing of digital audio and or video data carried within the incoming data packets and in line cryptographic functions performed on data carried within the incoming data packets.

Preferably the accelerator module is provided at the network interface device. Preferably the controller unit and accelerator module are coupled to one another by means of a SERDES link. Preferably the network interface device further comprises a physical network interface arranged to couple the first medium access controller of the accelerator module to the network the physical network interface supporting signalling over the network in accordance with a predetermined physical layer protocol.

Preferably the accelerator module further comprises additional interface logic operable to at least partially form memory transactions for performance over the data bus. Preferably the accelerator module is configured to encapsulate said at least partially formed memory transactions in network data packets directed to a network endpoint of the controller unit so as to cause the controller unit to perform the memory transactions over the data bus. Alternatively the at least partially formed memory transactions generated at the additional interface logic are provided to the controller unit over an additional link provided between the accelerator module and the controller unit.

Suitably the additional interface comprises logic sufficient to at least partially form PCIe Transaction Layer Packets.

Preferably the controller unit is further coupled to the accelerator module by a Network Controller Sideband Interface and a software driver of the network interface device at the host computing system is configured to relay out of band control messages generated by the software entity to the accelerator module by means of the Network Controller Sideband Interface.

Preferably the host computing system includes a software driver configured to manage the accelerator module by means of driver commands encapsulated within network data packets and directed to a network endpoint of the accelerator.

Preferably the host computing system is a virtualised system having a privileged software domain including a first software driver for the controller unit and configured to present a virtual operating platform to first and second guest domains the software entity being an application supported at the first guest software domain and the second guest software domain having a driver library for said accelerator module the software entity and driver library being accessible to one another as network endpoints.

Preferably the privileged software domain does not include a second software driver for the accelerator module.

Preferably the first guest software domain includes a transport library and the application is arranged to access the driver library and accelerator module by means of the transport library.

Preferably the accelerator module or network interface device further comprises a memory configured for storing data generated by said processing performed by the accelerator module and the software entity is configured to access said memory by means of a read request message encapsulated within a network data packet directed to an endpoint of the accelerator module.

Preferably the network interface device is an Ethernet network interface device and the first and second medium access controllers of the accelerator module are Ethernet MACs.

Suitably the accelerator module is located at a network entity distinct from the host computer system and network interface device and coupled to the controller unit by one or more network links. Preferably the software entity is configured to cause the network to direct said one or more streams to the accelerator module in preference to the host computer system.

According to a second aspect of the present invention there is provided a data processing system comprising a host computer system supporting a software entity and a transmit queue for the software entity a network interface device having a controller unit configured to provide a data port for transmitting data packets onto a network and a data bus interface for connection to a host computer system the network interface device being connected to the host computer system by means of the data bus interface and an accelerator module arranged between the controller unit and a network and having a first medium access controller for connection to the network and a second medium access controller coupled to the data port of the controller unit the accelerator module being configured to on behalf of the software entity process outgoing data packets received from the transmit queue in one or more streams associated with a first set of one or more network endpoints encapsulate data resulting from said processing in network data packets directed to said first set of one or more network endpoints and deliver the network data packets onto the network.

Preferably the software entity is configured to programme the first set of one or more network endpoints into the accelerator module.

Suitably said processing of outgoing data packets by the accelerator module comprises parsing the outgoing data packets so as to identify network messages carried therein that have one or more of a set of characteristics. Preferably the software entity is configured to programme said set of characteristics into the accelerator module.

Suitably the data resulting from the processing of the incoming data packets comprise said identified network messages.

Preferably the host computing system includes a software driver configured to manage the accelerator module by means of driver commands encapsulated within network data packets and directed to a network endpoint of the accelerator.

According to a third aspect of the present invention there is provided a reconfigurable logic device for processing data packets and comprising first and second medium access controllers each for communicating network data packets the reconfigurable logic device being programmable with a set of algorithms which when performed on data packets received by means of the first medium access controller cause the reconfigurable logic device to process the received data packets the reconfigurable logic device being configured to encapsulate data resulting from said processing in network data packets for transmission by means of the second medium access controller.

According to a fourth aspect of the present invention there is provided a network interface device for use with a reconfigurable logic device as described herein the network interface device comprising a controller unit configured to provide a data port for communicating network data packets and a data bus interface for connection to a host computer system and a socket for a reconfigurable logic device the socket being coupled to a physical interface for connection to a network and to the data port of the controller unit such that in use when a reconfigurable logic device is located in the socket network data packets received over the physical interface pass through the reconfigurable logic device prior to being received at the controller unit and or network data packets received from the data port of the controller unit pass through the reconfigurable logic device prior to being transmitted over the physical interface.

Preferably the data port of the controller unit is coupled to the socket for a reconfigurable logic device by means of a serial interface device configured to effect the physical communication of data between the accelerator module and the controller unit.

Preferably the physical interface comprises a physical layer transceiver for performing signalling according to the physical layer of a predetermined network protocol.

The following description is presented to enable any person skilled in the art to make and use the invention and is provided in the context of a particular application. Various modifications to the disclosed embodiments will be readily apparent to those skilled in the art.

The general principles defined herein may be applied to other embodiments and applications without departing from the spirit and scope of the present invention. Thus the present invention is not intended to be limited to the embodiments shown but is to be accorded the widest scope consistent with the principles and features disclosed herein.

The present invention provides solutions to the problems identified in the prior art by offering a novel network interface device and data processing system architecture. A network interface device NIC configured in accordance with the present invention is not limited to providing an interface to a particular network fabric having a particular kind of interface to a host system or to supporting a particular set of network protocols. For example such a NIC could be configured for operation with an Ethernet network IEEE 802.11 network or a FibreChannel network interface to a host system over a PCIe PCI X or HTX bus support communications over UDP TCP IP or IPsec. A host system could be any kind of computer system at which a network interface device can be supported such as a server. A host system comprising a network interface device will be referred to herein as a data processing system. Note that a network interface device configured in accordance with the present invention need not be provided as a device for connection to an expansion slot e.g. PCIe or communications port e.g. eSATA of a host system and could form part of the host system. For example the network interface device could be located at the motherboard of a host system. A controller or controller unit of a network interface device refers to any IC or collection of ICs configured to communicate data between a network and a host processing system to which the NIC is connected.

A network interface device and host system configured in accordance with the present invention is shown in . The NIC presents two ports and for connection to physical networks and but these ports are not directly connected to the ports of the NIC controller . A hardware accelerator unit is connected between the controller and the ports such that data incoming from and outgoing to the networks and passes through the hardware accelerator. Preferably the accelerator unit is a reconfigurable logic device such as an FPGA or other programmable integrated circuit. The accelerator unit could include a memory for the storage of data relating to the offloads performed at the accelerator.

In the present example since the NIC supports two external ports for connection to networks and the controller supports two ports and the accelerator unit provides four ports and for connection to the external ports of the NIC and and for connection to the ports of the NIC controller. More generally the NIC could support any number of ports with the accelerator and controller each providing a commensurate number of ports. Each of the ports and includes a Medium Access Controller MAC which in the case that and are Ethernet ports would be Ethernet MACs. MACs and of the accelerator unit are provided with PHYs and that implement the physical layer communication protocol in use over the NIC and couple the MACs to the physical medium of networks and . The PHYs could be provided at the accelerator but would preferably be provided at one or more separate integrated circuits. MACs and could be implemented at accelerator provided at a separate integrated circuit or could be part of a multi chip module MCM with the accelerator IC.

The accelerator unit is configured to support any custom hardware offloads required of the NIC so as to allow controller integrated circuit to remain uncustomised. Thus a standard commodity network interface controller can be used as controller which brings with it all the performance advantages of using commodity silicon. For example in the case of an Ethernet NIC controller could be a 40 Gb s part configured to support two ports each at up to 20 Gb s. Aside from the raw speed improvements gained by using a commodity ASIC controller ASIC controllers and their software drivers are generally more highly optimised and ASICs are cheaper smaller and consume less power for a given performance level than FPGAs or other programmable ICs. Furthermore the relatively expensive accelerator unit can be smaller and more straightforward because the accelerator ICs do not need to provide the functions of a regular NIC controller such as host interfaces support for parts of a network stack etc. .

Preferably the accelerator unit is a reconfigurable logic device programmable with the algorithms e.g. code firmware processing steps required for performing the required custom hardware offloads.

By providing the accelerator with MACs so as to allow layer 2 the accelerator or parts of it can be logically addressed as a network endpoint. This allows network messages to be communicated to the accelerator by encapsulating those messages in appropriately formed data packets addressed to logical endpoints held by the accelerator. Hardware accelerator therefore differs from other forms of custom accelerator for example a GPGPU that terminate data flows and that require a NIC configured to support a proprietary interface to the accelerator and or an interface that requires additional driver layers at the host computer system.

The accelerator is configured to communicate with both network and host entities by means of data packets formed in accordance with the network protocols in use over networks and and links and . Thus the accelerator is operable to encapsulate within data packets for delivery to host or network endpoints network messages that are formed at the accelerator or extracted from data streams at the accelerator. New data packet streams could be established between the accelerator and respective host network endpoint for carrying such newly formed data packets. In this manner the accelerator can communicate with host software or other network entities by means of network data packets that are conventionally routable. It is therefore possible to make use of existing network controllers.

The accelerator could include one or more processing engines optimised for performing different types of processing on behalf of host software. For example an accelerator for processing financial messages could include a parsing engine for parsing data packets of certain incoming or outgoing feeds so as to identify relevant messages or form exchange feeds. Such an accelerator could further include one or more different processing engines that operate in sequence on those identified messages in order to for example execute a predetermined trading algorithm and so generate financial network messages defining trades to be performed at a remote financial exchange or to write data values retrieved from memory to outgoing financial network messages.

The host facing MACs and of the accelerator and network facing MACs and of the controller preferably support the same low level communication protocol e.g. Ethernet as the network facing MACs and so as to avoid the overhead incurred by translating network data packets between protocols at the accelerator. The controller could be configured to provide to the accelerator data packets from the host that are directed to the accelerator through appropriate configuration of the routing table of the NIC.

Accelerator could be configured to perform processing of network messages received from one or both of the host and the network. The accelerator would preferably be configured to identify data packets that comprise network messages for processing by looking for characteristics that indicate that the data packet belongs to a stream that is to be handled at the accelerator. Typically this would be performed by looking at the header of the data packet only. For example by identifying data packets that are directed to network endpoints associated with the accelerator. This identification could be performed on the basis of source or destination address data packet type i.e. the communication protocols the packet complies with or any number of payload identifiers that indicate a message that is to be processed at the accelerator. Preferably the accelerator would be programmable with such characteristics by host software. Data packets which are not to be handled at the accelerator would by default be passed through to the NIC controller.

One form of processing that could be performed at the accelerator could be the parsing of data packets that have been identified as being for processing at the accelerator in order to identify the network messages contained therein. This would typically be the first step in processing the network messages of a stream of data packets. It might be the case for example that only some of the network messages comprised within data packets for the accelerator are required at the host or a remote network endpoint in which case the accelerator could identify those network messages that are required and encapsulate those network messages for delivery in one or more new streams. In other cases different network messages of a data packet might be processed in different ways at the accelerator.

Note that the network endpoints associated with the accelerator could in fact be terminated at the host or at a network entity. However by arranging that the accelerator identify data packets associated with these endpoints the accelerator can perform processing of the data packets before passing them on to those respective endpoints. This allows for example incoming data packets to undergo processing steps in hardware at the accelerator before being passed onto the controller for writing those data packets into the respective host receive queues and similarly for outgoing data packets to undergo processing steps in hardware at the accelerator before being transmitted over the network.

The accelerator preferably does however support one or more network endpoints that are at least addressable within the data processing system. This allows software supported at the host computing system to address the accelerator e.g. so as to configure or query the accelerator by directing messages to an endpoint of the accelerator. Software supported at the host could communicate with the accelerator by means of standard socket and transport libraries configured to translate messages at the application level into network data packets and vice versa. Similarly by supporting endpoints addressable over the network software running on a switch server router or other entity on the network could also communicate with the accelerator via standard network protocols.

In alternative embodiments of the present invention no identification of data packets to determine whether they are intended for processing at the accelerator is performed. This could be arranged if the port at which those data packets are received is configured to only receive data packets intended for processing at the accelerator from either the network or host . For example through appropriate configuration of network port could be provided with only those data feeds that the accelerator is configured to process. The accelerator could then be configured to pass through to the controller all data packets received at port or network could be connected directly into port of the controller such that data packets from that network do not first pass through the accelerator i.e. not all ports of the controller must be coupled to a network by means of the accelerator one or more ports could be directly connected to a network . By locating the accelerator between the network and controller in such cases complex parsing logic would not therefore be required active at the accelerator or the controller.

In terms of the receive path hardware accelerator is configured to process data packets arriving in data packet streams from the network for one or more endpoints associated with the accelerator and forward the processed data packets or data resulting from the processing of the received data packets onto one or more receive queues at the host computer system. The hardware accelerator passes data onto the NIC controller by encapsulating that data in network data packets. In terms of the transmit path hardware accelerator is configured to process data packets arriving in data packet streams from the host for one or more endpoints associated with the accelerator and forward the processed data packets or data resulting from the processing of the outgoing data packets onto one the network. The hardware accelerator passes data onto the NIC controller by encapsulating that data in network data packets. Thus the hardware accelerator can process streams of incoming and outgoing data packets on the fly.

Alternatively or additionally the hardware accelerator could process data packets arriving in data packet streams and store the results of that processing in its memory or at the host e.g. if the accelerator could write directly into host memory by means of the controller . It can be advantageous if the accelerator is in this case configured to allow appropriately formed data packets from the host or network to query the stored data. This provides particularly low latency responses to network entities because the accelerator is connected between the host and the network.

The MACs and of controller are coupled to the host facing MACs of accelerator such that data packets can be exchanged at low latency over links and . This also allows data packets that are received at the accelerator but which are not directed to endpoints at the accelerator to be passed through to the controller with little or no modification certain stateless processing such as checksum validation could be performed prior to the data packets being received at the controller accelerator . Since links and would typically be very short basic physical layer signalling could be used to exchange layer 2 data packets without necessarily employing the typically advanced physical layer signalling used over longer connections. For example serial interface devices such as KX4 serial devices could be used for physical signalling between the accelerator and controller. The use of serial interface devices has the advantages that they are low power and can be implemented using standard SERDES libraries. In order to effect signalling between the accelerator and controller the accelerator and controller would include integrally or as a separate or co located IC a serial interface device so as to provide a given physical interface between a MAC of the accelerator and the corresponding MAC of the controller.

The routing table of the NIC would preferably be configured to enable the controller to direct data packets between endpoint s associated with the accelerator receive queues of the host computer system and the network endpoints of remote hosts accessible over the network. The controller would generally be better optimised for performing such functions and it is preferable that any switching functions required of the NIC are performed at the controller.

The serial interface devices or less preferably full PHYs according to the network protocol in use at the NIC could be provided at integrated circuits separate from the respective controller accelerator or could be part of a multi chip module MCM with the respective controller accelerator or even integrated on die

NIC controller is configured so as to perform the switching of network data packets between its data ports and data bus . The controller is therefore operable to direct data packets to the hardware accelerator that are received from the host and identified as being directed to the hardware accelerator in the same way as it might direct data packets destined for a remote endpoint on network over port . This can be achieved in the conventional manner by programming the switch of controller to route data packets to particular data ports in dependence on the network endpoint i.e. network address to which each data packet is directed. Preferably controller is programmed such the particular network endpoint at the host system to which a data packet is directed determines the DMA channel into which it is delivered.

More generally a NIC configured in accordance with the present invention could have any number of ports with a corresponding number of ports being provided at the controller and each of the network facing and host facing sides of the accelerator. For example if the NIC provides three network ports the accelerator would have six ports in total three network facing and three host facing and the controller would have three ports coupled to the host facing ports of the accelerator. In alternative embodiments of the present invention it need not be the case that all network ports of the NIC connect through the accelerator and one or more ports of the NIC could be directly connected into one or more corresponding ports of the controller such that data packets received over those network ports do not traverse the accelerator. This can be advantageous if for example data packets received from certain networks are not required at the accelerator.

Note that the accelerator integrated circuits need not be programmable and could be bespoke ASICs. This is unusual because of the high cost of designing and manufacturing an ASIC. However it will be apparent that many of the advantages of the present invention remain a network interface controller ASIC is generally more highly optimised than a bespoke controller ASIC that is designed to support one or more hardware offloads and because many of the complex functions present in a network interface controller need not be designed and manufactured at great expense as part of the custom ASIC. It may be that for some acceleration functions the accelerator ASIC could be based upon other processing architectures such as a GPU or NPU.

By placing the accelerator before the NIC controller the accelerator is in a position to respond at very low latency to data received from the networks . For example accelerator could be configured to support financial trading algorithms configured to automatically place trades at a remote financial exchange in response to data feeds received from that exchange. This can be achieved through suitable programming of the accelerator to cause the accelerator to identify the network messages it is to process and then to process those messages so as to in response form orders according to the trading algorithms. Such a low latency data path could also be useful for other applications for which it would be advantageous to perform processing at the accelerator such as scientific and database applications digital audio video processing applications and in line cryptographic applications.

Furthermore by placing the FPGA before the NIC controller the FPGA can be configured to provide a fail to wire mode in which it diverts all incoming data packets back out onto the network in the event that the host becomes unresponsive and stops processing data packets. The accelerator could in this event be configured to update or encapsulate the headers of incoming data packets so as to cause those packets to be directed to another network entity at which those packets could be processed.

It can be advantageous for NIC to be provided in two parts hardware accelerator and a reference NIC that includes all the parts of the NIC shown in except for the hardware accelerator or equally an accelerator IC and a reference NIC that includes all the parts of the NIC shown in except for the accelerator IC. By providing at the reference NIC an interface configured to receive a hardware accelerator or accelerator IC a single reference NIC design can be used with a variety of different hardware accelerators. This allows the custom offloads provided at the NIC to be readily upgraded or modified by simply replacing the hardware accelerator or accelerator IC at the NIC and installing new versions of the driver libraries for the hardware accelerator accelerator IC at the host system. Such a reference NIC could be configured such that the accelerator forms an optional part of the NIC. This can be achieved through the use of switches or a dummy hardware accelerator part that causes the ports of the NIC e.g. and to be connected through to the controller unit e.g. to its MACs and .

The controller is configured to interface with host system over data bus which could be for example a PCIe data bus. The data bus could alternatively be the backplane of a blade server and could itself operate in accordance with one or more network protocols for example the data bus could be a high speed Ethernet backplane.

In the present example host system is a virtualised system comprising a privileged software entity such as a hypervisor or virtual machine monitor that presents a virtual operating platform to a plurality of guest operating systems and . The privileged software entity operates at a higher level of privilege e.g. kernel mode than the guest operating systems which operate at a lower level of privilege e.g. user level mode . However more generally host system need not be virtualised and could comprise a conventional monolithic software environment with a single operating system supporting a set of applications.

Privileged software entity includes a network interface device driver that is configured to provide a software interface to NIC controller . Importantly because controller is not customised driver can be a standard driver for the controller whose code has been certified by a trusted party such as the vendor of the privileged software entity e.g. through the VMWare IOVP or Microsoft WHQL programs . The driver could also be digitally signed so as to authenticate the origin of the code. For example if the NIC is an Ethernet NIC and the privileged software entity a Hyper V Hypervisor of Microsoft Windows Server then driver could be provided by the NIC vendor and certified by Microsoft for operation in the hypervisor. Since any software installed at the host system must necessarily trust the platform on which it was installed software executing at guest OS can trust the driver over which it communicates. Furthermore since driver does not provide any custom functionality and need not be updated when any offload functions implemented at the NIC are modified it would be possible for the operator of software running at guest domain to check the driver for any malicious or buggy code and trust that the driver is certified and remains unmodified throughout the production life of the machine.

Privileged software entity also includes a soft switch configured to route data packets between the guest operating systems and the network endpoints served by the NIC i.e. on networks or or at the hardware accelerator and between network endpoints at the guest operating systems themselves. Network endpoints are for example Ethernet or internet protocol IP network addresses. Typically the soft switch operates only on the standard set of network protocols supported by driver .

One of the guest operating systems is configured to include driver libraries for the hardware accelerator. Importantly driver libraries are configured to communicate with the hardware accelerator by means of data e.g. commands responses state information encapsulated within network packets directed to an endpoint of the hardware accelerator. Such data packets are routed at soft switch onto data bus for the NIC and at the switch functions of NIC controller the data packets are routed onwards to port or and hence the hardware accelerator. Similarly hardware accelerator is configured to communicate with driver libraries by means of data e.g. commands responses state information encapsulated within regular network packets directed to an endpoint of guest operating system e.g. a receive queue of the driver libraries . In this manner communications between the driver libraries of the hardware accelerator and the hardware accelerator itself can be achieved using regular network packets that can be handled as such at the switches of the system. The benefits of this are twofold firstly it allows the hardware accelerator to be implemented at a high speed port of a commodity NIC as though the hardware accelerator is a network entity addressable over a particular port and secondly it allows the driver libraries for the hardware accelerator to be located outside of the kernel at a guest operating system having a low privilege level.

The architecture of the host system is therefore arranged such that none of the code relating to the functions of the hardware accelerator is at a higher privilege level than any sensitive or secret software executing in another guest operating system . Software could be for example a bank s high frequency trading software comprising a set of highly valuable proprietary trading algorithms. By isolating driver libraries from software in this manner the owners of software can be confident that any malicious or buggy code provided by the vendor of the hardware accelerator cannot cause the activities of software to be revealed. Accelerator vendor domain could also include any management software for the hardware accelerator.

Accelerator vendor libraries and accelerator management software are arranged to configure the offload functions performed by the hardware accelerator. This can be by for example defining the normalisation parameters to be applied to each type of stock managing the use of memory by the offloads of the accelerator IC and defining the characteristics of data packets or messages received at the accelerator that are to be handled at the accelerator and not simply passed through to the controller network.

Software is configured to communicate with accelerator driver libraries by addressing the driver libraries as a network endpoint. In other words software transmits network data packets to a network endpoint represented by a receive queue of the driver libraries as though the driver libraries were a remote network entity. Similarly driver libraries are configured to communicate with software by addressing the software as a network endpoint. The data packets sent between the software and driver libraries encapsulate commands responses and other data in an analogous way to the system calls and responses exchanged between software and kernel drivers in conventional host systems.

Since data to and from the hardware accelerator can be encapsulated as network data packets software can communicate with vendor libraries and hardware accelerator by means of a generic application programming interface API at the software domain . The API maps network send and receive requests by software into the transmission and reception of network data packets. Preferably the protocol in use over connections between software and the hardware accelerator or vendor libraries is a light low latency protocol such as UDP User Datagram Protocol . The API could be a POSIX API or other generic API suitable for use at domain . No proprietary accelerator vendor code is therefore required at domain .

As is well known in the art some aspects of the formation of data packets in accordance with the network protocol could be performed at the NIC such as checksum formation. However it is preferable that connections between software and hardware accelerator or vendor libraries are configured such that checksums are not required in data packets exchanged between those entities. If the path between software and the accelerator is not reliable then a retransmission protocol would preferably be adopted so as to ensure an appropriate level of reliability.

Using a standard network encapsulation and a commodity NIC controller for all messages exchanged with the hardware accelerator has a number of advantages 

By locating the hardware accelerator between the network and the NIC controller the accelerator can receive data with the lowest possible latency. This is very important for certain applications such as in high frequency trading. The present invention provides an architecture in which trading algorithms can be performed as close as possible to the network such that data feeds can be processed and new trades generated at very low latency without the data having to first traverse the NIC controller.

Since a conventional high speed NIC controller can be used data flows can be delivered using receive side scaling RSS interrupt moderation and other techniques that improve performance at a host system having a multi core CPU architecture.

Data flows can be delivered using direct guest access to the guest domains of the virtualised host system with the hardware virtual switch of controller being configured to select the appropriate DMA delivery channel.

A PCIe controller can be selected that implements the SR IOV or MR IOV virtualisation standards that allow multiple DMA channels to be mapped directly into virtual guest address spaces.

These advantages can be achieved through the use of a conventional NIC controller and without requiring that additional functionality is provided at the controller. It can be particularly advantageous to use one or more of techniques 2 to 4 above together at a data processing system.

Note that the advantages described above of a NIC configured in accordance with the present invention do not rely on the NIC being supported at a host system having a virtualised architecture as shown in other host system architectures could be used with NIC in which the offload functions of the hardware accelerator can be accessed as network endpoints. However a data processing system comprising the combination of NIC and host system of is particularly advantageous since it provides all the performance cost and flexibility benefits of a NIC as described herein with all the security and stability benefits of a host system having the architecture shown in the figure.

The data processing system and network interface card described herein benefits from the fact that all the kernel mode components of the system can be provided by the commodity vendor and so can be more easily made robust over a large number of operating systems. For example commodity NIC software is implemented in the mass market and hence benefits from a commensurate level of engineering and investment. The use of such commodity code reduces the likelihood that the NIC driver would cause instabilities at the data processing system.

The operation of NIC with host system will now be described by way of example. Suppose the data processing system is a high frequency trading server owned by a bank and the hardware accelerator at the NIC provides a set of database normalisation offloads that can be performed on stock data received from an exchange accessible over network . Such offloads would be performed by the accelerator IC prior which could then optionally store the results of that offload processing at a database in memory or at the host.

By appropriately configuring the characteristics of exchange messages that are to be processed at the accelerator the accelerator is directed to identify those messages on which it is to operate. Other messages or data packets from sources other than the financial exchange s of interest would be passed through to the controller . In this manner messages from the desired feeds that arrive at the NIC from the exchange would be normalised by the appropriate hardware offloads defined at the accelerator IC. The accelerator can be configured by means of appropriate instructions from management software in response to requests from the bank s trading software to set up the accelerator so as to process the desired messages from a set of one or more exchange feeds received at the NIC. The hardware accelerator would preferably be associated with the endpoints to which the exchange feeds are directed at the host such that the accelerator receives the data packets of those feeds.

As stock feeds stream in over port and are normalised at the accelerator IC a normalised database of stock data could be built up at memory or at the host. This is the data that is valuable to the bank s trading algorithms embodied in trading software and that must be accessed in order to allow the software to make trading decisions. Alternatively the hardware accelerator could support trading algorithms such that in response to receiving exchange messages relating to for example certain security symbols the accelerator would issue trades in dependence on the values of those security symbols. The algorithms and parameters of the trades would preferably be programmed into the accelerator by the host software managing the accelerator.

Access to the hardware accelerator is mediated by accelerator vendor libraries . Thus if trading software requires access to the hardware accelerator e.g. to configure a trading strategy or normalisation performed at the accelerator the vendor libraries are configured to establish connection s between one or more endpoints of the hardware accelerator and one or more endpoints of the trading software. Once a connection between the trading software and hardware accelerator has been established e.g. a connection between an endpoint of the hardware and an endpoint at guest domain has been set up trading software can read and write to hardware accelerator by means of generic API and the protocol stack.

In this example data is exchanged between the trading software and hardware accelerator in accordance with the UDP protocol with the incoming exchange feeds comprising messages according to the a number of exchange specific protocols including FIX Itch OPRA references available if required . To ensure low latency delivery of data to the trading software the NIC controller is configured to deliver data packets directed to guest domain over DMA channels established between the NIC and the receive queues of the guest domain. In this manner the trading software can receive at low latency normalised exchange data or data indicating trades placed by the accelerator. If the NIC accelerator supports a database comprising data generated by the normalisation offloads of the accelerator IC the trading software can utilise the low latency network path between host and accelerator in order to allow the proprietary trading algorithms embodied in the software to access the database and make its trading decisions.

Note that the term database is used to refer to an organised cache of data and does not imply any particular general purpose database architecture. Database queries sent by the trading software in network data packets are preferably formatted in accordance with an API defined by the vendor of the hardware accelerator.

Trading algorithms embodied either at the accelerator itself or at trading software place orders in dependence on the exchange messages received from the network. In this example port is used to receive stock feed data and port is used to transmit the orders to one or more remote exchanges accessible over network .

In a second example the accelerator could be configured to perform header compression and or decompression. Network data packets having compressed headers and directed to endpoints associated with the accelerator would have those headers decompressed at the accelerator with the modified data packets being passed on to their respective endpoints at the host. Similarly on the transmit path data packets generated at the host system for transmission over data streams configured to carry data packets having compressed headers could have their headers compressed in hardware at the accelerator. This could be achieved by associating the endpoints to which those data streams are directed with the accelerator so as to cause the accelerator to process those streams and configuring the accelerator to perform the required layer 3 header compression before passing the modified data packets onto the network. Such an arrangement can be useful for handling packets received over or being for transmission over low bandwidth wireless links that require compressed layer 3 e.g. IP  headers.

In a third example the accelerator could be configured to perform arbitration between message flows received in streams of data packets. For example the same financial messages are sometimes provided in two or more streams for redundancy. In such circumstances it can be useful if the accelerator is configured to compare the sequence numbers of network messages received in redundant data streams and deliver only one copy of each message to the respective host receive queue. In some cases the redundant network messages would be received in data streams having different characteristics for example one stream could have compressed layer 3 headers and another might have uncompressed layer 3 headers in which case the accelerator might have to first perform decompression of the compressed data packet headers. The accelerator could be configured to provide the network messages in a new data packet stream originating at the accelerator or as a stream of modified data packets.

The hardware accelerator need not be physically located at network interface device and could be provided at another unit of the data processing system with links and being loop through connections between the accelerator and NIC units such that network traffic first passes through the accelerator and then onto the NIC card. For example the hardware accelerator could be provided at a PCIe card connected to the NIC by a low latency interconnect such as a serial link.

In the examples described herein the hardware accelerator is located at network interface device . However since the accelerator can be addressed as a network endpoint the accelerator could alternatively be provided at a network entity distinct from the host system such as at a switch or other item of network equipment. It may be necessary to configured the network e.g. its switch fabric such that incoming data flows that the accelerator is configured to process are directed in preference to the accelerator e.g. to the accelerator instead of the host system and or that outgoing data flows that the accelerator is configured to process are directed to the accelerator prior to being transmitted from the accelerator to their intended endpoints.

Additional logic could be provided at accelerator to at least partially support the formation of memory transactions over bus . This allows the accelerator to address memory at the host and hence permits low latency communication with data structures maintained at the host. This is especially advantageous for data structures that must remain coherent between the accelerator and host software.

The complexity of this additional logic would depend on the level at which the memory transactions are generated at the accelerator. For example the accelerator could be configured to merely form pseudo memory read write requests which would be translated into memory transactions for data bus at controller which includes the logic and physical interfaces necessary to communicate over bus . Or the accelerator could include sufficient logic to form requests in accordance with the link layer protocols of data bus and merely rely on the controller to perform physical signalling of the memory transactions onto the data bus. In both cases the physical layer of the data bus would terminate at controller . To give a particular example if data bus is a PCIe bus accelerator could include sufficient logic to allow it to form PCIe Transaction Layer Packets TLPs .

The memory transactions would preferably be passed to the controller as messages encapsulated in data packets over link e.g. as memory transaction messages encapsulated within Ethernet packets . By directing such data packets to a predetermined network endpoint supported at the controller the controller could be caused to perform the memory transactions over data bus on behalf of the accelerator and pass responses to the transactions back to the accelerator also encapsulated in data packets.

Alternatively such memory transactions could be passed to the controller over an additional link between the accelerator and controller. Such a link can be especially useful for conveying out of band control messages to the accelerator from the host e.g. from host software managing the accelerator . Such an arrangement means that the accelerator does not need to multiplex data and control messages and can use the entire bandwidth of links and for data. Link could for example be an NC SI bus for low power server management with control messages being encapsulated for transmission over the NC SI bus between the accelerator and controller. Control messages would be passed between host and device over data bus e.g. a PCIe bus as is conventional for such interfaces. This allows the device driver to relay control messages to the accelerator over an out of band path by providing suitable logic NC SI at the accelerator. It can be further advantageous to provide a memory mapping between user space onto a device driver of the controller so as to allow the user level control software to send messages to the accelerator over the out of band path by means of the device driver and controller. This provides the illusion of memory mapped hardware access for user level control applications.

The network interface device itself need not be provided at a discrete peripheral card of the system and could be located at the mainboard of the system i.e. as a LOM device . The controller and in less preferred embodiments the accelerator could be integrated into a CPU.

The data packets exchanged between the receive queues of the host system and the network endpoints of the hardware accelerator could be for example UDP data packets directed to network endpoints identified by IP addresses.

A MAC configured in accordance with the present invention could include multiple protocol layers and is not necessarily restricted to handling only MAC communications protocol. Which protocol layers are supported at a MAC depends on the particular network protocols in use over the data port for which the MAC is provided. For example if the data ports are Ethernet ports the MAC would preferably perform only the Ethernet MAC layer but could also perform the Ethernet LLC layer. With such an arrangement the network endpoint supported at the MAC of a hardware accelerator would be an Ethernet network address and data communicated with the hardware accelerator would be encapsulated in Ethernet frames at the NIC.

Since the accelerator provides the network facing MACs it is advantageous if the accelerator is configured to manage bringup and training of the physical connections e.g. Ethernet it supports at its network facing ports. It is also preferable that the accelerator is configured to make available MAC statistics to the host controller so as to allow the host to receive information relating to packet arrivals errors etc.

The term network message is used herein to refer to application layer messages that represent process to process communications carried over a network within a stream of data packets. Examples of network messages would therefore include FIX messages carrying financial information and HTTP IMAP and SSH messages.

A particular advantage of the present invention is that the arrangement of controller and accelerator taught herein allows all the external ports of the controller to be network facing which potentially permits NICs to be provided having larger port counts without modification of the controller hardware.

The applicant hereby discloses in isolation each individual feature described herein and any combination of two or more such features to the extent that such features or combinations are capable of being carried out based on the present specification as a whole in the light of the common general knowledge of a person skilled in the art irrespective of whether such features or combinations of features solve any problems disclosed herein and without limitation to the scope of the claims. The applicant indicates that aspects of the present invention may consist of any such individual feature or combination of features. In view of the foregoing description it will be evident to a person skilled in the art that various modifications may be made within the scope of the invention.

