---

title: Graphics processing unit employing a standard processing unit and a method of constructing a graphics processing unit
abstract: Employing a general processing unit as a programmable function unit of a graphics pipeline and a method of manufacturing a graphics processing unit are disclosed. In one embodiment, the graphics pipeline includes: (1) accelerators, (2) an input output interface coupled to each of the accelerators and (3) a general processing unit coupled to the input output interface and configured as a programmable function unit of the graphics pipeline, the general processing unit configured to issue vector instructions via the input output interface to vector data paths for the programmable function unit.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09123128&OS=09123128&RS=09123128
owner: Nvidia Corporation
number: 09123128
owner_city: Santa Clara
owner_country: US
publication_date: 20121221
---
This application is directed in general to graphics processing units GPUs and more specifically to components of a GPU.

In traditional GPUs fixed function units are statically connected together to form a fixed function graphics pipeline. The output packets of each of the fixed function units or fixed function stages are designed to match the input packets of the downstream fixed function unit. A more flexible approach is to define the graphics pipeline in software as a program or programs running on a programmable processor. In such a pipeline the functional stages are implemented in software with data being moved via a regular general purpose memory system.

In one aspect a graphics pipeline is disclosed. In one embodiment the graphics pipeline includes 1 accelerators 2 an input output interface coupled to each of the accelerators and 3 a general processing unit coupled to the input output interface and configured as a programmable function unit of the graphics pipeline the general processing unit configured to issue vector instructions via the input output interface to vector data paths for the programmable function unit.

In another aspect an apparatus is disclosed. In one embodiment the apparatus includes 1 a scalar processing core programmed to emulate a function unit of a graphics pipeline and 2 a memory directly coupled to the scalar processing core and including a graphics register configured to store input and output operands for accelerators of the graphics pipeline.

In yet one other aspect a method of manufacturing a graphics processing unit is disclosed. In one embodiment the method includes 1 coupling a fixed function unit to an input output interface 2 coupling a general processing unit to the input output interface 3 programming the general processing unit to emulate a function unit of a graphics pipeline and 4 coupling vector data paths to the input output interface wherein the vector data paths are associated with the general processing unit function unit.

In pipelines with software defined functional units a custom processor is typically used to implement the programmable function stages and is built to process parallel code. Thus the custom processor requires the necessary modifications and associated expense to achieve a processor specifically configured to process parallel code. While well suited for highly parallel code the special purpose processor is ill suited for scalar code.

As such disclosed herein is a programmable graphics pipeline that employs a general processing unit to implement function stages of the pipeline. The programmable graphics pipeline advantageously treats vector data paths of a function stage as accelerators and attaches the vector data paths to an input output interface that provides programmable non static connections between stages of the graphics pipeline. The general processing unit is a standard processing unit having a scalar core that is suited for processing scalar code. For example the general processing unit can be a standard CPU such as an ARM A9 A15. As disclosed herein the general processing unit also includes a processor memory.

An accelerator is a hardware implemented unit that is used to accelerate processing in the graphics pipeline. An example of an accelerator is a fixed function rasterizer or a texture unit. Thus the disclosure provides a graphics pipeline architecture having a general processing unit that issues vector instructions into an input output interface as regular graphics requests. A request is a work request to be performed by one of the accelerators. The vector instructions therefore can be issued via a control bus connecting the general processing unit to the input output interface. In one embodiment the input output interface is an input output connector as disclosed in patent application Ser. No. 13 722 901 by Albert Meixner entitled AN INPUT OUTPUT CONNECTOR FOR ACCESSING GRAPHICS FIXED FUNCTION UNITS IN A SOFTWARE DEFINED PIPELINE AND A METHOD OF OPERATING A PIPELINE filed on the same day as this disclosure and incorporated herein be reference.

The general processing unit includes a processor memory incorporated therein having a graphics register configured to include input and output operands for the vector data paths and the other accelerators of the graphics pipeline.

In one embodiment the processor memory is an L2 cache. An L2 or level 2 cache is part of a multi level storage architecture of the general processing unit. In some embodiments the processor memory is built into the motherboard of the general processing unit. The processor memory can be a random access memory RAM or another type of volatile memory. The processor memory is used to minimize the latency penalty on the general processing unit for sharing a data path connecting the general processing unit to the input output interface while still providing large amounts of bandwidth between the processing core of the processing unit and the vector paths.

The size of the processor L2 cache allows for the graphics register included therein to have a large number of active vector registers. In some embodiments L2 caches are sized at about 256 KB to about 2 MB in size. In these embodiments about half of the caches are used for registers so 64K to 512K registers. In one embodiment a minimum of at least registers per fast memory access FMA unit in the vector data paths and . As such the disclosed programmable graphics pipeline enables a large number of graphics threads to tolerate latency.

The general processing unit does not need to be massively multithreaded. In one embodiment the general processing unit is designed to support one hardware thread context per cooperative thread array CTA 4 8 total in order to run per CTA pipeline emulation code without requiring software context switches.

In one embodiment data stored on the processor memory e.g. input and output operands is accessed in blocks by the input output interface to maximize bandwidth at a low cost. In one embodiment the block size is in a range of 64 bytes to 256 bytes.

In one embodiment disclosed herein a scratch RAM for the processing unit of the programmable graphics pipeline is also implemented as an accelerator. As such a multi bank memory is provided as an additional accelerator to support a scratch RAM for the processing unit which requires highly divergent accesses. In some embodiments a texture stage of a graphics pipeline can minimize cost by using the scratch RAM accelerator for its data banks which also require highly divergent accesses.

The graphics pipeline architecture provided herein beneficially allows a lower entry barrier into building a GPU. Because a general processing unit is employed details involved in building a custom processor are avoided. Additionally the created accelerators are mostly stateless and easier to design and verify than a full custom processor.

Considering the programmable side most of the pipeline emulation code would run on a standard scalar core which lowers the amount of infrastructure such as compilers debuggers and profilers needed and makes it easier to achieve acceptable performance than on a custom scalar core.

Additionally the disclosed architecture removes or least reduces barriers between a general processing unit and GPU. Switching between scalar and data parallel sections can be done in a few cycles since the general processing unit has direct access to all vector registers that are stored in the processor memory. As an additional benefit the general processing unit would have access to all accelerators coupled to the input output interface and could use them directly in otherwise scalar code.

Before describing various embodiments of the novel programmable function unit and methods associated therewith a computing system within which the general processing unit may be employed in a GPU will be described.

As shown the system data bus connects the CPU the input devices the system memory and the graphics processing subsystem . In alternate embodiments the system memory may connect directly to the CPU . The CPU receives user input from the input devices executes programming instructions stored in the system memory operates on data stored in the system memory and sends instructions and or data i.e. work or tasks to complete to a graphics processing unit to complete. The system memory typically includes dynamic random access memory DRAM used to store programming instructions and data for processing by the CPU and the graphics processing subsystem . The graphics processing subsystem receives the transmitted work from the CPU and processes the work employing a graphics processing unit GPU thereof. In this embodiment the GPU completes the work in order to render and display graphics images on the display devices . In other embodiments the GPU or the graphics processing subsystem as a whole can be used for non graphics processing.

As also shown the system memory includes an application program an application programming interface API and a graphics processing unit GPU driver . The application program generates calls to the API in order to produce a desired set of results typically in the form of a sequence of graphics images.

The graphics processing subsystem includes the GPU an on chip GPU memory an on chip GPU data bus a GPU local memory and a GPU data bus . The GPU is configured to communicate with the on chip GPU memory via the on chip GPU data bus and with the GPU local memory via the GPU data bus . The GPU may receive instructions transmitted by the CPU process the instructions in order to render graphics data and images and store these images in the GPU local memory . Subsequently the GPU may display certain graphics images stored in the GPU local memory on the display devices .

The GPU includes accelerators an IO interface and a general processing unit . The accelerators are hardware implemented processing blocks or units that accelerate the processing of the general processing unit . The accelerators include a conventional fixed function unit or units having circuitry configured to perform a dedicated function. The accelerators also include a vector data path or paths and a multi bank memory that are associated with the general processing unit .

The IO interface is configured to couple the general processing unit to each of the accelerators and provide the necessary conversions between software and hardware formats to allow communication of requests and responses between the general processing unit and the accelerators . The IO interface provides a programmable pipeline with dynamically defined connections between stages instead of static connections. The IO interface can communicate various states between the general processing unit and the accelerators . Inputs and outputs are also translated between a software friendly format of the general processing unit and a hardware friendly format of the accelerators. Furthermore the IO interface translates states between a software friendly format and a hardware friendly format. In one embodiment the IO interface advantageously provides all of the above noted functions in a single processing block that is shared across all of the accelerators and the general processing unit . As such the IO interface advantageously provides a single interface that includes the necessary logic to dynamically connect the general processing unit to the accelerators to form a graphics pipeline and manage instruction level communications therebetween. The IO connector includes multiple components that provide bi directional connections including bi directional control connections to the scalar processor core of the general processing unit and bi directional data connections to the processor memory of the general processing unit .

The general processing unit is a standard processor having a scalar core that is suited for processing scalar code. The general processing unit also includes a processor memory that is directly connected to the scalar processor core. By being directly connected the processor memory is implemented as part of the general processing unit such the scalar processor core can communicate therewith without employing the IO interface . Thus turning briefly to both the processor memory and the CPU are part of the general processing unit and the CPU does not go through the IO interface to access the processor memory . In one embodiment the CPU may be employed as the general processing unit for the GPU and communicate with the accelerators via the system data bus and the IO interface . As such the CPU can be used with the general processing unit or used instead of the general processing unit .

Instead of coexisting as illustrated in some embodiments the general processing unit replaces the CPU entirely. In this configuration there s no distinction between the GPU local memory and the system memory . It is replaced with one shared memory. For example the CPU and system memory can be removed and Application Program API and GPU driver reside in the GPU local memory and are executed by the General Processing Unit . More detail of an embodiment of a graphic pipeline according to the principles of the disclosure is discussed below with respect to .

The GPU may be provided with any amount of on chip GPU memory and GPU local memory including none and may use on chip GPU memory GPU local memory and system memory in any combination for memory operations.

The on chip GPU memory is configured to include GPU programming code and on chip buffers . The GPU programming may be transmitted from the GPU driver to the on chip GPU memory via the system data bus .

The GPU local memory typically includes less expensive off chip dynamic random access memory DRAM and is also used to store data and programming used by the GPU . As shown the GPU local memory includes a frame buffer . The frame buffer stores data for at least one two dimensional surface that may be used to drive the display devices . Furthermore the frame buffer may include more than one two dimensional surface so that the GPU can render to one two dimensional surface while a second two dimensional surface is used to drive the display devices .

The display devices are one or more output devices capable of emitting a visual image corresponding to an input data signal. For example a display device may be built using a cathode ray tube CRT monitor a liquid crystal display or any other suitable display system. The input data signals to the display devices are typically generated by scanning out the contents of one or more frames of image data that is stored in the frame buffer .

Having described a computing system within which the disclosed programmable graphics pipeline and methods may be embodied or carried out a particular embodiment of a programmable graphics pipeline will be described in the environment of a GPU .

The general processing unit is a standard processing unit having a processing core and a processor memory . The processor core is a scalar core that is configured for processing scalar code. The processor memory is incorporated as part of the general processing unit . The processor memory is configured to store data to be processed by the processing core or data to be sent to the accelerators for processing. In one embodiment the processor memory is an L2 cache. In different embodiments the processor memory is random access memory RAM or a scratchpad.

The processor memory includes a graphics register file . The graphics register file is configured to include accelerator data including input and output operands for the accelerators . The graphics register file is configured to include a file of active vectors of the accelerators . An active vector is a vector or vector data presently being processed in the graphics pipeline .

The general processing unit is connected to the IO interface via two bi directional connections. As illustrated in the processing core is connected to the IO interface via a bi directional control connection and the processor memory is connected to the IO interface via a bi directional data connection . In one embodiment the bi directional control connection and the bi directional data connection are conventional connectors that are sized positioned and terminated to at least provide the communications associated therewith that are described herein.

The IO interface is a single interface that connects the general processing unit to the accelerators to form a graphics pipeline. The IO interface includes multiple components that are configured to provide non permanent connections between the general processing unit and the accelerators and provide the proper conversions to allow communication and processing of requests and responses therebetween. In one embodiment the IO interface provides dynamic connections between the general processing unit and the accelerators for each request generated by the general processing unit . A request is an instruction or command to perform an action or work on data. A request is generated by the processing core for one of the accelerators to perform. A response is generated or provided by one of the particular accelerators as a result of the request. Associated with each request are parameters and state information that the accelerators use to process the requests. In some embodiments the IO interface is the IO interface of .

The accelerators are hardware implemented units that are configured to accelerate processing of the graphics pipeline . The accelerators include three fixed function units and . The fixed function units are fixed function units that are typically found in a GPU. Each of the fixed function units is configured with the necessary circuitry dedicated to perform a particular function of a graphics pipeline. In one embodiment the fixed function units and are a geometry assist stage a surface assist stage and a texture stage respectively. One skilled in the art will understand that other types of fixed function units can be employed as accelerators.

The accelerators also include a multi bank memory a vector data path A denoted as and a vector data path B denoted as . In contrast to a programmable function unit implemented on special purpose processor configured to operate highly parallel code the general processing unit does not include vector data paths or a scratch RAM. Instead the functionality of these components has been placed external to a programmable processor and placed in a fixed format as accelerators. As such the processing core issues vector instructions as regular graphics request to the particular vector data path vector data path A or vector data path B . The vector instructions are communicated via the bi directional control connection to the IO interface . The IO interface translates the software format vector instruction into a hardware format for the appropriate vector data path and communicates the vector instruction thereto via an accelerator connection that is specifically dedicated to a single one of the accelerators . The dedicated accelerator connections are generally denoted in as elements and specifically denoted as elements and .

In two of the accelerators fixed function unit and multi bank memory are directly coupled together via a unit connector and a unit connector . The unit connectors provide access between two different ones of the accelerators . For the programmable graphics pipeline the unit connectors provide communication paths to transmit and receive data with the multi bank memory . The unit connectors therefore can be used to reduce or minimize the cost of graphics pipeline by allowing the multi bank memory to be shared by the fixed function unit . For example as noted above the fixed function unit can be a texture stage that typically requires a memory allowing highly divergent accesses. With the unit connectors the texture stage can use the multi bank memory for its texture cache data banks. In one embodiment the multi bank memory can be a scratch RAM configured for highly divergent access. In one embodiment the unit connectors and the dedicated accelerator connections are conventional connectors that are sized positioned and terminated to at least provide the communications associated therewith that are described herein.

In a step a fixed function unit is coupled to an input output interface. The input output interface can be the interface or the IO interface . In one embodiments multiple fixed function units are coupled to the input output interface. Each of the fixed function units can be conventional fixed function units.

A general processing unit is coupled to the input output interface in a step . In a step the general processing unit is programmed to emulate a function unit of a graphics pipeline. The general processing unit includes a processing core and a processor memory. In one embodiment the processing core is a scalar core and the processor memory is an L2 cache. Bi directional control and data connections can be used to couple the input output interface to the general processing unit.

The processor memory is configured as a graphics register for the graphics pipeline in a step . In one embodiment only a portion of the processor memory is configured as a graphics register.

In a step vector data paths are coupled to the input output interface. The vector data paths are hardware implemented accelerators associated with the general processing unit function unit. Moving the vector processing out from the processor core of a programmable processing unit allows an off shelf CPU to be used for a programmable function unit. As such a streaming multiprocessor can be replaced with a CPU. The method then ends in a step .

The disclosed architecture provides many benefits including a lower entry barrier into building a GPU. Since the architecture can employ an existing CPU many of the complications and details involved in building a custom processor can be avoided. The accelerators are mostly stateless and easier to design and verify than a full processor. On the software side most of the pipeline emulation code can run on a standard scalar core which lowers the amount of infrastructure compilers debuggers profilers needed and makes it easier to achieve acceptable performance than on a custom scalar core.

On the more technical side the proposed architecture can remove or at least reduce barriers between the CPU and GPU. Switching between scalar and data parallel sections would be done in a few cycles since the CPU has direct access to all vector registers. As a side benefit the CPU would have access to all accelerators and could use them directly in otherwise scalar code.

Due to the loose coupling of components in the disclosed architecture the design is highly modular. Some modifications to the CPU part to add the control bus and the memory system to add the data bus to L2 but the same graphics components could be connected to different CPU implementations to hit different performance ratios. The disclosed architecture provides an unmodified or mostly unmodified core with loosely coupled vector and fixed function accelerators. In some embodiments modifications include the addition of a control bus to send commands to the accelerators and receive responses. This functionality can be implemented using an existing memory. Nevertheless a dedicated control bus can increase performance. Modifications can also include pinning the registers of the cache in place and keep them from being evicted.

While the method disclosed herein has been described and shown with reference to particular steps performed in a particular order it will be understood that these steps may be combined subdivided or reordered to form an equivalent method without departing from the teachings of the present disclosure. Accordingly unless specifically indicated herein the order or the grouping of the steps is not a limitation of the present disclosure.

A portion of the above described apparatuses systems or methods may be embodied in or performed by various such as conventional digital data processors or computers wherein the computers are programmed or store executable programs of sequences of software instructions to perform one or more of the steps of the methods. The software instructions of such programs may represent algorithms and be encoded in machine executable form on non transitory digital data storage media e.g. magnetic or optical disks random access memory RAM magnetic hard disks flash memories and or read only memory ROM to enable various types of digital data processors or computers to perform one multiple or all of the steps of one or more of the above described methods or functions of the apparatuses described herein. As discussed with respect to disclosed embodiments a general processing unit having a scalar core that is suited for processing scalar code is employed.

Portions of disclosed embodiments may relate to computer storage products with a non transitory computer readable medium that have program code thereon for performing various computer implemented operations that embody a part of an apparatus system or carry out the steps of a method set forth herein. Non transitory used herein refers to all computer readable media except for transitory propagating signals. Examples of non transitory computer readable media include but are not limited to magnetic media such as hard disks floppy disks and magnetic tape optical media such as CD ROM disks magneto optical media such as floptical disks and hardware devices that are specially configured to store and execute program code such as ROM and RAM devices. Examples of program code include both machine code such as produced by a compiler and files containing higher level code that may be executed by the computer using an interpreter.

Those skilled in the art to which this application relates will appreciate that other and further additions deletions substitutions and modifications may be made to the described embodiments.

