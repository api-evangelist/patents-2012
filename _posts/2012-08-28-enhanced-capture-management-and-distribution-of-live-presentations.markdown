---

title: Enhanced capture, management and distribution of live presentations
abstract: Techniques are provided for converting live presentations into electronic media and managing captured media assets for distribution. An exemplary system includes capture devices that capture media assets of live presentations comprising a session, including image data of sequentially presented visual aids accompanying the live presentations and audio data. Each capture device has an interface for real-time image data marking of the image data for identification of individual images and session marking of the image data for demarcation of individual presentations of the session. A centralized device processes the captured media assets and automatically divides the captured media assets into discrete files associated with the individual presentations based on the session markings. An administrative tool manages the processed media assets to produce modified presentations and enables modification of the visual aid images identified by the image data markings. A production device formats the modified presentations for distribution on distribution media.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08918708&OS=08918708&RS=08918708
owner: Astute Technology, LLC
number: 08918708
owner_city: Reston
owner_country: US
publication_date: 20120828
---
This application is a continuation of U.S. application Ser. No. 12 749 215 filed Mar. 29 2012 which is a continuation of U.S. application Ser. No. 11 580 092 filed Oct. 13 2006 which is a continuation in part of U.S. application Ser. No. 09 955 939 filed Sep. 20 2001 which is a continuation in part of U.S. Pat. No. 6 789 228 issued Sep. 7 2004 all of which are herein incorporated by reference in their entireties. This application also claims the benefit of U.S. Provisional Application 60 726 175 filed Oct. 14 2005 which is herein incorporated by reference in its entirety.

The present invention generally relates to a data processing system for digitally recording and reproducing lectures presentations in physical and electronic format. More particularly the present invention relates to the capture management and distribution of live presentations.

The majority of corporate and educational institution training occurs in the traditional lecture format in which a speaker addresses an audience to disseminate information i.e. a live presentation . Due to difficulties in scheduling and geographic diversity of speakers and intended audiences a variety of techniques for recording the content of these lectures have been developed. These techniques include videotapes audio tapes transcription to written formats and other means of converting lectures to analog non computer based formats and converting lectures to appropriate digital formats for use over the Internet.

A challenge arises with respect to capture and distribution of live presentations at conferences and meetings during which a large number of lectures might be delivered over the course of one or several days thereby making it difficult for a conference attendee to attend each of the lectures. Because such conferences have limited shelf life speed to market of the conference content is a critical element of success. Also important is the ability to accurately capture presentation content for distribution such that the captured content is precise in its presentation and has the necessary speaker permissions i.e. does not contain information containing copyrighted materials without having the necessary permissions associated with them . Additionally as greater quantities of presentation content are captured over relatively short periods of time and as rapid release of the content becomes increasingly important effective management of the captured presentations is needed.

Systems and methods are described herein that can be employed for rapid conversion of live presentations into electronic media and for effective management of captured media assets for distribution.

An exemplary system for capturing and distributing presentations includes at least two capture devices configured to capture media assets of live presentations comprising a session the media assets including 1 image data of a plurality of sequentially presented visual aids accompanying the live presentations and 2 audio data. At least two of the visual aids are selected from the group of images consisting of slides photographs graphs discrete motion picture clips and text. Each capture device includes an interface that enables real time 1 image data marking of the image data for identification of individual images and 2 session marking of the image data for demarcation of individual presentations of the session. A centralized device is configured to process the captured media assets from each capture device. The centralized device is configured to automatically divide the captured media assets for the session into discrete files associated with the individual presentations based on the session markings. An administrative tool is configured to manage the processed captured media assets to produce modified presentations. The administrative tool enables modification of the visual aid images identified by the image data markings. A production device is configured to format the modified presentations of at least one session for distribution on distribution media.

Another exemplary system for capturing and distributing presentations includes means for capturing media assets of live presentations comprising a session the media assets including 1 image data of a plurality of sequentially presented visual aids accompanying the live presentations and 2 audio data. At least two of the visual aids are selected from the group of images consisting of slides photographs graphs discrete motion picture clips and text. The system includes means for real time image data marking of the image data for identification of individual images and means for real time session marking of the image data for demarcation of individual presentations of the session. Means for processing the captured media assets are configured to automatically divide the captured media assets for the session into discrete files associated with the individual presentations based on the session markings. Means for managing the processed captured media assets to produce modified presentations are configured to modify the visual aid images identified by the image data markings. The system also includes means for formatting the modified presentations of at least one session for distribution on distribution media.

An exemplary method for capturing and distributing presentations includes capturing media assets of live presentations comprising a session the media assets including 1 image data of a plurality of sequentially presented visual aids accompanying the live presentations and 2 audio data. At least two of the visual aids are selected from the group of images consisting of slides photographs graphs discrete motion picture clips and text. The capturing includes real time 1 image data marking of the image data for identification of individual images and 2 session marking of the image data for demarcation of individual presentations of the session. The method also includes processing the captured media assets for the session. The processing includes automatically dividing the captured media assets for the session into discrete files associated with the individual presentations based on the session markings. The method further includes managing the processed captured media assets to produce modified presentations. The managing includes modifying the visual aid images identified by the image data markings. Additionally the method includes formatting the modified presentations of at least one session for distribution on distribution media.

These and other features of the present disclosure will be readily appreciated by one of ordinary skill in the art from the following detailed description of various implementations when taken in connection with the accompanying drawings.

Systems consistent with the present disclosure digitally capture lecture presentation slides and speech and store the data in a memory. They also prepare this information for Internet publication and publish it on the Internet for distribution to end users. These systems comprise three main functions 1 capturing the lecture and storing it into a computer memory or database 2 generating a transcript from the lecture and the presentation slides and automatically summarizing and outlining transcripts and 3 publishing the lecture slides image data audio data and transcripts on the Internet for use by Internet end users.

In one implementation slides can be generated using conventional slide projectors. In this case when a presenter begins presenting and a first slide is displayed on the projection screen by a projector a mirror assembly can change the angle of the light being projected on the screen for a brief period of time to divert it to a digital camera. At this point the digital camera captures the slide image transfers the digital video image data to the computer and the digital video image data can be stored on the computer. The mirror assembly then quickly flips back into its original position to allow the light to be projected on the projection screen as the presenter speaks. When this occurs an internal timer on the computer begins counting. This timer marks the times of the slide changes during the lecture presentation. Simultaneously the system begins recording the sound of the presentation when the first slide is presented. The digital images of the slides and the digital audio recordings are stored on the computer along with the time stamp information created by the timer on the computer to synchronize the slides and audio.

Upon each subsequent slide change the mirror assembly quickly diverts the projected light to the digital camera to capture the slide image in a digital form and then it flips back into its original position to allow the slide to be displayed on the projection screen. The time of the slide changes marked by the timer on the computer is recorded in a file on the computer. At the end of the presentation the audio recording stops and the computer memory stores digital images of each slide during the presentation and a digital audio file of the lecture speech. Additionally the computer memory also stores a file denoting the time of each slide change.

Alternatively in another implementation slides can be generated without using conventional slide projectors. For example a computer generated slide presentation can be used thereby avoiding the need of the mirror assembly and the digital camera. In this case data from application software such as PowerPoint available from Microsoft Corporation of Redmond Wash. or from any other application software a presenter is using to generate a presentation on his or her computer can be captured. The digital video image data of a presentation slide from the presenter s computer can be transferred to the capture system s computer at the same time that the slide is projected onto the projection screen. Similarly slides may be projected from a machine using overhead transparencies or paper documents. This implementation also avoids the need for the mirror assembly and the digital camera because like the computer generated presentations the image data is transferred directly to the capture system s computer for storage at the same time that the image data is projected onto the projection screen. Any of these methods or other methods may be used to capture digital video image data of the presentation slides in the capture system s computer. Once stored in the computer the digital video and audio files may be published to the Internet or optionally enhanced for more efficient searching on the Internet.

During optional lecture enhancement optical character recognition software can be applied to each slide image to obtain a text transcript of the words on a slide image. Additionally voice recognition software can be applied to the digital audio file to obtain a transcript of the lecture speech. To enhance recognition accuracy each presenter may read a standardized text passage either in a linear or interactive fashion in which the system re prompts the presenter to re state passages that are not recognized in order to enhance recognition accuracy into the system prior to presenting and in doing so provide the speech recognition system with additional data to increase recognition accuracy. Speech recognition systems which provide for interactive training and make use of standardized passages which the presenter reads into the system to increase accuracy are available from a variety of companies including Microsoft IBM and others. Once transcripts are obtained automatic summarization and outlining software can be applied to the transcripts to create indexes and outlines that are easily searchable by an end user. In addition to the enhanced files the end user can also search the whole transcript of the lecture speech.

Alternatively if Closed Captioning is used during a presentation the Closed Caption data can be parsed from the input to the device and a time stamp can be associated with the captions. Parsing of the Closed Caption data can occur either through the use of hardware e.g. with Closed Caption decoder chips such as those offered by Philips Electronics or software such as that offered by Ccaption ccaption.com on the World Wide Web . The Closed Caption data can be used to provide indexing information for use in search and retrieval for all or parts of individual or groups of lectures.

In addition information and data which are used during the course of presentation s can be stored in the system to allow for additional search and retrieval capabilities. The data contained and associated with files used in a presentation can be stored and this data can be used in part or in whole to provide supplemental information for search and retrieval. Presentation materials often contain multiple media types including text graphics video and animations. With extraction of these materials they can be placed in a database to allow additional search and retrieval access to the content. Alternatively the data can be automatically indexed using products which provide this functionality such as Microsoft Index Server or Microsoft Portal Server.

Finally after transferring the files to a database systems consistent with the present disclosure can publish these slide image files audio files and transcript files to the Internet for use by Internet end users. These files can be presented so that an Internet client can efficiently search and view the lecture presentation.

Systems consistent with the present disclosure thus allow a lecture presentation to be recorded and efficiently transferred to the Internet as active or real time streaming files for use by end users. The present disclosure therefore describes systems that are not only efficient at publishing lectures on the Web but can be efficient at recording the content of meetings whether business medical judicial or another type of meeting. At the end of a meeting for instance a record of the meeting complete with recorded slides audio and perhaps video can be stored. The stored contents can be placed on a removable media such as compact discs digital versatile discs flash memory magnetic memory or any type of recordable media to be carried away by one or more of the meeting participants.

Further the present disclosure can be implemented as an effective teleconferencing mechanism. Specifically so long as a participant in a teleconference has a capture device in accordance with the present disclosure his or her presentation can be transmitted to other participants using the recorded presentation which has been converted to a suitable Internet format. The other teleconference participants can use similar devices to capture enhance and transmit their presentations or simply have an Internet enabled computer Internet enabled television wireless device with Internet access or like devices.

These and further aspects of the systems and methods will be described in the following sections. The explanation will be by way of exemplary embodiments to which the present invention is not limited.

As the presenter changes slides or transparencies the computer can automatically record the changes. Changes can be detected either by an infrared IR slide controller and IR sensor a wired slide controller not shown or an algorithm driven scheme implemented in the computer which detects changes in the displayed image.

As shown in when a slide change is detected either via the slide controller or an automated algorithm the mirror of the mirror assembly is moved into the path of the projection beam at a 45 degree angle. A solenoid which is an electromagnetic device often used as a switch can control the action of the mirror . This action directs all of the light away from the projection screen and towards the CCD . The image is brought into focus on the CCD digitally encoded and transmitted to the computer via the video capture board shown in described below . At this point the mirror flips back to the original position allowing the light for the new slide to be directed towards the projection screen . This entire process can takes less than one second since the image capture is a rapid process. Further this rapid process is not easily detectable by the audience since there is already a pause on the order of a second between conventional slide changes. In addition the exact time of the slide changes as marked by a timer in the computer can be recorded in a file on the computer .

The computer can also include or be connected to an infrared receiver to receive a slide change signal from the slide change controller . The CPU can also have a timer for marking slide change times and the secondary storage device can contain a database for storing and organizing the lecture data. The system can also allow for the use of alternative slide change data which is provided as either an automated or end user selectable feature obtained from any combination or singular use of 1 a computer keyboard which can be plugged into the system 2 software running on a presenter s presentation computer which can send data to the capture device or 3 an internally generated timing event within the capture device which triggers image capture. For example image capture of the slide s can be timed to occur at predetermined or selectable periods. In this way animation video inserts or other dynamic images in computer generated slide presentations can be captured at least as stop action sequences. Alternatively or additionally the slide capture can be switched to a video or animation capture during display of dynamically changing images such as that which occurs with animation or video inserts in computer generated slides. Thus the presentation can be fully captured including capture of dynamically changing images with a potential increase in file size.

Referring back to the computer can include an integrated LCD display panel and a slide out keyboard which can be used to switch among three modes of operation discussed below. For file storage and transfer to other computers the computer can also include a floppy drive and a high capacity removable media drive such as a Jaz drive available from Iomega of Roy Utah iomega.com on the World Wide Web among other devices. The computer may also be equipped with multiple CPUs thus enabling the performance of several tasks simultaneously such as capturing a lecture and serving a previously captured lecture over the Internet.

Simultaneously with the slide capturing audio signals can be recorded using a microphone connected by a cable to the audio capture card which is an analog to digital converter in the computer and the resulting audio files can be placed into the computer s secondary storage device in this exemplary embodiment.

In one implementation consistent with the present disclosure the presentation slides are computer generated. In the case of a computer generated presentation the image signal from the computer not shown generating the presentation slides is sent to a VGA to NTSC conversion device and then to the video capture board before it is projected onto the projection screen thus eliminating the need to divert the beam or use the mirror assembly or the CCD . This also results in a higher quality captured image.

In one implementation optical character recognition can be performed on the captured slide data using a product such as EasyReader Elite from Mimetics of Cedex France. Also voice recognition can be performed on the lecture audio using a product such as Naturally Speaking available from Dragon Systems of Newton Mass. The optical and voice recognition processes can be used to generate text documents containing full transcripts of both slide content and audio of an actual lecture. In another implementation these transcripts can be processed by outline generating software such as LinguistX from InXight of Palo Alto Calif. which can summarize the lecture transcripts improve content searches and provide indexing. Other documents and information can then be linked to the lecture e.g. an abstract author name date time and location based on the content determination. The information contained in the materials or the native files themselves used during the presentation can also be stored into the database to enhance search and retrieval through any combination or singular use of 1 the data in a native format which is stored within a database 2 components of the information stored in the database and 3 pointers to the data which are stored in the database.

Most of these documents except e.g. those stored in their native format along with the slide image information are converted to Web ready formats. This audio slide and synchronization data can be stored in the database e.g. Microsoft SQL which is linked to each of the media elements. The linkage of the database and other media elements can be accomplished with an object linking model such as Microsoft s Component Object Model COM . The information stored in the database can be made available to Internet end users through the use of a product such as Microsoft Internet Information Server IIS software among others and can be configured to be fully searchable.

Methods and systems consistent with the present disclosure thus enable the presenter to give a presentation and have the content of the lecture made available on the Internet with little intervention. While performing the audio and video capture the computer can automatically detect slide changes i.e. via the infrared slide device or an automatic sensing algorithm and the slide change information can be encoded with the audio and video data. In addition the Web based lecture can contain data not available at the time of the presentation such as transcripts of both the slides and the narration and an outline of the entire presentation. The presentation can be organized using both time coding and the database and can be searched and viewed using a standard Java enabled Web interface such as Netscape Navigator . Java is a platform independent object oriented language created by Sun Microsystems . The Java programming language is further described in The Java Language Specification by James Gosling Bill Joy and Guy Steele Addison Wesley 1996 which is herein incorporated by reference. In one implementation the computer can serve the lecture information directly to the Internet if a network connection is established using the Ethernet card or modem not shown . Custom software written in Java for example can be used to integrate all of the needed functions for the computer.

Generally three modes of operation will be discussed consistent with the present disclosure. These modes include 1 lecture capture mode 2 lecture enhancement mode and 3 Web publishing mode.

At the beginning of a lecture a presenter prepares the media of choice step . If using 35 mm slides the slide carousel is loaded into the tray on the top of the projector . If using a computer generated presentation the presenter connects the slide generating computer to the SVGA input port shown in the I O ports of a projection unit . If using overhead transparencies or paper documents the presenter connects the output of a multi media projector such as the Toshiba MediaStar described above and shown in to the SVGA input port . A microphone is connected to the audio input port and an Ethernet networking cable is attached between the computer and a network outlet in the lecture room. For ease of the discussion to follow any of the above projected media will be referred to as slides. 

At this point the presenter places the system into lecture capture mode step . In one implementation a keyboard or switch not shown can be used to set the lecture capture mode. When this mode is set the computer can create a directory or folder on the secondary storage device with a unique name to hold source files for this particular lecture. The initiation of the lecture capture mode can also reset the timer and slide counter to zero step . In one implementation three directories or folders can be created to hold the slides audio and time stamp information. Initiation of lecture capture mode can also cause an immediate capture of a first slide using the mirror assembly step for instance. The mirror assembly flips to divert the light path from the projector to the CCD of the digital camera. Upon capturing the first slide the digital image can be stored in an image format such as a JPEG format graphics file a Web standard graphics format in a slides directory on the secondary storage device of the computer e.g. slides slide01.jpg . After the capturing of the image by the CCD the mirror assembly flips back to allow the light path to project onto the projection screen . The first slide is then projected to the projection screen and the internal timer on the computer begins counting step .

Next the audio of the lecture can be recorded through the microphone and the audio signal can be passed to the audio capture card installed in the computer step . The audio capture card converts the analog signal into a digital signal that can be stored as a file on the computer . When the lecture is completed this audio file can be converted into a streaming media format such as Active Streaming Format or RealAudio format for efficient Internet publishing. In one implementation the audio signal can be encoded into the Active Streaming Format or RealAudio format in real time as it arrives and placed in a file in a directory on the secondary storage device . Although this implementation might require additional hardware e.g. an upgraded audio card it avoids conversion of the original audio file into Internet formats after the lecture is complete. Regardless the original audio file i.e. unencoded for streaming can be retained as a backup on the secondary storage device .

When the presenter changes a slide step using the slide control or by changing the transparency or document the computer can increment the slide counter by one and record the exact time of this change in an ASCII file a computer platform and application independent text format referred to as a time stamp file written on the secondary storage device step . This file can have for example two columns one denoting the slide number and the other denoting the slide change time. In one implementation the file is stored in the time stamp folder.

Using the mirror assembly a new slide can be captured into a JPEG format graphics file e.g. slide .jpg where is the slide number that can be stored in the slides folder on the secondary storage device . When the new slide is captured the mirror assembly quickly diverts the light from the slide image back to the projection screen step . If any additional slides are presented these slides can be handled in the same manner step and for each additional slide the system can record the slide change time and capture the new slide in the JPEG graphics file format.

At the completion of the lecture the presenter or someone else can stop the lecture capture mode with the keyboard . This action stops the timer and completes the lecture capturing process.

Initially optical character recognition OCR can be performed on the content of the slides step . OCR converts the text on the digital images captured by the CCD digital camera into fully searchable and editable text documents. The performance of the optical character recognition may be implemented by OCR software on the computer . In one implementation these text documents can be stored as a standard ASCII file. Through the use of the time stamp file this file can be chronologically associated with slide image data. Further Closed Caption data if present can be read from an input video stream and used to augment the indexing search and retrieval of the lecture materials. A software based approach to interpreting Closed Caption data is available from Leap Frog Productions San Jose Calif. on the World Wide Web. In addition data from native presentation materials can further augment the capability of the system to search and retrieve information from the lectures. Metadata including the presenter s name affiliation time of the presentation and other logistical information can also be used to augment the display search and retrieval of the lecture materials. This metadata can be formatted in XML Extensible Markup Language and can further enhance the system through compliance with emerging distance learning standards such as Shareable Courseware Object Reference Model Initiative SCORM . Documentation regarding distance learning standards can be found at among other websites elearningforum.com on the World Wide Web.

Similarly voice recognition can be performed on the audio file to create a transcript of the lecture speech and the transcript can be stored as an ASCII file along with time stamp information step . The system can also provide a system administrator the capability to edit the digital audio files so as to remove caps or improve the quality of the audio using products such as WaveConvertPro Waves Ltd. Knoxville Tenn. .

Content categorization and outlining of the lecture transcripts can be performed by the computer using a software package such as LinguistX from InXight of Palo Alto Calif. step . The resulting information can be stored as an ASCII file along with time stamp information.

Consistent with the present disclosure the system can obtain a temporary IP Internet Protocol address from a local server on the network node to which the system is connected step . The IP address may be displayed on the LCD panel display .

When a user accesses this IP address from a remote Web browser the system the server can transmit a Java applet to the Web browser the client via the HTTP protocol a standard Internet method used for transmitting Web pages and Java applets step . The transmitted Java applet provides a platform independent front end interface on the client side. The front end interface is described below in detail. Generally this interface can enable the client to view all of the lecture content including the slides audio transcripts and outlines. This information can be fully searchable and indexed by topic such as a traditional table of contents by word such as a traditional index in the back of a book and by time stamp information denoting when slide changes occurred .

The lecture data source files stored on the secondary storage device can be immediately served to the Internet as described above. In addition in one implementation the source files may optionally be transferred to external Web servers. These source files can be transferred via FTP File Transfer Protocol again using standard TCP IP networking to any other computer connected to the Internet. The source files can then be served as traditional HTTP Web pages or served using the Java applet structure discussed above thus allowing flexibility of use of the multimedia content.

The end user of a system consistent with the present disclosure can navigate rapidly through the lecture information using a Java applet front end interface. This platform independent interface can be accessed from traditional PCs with a Java enabled Web browser such as Netscape Navigator and Microsoft Internet Explorer as well as Java enabled Network Computers NCs .

Before the source files generated in the lecture capturing process can be published in a manner that facilitates intelligent searching indexes to the source files should be stored in a database. The database can maintain links between all source files and searchable information such as keywords author names keywords in transcripts and other information related to the lectures.

Two methods for organizing a database that contains multiple types of media text graphics and audio include object oriented and relational. An object oriented database links together the different media elements and each object contains methods that allow that particular object to interact with a front end interface. Any type of media can be placed into the object oriented database as long as methods of how this media is to be indexed sorted and searched are incorporated into the object description of the media.

The second method involving a relational database provides links directly to the media files instead of placing them into objects. These links determine which media elements are related to each other i.e. they are responsible for synchronizing the related audio and slide data .

The end user or client can interact with the MPU within the application tier. In addition information entering the database from the lecture capture mode of the system can enter at the application tier level as well. This information can then be processed within the MPU passed through the middleware and populate the database .

There are many different methods of implementing a system that performs functions consistent with the present disclosure. Several embodiments are described below.

For capturing computer generated presentations the mirror assembly is not used and the video signal and mouse actions from the user s slide generating computer pass through the capture computer before going to the LCD projector. This configuration enables the capture computer to record the slides and change times.

With the use of a Kodak Ektapro Slide Projector Kodak Rochester N.Y. which can either be incorporated into device or used as a stand alone slide projector another method of communicating the status of the slide projector to the computer can use the P Com protocol Kodak Rochester N.Y. . The P Com protocol is communicated between the slide projector and the computer over an RS 232 interface that is built into the Ektapro projector. The information obtained from the projector can provide the computer with the data signaling that a slide change has occurred whereupon the computer can then digitally capture the slide. This approach alleviates the need for detecting signals from the infrared controller and IR sensor or the wired slide controller.

Although the front end interface described above is Java based if the various modes of operation are separated other front end interfaces can be employed. For example if lecture capture is handled by a separate device its output is the source files. In this case these source files can be transferred to a separate computer and served to the Internet as a website including standard HTML files for example.

In another implementation the front end interface can also be a consumer level box which includes a speaker a small LCD screen several buttons used to start and stop the lecture information a processor used to stream the information and a network or telephone connection. This box can approach the size and utility of a telephone answering machine but can provide lecture content instead of just an audio message. In this implementation the lecture content can be streamed to such a device through either a standard telephone line via a built in modem for example or through a network such as a cable modem or ISDN . Nortel Santa Clara Calif. provides a Java phone which can be used for this purpose.

The system described in the Main Processing Unit and the Application Programming Interface can be programmed using a language other than Java e.g. C C and or Visual Basic Languages.

Another implementation of the present disclosure can replace the mirror assembly with a beam splitter not shown . This beam splitter allows for slide capture at any time without interruption but reduces the intensity of the light that reaches both the digital camera and the projection screen . If a beam splitter is used redundancies can be implemented in the slide capturing stage by capturing the displayed slide or transparency for example every 10 seconds regardless of the slide change information. This approach can help overcome any errors in an automated slide change detection algorithm and allow for transparencies that have been moved or otherwise adjusted to be recaptured. At the end of the lecture the presenter can select from several captures of the same slide or transparencies and decide which one should be kept.

In one implementation consistent with the present disclosure the user can connect a keyboard and a mouse along with an external monitor to the SVGA out port . This connection can allow the user access to the internal computer for software upgrades maintenance and other low level computer functions. Note that the output of the computer can be directed to either the LCD projection device or the LCD panel .

In one implementation consistent with the present disclosure the network connection between the computer and the Internet can be made using wireless technology. For example a 900 MHZ connection similar to that used by high quality cordless phones can connect the computer to a standard Ethernet wall outlet. Wireless LANs can also be used. Another implementation can use wireless cellular modems for the Internet connection.

In another implementation an electronic pointer can be added to the system. Laser pointers are traditionally used by presenters to highlight portions of their presentation as they speak. The movement of these pointers can be tracked and this information can be recorded and time stamped. This approach can enable the end user to search a presentation based on the movement of the pointer and have the audio and video portion of the lecture synchronized with the pointer.

Spatial positional pointers can also be used in the lecture capture process. These trackers can allow the system to record the presenter s pointer movements in either 2 dimensional or 3 dimensional space. Devices such as the Ascension Technology Corporation pcBIRD or 6DOF Mouse Burlington Vt. INSIDETRAK HP by Polhemus Incorporated Colchester Vt. or the Intersense IS 300 Tracker from Intersense Cambridge Mass. can be used to provide the necessary tracking capability for the system among others. These devices send coordinate x y z data through an RS 232 or PCI interface which communicates with the CPU and this data is time stamped by the timer .

In one embodiment consistent with the present disclosure the system can be separated into several physical units one for each mode or a subset combination of modes i.e. lecture capture enhancement and publishing . A first physical unit can include the projection device and computer that contains all of the necessary hardware to perform the lecture capturing process. This hardware can include the mirror assembly the CCD digital camera if this embodiment is used a computer with video and audio capturing ability an infrared sensing unit and networking ability. In this implementation the function of the first physical unit is to capture the lecture and create the source files on the secondary storage of the unit. This capture device contains the projection optics and can display one or more of 35 mm slides a computer generated presentation overhead transparencies and paper documents.

In this implementation the lecture enhancement activities can be performed in a second separate physical enclosure. This separate device contains a computer with networking ability that can perform the OCR voice recognition and auto summarization of the source files generated in the lecture capturing process.

Finally a third physical enclosure can provide Web publishing function and contain a computer with network ability a database structure and Internet serving software. The second and third functions can be combined in one physical unit the first and third functions can be combined in one physical unit or the first and second functions can be combined in one physical unit as circumstances dictate.

In this modular design several categories of products can be implemented. For example one implementation can provide lecture capturing ability only and require only the lecture capturing devices. This system would be responsible for the creation and serving of the generated source files. Another implementation can provide lecture capturing and Web serving and only require the lecture capturing devices and the Web publishing devices. Yet another implementation can add the lecture enhancement device to the above configurations and also provide the lecture transcripts and summaries to the Web. In addition to the modularization of the different tasks as described above modularization with respect to physical components different products with distributed task functions can be achieved. For instance several lecture capture units can be networked or otherwise connected to a centralized enhancement and publishing or just publishing unit.

The modular approach can facilitate additional embodiments where the presentation is developed at least regarding the slides as a computer generated presentation using available software such as PowerPoint etc. In these embodiments a chip set such as that made available from PixelWorks and other companies can be employed to enable auto detection of the video signal and also to provide digitization of the signal in a means which is appropriate to the resolution and aspect ratio and signal type video vs. data . The CPU and the digitization circuitry can be provided on a single chip along with a real time operating system and Web browser capability or on separate chips. Four embodiments with varying degrees of modularity and functionality are described below. Furthermore PixelWorks offers chip sets which provide a system on a chip by incorporating a Toshiba general purpose microprocessor an ArTile TX79 on the same chip as the video processing circuits pixelworks.com press on the World Wide Web . Leveraging the general purpose microprocessor embodiments containing this or similar devices can be configured to perform the following functions 

The first of these embodiments shown in includes a standard image e.g. slide and or video projector with an intermediary unit placed between the projector and the source of the projected images e.g. a general purpose computer . The intermediate unit can complete the media processing and contain either a USB port to communicate with the computer and possibly an analog modem and Ethernet to communicate directly with a server . The projector associated with this embodiment can be any commercial or proprietary unit that is capable of receiving VGA SVGA XGA or SXGA and or a DVI input for instance. The input to the video projector is received via cable from the intermediate unit from an associated output port . The intermediate unit receives its input at interface via cable from the general purpose computer or other computer used for generating the presentation. The intermediate unit can also contain an omni directional microphone and audio line input to be used concurrently or separately as desired by the user. The intermediate unit can function to capture the presentation through the computer generated slides encode time stamp information and capture the audio portion of the presentation. The captured data can then be stored in removable media or transferred via USB or other type of port from the intermediate units output by cable to the computer . This aspect can eliminate the need for storage in the intermediate unit and can use more reliable flash memory. The computer or other type of computer can receive the processed media from the intermediate unit and transfer the data via cable to the Web server through its connection to the net. Optionally the intermediate unit can connect directly to the media server via cable as described earlier.

The media server running standard media server software such as Apple Quicktime RealNetworks RealSystem Server or Microsoft Media Server can stream the data with a high bandwidth connection to the Internet. This process can occur both as a simulcast of the lecture as well as in an archive mode with transfer occurring after the event has transpired. Such arrangement with the computer can eliminate the need for an Ethernet card and modem built into the intermediate unit since most general purpose computers already have this functionality.

In splitting the image signals sent from the personal computer at step the present embodiment can facilitate two different methods. In the first method using an image signal splitter e.g. a Bayview 50 DIGI see on the World Wide Web baytek.de englisch BayView50.htm the image signal is split into a digital 24 bit RGB red green blue for media processing and an analog RGB image signal sent to the projector . However if the projector is capable of receiving digital RGB image signals then an image signal splitter such as a Bayview AD1 can be used which produces two digital outputs one for processing and one for projection.

While an objective is to employ a standard non customized computer to permit a presenter to use his own laptop for instance it is possible that the functions of the intermediate unit can be incorporated in the general purpose computer through software firmware and hardware upgrades.

In a second embodiment such as shown in for use with computer generated presentations an image projector can contain a digital output and formatting for output via USB or Firewire IEEE 1394 . A general purpose personal computer or other type of computer used for generating the presentation can supply the computer generated presentation to the projector through an input port via cable on the projector that has the capability of receiving VGA SVGA XGA or SXGA and or a DVI input for instance. Through the USB or Firewire IEEE 1394 interface interface via cable the projector can communicate with an intermediate unit at interface which can capture the computer generated presentation as well as the audio portion of the presentation through an omni directional microphone and or audio input. The output from the intermediary unit is in a raw media format and supplied to the general purpose computer via USB or Firewire interface and cable where the raw media can be processed using custom software for media conversion and processing or custom hardware software in the laptop computer. The media can be processed into HTML and or streaming format via the software hardware and supplied to the media server via cable which in turn can stream the media with high bandwidth to the Internet . This system utilizes the capabilities of the computer used in generating the presentation to process the media with the addition of software or some custom hardware. The intermediate unit can also have a removable storage media and presentation capture controls capable of adjusting certain parameters associated with the lecture capture. However the intermediate unit can be connected directly to the server .

In a third embodiment for use with computer generated presentations shown in the projector contains digital output and formatting for output via USB or Firewire and further contains the media processor which can process the media into HTML and or streaming format or other Internet language. The projector can communicate with a media server through an Ethernet interface via cable from which the media can be streamed to a connection to the Internet . Again this system can produce a simulcast of the lecture as well as store the lecture in an archive mode. This embodiment as with the previous embodiments can allow the use of removal media in the projector . The projector can also contain a control panel for controlling various parameters associated with capturing the presentation. Optionally the control panel can be created in software and displayed as a video overlay on top of the projected image. This overlay technique is currently used on most video and or data projectors to adjust contrast brightness and other projector parameters. The software control panel can thus be toggled on and off and controlled by pressing buttons on the projector or through the use of a remote control which communicates with the projector using infrared or radio frequency data exchange.

A fourth embodiment associated with computer generated presentations as seen in includes a projector that contains all the hardware necessary to capture and serve the electronic content of the live presentation through a connection to the network through Ethernet or fiber connection. As such the projector can capture the video content through its connection via interface and cable to a personal computer or other type of computer. The projector can capture the audio content via omni directional microphone or audio line input. The projector can also process the media into HTML and or streaming format and further act as a server connecting directly to the Internet . The projector can also contain a control panel which controls various parameters associated with capturing the presentation as well as removable media when it is desired to store the presentation in such a manner.

Various inputs associated with a standard projector can be incorporated into the integrated projector including but not limited to digital video image and or VGA. Outputs allowing the integrated projector to function with a standard projector thus expanding its versatility can also include a digital video image output for highest quality digital signal to the projector. VGA output can also be integrated into the integrated projector. USB connectors as well as Ethernet and modem connectors an audio input and omni directional microphone can also be included in the integrated projector . As the integrated projector is capable of many different functions using different sources input selection switches can also be included on the integrated projector as well as other features common in projectors such as remote control and a variety of interfaces associated with peripheral elements.

The previous four embodiments employ similar processes for the capture of the presentation. In general the presenter or someone else connects the personal computer e.g. laptop to the integrated projector or the in line of the intermediate unit. The system is configured through available switches depending on the source to capture characteristics unique to the source of the presentation. The audio is captured and converted to digital through an A and D converter along with the images if the digital output from the projector is not available. The image signal is split and the image is displayed and then compressed into a standard file format e.g. JPEG MPEG . The synchronization of audio and images can occur during the digitization and formatting processes. The media processing can include compression of images via a variety of methods including color palette optimization imagery sizing and image and audio compression as well as indexing. Compression for use of the data in an Internet stream format can also occur during processing. During media processing other data can also be entered into the system such as speaker s name title of the presentation copyright information and other pertinent information as desired. The information captured can then be transferred to the server allowing it to be streamed to clients connected to a network Internet or Intranet. As discussed in the above embodiments the media can be served directly from one of the intermediate units or projectors or it can be transferred to an external server which exists as part of an Internet or is directly connected to the Internet. When the data is made available immediately over an IP connection in either a uni directional or bi directional manner the device can be used for real time teleconferencing. As such these embodiments are compatible with other methods and systems for capturing a live presentation as discussed earlier and as such can include other applicable features presented in this disclosure as appropriate. More or less modularization of the system can be employed in response to varying needs and varying user assets.

Another embodiment involves the use of digital media which contain microprocessors and independent operating systems. One representative device the Mine from Teraoptix mineterapin.com terrapin on the World Wide Web contains the Linux operating system digital storage 12 gigabytes of storage and Ethernet USB and IEEE 1394 connectivity. This device also allows for Internet connectivity for file uploads and downloads. Coupling this device with the different embodiments can allow for a solution which provides or replicates the digital audio recording functionality as well as provides image storage through connection of the projector which may be equipped with a USB Ethernet or IEEE 1394 output .

The laptop or presentation computer in parallel with running the presentation can capture the presentation. The following components can be employed to affect lecture capture in a software based solution and are each described below in further detail 

The software involved in the capture process is referred to herein as the capture application CA . The CA can run on the presentation system or on the server or can partially run on both . The software can be written in standard personal computer programming languages such as C C JAVA or other software languages.

With each of the above time stamp generation approaches the presentation computer can initiate capture either locally on the presentation machine itself and or on the server.

Several approaches can be employed for ii visual media processing. For example the following approaches for image capture on the presentation computer can be implemented in singular or in combination.

Several approaches can be employed for iii audio capture and processing including the use of audio capture technology available on many computers in either hardware that exists on the motherboard or that is provided with the addition of a digital audio acquisition card from suppliers such as Creative Labs. Optionally a microphone which converts the audio signal into a digital format such as USB available from HelloDirect hellodirect.com on the World Wide Web can be connected to the PC to enable audio capture. Audio capture software can capture the audio into memory hard drive removable storage or transmit directly to a server through the use of TCP IP protocols or direct connection through standard data cables such as USB or IEEE 1394 cabling. After capture the audio can either be stored in a variety of standard audio formats e.g. MP 3 MP 2 AIFF WAVE etc. or directly into a streaming format such as QuickTime or RealNetworks streaming formats among others.

A device such as the Mine from Teraoptix Mine can be used to augment digital audio capture and or Internet connectivity. For example software written in C Java or other programming languages which is stored and executed on the Mine device can record the digital audio on the Mine device while communicating with the presentation personal computer. This communication can involve a standardized time generation which is used to generate the time stamps during the presentation. As a result this system can segment the audio recording and time stamping functionality to the Mine device and the image capture occurring on the system being used for the presentation.

Several approaches can be employed for iv addition of search methodologies to on line presentations. For example enhanced search capabilities can be created through the use of speech recognition as well as optical character recognition abstraction of text and other data and their use in a searchable database as described above . Metadata can also be used for indexing and search and retrieval.

Several approaches can be employed for v placement of materials on the Web and use of emerging distance learning standards. For example integration of the media and its presentation on the Web can be enabled by transmitting the captured audio visuals and time stamp information along with other available data including speech recognition format closed caption data obtained as described above. The additional search methodologies as well as support of distance learning standards described above can be applied to this embodiment. This data can be placed on a server and made available to end users over a network e.g. Intranet Internet or Wireless Internet network . Alternatively the presentation can be placed on a removable media such as a CD ROM or DVD for distribution.

A detailed description of an exemplary hardware software platform enabling rapid conversion of live presentations into electronic media and effective management of captured media assets for distribution follows. The electronic media can be transmitted over the Internet using a variety of audiovisual technologies. These technologies can offer a range of media formats from full motion video to visual only or audio only formats as described above. Exemplary streaming video formats include Macromedia Flash Microsoft Media 9 or 10 or O1 formats and Apple QuickTime among others. Exemplary audio formats include MP 3 and podcasts among others. An exemplary slide and audio format includes Macromedia Flash . Text based formats include PDF and other text graphic formats. In addition the transmission and storage of these media materials can range from electronic formats to actual physical formats e.g. DVD and CD or monographs .

Effective management of the captured presentations is needed to handle greater quantities of content captured over relatively short periods of time and to enable rapid release of the content for distribution. For example effective presentation management techniques can be implemented to appropriately determine when a presenter begins and ends a presentation to assign metadata associated with the presentation to the captured digital media audio visuals for the presentation to create enhanced navigation tools e.g. thumbnails and additional metadata which can be time based to provide editorial tools for replacement of copyrighted or other undesired information in a presentation and to enhance the presentation with additional information e.g. a standard title at the beginning of each presentation and or a title specifying a sponsor of a lecture as well as when the question and answer portion of a presentation began etc. .

In the embodiment of the hardware software capture platform includes a capture device a web server a database a centralized device a testing and certification tool and a reporting server . Hardware software capture platform as well as other embodiments of the present disclosure can support distribution of live presentations on different media for use on different platforms as well as support on line testing and certification.

A large number of presentations are typically delivered over the course of several days at a session such as a conference or meeting making it difficult for an attendee to be present at each of the live presentations. Thus recognizing the need to simultaneously record multiple live presentations of a session digital capturing devices such as those disclosed in U.S. Pat. No. 6 789 228 can be used to capture presentation content with simultaneous audio and video. Video of the data portion of a presentation including image data of sequentially presented visual aids can be input to a capture device such as the ENCORE capture device from Astute Technology using VGA or DVI. In some embodiments the resolution of the video images can be preserved by using a frame rate of capture of approximately 12 15 frames per second. The capture device can be configured to capture dynamic data embedded in a presentation such as movies animations transitions etc. as well as electronic pointer position data.

For example the hardware software capture platform shown in the embodiment of includes an ENCORE capture device which can be configured to capture media assets of live presentations of a session. The media assets can include image data of sequentially presented visual aids accompanying the live presentations as well as audio data. Exemplary visual aids include images of slides photographs graphs discrete motion picture clips and text accompanying the live presentations.

The capture device can also be configured to enable switchable sources for simultaneous capture of NTSC video e.g. talking head as well as the selection of other VGA DVI or video sources into the captured media file. For example in one embodiment the image data for a presentation can include the visual aid images the presenter displayed during the presentation as well as video images of the presenter during the presentation. In addition real time transition effects can be included as well as title information and lower third information such as the name of a presenter shown in the alpha channel during the capture and transmission processes. The ability to select and switch content sources can further be enabled through custom code extensions using for example Microsoft DirectShow technology for capturing and encoding the media files. In one implementation the media assets can be simulcast during archival capture.

Furthermore the capture device can be configured to simultaneously capture high resolution image data which can be used to augment the presentation. These high resolution images can be used for OCR as well as to enable close up views of content.

Each capture device can include an interface that enables real time image data marking of the image data for identification of individual images and session marking of the image data for demarcation of individual presentations of the session. For instance depicts an exemplary user interface for a capture application that can be executed on a capture device in accordance with an embodiment of the present disclosure. As shown in the embodiment of the capture application includes a capture control window and a Windows Media Encoder WME window . Using various fields in the capture control window a user can select an appropriate session or unidentified session if the session is not known and corresponding presentation lecture information as well as a capture operator and a capture station which can be a preset field . Next the user can initiate capture of a presentation by selecting a Start Recording button . Note that at this point the WME window does not need to be used to change preset capture parameters.

Once capture of a presentation is initiated the user can set markers at any point during the capture process to mark a particular event. For example marker buttons can be used to mark in the beginning and mark out the end of the presentation as well as to set random markers. Notes can be associated with the markers e.g. the user can type in a note after setting a marker which can be helpful in the subsequent editing phase. At the end of the presentation the user can select the Stop Recording button to stop the capture process. Note that the WME window can typically take a few seconds to stop as well. The user can then close the capture application.

A Media Editor preview window can be used to play back an entire recording for a captured session and adjust the markers for the individual presentations of the session. For example when a marker in the Markers window is selected a video progress slider can automatically advance to the proper position highlighting the beginning and the end of the selected segment i.e. presentation . The markers can be adjusted as needed and the start and end points of the segment can be reset if necessary. Additionally one second and ten second buttons can help the user navigate through the video of the selected segment.

When final markers are set the Confirm Lecture button can be selected to confirm the selected segment as an individual presentation lecture thereby associating a specific presenter speaker from the Lectures window with the confirmed segment. Once confirmed the status of the lecture presentation can be changed from Marked M to Markers Confirmed MC by selecting the Change Status Add Notes button . Optionally the editor application can be configured to display a warning if the user attempts to overwrite existing markers for a specific speaker lecture. Additionally an Auto advance checkbox can be selected to automatically advance to a next lecture segment after having confirmed a selected lecture segment. This editing process can be repeated until every lecture presentation in the session is confirmed.

In an embodiment the capture technology described herein can be employed for behavior monitoring of end users. For example a view of a user as they are interfacing with a computer can be captured. In this way developers can analyze the user s expressions and interactions with the computer interface to implement ergonomic modifications to the computer interface.

Because it can be cumbersome and time consuming to edit a captured media file e.g. to divide the media file into portions to incorporate only a specific speaker s presentation or to edit a presentation a high quality rapid technique for producing digital archives libraries of content is provided. A centralized device can be configured to process the captured media assets from each capture device. In one embodiment the centralized device can automatically divide the captured media assets for a session into discrete files associated with the individual presentations of the session based on the session markings which can be set and confirmed using the exemplary capture and editor applications shown in . For example the hardware software platform shown in the embodiment of includes a centralized device which can be configured to process the captured media assets from each capture device .

The centralized device can be used to rapidly specify when a given speaker is presenting during a presentation i.e. the start and stop points in time as well as to associate metadata e.g. the speaker s name title of the presentation etc. and other supplemental materials with the presentation. This information can then be associated with the recorded media files. After the edit in and out points of a particular lecture presentation are specified the centralized device can be used to split the captured audio visual files into discrete media files which are associated with particular lectures. In addition a scene slide detection technique can be employed to determine changes in the sequence of a presentation to create unique thumbnail images of the presentation thereby enhancing navigation.

After a session of presentations is captured and marked the centralized device can be employed for post processing. The post processing can include creation of thumbnail images for enhanced navigation. For example the scene slide detection technique can enable a user to save individual images from captured lecture files every time a slide change occurs thereby creating thumbnail images with appropriate time stamp information. The thumbnail images can also be used in conjunction with a player module for navigation of captured video. During navigation the location of the videos the output path and the media server URL for both ASX and RPM files can be specified.

Presentation lecture files can be created from captured session files based on in and out markers that have been confirmed using the editor application and by creating thumbnails via the scene slide detection technique. Additionally identified slides i.e. video segments can be replaced with placeholder slides that can be selected using an administrative tool in accordance with an embodiment of the present disclosure.

The centralized device can run unattended and process captured media assets which may include session files presentation lecture files or the like as the files are received or pass through production workflow. The centralized device can synchronize with a database via a Web server and can run concurrently on several computers for faster processing and turnaround time at a session venue.

The centralized device can primarily exchange data with individual capture stations to retrieve the captured media assets and to split out individual presentation files from the captured sessions. For example as shown in once the import of captured media assets is initiated the server application can search for and display identified recorded sessions in the Process Folder window . Successful imports can be listed in both the Completed Folder and the Import Log .

By selecting an Autosave XML checkbox an up to date event data file e.g. conferencedata.xml can be created at a certain interval which can be used by both the capture and editor applications. Automatic splitting of imported media assets can be initiated by selecting a Start Splitting button . The server application can then monitor for presentation segments for which markers have been confirmed i.e. the markers can be set during capture using the exemplary capture application shown in and confirmed using the exemplary editor application shown in . When the markers for a presentation are confirmed the server application can split out that particular presentation file from the media assets captured for the entire session. After a being split out the presentation s status can be changed e.g. to Ready for Quality Control RFQ .

Optionally the scene slide detection technique can be used in conjunction with OCR software to create time stamped files containing terms which can enhance search and retrieval of the captured media assets. Additionally OCR can be applied to the visual aid images displayed during the presentation and speech recognition can be applied to the audio data recorded during the presentation to enable transcription of the words spoken when the visual aid images were presented. In this way the entire presentation can be searched. For example a user can search transcripts of the visual aid images and the presenter s spoken words to locate particular topics of interest.

The scene slide detection technique can also be used to create thumbnail images which can be used to provide users with visual cues of lecture content thereby enabling rapid search and retrieval of captured presentations lectures.

The administrative tool can be configured to manage the processed captured media assets to produce modified presentations. For example it might be desirable to remove particular captured presentation content e.g. content which might be published in journals or content for which the speaker might not have copyright permission to reproduce in volumes that are to be sold as part of an electronic collection created using the techniques described herein . In one embodiment the administrative tool can be configured to enable modification of the visual aid images that are identified by image data markings set during capture using the capture application.

When a slide is selected for replacement or deletion a rapid stitching technique for editing captured presentation files can be used to change video content without the need for re rendering the video. Stitching can be used to eliminate the typical burden of video editing and re rendering and re compiling the edited files into a new digital media file. For example the administrative tool can be used to detect key frames in a video stream and enable an end user to select a video sequence that can be substituted for a scene to be deleted. Using low level C code or other low level code the administrative tool can stitch the video files together at edit points thereby eliminating the need for re rendering the video and reducing the time required to edit a captured video stream to seconds typical video editing applications can require up to a 1 1 time ratio between duration of a video and amount of rendering time required to process and produce a final edited product . The stitching technique can also preserve audio associated with the video sequence.

The administrative tool can also be used to divide captured sessions conferences into logical segments and to publish the segments for various media types which can include physical and or on line distribution media. In an embodiment the distribution media can be configured to incorporate e commerce functionality such as pay per view sponsorship based payment subscription models and institutional sales among others.

When physical media replication is desired a production tool can be used to enable rapid creation of digital media which can be transferred and reviewed on line prior to replication on or off site. Using the aforementioned techniques and tools replication can be achieved rapidly e.g. within minutes following a presentation.

In one embodiment the production tool can be configured to format the modified presentations of at least one session conference for distribution on distribution media. For example the production tool can be used to divide the conference into different logical volumes or tracks. The production tool can also be used to specify desired publication media e.g. on line DVD CD etc. as well as include navigation functions and other media associated with particular presentations such as abstracts articles or the like which can have various formats including PDF format.

Additionally sponsorship information can be created in the form of logos animations audio files or the like and incorporated into the published media files and or the media player format e.g. banner ads and or media labeling. Sponsorship information can also be customized on a per volume basis or per other logical divisions of content such as the production of discs for individuals or groups of particular individuals . Optionally the production tool can be used to enable custom distribution media creation by end users. For example end users can browse a library of captured content to select desired content for distribution and can also select a desired media type s .

In one implementation the production tool can include file system information such that the distribution media types are compatible with different operating systems. For example software running on Apple Inc. computers such as Toast roxio.com en products mac products.jhtml on the World Wide Web can be used to create hybrid distribution media that includes unique contents for Mac or PC users. Likewise software such running on computers equipped with the Windows operating system such as TransMac asy.com scrtm.htm on the World Wide Web can be also be used to produce such hybrid distribution media.

After the production tool generates an image of the captured media assets the image can be replicated for large scale distribution. Replication can be accomplished using a variety of devices including robotic replication machines such as the Rimage rimage.com home.html on the World Wide Web which can print labels on physical distribution media as well as replicate the content. Additionally replication can be accomplished using pre printed blank physical distribution media which can be replicated using systems such as the NEC based replication tower cddvdking.com on the World Wide Web .

A testing and certification tool such as the Net.SCORE application available from Astute Technology can be configured to combine on line testing with on line lectures for integration of learning objectives e commerce testing and on line certification. For example the hardware software capture platform shown in the embodiment of includes a Net.SCORE web based testing and certification tool .

The testing and certification tool can be implemented to enable on line transcript generation as well as maintenance of a log of live event attendance credits versus on line or enduring i.e. hardcopy material credits. The maintenance of such a log can help credentialing entities comply with regulations by preventing an award of double credit for the review and completion of a course that is available in both on line and live formats. The testing and certification tool can also be implemented to enable a presenter to identify weaknesses of attendees prior to presenting a lecture. In this way the presenter can customize the lecture prior to presenting it to specifically address the identified weaknesses if desired.

In step media assets of live presentations comprising a session are captured. The media assets can include image data of sequentially presented visual aids accompanying the live presentations and audio data. Exemplary visual aids include slides photographs graphs discrete motion picture clips and text. The capturing includes real time image data marking of the image data for identification of individual images and session marking of the image data for demarcation of individual presentations of the session.

For example a capture application such as the capture application illustrated in can be executed on a capture device to enable a user to mark the image data for identification of individual images e.g. by setting random markers and for demarcation of individual presentations of the session e.g. by setting in and out markers . Optionally the administrative tool illustrated in can also be used to mark the captured image data for identification of individual images e.g. by selecting the Replace With or Delete radio buttons as shown in . Step can also include synchronizing the captured media assets for each presentation by generating time stamp data.

Optionally method includes the step of editing the captured media assets including modifying the image data and or session markings for a presentation and confirming the markings for each presentation. For example in one implementation an editor application such as the editor application illustrated in can be executed on the capture device or on the centralized device to enable a user to modify and confirm the markings set for each presentation.

In step the captured media assets for the session are processed. The processing includes automatically dividing the captured media assets for the session into discrete files associated with the individual presentations based on the session markings. For example a server application such as the server application illustrated in can be executed on the centralized device for rapid enhancement and post processing of captured content received from the capture and or editor applications. Additionally to enable text searching of the presentations for the session step can include applying optical character recognition to the image data of the visual aid images and or applying speech recognition to the audio data to enable transcription of the spoken words during the presentation. In one implementation step can include analyzing the image data to automatically generate thumbnail images for each presentation that enable navigation of the presentations. In this case the analyzing can include detecting changes in sequences of visual aid images to mark the image data.

In step the processed captured media assets are managed to produce modified presentations. The managing includes modifying the visual aid images identified by the image data markings. For example an administrative tool such as the web based administrative tool illustrated in can be configured for managing captured presentations lectures as well as for specifying particular lectures for review quality control and editing prior to production. In one implementation step can include deleting the visual aid images identified by the image data markings to produce the modified presentations. In another implementation step can include replacing the visual aid images identified by the image data markings with replacement image data to produce the modified presentations. In this case the replacing can include stitching together the image data and the replacement image data.

In step the modified presentations of at least one session are formatted for distribution on distribution media. For example a production tool such as the production tool illustrated in can be configured for publishing captured presentations lectures. In one implementation step can include formatting the modified presentations for at least one session for on line media distribution. In another implementation step can include formatting the modified presentations for at least one session for physical media distribution. In this case the formatting can include dividing the formatted presentations for the at least one session into logical units when the formatted presentations span multiple physical media.

Additionally step can include generating mechanisms for navigating the formatted presentations on the distribution media. Step can also include enabling end user selection of desired formatted presentations for distribution on the distribution media. Step can further include incorporating customized sponsorship information into the formatted presentations for distribution on the distribution media as well as incorporation of additional media associated with the formatted presentations e.g. abstracts articles or the like for distribution on the distribution media.

Optionally the method can include the additional step of tracking credits awarded to an attendee for completing a course by attending a live presentation to prevent awarding additional credit to the attendee for completing the same course on line or for completing the same course using hardcopy materials. In another embodiment the method can include the additional step of identifying weaknesses of attendees prior to delivering a presentation to customize the presentation in accordance with the identified weaknesses. For example a testing and certification tool such as the testing and certification tool illustrated in can be configured to integrate on line testing and certification.

Techniques consistent with the present disclosure provide among other features rapid conversion of live presentations into electronic media and effective management of captured media assets for distribution. The foregoing description of an implementation of the invention has been presented for purposes of illustration and description. It is not exhaustive and does not limit the disclosure to the precise form disclosed. Modifications and variations are possible in light of the above teachings or may be acquired from practicing of the disclosure. The scope of the invention is defined by the claims and their equivalents.

