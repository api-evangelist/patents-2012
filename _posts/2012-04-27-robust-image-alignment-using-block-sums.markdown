---

title: Robust image alignment using block sums
abstract: A computing device may select a source tile from a source image. From the source tile, the computing device may select a first rectangular feature and a second rectangular feature. Based on the first and second rectangular features, the computing device may calculate a source feature vector. The computing device may also select a search area of a target image, and a target tile within the within the search area. Based on the target tile, the computing device may calculate a target feature vector. The computing device may determine that a difference between the source feature vector and the target feature vector is below an error threshold, and based on this determination, further determine a mapping between the source image and the target image. The computing device may then apply the mapping to the source image to produce a transformed source image.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08411962&OS=08411962&RS=08411962
owner: Google Inc.
number: 08411962
owner_city: Mountain View
owner_country: US
publication_date: 20120427
---
The present application claims priority to U.S. patent application Ser. No. 13 305 389 filed on Nov. 28 2011 and herein incorporated by reference in its entirety.

A large number of applications for manipulating the output of still and video cameras involve the alignment of two or more images. For instance by manipulating multiple low quality exposures of a still camera photo a higher quality photo can be formed. However existing image alignment techniques tend to lack either computational efficiency or robustness.

In an example embodiment a computing device may select a source tile from a source image. The source tile may be represented by a source m n rectangle that contains m n pixels and each of the m n pixels may be represented by one or more pixel values. The computing device may select a first rectangular feature and a second rectangular feature in the source tile. Both the first and second rectangular features may be smaller than the source m n rectangle in at least one dimension and at least one of the first and second rectangular features may be 2 2 pixels or larger.

The computing device may calculate a source feature vector where the source feature vector comprises i a first entry containing a first pixel value sum of the first rectangular feature in the source tile and ii a second entry containing a second pixel value sum of the second rectangular feature in the source tile. The computing device may also select a search area of a target image. The search area may be represented by an m n rectangle that contains m n pixels and each of the m n pixels may be represented by one or more pixel values. The m n rectangle may be larger than the source m n rectangle in at least one dimension. Additionally the computing device may select a target tile within the m n rectangle where the target tile contains m n pixels.

The computing device may also select a first rectangular feature and a second rectangular feature in the target tile. The first and second rectangular features in the target tile may be based on the first and second rectangular features in the source tile.

The computing device may further calculate a target feature vector. The target feature vector may comprise i a first entry containing a first pixel value sum of the first rectangular feature in the target tile and ii a second entry containing a second pixel value sum of the second rectangular feature in the target tile. The computing device may determine that a difference between the source feature vector and the target feature vector is below an error threshold. Based on the difference between the source feature vector and the target feature vector being below the error threshold the computing device may determine a mapping between the source image and the target image. The computing device may apply the mapping to the source image to produce an aligned source image or the computing device may apply the mapping to the target image to produce an aligned target image.

In another example embodiment a server device may receive a source feature vector. In response to receiving the source feature vector the server device may select a search area from a target image. The search area may be represented by an m n rectangle that contains m n pixels where each of the m n pixels is represented by one or more pixel values.

The server device may select a target tile from the search area. The target tile may be represented by a target m n rectangle that contains m n pixels and where the m n rectangle is larger than the target m n rectangle in at least one dimension. The server device may select a first rectangular feature and a second rectangular feature in the target tile where both the first and second rectangular features may be smaller than the target m n rectangle in at least one dimension and at least one of the first and second rectangular features may be 2 2 pixels or larger.

The server device may calculate a target feature vector where the target feature vector comprises i a first entry containing a first pixel value sum of the first rectangular feature in the target tile and ii a second entry containing a second pixel value sum of the second rectangular feature in the target tile. The server device may determine that a difference between the source feature vector and the target feature vector is below an error threshold. Based on the difference between the source feature vector and the target feature vector being below the error threshold the server device may transmit to the client device an indication of the target image.

These as well as other aspects advantages and alternatives will become apparent to those of ordinary skill in the art by reading the following detailed description with reference where appropriate to the accompanying drawings. Further it should be understood that the description provided in this summary section and elsewhere in this document is intended to illustrate the claimed subject matter by way of example and not by way of limitation.

Generally speaking image alignment may involve computational methods for arranging two or more images from a still or video camera over one another so that they match. Once the images are aligned various functions can be performed such as image enhancement e.g. noise reduction image stabilization e.g. to compensate for the shaking of a video camera pattern matching e.g. identification of common elements in two or more images and object recognition e.g. finding a specific object in two or more images as well as other functions.

As an example of image enhancement two digital photographs may be taken from slightly different angles of say a teacup. However the process of taking the capturing and digitizing these images may introduce noise e.g. Gaussian noise to the resulting digital photographs. In order to produce a final image of the teacup with at least some of the noise removed a composite image can be formed by processing the digital photographs so that the respective representations of the teacup in each are combined with one another. In order to perform this compositing of the respective representations of the teacup it may be advantageous to first align these representations with one another.

As an example of image stabilization a hand held video camera may be used to record a sequence of images of say a soccer ball in flight. However if the user of the hand held camera has shaky hands the resulting video sequence may exhibit frame to frame jitter. In order to reduce this jitter the location of soccer ball within two or more of these images can be matched. Then video sequence can be adjusted to compensate for the jitter and make the soccer ball appear to move in a smooth arc. However the embodiments herein can also be applied to static scenes in which there is little or no movement.

Further these embodiments can be combined with other forms of image alignment. For example some cameras include gyroscopes and encode gyroscope data along with images the data representing the relative position of the camera when each respective image was taken. A coarse level of image alignment can be performed by adjusting the positions of these images according to their respective gyroscope data then the embodiments herein can be applied to perform a finer level of image alignment.

Given the various uses of image alignment it is advantageous to be able to perform image alignment in an efficient and robust fashion. In practice more than a simple global alignment i.e. translating the whole image by a number of pixels on the x axis and by a potentially different number of pixels on the y axis is usually performed. Therefore instead of aligning the whole image according to one offset it may be advantageous to break the image into a number of smaller tiles and align each tile separately according to respective individual offsets. The result might be that some tiles are offset differently than others.

Given this tile based approach a brute force algorithm to determine such an offset for a given tile of a first or source image is to try each possible movement or translation of the given tile to a corresponding position in a second or target image. For each attempted alignment of tiles the net difference between all pixels in the translated source image and the target image may be determined and summed. This net difference or error is stored and the translation with the minimum error can be classified as the best fit. 

Trying all translations in this fashion can be computationally expensive. For example if each tile is of size m n it will take on the order of mn m squared times n squared operations per tile to determine the best fit. As a numerical example if m and n are both 64 mn 16 772 216.

Improvements may be made to the brute force algorithm. For instance the algorithm can be enhanced by summing pixel values on a per column basis for both the tile in the source image and an equivalent portion of the target image. These column based sums may be stored in respective vectors for the source image and the target image. Similarly pixel values can be summed on a per row basis both for the tile in the source image and for a portion of the target image corresponding to the tile. These row based sums may also be stored in respective vectors.

An error can be computed based on e.g. taking the sum of the squared difference between each respective element of the source and target vectors. In some cases before taking the error the vectors may be shifted or slid into various positions relative to one another and the error of each of these positions may be determined. The lowest of these errors may be considered the best fit error.

This best fit error represents an estimated quality of the alignment between the tile from the source image and portion of the target image. The lower the best fit error the better the estimated alignment quality. This optimization can be performed using on the order of mn operations. Therefore if m and n are both 64 this process will take on the order of 4096 operations.

This column and row based optimization however can provide inaccurate results. For example if the image is of the letter X or of a checkerboard pattern the row and column sums will provide a uniform result. Thus the resulting alignment between the source and target images is often inaccurate.

Computationally efficient and robust determination of tile alignments permits both more widespread use of image alignment and determination of alignments for more tiles at the same computational cost. Additionally computationally efficient and robust tile alignment can in turn lead to use of computationally efficient and robust algorithms for image alignment. For instance a relatively large number of tile alignments for tiles from the image can be determined and the image alignment may be based on these tile alignments. Such techniques can lead to faster and better image alignments for a variety of applications that use image alignment.

The methods devices and systems described herein can be implemented using so called thin clients and cloud based server devices as well as other types of client and server devices. Under various aspects of this paradigm client devices such as mobile phones and tablet computers may offload some processing and storage responsibilities to remote server devices. At least some of the time these client services are able to communicate via a network such as the Internet with the server devices. As a result applications that operate on the client devices may also have a persistent server based component. Nonetheless it should be noted that the methods processes and techniques disclosed herein may be able to operate entirely on a client device or a server device.

This section describes general system and device architectures for such client devices and server devices. However the methods devices and systems presented in the subsequent sections may operate under different paradigms as well. Thus the embodiments of this section are merely examples of how these methods devices and systems can be enabled.

Network may be for example the Internet or some form of public or private Internet Protocol IP network. Thus client devices and may communicate using packet switching technologies. Nonetheless network may also incorporate at least some circuit switching technologies and client devices and may communicate via circuit switching alternatively or in addition to packet switching.

A server device may also communicate via network . Particularly server device may communicate with client devices and according to one or more network protocols and or application level protocols to facilitate the use of network based or cloud based computing on these client devices. Server device may include integrated data storage e.g. memory disk drives etc. and may also be able to access a separate server data storage . Communication between server device and server data storage may be direct via network or both direct and via network as illustrated in . Server data storage may store application data that is used to facilitate the operations of applications performed by client devices and and server device .

Although only three client devices one server device and one server data storage are shown in communication system may include any number of each of these components. For instance communication system may comprise millions of client devices thousands of server devices and or thousands of server data storages. Furthermore client devices may take on forms other than those in .

User interface may comprise user input devices such as a keyboard a keypad a touch screen a computer mouse a track ball a joystick and or other similar devices now known or later developed. User interface may also comprise user display devices such as one or more cathode ray tubes CRT liquid crystal displays LCD light emitting diodes LEDs displays using digital light processing DLP technology printers light bulbs and or other similar devices now known or later developed. Additionally user interface may be configured to generate audible output s via a speaker speaker jack audio output port audio output device earphones and or other similar devices now known or later developed. In some embodiments user interface may include software circuitry or another form of logic that can transmit data to and or receive data from external user input output devices.

Communication interface may include one or more wireless interfaces and or wireline interfaces that are configurable to communicate via a network such as network shown in . The wireless interfaces if present may include one or more wireless transceivers such as a BLUETOOTH transceiver a Wifi transceiver perhaps operating in accordance with an IEEE 802.11 standard e.g. 802.11b 802.11g 802.11n a WiMAX transceiver perhaps operating in accordance with an IEEE 802.16 standard a Long Term Evolution LTE transceiver perhaps operating in accordance with a 3rd Generation Partnership Project 3GPP standard and or other types of wireless transceivers configurable to communicate via local area or wide area wireless networks. The wireline interfaces if present may include one or more wireline transceivers such as an Ethernet transceiver a Universal Serial Bus USB transceiver or similar transceiver configurable to communicate via a twisted pair wire a coaxial cable a fiber optic link or other physical connection to a wireline device or network.

In some embodiments communication interface may be configured to provide reliable secured and or authenticated communications. For each communication described herein information for ensuring reliable communications e.g. guaranteed message delivery can be provided perhaps as part of a message header and or footer e.g. packet message sequencing information encapsulation header s and or footer s size time information and transmission verification information such as cyclic redundancy check CRC and or parity check values . Communications can be made secure e.g. be encoded or encrypted and or decrypted decoded using one or more cryptographic protocols and or algorithms such as but not limited to the data encryption standard DES the advanced encryption standard AES the Rivest Shamir and Adleman RSA algorithm the Diffie Hellman algorithm and or the Digital Signature Algorithm DSA . Other cryptographic protocols and or algorithms may be used instead of or in addition to those listed herein to secure and then decrypt decode communications.

Processor may include one or more general purpose processors e.g. microprocessors and or one or more special purpose processors e.g. digital signal processors DSPs graphical processing units GPUs floating point processing units FPUs network processors or application specific integrated circuits ASICs . Processor may be configured to execute computer readable program instructions that are contained in data storage and or other instructions to carry out various functions described herein.

Data storage may include one or more non transitory computer readable storage media that can be read or accessed by processor . The one or more computer readable storage media may include volatile and or non volatile storage components such as optical magnetic organic or other memory or disc storage which can be integrated in whole or in part with processor . In some embodiments data storage may be implemented using a single physical device e.g. one optical magnetic organic or other memory or disc storage unit while in other embodiments data storage may be implemented using two or more physical devices.

Data storage may also include program data that can be used by processor to carry out functions described herein. In some embodiments data storage may include or have access to additional data storage components or devices e.g. cluster data storages described below .

Server device and server data storage device may store applications and application data at one or more places accessible via network . These places may be data centers containing numerous servers and storage devices. The exact physical location connectivity and configuration of server device and server data storage device may be unknown and or unimportant to client devices. Accordingly server device and server data storage device may be referred to as cloud based devices that are housed at various remote locations. One possible advantage of such could based computing is to offload processing and data storage from client devices thereby simplifying the design and requirements of these client devices.

In some embodiments server device and server data storage device may be a single computing device residing in a single data center. In other embodiments server device and server data storage device may include multiple computing devices in a data center or even multiple computing devices in multiple data centers where the data centers are located in diverse geographic locations. For example depicts each of server device and server data storage device potentially residing in a different physical location.

In some embodiments each of the server clusters and may have an equal number of server devices an equal number of cluster data storages and an equal number of cluster routers. In other embodiments however some or all of the server clusters and may have different numbers of server devices different numbers of cluster data storages and or different numbers of cluster routers. The number of server devices cluster data storages and cluster routers in each server cluster may depend on the computing task s and or applications assigned to each server cluster.

In the server cluster for example server devices can be configured to perform various computing tasks of server device . In one embodiment these computing tasks can be distributed among one or more of server devices . Server devices and in server clusters and may be configured the same or similarly to server devices in server cluster . On the other hand in some embodiments server devices and each may be configured to perform different functions. For example server devices may be configured to perform one or more functions of server device and server devices and server device may be configured to perform functions of one or more other server devices. Similarly the functions of server data storage device can be dedicated to a single server cluster or spread across multiple server clusters.

Cluster data storages and of the server clusters and respectively may be data storage arrays that include disk array controllers configured to manage read and write access to groups of hard disk drives. The disk array controllers alone or in conjunction with their respective server devices may also be configured to manage backup or redundant copies of the data stored in cluster data storages to protect against disk drive failures or other types of failures that prevent one or more server devices from accessing one or more cluster data storages.

Similar to the manner in which the functions of server device and server data storage device can be distributed across server clusters and various active portions and or backup redundant portions of these components can be distributed across cluster data storages and . For example some cluster data storages and may be configured to store backup versions of data stored in other cluster data storages and

Cluster routers and in server clusters and respectively may include networking equipment configured to provide internal and external communications for the server clusters. For example cluster routers in server cluster may include one or more packet switching and or routing devices configured to provide i network communications between server devices and cluster data storage via cluster network and or ii network communications between the server cluster and other devices via communication link to network . Cluster routers and may include network equipment similar to cluster routers and cluster routers and may perform networking functions for server clusters and that cluster routers perform for server cluster

Additionally the configuration of cluster routers and can be based at least in part on the data communication requirements of the server devices and cluster storage arrays the data communications capabilities of the network equipment in the cluster routers and the latency and throughput of the local cluster networks the latency throughput and cost of the wide area network connections and and or other factors that may contribute to the cost speed fault tolerance resiliency efficiency and or other design goals of the system architecture.

As shown in client device may include a communication interface a user interface a processor and data storage all of which may be communicatively linked together by a system bus network or other connection mechanism .

Communication interface functions to allow client device to communicate using analog or digital modulation with other devices access networks and or transport networks. Thus communication interface may facilitate circuit switched and or packet switched communication such as POTS communication and or IP or other packetized communication. For instance communication interface may include a chipset and antenna arranged for wireless communication with a radio access network or an access point. Also communication interface may take the form of a wireline interface such as an Ethernet Token Ring or USB port. Communication interface may also take the form of a wireless interface such as a Wifi BLUETOOTH global positioning system GPS or wide area wireless interface e.g. WiMAX or LTE . However other forms of physical layer interfaces and other types of standard or proprietary communication protocols may be used over communication interface . Furthermore communication interface may comprise multiple physical communication interfaces e.g. a Wifi interface a BLUETOOTH interface and a wide area wireless interface .

User interface may function to allow client device to interact with a human or non human user such as to receive input from a user and to provide output to the user. Thus user interface may include input components such as a keypad keyboard touch sensitive or presence sensitive panel computer mouse trackball joystick microphone still camera and or video camera. User interface may also include one or more output components such as a display screen which for example may be combined with a touch sensitive panel CRT LCD LED a display using DLP technology printer light bulb and or other similar devices now known or later developed. User interface may also be configured to generate audible output s via a speaker speaker jack audio output port audio output device earphones and or other similar devices now known or later developed. In some embodiments user interface may include software circuitry or another form of logic that can transmit data to and or receive data from external user input output devices. Additionally or alternatively client device may support remote access from another device via communication interface or via another physical interface not shown .

Processor may comprise one or more general purpose processors e.g. microprocessors and or one or more special purpose processors e.g. DSPs GPUs FPUs network processors or ASICs . Data storage may include one or more volatile and or non volatile storage components such as magnetic optical flash or organic storage and may be integrated in whole or in part with processor . Data storage may include removable and or non removable components.

Generally speaking processor may be capable of executing program instructions e.g. compiled or non compiled program logic and or machine code stored in data storage to carry out the various functions described herein. Therefore data storage may include a non transitory computer readable medium having stored thereon program instructions that if executed by client device cause client device to carry out any of the methods processes or functions disclosed in this specification and or the accompanying drawings. The execution of program instructions by processor may result in processor using data .

By way of example program instructions may include an operating system e.g. an operating system kernel device driver s and or other modules and one or more application programs e.g. address book email web browsing social networking and or gaming applications installed on client device . Similarly data may include operating system data and application data . Operating system data may be accessible primarily to operating system and application data may be accessible primarily to one or more of application programs . Application data may be arranged in a file system that is visible to or hidden from a user of client device .

Application programs may communicate with operating system through one or more application programming interfaces APIs . These APIs may facilitate for instance application programs reading and or writing application data transmitting or receiving information via communication interface receiving or displaying information on user interface and so on.

In some vernaculars application programs may be referred to as apps for short. Additionally application programs may be downloadable to client device through one or more online application stores or application markets. However application programs can also be installed on client device in other ways such as via a web browser or through a physical interface e.g. a USB port on client device .

As can be seen in the soccer ball appears in the lower left hand corner of image the upper middle of image and the lower right hand corner of image . The frame to frame location of the soccer ball may vary due to the soccer ball having been in motion while the photographs were taken and or the camera having been in motion. Thus the image of the soccer ball may suffer from some form of distortion such as blurring or jitter.

Regardless of exactly how the images came about one possible goal of processing these images would be to enhance the image quality of the soccer ball. Alternatively or additionally another goal of processing these images would be to remove the effect of camera motion. In order to perform image enhancement or image stabilization as discussed above it is helpful to be able to align these images in an efficient manner.

For purposes of simplicity image is shown in black and white. Thus in some embodiments image may be a bitmap with pixel values restricted to 0 and 1. In other embodiments each pixel of image can have one of three or more pixel values each pixel value representing a color such as red green and blue RGB cyan magenta yellow and black CYMK or other values than colors such as hue saturation and a brightness value HSV . Each of the pixel values can be an integer for example an integer in the range of 0 1 for bit maps 0 255 for RGB or HSV values or some other range. In other examples pixel values can be real valued e.g. a real number in the range 0 1 .

Image can be aligned with another image of the soccer ball such as image or image for noise reduction or for other purposes. For purposes of clarity and without loss of generality image will be referred to as the source image and the other image with which image is aligned will be referred to as the target image. As shown in The soccer ball in the target image may be offset to some degree compared to the source image. The soccer ball in the target image may also be captured from a different angle scaled increased or decreased in size and or rotated.

To align the source and target images image may first be divided into a number of rectangular tiles as shown in . Each tile represents a portion of image and contains a number of pixels. Some of these tiles may be compared to corresponding areas of the target image to align the images. In order to perform this comparison a subset of tiles may be chosen from image such as one or more of tiles and . While these tiles are rectangular in they may instead be square or take on some other shape.

In some embodiments tiles with the highest intra tile variance may be chosen. Intra tile variance may be determined as the variance e.g. the mean squared deviation computed over all of the pixels in the tile. For tiles with non bitmap pixels e.g. RGB CYMK or HSV encodings variances can be calculated separately for each component e.g. for RGB separate variances can be calculated for the red green and blue components . These separate variances may be weighted equally to determine a composite variance or may be weighted based on human visibility e.g. by scaling red by about 0.35 green by about 0.5 and blue by about 0.15 to determine the composite variance. Alternatively or additionally intra tile variance can be measured by subdividing a tile into a number e.g. 4 6 9 etc. of smaller regions and calculating the maximum difference of pixel values between any two of the regions.

Thus tiles with a relatively uniform color such as the four tiles in the corners of image are likely to have a relatively low variance. But tiles with a larger difference in color such as tiles and are likely to have a relatively high variance.

It should be noted that there are different ways of measuring the variance of a tile. For example the average variance of a tile in the vertical direction and the average variance of a tile in the horizontal direction can be measured separately and the minimum of the two taken to be the variance of the tile. Using this alternative calculation of variance helps eliminate tiles with high variance in one direction and low variance in the other direction. These tiles could be difficult to align in the low variance direction and therefore might decrease the accuracy of the overall alignment.

It may be preferable to use high variance tiles for image alignment as these tiles tend to have more distinct features than low variance tiles. For instance each of the corner tiles of image are very similar to one another while tiles and each exhibit features that no other tile in image exhibits. Therefore one or more high variance tiles may be chosen from image for comparison to the target image. In some embodiments the n tiles with the highest variance may be chosen where n could be 1 2 3 5 10 and so on. In other embodiments p of the tiles with the highest variance may be chosen where p could be 1 5 10 20 etc.

It should be understood that metrics other than variance can be used to select tiles from image . For example standard deviations inter quartile ranges or other ways of calculating or estimating variability may be used in place of or along with variance. Furthermore in practice square tiles may be chosen. However the examples herein utilize rectangular tiles in order to illustrate how the embodiments operate in a more general sense.

It should also be understood that prior to tile selection the images may be pre processed with a high pass filter. The result of this pre processing may serve to detect and or accentuate the edges of objects within the images thereby making the alignment less sensitive to brightness levels color saturation etc. Instead the alignment may focus on the two dimensional structure of the edges in the image. This pre processing can provide better results in some use cases such as when trying to align two images at different exposures.

To summarize the tile selection process may be expressed algorithmically in high level pseudo code as shown in Table 1.

Once tiles are selected from image each of these tiles may be processed for comparison with portions of the target image. In tile A is a pixel level example of a tile from a source image. For purposes of illustration tile A does not correspond to any of the tiles in . Instead it is a simplified representation of a 5 4 pixel tile. In general the techniques discussed herein can operate on tiles that are smaller than tile A e.g. 2 2 pixel tiles as well as tiles that are larger than tile A e.g. 256 128 pixels .

Each pixel in tile A is either white or shaded to represent bitmap values of 0 and 1 respectively. Accordingly tile A can be thought of as a 5 4 matrix wherein each cell in the matrix takes on a value of either 0 or 1 for non bitmap image each cell in the matrix may take on values other than 0 and 1 . These cells can be addressed using row and column coordinates. For example the upper left hand corner of tile A is designated as column 0 row 0 or coordinates 0 0 the lower right hand corner of tile A is at column 4 row 3 or coordinates 4 3 . Any rectangle can be unambiguously identified by specifying the coordinates of its upper left hand corner and lower right hand corner. Generally the pixel in column c row r of tile A can be referenced with coordinates c r .

In tile A a number of feature rectangles can be selected. Each feature rectangle is a sub rectangle of tile A . These feature rectangles are chosen to represent the pixel characteristics of tile A . Thus any number of feature rectangles can be chosen from any part of tile A . In some embodiments these feature rectangles may be chosen randomly. Alternatively or additionally the feature rectangles may be chosen so that their respective sizes i.e. the number of pixels comprising each feature rectangle is substantially evenly distributed.

In three feature rectangles are identified. Feature rectangle is defined by having an upper left hand corner at coordinates 1 0 and a lower right hand corner at coordinates 2 1 . Feature rectangle is defined by having an upper left hand corner at coordinates 1 3 and a lower right hand corner at coordinates 3 3 . Feature rectangle is defined by having an upper left hand corner at coordinates 3 1 and a lower right hand corner at coordinates 4 3 .

The sum of values in the cells for each feature rectangle is calculated and stored in feature vector . This sum may be referred to as a pixel value sum. Since tile A is from a bitmap image the pixel value sum is a count of the number of shaded pixels in each feature rectangle. Thus for feature rectangle feature vector at position 0 contains a 1 for feature rectangle feature vector at position 1 contains a 2 and for feature rectangle feature vector at position 2 contains a 4. If there are more than three feature rectangles defined feature vector may contain additional positions. Formally where feature vector V at position k corresponds to a feature rectangle FR having an upper left hand corner at coordinates c1 r1 and a lower right hand corner at coordinates c2 r2 

In order to reduce the extent of computation used to calculate feature vectors a summed area table SAT may be pre calculated for tile A . Formally an SAT of an m n tile X can be defined as

An SAT for a given m n tile can be calculated in order mn operations that is O mn in big O notation. One way of doing so in a recursive fashion is to take advantage of the summed area table property that SAT SAT 1 SAT 1 SAT 11 Where the value of tile X at any negative row or column index is assumed to be zero. In an alternate embodiment negative row and column indices can be avoided by enlarging tile X to be m 1 n 1 by appending a column of zeros to the left of tile X and a row of zeros at the top of tile X.

This summed area table property can also be used to compute the pixel value sum of a feature rectangle i.e. the value of a cell in the feature vector in constant time. An illustration of this computation is shown in .

The pixel value sum P X c1 r1 c2 r2 of a given feature rectangle of tile X defined by the upper left hand corner at coordinates c1 r1 and the lower right hand corner at coordinates c2 r2 can be found by 

As shown in the sum of the pixels in feature rectangle is 2. This value can be rapidly determined from SAT A using the formula 1122 SAT 22 SAT 21 1 SAT 1 12 SAT 1 11 1 Once more the value at any negative row or column index is assumed to be zero. Thus as shown in by block the sum of pixels for feature rectangle can be determined as 1 3 3 3 SAT 3 3 SAT 3 2 SAT 0 3 SAT 0 2 7 5 2 2 2

In addition to determining an SAT for tile A an SAT can also be determined for an area of the target image. Recall that the target image may depict the same object or objects as the image . However perhaps due to camera or scene movement in the target image the object s may be represented at a different position angle and or rotation. Thus in order to align tile A in an appropriate location in the target image tile A may be tested at one or more locations within a search area of the target image.

In order perform this matching an SAT may be formed for search area B . SAT may be derived from a pre calculated SAT for a larger portion of the target image or the whole target image. From SAT the pixel value sums of any rectangle within search area B can be determined in constant time using the techniques described above. For purposes of example shows SATs for two 4 3 sub rectangles of search area B . Sub rectangle B1 has its upper left hand corner at coordinates 0 0 of search area B while sub rectangle B2 has its upper left hand corner at coordinates 2 2 of search area B . SAT can be determined for sub rectangle B1 and SAT can be determined for sub rectangle B2 .

In general an SAT for a sub rectangle Y with its upper left hand corner placed at coordinates c r of a rectangle X can be determined using the formula SAT SAT SAT 1 SAT 1 SAT 1 1 Where the value of tile X at any negative row or column index is assumed to be zero. Once the SAT for a sub rectangle of the search area has been determined the feature rectangles of the tile A can be compared to the corresponding respective feature rectangles of this sub rectangle. Note that constructing an actual SAT for a sub rectangle of the search area may not be required. If an SAT for the entire search area is constructed the values of any sub rectangle SAT within this search area can be derived more or less directly by using the formula above. Therefore in practice just a search area SAT may be constructed. SATs and are shown mainly for purposes of illustration.

The difference between feature vectors and may be used to represent how well tile A matches sub rectangle B1 . One way of determining the difference between two vectors is to calculate the cumulative squared error of the vectors. Formally the cumulative squared error CSE of two feature vectors FV1 and FV2 each of length l can be expressed as

The CSE for feature vectors and is determined to be 14 as shown in block . shows a similar a comparison between SAT and SAT for sub rectangle B2 . In feature vector contains the sums 1 2 and 5 for feature rectangles and respectively of SAT . The CSE of feature vectors and is determined to be 1 as shown in block .

Thus based on these computational comparisons sub rectangle B2 is a better match of tile A than sub rectangle B1 . This result can be visually verified by comparing tile A in with sub rectangle B2 in . It can easily be concluded that tile A and sub rectangle B2 are identical except for the respective pixels in their lower right hand corners while a number of respective pixels in tile A and sub rectangle B1 differ.

The match quality of tile A and one or more sub rectangles in the search area may be determined as disclosed above. Then the sub rectangle with the best fit or the top n sub rectangles with the best fits may be stored. Additionally the coordinates x y of the offset between tile A in image and sub rectangle B2 of the target image may also be stored for each of these sub rectangles.

Additionally other tiles e.g. more high variance tiles may be chosen from the source image and compared to respective search areas of the target image. This process may continue until a number of translations have been determined for a number of tiles and the error in terms of for example CSE is known for each translation. Thus for a given tile X the error of tile X at offset x y may be stored in a data structure as E x y .

To summarize this tile processing may be expressed algorithmically in high level pseudo code as shown in Table 2.

Once the match qualities of a number of tiles and offsets of each tile are found a global fit of the source image to the target image can be determined. Alternatively a global fit of the target image to the source image can be performed. In some embodiments the selection process can be performed as follows.

First a set of one or more tiles may be chosen from the source image. For each tile in the set the respective translation with a low error e.g. the lowest error is selected. Alternatively the tiles in the set could be selected based on their having a high intra tile variance see the discussion above in reference to . Then based on the number of tiles in the set a different type of transform may be applied to a group of tiles from the source image to determine the net error of a disjointly translated source image. In some embodiments the group of tiles will include the tiles for which an SAT and or a feature vector already exists in order to be computationally efficient.

For example illustrates a set of four tiles being chosen from source image . Tile has a lowest error of 4 at offset 7 6 tile has a lowest error of 1 at offset 10 5 tile has a lowest error of 2 at offset 10 5 and tile has a lowest error of 1 at offset 9 4 .

With respect to the number of tiles chosen if the set consists of just one tile and the translation with the lowest error for this tile is represented by a given offset the given offset may be applied to a group of additional tiles. The resulting disjointly translated source image may be the source image shifted by the x y values of the offset. If the set consists of two tiles a similarity transform can be applied based on the respective translations of each tile in the group. The resulting disjointly translated source image may be the source image shifted and scaled according to these respective translations. If the set consists of three tiles an affine transform can be applied based on the respective translations of each tile in the group. The resulting disjointly translated source image may be the source image shifted scaled and rotated according to these respective translations. If the set consists of four tiles a homography transform can be applied based on the respective translations of each tile in the group. The resulting disjointly translated source image may be the source image shifted scaled rotated and warped according to these respective translations. Regardless of the number of tiles in the set or the type of transform used the net error of the transform can be determined by summing the error for each translated tile from the group. This net error represents a cost of the translation that was performed.

This process of i choosing tiles ii using the offsets of the tiles to translate a group of additional tiles thereby forming a disjointly translated source image and iii evaluating the net error of the performed transformation may take place a number of times perhaps anywhere from 50 100 times or even 10 500 times. For each such trial different sets of tiles may be chosen either based on their having a translation with a low error and or randomly. Advantageously since the error values of various translations for various tiles that were calculated at previous steps can be cached these error values can be looked up in constant time during this process.

Then once a disjointly translated source image with a low e.g. the lowest net error is found a general transformation may be derived based on the tile translations. All of the pixels in the source image may be transformed to form an aligned source image that serves as a match of the target image. The general transformation may comprise a high quality transform perhaps even computing the offsets at the sub pixel level and using bilinear interpolation to determine the final pixel colors. Alternatively the target image may be transformed to form an aligned target image that serves as a match of the source image.

It should be understood that the aligned source image and the target image may not be perfectly aligned but in practice the alignment between these two images may be close enough to provide a useful result. Once the source image and target images are aligned in this fashion various applications such as image enhancement image stabilization pattern matching and or object recognition can be facilitated.

It should also be understood that instead of applying the transform to a group of tiles to form a disjointly translated source image a full transform all of the tiles in the source image can be performed. Then the net error between the resulting translated source image and the target image can be determined. However performing a full transform can be computationally expensive. Thus transforms may be performed on a limited number of tiles to determine an alignment with a low net error and only then might a full transform be performed.

Another possible variation of the embodiments herein is to apply more than one transform to an image. For instance one transform may be applied to part of an image and another transform may be applied to another part of the image.

To summarize this selection of a global fit may be expressed algorithmically in high level pseudo code as shown in Table 3.

Any of the embodiments disclosed herein can optionally be enhanced by performing a multi resolution analysis of the target image. Generally speaking such a multi resolution analysis involves for a given tile downsampling the pixels in the given tile a number of times.

Downsampling can be implemented by for example dividing the given tile into 2 2 blocks and replacing each of these blocks by a single pixel. The value of this replacement pixel can be based on the values of the pixels in the block. For instance the value of the replacement pixel may be determined by taking an average of the values of the four pixels in the block resulting in a fuzzier lower resolution downsampled tile of one quarter the size of the full tile. Thus if a 64 64 tile is downsampled one level the result is a 32 32 tile. If the 64 64 tile is downsampled two levels or the 32 32 tile is downsampled one level the result is a 16 16 tile and so on. Nonetheless a tile can be downsampled in other ways. For example a 3 3 block or a 4 4 block can be replaced by a single pixel and more than just one or two levels of downsampling can be performed for a tile.

A downsampled tile perhaps the downsampled tile at the downsampling level with the lowest resolution may be compared to a similarly downsampled search area of the target image. The alignment procedures disclosed above for using feature rectangles to form feature vectors and to compute errors values based on these feature vectors may be used. As a result one or more low error alignments for the downsampled tile may be identified. For instance the n lowest error alignments may be identified.

Then one of these alignments is selected perhaps the lowest error translation and the alignment procedures are repeated using the next highest level of the downsampled tile and the downsampled search area for the alignment. If the result is a low error alignment the alignment procedure can be repeated with an even higher level of the downsampled tile. However if the result is not a low error alignment another of the n lowest error alignments may be selected the alignment procedures are repeated using the next highest level of the downsampled tile and the downsampled search area for this alignment.

Notably in search area there are 81 possible starting positions for tile i.e. the upper left most pixel of tile can be evaluated in 9 columns and in 9 rows for each of these columns . However in search area there are 16 possible starting positions for tile and in search area there are only 9 possible starting positions for tile . Thus in some cases it can be computationally efficient to start with a tile with a high degree of downsampling e.g. the and test alignments of that tile in the starting positions of an equivalently downsampled search area e.g. search area . Then based on any resulting one or more low error alignments corresponding tiles and search areas with lower levels of downsampling can be tested until a low error of the original tile and search area is found.

At step the alignment of tile is tested in 2 2 block of search area . As shown in block is located at starting position 0 0 of search area . This alignment results in a low error. Therefore at step the alignment of tile is then tested in 4 4 block of search area . Block is located at starting position 0 0 of search area . As part of step the alignment of tile may also be tested in nearby blocks of search area .

For example the alignment of tile may be tested in all blocks within one pixel of block i.e. the blocks with starting positions 1 0 0 1 and 1 1 in search area . The testing of nearby blocks within R pixels of an initial starting position can be referred to as using a refinement radius of R pixels. Thus alignments can be tested in blocks associated with any valid refinement radius. For instance using a value of R 2 the alignment of tile can be tested in all blocks within two pixels of 4 4 block .

If a low error alignment is found then this process can be repeated by testing the alignment of tile in a corresponding location of search area . However for sake of argument suppose that the alignments of tile with block and all blocks within the refinement radius of R results in high errors. Then at step another low error alignment of tile will be selected. In this case the low error alignment of tile in 2 2 block is selected and at step the alignment of tile is then tested in 4 4 block of search area . Block is located at starting position 2 0 of search area . As was the case for step in step the alignment of tile may also be tested for blocks within a refinement radius of R. Alternatively different values of R may be used at two or more levels of downsampling.

Suppose that one of these blocks other than 4 4 block results in a low error alignment. Then at step the alignment of tile is tested against the corresponding block i.e. 8 8 block of search area . Note that this test occurs at the highest resolution the resolution at which there has been no downsampling . Therefore if this test results in a low error block may be classified as a potential alignment for tile . Also similar to steps and step may involve the alignment of tile being tested in other starting positions according to a given refinement radius.

Alternatively a number of translations N of a given tile can be chosen at the downsampling level with the lowest resolution. The error for each translation may be determined. Then at the downsampling level with the next highest resolution and the magnitude of each translation is doubled and tested within the refinement radius. This process continues until the downsampling level with the highest resolution is reached i.e. the level with no downsampling resulting in N possible alignments for the given tile and a respective error for each.

In addition to using multi resolution analysis the embodiments disclosed herein can be distributed amongst two or more computing devices. While the individual steps functions features and or processes may be distributed in a number of ways two specific distributions are described below.

Accordingly at step client device may acquire a set of at least two images. This set may be for example two images of the same object from which the user of client device wishes to produce a de noised image of the object. On the other hand the set may include a series of frames from a video that the user of client device wishes to stabilize.

At step client device may transmit the set of images to server device . At step server device may align the images in the set perhaps via a transform. Particularly server device may perform at least some of the embodiments illustrated by through . Optionally at step server device may further process the aligned images. For instance server device may use the aligned images to produce a de noised image or a stabilized video stream.

Then at step server device may transmit an indication of the processed aligned images to client device . This indication may take various forms. For instance the indication may include a uniform resource locator URL that can be used to access the processed aligned images and the indication may include a copy of the processed aligned images. The indication may be delivered to client device via for instance email text message multimedia message instant message a social networking site or some other means. Instead of or in addition to transmitting the indication of the target image s server device may transmit an indication of the transform to client device and client device may perform the transform.

An advantage of this technique is that the majority of the image alignment processing is offloaded from client device to server device . However in some situations client device may not have sufficient network capacity available to efficiently transmit one or more images to server device .

Thus is a message flow that illustrates another possible way of distributing the image alignment processing between client device and server device . At step server device pre calculates SATs for one or more target images. In some cases server may store or have access to a large database of images. The user of client device may wish to use this database to facilitate identification of an object in an image on client device . For example if client device has a camera function the user may use the camera function of client device to take a picture of an unknown object and save this picture as a source image.

At step client device may determine feature vectors for n tiles T . . . Tn of the source image. At step client device may transmit the feature vectors and possibly their respective offsets to server device . At step server device may determine a match quality of tiles T . . . Tn with target image s from the database. Particularly server device may perform at least some of the embodiments illustrated by through . Then at step server device may transmit to client device an indication of the target image s that may match the source image.

As was the case for step this indication may take various forms such as a URL that can be used to access the matched target image s a copy of the matched target image s or a description of the matched target image s . Similarly the indication may be delivered to client device via email text message multimedia message instant message a social networking site or some other means. Also like step instead of or in addition to transmitting the indication of the target image s server device may transmit an indication of the transform to client device and client device may perform the transform.

At block a source tile may be selected from a source image. The source tile may be represented by a source m n rectangle that contains m n pixels where each of the m n pixels is represented by one or more pixel values. In some cases m n and thus the source tile will be square.

Selecting the source tile from the source image may involve calculating a variance metric for the source tile based on the one or more pixel values of each of the m n pixels in the source tile determining that the variance metric meets a variance threshold and selecting the source tile based on determining that the variance metric meets a variance threshold.

At block a first rectangular feature and a second rectangular feature may be selected in the source tile. Both the first and second rectangular features may be smaller than the source m n rectangle in at least one dimension. In some embodiments at least one of the first and second rectangular features can be 2 2 pixels or larger.

At block a source feature vector may be calculated. The source feature vector may include a first entry containing a first pixel value sum of the first rectangular feature in the source tile and a second entry containing a second pixel value sum of the second rectangular feature in the source tile.

In some embodiments calculating the source feature vector may be based on a source SAT for the source tile. For instance the source SAT may contain m n entries and may take the form of an m n rectangle. Constructing the source SAT for the source tile may involve determining a respective sum of pixels in a sub rectangle defined by upper left hand corner coordinates 0 0 of the source tile and lower right hand corner coordinates i j of the source tile for each respective entry i j of the source tile and writing the respective sum in the respective entry i j of the source SAT. As described in Section 3B the source SAT may be constructed from an image SAT in time proportional to the size of the source SAT. The image SAT may have been pre constructed from the source image.

At block a search area of a target image may be selected. The search area may be represented by an m n rectangle that contains m n pixels where each of the m n pixels is represented by one or more pixel values. The m n rectangle may be larger than the source m n rectangle in at least one dimension.

At block a target tile may be selected within the m n rectangle. The target tile may contain m n pixels. Selecting the target tile may involve downsampling the source tile to form a downsampled source tile downsampling the target tile to form a downsampled target tile and determining that an alignment difference between the downsampled source tile and the downsampled target tile is below an alignment threshold. The alignment difference may be calculated using the feature vectors of the downsampled source tile and target tile. Moreover any of the multi resolution techniques disclosed in Section 4 may be used as part of or in addition to block .

At block a first rectangular feature and a second rectangular feature may be selected in the target tile. The first and second rectangular features in the target tile may be based on the first and second rectangular features in the source tile. For example the first and second rectangular features in the target tile may have the same coordinates and or shape as the first and second rectangular features in the source tile.

At block a target feature vector may be calculated. The target feature vector may include a first entry containing a first pixel value sum of the first rectangular feature in the target tile and a second entry containing a second pixel value sum of the second rectangular feature in the target tile.

At block a difference between the source feature vector and the target feature vector may be determined to be below an error threshold. Determining the difference between the source feature vector and the target feature vector may involve calculating a respective error value between each respective entry k of the source feature vector and a respective entry k of the target feature vector. For instance each of the respective error values may be squared and summed to form a cumulative squared error and the difference may be based on the cumulative squared error.

At block based on the difference between the source feature vector and the target feature vector being below the error threshold a mapping may be determined between the source image and the target image. Determining the mapping between the source image and the target image may involve determining an offset between the source tile and the target tile.

At block the mapping may be applied to the source image to produce a transformed source image. Applying the mapping to the source image may include for instance applying one of an offset transform a similarity transform an affine transform and a homography transform to the source image using at least the offset between the source tile and the target tile.

In some embodiments the processing represented by the blocks of may be distributed between multiple devices. Thus for example before selecting the source tile from the source image the source image and the target image may be received from a client device. Additionally after applying the mapping to the source image an indication may be transmitted to the client device wherein the indication provides a reference to media based on the transformed source image.

At block in response to receiving the source feature vector a search area from a target image may be selected. The search area may be represented by an m n rectangle that contains m n pixels where each of the m n pixels is represented by one or more pixel values.

At step a target tile from the search area may be selected. The target tile may be represented by a target m n rectangle that contains m n pixels. The m n rectangle may be larger than the target m n rectangle in at least one dimension.

At step a first rectangular feature and a second rectangular feature may be selected in the target tile. Both the first and second rectangular features may be smaller than the target m n rectangle in at least one dimension and at least one of the first and second rectangular features may be 2 2 pixels or larger.

At step a target feature vector may be calculated. The target feature vector may include a first entry containing a first pixel value sum of the first rectangular feature in the target tile and a second entry containing a second pixel value sum of the second rectangular feature in the target tile.

For example a target SAT may be constructed for the target tile. The target SAT may contain m n entries and calculating the target feature vector may be based on the target SAT. Additionally the target SAT may take the form of an m n rectangle. Thus constructing the target SAT for the target tile may involve for each respective entry i j of the target tile calculating a respective sum of pixels in a sub rectangle defined by upper left hand corner coordinates 0 0 of the target tile and lower right hand corner coordinates i j of the target tile and writing the respective sum in the respective entry i j of the target SAT. The source SAT may be constructed from an image SAT in time proportional to the size of the source SAT. The image SAT may be pre constructed from the target image.

At step a difference between the source feature vector and the target feature vector may be determined to be below an error threshold.

At step based on the difference between the source feature vector and the target feature vector being below the error threshold an indication of the target image may be transmitted to the client device. This indication may be for instance a URL that can be used to access the target image and or a copy of the target image.

The above detailed description describes various features and functions of the disclosed systems devices and methods with reference to the accompanying figures. In the figures similar symbols typically identify similar components unless context dictates otherwise. The illustrative embodiments described in the detailed description figures and claims are not meant to be limiting. Other embodiments can be utilized and other changes can be made without departing from the spirit or scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure as generally described herein and illustrated in the figures can be arranged substituted combined separated and designed in a wide variety of different configurations all of which are explicitly contemplated herein.

With respect to any or all of the message flow diagrams scenarios and flow charts in the figures and as discussed herein each step block and or communication may represent a processing of information and or a transmission of information in accordance with example embodiments. Alternative embodiments are included within the scope of these example embodiments. In these alternative embodiments for example functions described as steps blocks transmissions communications requests responses and or messages may be executed out of order from that shown or discussed including in substantially concurrent or in reverse order depending on the functionality involved. Further more or fewer steps blocks and or functions may be used with any of the message flow diagrams scenarios and flow charts discussed herein and these message flow diagrams scenarios and flow charts may be combined with one another in part or in whole.

A step or block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein described method or technique. Alternatively or additionally a step or block that represents a processing of information may correspond to a module a segment or a portion of program code including related data . The program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique. The program code and or related data may be stored on any type of computer readable medium such as a storage device including a disk or hard drive or other storage media.

The computer readable medium may also include non transitory computer readable media such as computer readable media that stores data for short periods of time like register memory processor cache and or random access memory RAM . The computer readable media may also include non transitory computer readable media that stores program code and or data for longer periods of time such as secondary or persistent long term storage like read only memory ROM optical or magnetic disks and or compact disc read only memory CD ROM for example. The computer readable media may also be any other volatile or non volatile storage systems. A computer readable medium may be considered a computer readable storage medium for example or a tangible storage device.

Moreover a step or block that represents one or more information transmissions may correspond to information transmissions between software and or hardware modules in the same physical device. However other information transmissions may be between software modules and or hardware modules in different physical devices.

While various aspects and embodiments have been disclosed herein other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting with the true scope and spirit being indicated by the following claims.

