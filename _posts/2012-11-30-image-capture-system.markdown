---

title: Image capture system
abstract: A system is described that allows for the tracking of an exercise occurring within a physical environment. In one or more implementations, the system includes a depth sensing device configured to obtain depth values associated with a plurality of pixels. The depth values indicating distances from one or more physical objects in a physical environment to the depth sensing device. The system also includes a computing device in communication with the depth sensing device. The computing device includes a memory and a processor. The processor is configured to execute the one or more modules to cause the processor to: identify a point corresponding to at least one pixel representing a portion of at least one physical object within the physical environment based upon the pixel depth values; track the point through a plurality of image frames; and determine whether at least one repetition has occurred based upon the tracked point.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09072929&OS=09072929&RS=09072929
owner: NEBRASKA GLOBAL INVESTMENT COMPANY, LLC
number: 09072929
owner_city: Lincoln
owner_country: US
publication_date: 20121130
---
The present application claims the benefit under 35 U.S.C. 119 e of U.S. Provisional Application Ser. No. 61 565 865 entitled MOTION CAPTURE SYSTEM filed on Dec. 1 2011 U.S. Provisional Application Ser. No. 61 583 241 entitled IMAGE CAPTURE SYSTEM filed on Jan. 5 2012 U.S. Provisional Application Ser. No. 61 670 317 entitled IMAGE CAPTURE SYSTEM filed on Jul. 11 2012 and U.S. Provisional Application Ser. No. 61 709 354 entitled IMAGE CAPTURE SYSTEM filed on Oct. 4 2012. U.S. Provisional Application Ser. Nos. 61 565 865 61 583 241 61 670 317 and 61 709 354 are herein incorporated by reference in their entireties.

Cameras are configured to capture images within the cameras field of view. Cameras may be configured to capture data representing color frame images such as red green blue cameras and or configured to capture data representing depth frame images. In some configurations cameras configured to capture depth frame data transmit a near infrared light over a portion of the cameras field of view and determine a time of flight associated with the transmitted light. In other implementations the depth may be determined by projecting a structured pattern of infrared light and determining depth from an infrared camera utilizing suitable parallax techniques.

A system and a computer implemented method are described that allows for the tracking of an exercise occurring within a physical environment. In one or more implementations the system includes a depth sensing device configured to obtain depth values associated with a plurality of pixels. The depth values indicating distances from one or more physical objects in a physical environment to the depth sensing device. The system also includes a computing device in communication with the depth sensing device. The computing device includes a memory configured to store one or more modules and a processor coupled to the memory. The processor is configured to execute the one or more modules to cause the processor to identify a point corresponding to at least one pixel representing a portion of at least one physical object within the physical environment based upon the pixel depth values track the point through a plurality of image frames and determine whether at least one repetition has occurred based upon the tracked point. In another implementation the system may be configured to identify pixels as representing a body segment of a subject and track the movement of the body segment. In this implementation the system may provide near real time feedback to the subject and or the subject s medical personnel.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter.

In an implementation the cameras provide the ability to capture and map three dimensional video imagery in addition to two dimensional video imagery. For example the cameras can capture two dimensional data for a plurality of pixels that comprise the video image. These data values represent color values for the pixels e.g. red green and blue RGB values for each pixel that represents the environment . Thus objects captured by the cameras can appear as two dimensional objects via a monitor e.g. a display as described in greater detail herein . As mentioned above the cameras can also capture depth data within the cameras FOV. Thus the cameras are configured to capture the x and y components e.g. x and y values of the environment within the FOV using RGB values captured image data such as the color frame data representing a color frame image for each pixel in the scene. However the cameras are configured to also capture the z components of the environment which represent the depth values e.g. depth estimate data corresponding to the z axis within the environment. In other words the camera s provide the ability to capture and map with the processor described below the third dimension in addition to two dimensional video imagery. Thus the camera s are configured to capture two dimensional data for a plurality of pixels that comprise the video image. These values are color values e.g. color frame image data for the pixels e.g. the RGB values for each pixel . In addition the camera s also capture the z components of the scene e.g. depth frame image data which represent the depth values for the scene.

The cameras furnish the captured data color frame image data depth frame image data to the computing device . In a specific implementation the cameras may be configured to capture images representing environmental views within the FOV of the cameras . For example in a specific implementation the cameras may capture image data representing floor and a weight rack within the FOV see . The weight rack may be configured to hold a weight bar such as the weight bar shown in in position while the weight bar is not in use. In other implementations the cameras may be utilized to track a subject s rehabilitation progress as discussed in greater detail below.

The computing device may be configured in a variety of ways. For example the computing device may be a server computing device a desktop computing device a laptop computing device a mobile computing device or the like. As shown in the computing device includes a processor and a memory . The camera s provides data representing the video images to the processor which is configured to utilize the data as described herein.

The processor provides processing functionality for the computing device and may include any number of processors micro controllers or other processing systems and resident or external memory for storing data and other information accessed or generated by the computing device . The processor may execute one or more software programs e.g. modules that implement techniques described herein. For example the processor in conjunction with one or more modules as described herein is configured to generate a depth mask i.e. video image of the environment based upon the depth estimate data e.g. z component data captured by the cameras . For example one or more modules are configured to cause the processor to continually monitor the depth value of at least substantially all of the pixels that represent the captured environment and stores the greatest deepest depth value associated with each pixel. For instance the modules cause the processor to continually monitor for a pre determined amount of time e.g. a plurality of frames the depth value of the pixels and stores the deepest depth value measured during the time interval. Thus the depth mask comprises an accumulation of depth values and each value represents the deepest depth value of a pixel measured over the time interval. The processor can then be instructed to generate a point cloud based upon the depth mask that includes a set of point values that represent the captured environment.

The memory is an example of tangible computer readable media that provides storage functionality to store various data associated with the operation of the computing device such as the software program a non transitory computer readable medium embodying a program executable by the processor and code segments mentioned herein or other data to instruct the processor and other elements of the computing device to perform the steps described herein.

The computing device is communicatively coupled to the cameras over a communication network through a communication module included in the computing device . The communication module may be representative of a variety of communication components and functionality including but not limited to one or more antennas a browser a transmitter and or receiver a wireless radio data ports software interfaces and drivers networking interfaces data processing components and so forth.

The communication network may comprise a variety of different types of networks and connections that are contemplated including but not limited to the Internet an intranet a satellite network a cellular network a mobile data network wired and or wireless connections and so forth. Examples of wireless networks include but are not limited to networks configured for communications according to one or more standard of the Institute of Electrical and Electronics Engineers IEEE such as 802.11 or 802.16 Wi Max standards Wi Fi standards promulgated by the Wi Fi Alliance Bluetooth standards promulgated by the Bluetooth Special Interest Group and so on. Wired communications are also contemplated such as through universal serial bus USB Ethernet serial connections and so forth.

The computing device is also communicatively coupled to a display to display information to a user of the computing device . In embodiments the display may comprise an LCD Liquid Crystal Diode display a TFT Thin Film Transistor LCD display an LEP Light Emitting Polymer or PLED Polymer Light Emitting Diode display and so forth configured to display text and or graphical information such as a graphical user interface. The processor is configured to request depth image data and color frame image data from the cameras e.g. image capture device and create an association between the depth image data and the color frame image data. In an implementation the processor may be configured to provide the associated data to the display for display purposes. For example the processor may be configured to cause the display of video imagery that represents the captured environment at the display .

As shown in the computing device is also communicatively coupled to one or more input output I O devices e.g. a keypad buttons a wireless input device a thumbwheel input device a trackstick input device a touchscreen and so on . The I O devices may include one or more audio I O devices such as a microphone speakers and so on.

The computing device includes a point tracking module which is storable in memory and executable by the processor . The point tracking module is representative of functionality to classify a point within one or more frame images e.g. depth frame images color frame images captured by the cameras . For example the point tracking module receives data representing the frame images from the cameras and classifies pixels of the frame images as representing the object or not representing the object. In a specific implementation the object may comprise the weight bar shown in . The point tracking module then generates a point to correspond to the classified pixels for tracking purposes. The classified point may represent an object captured within the FOV of the cameras and the point may be utilized to track data e.g. repetitions etc. relating to an object such as exercise equipment or the like. For example the point tracking module may be configured to classify a point representing a weight bar based upon an association between the color frame image and the depth frame image.

As shown in the point tracking module includes a spatial engine . The spatial engine is configured to instantiate one or more modules described below that are configured to classify a point or points representing a weight bar e.g. weight bar shown in within one or more captured frames. Thus once the spatial engine instantiates the modules the modules utilize data representing the captured environment e.g. image data representing the weight rack image data representing the floor image data representing the weight bar etc. to classify pixels and generate a point representing an object e.g. the weight bar in successive frames. The object may be at least partially represented within the frame images represented by one or more pixels . The point may be utilized to generate and track data relating to the item captured within the FOV. For example the point may be utilized to generate and track data e.g. repetitions velocity of the repetition etc. relating to a user using the weight bar.

As shown in the spatial engine includes a bar classifier module a line detection module and a segmentation module . The modules are configured to receive image data from the cameras and parameters or coefficients from a user interface e.g. communication module I O devices etc. relating to a specific exercise type. In a specific implementation the parameters may indicate a specific lift exercise type e.g. squat bench press hang clean etc. . In an implementation the spatial engine may utilize the parameters to determine a combination of modules to classify a point to represent an object within the FOV. In another implementation the spatial engine may utilize the parameters to determine which module is used to classify a point to represent an object within the FOV. It is contemplated that a specific module or combination of modules may be better suited to classify a point depending upon the type of exercise occurring and the request provided by the point tracking module . For example the bar classifier module may be better suited to classify a point representing the weight bar when a bench press is occurring within the FOV of the cameras .

The bar classifier module which is storable in memory and executable by the processor is representative of functionality to classify each depth frame pixel e.g. based upon the z value of the pixels as potentially corresponding to e.g. representing a bar e.g. a per pixel bar classifier within the captured FOV. In a specific example a user in the cameras FOV may be utilizing a weight bar for exercising and or rehabilitation. In this example the bar classifier module is configured to classify e.g. an outward classifier each pixel within the depth frame image as representing the bar or not representing the bar e.g. in a specific example each pixel is classified as representing the weight bar see or classified as not representing the weight bar . The bar classifier module is configured to classify each pixel by identifying e.g. selecting a starting point a starting pixel starting point C illustrated in within the depth frame image e.g. an outward classifier . The module then identifies four 4 depth variant offset points pixels within at least a portion of the depth frame image. For example the module may only identify e.g. select offset points within an at least partially segmented depth frame image e.g. analyze the points comprising the largest segment in the scene . In another example the module may only identify offset points within a specified region within the depth frame image. In some instances by only analyzing a portion of the depth frame image the number of falsely identified pixels points may be reduced. The offset points are represented as points A B D and E in . Two 2 points of the four 4 offset points are identified points B and D that are at least substantially equidistant and at least substantially collinear with the starting point point C . The module then determines whether the remaining offset points points A and E are zero i.e. infinite distance and whether B and D are not zero and within an acceptable distance from the cameras relative to point C the starting pixel point C is classified as representing the weight bar within the depth frame image. In an implementation the module may be configured to return a zero for any pixels not belonging to the object. The distance values between the various classified points may be input to a mean shift algorithm to identify approximately the position of the center point and remove identified outliers and the resultant is utilized to track the weight bar . Thus the classified center point of the bar e.g. select point C as point or pixel to track in the image may be used to represent the weight bar s location for tracking purposes e.g. track the point through successive image frames . The module may also fit a line to the points along the pixels classified as representing the weight bar overlay a line extending along points B to D . For example the bar classifier module may utilize offset data as described herein to generate e.g. apply a line to represent the weight bar displayed within a display .

Additionally the module is configured to classify whether a pixel represents the weight bar through a downward classifier. More specifically the module causes the processor to determine the depth of the vertical line segments above and below e.g. segments of one or more pixels oriented above or below the selected pixel e.g. selected point C . If the selected pixel represents the object the pixels above and below the selected pixel may have a depth value that is significantly higher e.g. greater z component as compared to the depth value of the selected pixel rather than a significantly lower depth value. If the processor determines that the plurality of pixels above and or below the selected pixel have corresponding depth values that are significantly higher than the depth value of the selected pixel the processor moves to determining the depth of the horizontal line segments. The module also causes the processor to determine the depth of the horizontal line segments e.g. plurality of pixels to the left and the right of the selected pixel. The length e.g. number of pixels tested e.g. depth determination are determined by the depth of the selected pixel. Based upon the depth determination of the horizontal line segments the depth of the horizontal line segment e.g. depth values for the plurality of pixels within the horizontal line segment should be at least substantially e.g. within ten percent 10 of the same depth of as the depth of the selected pixel. It is understood that the module may permit the processor to continue depth determination when a certain percent e.g. between one to five percent 1 to 5 of depth information within the horizontal line segment is missing. Thus if the processor determines that the vertical line segments have a depth significantly higher than the depth value of the selected pixel and that the horizontal line segments have a depth value that is at least substantially the same as the depth value of the selected pixel the processor is instructed to classify the pixel as belonging to the object.

The bar classifier module may also utilize other types of analysis or in combination with the analysis described above and may be utilized to identify classify and track the weight bar as well. For example the bar classifier module may be configured to examine a predetermined range of neighboring pixels e.g. pixels that are positioned above below and centered with the pixel of interest . Thus the module may identify a pixel for classification and examine the pixels surrounding and proximal to the identified pixel for classification of the identified pixel. In another example the module may identify the hue of the pixels representing the weight bar e.g. weight bar has a consistent hue to allow for identification and tracking purposes marks included on the weight bar and so forth.

The line detection module which is storable in memory and executable by the processor evaluates e.g. through a Hough transform RANSAC or the like in motion color frame image data captured by the cameras to identify at least a portion of in motion color frame image pixels as representing the object e.g. weight bar . In an implementation the line detection module utilizes a Hough transform to identify a subset of in motion color frame image pixels that most likely represent the weight bar . For example the line detection module analyzes the brightest e.g. most intense color frame image pixels in motion. Thus the module may utilize the Hough transform to identify pixels having a greater intensity as compared to neighboring pixels and that are nearly horizontal lines among the pixels whose intensities have changed at least significantly since the previous frame. In an implementation the line detection module divides a current color frame image into a number of evenly sized rectangular regions that are utilized by the Hough transform. Each rectangular region represents a distinct subset of the current color frame image. For instance as shown in the module is configured to group or collect in motion pixels represented as asterisks in into evenly sized rectangular regions e.g. a 2 10 grid of pixels correlated to a portion of the current color frame image e.g. the module identifies horizontal non vertical strips e.g. rows of in motion pixels . The module maintains the in motion pixels if a predefined minimum pixel threshold e.g. approximately ninety percent 90 of the pixels within the rectangular region are identified as in motion pixels. The module then removes at least substantially all of the pixels not identified as within the rectangular regions that have surpassed the minimum pixel threshold. The module then utilizes a Hough transform to generate a bar line in color that represents the object weight bar based upon the modified image e.g. the image containing the in motion pixels . For instance the module may utilize the Hough transform to locate collinear points pixels within the modified image. This approach may assist in removing one or more false lines from the color frame image s so only the desired object is identified and represented by the bar line. In a specific embodiment a frame difference operator e.g. subroutine may identify in motion pixels for the line detection module e.g. comparing each pixel of a current frame image with the respective pixel of the previous frame image or the like . In another implementation the module may reject the generated bar line generated via the Hough transform if the generated bar line does not intersect e.g. cross one or more pixels identified as representing the user. The one or more pixels that represent the user may be identified utilizing the segmentation module as described herein. The modules are configured to cause the processor to re fit the line identified through the in motion color frame image data RGB process. Thus as described above the module may have caused the classification of one or more pixels as representing the weight bar as well as fit a line to those classified pixels and the module may have caused the classification of one or more pixels as representing the weight bar as well as fit a line to those classified pixels . The module may then cause the processor to re fit a line to the RGB pixels that have been classified as belonging to the weight bar based upon the depth information. For example the module may cause the processor to attempt to fit a line to corresponding color frame image pixels based the depth frame image pixels. If this re fit requires the line to be adjusted beyond a certain pre defined threshold in the color frame images to match the corresponding depth pixels the module causes the re fit line to be rejected.

The segmentation module which is storable in memory and executable by the processor is configured to remove non essential pixels e.g. pixels identified as not representing the object and or noise with the depth frame images. For example the point tracking module and the spatial engine may be configured to only identify pixels associated with the object and not pixels associated with the background of the frame image. For example due to the nature of a squat exercise a weight bar may be partially obstructed by a person s head. The partial obstruction may impede the classification of pixels by the bar classifier module and or the line detection module . Thus the segmentation module may be configured to classify a point representing a portion of the person s head to assist with tracking of the weight bar e.g. weight bar . In an implementation the segmentation module may identify a portion of the person s head utilizing the depth frame pixels and generate a point representing a portion of the person s head via a suitable mean shift algorithm or the like. While the person within the FOV is engaged in a squat exercise the segmentation module may continuously classify pixels within successive depth frame images as representing a portion of the person s head and the points may be utilized to track repetitions relating to the squat exercise.

The computing device also includes an object detection module which is storable in memory and executable by the processor which is representative of functionality to determine a floor within the FOV of the cameras . The module is configured to cause the processor to identify pixels representing the floor see . In an implementation the processor randomly selects a number of regions e.g. subset of pixels within the depth frame. The regions may be of varying size in specific examples. From those selected regions the processor randomly selects identifies a number of pixels and from those pixels the processor identifies the pixels having the greatest z component value e.g. pixels representing an object that is most distant from the camera as these are initially considered as pixels most likely representing the floor . With these identified pixels the processor utilizes a RANSAC processor to determine a best fit three dimensional plane for the identified pixels. This best fit plane is compared against a pre defined model plane of the floor e.g. comparison may be the dot product between the two planes normal vectors . If the scalar result of the dot product comparison is within a pre defined acceptable range the identified three dimensional plane represents the floor. Based upon the analysis the processor is configured to generate a three dimensional plane equation that represents the floor within the FOV. The equation may be stored in the memory to allow for retrieval during later uses of the system .

The module is configured to cause the processor to identify an active region that represents the floor plane. For example the active region identifies the bounds of the floor plane. Thus the processor is configured to identify activity occurring within the active region e.g. pixels representing an activity having x y and z components outside of the active region are not identified by the processor . In other words objects and or activity not within the active region are ignored e.g. defined as not in the trackable area .

In one or more implementations the object detection module is further configured to determine the pixel representing the weight rack during initialization of the system . For example during initialization of the system e.g. before any objects or exercises are performed within the active region the module is configured to cause identification of the weight rack within the FOV. Thus the processor is configured to identify pixels based upon x y and z components of the pixels as representing the weight rack or not representing the weight rack . For instance the weight rack may be identified based upon the characteristics of the weight rack e.g. pixels having z components closer to the camera as compared to pixels representing the wall pixels arranged in a generally linear configuration etc. . Once the weight rack pixels have been identified a mask may be applied to subsequent depth frames during operation of the system e.g. depth frames where exercises are being performed etc. to remove the data that represents the weight rack. Thus depth frame images are generated that include at least substantially no depth frame image data that represent the weight rack.

The computing device includes an object in use module which is storable in memory and executable by the processor . The object in use module is representative of functionality to determine whether the weight bar is in use e.g. weight bar is positioned within a weight rack . The object in use module utilizes data representing an environment within the FOV. For example the module may utilize image data e.g. color frame image data depth frame image data combinations thereof representing the weight rack to determine whether a weight bar is not in use e.g. the weight bar is positioned within the weight rack . In a specific implementation the module may first determine the relative position of weight rack within the captured frame or frames. Once the relative position is defined the object in use module may then define a buffer region proximate e.g. adjacent etc. to the position of the weight rack . As the image data is provided to the module the module determines whether the relative position of the weight bar as represented by the image data is within the buffer region. The module may determine that the weight bar is no longer in use when the relative position of the weight bar e.g. pixels representing the weight bar is within the buffer region for a predetermined time e.g. a predetermined amount of successive frames captured by the cameras . If the module determines the weight bar to be no longer in use the point tracking module may not utilize the classified pixels for tracking and monitoring purposes.

As shown in the computing device also includes a tracking module which is storable in memory and executable by the processor . The tracking module utilizes the data generated by the bar classifier module the line detection module and or the segmentation module to determine and track the number of repetitions that occurred within the FOV of the cameras . For example the tracking module may utilize the data provided by the bar classifier module the line detection module and or the segmentation module to determine the number of repetitions a user of a weight bar within the FOV has executed as well as the velocity and or acceleration of the weight bar during various defined intervals e.g. a single repetition mean average over multiple repetitions etc. .

The tracking module utilizes the datasets e.g. classified pixels etc. generated by the point tracking module to determine statistics associated with the object being tracked e.g. weight bar within the FOV during each cameras capture event e.g. each time the cameras captures data representing the depth frame image and data representing the color frame image . In a specific implementation the tracking module receives a point from the point tracking module to determine an approximate position of the represented object within the FOV during each cameras capture event. The tracking module may utilize the datasets generated by the object detection module to determine a relative position of the representative point point C with respect to the floor. Then the tracking module may determine that a repetition has occurred by tracking a representative point e.g. point C through a number of frame images. In an implementation the module may declare a repetition has occurred when 1 the representative point which may be computed from the line determined by the Hough transform and the pixels identified by the bar classifier module has moved in subsequent frames between a predetermined minimum distance and a predetermined maximum distance and 2 the representative point has a predetermined velocity attribute between a predetermined minimum velocity and a predetermined maximum velocity. In another implementation the module may utilize other characteristics of motion to determine statistics associated with the tracked object. For example the module may determine a horizontal displacement of the object.

The tracking module may be configured to determine the velocity and or acceleration attributes of the weight bar by comparing the respective positions of the representative point throughout a pre determined amount of frames generated from the data sets corresponding to the depth frame image and the color frame image utilizing the known cameras capture configurations e.g. the cameras frames per second etc. . The track module may cause the processor to determine the velocity associated with the object by determining the change in distance for the tracked position between a first frame and a second frame and dividing the positional value by the change in time between the first frame and the second frame. Thus the velocity and or acceleration may be approximated on a per repetition basis a multiple repetition basis and so forth. Additionally the tracking module may utilize the data generated from the object detection module to determine the relative position of the weight bar within the captured frame. For instance the tracking module may utilize the floor data object to determine a relative position e.g. height of the point representing the weight bar within the captured frame image by comparing the data representing the point to the data representing the floor.

In one or more implementations the processor may initiate instructions to cause the display to display one or more images e.g. a depth frame image a color frame image an image represented by both the data sets corresponding to the depth frame image and the data sets corresponding to the color frame image . The displayed images may also include the points representing the respective objects e.g. weight bar portion of the person s head etc. and or a line overlay representing the weight bar . Moreover the results e.g. repetition velocity acceleration may be displayed by the display stored in memory for future utilization and so forth. In another implementation as shown in the point tracking module may be configured to cause display of one or more graphical representations related to the tracked data associated with the exercise. For instance the graphical representations may convey the velocity associated with weight bar for athletes which may correlate to an athlete s exercise performance. The graphical representations may also convey an athlete s exercise performance e.g. velocity associated with a squat exercise performance over a predetermined time period.

While the present disclosure describes one specific example e.g. identifying a weight bar it is contemplated that the present disclosure may be utilized in other settings as well. For example the present disclosure may be utilized to identify a medical rehabilitation apparatus within a medical rehabilitation setting and so forth.

Based upon the classification of the pixel a point e.g. a representative point is generated to represent the object Block . As described above the point tracking module generates a representative point e.g. point C in to represent the object within the frame image. As shown in a determination is made whether the generated point represents the object upon a failure to generate a point utilizing current frame image data Block . For example previous frame image data may be utilized to generate and maintain an offset dataset to assist in determining whether the generated point represents the object when the point tracker module failed to generate a point e.g. due to object being too close to a user s body etc. . For example the offset dataset may be utilized to extrapolate whether the current generated point is within an acceptable range of values based upon the offset dataset. In another example the offset dataset may comprise prior relative positional data of the classified pixels e.g. relative position of the classified pixels within the previous frame images . Thus when the point tracking module is tracking a person during a bench press exercise the point tracking module may utilize the prior positional data of the classified pixels to extrapolate a current relative position of the weight bar within the frame image and continue tracking the object. In another implementation the offset dataset may be a function of previously generated points that represented the object in previous frame images. Additionally in the event of failure to identify a point one or more suitable filters may be utilized in approaching a best possible guess as to the location of the tracked point. For example the filters may include but are not limited to a Kalman filter a Particle filter and so forth. Thus the tracking may be recovered from lost points by causing the processor to estimate the most likely position of the point.

The point may be utilized to generate and to track data relating to the object Block . Specifically as shown in the point may be utilized to generate data and to track the generated data related to exercise repetitions Block . In an implementation the tracking module utilizes the data generated and furnished by the point tracking module e.g. the bar classifier module the line detection module and or the segmentation module to determine and to track the number of repetitions that occurred within the FOV of the cameras .

As shown in a determination is made of a relative position of the point representing the object with respect to a reference plane Block . In an implementation the tracking module may utilize the floor object data to determine the relative position e.g. height of the point representing the object with respect to the reference plane e.g. the generated floor which represents the floor as represented by suitable three dimensional 3D mapping techniques . The tracking module maintains the points representing the repetitions for comparison purposes. In a specific implementation the tracking module may utilize the points to determine local extrema within the context of the exercise. For example the tracking module may utilize parameters and or coefficients relating to specific exercise to determine acceptable ranges for local minimum points and or local maximum points. In an implementation the parameters and or coefficients may be stored from previous repetitions. The local minimum and maximum points are utilized by the tracking module to determine whether a repetition has occurred. Once the tracking module determines a repetition has occurred the module stores the repetition data and furnishes the repetition data through an application programming interface API . The tracking module then awaits data corresponding to the next repetition.

The tracking module may utilize multiple techniques to determine whether the current point e.g. point representing the weight bar within the current frame image may be a result of noise or whether the current point represents a local minimum maximum value. The current repetition point can be compared to previous points within a predetermined number of previous frame images e.g. compare the current point to the each point occurring within the previous two frame images . If the current point is a local minimum maximum point the tracking module may maintain the current point. If the current repetition data point is not a local minimum maximum point the tracking module may classify the current point as noise. However the repetition points not classified as a local minimum maximum point may be maintained for later statistical analysis.

As shown in a velocity associated with a current point is determined Block . For example the tracking module is configured to cause the determination of the instantaneous velocity of the current point as compared to the previous frame s point. For example the tracking module may cause the processor to determine the instantaneous velocity of a point that represents a local minimum maximum value. If the instantaneous velocity associated with the current repetition point is greater than a predetermined value e.g. ten 10 meters per second etc. the current point may be discarded as noise. It is contemplated that the instantaneous velocity may vary as a function of the exercise.

The tracking module may also be configured to cause determination of whether the point has reached a relative minimum height with respect to the generated floor. If the minimum height has not been reached the tracking module can continue tracking the points as a current repetition. Otherwise the tracking module determines a repetition has occurred determines velocity data corresponding to the repetition stores the repetition data and awaits data corresponding to the next repetition. In one or more implementations the minimum height may be determined from configuration data e.g. parameters coefficients etc. or previous repetition data associated with the same exercise.

The tracking module may also be configured to fit a parabola to one or more points representing the repetition. In an implementation the tracking module utilizes the parabola to determine local minimum maximum points for further determination of whether a repetition has occurred.

The point e.g. current representative point may also be compared to other parameters and or coefficients corresponding to a specified exercise to determine whether a repetition has occurred Block . For example the tracking module is configured to utilize parameters and or coefficients corresponding to a specified exercise to determine whether a repetition has occurred. The parameters and or coefficients may include an acceptable range of values e.g. an acceptable range of minimum values an acceptable range of maximum values etc. that the tracking module utilizes to determine whether a repetition has occurred. The parameters coefficients may be based upon previously tracked datasets corresponding to the specified exercise predetermined parameters coefficients associated with a specified exercise and so forth. Thus the tracking module may determine a repetition has occurred when at least a portion of the current repetition dataset e.g. cumulative dataset of points representing positions of the weight bar in two or more successive frame images occurs within the acceptable range. Moreover the tracking module may maintain parameters or coefficients corresponding to previous repetitions of the specific exercise. For example the tracking module may compare the current repetition dataset to the one or more previous repetition datasets to determine whether a repetition has occurred due to a portion of the current repetition dataset occurring within the acceptable ranges of local maximum minimum values.

In one or more implementations the system may be utilized to track a subject s patient s rehabilitation see . For example the cameras may be positioned proximate to a bed the subject uses. The subject s regimen may include one or more rehabilitation exercises. Thus the image capture system may be utilized to track the subject s progress. As shown in the computing device includes a rehabilitation module that is storable in memory and executable by the processor . As described below the module is configured to cause the processor to detect and monitor a subject s rehabilitation exercise occurring with the FOV of the cameras .

The rehabilitation module is configured to cause the processor to identify pixels representing the floor and pixels representing the wall . In one or more implementations the module utilizes an iterative Random Sample Consensus RANSAC method e.g. an algorithm to define identify pixels that represent the floor and pixels that represent the wall. Thus the module may cause the processor to initially scan the pixels and define a portion of the pixels as representing the floor and another portion of the pixels as representing the wall. The module may then cause the processor to further define e.g. iteratively define other pixels that represent the floor and or the wall . Additionally the module may employ a pseudo RANSAC method where the module utilizes pre defined parameters to select pixels having values e.g. x y z values within the pre defined parameters. In other words the processor selects pixels having values within the pre defined parameters before the processor utilizes the RANSAC method to identify whether the pixel represents the wall or the floor or the pixel does not represent either the floor or the wall .

Once the pixels representing the floor and the pixels representing the wall have been identified the pixels representing the bed are identified. The module is configured to cause the processor to generate a depth mask image representing the environment as described above. Based upon the depth mask the module is configured to cause the processor to generate a point cloud representing the environment . In an implementation the module is configured to cause the processor to identify pixels representing the bed within the depth mask. The processor may be instructed to scan the depth mask in a first direction e.g. left to right with respect to the orientation of the cameras right to left with respect to the orientation of the cameras . Then the processor may be instructed to identify pixels having a predetermined gradient change change is z value with respect to the adjacent pixels e.g. pixels representing the bed have a closer z value as compared to pixels representing the wall and or the floor . For example the processor compares the pixels in a left to right orientation to determine the pixels representing the bed. For instance the processor is configured to compare the z components of the pixels within the depth mask to the z components of the adjacent pixels to identify portions of the bed bed plane etc. . Thus the processor is configured to identify pixels having a z component that are closer to the cameras and within a predefined gradient change as compared to the z component of the floor wall pixels are identified as bed pixels.

The pixels representing the edge of the bed may be then extracted identified by utilizing one or more suitable processes to define the edges of the bed . For example the processor may incorporate a Hough line transformation process to define the edges of the bed . In this example the processor may detect identify multiple Hough lines among the potential edge of bed pixels. The module is configured to cause the processor to select the lines that most likely correspond or represent to the edges of the bed . For example the processor may identify an edge of the bed based upon a minimum number of pixels within the line location position of the pixels toward the center of the image a line of pixels that are spaced approximately the width of the bed and the like. The baseboard of the bed may then be identified. For example the processor may scan the depth mask in a first direction left to right and in a second direction down to up with respect to orientation the cameras to identify pixels having z components that are proximal near to the cameras as compared to the z components of wall floor. The module is then configured to cause the processor to fit a polygon active region to the pixels representing the bed e.g. intersections of the side bed pixels with the back wall pixels intersections of the side bed pixels with the baseboard pixels intersection points with the pixels representing the baseboard shifted along the sides of the bed by the length of the bed etc. . The above description is utilized with the cameras in a first orientation e.g. baseboard is proximal to the cameras and the head of the bed is distal to the cameras . However it is understood that the cameras may be re positioned and different techniques may be utilized to define portions of the bed e.g. baseboard is distal to the camera as compared to the plane of the bed etc. . For example the processor may identify the baseboard pixels as having a z component as distal to the cameras as compared to the z component of the bed plane pixels.

Once the bed has been identified the processor identifies one or more object points pixels within the field of view FOV of the cameras . In a specific implementation the module is configured to identify e.g. find points representing the body above the bed e.g. identify pixels having a depth value closer to the cameras as compared to the depth values of pixels representing the bed . For instance the module may instruct the processor to determine the gradient change between the depth values of the pixels representing the body to the depth values of the pixels representing the bed . In an implementation the object points may be a grouping of pixels that represent a body region of the subject. For example the processor may identify pixels that potentially represent feet points e.g. represent the feet of the subject within the FOV . In another example the processor may identify pixels that potentially represent leg points arm points hand points and so forth. In an implementation the processor is instructed to identify pixels that are proximate to the bed in two dimensional coordinates e.g. x and y components of the potential feet pixels are proximal to the bed pixels which is based upon the bed polygon the potential foot pixels are sufficiently above the bed pixels pixels representing the bed plane in three dimensions and the potential foot pixels are sufficiently close e.g. z component of the pixels are proximal to the cameras . Thus the point cloud may only include those points identified as representing the feet . Additionally a variety of other filters may be utilized to identify points representing the feet . For example the module is configured to cause the identification of points that are within a certain range of the points representing the hips as well as restricting those identified points that are within a predetermined distance from the center of the bed . These points may further be restricted to points that are within a predefined range of angles with respect to the center of the bed. The module may also cause the processor to maintain the points whose rate of change in height in the direction of the identified hip area is large e.g. greater than a predetermined threshold . For example the module may cause the processor to maintain points whose rate of change is approximately 0.125 millimeter for every 1 millimeter e.g. points that move in the direction of the hip . In another implementation the module may cause the processor to identify pixels distal from the pixels representing the surface of the bed e.g. pixels having z components that are closer to the cameras as compared to the z components of the pixels representing the surface of the bed . For example the processor may identify pixels having z components ranging from approximately three inches to approximately twelve inches 3 in. to 12 in. proximal to the cameras as compared to the pixels representing the bed e.g. pixels are identified that have a z component approximately three inches to approximately twelve inches closer to the cameras as compared to the z components of the bed .

The module is then configured to cause the processor to segment the potential body e.g. feet points to isolate one or more body e.g. feet segments e.g. a plurality of pixels representing the feet . In an implementation the processor may utilize an image segmenter to segment the pixels according to predefined parameters. Upon the initial segmentation the processor may discard smaller segments e.g. discards segments with fewer than ten 10 points etc. . Segments that represent the feet may be combined that do not differ horizontally but differ vertically. This may combine extra segments introduced from cloth proximal to the feet. At least substantially all of the feet segment positions e.g. feet segments identified in earlier frames may be stored in the memory . Thus if the current potential feet points are identified as a single segment e.g. feet are touching the processor can split the current signal segment based upon the positions of the feet segments previously identified e.g. segments identified in previous frames stored in memory . The processor then identifies the pixels representing the feet as the highest remaining segments in the depth mask. It is understood that the module is configured to cause the processor to classify feet segments based upon one or more classified body segments as well. For example the processor may identify pixels as representing a body. Upon identifying the body pixels the processor is configured to identify feet pixels based upon the previously identified body pixels.

In another implementation the module is configure to cause the processor to identify an instruments such as a pipe like structure that the subject may be utilizing in the subject s exercise. In this implementation the processor scans a sub set region of the depth frame to identify pixels as representing the instrument. The location of the region may be determined by the location of the pixels representing the bed . The instrument pixels may be defined as pixels having the same or at least substantially same z component aligned in a linear configuration e.g. near straight line etc. that are within one or more predefined parameters e.g. linear pixels having a predefined height component above the bed pixels etc. . Thus once the processor identifies pixels as potentially representing the instrument the processor may filter a portion of the identified pixels that have a z component outside the predefined parameters e.g. proximal to the baseboard pixels proximal to the bed pixels below the bed pixels etc. . These pixels points are then converted to the point cloud to allow for three dimensional modeling of the instrument. A line is then fitted through the points representing the instrument. In an implementation the line may be fitted via RANSAC or another suitable line fitting process. The center of mass of the instrument can be approximated by determining where the line approximately intersects the middle of the environment e.g. approximately where the line intersects the middle of the screen . A point classifier module e.g. point tracking module may be utilized to classify the points along the instrument.

The processor is configured to determine three dimensional tracked points e.g. track the movement of each foot based upon the feet segments. In an implementation the module causes the processor to take the mean shifted center of a subset of points in each identified foot segment that are higher higher y component as compared to the mean shifted center of the remaining subset of points in the foot segment. The module is configured to cause the processor to store the location of each tracked point in the memory . The processor is configured to create a weighted distance between the current tracked points and the stored track points that have a greater change in the horizontal directions as compared to the vertical direction. Thus the processor is configured to measure e.g. track the movement of the feet based upon the feet segments.

A weighted coefficient may then be determined based upon one or more suitable distribution methods. For example the processor may utilize a Gaussian distribution of weighted distances to determine a weighted coefficient that decays via a Gaussian distribution over time. Time may be defined as the difference between the time stamps of the current tracked points and the time stamps of the previously tracked points e.g. tracked points associated with previous frames that are stored in the memory . The tracked points may then be passed through one or more parsing algorithms. The processor may then output the weighted sum value of the current tracked points and previously tracked points using this weighted coefficient. The weighted sum value is utilized to reduce jitter associated with the tracked objects. The values may be utilized to monitor the subject s rehabilitation exercise. For example one or more values may be utilized to generate one or more graphs values instructions reports or the like associated with the rehabilitation exercise. Thus the subject s therapist doctor or the like can monitor the subject s rehabilitation. For example the processor may be configured to generate an at least near real time interactive report see graphical representation shown in allowing the subject to view data and or results associated with the exercise. In another example the processor may be configured to generate an at least near real time instruction based upon the exercise. The instruction may include pre programmed instructions for improving the exercise performed additional exercises the subject may find of interest and so forth. The instructions and or reports may be presented via the display or the like. It is contemplated that the subject may perform varying types of rehabilitation exercises that can be tracked utilizing the system .

A depth mask representing the captured physical environment is generated Block . As described above the module causes the processor to generate a depth mask e.g. depth map which is utilized to identify pixels having a pre determined gradient change with respect to the adjacent pixels. Pixels representing the edge of a bed are identified Block . For example as described above the processor may identify one or more edges of the bed based upon a minimum number of pixels within the line location position of the pixels toward the center of the image a line of pixels that are spaced approximately the width of the bed and the like. The baseboard of the bed may then be identified. The module is configured to cause the processor to fit a polygon e.g. an active region that the subject patient is located within to the pixels representing the bed .

Once the bed has been identified one or more pixels representing an object i.e. the subject are identified Block . As described above the module is configured to instruct the processor to identify pixels having a z value above the bed . These pixels may represent an object such as a patient s feet body legs arms and so forth. As shown in one or more subsets of pixels are segmented to isolate e.g. identify one or more groupings of pixels that represent body segments of the object Block . Based upon the identified body points the movement of the pixels representing the body points is tracked. Block . An at least near real time instruction is generated based upon the exercise Block . For example as described above the module is configured to instruct the processor to generate an least near real time instruction which may include pre programmed instructions for improving the exercise performed additional exercises the subject may find of interest and so forth. The instructions and or reports may be presented via the display or the like.

As shown in the computing device further includes a body tracker module which is storable in the memory and executable by the processor . The body tracker module is representative of functionality to identify and track one or more portions of a subject s body within the FOV of the cameras e.g. subject s body is positioned over a bed . In an implementation as shown in the module is configured to cause the processor to identify a point e.g. one or more pixels of interest or an anchor point. The point of interest represents one or more pixels that represent the portion of the subject s body that are most likely not going to move while the subject is exercising or moving. In this implementation the portion of the subject s body refers to the lower portion of the subject s body e.g. subject s in seam A subject s crotch area B the subject s legs C and D the subject s knees E and F the subject s feet G and H . For convenience the in seam refers to the groin portion i.e. the medial abductor compartment of the thigh of the human body.

The module is configured to cause identification of the anchor point e.g. groin region of the subject s body while the subject is in a calibration pose e.g. patient is staying at least substantially motionless such as laying in the supine position and not moving for a predefined amount of time . The anchor point may identified as being a group of points e.g. pixels that have points directly below with respect to the orientation of the cameras in projective space that are further away from the cameras i.e. pixels adjacent to or nearly adjacent to the identified point that have greater z values as compared to the identified point s z value as well as points e.g. pixels to the lower left and the lower right that are closer to the cameras .

Additionally the system identifies the kinematics e.g. system s calibration period of the subject to allow for identification of various joint positions i.e. identification of pixels representing joint positions e.g. knees E F or areas of the subject s body as shown in . The term kinematics or kinematically may refer to determining whether a distance value or an angle value between joints is within a plausible range e.g. determining whether a joint identified as a knee joint has a corresponding angle or distance value is acceptable based upon human knee joint contortions . For example a knee E F can bend ninety 90 degrees backwards however the knee cannot bend ninety 90 degrees forwards. Thus if an identified joint does not fall within an acceptable range the system is configured to reject the identified joint and utilize the last known best guess value.

It is contemplated that the module may be configured to identify a top portion e.g. torso arms head of the subject and a bottom portion e.g. legs feet and crotch of the subject. The module may cause the identification of these portions based upon the identification of pixels that likely represent the corresponding portions of the body e.g. pixels representing the arms J may have a similar shape or pattern pixels representing the torso 704K may have a similar shape or pattern pixels of the body are compared to surround pixels based upon the depth values of the pixels etc. . Given that the subject may have the subject s legs separated during the calibration period the module may apply a coefficient that is utilized to scale the subject s length accordingly. In a specific implementation the module may cause the processor to utilize the legs C D as a hypotenuse of a right triangle with respect to the subject s midsection. Thus the processor may utilize appropriate trigonometric functions to approximate the height of the subject.

As shown in the module is configured to cause the processor to identify the hips points X and Y of the subject e.g. identify pixels representing the hips of the subject . The module may utilize one or more suitable kinematic methods for identifying the various parts of the subject as described herein. For example as shown in the module may cause the processor to identify a particular distance above the in seam and which are at least approximately equidistant from a line separating the subject in half where the line intersects the defined in seam point e.g. pixel s . For convenience point A represents the mid section of the subject s body . It is contemplated that these identified pixels remain at least substantially constant due to the subject lying in a steady supine position in the bed .

The module is then configured to cause identification pixels representing the knees of the subject. In an implementation the module causes the processor to determine e.g. inspect one or more pixels or points that are within a predefined distance e.g. a range of distances from the in seam and that have a depth value within a pre defined range are knee points e.g. pixels representing knees of the subject . For descriptive purposes as shown in a region centered around the in seam point A is created to illustrate a pre defined range of distances. A circle region is illustrated for illustrative purposes. These distances are utilized to identify pixels to inspect to determine whether the pixels represent a portion of the subject s knee F. Continuing with this descriptive illustration the pixels that intersect the region are examined inspected for the purposes of determining whether pixels represent the subject s knees. Thus in a specific implementation the pixels having x values and y values corresponding to the pre defined distance are examined to determine whether those pixels represent the subject s knees.

However it is contemplated that a problem may arise if the subject re positions the knees or the knees are not within the pre defined distance see . Thus the module may utilize a point classifier identification process similar to the point classifier module described above to compensate or to replace the kinematic identified points as described in greater detail above. In an implementation as shown in one or more points pixels are examined and identified as being a particular distance further from e.g. larger depth value as compared to a point of interest pixel of interest . For example points T L and R represent pixels having depth values that are greater which represent a further distance from the cameras as compared to the point P. Thus P may be identified as a possible knee point. This process may be used to either further filter other leg points or to identify knee points when no valid knees are identified kinematically. In one or more implementations a machine learned classifier as described herein may also be applied to the process of identifying e.g. finding determining knees or other body points. Moreover in the event the knee is partially obscured by other body parts e.g. legs arms or other objects within the FOV e.g. clinician bed sheets etc. the last known identified position is maintained in the memory . Adjustments in order to maintain kinematic restraints may be utilized while the point is obscured in order to create a best guess estimate of the position of the body joint.

It is contemplated that the once the points pixels representing the various portions of the subject s body the module causes the processor to monitor one or more exercises that the subject performs. For instance the module may cause the processor to continually identify and monitor the positions of the identified pixels frame to frame e.g. image frame to image frame . Based upon the continuous monitoring the module causes the processor to measure the exercises performed. For instance the subject may be instructed to move the subject s foot ankle region as part of the subject s rehabilitation regimen. Thus the system is configured to measure the movement of the foot ankle region and provide information to the subject s doctor physical therapist relating to the movement of the ankle e.g. how far the subject moved the foot ankle region how many times the exercise was performed a graphical representation of the movement of the foot ankle region etc. . Moreover it contemplated that the present disclosure may be utilized to identify the upper body portions of the subject while the subject is in a calibration position e.g. supine position seated position standing position etc. . The module may be configured to instruct the processor to cause the display of data relating to the exercises to medical personnel e.g. subject s physical therapist subject s doctor subject s nursing assistant etc. . For example the processor may be instructed to cause the display of graphical representations similar to the graphical representations illustrated in relating to the exercises performed by the subject.

Upon identifying the pixels representing the various portions or parts of the subject s body the pixels are tracked to monitor an exercise performed by the subject Block . In one or more implementations the module may cause the processor to continually identify and monitor the positions of the identified pixels frame to frame e.g. image frame to image frame . Based upon the continuous monitoring the module causes the processor to measure the exercises performed. By continuously monitoring the exercises performed the module may be configured to instruct the processor to furnish data relating to the exercises to medical personnel such as the subject s physical therapist the subject s doctor and so forth.

Although the subject matter has been described in language specific to structural features and or process operations it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims. A person skilled in the art will recognize that portions of the present disclosure may be implemented in hardware software firmware combinations thereof and so forth.

