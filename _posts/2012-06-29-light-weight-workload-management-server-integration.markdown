---

title: Light weight workload management server integration
abstract: A method, computer program product and system for workload management for an Extract, Transform, and Load (ETL) system. A priority of each workload in a set of workloads is determined using a priority rule. In response to determining that the priority of a workload to be checked has a highest priority, it is indicated that the workload has the highest priority. It is determined whether at least one logical resource representing an ETL metric is available for executing the workload. In response to determining that the workload has the highest priority and that the at least one logical resource is available, it is determined that the workload is runnable.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09262210&OS=09262210&RS=09262210
owner: International Business Machines Corporation
number: 09262210
owner_city: Armonk
owner_country: US
publication_date: 20120629
---
Data integration may be described as extracting data from a source transforming the data and loading the data to a target. That is data integration is Extract Transform Load ETL processing. Data integration processing engines may be scalable and capable of processing large volumes of data in complex data integration projects. It is common for multiple users e.g. customers and projects to share a single data integration processing engine that is responsible for handling all of the data integration processing for those multiple users. This high volume highly concurrent processing may be resource intensive and users try to balance the availability of system resources with the need to process large volumes of data efficiently and concurrently.

Workload management capabilities may be available at Operating System OS or lower levels. Workload management operates at a level that is removed from the data integration environment.

Some users use a multi node grid so that they can utilize a grid resource manager to better control system resources. The grid resource manager deploys data integration projects on a grid.

Some users use Symmetric Multiprocessing SMP which are multi core hyper threaded systems for data integration.

Some users rely on workload schedulers to control workloads. This requires coordination between different groups or the same group running different projects.

Provided are a method computer program product and system for workload management for an Extract Transform and Load ETL system. A priority of each workload in a set of workloads is determined using a priority rule. In response to determining that the priority of a workload to be checked has a highest priority it is indicated that the workload has the highest priority. It is determined whether at least one logical resource representing an ETL metric is available for executing the workload. In response to determining that the workload has the highest priority and that the at least one logical resource is available it is determined that the workload is runnable.

The descriptions of the various embodiments of the present invention have been presented for purposes of illustration but are not intended to be exhaustive or limited to the embodiments disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the described embodiments. The terminology used herein was chosen to best explain the principles of the embodiments the practical application or technical improvement over technologies found in the marketplace or to enable others of ordinary skill in the art to understand the embodiments disclosed herein.

The workload management server includes a Representational State Transfer REST service layer a socket server and a workload management WLM core .

The data integration processing engine is coupled to a command line client WLMClient Application Programming Interface API . The data integration processing engine is also coupled to a socket client . The command line client WLMClient API are coupled to the REST service layer at the workload management server . The socket client is coupled to a socket server at the workload management server .

The workload management core includes the following components a queue manager that manages for example active queues and workload queues a workload repository for storing workload information a configuration manager a policy engine a resource manager persistence storage for storing a file a database etc. and a priority manager .

The configuration manager manages one or more configuration files . A configuration file may be described as a file e.g. an Extensible Markup Language XML file containing class description resource policies priority rules and system parameter settings.

The policy engine manages the one or more resource policies . A resource policy may be described as a policy for how physical and logical resources are to be used.

The priority manager manages one or more priority rules . A priority rule may be described as providing a technique for assigning a priority to a workload.

The workload management core enables creation and management of ETL workloads via an asynchronous queuing mechanism. The workload management core uses two level abstraction to represent both logical and physical resources grants users e.g. applications executing at the data integration processing engine control over workload priority and scheduling and determines workload execution order based on three factors at least one user specified run schedule at least one priority rule i.e. either system provided or user specified and at least one resource policy .

In certain embodiments in order to determine whether a workload is ready to begin execution the user may either issue the submit workload command followed by a check workload command or issue the begin workload command. In particular the begin workload command is a synchronized workload management model that submits the workload to the workload management core and waits on the workload to become runnable i.e. ready to begin execution . The submit workload command is an asynchronous model in which the user specifies a run schedule using a check workload command to determine whether a workload is ready to begin execution.

Unlike the asynchronous model the begin workload command does not return until the workload becomes the next runnable workload and with the begin workload command there is no need to issue the check workload command. With the asynchronous model if the user issues the submit command the user also issues the check workload command to check the resource availability.

Regardless of whether the submit workload command followed by the check workload command or the begin workload command is issued once the workload is runnable the user receives an indication that the workload is runnable and may then start the workload execution within the data integration processing engine outside of the workload management server .

In certain embodiments the workload management core priority rules provided include but are not limited to an elapsed time rule a priority weight rule and a workload run ratio rule. Also users may specify other priority rules to be used to determine the priority of a workload.

In certain embodiments the resource policy may be a workload count policy that imposes a limit on the total number of workloads that can run concurrently. The user can specify a value that applies to the entire system and also specify a value that applies to each queue. For example one resource policy says system level workload count is 60 and the workload count for each queue is 20. In certain embodiments the specified workload count for a queue can exceed that of the system but the system value determines the final workload count.

With embodiments the workload management core is tightly coupled within an ETL application and is able to create logical resources out of the ETL system. This means that the workload management core uses metrics available to an ETL system to co ordinate workload for example limiting the number of ETL workloads started in a given time frame or giving different types of ETL workloads precedence over other types of workloads i.e. a data quality workload has higher priority than a data integration workload or giving workloads owned by a particular project a higher priority. That is a logical resource may be described as representing an ETL metric.

The workload management core manages and optimizes ETL workload execution based on system capacity system resource usage level and a configuration file. The workload management core is transparent to normal information integration developers and an administrator may adjust configuration defined in the configuration file on an as needed basis. The configuration of workloads may be adjusted dynamically with the existing workloads present in the workload management core . With the workload management core new configuration information may be applied automatically without workload management server shutdown and restart.

In certain embodiments the workload management server wraps on top of the workload management core and provides the following functions a multi thread socket server so that socket clients from the data integration processing engine can interact with the workload management core via a socket client for workload scheduling and control and a REST service layer so that the socket client can choose to interact with the workload management core for configuring and monitoring workloads via a centralized interface e.g. a web interface . Thus the data integration processing engine may issue commands to the workload management server via the components or via components .

Embodiments provide the WLMClient API to interact with workload management core so that the user can submit a workload for execution check a workload for execution report back a completed workload query system state and create or manage the queue state. That is the WLMClient API handles the incoming user commands for submitting workload administration and maintenance reporting and status etc. The WLMClient API makes calls to the appropriate workload management core components to execute the commands.

The workload management core supports logical resources in addition to physical resources to better meet data integration requirements. Physical resources include but are not limited to Central Processing Unit CPU or processor memory disk space and swap space. In addition to physical resources the workload management core allows the user to define logical resources and how to share those logical resources among workloads through resource policies. In certain embodiments a logical resource may be defined as one of many different types including but not limited to 

In certain embodiments a resource policy has a two level hierarchical representation that defines what logical physical or both resources are allocated to that resource policy relative to the system level resources and how those resources are distributed among different queues. With embodiments different queues may share the same resource policy but with different resource distribution. illustrates an example snippet of a workload manager configuration file in accordance with certain embodiments. The workload manager configuration file shows queue class definition WorkloadCount resource and NormalPolicy.

The workload management core provides an asynchronous callback mechanism. With the workload management core a workload is started once the workload has been determined as the next runnable workload. However with the workload management core the user has control over the transition phase between queued and running. In particular if the user needs to perform additional tasks right before workload run with embodiments there is a time window available to do so. For example if disk space is full and a workload may fail due to lack of disk space the user is able to clean up disk before the workload starts. Another example is that a workload may not be runnable so the user may reset the workload i.e. clean up failure state . The workload management core provides flexibility for the user to start the workload when the workload is ready. The asynchronous calls e.g. submit workload check workload end workload may be made by the user to indicate that a transition phase e.g. queued running running completed is ready to occur from an application perspective.

The workload management core provides an interface for the user to integrate a workload scheduler with workload management. One example of a workload scheduler is a workload sequencer which may consist of several workloads that are to be executed in a predefined sequence. With embodiments the workload sequencer is not treated as a workload instead the workloads inside the workload sequencer are treated as separate workloads. The workloads that are ready to run according to the order predefined by the workload sequencer are run. No extra resources need to be reserved making it possible for other workloads to run. The reason this approach can work is because the sequence order may be maintained outside the workload management core because of the asynchronous callback mechanism.

Examples of priority rules include but are not limited to an elapsed time rule a priority weight rule and a workload run ratio rule.

The elapsed time rule treats all workloads as having the same priority and ensures first in first out fairness.

The priority weight group assigns a coefficient to a specific attribute and uses the coefficient to calculate a priority index. In certain embodiments the following attributes may be used to determine a priority weight with example priority weights shown in parenthesis 

In certain embodiments the priority index may be calculated using the formula given below Priority Index PriorityWeight 20 WorkloadQueuedTime

For example assume that the user wants to assign high medium or low priority to three queues respectively. The user then selects the priority weight rule and this rule ensures workloads in the high priority queue get higher priority to run next. The workload management core determines the priority index of a queued workload based on the priority of that workload and the amount of time the workload waits in queue. Then the workload management core picks up the workload with the highest priority index to run next.

The workload run ratio rule defines the order and the number of workloads that may be run across all queues. The following are examples 

The workload management server parses the configuration file which models the system capacity the workload execution policy etc. and looks up system utilization in real time to determine how to optimize the workload execution from different queues. For instance if the system constraint is if the system CPU usage is above 90 do not schedule any workload to execute then when the user is checking whether the user may proceed with workload execution the resource manager looks at the system utilization and configuration information to determine whether the system has capacity to execute this workload or not. If the resources needed by the workload are not available the user will have to check back again.

When the user issues the save configuration command to update the workload management configuration file in terms of classes resources policies and parameters the workload management core performs the following tasks 

Various handlers are described herein. In certain embodiments such handles are part of the server socket .

In block the command handler determines whether this is a begin workload command. If so processing continues to block otherwise processing continues to block . In block the command handler initiates begin workload processing e.g. by creating a begin workload handler .

In block the command handler determines whether this is a check workload command. If so processing continues to block otherwise processing continues to block . In block the command handler initiates check workload processing e.g. by creating a check workload handler .

In block the command handler determines whether this is an end workload command. If so processing continues to block otherwise processing continues to block . In block the command handler initiates end workload processing e.g. by creating an end workload handler .

In block the command handler determines whether this is a save configuration command. If so processing continues to block otherwise processing continues to block . In block the command handler initiates save configuration workload processing e.g. by creating a save configuration handler .

In block the command handler processes the command. In certain embodiments in block the command may be determined to be check class check status of workload get average queued time get configuration list all workloads list active workloads list pending workloads list class move workload between queues or within a queue and remove workload from queue.

In block the submit workload handler determines whether the workload management core is in configuration update mode. If so processing continues to block otherwise processing continues to block . In block the submit workload handler returns an indication that the submit workload command cannot be processed because the workload management core is in configuration update mode.

In block the submit workload handler determines whether the workload management core is in pause mode. In certain embodiments no workload may be submitted when the pause mode is on. If so processing continues to block otherwise processing continues to block . In block the submit workload handler returns an indication that the submit workload command cannot be processed because the workload management core is in pause mode.

In block the submit workload handler creates a workload. In certain embodiments creating a workload creates a workload object. In certain embodiments a workload class is obtained from the workload class name and then the workload is created based on the workload class the command options project name workload name process ID workload type and workload description.

In block the submit workload handler assigns a workload ID to the workload. In block the submit workload handler adds the workload to the workload repository . In block the submit workload handler adds the workload to the queue identified by parsing the submit workload command string. For example the queue may be a low priority queue a medium priority queue or a high priority queue. From block processing continues to block .

In block the submit workload handler optionally persists the current workload management server state to storage e.g. to file or database per the user s selection . In block the submit workload handler increments a global resource counter e.g. a workload counter . In block the submit workload handler returns the workload ID to the socket client .

In block the begin workload handler determines whether the workload management core is in configuration update mode. If so processing continues to block otherwise processing continues to block . In block the begin workload handler returns an indication that the begin workload command cannot be processed because the workload management core is in configuration update mode.

In block the begin workload handler determines whether the workload management core is in pause mode. If so processing continues to block otherwise processing continues to block . In block the begin workload handler returns an indication that the begin workload command cannot be processed because the workload management core is in mode.

In block the begin workload handler obtains a workload class. In block the begin workload handler determines whether the workload is runnable using the operations of . If so processing continues to block otherwise processing is continues to block . In block the begin workload handler returns an indication to the socket client that the workload is not runnable.

In block the begin workload handler creates a managed workload using the workload class. In certain embodiments a managed workload may be described as an object that encapsulates information related to a workload as described in blocks and for the submit workload command. In block the begin workload handler adds the managed workload to the workload repository . In block if needed the begin workload handler moves the managed workload from a pending queue to an active queue. In certain embodiments the managed workload is removed from the pending queue and added to the active queue. That is the managed workload may already be in an active queue e.g. if a check workload command was issued that already moved the managed workload . In block the check workload handler increments a global resource counter. In block the check workload handler returns an indication that the workload is runnable. The begin workload handler may also perform other processing such as bookkeeping.

In certain embodiments the pending queue and the active queue are two internal queues maintained by the workload management server . Other queues may be user defined. The pending queue may be described as a temporary space where a workload stays before the workload is classified by the workload management server . Once classification is done the workload is removed from the pending queue to an associated class queue waiting for the workload to run. When the workload becomes the next runnable workload the workload is moved from the class queue to the active queue. The workloads in the active queue are currently running.

In block the policy engine obtains system CPU usage and system memory usage. In block policy engine determines whether the system CPU usage is greater than the allowed CPU cap. If so processing continues to block otherwise processing continues to block . In block the policy engine returns an indication that the workload is not runnable.

In block the policy engine determines whether the system memory usage is greater than the allowed memory cap. If so processing continues to block otherwise processing continues to block . In block the policy engine returns an indication that the workload is not runnable.

In block the policy engine determines whether the logical resources at the system level are available. If so processing continues to block otherwise processing continues to block . In block the policy engine returns an indication that the workload is not runnable.

In block the policy engine determines whether the logical resources at the queue level are available. If so processing continues to block otherwise processing continues to block . In block the policy engine returns an indication that the workload is not runnable.

In block the check workload handler identifies the workload priority using the operations of . In block the check workload handler checks available resources of the system and the queue using the operations of .

From block processing continues to block . In block the check workload handler determines whether the workload is runnable based on the workload priority and available resources. In certain embodiments the workload is runnable if the workload priority of the workload is the highest priority of the submitted workloads i.e. the has highest priority indicator for the workload is set to true and the resources are available for the workload to be executed. If so processing continues to block otherwise processing continues to block . In block the check workload handler returns an indication to the socket client that the workload is not runnable.

In block if needed the check workload handler moves the managed workload from a pending queue to an active queue. That is the managed workload may already be in an active queue e.g. if a begin workload command was issued followed by a check workload command possibly by different users . In block the check workload handler increments a global resource counter. In block the check workload handler returns an indication that the workload is runnable.

In block the priority manager determines whether an elapsed time priority rule is to be used to determine priority. If so processing continues to block otherwise processing continues to block .

In block the priority manager sets a priority index of each of the workloads that have been submitted equal to an elapsed time for that workload where the elapsed time is the time period since the workload was submitted . In block the priority manager stores the priority index of each of the workloads into a tree map. In block the priority manager sorts the workloads on the tree map by priority index. In block the priority manager selects a workload ID associated with the workload that has the highest priority index. In block the priority manager determines whether the workload to be checked is the workload having the highest priority. If so processing continues to block otherwise processing continues to block . In certain embodiments the priority manager makes this determination by matching the workload ID of the workload to be checked against the workload ID of the workload having the highest priority. In block the priority manager sets the has highest priority indicator for the workload to true. In Block the priority manager sets the has highest priority indicator for the workload to false.

Returning to in block the priority manager determines whether a priority weight rule is to be used to determine priority. If so processing continues to block otherwise processing continues to block .

In block the priority manager represents each priority e.g. high medium low with a priority weight. In block the priority manager calculates the priority index of each of the workloads based on the priority weight of the priority associated with that workload and the elapsed time for that workload. From block processing continues to block .

Returning to in block the priority manager determines whether the workload run ratio rule is to be used to determine priority. If so processing continues to block otherwise processing continues to block .

In block the priority manager determines whether the workload for which the request was received is of medium priority. If so processing continues to block otherwise processing continues to block . In block the priority manager determines whether there are no high or low priority workloads. If so processing continues to block otherwise processing continues to block . In block the priority manager sets a has highest priority indicator for the workload to be true.

In block the priority manager determines whether the high priority workloads exceed the quota for high priority workloads and the medium priority workloads do not exceed the quota for the medium priority workloads. If so processing continues to block otherwise processing continues to block . In block the priority manager sets a has highest priority indicator for the workload to be false.

In block the priority manager priority manager determines whether the workload for which the request was received is of high priority. If so processing continues to block otherwise processing continues to block . In block the priority manager performs high priority processing. In various embodiments any type of high priority processing may be performed. In block the priority manager performs low priority processing. In various embodiments any type of low priority processing may be performed.

In block the save configuration handler saves the global resource counters. In block the save configuration handler updates the configuration in memory. In block the save configuration handler saves the updated configuration to file on disk. From block processing continues to block . In block the save configuration handler loads the saved configuration file from disk into memory. In block the save configuration handler restores the global resource counters.

When the configuration changes there is no need to stop and restart the workload management server . The workload management server continues to work with existing workloads with new queue management specifications.

In control begins at block with saving an initial configuration in response to receiving a save configuration command. In block in response to receiving a submit workload command a workload is created and a workload ID is returned. In block in response to receiving a check workload command it is determined whether the workload is runnable based on a priority of the workload and resources that are available for executing the workload and an indication of whether the workload is runnable is returned. In block in response to receiving a begin workload command it is determined whether the workload is runnable based on resources that are available an indication of whether the workload is runnable is returned and if runnable a managed workload is created. In block in response to receiving an end workload command the workload is deleted.

When the begin workload command is called and returns an indication that the workload is runnable the data integration processing engine executes the workload i.e. performs ETL processing .

Embodiments provide the ability to optimize the workload execution so that users can maximize the system throughput and workload success rate. Embodiments provide support for managing and optimizing the workload execution environment to avoid utilization spikes and troughs frequent workload failures or poor performance due to resource over subscription.

With embodiments if a particular workload or set of workloads need to be executed e.g. to meet an Service Level Agreement SLA but such execution is being prevented or slowed by an OS level workload manager a user is able to modify the workload management settings to allow this workload or set of workloads to be executed.

For the data integration environment embodiments provide a workload management tool with a built in scheduling capability that allows the user to control when to start a runnable workload.

Thus embodiments dynamically manage workloads in an ETL system based on physical and logical resources user specified run schedules user specified priority rules priorities associated with each workload and resource availability.

Embodiments not manage the run time environments of ETL workloads. In particular embodiments manage system resources for ETL workloads by focusing on run time resource distribution. With embodiments there are user specified resource policies and priority rules that focus on how to balance resources among different groups and how to control overall resource utilization on top of all the groups who share the same processing environment but who are working on different projects at the system level. These are user specified resource policies and priority rules describe how system resources should be distributed among workloads at run time.

Embodiments allow the user to control when to actually start a workload after the resources requested have been allocated how to utilize those resources and when to release those resources.

With embodiments physical and logical resources are used to distribute system resources among different groups to help avoid bottlenecks and prevent the system from being overloaded. Logical and physical resources are represented using one resource policy in a two level hierarchical structure making it possible to correlate high level logical resources to low level physical resources. This approach allows the user to directly work with high level easy to understand logical resources while the underlying physical resources are handled automatically by the workload management core .

Embodiments are implemented at a data integration application level in an Symmetric Multiprocessing SMP data integration environment. Workload submission and run is controlled by the user and the workload management core detects whether or not there are enough resources for a given workload. The workload management core then gives the control back to the user to actually start the workload. Because the user has control over when to run a workload workload scheduling is integrated into the workload management server . The workload execution factors include user specified run schedules user specified priority rules and available resources.

It is understood in advance that although this disclosure includes a detailed description on cloud computing implementation of the teachings recited herein are not limited to a cloud computing environment. Rather embodiments of the present invention are capable of being implemented in conjunction with any other type of computing environment now known or later developed.

Cloud computing is a model of service delivery for enabling convenient on demand network access to a shared pool of configurable computing resources e.g. networks network bandwidth servers processing memory storage applications virtual machines and services that can be rapidly provisioned and released with minimal management effort or interaction with a provider of the service. This cloud model may include at least five characteristics at least three service models and at least four deployment models.

On demand self service a cloud consumer can unilaterally provision computing capabilities such as server time and network storage as needed automatically without requiring human interaction with the service s provider.

Broad network access capabilities are available over a network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms e.g. mobile phones laptops and PDAs .

Resource pooling the provider s computing resources are pooled to serve multiple consumers using a multi tenant model with different physical and virtual resources dynamically assigned and reassigned according to demand. There is a sense of location independence in that the consumer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction e.g. country state or datacenter .

Rapid elasticity capabilities can be rapidly and elastically provisioned in some cases automatically to quickly scale out and rapidly released to quickly scale in. To the consumer the capabilities available for provisioning often appear to be unlimited and can be purchased in any quantity at any time.

Measured service cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service e.g. storage processing bandwidth and active user accounts . Resource usage can be monitored controlled and reported providing transparency for both the provider and consumer of the utilized service.

Software as a Service SaaS the capability provided to the consumer is to use the provider s applications running on a cloud infrastructure. The applications are accessible from various client devices through a thin client interface such as a web browser e.g. web based email . The consumer does not manage or control the underlying cloud infrastructure including network servers operating systems storage or even individual application capabilities with the possible exception of limited user specific application configuration settings.

Platform as a Service PaaS the capability provided to the consumer is to deploy onto the cloud infrastructure consumer created or acquired applications created using programming languages and tools supported by the provider. The consumer does not manage or control the underlying cloud infrastructure including networks servers operating systems or storage but has control over the deployed applications and possibly application hosting environment configurations.

Infrastructure as a Service IaaS the capability provided to the consumer is to provision processing storage networks and other fundamental computing resources where the consumer is able to deploy and run arbitrary software which can include operating systems and applications. The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems storage deployed applications and possibly limited control of select networking components e.g. host firewalls .

Private cloud the cloud infrastructure is operated solely for an organization. It may be managed by the organization or a third party and may exist on premises or off premises.

Community cloud the cloud infrastructure is shared by several organizations and supports a specific community that has shared concerns e.g. mission security requirements policy and compliance considerations . It may be managed by the organizations or a third party and may exist on premises or off premises.

Public cloud the cloud infrastructure is made available to the general public or a large industry group and is owned by an organization selling cloud services.

Hybrid cloud the cloud infrastructure is a composition of two or more clouds private community or public that remain unique entities but are bound together by standardized or proprietary technology that enables data and application portability e.g. cloud bursting for load balancing between clouds .

A cloud computing environment is service oriented with a focus on statelessness low coupling modularity and semantic interoperability. At the heart of cloud computing is an infrastructure comprising a network of interconnected nodes.

Referring now to a schematic of an example of a cloud computing node is shown. Cloud computing node is only one example of a suitable cloud computing node and is not intended to suggest any limitation as to the scope of use or functionality of embodiments of the invention described herein. Regardless cloud computing node is capable of being implemented and or performing any of the functionality set forth hereinabove.

In cloud computing node there is a computer system server which is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use with computer system server include but are not limited to personal computer systems server computer systems thin clients thick clients handheld or laptop devices multiprocessor systems microprocessor based systems set top boxes programmable consumer electronics network PCs minicomputer systems mainframe computer systems and distributed cloud computing environments that include any of the above systems or devices and the like.

Computer system server may be described in the general context of computer system executable instructions such as program modules being executed by a computer system. Generally program modules may include routines programs objects components logic data structures and so on that perform particular tasks or implement particular abstract data types. Computer system server may be practiced in distributed cloud computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed cloud computing environment program modules may be located in both local and remote computer system storage media including memory storage devices.

As shown in computer system server in cloud computing node is shown in the form of a general purpose computing device. The components of computer system server may include but are not limited to one or more processors or processing units a system memory and a bus that couples various system components including system memory to processor .

Bus represents one or more of any of several types of bus structures including a memory bus or memory controller a peripheral bus an accelerated graphics port and a processor or local bus using any of a variety of bus architectures. By way of example and not limitation such architectures include Industry Standard Architecture ISA bus Micro Channel Architecture MCA bus Enhanced ISA EISA bus Video Electronics Standards Association VESA local bus and Peripheral Component Interconnects PCI bus.

Computer system server typically includes a variety of computer system readable media. Such media may be any available media that is accessible by computer system server and it includes both volatile and non volatile media removable and non removable media.

System memory can include computer system readable media in the form of volatile memory such as random access memory RAM and or cache memory . Computer system server may further include other removable non removable volatile non volatile computer system storage media. By way of example only storage system can be provided for reading from and writing to a non removable non volatile magnetic media not shown and typically called a hard drive . Although not shown a magnetic disk drive for reading from and writing to a removable non volatile magnetic disk e.g. a floppy disk and an optical disk drive for reading from or writing to a removable non volatile optical disk such as a CD ROM DVD ROM or other optical media can be provided. In such instances each can be connected to bus by one or more data media interfaces. As will be further depicted and described below memory may include at least one program product having a set e.g. at least one of program modules that are configured to carry out the functions of embodiments of the invention.

Program utility having a set at least one of program modules may be stored in memory by way of example and not limitation as well as an operating system one or more application programs other program modules and program data. Each of the operating system one or more application programs other program modules and program data or some combination thereof may include an implementation of a networking environment. Program modules generally carry out the functions and or methodologies of embodiments of the invention as described herein.

Computer system server may also communicate with one or more external devices such as a keyboard a pointing device a display etc. one or more devices that enable a user to interact with computer system server and or any devices e.g. network card modem etc. that enable computer system server to communicate with one or more other computing devices. Such communication can occur via Input Output I O interfaces . Still yet computer system server can communicate with one or more networks such as a local area network LAN a general wide area network WAN and or a public network e.g. the Internet via network adapter . As depicted network adapter communicates with the other components of computer system server via bus . It should be understood that although not shown other hardware and or software components could be used in conjunction with computer system server . Examples include but are not limited to microcode device drivers redundant processing units external disk drive arrays RAID systems tape drives and data archival storage systems etc.

Referring now to illustrative cloud computing environment is depicted. As shown cloud computing environment comprises one or more cloud computing nodes with which local computing devices used by cloud consumers such as for example personal digital assistant PDA or cellular telephone A desktop computer B laptop computer C and or automobile computer system N may communicate. Nodes may communicate with one another. They may be grouped not shown physically or virtually in one or more networks such as Private Community Public or Hybrid clouds as described hereinabove or a combination thereof. This allows cloud computing environment to offer infrastructure platforms and or software as services for which a cloud consumer does not need to maintain resources on a local computing device. It is understood that the types of computing devices A N shown in are intended to be illustrative only and that computing nodes and cloud computing environment can communicate with any type of computerized device over any type of network and or network addressable connection e.g. using a web browser .

Referring now to a set of functional abstraction layers provided by cloud computing environment is shown. It should be understood in advance that the components layers and functions shown in are intended to be illustrative only and embodiments of the invention are not limited thereto. As depicted the following layers and corresponding functions are provided 

Hardware and software layer includes hardware and software components. Examples of hardware components include mainframes in one example IBM zSeries systems RISC Reduced Instruction Set Computer architecture based servers in one example IBM pSeries systems IBM xSeries systems IBM BladeCenter systems storage devices networks and networking components. Examples of software components include network application server software in one example IBM WebSphere application server software and database software in one example IBM DB2 database software. IBM zSeries pSeries xSeries BladeCenter WebSphere and DB2 are trademarks of International Business Machines Corporation registered in many jurisdictions worldwide .

Virtualization layer provides an abstraction layer from which the following examples of virtual entities may be provided virtual servers virtual storage virtual networks including virtual private networks virtual applications and operating systems and virtual clients.

In one example management layer may provide the functions described below. Resource provisioning provides dynamic procurement of computing resources and other resources that are utilized to perform tasks within the cloud computing environment. Metering and Pricing provide cost tracking as resources are utilized within the cloud computing environment and billing or invoicing for consumption of these resources. In one example these resources may comprise application software licenses. Security provides identity verification for cloud consumers and tasks as well as protection for data and other resources. User portal provides access to the cloud computing environment for consumers and system administrators. Service level management provides cloud computing resource allocation and management such that required service levels are met. Service Level Agreement SLA planning and fulfillment provide pre arrangement for and procurement of cloud computing resources for which a future requirement is anticipated in accordance with an SLA.

Workloads layer provides examples of functionality for which the cloud computing environment may be utilized. Examples of workloads and functions which may be provided from this layer include mapping and navigation software development and lifecycle management virtual classroom education delivery data analytics processing transaction processing and workload management processing.

Thus in certain embodiments software or a program implementing workload management processing in accordance with embodiments described herein is provided as a service in a cloud environment.

In certain embodiments the workload management server has the architecture of computing node . In certain embodiments the workload management server is part of a cloud environment. In certain alternative embodiments the workload management server is not part of a cloud environment.

In certain embodiments the a data integration processing engine executes on a node in the cloud environment. In certain alternative embodiments the a data integration processing engine is not part of the cloud environment.

As will be appreciated by one skilled in the art aspects of the present invention may be embodied as a system method or computer program product. Accordingly aspects of the present invention may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may all generally be referred to herein as a circuit module or system. Furthermore aspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium s having computer readable program code embodied thereon.

Any combination of one or more computer readable medium s may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the computer readable storage medium would include the following an electrical connection having one or more wires a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device solid state memory magnetic tape or any suitable combination of the foregoing. In the context of this document a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system apparatus or device.

A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein for example in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms including but not limited to electro magnetic optical or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate propagate or transport a program for use by or in connection with an instruction execution system apparatus or device.

Program code embodied on a computer readable medium may be transmitted using any appropriate medium including but not limited to wireless wireline optical fiber cable RF etc. or any suitable combination of the foregoing.

Computer program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The program code may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider .

Aspects of the embodiments of the invention are described below with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable medium that can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function act specified in the flowchart and or block diagram block or blocks.

The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational processing e.g. operations or steps to be performed on the computer other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

The code implementing the described operations may further be implemented in hardware logic or circuitry e.g. an integrated circuit chip Programmable Gate Array PGA Application Specific Integrated Circuit ASIC etc. The hardware logic may be coupled to a processor to perform operations.

Devices that are in communication with each other need not be in continuous communication with each other unless expressly specified otherwise. In addition devices that are in communication with each other may communicate directly or indirectly through one or more intermediaries.

A description of an embodiment with several components in communication with each other does not imply that all such components are required. On the contrary a variety of optional components are described to illustrate the wide variety of possible embodiments of the present invention.

Further although process steps method steps algorithms or the like may be described in a sequential order such processes methods and algorithms may be configured to work in alternate orders. In other words any sequence or order of steps that may be described does not necessarily indicate a requirement that the steps be performed in that order. The steps of processes described herein may be performed in any order practical. Further some steps may be performed simultaneously.

When a single device or article is described herein it will be readily apparent that more than one device article whether or not they cooperate may be used in place of a single device article. Similarly where more than one device or article is described herein whether or not they cooperate it will be readily apparent that a single device article may be used in place of the more than one device or article or a different number of devices articles may be used instead of the shown number of devices or programs. The functionality and or the features of a device may be alternatively embodied by one or more other devices which are not explicitly described as having such functionality features. Thus other embodiments of the present invention need not include the device itself.

The illustrated operations of the flow diagrams show certain events occurring in a certain order. In alternative embodiments certain operations may be performed in a different order modified or removed. Moreover operations may be added to the above described logic and still conform to the described embodiments. Further operations described herein may occur sequentially or certain operations may be processed in parallel. Yet further operations may be performed by a single processing unit or by distributed processing units.

The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. As used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will be further understood that the terms comprises and or comprising when used in this specification specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

The terms an embodiment embodiment embodiments the embodiment the embodiments one or more embodiments some embodiments and one embodiment mean one or more but not all embodiments of the present invention s unless expressly specified otherwise.

The terms including comprising having and variations thereof mean including but not limited to unless expressly specified otherwise.

The enumerated listing of items does not imply that any or all of the items are mutually exclusive unless expressly specified otherwise.

The corresponding structures materials acts and equivalents of all means or step plus function elements in the claims below are intended to include any structure material or act for performing the function in combination with other claimed elements as specifically claimed. The description of embodiments of the present invention has been presented for purposes of illustration and description but is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the invention. The embodiments were chosen and described in order to best explain the principles of the invention and the practical application and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.

The flowchart and block diagrams in the figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present invention. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of code which comprises one or more executable instructions for implementing the specified logical function s . It should also be noted that in some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or combinations of special purpose hardware and computer instructions.

The foregoing description of embodiments of the invention has been presented for the purposes of illustration and description. It is not intended to be exhaustive or to limit the embodiments to the precise form disclosed. Many modifications and variations are possible in light of the above teaching. It is intended that the scope of the embodiments be limited not by this detailed description but rather by the claims appended hereto. The above specification examples and data provide a complete description of the manufacture and use of the composition of the embodiments. Since many embodiments may be made without departing from the spirit and scope of the invention the embodiments reside in the claims hereinafter appended or any subsequently filed claims and their equivalents.

