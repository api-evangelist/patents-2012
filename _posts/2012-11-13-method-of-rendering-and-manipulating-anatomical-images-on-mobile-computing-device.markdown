---

title: Method of rendering and manipulating anatomical images on mobile computing device
abstract: Methods are provided for rendering an image of an anatomical object on a display of a mobile computing device. Sensors of the mobile computing device are interrogated to determine an orientation, or a change in orientation, of the computing device. Transformations are determined for rotating and positioning the image of the anatomical object such that the image appears to be stationary after a change in the orientation of the mobile computing device. Additional image data associated with a surgical tool or a surgical plan may also be rendered on the device for planning and/or training simulations.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08933935&OS=08933935&RS=08933935
owner: 7D Surgical Inc.
number: 08933935
owner_city: Toronto
owner_country: CA
publication_date: 20121113
---
This application claims priority to U.S. Provisional Application No. 61 558 039 titled METHOD OF RENDERING AND MANIPULATING ANATOMICAL IMAGES ON MOBILE COMPUTING DEVICE and filed on Nov. 10 2011 the entire contents of which are incorporated herein by reference.

Simulation is a useful tool in medical training and in preparation for medical procedures. For example simulation can be used for training students in a professional discipline on the different techniques related to that discipline. Similarly simulation can also be used to assist experienced professionals in planning or testing out different scenarios that would either be infeasible or outright dangerous to do in the field.

The use of simulations by professionals in the medical field is generally restricted to training medical students and planning complex procedures. The advantage of simulations is that they allow the surgeon to practice and plan techniques such as pedicle screw placement for spinal surgery in advance of the actual procedure thereby reducing risk to the patient. Due to recent advances in imaging technology accurate three dimensional models of the anatomy can be presented by the simulator to the surgeon. The surgeon is then able to utilize pre defined anatomical atlases or actual patient data for planning the surgical procedure.

A disadvantage to current medical simulation systems is the lack of natural interaction with the simulated anatomical structure. Existing simulation software often consists of nothing more than a standard desktop software package whereby all interactions are done through a keyboard and or mouse. More complex simulation systems are available that have the ability to accurately simulate one particular surgical technique for training and planning. However these systems require specialized and costly hardware and lack the flexibility of a purely software solution.

Methods are provided for rendering and manipulating an image of an anatomical object on a display of a mobile computing device. Sensors of the mobile computing device are interrogated to determine an orientation or a change in orientation of the computing device. Transformations are determined for rotating and positioning the image of the anatomical object such that the image appears to be stationary after a change in the orientation of the mobile computing device. Additional image data associated with a surgical tool or a surgical plan may also be rendered on the device for planning and or training simulations.

In one embodiment the user is able to interact with the model by placing screws on the surface of the model. This process is intended to simulate the process that a surgeon would go through when performing a surgery. Through a number of different interaction modalities such as but not limited to touch gestures and motion the user can determine both the entry point and trajectory of the screw for implantation. The user is also able to select a number of different screws where the geometry of the screw can be optimized for the ideal implantation of the mobile computing device. Example implementations are provided to describe how the present embodiments can be used for surgical planning and training.

Accordingly in one aspect there is provided a computer implemented method of dynamically displaying an anatomical object on a mobile computing device the mobile computing device including sensors for determining an orientation of the mobile computing device and a display for displaying the anatomical object the method comprising the steps of a obtaining three dimensional image data associated with the anatomical object b determining a virtual camera vector defining a distance between a virtual camera and the anatomical object upon rendering of the anatomical object c determining a reference vector associated with the anatomical object such that when rendering the anatomical object on the display the reference vector is aligned with a gravity vector when the mobile computing device is oriented in a reference orientation d interrogating the sensors of the mobile computing device and determining an orientation of the mobile computing device e determining a transformation such that the reference vector is aligned with the gravity vector and such that the anatomical object is perceived as residing within the display at a location corresponding to the virtual camera vector f applying the transformation to three dimensional image data and obtaining transformed three dimensional image data such that when rendered on the display the anatomical object image appears to be stationary after a change in the orientation of the mobile computing device and g rendering an image of the anatomical object on the display of the mobile computing device according to the transformed three dimensional image data.

In another aspect there is provided a computer implemented method of dynamically displaying an anatomical object on a mobile computing device the mobile computing device including sensors for determining an orientation of the mobile computing device and a display for displaying the anatomical object the method comprising the steps of a obtaining three dimensional image data associated with the anatomical object b interrogating the sensors of the mobile computing device and determining a change in an orientation of the mobile computing device c rendering an image of the anatomical object on the display such that the anatomical object image appears to be stationary after the change in the orientation of the mobile computing device.

In another aspect there is provided a computer readable storage medium comprising instructions for dynamically displaying an anatomical object on a mobile computing device the mobile computing device including sensors for determining an orientation of the mobile computing device and a display for displaying the anatomical object wherein execution of the instructions by one or more processors causes the one or more processors to carry out the steps of a obtaining three dimensional image data associated with the anatomical object b determining a virtual camera vector defining a distance between a virtual camera and the anatomical object upon rendering of the anatomical object c determining a reference vector associated with the anatomical object such that when rendering the anatomical object on the display the reference vector is aligned with a gravity vector when the mobile computing device is oriented in a reference orientation d interrogating the sensors of the mobile computing device and determining an orientation of the mobile computing device e determining a transformation such that the reference vector is aligned with the gravity vector and such that the anatomical object is perceived as residing within the display at a location corresponding to the virtual camera vector f applying the transformation to three dimensional image data and obtaining transformed three dimensional image data such that when rendered on the display the anatomical object image appears to be stationary after a change in the orientation of the mobile computing device and g rendering an image of the anatomical object on the display of the mobile computing device according to the transformed three dimensional image data.

A further understanding of the functional and advantageous aspects of the disclosure can be realized by reference to the following detailed description and drawings.

Various embodiments and aspects of the disclosure will be described with reference to details discussed below. The following description and drawings are illustrative of the disclosure and are not to be construed as limiting the disclosure. Numerous specific details are described to provide a thorough understanding of various embodiments of the present disclosure. However in certain instances well known or conventional details are not described in order to provide a concise discussion of embodiments of the present disclosure.

As used herein the terms comprises and comprising are to be construed as being inclusive and open ended and not exclusive. Specifically when used in the specification and claims the terms comprises and comprising and variations thereof mean the specified features steps or components are included. These terms are not to be interpreted to exclude the presence of other features steps or components.

As used herein the term exemplary means serving as an example instance or illustration and should not be construed as preferred or advantageous over other configurations disclosed herein.

As used herein the terms about and approximately when used in conjunction with ranges of dimensions of particles compositions of mixtures or other physical properties or characteristics are meant to cover slight variations that may exist in the upper and lower limits of the ranges of dimensions so as to not exclude embodiments where on average most of the dimensions are satisfied but where statistically dimensions may exist outside this region. It is not the intention to exclude embodiments such as these from the present disclosure. The following terms used in this description have the following meanings 

As used herein the term orientation refers to the angular position of a mobile computing device with respect to a global coordinate frame.

As used herein the term anatomical refers to any portion of a living entity. For example anatomical may refer to a portion of the human anatomy where portion of the human anatomy may include a portion of a tissue organ and or cavity.

As used herein the term transformation refers to a linear or nonlinear mapping of R R. A subset of transformations known as affine transformations represents the set of all linear mappings of R Rsuch that the resulting transformation is a rotation scaling and or shearing of an object and an optional translation. An affine transform is represented as a 4 4 matrix such that the fourth column represents a translation. Affine transformations matrices can be multiplied together to produce a chain of transformations.

As used herein the term quaternion refers to a four dimensional mathematical entity that represents the space of all rotations in R. For example a single normalized quaternion having a magnitude of 1 represents a rotation transform through some angle and axis about the origin of the current coordinate system. Multiplication of n 1 quaternions represents a series of rotations about the origin of the current coordinate system. Quaternions can be converted into the rotational component of an affine transformation matrix and mixed with other affine transformations.

As used herein the term rendering refers to the process of constructing an image from image data such as three dimensional data. The rendering process can occur completely in hardware completely in software or a combination of both hardware and software.

As used herein the term virtual camera refers to a position in three dimensional space from which a synthetic viewpoint is generated. A virtual camera may be employed to specify how three dimensional anatomical image information should by displayed to a user upon rendering.

Embodiments of the present disclosure provide systems and methods for the rendering and displaying of a three dimensional 3D anatomical object on a display of a mobile computing device where an image of the 3D anatomical object is rendered in a dynamic manner that is dependent on the orientation of the mobile computing device. The orientation of the mobile computing device may be employed to vary the perceived view of the 3D anatomical object. By changing the orientation of the mobile computing device the orientation of the displayed 3D anatomical object moves responsively such that from the perspective of the user the 3D anatomical object appears to be floating in front of them. In selected embodiments described further below the user is able to manipulate the spatial orientation of the displayed 3D anatomical object through additional input to the mobile computing device.

Embodiments of the present disclosure therefore employ mobile devices with sensors for determining an orientation of the device which can enable a more intuitive interaction of the user with anatomical and or patient data. A mobile device configured according to embodiments presented below may act as a standalone simulation device optionally providing a planning system or act as an extension to augment traditional simulation and or planning methods. As mobile computing devices become ubiquitous further benefits exist for practitioners of medicine to plan surgeries or review plans on the go without having to be physically situated at a desktop workstation.

Virtual camera represents the viewpoint of a user operating mobile computing device . While 3D anatomical object resides on display of mobile computing device virtual camera is virtually located at a position external to that of mobile computing device and the position and orientation of virtual camera in global reference frame do not change when the orientation of mobile computing device is changed. Virtual camera is thus effectively located on the surface of display such that the distance between virtual camera and object determines how far within the display a user perceives object .

The final view presented to the user is created by issuing specific commands to the graphics application programming interface API provided by the operating system running on the mobile device . When given the relative position of the camera to the object affine transformations are determined that will present the user with the aforementioned view. The rendering is generated by a graphics API based on the parameters determined to place the camera and object in their appropriate positions.

The mobile computing device resides in the same global reference frame denoted by coordinate system while an orientation of mobile computing device is specified by device reference frame . The orientation of device reference frame relative to that of global reference frame may be determined by interrogating sensors that reside on or within mobile computing device . For example output from such sensors may be employed to provide a determination of the orientation of device reference frame relative to gravity vector .

As shown in the virtual camera is defined by its distance from object and by a down vector perpendicular to the camera s line of sight and located in the same plane as that of the gravity vector . The model is rotated and positioned by finding a transform that maps mobile computing device local coordinate system to the global coordinate system . In one embodiment this is accomplished by transforming the gravity vector such that its direction matches that of the virtual camera down vector . This is done for two reasons. First relative to the device the direction of gravity is not constant to that of the device and therefore its global coordinate frame. Second the convention used in computer graphics is that the camera location is fixed at the origin of the coordinate system and the world is positioned relative to the camera.

As shown in mobile computing device includes a processing unit CPU in communication with a mass memory via a bus . Mobile computing device also includes a power supply a display and sensors . Mobile computing device may also include an audio device interface video interface a keypad an input output interface a global positioning systems GPS receiver a hard disk drive or other internal storage device an a network interface .

Display may be a liquid crystal display LCD gas plasma light emitting diode LED or any other type of display used with a computing device. Display may also include a touch sensitive screen arranged to receive input from an object such as a stylus or a digit from a human hand.

Sensors may include a sufficient number and or type of sensors to determine the direction of the gravity vector relative to mobile computing device . The sensors may include any one or more of a gyroscope accelerometer and or digital compass and a combination thereof.

The sensors may also provide sufficient information to determine the orientation of the mobile computing device relative to the gravity vector including a relative angle about a yaw axis. For example the sensors may provide only the direction of gravity as a vector or as gravity and relative rotation as a quaternion. In the latter case the yaw information is already included in the quaternion and an application running on the mobile device does not have to generate a quaternion from the gravity vector.

In one example implementation the orientation of the device is derived from an onboard 3 axis accelerometer. To the device gravity appears as a constant acceleration in a given direction as viewed from the device s coordinate system .

In one example embodiment a three axis accelerometer is employed to provide information associated with the orientation of the mobile computing device. In such an embodiment the acceleration due to gravity may be separated from other sources of acceleration i.e. movement. For example a low pass filter may be employed to separate the acceleration of the device due motion from the force applied due to gravity. For example to produce suitably smooth motion so that small tremors in the user s hands are not noticeable the cutoff frequency of the filter can be close to DC a suitable example filter cutoff is about 5 10 Hz .

Such low pass filtering can introduce a noticeable lag between when the device is moved and when the anatomical object moves. If the device has another sensor such as a gyroscope the method may include utilizing the information from that sensor to stabilize the accelerometer data without the need for a low pass filter. This substantially eliminates the lag and makes the motion of the anatomical object more responsive to user input.

In one example implementation the sensors include a three axis gyroscope for determining the orientation and integrated motion yielding position of the computing device relative to that of the global reference frame. This can create the effect that an anatomical object s position is kept static in the global reference frame. When the computing device is moved away from the anatomical object the object s size shrinks accordingly.

In another embodiment sensors may provide output signals for determining both relative motion and relative orientation of mobile computing device . In one example implementation a three axis accelerometer may be employed to detect relative motion for moving the displayed image of the anatomical object accordingly.

Power supply provides power to mobile computing device . A rechargeable or non rechargeable battery may be used to provide power. The power may also be provided by an external power source such as an AC adapter or a powered docking cradle that supplements and or recharges a battery.

Mobile computing device may optionally communicate with a base station not shown or directly with another computing device. Network interface includes circuitry for coupling mobile computing device to one or more networks and is constructed for use with one or more communication protocols and technologies including but not limited to global system for mobile communication GSM code division multiple access CDMA time division multiple access TDMA user datagram protocol UDP transmission control protocol Internet protocol TCP IP SMS general packet radio service GPRS WAP ultra wide band UWB IEEE 802.16 Worldwide Interoperability for Microwave Access WiMax SIP RTP Bluetooth infrared Wi Fi Zigbee or any of a variety of other wireless communication protocols. Network interface is sometimes known as a transceiver transceiving device or network interface card NIC .

Audio interface is arranged to produce and receive audio signals. For example audio interface may be coupled to a speaker and microphone not shown to enable telecommunication with others and or generate an audio acknowledgement for some action.

Video interface is arranged to capture video images such as a still photo a video segment an infrared video or the like. For example video interface may be coupled to a digital video camera a web camera or the like. Video interface may comprise a lens an image sensor and other electronics. Image sensors may include a complementary metal oxide semiconductor CMOS integrated circuit charge coupled device CCD or any other integrated circuit for sensing light.

Keypad may comprise any input device arranged to receive input from a user. For example keypad may include a push button numeric dial or a keyboard. Keypad may also include command buttons that are associated with selecting and sending images.

Mobile computing device also includes input output interface for communicating with external devices such as a headset or other input or output devices not shown in . Input output interface can utilize one or more communication technologies such as USB infrared Bluetooth Wi Fi Zigbee or the like. Haptic interface is arranged to provide tactile feedback to a user of the mobile computing device.

Optional GPS transceiver can determine the physical coordinates of mobile computing device on the surface of the Earth which typically outputs a location as latitude and longitude values. GPS transceiver can also employ other geo positioning mechanisms including but not limited to triangulation assisted OPS AGPS E OTD CI SAI ETA BSS or the like to further determine the physical location of mobile computing device on the surface of the Earth. It is understood that under different conditions GPS transceiver can determine a physical location within millimeters for mobile computing device and in other cases the determined physical location may be less precise such as within a meter or significantly greater distances. In one embodiment however a mobile computing device may through other components provide other information that may be employed to determine a physical location of the device including for example a MAC address IP address or the like. In another embodiment GPS transceiver may be employed to determine the relative translation of the device reference frame relative to global reference frame .

In one embodiment GPS transceiver may operate with one or more other components of mobile computing device to connect to a network to provide location information to another computing device. It should be noted that where the user s configuration includes a GPS or other location detection device that is separate from mobile computing device then that device may also include in one embodiment an ability to connect to a network to provide location information to another computing device.

Mass memory includes a RAM a ROM and other storage means. Mass memory illustrates another example of computer storage media for storage of information such as computer readable instructions data structures program modules or other data. Mass memory stores a basic input output system BIOS for controlling low level operation of mobile computing device . The mass memory also stores an operating system for controlling the operation of mobile computing device . It will be appreciated that this component may include a general purpose operating system such as a version of UNIX or LINUX or a specialized client communication operating system such as iOS Android Windows Mobile or the Symbian operating system. The operating system may include or interface with a Java virtual machine module that enables control of hardware components and or operating system operations via Java application programs.

Memory further includes one or more data storage which can be utilized by mobile computing device to store among other things applications and or other data. For example data storage may also be employed to store information that describes various capabilities of mobile computing device . The information may then be provided to another device based on any of a variety of events including being sent as part of a header during a communication sent upon request or the like. Moreover data storage may also be employed to store personal information including but not limited to address lists contact lists personal preferences or the like.

In one embodiment data storage may be configured to store information associated with the rendering of 3D anatomical images. For example data storage may store 3D anatomical object data which may be stored in mesh form as described above . As described below data storage may also store computer readable instructions for carrying out the methods described herein. At least a portion of the information may also be stored on a disk drive or other storage medium within mobile computing device such as hard disk drive or an external storage device or the like. In one embodiment a portion of the information may also be located remote to mobile computing device .

Applications may include computer executable instructions which when executed by mobile computing device perform the methods described herein for the dynamic rendering of 3D anatomical images. In one embodiment applications may include graphical user interface for facilitating user interaction and image processing and rendering application for processing 3D anatomical object data and sensor data and rendering the 3D anatomical object on display . The steps performed to process the sensor data and render the 3D anatomical object image data is described in detail below.

Embodiments of the disclosure can be implemented via the microprocessor s and or the memory. For example the functionalities described above can be partially implemented via hardware logic in the microprocessor s and partially using the instructions stored in the memory. Some embodiments are implemented using the microprocessor s without additional instructions stored in the memory. Some embodiments are implemented using the instructions stored in the memory for execution by one or more general purpose microprocessor s . Thus the disclosure is not limited to a specific configuration of hardware and or software.

While some embodiments can be implemented in fully functioning computers and computer systems various embodiments are capable of being distributed as a computing product in a variety of forms and are capable of being applied regardless of the particular type of machine or computer readable media used to actually effect the distribution.

At least some aspects disclosed can be embodied at least in part in software. That is the techniques may be carried out in a computer system or other data processing system in response to its processor such as a microprocessor executing sequences of instructions contained in a memory such as ROM volatile RAM non volatile memory cache or a remote storage device.

A computer readable storage medium can be used to store software and data which when executed by a data processing system causes the system to perform various methods. The executable software and data may be stored in various places including for example ROM volatile RAM nonvolatile memory and or cache. Portions of this software and or data may be stored in any one of these storage devices.

As noted above mobile computing device is programmed such that the rendering of the 3D anatomical object is dependent on the orientation of mobile computing device . The process of rotating the object according to one embodiment is demonstrated in the example flow chart shown in . Upon initialization the appropriate mobile computing device motion orientation sensors are enabled in step . Such sensors may include but are not limited to gyroscopes and accelerometers as described above. The orientation of mobile computing device is then determined in step such that the relative orientation of device reference frame and global reference frame is determined at least in part. With this available information the method then determines a transformation in step that will orient the anatomical object such that reference vector associated with the object is aligned with the measured gravity vector . In one example the pre defined vector associated with the object is selected such that the pre defined vector is aligned with the gravity vector when the display is positioned in a horizontal orientation.

After rotation a transformation is found in step that will position the camera a fixed distance away from the anatomical object. Finally the two transformations an orientational transformation and a translational transformation are applied to the model either serially or as a net transformation. This process repeats as long as the application receives data from the sensors.

In one example implementation mobile computing device determines the relative orientation of the device reference frame as follows. Sensor data is initially employed to determine the direction of the normalized gravity vector g g g in the device reference frame. In the simplest case gravity information can be obtained through the onboard accelerometer. The observed acceleration in the device coordinate system is recorded for each of the major axes. As shown in this can be interpreted as three time varying signals along the x y and z axes. The process for obtaining gravity given this information is outlined in . When a sample is generated each axis is filtered with a low pass filter such that only slow varying signals are accepted. When combined into a vector the new sample represents the gravity vector as expressed in the mobile device s own coordinate system . Since only the direction is necessary the vector is normalized to remove the effects on magnitude that the filtering may have had.

With the orientation of the gravity vector within the device reference frame known an affine transformation R is determined in step such that the application of the transformation R to the anatomical object will rotate the reference vector to align with the gravity vector. As noted above the reference vector is a reference vector associated with the orientation of the anatomical object within the coordinate frame of the mobile computing device such that the reference vector is initially aligned with the gravity vector when the mobile computing device is positioned in a reference orientation. In one example the reference vector is given in the coordinate frame of the mobile computing device as 0 0 1 such that the reference orientation is a horizontal orientation. The affine transform R may be obtained by finding the quaternion q that represents the rotation of to . shows an example of how the gravity direction is used to obtain a transform. Given the direction of gravity two items are necessary to generate the transformation the angle between the two vectors and the axis of rotation . From this data a suitable transformation can be found and used to rotation the anatomical object .

A specific example of the determination of suitable transform R or equivalently quaternion q is shown in . In the figure the device is shown oriented to the world coordinate system . In this case the gravity vector will always point in direction of the negative z axis. The reference vector is chosen in this example such that it will always point into the screen regardless of the direction of the handedness of the coordinate system. In this example the device coordinate system is assumed to be left handed so that the z axis comes out of the screen.

To correctly orient the anatomical model the transform is chosen such that if applied to the gravity vector the gravity vector would be rotated onto the reference vector as the gravity vector is variable with respect to the device. To obtain this transform two items are necessary the angle between the gravity vector and the reference and the axis of rotation . In the following derivation all vectors have been normalized so that their lengths are unity. The order in which the two items are calculated is not important. The angle is obtained by arccos and the axis is obtained by

Given the angle and axis a quaternion q can be constructed. In one representation a quaternion is a 4 tuple such that q x y z w . In another representation a quaternion is a combination of a vector and a scalar such that q s. The vector component of the rotation quaternion can be expressed as sin 2 and the scalar component can be expressed as cos 2 To ensure that the quaternion is rotated it is normalized so that its magnitude is equal to unity. The normalized quaternion can then be expressed as a rotation matrix R using 

To demonstrate this method consider the trivial case where reference vector and gravity vector are aligned such that 0 0 1 . In this case the dot or inner product will be 1 and the corresponding angle between the two vectors is 0. Similarly the cross product between the two identical vectors will be 0 0 0 . The quaternion generated from this rotational axis and angle will be 0 0 0 1 when represented by a 4 tuple. As a rotational transformation matrix the quaternion is represented as 

To position the virtual camera an affine transform T is determined in step such that if the camera is located at the origin of the global reference frame with coordinates 0 0 0 then the anatomical object is located a distance d from the camera such that its position is 0 0 d . The distance d is chosen based on a number of factors such as a radius radii of the anatomical object s being viewed or how close the user wishes the camera to be to the object. The sign of d depends on the handedness of the coordinate system. If the positive z axis is coming out of the screen d must be negative otherwise it must be positive.

In one embodiment the distance d is prescribed automatically by the application and need not be provided by the user. For instance if the anatomical model has a radius of r units then a suitable value of d may be chosen such that the object is completely visible on the display surface . One method to accomplish this is to choose d so that

The two transforms are concatenated to produce a new transform H in step such that H RT. This transform is applied in step to the anatomical object to give the appearance that the anatomical object is stationary while the user manipulates the mobile computing device . provides an example of how an object is transformed. In the object is located at the origin of the coordinate system . Should the object not be located at the origin the average value or centroid of the object vertices is first subtracted to center the object prior to performing the procedure. This represents the default position and orientation of the object regardless of the current device orientation. In the object is translated along the z axis based on the value of d in step . In this example d is a positive value and it is moved along the positive z axis. In the object is rotated based on the rotation transform R.

The transformation procedure may be applied to anatomical object either serially or in parallel. provides an example of transforming a mesh through a serial process. Given a set of vertices the three dimensional coordinate of a vertex is first converted into its homogenous equivalent such that it s new coordinate is x y z 1 . An affine transform is applied to the vertex to produce a transformed vertex position. If there are more vertices then the process repeats until all vertices in the object have been transformed. After the affine transformation has been applied to all vertices the object is rendered .

To transform the object using a parallel method a 4 N matrix where N is the number of vertices in the object is constructed such that each column in the matrix contains the homogeneous representation of each vertex 

The preceding steps may be repeated to provide real time updating of the rendered orientation of the anatomical object in response to the changing orientation of the mobile computing device. As shown in step each time the anatomical object image is rendered the motion and orientation sensors are sampled and the process of obtaining the transform is performed.

In another embodiment using the motion information obtained from the accelerometer and gyroscope the size of the anatomical object can be changed. When the mobile computing device is first used its current position in space is recorded. By moving the mobile computing device towards and way from the apparent location of the anatomical object the recorded acceleration can be used to determine how much closer or father away the camera is from the object thus updating the value of distance d. When combined with the orientation information it can be used to simulate a stationary object in the user s field of view in effect fixing its position in space relative the user.

The process of transforming the rendering of the anatomical image via the present method enables a user to interact with the virtual anatomical image in a dynamic manner where the anatomical object appears stationary and the user effectively rotates around the anatomical object. The user is thus given a sense that the anatomical object is floating in a space behind the screen of the mobile computing device and that they are observing it through a window .

Prior to rendering a user may be presented through the user interface with a number of different preprocessed anatomical objects or models for selection. Anatomical object imaging data may be stored within memory of mobile computing device or obtained from an external source such as through a network connection. For example the mobile computing device may communicate with or access a remote server and securely transmit anatomical object image data onto the medical mobile computing device. Once an anatomical object has been selected and rendered on the display according to the above steps the user can manipulate the display of the anatomical object in a number of ways. The anatomical object data may be provided based on measurements of a real anatomical object or may be artificial or simulated data.

In one embodiment three dimensional data obtained from performing an imaging modality is first converted into a format that can be read and or processed by mobile computing device such as a digital imaging and communications in medicine DICOM image. The data may originate from a number of imaging modalities such as but not limited to magnetic resonance imaging MRI fluoroscopy computed tomography CT positron emission tomography PET ultrasound US optical coherence tomography OCT and single photon emission computed tomography SPECT scans. The data may also be segmented into smaller models or retained as a single large model. After the conversion the data is transmitted and stored on the mobile computing device or storage media for future use.

It is to be understood that the embodiments disclosed herein are not limited to merely viewing rendered anatomical object image data but may also be employed for interacting with it the anatomical object virtually for surgical training and or planning. To this extent different input modalities available to the mobile computing device may be utilized. Examples of the available inputs include touch sensitive regions not just limited to the display orientation and motion sensors and cameras capable of capturing low medium and high resolution video. Any or all of these inputs may be utilized to manipulate the rendering of stored data according to various embodiments disclosed herein.

In one embodiment the aforementioned methods may be employed for surgical training and or preparation. As discussed in further detail below the mobile computing device may also contain or obtain from a remote server or storage location a collection of three dimensional models of surgical devices that may be available to the user at the time of surgery. The surgical devices may be surgical implants or fastening devices such as screws. After having processed and co rendered the 3D surgical device object image data the user may interact with both the anatomical object and the surgical device using the mobile computing device.

In one example the surgical device may be a screw and the mobile computing device may be employed for training and or surgical planning. After screw placement is complete the user may be able to view the screw placement in relation to both the original imaging data and the processed 3D model to allow for an assessment of the positioning based upon the user s training and experience.

For example the aforementioned embodiments may be utilized for spinal surgery training or preparation. A generic or anonymous skeletal model may be stored on the mobile computing device for training purposes. A collection of standard screw sizes may also be stored on the mobile computing device. Pedicle screw volumes that could potentially injure or kill a patient for example the placement of a pedicle screw that intersects the spinal may also be stored within the model data.

The user interacts with generic model data and places the screw using standard surgical methods. This process can be repeated multiple times for as many screws that need to be inserted for the full implementation of the surgical construct. If the screw passes through a marked volume such as the spinal canal the user is alerted to the intrusion through audio visual and haptic feedback. After placement is complete the mobile computing device determines the accuracy of the placement by comparing the entry point and angle to a list of entry points and angles appropriate for the model. The user receives feedback on the overall accuracy and the number of surgical incidents where a screw could have resulted in surgical complications.

An example method of the virtual placement of a screw is presented in . In step an input coordinate is first registered that indicates the location of the input in the global coordinate system . This coordinate can be a result of an automatic process such as being derived from the orientation of the device or from a direct input by the user. In one embodiment given the orientation of the camera which is determined as described above a point on the surface of the object is found in step such that the point is a projection of onto the surface of the object. With the entry point fixed the entry angle of the screw is obtained . In one example the screw orientation may be determined such that the direction of the screw is equal to the direction of the camera the screw may also be displayed in such a way to make the entry point easier to observe e.g. translucent . In another example the entry point and angle may be determined through touch based interaction or through another form of user input. The depth of the screw is obtained in a similar manner in that through either a touch or gestural input the user is able to specify how deep the screw will penetrate the particular anatomical object. The screw parameters are stored for later use in step .

When used for training the mobile computing devices and methods disclosed herein can provide feedback to the user in a number of ways such as but not limited to visual indicators audible warnings or haptic feedback. In one embodiment when used for training surgeons in surgical screw placement the user must be taught in what regions the screw can and cannot be inserted. presents a description of a method to ensure a screw will produce patient complications. Each vertebral body is defined via boundary data that outlines forbidden regions where if an interventional mobile computing device e.g. pedicle screw were to intersect this region patient complications would occur. Given a convex hull for example but not limited to a bounding box around the screw each mesh comprising the complete object is compared to the hull surrounding the screw. An intersection test is performed between the screw and the mesh . Should an intersection occur a notification is presented that the screw is in a region that would harm the patient . Should no intersections be detected it is assumed that the screw has been placed without inducing any patient complications.

As skeletal models could be processed to yield extra training data information about proper screw placement could also be stored alongside these anatomical models. After placement is complete the angle of entry may be compared to a list of known angles for the particular model. The user s screw placement may be then graded based on how close their placement angle was to the optimal angle for the particular model.

Screw placement is done in a similar manner to the one used for training. In the user manipulates the mobile computing device as before. The screws are placed into the three dimensional representation of the data obtained from the patient. This is termed the perspective view . In the representation of the data is shown as a projection onto one of the major coordinate planes for example but not limited to the x y plane. The user can manipulate this view through touch and gestures . This is termed the orthographic view and replicates the user looking at a conventional medical imaging representation. This treatment planning information e.g. pedicle screw entry point screw trajectory could then be stored and transmitted for use during the actual surgery in combination with surgical navigation or robotic navigation technology.

When developing a surgical plan a surgeon must be able to simulate what will be done during the planned surgical procedure with sufficient precision. To that end the mobile computing device may utilize a front facing camera to assist with planning. Through simple gestures the user can perform the actions described in Surgical Training and Planning without physically touching the touch screen. This allows more of the object to be visualized on the interface screen such that the user themself does not obscure their view of the data. In one example embodiment three dimensional surgical plan data associated with a surgical plan may be provided and rendered on the display of the mobile computing device such that a relative orientation of the three dimensional surgical plan data and the three dimensional image data is preserved when rendering the image of the surgical plan.

The process of obtaining is described in . First a full colour RGB red green blue image is obtained from the front facing camera . The image is then converted from colour to intensity such that each pixel only assumes one value. In one embodiment the pixel values are integers on the range of 0 I 255 where in another embodiment the pixel values are real numbers on the range of 0.0 I 1.0. The image is blurred with a low pass filter to remove noise. In one embodiment this may be a Gaussian low pass filter. After blurring a morphological closing is applied to only retain large scale structure in the image. A threshold value is chosen by examining the statistics of the image and choosing a value T such that for any pixel I x y in the image a new image is created where I x y 0 if I x y 

The proposed tracking method is only one such method for gestural input. Other methods could also be used to obtain the position of the thumb or other fingers in a similar manner. In one embodiment optical flow is used to estimate the motion of pixels over time such the moving objects such as the thumb are separating from stationary objects such as the background. In another embodiment texture and edge information are used to estimate what pixels in the image constitute the background and what pixels constitute the thumb as to the camera the thumb appears as a feature less dark area. In another embodiment machine identifiable markets such as coloured stickers may be applied to one or more fingers of the user s hand for tracking purposes. Alternatively image based foreground background segmentation may be employed to determine the location of a user s thumb or finger. In other embodiments feature point tracking or template tracking in which fiducial markers would be utilized may be employed. The specific embodiments described above have been shown by way of example and it should be understood that these embodiments may be susceptible to various modifications and alternative forms. It should be further understood that the claims are not intended to be limited to the particular forms disclosed but rather to cover all modifications equivalents and alternatives falling within the spirit and scope of this disclosure.

