---

title: Update mask for handling interaction between fills and updates
abstract: A multi core processor implements a cash coherency protocol in which probe messages are address-ordered on a probe channel while responses are un-ordered on a response channel. When a first core generates a read of an address that misses in the first core's cache, a line fill is initiated. If a second core is writing the same address, the second core generates an update on the addressed ordered probe channel. The second core's update may arrive before or after the first core's line fill returns. If the update arrived before the fill returned, a mask is maintained to indicate which portions of the line were modified by the update so that the late arriving line fill only modifies portions of the line that were unaffected by the earlier-arriving update.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09251073&OS=09251073&RS=09251073
owner: Intel Corporation
number: 09251073
owner_city: Santa Clara
owner_country: US
publication_date: 20121231
---
This invention was made with Government support under contract number H98230 11 3 0011 awarded by the Department of Defense. The Government has certain rights in this invention.

The present disclosure relates to cache architecture in a processor and in particular for a write update protocol extension to a channel based cache coherency protocol for handling interaction between fills and updates arriving out of order.

In a shared memory multiprocessor system with a separate cache memory for each processor or core cache coherency must be maintained to ensure that changes in the values of shared operands are propagated throughout the coherency system to maintain the consistency of data stored in local caches of a shared resource. As more cores are integrated on a socket the number of outstanding requests that the memory hierarchy and coherence system has to handle also grows because each tag directory entry needs to maintain and track each additional core. A cache coherency protocol may be implemented to maintain the property that all reads to a cache obtain the latest value for all of the caches in a distributed shared memory system.

Embodiments of disclosed subject matter pertain to the use of scalable cache protocols in a multiprocessor shared memory system. In some embodiments disclosed scalable cache coherence protocols are suitable for use in conjunction with moderate to large scale shared memory multiprocessors.

At least one embodiment of a scalable cache coherency protocol referred to herein as a hierarchical scalable protocol HSP is suitable for use in a shared memory multiprocessor because embodiments of the protocol possess one or more of the following characteristics free of dead locks i.e. every request is processed eventually free of live locks i.e. progress can be made from any state that the system generates fairness with respect to each core e.g. no core is starved of cache space low occupancy i.e. messages are processed quickly enough to prevent excessive queuing low resource overhead i.e. required resources do not scale linearly with the number of cores efficiency e.g. the number of messages required to complete transactions and structural scalability the number of operations and bookkeeping resources are not strictly dependent on the number of cores. While disclosed embodiments employ an HSP other embodiments may employ different coherency protocols.

In at least one embodiment an HSP processes all coherence ordering points atomically and maintains an ordered chain of all uncompleted requests to a particular address. The ordered chain enables coherency logic to complete processing of requests from multiple accessors in the ordered determined by the applicable coherence ordering point. In some embodiments hierarchy is used in the organization of tag directories to reduce data communication latency and provide a scalable representation of data storage.

In at least one embodiment an HSP includes a mechanism to track a final value of a cache line in the specific case of a read request co existing with a forwarded write update involving the same cache line. A line fill may arrive before or after subsequent updates. In the case of the fill arriving after subsequent updates the late arriving fill must not overwrite updated data in the cache.

In some embodiments a scalable coherency protocol for use in conjunction with a multiprocessor may include any one or more of the following features non blocking tag directory processing an address ordered probe channel maintaining a line in a temporary state following a fill until the line validated after all outstanding coherence actions complete last accessor managed tag directory entries that limit the number of probes that could possibly match an outstanding entry in a miss address file MAF and use of single ownership to manage writes in a coherent manner.

In at least one embodiment the coherency protocol implements a write update extension that allows writes to be performed by sending the write data to all copies of the data and updating them. A write update protocol extension will save energy and allow more time to perform the same communication. It is to be noted that while a HSP has been exemplified in this disclosure other scalable cache coherency protocols with similar attributes may be applicable.

At least one embodiment pertains to a method of updating a mask for handling interaction between fills and updates arriving out of order. Some embodiments of the method implement by a write update extension. In response to detecting a read request from a first entity referencing a specific memory address that misses in a core cache of the first entity an order marker corresponding to the read request to the first entity is sent on a first channel. In some embodiments a fill corresponding to the read request may be sent on a second channel. The order marker may be sent on an address ordered probe channel while the fill may be sent on an un ordered response channel. In some embodiments a bit in an entry of a MAF may indicate that the read request order marker has arrived.

Some embodiments may respond to detection of an update request referencing a specific memory address ordered at a tag directory after the read request by sending update data upd packets on the first channel to all non last accessor entities having copies of the specific memory address. Additionally some embodiments may send a last accessor probe update data packet on the first channel if the last accessor entity is different than the second entity. All probe channel packets are processed immediately upon arrival thereby probe channels never stall due to coherency dependencies. Lastly in response to the update request referencing a specific memory address an update acknowledgement upd ack is sent to the second entity.

In some embodiments the update request sets update mask bits in a corresponding entry of the MAF. The update mask bit may indicate portion of a cache line corresponding to a specific memory address that was updated. Once an update mask has been created and the read request order marker arrives before the probe request a fill is prevented from overwriting the updated data. In at least one embodiment the write update extension allows the fill to be first merged with prior updates and then written selectively to the non updated portions of the cache line. Upon updating of the cache line some embodiments send an update acknowledgement upd ack to the first entity that initiated the update request if a probe request has been recorded during the probe processing phase of the MAF entry.

In at least one embodiment a processor includes multiple cores each containing a front end unit an execution unit a core cache and storage to record information for a miss address file. In some embodiments the processor includes an uncore region that includes a last level cache and a cache controller containing a tag directory.

In some embodiments the processor is operable to perform a novel write update protocol extension by an update mask to handle interaction between fills and updates arriving out of order. Embodiments of the processor may be operable to respond to detecting a read request referencing a memory address from a first entity that misses in a core cache of the first entity by sending an order marker corresponding to the read request to the first entity on a first channel and sending a fill corresponding to the read request to the first entity on a second channel.

Additionally some embodiments of the processor respond to detecting an update request referencing the same memory address but ordered after the read request by sending update data upd packets on the first channel to all non last accessor entities having copies of the memory address sending a last accessor update upd LA packet on the first channel if the last accessor entity is different than the second entity and also sending an update acknowledgement upd ack to the second entity.

At least one embodiment of the processor allows the update request to set update mask bits in a corresponding MAF entry. In some embodiments the write update extension allows a fill to be first merged with prior updates and then write selectively to the non updated portions of the cache line. Upon updating the cache line an update acknowledgement upd ack is sent to the first entity that initiated the update request if a probe request has been recorded during the probe processing phase of the MAF entry.

In at least one embodiment a disclosed multiprocessor system includes a plurality of multicore processors a last level cache and a cache controller with a tag directory. In some embodiments each core includes a front end unit an execution unit a core cache and a miss address file MAF . Embodiments of the cache controller maintain a tag directory which may keep track of all the sharers and this bit vector increases by one bit per core as the number of cores increases but only stores one location for the last accessor which does not increase by increasing the number of cores.

In some embodiments the system is operable to perform a novel write update protocol extension by an update mask for handling interaction between fills and updates arriving out of order. The system in response to detection of a read request referencing a specific memory address from a first entity that misses in a core cache of the first entity sends an order marker corresponding to the read request to the first entity on a first channel and sends a fill corresponding to the read request to the first entity on a second channel.

In at least one embodiment the system is operable to detecting an update request referencing a specific memory address ordered after the read request and respond by sending update data upd packets on the first channel to all non last accessor entities having copies of the specific memory address sending last accessor update upd LA packets on the first channel if the last accessor entity is different than the second entity and also send an update acknowledgement upd ack to the second entity.

In some embodiments the system allows an update request to set updated mask bits in a corresponding MAF entry. The write update extension allows the fill to be first merged with prior updates and then write selectively to the non updated portions of the cache line. Upon updating of the cache line an update acknowledgement upd ack is sent to the first entity that initiated the update request if a probe request has been recorded during the probe processing phase of the MAF entry.

In the following description details are set forth by way of example to facilitate discussion of the disclosed subject matter. It should be apparent to a person of ordinary skill in the field however that the disclosed embodiments are exemplary and not exhaustive of all possible embodiments.

Throughout this disclosure a hyphenated form of a reference numeral refers to a specific instance of an element and the un hyphenated form of the reference numeral refers to the element generically or collectively. Thus for example widget 12 1 refers to an instance of a widget class which may be referred to collectively as widgets 12 and any one of which may be referred to generically as a widget 12.

Embodiments may be implemented in many different types of systems and platforms. Referring now to a block diagram of selected elements of a multiprocessor system in accordance with an embodiment of the present disclosure. shows a system in which a first multi core processor a second multi core processor memory and input output devices are interconnected by a number of point to point P P interfaces as will be described in further detail. However in other embodiments not shown in the multiprocessor system may employ different bus architectures such as a front side bus a multi drop bus and or another implementation. Although two processors are depicted in the example embodiment of for descriptive clarity in various embodiments a different number of processors may be employed using elements of the depicted architecture.

As shown in processor system is a point to point interconnect system and includes processors and whose internal components are individually referenced using like element numbers ending in and respectively. As shown in processor is a multi core processor including first core and second core . It is noted that other elements of processor besides cores may be referred to as an uncore. In different embodiments not shown in a varying number of cores may be present in a particular processor. Cores may comprise a number of sub elements not shown in also referred to as clusters that provide different aspects of overall functionality. For example cores may each include a memory cluster not shown in that may comprise one or more levels of cache memory. Other clusters not shown in in cores may include a front end cluster and an execution cluster.

In particular embodiments first core and second core within processor are not equipped with direct means of communication with each other but rather communicate via cache controller which may include intelligent functionality such as cache control data queuing P P protocols and multi core interfacing. Cache controller may thus represent an intelligent uncore controller that interconnects cores with memory controller hub MCH last level cache memory LLC and P P interfaces . In particular to improve performance in such an architecture cache controller functionality within cache controller may enable selective caching of data within a cache hierarchy including LLC and one or more caches present in cores . As shown in cache controller includes memory management unit MMU that handles access to virtual memory addresses and maintains translation lookaside buffers TLB not shown in for improved performance with regard to memory access.

In LLC may be coupled to a pair of processor cores respectively. For example LLC may be shared by core and core . LLC may be fully shared such that any single one of cores may fill or access the full storage capacity of LLC . Additionally MCH may provide for direct access by processor to memory via memory interface . For example memory may be a double data rate DDR type dynamic random access memory DRAM while memory interface and MCH comply with a DDR interface specification. Memory may represent a bank of memory interfaces or slots that may be populated with corresponding memory circuits for a desired DRAM capacity.

Also in processor may also communicate with other elements of the processor system such as near hub and far hub which are also collectively referred to as a chipset that supports processor . P P interface may be used by processor to communicate with near hub via interconnect link . In certain embodiments P P interfaces and interconnect link are implemented using Intel QuickPath Interconnect architecture.

As shown in near hub includes interface to couple near hub with first bus which may support high performance I O with corresponding bus devices such as graphics and or other bus devices. Graphics may represent a high performance graphics engine that outputs to a display device not shown in . In one embodiment first bus is a Peripheral Component Interconnect PCI bus such as a PCI Express PCIe bus and or another computer expansion bus. Near hub may also be coupled to far hub at interface via interconnect link . In certain embodiments interface is referred to as a south bridge. Far hub may provide I O interconnections for various computer system peripheral devices and interfaces and may provide backward compatibility with legacy computer system peripheral devices and interfaces. Thus far hub is shown providing network interface and audio I O as well as providing interfaces to second bus third bus and fourth bus as will be described in further detail.

Second bus may support expanded functionality for a processor system with I O devices and may be a PCI type computer bus. Third bus may be a peripheral bus for end user consumer devices represented by desktop devices and communication devices which may include various types of keyboards computer mice communication devices data storage devices bus expansion devices etc. In certain embodiments third bus represents a Universal Serial Bus USB or similar peripheral interconnect bus. Fourth bus may represent a computer interface bus for connecting mass storage devices such as hard disk drives optical drives disk arrays which are generically represented by persistent storage shown including OS that may be executable by processor .

The embodiment of system emphasizes a computer system that incorporates various features that facilitate handheld or tablet type of operation and other features that facilitate laptop or desktop operation. In addition the embodiment of system includes features that cooperate to aggressively conserve power while simultaneously reducing latency associated with traditional power conservation states.

The embodiment of system includes an operating system that may be entirely or partially stored in a persistent storage . Operating system may include various modules application programming interfaces and the like that expose to varying degrees various hardware and software features of system . The embodiment of system includes for example a sensor application programming interface API a resume module a connect module and a touchscreen user interface . System as depicted in may further include various hardware firm features include a capacitive or resistive touch screen controller and a second source of persistent storage such as a solid state drive .

Sensor API provides application program access to one or more sensors not depicted that may be included in system . Examples of sensors that system might have include as examples an accelerometer a global positioning system GPS device a gyro meter an inclinometer and a light sensor. The resume module may be implemented as software that when executed performs operations for reducing latency when transition system from a power conservation state to an operating state. Resume module may work in conjunction with the solid state drive SSD to reduce the amount of SSD storage required when system enters a power conservation mode. Resume module may for example flush standby and temporary memory pages before transitioning to a sleep mode. By reducing the amount of system memory space that system is required to preserve upon entering a low power state resume module beneficially reduces the amount of time required to perform the transition from the low power state to an operating state. The connect module may include software instructions that when executed perform complementary functions for conserving power while reducing the amount of latency or delay associated with traditional wake up sequences. For example connect module may periodically update certain dynamic applications including as examples email and social network applications so that when system wakes from a low power mode the applications that are often most likely to require refreshing are up to date. The touchscreen user interface supports a touchscreen controller that enables user input via touchscreens traditionally reserved for handheld applications. In the embodiment the inclusion of touchscreen support in conjunction with support for communication devices and the enable system to provide features traditionally found in dedicated tablet devices as well as features found in dedicated laptop and desktop type systems.

Referring now to a block diagram of selected elements of processor is shown. Processor may be a multi core processor including a plurality of processor cores. In processor is shown including first core and second core whose internal components are individually referenced using like element numbers ending in and respectively. It is noted that other elements of processor besides cores may be referred to as the uncore region . Although two cores are depicted in the example embodiment of for descriptive clarity in various embodiments a different number of cores may be employed using elements of the depicted architecture. Cores may comprise a number of sub elements also referred to as clusters that provide different aspects of overall functionality. For example cores may each include front end execution engine core memory and miss address file MAF . This may be considered the core region of the processor.

In front end may be responsible for fetching instruction bytes and decoding those instruction bytes into micro operations that execution engine and or core memory consume. Thus front end may be responsible for ensuring that a steady stream of micro operations is fed to execution engine and or core memory . Execution engine may be responsible for scheduling and executing micro operations and may include buffers for reordering micro operations and a number of execution ports not shown in . Core memory may include multiple levels of a cache hierarchy. Specifically as shown in core memory may include a core cache . MAF may be responsible for keeping track of pending misses.

In particular embodiments first core and second core within processor are not equipped with direct means of communicating with each other but rather communicate via cache controller which may include intelligent functionality such as cache control data queuing P P protocols and multi core interfacing. Cache controller may include tag directory which may keep track of all the sharers. Cache controller may thus represent an intelligent uncore controller that interconnects cores with LLC . This may be considered the uncore region of the processor.

As shown in processor includes a last level cache LLC which may be a higher level cache that operates in conjunction with core cache . Thus core cache and LLC may represent a cache hierarchy. During operation memory requests from execution engine may first access core cache before looking up any other caches within a system. In the embodiment shown in core cache may be a final lookup point for each core before a request is issued to LLC which is a shared cache among cores .

Referring now to a representation of an exemplary chaining of requests for a specific memory address among multiple cores in a cache coherency protocol. Hierarchical scalable protocol HSP may utilize chaining of requests and atomicity. Chaining may require some additional fields to be added to the MAF structure that manages all outstanding requests from cores . Atomicity at the tag directory means that processing at the tag directory runs ahead of the processing of the core . The tag directory may keep track of all the sharers and this bit vector increases by one bit per core as we increase the number of cores but only stores one location for the last accessor which does not increase by increasing the number of cores. The tag directory stores a plurality of tracking information for specific memory addresses the last accessor and the associated CV vector including one bit per core used to indicate which core s include a copy of the data.

Referring now to a block diagram representative of a path of communication for first access in a cache coherency protocol in the case that a request that misses in the core cache and must go to the memory . A hierarchy of two levels TD 1 and TD 2 where the first level may track all the cores in a domain and the second level tracks all the tag directories of a plurality of first levels. TD 1 may be associated with the LLC of a processor while TD 2 may be associated with a global LLC . A hierarchy may require the request to make two hops to go to memory since the first hop indicates the data is not present in the originating core domain and the second hop indicates the data is not present in the multi core domain and . Each of the tag directories may be smaller than the size of a traditional flat directory and therefore access latency is less.

Referring now to a block diagram of the implementation of the cache coherency protocol using channels to process messages. The message channels and cores are depicted in . VCReq may carry all requests VCResp may carry all responses and acknowledgements and VcPrb may carry all probes and invalidation messages. While VcReq and VcResp have no ordering constraints VcPrb may be required to maintain an address ordering which means that all messages for a particular address travel in order from one source to one destination.

Referring now to a block diagram of selected elements of an embodiment of a tag directory in a last level cache LLC . LLC may contain status information tag directories and data for a plurality of specific memory addresses . Status information contained in the LLC may include but is not limited to the coherency status . Tag directory information may include but is not limited to domain state DS in progress IP tag field last accessor last accessor state and core valid vector . Tag field specifies the particular line being described by this entry. Last accessor keeps track of the last entity to touch this line while last accessor state keeps track of the state of the last entity. The core valid vector is a bit vector that denotes all the sharers of the line.

Disclosed method is initiated by an outstanding read request to a specific memory address. In event sequence detection is made of the arrival of an update packet for a specific memory address M on a first channel wherein the MAF is indicative of an outstanding read request to the M . In event sequence determination is made whether the read request order marker has arrived from the status bit in MAF entry. This is indicative that the update packet was ordered after the read request and must be applied to the value obtained by the read. Next in event sequence the write update is performed to the cache line corresponding to M in all caches that share the cache line. The probe information is saved to the MAF. In event sequence an update acknowledgement is sent from each associated core to the originating entity of the update request. Event sequence describes the update mask bits being set to indicate portions of the cache line corresponding to a specific memory address M .

Referring now to a block diagram representative of an update request across multiprocessors. The update request travels in the request channel to the corresponding TD 1 . The domain state field is examined and indicates that there are other copies in other domains. The update request packet then continues on to TD 2. After winning arbitration at TD 2 using first come first served method of arbitration an order marker is sent back to the domain of the update request and a probe update is sent to the domain of the last accessor. The last accessor state of the TD 2 entry is changed to point to the domain of the update request. The order marker returns to TD 1 and sends an update to the other copy in the domain C2 and then changes the last accessor in TD 1 to point to the core that issued the update request and continues the order marker towards the core that issued the update request. Once the order marker returns to the originating core the update is applied to the cache. The probe update arrives at the other domains TD1 and sends a probe update to the last accessor core in that domain and sends an update packet to any other core in that domain with a copy.

After each core performs its update an update acknowledgement is sent back to the originating core. Once the originator receives all of the update acknowledgement packets and the order marker packet the state is transitioned to the O state. It is to be noted that any core in the O state receiving an update will transition to S state and the originating core will transition to the O state.

Referring to a block diagram representative of the ordering of multiple updates to the same specific memory address is performed at the tag directories is illustrated. Update request from the O state version of the line wins arbitration first at TD 2 and results in an order marker returning towards the domain of the O state line as well as an update packet towards the other domain. It is to be noted that the O state line must be the last accessor. The update request from the S state line wins arbitration at TD 2 after the first update request and follows the order marker with a probe update towards the last accessor and an order marker back to its own domain after the update packet. All updates arrive in coherent order determined by the tag directories and can be applied to any cache in that order.

Referring now to a representation for simulation emulation and fabrication of a design implementing the disclosed techniques. Data representing a design may represent the design in a number of manners. First as is useful in simulations the hardware may be represented using a hardware description language or another functional description language which essentially provides a computerized model of how the designed hardware is expected to perform. The hardware model may be stored in a storage medium such as a computer memory so that the model may be simulated using simulation software that applies a particular test suite to the hardware model to determine if it indeed functions as intended. In some embodiments the simulation software is not recorded captured or contained in the medium.

Additionally a circuit level model with logic and or transistor gates may be produced at some stages of the design process. This model may be similarly simulated sometimes by dedicated hardware simulators that form the model using programmable logic. This type of simulation taken a degree further may be an emulation technique. In any case re configurable hardware is another embodiment that may involve a tangible machine readable medium storing a model employing the disclosed techniques.

Furthermore most designs at some stage reach a level of data representing the physical placement of various devices in the hardware model. In the case where conventional semiconductor fabrication techniques are used the data representing the hardware model may be the data specifying the presence or absence of various features on different mask layers for masks used to produce the integrated circuit. Again this data representing the integrated circuit embodies the techniques disclosed in that the circuitry or logic in the data can be simulated or fabricated to perform these techniques.

In any representation of the design the data may be stored in any form of a tangible machine readable medium. An optical or electrical wave modulated or otherwise generated to transmit such information a memory or a magnetic or optical storage such as a disc may be the tangible machine readable medium. Any of these mediums may carry the design information. The term carry e.g. a tangible machine readable medium carrying information thus covers information stored on a storage device or information encoded or modulated into or on to a carrier wave. The set of bits describing the design or the particular part of the design are when embodied in a machine readable medium such as a carrier or storage medium an article that may be sold in and of itself or used by others for further design or fabrication.

To the maximum extent allowed by law the scope of the present disclosure is to be determined by the broadest permissible interpretation of the following claims and their equivalents and shall not be restricted or limited to the specific embodiments described in the foregoing detailed description.

