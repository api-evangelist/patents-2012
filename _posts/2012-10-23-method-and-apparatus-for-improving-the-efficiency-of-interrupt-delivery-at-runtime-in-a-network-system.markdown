---

title: Method and apparatus for improving the efficiency of interrupt delivery at runtime in a network system
abstract: Processor affinity of an application/thread may be used to deliver an interrupt caused by the application/thread to a best processor at runtime. The processor to which the interrupt is delivered may either run the target application/thread or be located in the same socket as the processor that runs the target application/thread. The processor affinity of the application/thread may be pushed down at runtime to a network device, a chipset, a memory control hub (“MCH”), or an input/output hub (“IOH”), which will facilitate delivery of the interrupt using that affinity information.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08838864&OS=08838864&RS=08838864
owner: Intel Corporation
number: 08838864
owner_city: Santa Clara
owner_country: US
publication_date: 20121023
---
This application is a continuation of U.S. patent application Ser. No. 11 771 209 filed on Jun. 29 2007 now U.S. Pat. No. 8 296 490 which is herein incorporated by reference in its entirety.

This disclosure relates generally to computer network systems and more specifically but not exclusively to technologies for improving the efficiency of interrupt delivery at runtime in a network system.

In a computer network system an application or a thread of the application may be scheduled by the Operating System OS at runtime onto any processor or processing core hereinafter the term processor will be used to refer to a processor or a processing core depending on the utilization of different processors in the system at that moment and on the OS scheduling policy. Typically the OS runtime thread scheduling policy prefers a processor from a socket which has more than one processor available for thread rescheduling. However a device interrupt is typically delivered to a processor which is pinned down by a static configuration or to all the processors in the system using a round robin scheme. That is a device interrupt might not be delivered to a processor where the application thread that the interrupt is targeted for target application thread runs or to a socket where the processor is located. When this occurs there is mismatch between an interrupt and a processor that the interrupt is delivered to. Such mismatches when occur frequently enough degrade the system performance greatly because of cache thrashing between interrupt processing and the target application thread cost of inter processor locking inter processor interrupts other system software overheads and etc. For some applications e.g. UDP User Datagram Protocol applications where the interrupt rate is high the rate of such mismatch is also high which result in more performance degradation. Therefore it is desirable to improve the efficiency of interrupt delivery at runtime in a network system.

According to embodiments of the subject matter disclosed in this application processor affinity of an application thread may be used to deliver an interrupt to a best processor at runtime. The processor to which the interrupt is delivered may either run the target application thread or be located in the same socket as the processor that runs the target application thread. In one embodiment the processor affinity of the application thread may be pushed down to a network device at runtime and the network device will deliver an interrupt using that affinity information. In another embodiment the processor affinity of the application thread may be pushed down to a chipset a memory control hub MCH or an input output hub IOH which will use the affinity information to deliver an interrupt to a best processor. There may be other embodiments according to the spirit of the subject matter disclosed in this application.

Reference in the specification to one embodiment or an embodiment of the disclosed subject matter means that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the disclosed subject matter. Thus the appearances of the phrase in one embodiment appearing in various places throughout the specification are not necessarily all referring to the same embodiment.

Each processor A B . . . N may be a coprocessor. In an embodiment one or more processors A B . . . N may perform substantially the same functions. Each processor may be electronically coupled to a system motherboard through a socket. Two or more processors may share a socket. For example processor A and B may share a socket while processor N may has its own socket . When two or more processors share a socket they may also share a common cache.

System may additionally comprise memory . Memory may store machine executable instructions that are capable of being executed and or data capable of being accessed operated upon and or manipulated. Machine executable instructions as referred to herein relate to expressions which may be understood by one or more machines for performing one or more logical operations. For example machine executable instructions may comprise instructions which are interpretable by a processor compiler for executing one or more operations on one or more data objects. However this is merely an example of machine executable instructions and embodiments of the present invention are not limited in this respect. Memory may for example comprise read only mass storage random access computer accessible memory and or one or more other types of machine accessible memories.

Chipset may comprise one or more integrated circuit chips such as those selected from integrated circuit chipsets commercially available from Intel Corporation e.g. graphics memory and controller hub chipsets although other one or more integrated circuit chips may also or alternatively be used. According to an embodiment chipset may comprise an input output control hub ICH and a memory control hub MCH although embodiments of the invention are not limited by this. Chipset may comprise a host bridge hub that may couple processor A B . . . N and host memory to each other and to local bus . Chipset may communicate with memory via memory bus and with host processor via system bus . In alternative embodiments host processor and host memory may be coupled directly to bus rather than via chipset .

Local bus may be coupled to a circuit card slot having a bus connector not shown . Local bus may comprise a bus that complies with the Peripheral Component Interconnect PCI Local Bus Specification Revision 3.0 Feb. 3 2004 available from the PCI Special Interest Group Portland Oreg. U.S.A. hereinafter referred to as a PCI bus . Alternatively for example bus may comprise a bus that complies with the PCI Express Base Specification Revision 1.1 Mar. 28 2005 also available from the PCI Special Interest Group hereinafter referred to as a PCI Express bus . Bus may comprise other types and configurations of bus systems. System bus may comprise a frond side bus FSB a links based point to point connection system or other types of interconnection systems.

System may additionally comprise one or more network interfaces only one shown . A network interface as referred to herein relates to a device which may be coupled to a communication medium to transmit data to and or receive data from other devices coupled to the communication medium i.e. to send and receive network traffic. For example a network interface may transmit packets to and or receive packets from devices coupled to a network such as a local area network. As used herein a packet means a sequence of one or more symbols and or values that may be encoded by one or more signals transmitted from at least one sender to at least one receiver. Such a network interface may communicate with other devices according to any one of several data communication formats such as for example communication formats according to versions of IEEE Institute of Electrical and Electronics Engineers Std. 802.3 CSMA CD Access Method 2002 Edition IEEE Std. 802.11 LAN MAN Wireless LANS 1999 Edition IEEE Std. 802.16 2003 and 2004 Editions LAN MAN Broadband Wireless LANS Universal Serial Bus Firewire asynchronous transfer mode ATM synchronous optical network SONET or synchronous digital hierarchy SDH standards.

In an embodiment network interface may reside on system motherboard . In another embodiment network interface may be integrated onto chipset . Yet in another embodiment network interface may instead be comprised in a circuit card e.g. NIC or network interface card that may be inserted into circuit card slot . Circuit card slot may comprise for example a PCI expansion slot that comprises a PCI bus connector not shown . PCI bus connector not shown may be electrically and mechanically mated with a PCI bus connector not shown that is comprised in circuit card . Circuit card slot and circuit card may be constructed to permit circuit card to be inserted into circuit card slot . When circuit card is inserted into circuit card slot PCI bus connectors not shown may become electrically and mechanically coupled to each other. When PCI bus connectors not shown are so coupled to each other logic in circuit card may become electrically coupled to system bus .

System may comprise logic . Logic may comprise hardware software or a combination of hardware and software e.g. firmware . For example logic may comprise circuitry i.e. one or more circuits to perform operations described herein. For example logic may comprise one or more digital circuits one or more analog circuits one or more state machines programmable logic and or one or more ASIC s Application Specific Integrated Circuits . Logic may be hardwired to perform the one or more operations. Alternatively or additionally logic may be embodied in machine executable instructions stored in a memory such as memory to perform these operations. Alternatively or additionally logic may be embodied in firmware. Logic may be comprised in various components of system including network interface chipset one or more processors A B . . . N and or on motherboard . Logic may be used to perform various functions by various components according to embodiments of the subject matter disclosed in the present application.

System may comprise more than one and other types of memories buses processors and network interfaces. For example system may comprise a server having multiple processors A B . . . N and multiple network interfaces . Processors A B . . . N memory and busses may be comprised in a single circuit board such as for example a system motherboard but embodiments of the invention are not limited in this respect.

In network one or more of the nodes A . . . N may comprise one or more intermediate stations such as for example one or more hubs switches and or routers additionally or alternatively one or more of the nodes A . . . N may comprise one or more end stations. Also additionally or alternatively network may comprise one or more not shown intermediate stations and medium may communicatively couple together at least some of the nodes A . . . N and one or more of these intermediate stations. Of course many alternatives are possible.

Packet buffer may include multiple buffers and each buffer may store at least one ingress packet received from a network. Packet buffer may store packets received by network interface that are queued for processing at least by device driver operating system intermediate driver transmit queues Tx queues A N and or applications .

Receive queues may include input queues and output queues. Input queues may be used to transfer descriptors from a processor e.g. A a memory e.g. or other storage coupled to the processor e.g. a cache of the processor to one or more network interfaces e.g. network interface . A descriptor may be transferred to a single network interface. A descriptor may describe a location within a buffer and length of the buffer that is available to store an ingress packet. Output queues may be used to transfer return descriptors from any of network interfaces to a processor a memory or other storage coupled to the processor. A return descriptor may describe the buffer in which a particular ingress packet is stored within packet buffers and identify features of the packet such as the length of the ingress packet hash values and packet types and checksum pass fail. In one embodiment receive queues may include multiple input and multiple output queues. In one embodiment where there are multiple network interfaces intermediate driver may allocate the receive queues associated with each of network interfaces for use by any of the network interfaces.

Device driver may be device drivers for each of network interfaces e.g. network interface . Although not depicted in one embodiment there may be a separate device driver for each of the multiple network interfaces. Device driver may provide an interface between OS and network interfaces e.g. network interface . Device driver may create descriptors and may manage the use and allocation of descriptors in receive queue . Device driver may request transfer of descriptors to network interfaces using one or more input queues. Device driver may signal to one of network interfaces that a descriptor is available on an input queue. Device driver may determine the location of the ingress packet in packet buffer based on a return descriptor that describes such ingress packet and device driver may inform operating system as well as other routines and tasks of the availability and location of such stored ingress packet.

Operating system may manage system resources and control tasks that are run on system . For example OS may be implemented using Microsoft Windows HP UX Linux or UNIX although other operating systems may be used. In one embodiment OS may be executed by each of the processors to N. In one embodiment when a Microsoft Windows operating system is used the ndis.sys driver may be utilized at least by device driver and intermediate driver . For example the ndis.sys driver may be utilized to define application programming interfaces APIs that can be used for transferring packets between layers. In one embodiment OS shown in may be replaced by a virtual machine which may provide a layer of abstraction for underlying hardware to various operating systems running on one or more processors.

Operating system may implement one or more protocol stacks only one shown . Protocol stack may execute one or more programs to process packets . An example of a protocol stack is a TCP IP Transport Control Protocol Internet Protocol protocol stack comprising one or more programs for handling e.g. processing or generating packets to transmit and or receive over a network. Protocol stack may alternatively be comprised on a dedicated sub system such as for example a TCP offload engine.

In one embodiment intermediate driver may allocate the receive queues associated with each of network interfaces for use by any of the network interfaces so that network interfaces appear as a single virtual network interface with multiple receive queues to layers above intermediate driver such as but not limited to OS . For example for two network interfaces with two receive queues each intermediate driver may provide a single virtual network interface with four receive queues e.g. four input and four output receive queues . Where multiple network interfaces are used intermediate driver allows taking advantage of features of OS of directing packets for processing by a specific processor even when the device driver for one or any of network interfaces does not support use of multiple receive queues.

In addition to or as an alternative to providing load balancing of packet processing by processors intermediate driver may provide for load balancing of traffic received from a network via network interfaces. In one embodiment intermediate driver may provide for load balancing of traffic received from a network among network interfaces. For example in one embodiment intermediate driver may include the capability to alter ARP replies described in Ethernet standards to request that traffic from a source device is thereafter addressed to a particular network interface among network interfaces for load balancing of packets received among network interfaces. Accordingly packets thereafter may be transmitted from a source node to the selected network interface among network interfaces so that load balancing may take place among network interfaces. For example intermediate driver may use ARP replies to allocate a first connection for receipt at a first network interface and a second connection for receipt at a second network interface.

Tx queues A N may buffer data to be transmitted from an output port for example an input output port or outlet of a node e.g. node A to another node. If a network device driver only supports one Tx queue the network device driver may have to acquire a spin lock on the single Tx queue and wait until other processors have released their locks on the Tx queue. The spin lock may result in lock contention which may degrade performance by requiring threads on one processor to busy wait and unnecessarily increasing processor utilization for example. Thus many modern network device drivers support multiple Tx queues. Distribution of a packet among multiple Tx queues may be based on which processor generates the packet type class or quality of service associated with the packet or data in the packet. Sometimes even different frames within a packet may be distributed to different Tx queues based on type class or quality of service associated with frames. In any case if frames packets of data are received at a node faster than the frames can be transmitted to another node the Tx queue or queues begin to fill up with frames. Generally recently received frames wait in the queue while frames received ahead of them in the queue are first transmitted.

Memory may additionally comprise one or more applications only one shown . Applications can be one or more machine executable programs that access data from a host system e.g. or a network. Application may include for example a web browser an email serving application a file serving application or a database application.

Memory may further comprise at least some components of logic . Some components of logic may be included in network interface . The components of logic in memory and network interface together may improve the efficiency of interrupt delivery at runtime in the network system where memory is located. For example logic may facilitate interrupt from network interface to be delivered to a processor where the target application is running as illustrated by .

Example system also includes an application running in the system at time or time period X. Based on the utilization of all the processors in the system at time or time period X and OS scheduling policy application is running on processor at time or time period X. When network controller interface generates an interrupt which targets application the interrupt might not be delivered to processor in a conventional network system. For example the interrupt may be delivered processor in socket via a static configuration or the interrupt may be delivered randomly to any of the four processors using a round robin scheme. According to an embodiment of the subject matter disclosed in the present application network controller interface as well as a device driver in the network system may be so configured that an interrupt that targets an application or a thread of an application will be delivered to the processor where the application thread is running. In the example shown in the interrupt that targets application will be delivered to processor .

Since the OS may reschedule an application thread at any time during runtime the processor affinity of the application thread i.e. the identification of a processor where the application thread runs may change during runtime. In one embodiment a network device driver e.g. or in may push down the processor affinity of the application thread to the network device e.g. network interface in at runtime. The network device will then generate the interrupt with the same processor affinity.

If the network system supports the MSI X based interrupt scheme each interrupt vector of the MSI X scheme may be associated with a Tx queue number. shows a table illustrating such association between the Tx queue index and the MSI vector index. Table includes two columns column Tx queue index and column MSI vector index . For example Tx queue may be associated with MSI vector Tx queue may be associated with MSI vector Tx queue may be associated with MSI vector etc. Table is used for illustration purpose. In practice there may more or less than 4 Tx queues or MSI vectors and the total number of Tx queues does not necessarily equal to the total number of MSI vectors. The association information between the Tx queues and MSI vectors may be stored in a network device such as network interface shown in .

Returning now to each MSI interrupt vector may include processor identification information of a processor for which the interrupt is targeted. At block a Tx queue associated with an MSI interrupt vector whose processor identification information matches the processor affinity of the application thread identified at block may be selected. If there is no MSI interrupt vector whose processor identification information matches the identified processor affinity of the application thread an alternative MSI interrupt vector whose processor identification information matches any one of processors in the same socket as the identified processor that runs the application thread may be used. Accordingly a Tx queue associated with such an alternative MSI interrupt vector may be selected.

At block the message may be sent through the Tx queue selected at block . At block the network device may obtain packets in the message through direct memory access DMA from the selected Tx queue. Once the message is in the device buffer or is sent out the network device generates an interrupt using the interrupt vector associated with the Tx queue selected at block . At block the interrupt may be delivered to the target processor which matches the processor affinity of the application thread or a peer processor in the same socket as the processor that runs the application thread. At block the device driver may process Tx completion and free the packet buffer at the selected Tx queue. The application thread may then wake up and keep running.

Process shown in assumes that the network system supports multiple Tx queues and there are multiple interrupt vectors available for use. This assumption is typically true with modern network systems. If the network system supports only one Tx queue but with multiple MSI X vectors available the device driver may inform the network device the next interrupt vector to be used the identified processor affinity at runtime so that the device interrupt can be delivered to the target processor directly.

In another embodiment where an intermediate device driver e.g. in is used the subject matter disclosed above may be altered to fit the intermediate device driver. For example the intermediate device driver may distribute the packet to a miniport instance whose device has an interrupt vector whose processor identification information matches the processor affinity of the current running application thread.

Yet in another embodiment the device driver may push down the processor affinity of the application to the chipset such as memory control hub MCH or IOH. When the chipset receives the interrupt from the device it delivers the interrupt to the right CPU. This approach may work well for a device supporting only one interrupt vector such a PCI legacy interrupt or MSI with a single message.

The experimental performance indicates that when the processor affinity of the interrupt i.e. the processor to which the interrupt is delivered and the processor affinity of the application are the same or are in the same socket see row and row in table the throughput is the highest while the overall CPU utilization is the lowest. In other situations e.g. rows and shown in table where the processor affinity of interrupt and the processor affinity of application are not the same or in the same socket the throughput degrades and the overall CPU utilization increases.

Although an example embodiment of the disclosed subject matter is described with reference to drawings in persons of ordinary skill in the art will readily appreciate that many other methods of implementing the disclosed subject matter may alternatively be used. For example the order of execution of the blocks in flow diagrams may be changed and or some of the blocks in block flow diagrams described may be changed eliminated or combined.

In the preceding description various aspects of the disclosed subject matter have been described. For purposes of explanation specific numbers systems and configurations were set forth in order to provide a thorough understanding of the subject matter. However it is apparent to one skilled in the art having the benefit of this disclosure that the subject matter may be practiced without the specific details. In other instances well known features components or modules were omitted simplified combined or split in order not to obscure the disclosed subject matter.

Various embodiments of the disclosed subject matter may be implemented in hardware firmware software or combination thereof and may be described by reference to or in conjunction with program code such as instructions functions procedures data structures logic application programs design representations or formats for simulation emulation and fabrication of a design which when accessed by a machine results in the machine performing tasks defining abstract data types or low level hardware contexts or producing a result.

For simulations program code may represent hardware using a hardware description language or another functional description language which essentially provides a model of how designed hardware is expected to perform. Program code may be assembly or machine language or data that may be compiled and or interpreted. Furthermore it is common in the art to speak of software in one form or another as taking an action or causing a result. Such expressions are merely a shorthand way of stating execution of program code by a processing system which causes a processor to perform an action or produce a result.

Program code may be stored in for example volatile and or non volatile memory such as storage devices and or an associated machine readable or machine accessible medium including solid state memory hard drives floppy disks optical storage tapes flash memory memory sticks digital video disks digital versatile discs DVDs etc. as well as more exotic mediums such as machine accessible biological state preserving storage. A machine readable medium may include any mechanism for storing transmitting or receiving information in a form readable by a machine and the medium may include a tangible medium through which electrical optical acoustical or other form of propagated signals or carrier wave encoding the program code may pass such as antennas optical fibers communications interfaces etc. Program code may be transmitted in the form of packets serial data parallel data propagated signals etc. and may be used in a compressed or encrypted format.

Program code may be implemented in programs executing on programmable machines such as mobile or stationary computers personal digital assistants set top boxes cellular telephones and pagers and other electronic devices each including a processor volatile and or non volatile memory readable by the processor at least one input device and or one or more output devices. Program code may be applied to the data entered using the input device to perform the described embodiments and to generate output information. The output information may be applied to one or more output devices. One of ordinary skill in the art may appreciate that embodiments of the disclosed subject matter can be practiced with various computer system configurations including multiprocessor or multiple core processor systems minicomputers mainframe computers as well as pervasive or miniature computers or processors that may be embedded into virtually any device. Embodiments of the disclosed subject matter can also be practiced in distributed computing environments where tasks may be performed by remote processing devices that are linked through a communications network.

Although operations may be described as a sequential process some of the operations may in fact be performed in parallel concurrently and or in a distributed environment and with program code stored locally and or remotely for access by single or multi processor machines. In addition in some embodiments the order of operations may be rearranged without departing from the spirit of the disclosed subject matter. Program code may be used by or in conjunction with embedded controllers.

While the disclosed subject matter has been described with reference to illustrative embodiments this description is not intended to be construed in a limiting sense. Various modifications of the illustrative embodiments as well as other embodiments of the subject matter which are apparent to persons skilled in the art to which the disclosed subject matter pertains are deemed to lie within the scope of the disclosed subject matter.

