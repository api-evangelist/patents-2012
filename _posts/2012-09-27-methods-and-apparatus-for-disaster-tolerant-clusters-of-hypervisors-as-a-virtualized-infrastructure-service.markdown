---

title: Methods and apparatus for disaster tolerant clusters of hypervisors as a virtualized infrastructure service
abstract: Methods and apparatus for configuring disaster tolerant clusters of hypervisors as a virtualized service. In an exemplary embodiment, a system discovers WWPNs components, identifies networks zones in SANs having connectivity with at least one of the storage arrays and at least one of the journaling appliances, and creates a disaster recovery service offerings including replication in accordance with the selected service offering.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08898507&OS=08898507&RS=08898507
owner: EMC Corporation
number: 08898507
owner_city: Hopkinton
owner_country: US
publication_date: 20120927
---
As is known in the art cloud computing systems contain a large number of servers and hardware devices servers storage systems networking devices software modules and components. When configuring a highly available HA computer system these systems are independent and redundant to provide services regardless of single points of failure. When a computer system host communicates with a storage system array using network attached storage NAS provided over a network network the host network and array are single points of failure. Creating an HA service is achieved by mirroring storage volumes volume to redundant arrays as the volumes are modified by the host thereby creating a copy of the volume and making it available via redundant hosts and networks.

The array contains a module or appliance having a splitter splitter that splits host operations on a volume into duplicates. A splitter is utilized by replication software services replication to duplicate volume operations and transport them to a redundant array. The replication reproduces the operation on a redundant volume in a redundant array.

In cloud computing the host is insulated from the operating system software OS by use of a hypervisor hypervisor . The hypervisor creates a software virtualized representation of the host hardware upon which the OS is installed and virtualized creating a virtual machine VM . Hypervisors on many hosts can communicate across networks regarding VM and host health. In addition hypervisors can use these networks in conjunction with the replicated volume to manage multiple copies of the VM using redundant hosts. The combination of replication and virtual machines provide an enhanced service where virtual machines will be protected against failures in the infrastructure.

Sets of hosts switches and arrays are assembled in close proximity to make up a unit of cloud infrastructure infrastructure sometimes referred to as a pod pod of devices. The pod components are physically connected via Ethernet networks for example. The logical configuration of pods components and network creates a platform that is sold or leased as a service service from a menu of predefined configuration offerings offering for consumers of cloud computing. Offerings from vendors name the type and quantity of resources such as 3 highly available servers with 16 GB of memory having 4 processors with 20 GB of disk space each and a shared storage volume of 200 GB. This allows for the consumer to use the host and storage resources in predetermined offerings within the pods making up the cloud.

The present invention provides automated configuration for disaster tolerant clusters of hypervisors as a virtualized infrastructure service. Using an information model IM including hosts arrays network devices and replication and service offerings in the cloud computing environment steps to configure array and replication system can be computed to create a storage service from a storage service offering. Provisioning can execute changes to logical configurations of array and replication services comprised of hosts networks replication nodes and array resources. Provisioning identifies a set of hosts and adaptors along with the replication configurations and array configurations required to provide hosts with high availability as it relates to geographic disasters and or single component failures.

Exemplary embodiments of the invention allow for provisioning and configuration of sets of logical services staged at geographic locations utilizing replication and hypervisors and for automated staging of hosts for use as redundant services and identifies available locations where volumes may be replicated to provide HA or disaster recovery DR services.

In one embodiment using an information model of splitter and POD locations in conjunction with a grading and classification system for physical resources the system can identify and create logical resources required to satisfy a set of constraints for logical configuration replication and other resource selections. Resources can then be provisioned into infrastructure services based on Class of Service using a service offering definition into sets of pod infrastructures. Selecting and activating a predefined disaster recovery DR service offering initiates the automation of the disaster tolerant virtualized infrastructure service.

In one aspect of the invention a method comprises discovering using a processor WWPNs for storage arrays in a system by reading an interface of the storage arrays discovering WWPNs of ports of the discovered storage arrays discovering WWPNs of journaling appliances discovering WWPNs of splitters seen by the journaling appliances discovering paths for the splitters discovering network adaptor addresses for the paths identifying networks zones in SANs having connectivity with at least one of the storage arrays and at least one of the journaling appliances determining ones of the storage arrays logged in and zoned in the SANs determining WWPNs of the storage arrays logged in and zoned in the SANs determining WWPN zoning of the storage arrays identify adaptors for storage arrays zoned and logged in determining ones of the storage arrays and ones of the journaling appliances zoned together for correlating replication networks host determining ones of the storage arrays seen the journaling appliances for correlating ones of the splitters identifying disaster recovery pools creating a disaster recovery policy creating disaster recovery service offerings the service offerings including geographically separated primary and secondary storage receiving a user selection of a disaster recovering service offering assigning hosts to the selected service offering creating a hypervisor at primary and secondary infrastructure for the selected service offering creating a volume at the primary and secondary infrastructure for the selected service offering mapping and masking volumes to the journaling appliance and the hypervisor for the selected service offering and enabling replication in accordance with the selected service offering.

The method can further include one or more of the following features applying disaster recovery policy settings to the service determining an ability to connect volumes from a first pod to a second pod discovering configuration information for the splitters including adaptor ports of a storage array on which a first one of the splitters is running using the splitter configuration information to associate replication networks to the storage arrays and to build disaster recovery pools determining a list of paths for the splitters and a list of network adapter addresses for the paths correlating the storage arrays to replication networks via a storage area network correlating the journaling appliances via the storage area networks and or deleting the selected service.

In another aspect of the invention an article comprises a computer readable medium containing stored non transitory instructions that enable a machined to perform discovering WWPNs for storage arrays in a system by reading an interface of the storage arrays discovering WWPNs of ports of the discovered storage arrays discovering WWPNs of journaling appliances discovering WWPNs of splitters seen by the journaling appliances discovering paths for the splitters discovering network adaptor addresses for the paths identifying networks zones in SANs having connectivity with at least one of the storage arrays and at least one of the journaling appliances determining ones of the storage arrays logged in and zoned in the SANs determining WWPNs of the storage arrays logged in and zoned in the SANs determining WWPN zoning of the storage arrays identify adaptors for storage arrays zoned and logged in determining ones of the storage arrays and ones of the journaling appliances zoned together for correlating replication networks host determining ones of the storage arrays seen the journaling appliances for correlating ones of the splitters identifying disaster recovery pools creating a disaster recovery policy creating disaster recovery service offerings the service offerings including geographically separated primary and secondary storage receiving a user selection of a disaster recovering service offering assigning hosts to the selected service offering creating a hypervisor at primary and secondary infrastructure for the selected service offering creating a volume at the primary and secondary infrastructure for the selected service offering mapping and masking volumes to the journaling appliance and the hypervisor for the selected service offering and enabling replication in accordance with the selected service offering.

The article can further include instructions for enabling one or more of the following features applying disaster recovery policy settings to the service determining an ability to connect volumes from a first pod to a second pod discovering configuration information for the splitters including adaptor ports of a storage array on which a first one of the splitters is running using the splitter configuration information to associate replication networks to the storage arrays and to build disaster recovery pools determining a list of paths for the splitters and a list of network adapter addresses for the paths correlating the storage arrays to replication networks via a storage area network correlating the journaling appliances via the storage area networks and or deleting the selected service.

In a further aspect of the invention a system comprises at least one processor memory coupled to the at least one processor the memory and the at least one processor configured to enable discovering WWPNs for storage arrays in a system by reading an interface of the storage arrays discovering WWPNs of ports of the discovered storage arrays discovering WWPNs of journaling appliances discovering WWPNs of splitters seen by the journaling appliances discovering paths for the splitters discovering network adaptor addresses for the paths identifying networks zones in SANs having connectivity with at least one of the storage arrays and at least one of the journaling appliances determining ones of the storage arrays logged in and zoned in the SANs determining WWPNs of the storage arrays logged in and zoned in the SANs determining WWPN zoning of the storage arrays identify adaptors for storage arrays zoned and logged in determining ones of the storage arrays and ones of the journaling appliances zoned together for correlating replication networks host determining ones of the storage arrays seen the journaling appliances for correlating ones of the splitters identifying disaster recovery pools creating a disaster recovery policy creating disaster recovery service offerings the service offerings including geographically separated primary and secondary storage receiving a user selection of a disaster recovering service offering assigning hosts to the selected service offering creating a hypervisor at primary and secondary infrastructure for the selected service offering creating a volume at the primary and secondary infrastructure for the selected service offering mapping and masking volumes to the journaling appliance and the hypervisor for the selected service offering and enabling replication in accordance with the selected service offering.

The compute layer comprises components such as blade servers chassis and network interconnects that provide the computing power for the platform. The storage layer comprises the storage components for the platform. The network layer comprises the components that provide switching and routing between the compute and storage layers within and between platforms and to the client or customer network.

It is understood that a variety of other configurations having different interconnections and storage configuration can be provided to meet the needs of a particular application.

The management layer can include a number of applications to perform various functions for overall control configuration etc. of the various platform components. For example management applications can include a visualization function such as vSphere vCenter by VMware of Palto Alto Calif. A further management application can be provided as part of the Unified Computing System UCS by Cisco. It is understood that the blade chassis and fabric interconnection can be considered part of the UCS. Another management application can includes a management interface such as EMC Unisphere to provide a flexible integrated experience for managing existing storage systems such as CLARIION and CELERRA storage devices from EMC. A further management application includes a platform element manager such as unified infrastructure manager UIM by EMC for managing the configuration provisioning and compliance of the platform.

It is understood that various vendor specific terminology product name jargon etc. may be used herein. It is further understood that such vendor specific information is used to facilitate an understanding of embodiments of the invention and should not limit the invention in any way. Any specific vendor information should be construed to mean a generic product function or module absent a clear indication to the contrary.

The unified infrastructure manager further includes a change and configuration management module a policy based compliance and analysis module a unified infrastructure provisioning module a consolidation topology and event service module and an operational awareness module . The various modules interact with platform elements such as devices in compute network and storage layers and other management applications. The unified infrastructure manager performs platform deployment by abstracting the overall provisioning aspect of the platform s and offering granular access to platform components for trouble shooting and fault management.

APIs provide a native computer programming language binding that can be executed from the native computer programming language. Java is a widely used language in computer programming and many vendors provide java language libraries and examples to execute commands against the management interface of their devices.

Referring again to one or more hosts are associated with a service to fulfill the compute requirements of the service. One or more network adaptors are associated with a service to indicate the need for Ethernet connectivity to a network. A network profile is associated with each adaptor that indicates the VLAN and IP address required to fulfill the storage connectivity using the Ethernet. This network profile is associated to a storage profile to indicate that the host is to obtain storage from a network with this VLAN and IP network.

The service offering is used to hold the relationships and detailed description for the user to choose the offering from a menu of offerings. The storage profile is associated with the offering to indicate the class of storage and service settings for the storage to be configured such as features like de duplication write once read many auto extension maximum auto extensions thin provisioning etc. A volume profile is associated with the storage profile to indicate specific volume properties and characteristics such as size and quota limitations.

In step based on the offering chosen by the user a pod can be selected by the management layer using the service information model that defines the desired quality of service and class of service required. This information is described in the service information model storage profile and volume profile see . Using this information the system can determine the correct pod placement for the volume.

A POD is chosen by matching the class of service defined in the offering with the class of service provided by the POD. The Class of Service CoS is defined as a label that is relative to the service provided by the POD. For example as shown in a Gold class of service could be defined as having a dedicated LAN large storage dedicated ESX and a large server. Silver and Bronze offerings provide less in at least one area. In embodiment the Gold class of service provides de duplication. When selecting a POD for service placement the management layer selects an array that provides the de duplication feature for Gold services. The list of CoS for an array would be cross referenced with the list of CoS for a service offering to locate an acceptable pod and array on which to create the service.

In step once the volume characteristics and properties are known from the service information model the service layer can begin to create the volume by defining the configuration settings such as size protection levels and placement of the volume on a data mover in relationship to the VLANs connecting the host and array adaptors from the configuration model. The properties of the volume are used in conjunction with the CoS of the Storage Profile to create the configuration settings of the volumes in the service. The Service s network profile determines the VLAN upon which to establish the network connection to the network shared volume. In step it is determined whether the volume exists such as by using the management interface of the array the service layer to query the array s of the pod to see if there were already volumes created for that service If it is determined the service docs not already contain a volume that meets the criteria of the offering the management layer executes commands through a mediation interface to the physical device management system to create the volume using the characteristics determined above in step .

Processing continues in step wherein the volume is exported. Using the network profile information contained in the service information model the service layer determines the network access restrictions consisting of the VLAN IP address and authentication credentials for the network attached volume.

In step a host interface is determined using the service information model network profile and the VLAN and IP address settings are determined for the host adaptor. In step the management layer determines cross connects. Using the VLAN the correct adaptor for the array can be determined by cross referencing the membership of the adaptor in a given VLAN. In step using the information determined above the array management interface is configured to expose the created volume using NFS protocol to the determined IP address.

Exemplary disaster recovery journaling systems are shown and described in exemplary U.S. Pat. Nos. 8 214 612 8 060 714 and 7 774 565 all of which are incorporated herein by reference. In production mode a journaling appliance is configured to act as a source side protection agent for a host device. The appliance replicates SCSI I O requests where a replicated SCSI I O request is sent to the other appliance. After receiving an acknowledgement from the other appliance the appliance sends the SCSI I O request to the logical unit. A splitter is provided as part of the journaling system to split write requests.

The array is instructed via mediation to create a volume and export it to network adaptor . The volume is created using the service offering s storage and volume profiles. The array adaptor exports the volume to the network adaptor and implements the access control defined in the service network profile. The switch VLAN carries NFS over IP traffic from the adaptor of the array to the adaptor of the host . The host uses its operating system network file system sub system to make the array volume visible to the guest of the host. The replication host appliances and network connectivity supply replication services

Table 1 below shows administration information needed for the replication services. This information provides the replication system s information. Replication splitter and array information is discovered using this management information.

Table 2 below represents the DR policy settings for an exemplary infrastructure service offering. These DR settings will store policies for the replication settings applied when provisioning the DR storage volumes. This information is entered as policy into the algorithm.

Table 3 contains information discovered using the replication system management API. This table represents the ability to connect volumes from one POD to another using the replication network.

The initiator group includes servers the port group includes a series of ports and the storage group includes a volume . The data masking view in addition to the servers ports and volumes includes journaling appliances with HBAs.

Table 5 below identifies the zones in the SAN containing connectivity to a storage array and replication network.

To perform correlation WWPNs of storage arrays must be discovered step . This is performed by reading the management interface of each array. Exemplary pseudo code below organizes the data into a list type StorageArrayWWPN containing each storage array and all of the WWPNs of the storage array ports.

Step of refers to replication system discovery. In addition to discovery of the replication system additional configuration information from each replication system host is needed to perform the correlation. From each replication host the network adaptors are discovered as well as the splitter configuration. The splitter configuration contains the adaptor ports of the storage array where the splitter is running. The replication host adaptors can be used to discover the host connectivity within the network and the splitter configuration will be used while associating replication networks to storage arrays and building DR pools. Exemplary pseudo code for replication discovery is set forth below.

The recover point appliance RPA class contains a list of splitters and each splitter contains a list of paths and each path contains a list of network adaptor addresses. These adaptor addresses are the addresses of the storage adaptor ports on the storage arrays running the splitter.

For correlating splitters and replication networks given that the adaptor addresses are known for the array and replication network the endpoints in the SAN for each device have been identified correlation of the storage arrays to the replication networks via the SAN topology can be performed.

Step of identifies the set of storage adaptors that are connected to the SAN. The first step is to collect all of the storage adaptor identifiers WWPNs that are logged into the SAN. This is maintained and discovered from the SAN switch management interface API. FIG. WHAT FIG models the connectivity table entries as ConnectedPort objects. The ConnectedPort class contains the WWPN adaptor address of the device logged into the SAN array adaptor port e.g. the WWPN of the device physically connected to the SAN switch port . A list loggedIn List is created from this data. The system establishes the physical connectivity of the storage host or array to the SAN switches. Exemplary pseudo code is set forth below.

Step of correlates replication networks host RPA via the storage networks. Creating an intersection of the data collected correlates this information. Processing calls the previous processing and then inspects the collected data to find zones that contain both storage array WWPNs and RPA WWPNs. This identifies zones on the SAN that have both physical and logical connectivity to a storage array and the RPA. The data collection and correlation is repeated for each SAN switch. Exemplary pseudo code is set forth below.

Step of correlates the splitters including host splitter correlations. This ensures the replication network host RPA is properly configured and can utilize the storage array splitters. Exemplary pseudo code is set forth below.

Processing utilizes information models before provisioning of the configuration changes on the storage array and replication networks. Processing is performed once for each new POD infrastructure put under system management.

In step a connectivity model is produced such as the model of . In step pools are identified as shown in . In step a DR policy is created such as that shown in . In step a DR service offering is created such as shown in A A. The service offering is then ready for provisioning.

After the above discovery and processing exemplary embodiments of the invention provide automated provisioning of compute systems in DR cluster configurations. To manage these configurations the system performs a series of processing steps. The system discovers physical and logical topology for the SAN and replication network connectivity as shown in and and accompanying pseudo code to determine connectivity and relationships between storage arrays splitters and replication. This allows the system to understand logical connectivity between replication and storage systems understand the replication connectivity between PODs and supply storage selection criteria for service catalogs.

The system creates predefined services using offerings described above. The system can provision primary and secondary POD resources such as volumes and hypervisors provision NAS volume exports to hypervisors and hosts on the hypervisor and provision replication services between volumes being replicated. The system can provide two volumes that are geographically separated as one logical DR enabled volume and provide DR services using a DR enabled volume.

In step hypervisor s are created at primary and secondary POD s and in step volumes are created at the primary and secondary POD. In step the system maps and masks volumes to RPA s and hypervisor s see e.g. . After creating the hypervisors at step an API can be run at the hypervisor management system in step . After creating volumes in step and map mask in step an API can be run at the array management system in step . After map mask in step in step replication is enabled see e.g. .

It is understood that a splitter can be located in variety of locations to meet the needs of a particular location. In one embodiment a splitter SPL can be located in the arrays AR as shown in . In another embodiment a splitter SPL can be located in the SAN as shown in . Splitters can reside on the array and or in the SAN fabric based splitter . The journaling appliance such as Recoverpoint by EMC Corporation works to replicate this split traffic. The appliance can work in pairs or in clusters to keep locations replicated.

Referring to a computer includes a processor a volatile memory an output device a non volatile memory e.g. hard disk and a graphical user interface GUI e.g. a mouse a keyboard a display for example . The non volatile memory stores computer instructions an operating system and data for example. In one example the computer instructions are executed by the processor out of volatile memory to perform all or part of the processing described above. An article can comprise a machine readable medium that stores executable instructions causing a machine to perform any portion of the processing described herein.

Processing is not limited to use with the hardware and software described herein and may find applicability in any computing or processing environment and with any type of machine or set of machines that is capable of running a computer program. Processing may be implemented in hardware software or a combination of the two. Processing may be implemented in computer programs executed on programmable computers machines that each includes a processor a storage medium or other article of manufacture that is readable by the processor including volatile and non volatile memory and or storage elements at least one input device and one or more output devices. Programs may be implemented in a high level procedural or object oriented programming language to communicate with a computer system. However the programs may be implemented in assembly or machine language. The language may be a complied or an interpreted language and it may be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program may be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network. A computer program may be stored on a storage medium or device e.g. CD ROM hard disk or magnetic diskette that is readable by a general or special purpose programmable computer for configuring and operating the computer when the storage medium or device is read by the computer to perform processing.

One skilled in the art will appreciate further features and advantages of the invention based on the above described embodiments. Accordingly the invention is not to be limited by what has been particularly shown and described except as indicated by the appended claims. All publications and references cited herein are expressly incorporated herein by reference in their entirety.

