---

title: Method and apparatus for using a single multi-function adapter with different operating systems
abstract: A flexible arrangement allows a single arrangement of Ethernet channel adapter (ECA) hardware functions to appear as needed to conform to various operating system deployment models. A PCI interface presents a logical model of virtual devices appropriate to the relevant operating system. Mapping parameters and values are associated with the packet streams to allow the packet streams to be properly processed according to the presented logical model and needed operations. Mapping occurs at both the host side and at the network side to allow the multiple operations of the ECA to be performed while still allowing proper delivery at each interface.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08489778&OS=08489778&RS=08489778
owner: Intel-NE, Inc.
number: 08489778
owner_city: Wilmington
owner_country: US
publication_date: 20120817
---
This application is a continuation of prior co pending U.S. patent application Ser. No. 13 219 145 filed Aug. 26 2011 which is a continuation of prior U.S. patent application Ser. No. 12 874 739 filed Sep. 2 2010 which is a continuation of prior U.S. patent application Ser. No. 11 356 501 filed Feb. 17 2006.

In complex computer systems particularly those in large transaction processing environments as shown in the available servers are often clustered together to improve overall system performance. Second these clustered servers are then connected by a storage area network SAN to storage units so that all have high performance access to storage. Further the servers are also connected to an Ethernet network to allow the various user computers to interact with the servers . Thus the servers use a first fabric for clustering a second fabric for the SAN and a third fabric to communicate with the users. In normal use the cluster fabric is one such as InfiniBand the SAN fabric is one such as Fibre Channel and the user fabric is one such as Ethernet. Therefore in this configuration each of the servers must have three different adapters to communicate with the three fabrics. Further the three adaptors take up physical space in a particular server thus limiting the density of available servers in a high processor count environment. This increases cost and complexity of the servers themselves. Additionally three separate networks and fabrics must be maintained.

This is shown additionally in where the software components are shown. An operating system is present in the server . Connected to the operating system is a clustering driver which connects with an InfiniBand host channel adapter HCA in the illustrated embodiment. The InfiniBand HCA is then connected to the InfiniBand fabric for clustering. A block storage driver is connected to the operating system and interacts with a Fibre Channel host bus adapter HBA . The Fibre Channel HBA is connected to the Fibre Channel fabric to provide the SAN capability. Finally a networking driver is also connected to the operating system to provide the third parallel link and is connected to a series of network interface cards NICs which are connected to the Ethernet fabric .

Legacy operating systems such as Linux 2.4 or Microsoft NT4 were architected assuming that each I O Service is provided by an independent adapter. An I O Service is defined as the portion of adapter functionality that connects a server onto one of the network fabrics. Referring to the NIC provides the Networking I O Service the HCA provides the Clustering I O Service and the HBA provides the Block Storage I O Service. It would be desirable to allow a single ECA or Ethernet Channel Adapter to provide all three of these I O Services. Since most traditional high performance networking storage and cluster adapters are PCI based and enumerated as independent adapters by the Plug and Play PnP component of the operating system the software stacks for each fabrics have evolved independently. In order for an ECA to be deployed on such legacy operating systems its I O Services must be exported using independent PCI functions. While this type of design fits nicely into the PnP environment it exposes issues related to shared resources between the PCI functions. For example networking and storage may want to utilize a specific Ethernet port concurrently.

Modern operating systems such as Microsoft Windows Server 2003 provide a mechanism called a consolidated driver model which could be used to export all ECA I O Services using only a single PCI function. However the software associated with the consolidated driver model has implicit inefficiencies due to the layers involved in virtualizing each I O Service using host software. In some deployment environments it may be desirable to support the consolidated driver model but in environments that are sensitive to latency and CPU utilization it is desirable to deploy an ECA using multiple PCI functions.

Microsoft has made some progress in integrating networking and clustering using the Winsock Direct WSD model. One issue with WSD is that it does not export the various RDMA Remote Direct Memory Access APIs Application Programming Interfaces such as DAPL Direct Access Provider Library or MPI Message Passing Interface that have been widely accepted by the clustering community. One approach to exporting DAPL and MPI when not natively supported on an operating system is to use an independent PCI function for clustering. Another issue with WSD is that it is not deployed on all Microsoft operating systems so hardware vendors cannot rely on it to export their adapter I O services in all Microsoft operating system environments.

Future operating systems architectures will certainly start to take into account the unique characteristics of ECAs e.g. multiple network ports and multiple I O Services implemented in one adapter. Network ports accelerated connections and memory registration resources are all examples of resources that the operating system has an interest of managing in a way that is intuitive and in a way that takes the best advantage of the functionality provided by an ECA. This results in a very high probability for even more deployment models which would be desirable to support.

To address these various deployment models and yet provide the broadest use of a single ECA at its full capabilities it would be desirable to have an ECA that is able to adapt to each deployment model.

In a design according to the present invention a flexible arrangement allows a single arrangement of ECA hardware functions to appear as needed to conform with various operating system deployment models. A PCI interface presents a logical model appropriate to the relevant operating system. Mapping parameters and values are associated with the packet streams to allow the packet streams to be properly processed according to the presented logical model and needed operations. The mapping arrangement allows different logical models to be presented and yet have only a single hardware implementation. Mapping occurs at both the host side and at the network side to allow the multiple operations of the ECA to be performed while still allowing proper delivery at each interface.

In the preferred embodiment as shown in three servers are connected to an Ethernet fabric . Preferably this is a higher performance Ethernet fabric than the Ethernet fabric as it is also used for storage area networking and clustering capabilities. As can be seen there is logically only a single link to a single fabric in the system according to the preferred embodiment though this may be any number of actual physical links. Storage units are also directly connected to the Ethernet fabric . Further a conventional user is connected to the Ethernet fabric . Because only a single fabric is utilized in the designs according to the preferred embodiment significantly less maintenance and management is required than as in the prior art as shown in . To handle the three varying tasks clustering storage and user interface the server includes an Ethernet channel adapter ECA . This ECA includes builtin capabilities to provide enhanced capabilities over current Ethernet fabrics. These enhancements include RDMA capability particularly according to the iWARP standard and iSCSI. iWARP is utilized in the clustering environment whereas iSCSI is the standard for doing SANs using Ethernet.

Referring to the server according to the preferred embodiment includes an operating system as in the prior art. It similarly contains a networking driver which is connected to a NIC . A block storage driver is also connected to the operating system. It differs slightly from that used in because in this case it is an iSCSI driver as opposed to the Fibre Channel driver utilized in . The driver communicates with iSCSI hardware present in the ECA . A clustering driver is also slightly different in that it utilizes RDMA capabilities and complies with the iWARP standard. To that end it is connected to an iWARP module in the ECA . An RDMA chimney according to the Microsoft Scalable Networking Initiative is present for appropriate Microsoft operating systems to interact with the iWARP module and the operating system to provide improved RDMA capabilities. Because both the iSCSI storage function and the clustering iWARP function need to be very high performance a TCP offload engine TOE is provided to connect to the iWARP module and the iSCSI module . Further a TCP chimney also according to the Microsoft Scalable Networking Initiative is present for appropriate Microsoft operating systems and is connected to the TOE . Both the TOE and the NIC are connected to an Ethernet crossbar switch contained in the ECA to allow flexibility of the various connections to the Ethernet fabric .

Referring then to a simple block diagram of a server is shown. The various host CPUs are connected to a server chipset which is also connected to server or host memory . A hard drive is coupled to the server chipset to provide storage of the operating system device drivers and relevant programs. In the illustrated embodiment further connected to the server chipset using a PCI bus such as a PCI X bus is a first ECA A which is shown to be operating in one Gb Ethernet mode. RAM is connected to the ECA A to form temporary buffer storage. Four one Gb Ethernet ports are connected to the ECA A to provide the actual output capability. In the illustrated embodiment a second ECA in this case ECA B is connected to the server chipset using a PCI bus such as a PCI Express bus and is operating in ten Gb Ethernet mode and includes a pair of ten Gb Ethernet ports . RAM is also connected to ECA B to provide buffers for its various functions.

Referring then to a block diagram of the ECA according to the preferred embodiment is shown. Various server bus interfaces and such as PCI X or PCI Express are shown to provide connections to the server chip set . A set of configuration registers is connected to the server bus interfaces and to present the ECA to the appropriate bus as more fully described below. The server bus interfaces and are connected to a PCI frame parser PFP . The PFP interfaces the host CPUs into a transaction switch . In the preferred embodiment the internal architecture of the ECA is based on serial data packet flows and the transaction switch is used to connect the various internal blocks of the ECA as well as providing the crossbar Function. For example one of these blocks is a local memory interface . This is where the RAM or is connected to allow storage of data being received and transmitted. A series of MAC packet parsers MPP are provided to parse data being received from the Ethernet fabric . These are also connected to the transaction switch to allow them to provide data to or retrieve data from the local memory interface or the transaction switch . The various MPPs are connected to the relevant Ethernet MACs to provide the actual interface to the Ethernet fabric . A protocol engine PE is connected to the transaction switch and also has a direct connection to the local memory interface to allow higher speed operation. The protocol engine performs all the processing relating to the NIC TOE iSCSI and iWARP modules shown in the prior logical block diagram. A special loop back MPP is provided to allow improved switching capabilities inside the ECA . Finally an IP address table is present to provide the IP addresses utilized by the ECA in its communications over the Ethernet fabric .

In basic operations a series of tasks are performed by the various modules or sub modules in the protocol engine to handle the various iWARP iSCSI and regular Ethernet traffic. A context manager is provided with a dedicated datapath to the local memory interface . As each connection which is utilized by the ECA must have a context various subcomponents or submodules are connected to the context manager as indicated by the arrows captioned by cm. Thus all of the relevant submodules can determine context of the various packets as needed. The context manager contains a context cache which caches the context values from the local memory and a work available memory region cache which contains memory used to store transmit scheduling information to determine which operations should be performed next in the protocol engine . The schedules are effectively developed in a work queue manager WQM . The WQM handles scheduling for all transmissions of all protocols in the protocol engine . One of the main activities of the WQM is to determine when data needs to be retrieved from the external memory or or from host memory for operation by one of the various modules. The WQM handles this operation by requesting a time slice from the protocol engine arbiter to allow the WQM to retrieve the desired information and place it in on chip storage. A completion queue manager CQM acts to provide task completion indications to the CPUs . The CQM handles this task for various submodules with connections to those submodules indicated by arrows captioned by cqm. A doorbell submodule receives commands from the host such as a new work item has been posted to SQ x and converts these commands into the appropriate context updates.

A TCP off load engine TOE includes submodules of transmit logic and receive logic to handle processing for accelerated TCP IP connections. The receive logic parses the TCP IP headers checks for errors validates the segment processes received data processes acknowledges updates RTT estimates and updates congestion windows. The transmit logic builds the TCP IP headers for outgoing packets performs ARP table look ups and submits the packet to the transaction switch . An iWARP module includes a transmit logic portion and a receive logic portion . The iWARP module implements various layers of the iWARP specification including the MPA DDP and RDMAP layers. The receive logic accepts inbound RDMA messages from the TOE for processing. The transmit logic creates outbound RDMA segments from PCI data received from the host CPUs .

A NIC module is present and connected to the appropriate items such as the work queue manager and the protocol engine arbiter . An iSCSI module is present to provide hardware acceleration to the iSCSI protocol as necessary.

Typically the host operating system provides the ECA with a set of restrictions defining which user level software processes are allowed to use which host memory address ranges in work requests posted to the ECA . Enforcement of these restrictions is handled by an accelerated memory protection AMP module . The AMP module validates the iWARP STag using the memory region table MRT and returns the associated physical buffer list PBL information. An HDMA block is provided to carry out the DMA transfer of information between host memory via one of the bus interfaces or and the transaction switch on behalf of the WQM or the iWARP module . An ARP module is provided to retrieve MAC destination addresses from an on chip memory. A free list manager FLM is provided to work with various other modules to determine the various memory blocks which are available. Because the data be it data packets or control structures is all contained in packets a list of the available data blocks is required and the FLM handles this function.

The protocol engine of the preferred embodiment also contains a series processors to perform required operations each processor including the appropriate firmware for the function of the processor. The first processor is a control queue processor CQP . The control queue processor performs commands submitted by the various host drivers via control queue pairs. This is relevant as queue pairs are utilized to perform RDMA operations. The processor has the capability to initialize and destroy queue pairs and memory regions or windows. A second processor is the out of order processor OOP . The out of order processor is used to handle the problem of TCP IP packets being received out of order and is responsible for determining and tracking the holes and properly placing new segments as they are obtained. A transmit error processor TEP is provided for exception handling and error handling for the TCP IP and iWARP protocols. The final processor is an MPA reassembly processor . This processor is responsible for managing the receive window buffer for iWARP and processing packets that have MPA FPDU alignment or ordering issues.

The components and programming of the ECA are arranged and configured to allow the ECA to work with the known deployment models described above including independent adapter consolidated driver and Winsock Direct and potential future deployment models. The ECA can present itself on the PCI bus as one or many PCI functions as appropriate for the deployment model. The various I O services such as networking clustering and block storage can then be arranged in various manners to map to the presented PCI function or functions as appropriate for the particular deployment model. All of the services are then performed using the protocol engine effectively independent of the deployment model as the various services are mapped to the protocol engine .

Virtual Device Generic term for the I O adapters inside ECA . The ECA of the preferred embodiments implements these virtual devices four host NICs which are connected to the operating system 12 internal NICs which are private or internal NICs that are not exposed to the operating system directly four management NICs one TCP Offload Engine TOE one iSCSI acceleration engine and one iWARP acceleration engine.

I O Service One or more virtual devices are used in concert to provide the I O Services implemented by ECA . The four major ECA I O Services are Network Accelerated Sockets Accelerated RDMA and Block Storage. A given I O Service may be provided by different underlying virtual devices depending on the software environment that ECA is operating in. For example the Accelerated Sockets I O Service is provided using TOE and Host NIC s in one scenario but is provided using TOE and Internal NIC s in another scenario. Virtual devices are often not exclusively owned by the I O Services they help provide. For example both the Accelerated Sockets and Accelerated RDMA I O Services are partly provided using the TOE virtual device. The only virtual device exclusively owned is iSCSI which is owned by Block Storage.

PCI Function ECA is a PCI multi function device as defined in the PCI Local Bus Specification rev 2.3. ECA implements from one to eight PCI Functions depending on configuration. Each PCI Function exports a group of I O Services that is programmed by the same device driver. A PCI Function usually has at least one unique IP address and always has at least one unique MAC address.

ECA Logical Model The ECA Logical Model describes how ECA functionality e.g. Ethernet ports virtual devices I O Services etc will be presented to end users. It is to be understood that certain aspects of the ECA Logical Model do not map directly and simply to the physical ECA implementation. For example there are no microswitches in the ECA implementation. Microswitches are virtual and the transaction switch implements their functionality. Further the ECA Logical Model is dynamic. For example different software environments and different ECA Ethernet port configurations will lead to different ECA Logical Models. Some of the things that can change from one ECA Logical Model to another number of microswitches can vary from 1 to 4 number of active PCI Functions can vary from 1 to 8 number of I O Services can vary from 1 to 7 and number of virtual devices can vary widely. Management and configuration software will save information in NVRAM that defines the Logical Model currently in use. Following are several examples of ECA Logical Models.

Each microswitch basically has the functionality of a layer 2 Ethernet switch. Each arrow connecting to a microswitch represents a unique endnode. The ECA preferably comprises at least 20 unique Ethernet unicast MAC addresses as shown.

A microswitch is only allowed to connect between one active Ethernet port or link aggregated port group and a set of ECA endnodes. This keeps the microswitch from requiring a large forwarding table resulting in a microswitch being like a leaf switch with a single default uplink port. Inbound packets always terminate at one or more ECA endnodes so that there is no possibility of switching from one external port to another. Outbound packets sent from one ECA endnode may be internally switched to another ECA endnode connected to the same microswitch. If internal switching is not required the packet always gets forwarded out the Ethernet or uplink port.

Each Ethernet port has its own unique unicast MAC address termed an ECA management MAC address . Packets using one of these management MAC addresses are always associated with a management NIC virtual device. Packets sent to these addresses will often be of the fabric management variety.

A box labeled mgmt filter within the microswitch represents special filtering rules that apply only to packets to from the management NIC virtual devices. An example rule Prevent multicast packets transmitted from a management NIC from internally switching.

If there is a mux or multiplexer in an ECA Logical Model this signifies packet classification. In for example the muxes associated with Block Storage Accelerated Sockets and Accelerated RDMA I O Services represent the quad hash from the TCP and IP values. The quad hash is used to determine whether a given packet is accelerated or not so that non accelerated packets go to the connected NIC and the accelerated packets go to the connected TOE.

Each I O Service is associated with an affiliated NIC group . An affiliated NIC group always contains four NIC virtual devices. The number of active NIC virtual devices within an affiliated NIC group is always equal to the number of ECA Ethernet ports in use. Organizing ECA NIC virtual devices into affiliated NIC groups is useful because it helps determine which NIC should receive an inbound packet when link aggregation is active and because it helps prevent outbound packets from being internally switched in some cases.

Each accelerated I O Service Accelerated Sockets Accelerated RDMA and Block Storage is associated with an affiliated NIC group because it provides a portion of its services using an affiliated TCP IP stack running on the host or server. The affiliated TCP IP stack transmits and receives packets on ECA Ethernet ports via these affiliated NICs. There may be multiple TCP IP stacks simultaneously running on the host to provide all of the ECA I O Services. The portion of services provided by an affiliated TCP IP stack are 

Initiates TCP IP connection An affiliated TCP IP stack is responsible for initiating each TCP IP connection and then notifying the ECA . Once notified the ECA will perform the steps required to transfer the connection from the host to the corresponding Accelerated I O Service and will then inform the host of the success or failure of the transfer in an asynchronous status message.

Performs IP fragment reassembly the ECA does not process inbound IP fragmented packets. Fragmented packets are received by their affiliated TCP IP stack for reassembly and are then returned to the ECA for higher layer processing.

This portion of services is algorithmically complex subject to numerous interoperability concerns is favored by Denial of Service DoS attackers and does not require hardware acceleration to achieve good performance in typical scenarios. For these reasons in the preferred embodiment these functions are provided using a host software solution rather than on board logic. It is understood that on board logic could be utilized if desired.

All I O Services transfer data between the ECA and the host using the Queue Pair QP concept from iWARP verbs. While the specific policy called out in the iWARP verbs specification may not be enforced on every I O Service the concepts of submitting work and completion processing are consistent with iWARP verbs. This allows a common method for submitting and completing work across all I O Services. The WQE and CQE format used on QPs and CQs across QPs on different I O Services vary significantly but the mechanisms for managing WQs work queues and CQs completion queues are consistent across all I O Services.

The ECA preferably uses a flexible interrupt scheme that allows mapping of any interrupt to any PCI Function. The common elements of interrupt processing are the Interrupt Status Register Interrupt Mask Register CQ and the Completion Event Queue CEQ . ECA has sixteen CEQs that can be distributed across the eight PCI Functions. CEQs may be utilized to support quality of service QOS and work distribution across multiple processors. CQs are individually assigned to one of the sixteen CEQs under software control. Each WQ within each QP can be mapped to any CQ under software control. This model allows maximum flexibility for work distribution.

The ECA has 16 special QPs that are utilized for resource assignment operations and contentious control functions. These Control QPs CQPs are assigned to specific PCI Functions. Access to CQPs is only allowed to privileged entities. This allows overlapped operation between verbs applications and time consuming operations such as memory registration.

System software controls how the ECA resources are allocated among the active I O Services. Many ECA resources can be allocated or reallocated during run time including Memory Regions PBL resources and QPs CQs associated with Accelerated I O Services. Other ECA resources such as protection domains must be allocated once upon reset. By allowing most ECA resources to be allocated or reallocated during run time the number of reboots and driver restarts required when performing ECA reconfiguration is minimized.

As noted above the ECA allows I O Services to be mapped to PCI Functions in many different ways. This mapping is done with strapping options or other types of power on configuration settings such as NVRAM config bits. This flexibility is provided to support a variety of different operating systems. There are two major operating system types 

Unaware operating systems In the context of this description unaware operating systems are those that do not include a TCP IP stack that can perform connection upload download to an Accelerated Sockets Accelerated RDMA or Block Storage I O Service. The TCP IP stack is unaware of these various ECA I O Services. With such operating systems the host TCP IP stack is only used for unaccelerated connections and one or more additional TCP IP stacks referred to throughout this description as internal stacks exist to perform connection setup and fabric management for connections that will use Accelerated I O Services. For example any application that wishes to use an Accelerated RDMA connection will establish and manage the connection through an internal stack not through the host stack.

Aware operating systems In the context of this description aware operating systems are those that include a TCP IP stack that can perform connection upload download to one or more of Accelerated Sockets Accelerated RDMA or Block Storage I O Service i.e. the TCP IP stack is aware of these various I O Services. Currently those operating systems are only from Microsoft. Future Microsoft operating systems will incorporate a TOE chimney or TOE RDMA chimney enabling connection transfer between the host TCP IP stack and the Accelerated Sockets or Accelerated RDMA I O Services. Typically the host TCP IP stack is used to establish a connection and then the ECA performs connection transfer to the Accelerated Sockets or Accelerated RDMA I O Service. The advantage of this cooperation between the host stack and the ECA is to eliminate the need for many or all of the internal stacks.

Each of the operating system types described above can be further classified by what driver model they support as described above. The two driver models are described below 

Independent Driver model Legacy operating systems such as Windows NT4 typically support only this model. These operating systems require a separate independent driver to load for each I O Service. With this model the I O Service to PCI Function ratio is always 1 1.

Consolidated Driver model Also known as a Bus Driver model. Newer operating systems such as Windows 2000 and to a greater extent Windows Server 2003 support this type of driver. Here a single operating system driver can control multiple I O Services which means that the I O Service to PCI Function ratio can be greater than one.

All of the examples below in this section show one Ethernet port per microswitch. It is understood that the ECA can be configured where there is more than one Ethernet port assigned per microswitch.

The first example is unaware operating systems independent driver model and is shown in . This Logical Model uses at least 16 IP addresses when all ports are active with one IP address per I O Service per active port. This programming model uses at least 20 MAC addresses with five per active microswitch . There are four independent TCP IP stacks running on the host in this environment the host stack connected to the Network I O Service an internal stack connected to the Block Storage I O Service an internal stack connected to the Accelerated Sockets I O Service and an internal stack connected to the Accelerated RDMA I O Service .

The Block Storage I O Service has access to both the iSCSI and iWARP virtual devices which allows it to support both iSCSI and iSER transfers.

If the host supports the simultaneous use of more than one RDMA API VI and DAPL then these APIs connect to the ECA through a single shared PCI Function.

It is understood that administration of a machine with multiple active TCP IP stacks is more complicated than administration of a machine with a single active TCP IP stack and that attempts to interact between stacks must use unconventional means to provide a robust implementation since no OS architected method for interaction is available.

Thus the Logical Model according to presents four virtual host NICs a virtual TOE a virtual iSCSI engine a virtual iWARP unit and a virtual management device . Each of the virtual devices is then connected to the virtual microswitches which in turn are connected to ports . The devices are virtual because as shown in no such devices actually exist only the devices shown in those Figures. However the ECA presents these virtual devices to conform to the requirements of the unaware operating system independent driver deployment situation. These virtual devices are configured as appropriate to provide the desired I O service or function such as Network I O Services Block Storage I O Services RDMA I O Service and Accelerated Sockets I O Service .

The second example is the unaware operating systems consolidated driver model as shown in . For simplicity only the differences from the unaware operating system independent driver logical model are discussed here.

All I O Services plus ECA management can be programmed via a common PCI Function. For some operating systems the Block Storage I O Service might continue to require its own PCI Function.

By consolidating the Accelerated Sockets Accelerated RDMA and Block Storage I O Services under a common PCI Function I O Services are able to share a common internal stack. Since only two stacks are used the used number of IP addresses can be reduced from 16 to 8. Further eight Internal NICs are not used reducing the required number of MAC addresses from 20 to 12.

This model uses this fixed mapping between I O Services and PCI Functions PCI Function 0 Management network Accelerated Sockets Accelerated RDMA and Block Storage I O Service.

The operating system software overhead is higher in this model as discussed above especially in the interrupt distribution area. The device driver portion of the bus model is also more complicated to implement than legacy device drivers.

The virtual devices presented in the Logical Model according to are slightly different from those presented according to . According to only a single virtual device is presented to the operating system. Here however the virtual devices of host NICs TOE iWARP engine and iSCSI engine are configured for one combined I O Service which handles block storage RDMA accelerated sockets and normal network operations.

The third model is the aware operating system consolidated driver model and is shown in . Again for simplicity only the differences from unaware operating system consolidated driver logical model are described.

With the operating system aware the host NICs and host TCP IP stack can be used to set up accelerated TOE and iWARP connections. An internal stack is present to supply the Block Storage I O Service and may be used to supply the Accelerated RDMA I O Service as well for those RDMA APIs that are not native to the operating system. For example the DAPL API will not be native to the Microsoft chimney enabled operating system. The used number of IP addresses is eight. The used number of MAC addresses is 12

The Logical Model according to presents slightly different virtual devices from that of as a second iWARP engine is presented. Here again a consolidated I O service is provided.

As common background WSD requires a SAN NIC to support both accelerated RDMA enabled traffic and unaccelerated host TCP IP traffic. The SAN NIC accomplishes this by providing a normal NDIS driver interface for connection to the host TCP IP stack and by providing a proprietary interface to the WSD Provider or SAN Provider and the WSD Proxy or SAN Management Driver for SAN services.

WSD allows for each SAN NIC to connect to a fabric that contains some IP subnets that are RDMA enabled and some that are not. For example on an InfiniBand SAN there might be an IP over IB gateway that connects the SAN to an Ethernet network that is reachable only via the SAN. Also for example on an iWARP SAN there might be some subnets that do not have ECA adapters but rather are connected using ordinary Ethernet NICs.

The Windows Sockets Switch keeps a list of IP subnets that are RDMA enabled. When both endnodes in a sockets session are not RDMA enabled or are not on the same IP subnet or if the session is not using TCP transport then the Windows Sockets Switch implements the session using the host TCP IP stack. Only when both endnodes in a sockets session are RDMA enabled and on the same IP subnet and when the session is using TCP transport will the Windows Sockets Switch implement the connection using the WSD Provider path. The concern here is that there will be a combination of accelerated and unaccelerated traffic on the RDMA enabled IP subnets of the SAN.

In one implementation the WSD proxy driver includes an internal stack for initiation of accelerated connections etc. The WSD architecture assumes that the SAN fabric does not use IP addressing and that a translation from IP addresses to SAN addresses is required. The translation is expected to take place in the NIC driver for unaccelerated traffic and in the WSD Proxy Driver for accelerated traffic. Of course this assumption is not correct for the ECA . The ECA NIC driver does not require address translation capability. However a translation is still required for accelerated traffic so that accelerated traffic can be distinguished from unaccelerated traffic on the RDMA enabled IP subnets of the SAN. This translation is carried out in the WSD Proxy Driver.

According to the Logical Model of multiple iWARP engines are presented one associated with each host NIC as accelerated RDMA Network I O Services are provided.

ECA configuration software uses silicon capabilities combined with user input to configure which PCI functions to enable and which I O Services are mapped to which enabled PCI functions. This configuration information termed EEPROM Boot up Register Overrides is stored in the ECA EEPROM not shown . Upon hard reset the ECA automatically reads this configuration information out of EEPROM and applies it to the ECA PCI configuration space registers. Typical registers that require EEPROM Boot up Register Override include Device ID Class Code Subsystem Vendor ID Subsystem ID Interrupt Pin and Config Overrides.

During reset initialization the ECA decides which PCI functions to enable using information stored in the Config Overrides PCI Configuration register. When a given PCI function is not enabled then attempts to access its config space will result in master abort.

Thus the variation between Logical Models of the ECA can be seen. The configuration registers are configured to present the appropriate Functions or I O Services and their related register sets to the PCI bus. For example eight separate Functions are presented in instances while one combined Function is presented for instances.

As the protocol engine is a single unit mapping values inside the protocol engine are used to associate I O Services and related virtual devices to the exposed PCI Functions. Exemplary mapping values include the NIC or NICs associated with a given MAC address the outcome of the quad hash function and connection context fields including protocol such as iSCSI iWARP etc. a value designating the responsible NIC and the relevant PCI Function. A given NIC is only a virtual or logical construct inside the protocol engine as only one actual hardware grouping is provided to do each function.

Each packet received from the Ethernet fabric is identified using its destination MAC address quad and other packet header fields with a set of mapping values managed by the protocol engine which determine the Virtual Device s that will perform processing on the packet and the I O Service and PCI function the packet is affiliated with. The protocol engine uses the mapping values to transfer relevant portions of this packet across the PCI interface or and into host memory using the proper PCI Function. In the preferred embodiment the ECA supports the programming of any I O Service and any Virtual Device from any PCI Function. When drivers load they learn through configuration parameters which I O Services and Virtual Devices are configured as active on their PCI function and restrict themselves to programming only these I O Services and Virtual Devices. When a driver posts a new command to the adapter mapping values inside the protocol engine are used to associate each command with the appropriate I O Service Virtual Device s and an Ethernet port. This enables the protocol engine to determine the correct sequence of Virtual Devices that must process the command in order to carry it out. When processing a command involves transmission of packets the packets are transmitted on the Ethernet port defined by said mapping values. The mapping values are chosen and resulting values are sufficiently flexible to allow handling of the various instances described above and others that will arise in the future.

As an example consider the logical model of and specifically PCI Function 6. The configuration parameters typically include standard PCI configuration space register fields Vendor ID Device ID Revision ID Class Code Subsystem Vendor ID and Subsystem ID and may include additional fields. In the preferred embodiment most of the standard PCI configuration space register fields can be modified by user configuration software to suit the particular logical model desired while keeping said fields as read only from the perspective of the host driver. In this example the driver loaded by the operating system on PCI function uses the configuration parameters to determine that it is to provide the Block Storage I O service and has been allocated a set of virtual devices NES NICs 12 15 iWARP iSCSI and TOE to do so. This set of virtual devices enables the driver to transmit and receive block storage traffic on any Ethernet port of the device but in this example storage traffic is configured as best provided on port 2.

Each I O Service has one or more dedicated host memory work queues not shown in for posting driver commands to the ECA . When the driver posts a command for transmission of block storage traffic the mapping values enable the protocol engine to know for example that all commands posted to a given work queue are associated with PCI Function 6 Block Storage I O Service iWARP virtual device the TOE virtual device and Ethernet port 2 . This knowledge of Virtual Devices enables the protocol engine to determine the correct sequence of submodules to carry out the command which in this case would be WQM then ITX then TTX . Each work queue has independent mapping values. The mapping values enable the protocol engine to interpret any posted command in the context of the configured Logical Model and to carry out packet generation stipulated by the command using the correct set of Virtual Device s and Ethernet port.

When a packet is received at Ethernet port 2 the ECA uses its header fields to identify it with a set of mapping values. In this case a first packet s header fields might identify it with mapping values that affiliate the packet with PCI Function 6 Block Storage I O Service NES NIC 14. A second packet s header fields might identify it with mapping values that affiliate the packet with PCI Function 6 Block Storage I O Service TOE virtual device and iWARP virtual device . This knowledge of Virtual Devices enables the protocol engine to determine the correct sequence of submodules to carry out packet processing which for second packet would be TRX then IRX then WQM then CQM . The mapping values enable the protocol engine to interpret any received packet in the context of the configured Logical Model to carry out received packet processing using the correct set of Virtual Device s and to transfer relevant portions of this packet across the PCI interface or using the proper PCI Function.

Had the same packet stream going to the same storage device be provided in a case according to the mapping values are different to provide for only PCI Function 0 being identified. However the mapping values would still identify the various logical components that are relevant to simplify tracking and sharing of the resources on the ECA .

By having the mapping capability and the flexibility in the mapping capability and the various internal components numerous operating system deployment models can be handled by a single ECA . This flexibility allows maximum usage of the ECA in the maximum number of environments without requiring different ECAs or major user reconfiguration.

It will be understood from the foregoing description that modifications and changes may be made in various embodiments of the present invention without departing from its true spirit. The descriptions in this specification are for purposes of illustration only and are not to be construed in a limiting sense. The scope of the present invention is limited only by the language of the following claims.

