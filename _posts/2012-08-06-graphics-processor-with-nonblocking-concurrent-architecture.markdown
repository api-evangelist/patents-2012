---

title: Graphics processor with non-blocking concurrent architecture
abstract: In some aspects, systems and methods provide for forming groupings of a plurality of independently-specified computation workloads, such as graphics processing workloads, and in a specific example, ray tracing workloads. The workloads include a scheduling key, which is one basis on which the groupings can be formed. Workloads grouped together can all execute from the same source of instructions, one or more different private data elements. Such workloads can recursively instantiate other workloads that reference the same private data elements. In some examples, the scheduling key can be used to identify a data element to be used by all the workloads of a grouping. Memory conflicts to private data elements are handled through scheduling of non-conflicted workloads or specific instructions an deferring conflicted workloads instead of locking memory locations.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08692834&OS=08692834&RS=08692834
owner: Caustic Graphics, Inc.
number: 08692834
owner_city: San Francisco
owner_country: US
publication_date: 20120806
---
This application is a continuation of PCT 2012 42591 filed on Jun. 15 2012 entitled Graphics Processor with Non Blocking Concurrent Architecture which claims priority from U.S. Provisional Patent App. No. 61 497 915 filed on Jun. 16 2011 all such applications are hereby incorporated by reference in their entireties for all purposes.

The following relates generally to utilization of parallel processing resources and in one more particular aspect to programming interfaces and architectures that can be used in different classes of parallel computing problems and in particular to graphics processing including ray tracing.

Computer architectures seek to increase parallelism at a hardware level and at a software operating system level. A variety of techniques have been proposed to address parallelism for hardware and software. One general concept is to use multi threading as a mechanism to increase parallelism.

Regarding software based multithreading an operating system can concurrently execute multiple processes typically multiple independent programs on a single hardware execution resource by interleaving in time the execution of the processes. Such interleaving can be accomplished by an OS kernel which determines what process has priority and also can access the resources it needs to proceed with execution.

When a process has two or more semi independent subtasks multiple threading can increase throughput give better response time speed operations improve program structure use fewer system resources and make more efficient use of multiprocessors. With multi threading a process has multiple threads of control. The concept of multithreaded computing is increasingly common as faster clocks for processors becomes prohibitively power consumptive and transistor budgets are used on wider architectures. As such the term thread has come into increasing usage but is not always used to refer to the same concept but rather a context of usage can inform the meaning of the term.

A process can be viewed as an individual program or application that can be executed. As such a process has a sequence of instructions to be executed. A thread also has a sequence of instructions for execution but typically is more granular. For example a function can represent a thread s sequence of instructions. If instantiated by a process a thread shares that process address space yet can own unique resources within the process. Instructions from each thread can be individually scheduled which allows threads of a process to be executed concurrently as resource availability dictates.

Threads can be managed by a creating process and be invisible to a kernel of an operating system also referred to as user level or application level threads . User threads are handled in user space and controlled using a thread API provided in a library. Such control includes operations such as creation termination synchronization scheduling. Creation termination and synchronization operations can be performed using user space threads. Because user space threads are not directly visible to the kernel which is aware only of the overriding process containing the user space threads if a user space thread blocks its entire process blocks. When this happens the benefit of threads parallelism is lost or reduced. Additional layers of complexity can be introduced to reduce blocking but at a performance cost.

Kernel threads are handled in kernel space and created by the thread functions in the threads library. Kernel threads are kernel schedulable entities visible to the operating system. Kernel threads exist within the context of a process and provide the operating system the means to address and execute smaller segments of the process. Kernel threads also enable programs to take advantage of capabilities provided by the hardware for concurrent and parallel processing. With kernel threads each user thread can have a corresponding kernel thread. Each thread is independently schedulable by the kernel so if an instruction being executed from one thread blocks instructions from other threads may be able to run. Creation termination and synchronization can be slower with kernel threads than user threads since the kernel must be involved in thread management. Overhead may be greater but more concurrency is possible using kernel threads even with a uniprocessor system. As a result total application performance with kernel space threads can surpass user space thread performance. However developers must be more careful when creating large amounts of threads as each thread adds more weight to the process and burdens the kernel.

Kernel threads can change the role of processes in that a process is more of a logical container used to group related threads of an application in such an environment. Each process contains at least one thread. This single initial thread is created automatically by the system when the process starts up. An application must explicitly create the additional threads. An application with only one thread is a single threaded. An application with more than one thread is a multi threaded. An example treatment of a thread with respect to other threads is provided below.

A process s state information includes a program counter indexing a sequence of instructions to execute and register state. The register context and program counter contain values that indicate the current state of program execution. The sequence of instructions to execute is the actual program code. For example when a process context switch takes place the newly scheduled process s register information tells the processor where the process left off in its execution. More specifically a thread s program counter would contain the current instruction to be executed upon start up.

Like the context of a process the context of a thread consists of instructions attributes user structure with register context private storage thread structure and thread stack. Like a process a thread has a kind of life cycle based on the execution of a set of control instructions. Through the course of time threads like processes are created run sleep and are terminated. New processes and in a multi threaded OS at least one thread for the process typically are created using a fork call which produces a process and thread ID. The process and its thread are linked to the active list. The new thread is flagged runnable and thereafter it is placed in a run queue. The kernel schedules the thread to run changing the thread state to an active running state. While in this state the thread is given the resources it requests. This continues until a clock interrupt occurs or the thread relinquishes its time to wait for a requested resource or the thread is preempted by another higher priority thread. If this occurs the thread s context is switched out.

A thread is switched out if it must wait for a requested resource otherwise the processor would block . This causes the thread to go into a sleep state. The thread sleeps until its requested resource returns and makes it eligible to run again. During the thread s sleep state the kernel charges the currently running thread with CPU usage. After a period of time the kernel can cause a context switch to another thread. The next thread to run will be the thread with the highest priority of the threads that are ready to run. For the remaining threads in that ready state their priority can be adjusted upwards. Once a thread acquires the requested resource it calls the wakeup routine and again changes state from sleep to run making the thread eligible to run again. On the next context switch the thread is permitted to run provided it is the next eligible candidate. When allowed to run the thread state changes again to active running. Once the thread completes it exits releases all resources and can transfer to a zombie state. Once all resources are released the thread and the process entries are released and the linked lists of active processes threads can be updated.

Regarding more hardware oriented parallelism the capability of implementing parallel computing hardware is available in almost all computing devices ranging from powerful mobile or desktop CPUs that may have 4 8 16 32 or several dozen processing elements that are relatively complex to Graphics Processing Units GPU with many e.g. hundreds of relatively simple processors.

While GPU s where originally designed and still are primarily used to accelerate raster graphics GPUs have gradually become more programmable. Such increased programmability has allowed for some multi threaded computing problems to be expressed and executed on GPUs. Although GPU architectures have extremely high theoretical peak Floating Point Operations per Second FLOPS only rasterised graphics and certain well behaved highly streamable compute problems can come close to realizing an actual throughput near to the theoretical throughput.

Increasing parallelism of computation typically involves a tradeoff between algorithmic complexity and overhead incurred in managing sharing of computation resources. Another important consideration is that algorithms execute correctly such that data values are not corrupted. Parallel execution of threads which may use shared data can cause data corruption by improper timing of reads and writes to shared data values. Negotiating access or serializing access to such shared data values incurs overhead.

Another concern largely is ensuring correctness of data during increasingly parallel execution. Principally parallelism and avoidance of data corruption among different threads is handled through locks on variables that are at risk for being written or read out of correct order by conflicting processes in a system. The concept essentially is that when a given process wants to write to such a variable e.g. a global variable the process attempts to lock the variable. If no other process has a lock on the variable then the lock can be granted to the requesting process. The process performs its write and then releases the lock. As can be discerned the usage of locks depends on a mechanism to detect or identify which variables should be protected by locks and an arbiter to check lock status grant lock status and revoke lock status. Often which variables need to be protected depends also on whether a given software procedure is intended to be available for usage on multithreaded platforms in which a number of such instances of such procedure may be executing in concurrently executing processes.

Programmers can specify portions of code in a process to be protected by a lock. At compile time a compiler can process such code and in some cases also could detect variables that may be at risk but which were not protected and can insert such locks into the compiled object code.

One implementation of a lock is Mutual Exclusion mutex which controls how different portions of computer code can access a shared resource such as a global variable. For example a portion of code that needs to read a given variable should not be allowed to execute during an update of that variable by another process. The term mutex also is used in the sense of a program element that operates to negotiate a mutual exclusion of different program elements for conflicting variables. In a multi threaded execution environment a number of threads can be executing and ultimately may need to access a given data object protected by a mutex. If the mutex currently is active then typically the operating system will cause the threads that are waiting for that mutex to go to sleep i.e. cease execution . Thus as more and more threads reach the mutex they each are made to go to sleep to await a time when the operating system indicates that they can continue execution. The operating system can wake these threads by a relative order in which they arrived at the mutex the order in which they are awakened may simply be indeterminate.

Other approaches to allocation of shared resources includes semaphores which allocate a set of undifferentiated resource elements among processes that desire to use the elements. A semaphone essentially counts how many elements of the resource are available for use from the set adjusting the count as resources are reserved and released. When no elements of a given resource are available then a process requesting use of the resource waits until an element of the resource has been released. Typically semaphores are implemented in operating systems for resource allocation.

Variations on basic locking concepts exist such as spinlocking which allows threads to continue to actively ask when a given lock is free such that their responsiveness is increased by not having to be awakened which incurs overhead context swapping . However spinlocking does use processor cycles and consequently inherently reduces efficiency. Other variations include recursive locking implementing time limits on locks such that a process will not wait indefinitely to acquire the lock for a particular object but instead continue execution if possible if its lock request is not fulfilled in a given time period. Such an approach is useful in embedded systems where absolute timing execution for critical processes may be necessary.

A further example variation is recursive locking which is a strategy proposed for computing architectures that have Non Uniform Access NUMA to a shared memory. Typically groups of general purpose processors each with an onchip cache and operating in a system with a shared main memory will have NUMA properties practically a large majority of low cost computation platforms . Recursive locking is based on the realization that a process resident on a processor that has a locked memory location standard test and set locking is more likely to be granted access to that locked memory location than a process executing on a different chip due to differences in communication delay incurred by interconnect between the processors. Recursive locking seeks to enforce a predictable and controllable preference for granting a lock to a process that may have more data locality by granting locks to processes executing on the node that already has the lock by adding a condition to the lock granting process that tests an identifier for which node is hosting the thread requesting the lock.

Aspects of an example system for performing concurrent graphics computation include a cluster of computation elements where each computation element comprises a local memory and a plurality of Single Instruction Multiple Data SIMD Arithmetic Logic Units ALUs . The system also has a scheduler for the cluster of computation elements. The scheduler is operable to receive inputs defining instances of a first type of computation to be performed on the cluster where each of the instances is associated with a respective scheduling key. The scheduler is operable to sort the instances according to their scheduling keys into packets and emit a packet comprising a plurality of instances of computation. The system has a distributor that is operable to receive the packet and distribute the instances among the computation elements of the cluster. Each of the computation elements is operable to combine instances with different scheduling keys for concurrent execution for which the same computation is to be performed and to concurrently execute the computation for the combined discrete portions of computation.

Exemplary further aspects can include that instances can be synchronized for memory accesses to memory locations that are open to reading and writing with other instances by identifying conflicting memory ranges among different instances before instructions accessing those conflicting memory ranges have been dispatched for execution. The scheduler can perform such identifying of conflicting memory ranges using instance specific memory addresses for a variable declaration. The system can disperse instance specific memory ranges among the plurality of computation elements during a setup phase. Each computation element can be operable to schedule groupings of instances of the second type of computation according to criteria comprising 1 execution from a common source of instructions and 2 enforcing a rule that no grouping of instances to be executed in parallel may include multiple instances that write the same memory location.

In a further exemplary aspect a method to be performed in scheduling graphics computation on a computer system provides for distributing workloads of a first type among a plurality of computation clusters. Each cluster is operable to schedule execution of instructions from first type workloads assigned to that cluster independently from other clusters of the plurality. Groupings of workloads of a second type are formed for execution in the plurality of computation clusters. The groupings are determined based on respective scheduling keys associated with workloads of the second type. Each scheduling key defines one or more of a data element to be used during execution of each grouping of workloads and a set of instructions to be executed for each workload of the second type. A grouping of workloads is selected and workloads of the grouping are distributed among the plurality of computation clusters.

A further exemplary aspect includes a computation component for use in a computation system comprising a memory comprising a transitory portion and a persistent storage portion. The component further comprises a plurality of Arithmetic Logic Units ALUs each having read and write access to the memory and an input queue for receiving discrete definitions of computation to be performed on the plurality of ALUs. The definitions comprise a program reference and a scheduling key. A local scheduler is operable to schedule the plurality of ALUs. The local scheduler is operable to prioritize scheduling of computation by identifying groupings of definitions of computation that have the same program reference and the same scheduling key. The scheduling key is used for identifying a location in the high turnover portion of the memory storing data to be used by the ALUs when executing the referenced program.

In a further aspect a tangible computer readable medium having computer executable instructions stored comprises instructions for a thread of computation to be executed on a computation unit. The instructions for the thread comprise instructions to call a computation routine. The call of the computation routine comprises a definition of a scheduling key a limited memory reference identifying a memory location for reading and writing and a memory location to be read for that computation routine and instructions that are to be executed after calling the computation routine without waiting for return of information produced by the called computation routine during its execution. The computation routine further can be recursively instantiated and each recursively instantiated instance automatically inherits the limited memory reference and the read memory location is different. The read memory location can be updated based on a regular access pattern of a type of data being accessed.

One reason that parallelism does not scale in accordance with computation resource availability is that memory bandwidth is not scaling in proportion to the available computation capability. Therefore computational problems that do not enjoy inherent data locality can cause the computation elements to wait on data being accessed from memory. Memory access remains a problem even as peak theoretically available memory bandwidth has gone up because such peak bandwidths assume burst transfer rates that often are unrealistic for these kinds of computation problems. Further scalability of memory interface performance is hindered by other factors such as pin count of the memory interface and the fact that memory access latency does not readily scale with process technology changes. For example DDR3 Column Address Strobe CAS latency can be around 9ns while typical DDR2 CAS latency is around 10ns. Also memory latency between processing nodes also may be a factor in different computational paradigms such as NUMA architectures.

Further the SIMD nature of the compute clusters means that simultaneously executing threads must follow identical program execution paths in order to realize peak computation throughput. For example if half of the threads in such a SIMD cluster take a branch one way and the remainder the other branch the hardware must serialize these two paths i.e. one half stalls while the other half executes so that ultimately the SIMD cluster can once again execute on vectorized data . In such a situation the computation unit executes at only 50 throughput. In situations where a code base being executed contains many branches a worst case performance from SIMD loss alone can be 1 SIMD width which is about 3 efficiency on a 32 wide SIMD architecture. There are a large number of computing problems including ray tracing spatial search sorting and database traversal which while theoretically parallizable have not mapped efficiently to such wide SIMD architectures.

Some applications of the technology described below relates to graphics processors such as processors that can perform rasterization and or ray tracing. With particular regard to ray tracing ray tracing can be used to produce realistic images rendered from 3 D scenes in the context of video games motion pictures animated advertisement industrial models architectural simulation and so on. One construct employed in the field of rendering is to provide a physical scene model and associate surface information with different parts of the scene model. For example a scene model can contain objects including a person an automobile and a building. The physical model of the scene would describe the surfaces of these objects for example as a wire frame model which can include a large number of primitive shapes interconnected with each other to describe boundaries of the surfaces. This physical model generally lacks information about the visual appearance of the objects surfaces. Then information and programming is associated with particular surfaces and or portions of particular surfaces that describe their appearance. Such information can include textures for the surfaces while programming associated with the surfaces often is intended to model what effect the surface has on light hitting the surface. For example programming allows modeling of glass a shiny surface a bumpy surface and so on. Such programming and information thus is bound or otherwise associated with portions of the physical model descriptive of those surfaces. For example programming can be associated with or bound to a particular primitive. Such programming and other description or portions thereof for a particular primitive or scene object can be referred to generally as a shader for that primitive or object.

The above description shows that the term thread can connote different meanings in different circumstances but is not an exhaustive explanation of potential usages of the term . Therefore in the description that follows terminology will be used in an attempt to reduce reliance on terminology associated with multi threading e.g. by introducing the term fibre as a moniker that is intended to refer collectively and individually to a plurality of concepts not all of which may need to be practiced in every implementation according to this disclosure. Nevertheless multi threaded operating systems and machines that support concurrent and parallel processing of multi threaded applications may be adapted to implement aspects of these disclosures as will become more apparent from the description that follows.

In one aspect a process in a multiprocessing environment or a thread in a multi threaded environment or both in a multiprocessing multi threaded environment can use an API to create one or more units of work to be performed each called a fibre herein for sake of convenience.

In one aspect a fibre is an instance of code such as a routine e.g. a C function declaration that can be forked repeatedly by a parent thread or recursively entered from another instance of the same fibre routine. An instance of a fibre routine instances of fibre routines are for convenience simply called fibres includes an identifier for an element of data used as a scheduling key and identification of a particular element a declaration of of fibre storage.

Fibres sharing a scheduling key can be grouped for concurrent execution as described below such grouping does not impose a requirement that such execution occur strictly in parallel . All fibres referencing the same element of fibre storage can be channeled for execution to a computation element having that element of fibre storage locally cached. In one exemplary implementation a given element of fibre storage for a given fibre or set of fibres has at least the characteristic that this element of fibre storage is readable and writable only by that fibre or set of fibres this limitation can be time variant or absolute. This limitation can be enforced by appropriate programming practices by a compiler by hardware and by some combination thereof.

In one aspect a fibre can be defined by a standard C function declaration and contains all the code required to execute the fibre to completion standard inline expansions of functions can be provided . An operative difference being that a fibre is not to wait for a return value from another function called within the fibre. In the exemplary aspect the fibre routine function declaration contains at least one argument for which a value is stored in fibre storage associated with the thread that forks fibres using that fibre routine declaration.

In one exemplary aspect all fibres that reference a given element of fibre storage e.g instantiated by a parent thread or fibre have their execution controlled or distributed so that they execute serially within a given processor a processor can itself by a grouping of processing elements and in one example can include a plurality of SIMD processor elements and concurrently or generally in parallel with other fibre routines that have the same or matching fibre key s . In one example a grouping of processor elements considered to be a processor in this context is determined according to which processing elements can write to a particular fibre storage location. In one example all processor elements that can write to a particular fibre storage location are considered as a group to be a processor for the purposes of enforcing serial execution of fibres that reference that particular fibre storage location. Such writing can be considered with respect to a subset of memories in a system such as local memories whereas accesses may not be so treated.

In one exemplary aspect only fibres that ultimately trace to a common parent access a fibre storage element allocated for these fibres. Thus a fibre that traces to a different parent for that set do not access such fibre storage location. Further in exemplary aspects no two or more fibres from the same parent are scheduled to be executed in parallel so that no mutex or lock management is required to fibre storage locations.

However in such exemplary aspect a fibre otherwise can have full access to all the features that a standard thread would have including full global memory access local memory barriers texture samplers private variables and all arithmetical and logical operations. The fibre routine may also contain arguments that are directly taken from the kernel arguments of its parent thread which allows the fibre to have access to global memory pointers and other shared variables. These memory locations can be accessed using standard memory protection techniques.

Each thread in the system can define a plurality of fibres each with a respective scheduling key. Further a plurality of fibre routines can be defined such that a given thread can define a number of fibres which are instances of different fibre routines. As explained above in an exemplary aspect fibres of the same fibre routine sharing a code base and from the same parent e.g. a parent thread or fibre do not overlap in execution in implementations that avoid mutexes or locks for accessing at least one variable or data source that is available only to these fibres.

In some implementations an API can be provided for instantiation invocation of fibre routines each fibre being an invocation of a fibre routine as explained above by a parent thread or another fibre through a semantic an example of which is provided below . In some aspects and unlike a function call in a typical multi threaded environment a kernel or other system scheduler resource does not necessarily immediately attempt to execute or make preparations to execute the fibre such invoked as it would a thread in a typical multi threaded environment . Examples of such preparations were introduced above.

Instead in one exemplary aspect its activation allocating actual processing resources for execution is deferred until an unknown future time. Further after a parent thread instantiates a fibre e.g. by using a fibre definition API semantic the parent thread can continue execution if otherwise permitted e.g. continue to be an active thread incrementing its program counter and using hardware resources . An API also can provide a semantic for instantiating a number of fibres e.g. of the same fibre routine or of different fibre routines at the same time. In one example each of the fibres can have a common fibre key and in another example the fibres can have the same fibre key but operate on one or more different values.

Because in this exemplary aspect a parent thread does not block or wait for a return value from this fibre a fibre does not return any return value to a parent thread as would a standard C function. By contrast upon completion a fibre routine can simply cease to exist e.g. go into a zombie state awaiting deallocation of memory resources and does not return to the caller parent thread. Thus for a fibre in this aspect to have effect or produce useful output the fibre is to write results into fibre storage. Note that although the fibre has the capability to write results to a memory resource shared outside of the fibre e.g. global or local memory using a mutex or locking management such an operation would be much slower than a write to fibre storage.

In an exemplary aspect because fibres do not return a value data that persists between fibres e.g. fibres instantiated from a common thread is confined by data resident or declared as resident in fibre storage. In practice it is expected that the exemplary aspects disclosed herein would be practiced by allocating a fast available memory resource to fibre storage and as such would typically be expected to be a more limited resource. For example as between a main memory and a cache memory a comparatively larger portion of the cache would be dedicated to fibre storage than to other data that may be used by fibres.

Another aspect of some generality is that fibres that reference the same element of fibre storage are reference counted e.g. a counter is incremented when a fibre referencing that element of fibre storage is emitted and decremented when such a fibre completes . A reference count hitting zero thus indicates that there are no more fibres outstanding which reference a given fibre storage location or locations in a case where a given set of fibre references a number of locations . Thus such a reference count reaching zero can be a trigger to a parent thread that data in the fibre storage is available for usage and or fetching. A flag set in response to the zero value such as a flag provided by a built in function can be monitored by the parent thread by way of further example.

The following is an example construct of coding semantics that can be used in an implementation according to these disclosures. This example is by way of explanation and not limitation as to how a particular implementation may be structured or the contents of such an implementation.

 fibre This qualifier is used to declare a function as being a fibre routine. A fibre routine place this qualifier at the beginning of the function declaration.

 fibrestorage This qualifier declares a variable as being present in fibre storage. Inside a parent thread variables that are to be shared between child fibres are declared with this qualifier. Fibre routines do not declare fibre storage. Fibre routines contain at least on argument that has this qualifier.

 fibrekey A fibre key is used to indicate one or more criteria used in scheduling the execution of the fibre. Examples of fibre keys include an offset in global memory and a branch condition. A platform can schedule a fibre to run in parallel with other fibres based on fibre key matching. In one implementation a fibre routine function declaration contains exactly one argument of this type.

 Fibreroutine a b A fibre routine is invoked by making a standard function call but placing a symbol ahead of the function name. Such invocation does not cause a jump execution to the fibre routine immediately but rather defers its invocation to a future time. Execution of the calling routine can continue as normal and the fibre routine does not return a value as does a normal function call.

bool is initial thread Iaunch This is a built in function which only returns true when called by the parent thread on its initial launch. This function is provided for convenience to differentiate between the parent thread routine being called by a typical thread launch and notification that all child fibres from a parent thread are finished can be determined by a reference count hitting zero .

The pseudocode in the Appendix exemplifies an approach to fibre computing in which threads can instantiate fibres that behave according to this disclosure and in which fibres can recursively instantiate other fibres that reference the same fibre data storage but which have varying scheduling keys.

In view of the above introductory explanation and by way of further explanation a variety of architectural examples and operational situations are explained below. Architectures in which fibre computing aspects can be practiced are diverse. One exemplary type of architecture is one where a plurality of computation clusters each comprise a plurality of Arithmetic Logic Units ALUs and a local controller that controls the ALUs. In a more particular instance of such architecture all the ALUs of a cluster are run from a common program counter selected by the local controller. Each ALU however can have an independent port to a local memory. In some examples the local memory can operate similar to a register set of a general purpose processor each ALU can read and write to and from the local memory. In some implementations the active program counter in each cluster can be changed on an instruction by instruction basis without latency. In some architectures having such a plurality of clusters full threads can be executed in each cluster and state for such threads maintained in the local memories of the clusters. In some implementations a local controller may treat fibres differently from threads in terms of scheduling their execution in the ALUs controlled by that local controller. In some implementations no fibre that references a common fibre memory storage location i.e. the fibres share a common origin would be scheduled for concurrent execution in the cluster in any implementation where memory arbitration is not implemented. One example architecture explicitly avoids arbitrating for local memory access by fibres so that memory accesses can be accelerated.

Example workloads processed in computation architectures implementing disclosed aspects primarily relate to graphics rendering workloads and more particularly ray tracing is provided as a principal application example of fibre computing principals. However a variety of computational problems can be addressed by application of disclosed aspects of fibre computing. In the context of implementing ray tracing functionality using aspects of fibre computing a graphics chip may continue to implement rasterization based graphics processing with thread computing principals and using the same plurality of computation clusters. Vertex and pixel shading functions which may apply to geometry and image processing for similarly can be implemented according to fibre computing or thread computing principles.

Fibre computing system architectures thus provide for acceptance of workloads specified from divergent sources such as different threads or other fibres. Each workload can have a first and a second data element where one of the data elements can be used as a scheduling key to control how that workloads is grouped with other workloads and the other controls where among a plurality of computation elements that workload is executed.

Example system architectures in which disclosed aspects may be practiced are depicted in . depicts an architecture of an example system in which concurrent and parallel processing and scheduling can be implemented according to these disclosures. Systems according to exemplary system can be used to implement graphics processors for example.

A host interface may includes a thread API and may include a fibre API . Thread API can be used by routines running on a host processor not depicted in order to instantiate new threads to be executed on system . Thread API can be provided for example as a firmware coded API available in a System on Chip SoC with one or more generally programmable processor cores such as a computation core operating according to an architecture specification and with an instruction set architecture. In other examples thread API can be provided with a driver for an add on card that off loads graphics processing loads from the host processor.

In one example and as described below fibre API can be used by threads running on the host processor to instantiate fibres that are executed in systems according to example system . As will be described below threads executing in such systems also can access an API provided according to these disclosures such that threads executing on the system also can define fibres for execution. In some examples a fibre API is not exposed at the host interface level but rather is made available only to threads executing within the graphics processor system .

Example system includes a number of masters which function to setup computation to be performed in system . In some examples computation performed in system is considerably more regular and voluminous than typical application code intended for execution on a processor. Rather example workloads can include workloads to shade large set of vertices or pixels. Vertex data master and pixel data master can be provided to setup the usage of the available computation elements described below to perform such computation . By further example compute data master and ray data master can be provided to setup computation for a large scale numerical analysis programs and for ray tracing workloads.

Coarse scheduler receives inputs from data masters such as data masters described above. Coarse scheduler can operate to allocate independently operable computation elements to perform the computation loads that can come from the data masters. Coarse scheduler receives status information from resources available in system . Such resources include status of memories located within an array of clusters described below . Such memories may be private to particular computation units e.g. cores within the array of clusters such as memories . These memories can be implemented as caches that can be allocated among threads that are executing on the core coupled with each memory e.g. memory can be allocated among threads executing on core . The allocation of such memory can be handled by coarse scheduler . Each data master vertex pixel computer ray can communicate memory allocation requirements to coarse scheduler . Aspects of operation of coarse scheduler and communication between a data master for the purposes of thread scheduling and resource allocation is described in U.S. Patent Pub. No. 2007 0101013 to Howson which is incorporated by reference in its entirety herein. Such operation is exemplary rather than exhaustive such operation also may be adapted in view of the present disclosures.

The exemplary system also includes a packet unit that includes constituent components of a ready stack a collection definition memory an empty stack and a packer unit . The functionality usage and operation of packet unit within example architecture will be described below. Example system also may comprise a variety of co processors adapted to perform specific functions and are depicted as co processors . Other special purpose functionality can be provided such as texture loader .

Exemplary system also may comprise a cache hierarchy that includes one or more levels of cache memory and a system memory interface that can interface with a main memory which can be implemented as one or more of high speed graphics RAM DRAM and the like. Approaches to large scale memory capacity may be adapted as new technologies are developed and usage of well known acronyms such as DRAM is not intended to confine the applicability of disclosed aspects to a given process or memory technology.

In one example a thread executing on a particular cluster can instantiate a fibre routine thereby making a fibre . Coarse scheduler can receive the information concerning the instance and allocate a particular cluster to execute the fibre. As introduced above allocation of a fibre to execute on a cluster does not indicate that execution would commence immediately but rather execution of such fibre depends on

An abstraction distributor layer separates a series of computation clusters clusters and are depicted from coarse scheduler and from packet unit . Distributor layer accepts groupings of fibres from packet unit and causes the fibres to be distributed among the computation clusters according to an exemplary approach described below. Layer is an example means for distributing the groupings into regions of compute that will execute that computation.

Each cluster comprises a respective controller controllers and depicted for cluster and respectively . Each cluster controller e.g. and controls a plurality of arithmetic logic units ALU e.g. cluster controller controls a plurality of ALUs including ALU and ALU . Each ALU of a cluster communicates with a thread local and fibre storage memory e.g. thread local and fibre storage . In one implementation each ALU has a separate and dedicated access path to thread local and fibre storage such that each ALU can read or write concurrently from and to the memory with the other ALUs of that cluster. Memory resources of a given cluster further comprise a broadcasted data memory e.g. broadcasted data memory of cluster . In an example implementation broadcasted data memory can be implemented in the same physical storage medium as thread local and fibre storage . An example implementation broadcasted data memory can be highly interleaved cache that allows a particular location of memory map to a number of different locations in the broadcast data memory. In some implementations broadcasted data memory may comprise a ring buffer or FIFO memory implementation. These broadcasted data memories are fed by a direct memory access unit DMA . In one example implementation DMA control storage of data in a plurality of broadcasted data memories in a number of clusters. A cluster is a means for performing concurrent computation on respective pluralities of data elements using a respective single control flow.

Each cluster comprises an input buffer e.g. cluster comprises input buffer . Each input buffer for each cluster is written by distribution layer and read by the respective controller of that cluster. For example distribution layer writes to input buffer which is read by cluster controller .

In view of the above introduction to the components of example system aspects of the operation of this example system are described below.

In the example depicted in a first instantiated fibre such as fibre instantiation results in allocation of declared fibre data storage which can be populated by a memory write . As computation time progresses more and more fibres may be recursively instantiated which all reference fibre data storage . However each of these instantiated fibres typically would have a different scheduling key and therefore may be grouped with widely disparate fibres that reference different fibre data elements storage. Each time a further fibre is recursively instantiated the reference count associated with those fibres is increased. Example usage and other manipulations of such reference count is described below.

These concepts are depicted in with declared fibre data elements storage and which each are referenced by a different lineage of fibres. further depicts that all the instantiated fibres can be managed by central fibre collection and instantiation which forms collections of those fibres according to their scheduling keys. Fibre collection and instantiation outputs collections for execution within the computational resources available. During such execution each fibre may reference its respective fibre data element as depicted by the legend which indicates that read write during execution may be repeated for all fibres that descend from an initial fibre instance that created a particular fibre data element.

Regardless of the specific implementation sequence of instructions e.g. instruction and instruction can be provided from instruction memory hierarchy to a plurality of ALUs . Thus in the implementation depicted in each ALU executes the same instruction however data provided for execution of that instruction may vary. Each ALU communicates fibre completion information and can make requests or otherwise provide information for new fibres . Fibre assignment status can be maintained for the cluster by a fibre assignment module . In some examples such module may be included in controller . Such module may receive status information from the global scheduler . For example such fibre information can include information for new fibres to be executed within the cluster. Other information that can be maintained between global scheduler and the cluster includes fibre reference count information in some examples such fibre reference count information can be maintained within the cluster on which related fibres execute. In other words one example implementation causes all related fibres to be executed on a single cluster and in such implementation reference counts can be maintained within that cluster for those related fibres.

The example of also shows that each ALU maintains a port to cache . Cache stores thread local data as exemplified by thread local memory cache also can store cache global variables . Cache also includes a plurality of fibre memory locations . The example of also comprises a broadcast input queue . In the example of each ALU can use cache in a manner similar to a register set such that SIMD cluster controller schedule instructions for different threads and different fibres on an instruction by instruction basis without incurring latency. Cache can be managed or otherwise made coherent by a cache coherency control . In some implementations cache can be allocated into different portions that are managed differently. In one example cache can be divided into a cache portion and a managed portion. The cache portion is populated according to the read write transactions generated during the sequence of instructions being processed in ALUs . The cache portion seeks to maintain coherency with other parts of a memory hierarchy and thus writes to the cache portion would generate coherency transactions. Also to provide cache like behavior for the cache portion techniques such as tag memory and coherency indications can be maintained for the cache portion.

The managed portion is allocated to specific store local data during computation and does not have an expectation or goal of being coherent with another memory or memory hierarchy. The managed portion does not use tag memory or coherency indications because the managed portion is setup with data in known locations and the memory is not maintained for coherency. Therefore properly initiated and executing code accessing the managed portion will be able to read and or write the managed portion without limitations enforced for coherency. Fibre memory is an example of such memory type. A global scheduler or work distribution can participate in setting up the managed portions of cluster local memories. For example scheduling abstraction or local storage allocation can perform this function. Coarse scheduler can interoperate with local storage allocation to coordinate allocation of caches used by threads and or fibres and the portion of such caches that will operate like caches. In some situations the cache portion can be treated as a read only memory by fibres so that write back transactions are not generated in furtherance of coherency.

For example in some implementations all fibres that reference a particular element of fibre storage can be made to execute on the core with the local storage that has access to that element of fibre storage in an example where there is a disjoint separation of fibre elements among memories . Therefore in such implementations when one fibre that references such memory element instantiates another fibre some portion of information about the newly instantiated fibre can be kept locally. For example information identifying the program that the fibre will execute can be kept locally the location of the fibre memory element being referenced also can be kept locally in contrast for example from sending all fibre data to a central collection maintenance function here collection forming module .

Fibre storage maintenance module in conjunction with fibre memory set up operate to provide or allocate fibre memory storage locations in the distributed memories of cores . As such fibre storage maintenance module can maintain information about current memory usage in cores . Such memory usage can include thread local storage for threads in addition to fibre memory storage.

Collection storage stores identifications of fibres correlated with their scheduling key such that groupings of fibres that have a common scheduling key can be selected and output together. Scheduler can select which grouping of fibres is to be outputted and by particular example a dispatched packet of fibres is shown. Packet also includes a packet identifier and a packet priority indicator. These elements can be created by scheduler upon selection of a group of fibres based on matching their scheduling keys. Subsequently the fibres of a given packet are distributed among input buffers each corresponding to a respective core . The distribution of the fibres is determined based on where a fibre data elements used by a particular fibre is located among the distributed memories of the cores. Such memory can be selected based on a fibre ID or based on an explicit memory reference for example.

Each core as it is executing workloads which can include threads and fibre routines can output fibre status information . Such status information can include for example new fibres instantiated by other fibres or threads as well as information about which fibres have been completed. Information about completing fibres can be used to decrement reference counts of fibres that require access to the same element of fibre storage likewise instantiation of new fibres that reference a given element of fibre storage result in increases of such reference counts. These reference counts can be maintained by a given core where a group of related fibres executes or by a centralized resource as shown in for example.

In one example implementation collection forming is implemented using a hardware based hash in which each fibre can be slotted into a collection according to a hash of a respective fibre identifier. The collection forming function groups fibres according to scheduling keys. depicts scheduling keys respectively associated with collections having varying numbers of fibres each comprised within collections storage .

In sum depicts how streams of fibres can be instantiated and processed concurrently in a distributed computation resource but without rigid criteria concerning synchronization of accesses to memory or synchronization of program execution among different cores of the computation resource. Rather depicts that collections of work to be performed e.g. fibres are made with one objective being amortization of access to a larger slower memory resource and persistency of storage of data that will be persistently used or updated during a given sequence of computational elements.

Such status information is more relevant for implementations where ALU cluster scheduler can interrupt fibre routines during execution such as on an instruction by instruction basis. However in other implementations a group of fibre routines from input buffer that have not yet begun execution can be scheduled and can be entirely executed without interruption and execution of a different program stream. In either case threads can be treated differently in that ALU cluster controller can interrupt execution of threads such as for the purpose of executing selected fibre routines from input buffer . For example threads that are executing can be interrupted to perform a fibre routine.

In one example each fibre can reference different data elements in simple cache and in other implementations multiple of the fibres scheduled for execution on ALU can reference the same element of data from simple cache . Thus each ALU executes the same thread of control but can operate on different elements of fibre data in different elements of data from simple cache . Each ALU further can output information to be stored in thread local and fibre storage . These writes are to respective elements of fibre storage and because no two fibres that reference the same element of fibre storage are scheduled for parallel execution by ALUs protection mechanisms for thread local and fibre storage for such fibre storage locations is unnecessary in such implementation. Information concerning fibre scheduling and status also can be provided from ALUs to ALU cluster controller . In turn ALU cluster controller can update fibre memory location counts to account for new fibres created by fibre routines that have executed and also to account for fibres that now have completed. It is noted however that in many implementations ALU cluster controller does not control population of its input buffer with new fibres that have been created. Rather input buffer is populated by a central controller which also populates one or more other input buffers for other ALU clusters not depicted here .

As would be understood from the foregoing memory can be allocated to multiple different management schemes. One portion of memory can be managed as a cache in which cache management policies such as least recently used can control what data is stored in the memory. Also such cache portion of memory would have an intention of being coherent with other memory components in a memory hierarchy e.g. main memory . Such cache portion can be arranged as an interleaved memory or other appropriate organizational approach within that cache portion. depicts such a memory portion as cache portion . An amount of memory allocated to cache portion can be changed or varied. In some implementations the change can occur dynamically in other cases the change can be made during a setup phase of a computation such as between two distinct parts of a computation workload. For example in the graphics processing workload reallocations can be made between rendering of frames. Other conditions that can trigger reallocation are threads e.g. thread completing freeing additional memory.

In addition to cache portion memory also includes a managed portion of which fibre storage is an example. The managed portion has characteristics that differ from the characteristics of cache portion . The managed portion does not have an expectation or intention of being coherent with other memories in a memory hierarchy e.g. memory . Note that eventually data in the managed portion may be written out to memory or otherwise available for access by non local computation. However that writing or accessibility occurs after a sequence of local computation that may include many reads and writes to the memory for which no memory coherency operations are generated.

Fibres can be scheduled to begin execution based on determining existence of a memory conflict with another executing or scheduled fibre or instructions from fibres can similarly be dispatched or held for dispatch based on such memory conflict checks. In thread oriented computing instructions from a thread are dispatched for execution and memory correctness is maintained by locking memory locations when an instruction initiates a memory transaction that would present a hazard to a different instruction. The different instruction can begin execution but the thread of which that instruction is a part will halt waiting on the hazard to clear.

As explained above fibres can be instantiated individually either by a thread or another fibre. However fibres are dispatched for execution in groups. If a prioritization scheme is desired to be implemented for such groupings of fibres then one approach to such prioritization is exemplified in . depicts that a number of fibres to be grouped are available these fibres can be collected or sorted according to their fibre keys also called scheduling keys herein . Each fibre also can be associated with an individual priority which can be obtained . Upon determining groupings of such fibres a priority for each determining grouping also can be determined . A packet identifier also can be determined .

A variety of approaches can be provided to determining a priority for a grouping of fibres based on priorities of individual fibres comprised within that grouping. In one example average of the individual fibre priorities can be used to establish a group priority. In another example of weighted average can be used where a fibre with especially high priority can have a much higher effect on the overall priority of the group then a simple average would have provided. In other implementations priorities assigned to groupings of fibres can be influenced by the current state of threads executing within the system as well.

In furtherance of such example variations also depicts expected storage locations for fibre data used by the fibres of a packet can be used in determining priority of the packet. For example if multiple memories storing fibre data for fibres in a given packet are nearing capacity then a priority of such packets can be increased. Notably since fibre storage already has been allocated for fibres except for newly instantiated families of fibres more local storage is not necessarily required in order to increase the number of fibres that are within the same family.

It is expected that fibres effect or outputs by writing back to local fibre storage and do not arbitrate for shared memory resources. Further in many implementations fibre routines are coded to avoid calling another routine that returns a value and which would cause the fibre routine to stall for an indefinite period waiting for such value to return. Thus for grammatical components and instantiate or otherwise call fibres in these implementations also do not need to stall or wait for the fibre to return any value to the calling thread or other fibre. As such the time to complete execution of any particular fibre is not as critical because other computational flows do not need to be stalled awaiting completion of that fibre.

Scheduling keys were described in many foregoing example implementations. In some implementations and in some cases depending on a nature of a workload being performed scheduling keys can be compressed or otherwise referenced by shorthand notation. An example workload of ray tracing will be used as an example. In such example a parent ray tracing thread may output a different fibre for each ray that it wants traced in a scene. Thus some threads may output a few fibres some may output tens or hundreds of millions or billions of fibres. In some cases each fibre may initially start with the same scheduling key. In an example such scheduling key can reference a starting storage address of a first element of an acceleration structure that sub divides the 3 D scene. Elements of the structure can be stored in memory at regular intervals e.g. each element can be stored in a quadword . As such a certain kind of fibre routine can be setup to have a default scheduling key for example an acceleration structure traversal fibre routine can have the head element of the acceleration structure as a default. Thus a thread instantiating such fibres does not need to explicitly identify that location.

Examples of acceleration structures include bounding volume hierarchies such as sphere and rectangle hierarchies and kD trees without limitation. For example in a uniform sphere hierarchy the elements of such structure are spheres. If the acceleration structure is homogeneous then leaf nodes in the structure bound primitives a heterogeneous structure can have elements that bound other acceleration structure elements and primitives.

A variety of structures devices components and other constituent elements were depicted and described above along with interrelationships allowing the flow of data and other information between and among them and functions and purposes for them were described. These are means for performing the functions and purposes attributed to them in the disclosure.

Code for any method can be stored in non transitory computer readable media such as solid state drives hard drives CD ROMs and other optical storage means and transiently in nonvolatile memories. A computer readable medium also can include communication signals. When such code is embodied in a communication signal and that signal is read and processed by a computer the computer uses that signal and its physical medium as a computer readable medium.

Computer executable instructions comprise for example instructions and data which cause or otherwise configure a general purpose computer special purpose computer or special purpose processing device to perform a certain function or group of functions. The computer executable instructions may be for example binaries intermediate format instructions such as assembly language or source code. Some aspects of the API described herein can be implemented as procedures functions or calls to such procedures and functions. This description implies no limitation as to a programming methodology that can be used to implement or provide the functionality described as being available through these procedures or functions so long as software hardware or a mixture thereof provides a programmer with an ability to access such functionality through an interface provided therefore. These names imply no requirement as to what code performing these functions need to called in an implementation.

In some examples the computation resources used to execute threads and fibres can be largely undifferentiated. Rather many of the fibre computing characteristics described herein can be implemented by a central controller that groups fibres for dispatch and dispersion among the computation resources input queues serving these computation resources and the coordination and counting of which fibres still exist that require a given element of fibre storage.

As would be discerned from these disclosures one example situation in which fibre computing can be used is a situation where a large number of data elements of a first type which are not all known a priori need to be processed or used in conjunction with a large and predetermined data set. In some example situations the processing of some first type data elements with one portion of the predetermined data set affects how those first type data element are processed with other portions of the predetermined data set. The predetermined data set can vary as well such as being updated relatively infrequently. In an example graphics application the predetermined data set may be updated a few hundred times a second and could include many millions of items which can be accessed millions or billions a second. In general a centralized scheduler for fibre computing can schedule collections of fibres based on expected maximization of computing throughput rather than explicitly serving the prioritization of any particular fibre.

The various examples described above are provided by way of illustration only and should not be construed as limiting. For example only a limited example of ray tracing behavior was presented and it would be understood that practical implementations involve many more rays and often more concurrent processing thereof. The disclosures herein can be adapted and understood from that perspective. In addition separate boxes or illustrated separation of functional elements of illustrated systems implies no required physical separation of such functions as communications between such elements can occur by way of messaging function calls shared memory space and so on without any such physical separation. More generally a person of ordinary skill would be able to adapt the disclosures relating to the programming semantic to a variety of other ray tracing ray shading implementations and no implied limitation as to its application exists from the systems methods and other disclosure used to explain examples thereof

