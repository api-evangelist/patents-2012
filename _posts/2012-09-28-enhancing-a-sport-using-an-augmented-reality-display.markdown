---

title: Enhancing a sport using an augmented reality display
abstract: Technology is described for providing a personalized sport performance experience with three dimensional (3D) virtual data displayed by a near-eye, augmented reality display of a personal audiovisual (A/V) apparatus. A physical movement recommendation is determined for the user performing a sport based on skills data for the user for the sport, physical characteristics of the user, and 3D space positions for at least one or more sport objects. 3D virtual data depicting one or more visual guides for assisting the user in performing the physical movement recommendation may be displayed from a user perspective associated with a display field of view of the near-eye AR display. An avatar may also be displayed by the near-eye AR display performing a sport. The avatar may perform the sport interactively with the user or be displayed performing a prior performance of an individual represented by the avatar.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09345957&OS=09345957&RS=09345957
owner: MICROSOFT TECHNOLOGY LICENSING, LLC
number: 09345957
owner_city: Redmond
owner_country: US
publication_date: 20120928
---
This is a continuation in part application of U.S. patent application Ser. No. 13 250 878 titled Personal Audio Visual System to K. Stone Perez et al. filed Sep. 30 2011 published as US 2013 0083003 on Apr. 4 2013 and incorporated herein by reference.

When playing a sport a person may daydream as if he is playing with his favorite sports star. At other times he or she may wish an instructor could be where he or she is playing and providing advice for improving his or her performance in real time and under current environmental conditions for play. Additionally even when a coach or friend is with the player and giving advice the player may not be able to translate what was heard or seen into his or her own physical actions.

Technology is described for providing a personalized sport performance experience with three dimensional 3D virtual data being displayed by a near eye augmented reality AR display of a personal audiovisual A V apparatus. A visual guide for tracking a sport object like a ball or providing a tip on performing a sport movement may be displayed from the user perspective in some examples. In other embodiments a user can interactively play a sport with an avatar which may represent another person like a friend or celebrity or even the user. In other embodiments an avatar performing a prior performance may be displayed by the AR display from a user perspective associated with a display field of view of the display. As these examples illustrate coaching gaging one s performance and play may be enhanced by the various embodiments of the technology.

The technology provides one or more embodiments of a method for providing a personalized sport performance experience with three dimensional 3D virtual data being displayed by a near eye augmented reality AR display of a personal audiovisual A V apparatus. An embodiment of the method comprises automatically identifying a physical location which the personal A V apparatus is within based on location data detected by the personal A V apparatus and automatically identifying at least one or more sports objects in a sport performance area associated with the physical location based on a three dimensional mapping of objects in the physical location.

Physical characteristics of a user and skills data for a sport stored for the user in user profile data are accessed from a memory. A physical movement recommendation is determined by a processor for the user performing the sport based on the skills data for the sport the physical characteristics of the user and 3D space positions for the at least one or more sport objects in the physical location. Three dimensional 3D virtual data depicting one or more visual guides for assisting the user in performing the physical movement recommendation is displayed from a user perspective associated with a display field of view of the near eye AR display.

The technology provides one or more embodiments of a portable personal audiovisual A V apparatus for providing a personalized sport performance experience with three dimensional 3D virtual data. An embodiment of the portable personal A V apparatus comprises a near eye augmented reality AR display having a display field of view and being supported by a near eye support structure. One or more processors have access to memory storing physical characteristics of a user wearing the personal A V apparatus and skills data for the user for one or more sports. A natural user interface of the apparatus includes at least one image capture device communicatively coupled to the one or more processors and being supported by the near eye support structure for tracking a physical sport movement being performed by a user.

The one or more processors determine a physical sport movement recommendation based on a sport being played skills data for the user for the sport being played and one or more objects in a sport performance area in which the user is playing the sport. The one or more processors control the near eye AR display of the personal A V apparatus for displaying 3D virtual data for assisting the user in performing the physical sport movement recommendation from a user perspective associated with the display field of view.

The technology provides one or more embodiments of one or more processor readable storage devices comprising instructions encoded thereon which instructions cause one or more processors to execute a method for providing a personalized sport performance experience with three dimensional 3D virtual data displayed by a near eye augmented reality AR display of a personal audiovisual A V apparatus. An embodiment of the method comprises receiving data indicating user selection of an avatar for display performing a sport during a sport performance session. One or more memories storing sport performance data for the avatar are accessed by a processor of the personal A V apparatus. 3D virtual data of the avatar performing the sport in a context of the sport performance session is displayed based on the sport performance data for the avatar from a user perspective associated with a display field of view of the near eye AR display of the personal A V apparatus.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter.

A personal A V apparatus including a near eye augmented reality AR display can enhance performing a sport by displaying virtual data. Some examples of sports are running golf basketball baseball swimming etc. A user of the personal A V apparatus may be playing with real sport equipment or virtual sport equipment displayed by the AR display. Furthermore a user may be performing a sport at a real sport performance location like a real golf course a real golf range a real basketball court or a real physical location like a neighborhood for a run. A user may also be performing a sport in the context of a virtual sport performance area which may be displayed by the personal A V apparatus or by the apparatus and other display devices. Avatars for presenting a performance or for interaction with the user may be displayed as well. In some embodiments a sport engine provides a physical sport movement recommendation for a user based on a current context in a sport performance session and user skills data. Some examples of information a physical sport movement recommendation may include are what piece of sports equipment to use a direction of aim a suggested body movement e.g. duck run a position on a field or court at which to perform the movement and a strength to use for a movement. A user s physical sport movement may also be tracked and analyzed for updating skills data for the user.

A user using a near eye AR display sees virtual objects displayed with real objects in real time. In particular a user wearing an optical see through augmented reality display device actually sees with his or her natural sight a real object which is not occluded by image data of a virtual object or virtual effects in a display field of view of the see through display hence the names see through display and optical see through display. For other types of augmented reality displays like video see displays sometimes referred to as video see through displays or a display operating in a video see mode the display is not really see through because the user does not see real objects with his natural sight but sees displayed image data of unoccluded real objects as they would appear with natural sight as well as image data of virtual objects and virtual effects. References to a see through display below are referring to an optical see through display.

Image data may be moving image data like video as well as still image data. Image data may also be three dimensional. An example of 3D image data is a hologram. Image data may be that captured and in some examples displayed of real objects or image data may be generated to illustrate virtual objects or imagery. Virtual image data referred to hereafter as virtual data is image data of a virtual object or virtual effect. An example of a virtual effect is an environmental condition like fog or rain. Another example of a virtual effect may be a simulated effect on a real object like a smashing of a window when the real is still intact or a displayed change of color of one s shirt. Virtual data which is registered to an object real or virtual means the data tracks its position in the display field of view in reference to or dependent upon a position of the object to which it is registered.

Virtual data is seen through a near eye augmented reality AR display of the personal A V system from a user perspective which is not predefined as the display moves with user movements. The display field of view approximates a user field of view as seen from a user perspective where a user is looking from . A user perspective may be associated with a display field of view. A user perspective may be approximated with varying degrees of accuracy. For example a predetermined approximation of a user perspective may be associated with a near eye AR display without reference to specific user data. In other examples more sophisticated techniques may use individual gaze determined from eye tracking data to more precisely pinpoint from where a user is looking. In some embodiments a user perspective of an object real or virtual is determined from a position and orientation of the object in the display field of view.

In some embodiments the display field of view may be mapped by a view dependent coordinate system having orthogonal X Y and Z axes in which a Z axis represents a depth position from a reference point on the personal A V apparatus.

The use of the term actual direct view refers to the ability to see real world objects directly with the human eye rather than seeing created image representations of the objects. For example looking through glass at a room allows a user to have an actual direct view of the room while viewing a video of a room on a television is not an actual direct view of the room. Each display optical system is also referred to as a see through display and the two display optical systems together may also be referred to as a see through meaning optical see through augmented reality display .

Frame provides a support structure for holding elements of the apparatus in place as well as a conduit for electrical connections. In this embodiment frame provides a convenient eyeglass frame as support for the elements of the apparatus discussed further below. Some other example of a near eye support structure are a visor frame or a goggles support. The frame includes a nose bridge with a microphone for recording sounds and transmitting audio data to control circuitry . A side arm or temple of the frame rests on each of a user s ears and in this example the temple is illustrated as including control circuitry for the display device .

As illustrated in an image generation unit is included on each temple in this embodiment as well. Also not shown in this view but illustrated in are outward facing capture devices e.g. cameras for recording digital image data such as still images videos or both and transmitting the visual recordings to the control circuitry which may in turn send the captured image data to the companion processing module which may also send the data to one or more computer systems or to another personal A V apparatus over one or more communication networks .

The companion processing module may take various embodiments. In some embodiments companion processing module is a separate unit which may be worn on the user s body e.g. a wrist or be a separate device like a mobile device e.g. smartphone . The companion processing module may communicate wired or wirelessly e.g. WiFi Bluetooth infrared an infrared personal area network RFID transmission wireless Universal Serial Bus WUSB cellular 3G 4G or other wireless communication means over one or more communication networks to one or more computer systems whether located nearby or at a remote location other personal A V apparatus in a location or environment for example as part of peer to peer communication and if available one or more 3D image capture devices in the environment. In other embodiments the functionality of the companion processing module may be integrated in software and hardware components of the display device as in . Some examples of hardware components of the companion processing module are shown in .

One or more network accessible computer system s may be leveraged for processing power and remote data access. An example of hardware components of a computer system is shown in . The scale and number of components may vary considerably for different embodiments of the computer system and the companion processing module . An application may be executing on a computer system which interacts with or performs processing for an application executing on one or more processors in the personal A V apparatus . For example a 3D mapping application may be executing on the one or more computers systems and the user s personal A V apparatus . In some embodiments the application instances may perform in a master and client role in which a client copy is executing on the personal A V apparatus and performs 3D mapping of its display field of view receives updates of the 3D mapping from the computer system s including updates of objects in its view from the master 3D mapping application and sends image data and depth and object identification data if available back to the master copy. Additionally in some embodiments the 3D mapping application executing on different personal A V apparatus in the same environment share data updates in real time for example real object identifications in a peer to peer configuration between apparatus .

In the illustrated embodiments of the one or more computer system and the portable personal A V apparatus also have network access to one or more 3D image capture devices which may be for example one or more cameras that visually monitor one or more users and the surrounding space such that gestures and movements performed by the one or more users as well as the structure of the surrounding space including surfaces and objects may be captured analyzed and tracked. Image data and depth data if captured by the one or more 3D capture devices may supplement data captured by one or more capture devices of one or more personal A V apparatus in a location. The one or more capture devices may be one or more depth cameras positioned in a user environment.

In some examples the capture devices may also be depth sensitive for example they may be depth sensitive cameras which transmit and detect infrared light from which depth data may be determined. In other examples a separate depth sensor not shown on the front of the frame may also capture and provide depth data to objects and other surfaces in the display field of view. The depth data and image data form a depth map of the captured field of view of the capture devices which are calibrated to include the display field of view. A three dimensional 3D mapping of the display field of view can be generated based on the depth map.

In some embodiments the outward facing capture devices provide overlapping image data from which depth information for objects in the image data may be determined based on stereopsis. Parallax and contrasting features such as color may also be used to resolve relative positions of real objects.

The capture devices are also referred to as outward facing capture devices meaning facing outward from the user s head. The illustrated capture device is a front facing capture device which is calibrated with respect to a reference point of its respective display optical system . One example of such a reference point is an optical axis see in of its respective display optical system . The calibration allows the display field of view of the display optical systems to be determined from the data captured by the capture devices .

Control circuitry provide various electronics that support the other components of head mounted display device . In this example the right temple includes control circuitry for the display device which includes a processing unit a memory accessible to the processing unit for storing processor readable instructions and data a wireless interface communicatively coupled to the processing unit and a power supply providing power for the components of the control circuitry and the other components of the display device like the cameras the microphone and the sensor units discussed below. The processing unit may comprise one or more processors including a central processing unit CPU and a graphics processing unit GPU particularly in embodiments without a separate companion processing module which contains at least one graphics processing unit GPU .

Inside or mounted to temple are an earphone of a set of earphones an inertial sensing unit including one or more inertial sensors and a location sensing unit including one or more location or proximity sensors some examples of which are a GPS transceiver an infrared IR transceiver or a radio frequency transceiver for processing RFID data. In one embodiment inertial sensing unit includes a three axis magnetometer a three axis gyro and a three axis accelerometer as inertial sensors. The inertial sensors are for sensing position orientation and sudden accelerations of head mounted display device . From these sensed movements head position and thus orientation of the display device may also be determined which indicate changes in the user perspective and the display field of view for which virtual data is updated to track with the user perspective.

In this embodiment each of the devices processing an analog signal in its operation include control circuitry which interfaces digitally with the digital processing unit and memory and which produces or converts analog signals or both produces and converts analog signals for its respective device. Some examples of devices which process analog signals are the location and inertial sensing units and earphones as well as the microphone capture devices and a respective IR illumination source A and a respective IR detector or camera B for each eye s display optical system discussed below.

Mounted to or inside temple is an image source or image generation unit which produces visible light representing images. The image generation unit can display a virtual object to appear at a designated depth location in the display field of view to provide a realistic in focus three dimensional display of a virtual object which can interact with one or more real objects. Some examples of embodiments of image generation units which can display virtual objects at various depths are described in the following applications which are hereby incorporated by reference Automatic Variable Virtual Focus for Augmented Reality Displays having U.S. patent application Ser. No. 12 941 825 and inventors Avi Bar Zeev and John Lewis and which was filed Nov. 8 2010 and Automatic Focus Improvement for Augmented Reality Displays having U.S. patent application Ser. No. 12 949 650 and inventors Avi Bar Zeev and John Lewis and which was filed Nov. 18 2010. In these examples a focal length for an image is changed by the image generation unit resulting in a change in a region of the display field of view in which the virtual data appears. In other examples rapid display of multiple images and a composite image of the in focus portions of the virtual images are techniques describe which may cause displayed virtual data to appear in different focal regions.

In some embodiments the image generation unit includes a microdisplay for projecting images of one or more virtual objects and coupling optics like a lens system for directing images from the microdisplay to a reflecting surface or element . The reflecting surface or element directs the light from the image generation unit into a light guide optical element which directs the light representing the image into the user s eye.

In the illustrated embodiment the display optical system includes an opacity filter for enhancing contrast of virtual data which is behind and aligned with optional see through lens in this example light guide optical element for projecting image data from the image generation unit is behind and aligned with opacity filter and optional see through lens is behind and aligned with light guide optical element .

Light guide optical element transmits light from image generation unit to the eye of the user wearing head mounted display device . Light guide optical element also allows light from in front of the head mounted display device to be received through light guide optical element by eye as depicted by an arrow representing an optical axis of the display optical system thereby allowing the user to have an actual direct view of the space in front of head mounted display device in addition to receiving a virtual image from image generation unit . Thus the walls of light guide optical element are see through.

In this embodiment light guide optical element is a planar waveguide which acts as part of the display and also integrates eye tracking. A representative reflecting element E represents the one or more optical elements like mirrors gratings and other optical elements which direct visible light representing an image from the planar waveguide towards the user eye . Infrared illumination and reflections also traverse the planar waveguide for an eye tracking system for tracking the position and movement of the user s eye typically the user s pupil. Eye movements may also include blinks. The eye tracking system comprises an eye tracking IR illumination source A an infrared light emitting diode LED or a laser e.g. VCSEL and an eye tracking IR sensor B e.g. IR camera arrangement of IR photodetectors or an IR position sensitive detector PSD for tracking glint positions . Wavelength selective filters C and D with representative reflecting element E implement bidirectional infrared IR filtering which directs IR illumination towards the eye preferably centered about the optical axis and receives IR reflections from the user eye preferably including reflections captured about the optical axis which are from the light guide optical element to an IR sensor B.

In other embodiments the eye tracking unit optics are not integrated with the display optics. For more examples of eye tracking systems for HMD devices see U.S. Pat. No. 7 401 920 entitled Head Mounted Eye Tracking and Display System issued Jul. 22 2008 to Kranz et al. see U.S. patent application Ser. No. 13 221 739 Lewis et al. entitled Gaze Detection in a See Through Near Eye Mixed Reality Display filed Aug. 30 2011 and see U.S. patent application Ser. No. 13 245 700 Bohn entitled Integrated Eye Tracking and Display System filed Sep. 26 2011 all of which are incorporated herein by reference.

Opacity filter which is aligned with light guide optical element selectively blocks natural light from passing through light guide optical element for enhancing contrast of virtual imagery. The opacity filter assists the image of a virtual object to appear more realistic and represent a full range of colors and intensities. In this embodiment electrical control circuitry for the opacity filter not shown receives instructions from the control circuitry via electrical connections routed through the frame. More details of an opacity filter are provided in U.S. patent application Ser. No. 12 887 426 Opacity Filter For See Through Mounted Display filed on Sep. 21 2010 incorporated herein by reference in its entirety.

Again show half of the head mounted display device . For the illustrated embodiment a full head mounted display device may include another display optical system with another set of optional see through lenses and another opacity filter another light guide optical element as well as another image generation unit another of outward facing capture devices eye tracking system and another of the earphones . In some embodiments there may be a continuous display viewed by both eyes rather than a display optical system for each eye. Additional details of a head mounted personal A V apparatus are illustrated in U.S. patent application Ser. No. 12 905 952 entitled Fusing Virtual Content Into Real Content Filed Oct. 15 2010 fully incorporated herein by reference.

In this embodiment software for a sport engine may be executing on one or more processors of the personal A V apparatus for communicating with an image and audio processing engine and perhaps other image capture devices like other 3D image capture devices and other personal A V apparatus for exchanging and accessing data to personalize the user s sport performance experience. In the illustrated embodiment a virtual data provider system executing on a remote computer system can also be executing a version N of the sport engine as well as other personal A V apparatus with which it is in communication for enhancing the experience.

The sport engine accesses sport databases for each sport supported. For example the sport engine may include a rule engine for each sport which accesses one or more rules stored in the databases . Some example of data stored in the sport databases for a sport include rules for the sport e.g. rules of the game and recommendation rules for use by a recommendation engine included in the sport engine . For example the rules for the sport may identify which types of kicks are allowed by an offense player and the recommendation rules provide logic for determining which type of kick will be most effective for advancing the ball to the goal in the current context in a game.

Some other examples of data stored in the databases include reference data for one or more physical sport movements associated with the sport and execution criteria for the one or more associated movements. In some examples the reference data for a physical sport movement is gesture filter see below which may be registered with the gesture recognition engine so it can notify the sport engine when a movement has occurred. Execution criteria may be reference image data or structural data e.g. skeletal data of a movement. In some instances the reference image data is for a generic human within a range of physical characteristics. In other examples such reference image data can be tailored to an individual based on the physical characteristics of the specific user. In other examples the execution criteria may include relationships of body parts in a movement and ranges of distance and direction for each body part associated with the movement. Age may also be a factor of the execution criteria.

Another example of data stored in the databases may be how skill levels are defined with respect to satisfaction of the execution criteria. For example some actions like keeping your eye on the ball within a movement like a golf swing may have a higher priority in satisfying execution criteria than other actions like how high the club is swung. Criteria for assigning skill levels may be stored as well. For example there may be criteria for categorizing a sport movement performance as bad good excellent or beginner intermediate pro and there may also be criteria by which an overall skill level is assigned for a user. Skills data for a particular user may be stored in user profile data .

Another example of data which may be stored in the sport databases are object properties for predetermined visual guides or previously generated visual guides and rule logic for selection and when to display one or more visual guides.

Another example of data which may be stored in the sport databases is sport performance data for avatars generated by the sport engine for example for a coach avatar a generic skill level avatar an avatar of the user or a celebrity avatar. Some examples of sport performance data are motion capture data or image data from a prior performance. Other examples are accessible physics models for sport movements performed by the avatar e.g. how the avatar runs dunks a basketball or several styles of serving a tennis ball. Sport performance data may also be stored for the user and be accessible via user profile data. Additionally object properties like color shape facial features clothing and the like may be stored for an avatar for the virtual data engine see below to access when displaying the avatar.

As shown in the embodiment of the software components of a computing environment comprise the image and audio processing engine in communication with an operating system . Image and audio processing engine processes image data e.g. moving data like video or still and audio data in order to support applications executing for a head mounted display HMD device system like a personal A V apparatus including a near eye augmented reality display. Image and audio processing engine includes object recognition engine gesture recognition engine virtual data engine eye tracking software if eye tracking is in use an occlusion engine a 3D positional audio engine with a sound recognition engine a scene mapping engine and a physics engine which may communicate with each other.

The computing environment also stores data in image and audio data buffer s . The buffers provide memory for receiving image data captured from the outward facing capture devices image data captured by other capture devices if available image data from an eye tracking camera of an eye tracking system if used buffers for holding image data of virtual objects to be displayed by the image generation units and buffers for both input and output audio data like sounds captured from the user via microphone and sound effects for an application from the 3D audio engine to be output to the user via audio output devices like earphones .

Image and audio processing engine processes image data depth data and audio data received from one or more capture devices which may be available in a location. Image and depth information may come from the outward facing capture devices captured as the user moves his head or body and additionally from other personal A V apparatus other 3D image capture devices in the location and image data stores like location indexed images and maps .

The individual engines and data stores depicted in are described in more detail below but first an overview of the data and functions they provide as a supporting platform is described from the perspective of an application like the sport engine . The sport engine executing in the personal A V apparatus or executing remotely on a computer system for the personal A V apparatus leverages the various engines of the image and audio processing engine for implementing its one or more functions by sending requests identifying data for processing and receiving notification of data updates. For example notifications from the scene mapping engine identify the positions of virtual and real objects at least in the display field of view. The sport engine identifies data to the virtual data engine for generating the structure and physical properties of an object for display. The sport engine may supply and identify a physics model for each virtual object generated for its application to the physics engine or the physics engine may generate a physics model based on reference properties stored for an object in structure data .

The operating system makes available to applications which gestures the gesture recognition engine has identified which words or sounds the sound recognition engine has identified the positions of objects from the scene mapping engine as described above and eye data such as a position of a pupil or an eye movement like a blink sequence detected from the eye tracking software . A sound to be played for the user in accordance with the sport engine can be uploaded to a sound library and identified to the 3D audio engine with data identifying from which direction or position to make the sound seem to come from. The device data makes available to the sport engine location data head position data data identifying an orientation with respect to the ground and other data from sensing units of the display device .

The scene mapping engine is first described. A 3D mapping of the display field of view of the augmented reality display can be determined by the scene mapping engine based on captured image data and depth data. The depth data may either be derived from the captured image data or captured separately. The 3D mapping includes 3D space positions or position volumes for objects. A 3D space is a volume of space occupied by the object. Depending on the precision desired the 3D space can match the 3D shape of the object or be a less precise bounding volume around an object like a bounding box a bounding 3D elliptical shaped volume a bounding sphere or a bounding cylinder. A 3D space position represents position coordinates for the boundary of the volume or 3D space. In other words the 3D space position identifies how much space an object occupies and where in the display field of view that occupied space is. As discussed further below in some examples the 3D space position includes additional information such as the object s orientation.

A depth map representing captured image data and depth data from outward facing capture devices can be used as a 3D mapping of a display field of view of a near eye AR display. As discussed above a view dependent coordinate system may be used for the mapping of the display field of view approximating a user perspective. The captured data may be time tracked based on capture time for tracking motion of real objects. Virtual objects can be inserted into the depth map under control of an application like sport engine . Mapping what is around the user in the user s environment can be aided with sensor data. Data from an orientation sensing unit e.g. a three axis accelerometer and a three axis magnetometer determines position changes of the user s head and correlation of those head position changes with changes in the image and depth data from the outward facing capture devices can identify positions of objects relative to one another and at what subset of an environment or location a user is looking.

Depth map data of another HMD device currently or previously in the environment along with position and head orientation data for this other HMD device can also be used to map what is in the user environment. Shared real objects in their depth maps can be used for image alignment and other techniques for image mapping. With the position and orientation data as well what objects are coming into view can be predicted as well so physical interaction processing occlusion and other processing can start even before the objects are in view.

The scene mapping engine can also use a view independent coordinate system for 3D mapping and a copy of a scene mapping engine may be in communication with other scene mapping engines executing in other systems e.g. and so the mapping processing can be shared or controlled centrally by one computer system which shares the updated map with the other systems. Image and depth data from multiple perspectives can be received in real time from other 3D image capture devices under control of one or more network accessible computer systems or from one or more personal A V apparatus in the location. Overlapping subject matter in the depth images taken from multiple perspectives may be correlated based on a view independent coordinate system and time and the image content combined for creating the volumetric or 3D mapping of a location e.g. an x y z representation of a room a store space or a geofenced area . Thus changes in light shadow and object positions can be tracked. The map can be stored in the view independent coordinate system in a storage location e.g. accessible as well by other personal A V apparatus other computer systems or both be retrieved from memory and be updated over time. For more information on collaborative scene mapping between HMDs like apparatus and computer systems with access to image data see Low Latency Fusing of Virtual and Real Content having U.S. patent application Ser. No. 12 912 937 having inventors Avi Bar Zeev et al. and filed Oct. 27 2010 and which is hereby incorporated by reference. 

When a user enters a location or an environment within a location the scene mapping engine may first query networked computer systems e.g. or or a network accessible location like location indexed images and 3D maps for a pre generated 3D map or one currently being updated in real time which map identifies 3D space positions and identification data of real and virtual objects. The map may include identification data for stationary objects objects moving in real time objects which tend to enter the location physical models for objects and current light and shadow conditions as some examples.

The location may be identified by location data which may be used as an index to search in location indexed image and 3D maps or in Internet accessible images for a map or image related data which may be used to generate a map. For example location data such as GPS data from a GPS transceiver of the location sensing unit on the display device may identify the location of the user. In another example a relative position of one or more objects in image data from the outward facing capture devices of the user s personal A V apparatus can be determined with respect to one or more GPS tracked objects in the location from which other relative positions of real and virtual objects can be identified. Additionally an IP address of a WiFi hotspot or cellular station to which the personal A V apparatus has a connection can identify a location. Additionally identifier tokens may be exchanged between personal A V apparatus via infra red Bluetooth or WUSB. The range of the infra red WUSB or Bluetooth signal can act as a predefined distance for determining proximity of another user. Maps and map updates or at least object identification data may be exchanged between personal A V apparatus via infra red Bluetooth or WUSB as the range of the signal allows.

The scene mapping engine identifies the position and orientation and tracks the movement of real and virtual objects in the volumetric space based on communications with the object recognition engine of the image and audio processing engine and one or more executing applications generating virtual objects like the sport engine .

The object recognition engine of the image and audio processing engine detects tracks and identifies real objects in the display field of view and the 3D environment of the user based on captured image data and captured depth data if available or determined depth positions from stereopsis. The object recognition engine distinguishes real objects from each other by marking object boundaries and comparing the object boundaries with structural data. Colors may also be detected as well and shapes and surfaces derived. One example of marking object boundaries is detecting edges within detected or derived depth data and image data and connecting the edges. A polygon mesh may also be used to represent the object s boundary. The object boundary data is then compared with stored structure data in order to identify a type of object within a probability criteria. Besides identifying the type of object an orientation of an identified object may be detected based on the comparison with stored structure data .

For real objects data may be assigned for each of a number of object properties like 3D size 3D shape type of materials detected color s and boundary shape detected. In one embodiment based on a weighted probability for each detected property assigned by the object recognition engine after comparison with reference properties the object is identified and its properties stored in object properties data N.

One or more databases of structure data accessible over one or more communication networks may include structural information about objects. As in other image processing applications a person can be a type of object so an example of structure data is a stored skeletal model of a human which may be referenced to help recognize body parts. Structure data may also include structural information regarding one or more inanimate objects in order to help recognize the one or more inanimate objects some examples of which are furniture sporting equipment automobiles and the like.

The structure data may store structural information such as structural patterns for comparison and image data as references for pattern recognition. The image data may also be used for facial recognition. The object recognition engine may also perform facial and pattern recognition on image data of the objects based on stored image data from other sources as well like user profile data of the user other users profile data which are permission and network accessible location indexed images and 3D maps and Internet accessible images . Motion capture data from image and depth data may also identify motion characteristics of an object. The object recognition engine may also check detected properties of an object like its size shape material s and motion characteristics against reference properties stored in structure data .

The reference properties may have been predetermined manually offline by an application developer or by pattern recognition software and stored. Additionally if a user takes inventory of an object by viewing it with the personal A V apparatus and inputting data in data fields reference properties for an object can be stored in structure data by the object recognition engine . The reference properties e.g. structure patterns and image data may also be accessed by applications for generating virtual objects.

There may also be stored in structure data a physics parameters data set for an object. Some example physics parameters include a mass range for the type of object one or more inner material type s a modulus of elasticity for use with Hooke s Law one or more tensile strengths associated with one or more material types including at least a surface material type and a surface coefficient of friction associated with the surface material. Air may be considered a type of inner material. These parameters may be selected for a physics model representing an object for use by a physics engine as discussed below.

The scene mapping engine and the object recognition engine exchange data which assist each engine in its functions. For example based on an object identification and orientation determined by the object recognition engine the scene mapping engine can update a 3D space position or position volume for an object for more accuracy. For example a chair on its side has different position coordinates for its volume than when it is right side up. A position history or motion path identified from position volumes updated for an object by the scene mapping engine can assist the object recognition engine track an object particularly when it is being partially occluded.

Upon detection of one or more objects by the object recognition engine image and audio processing engine may report to operating system an identification of each object detected and a corresponding 3D space position which may include object orientation which the operating system passes along to other executing applications like the scene mapping engine the occlusion engine the physic engine and other upper level applications like the sport engine .

The gaze or point of gaze of a user may be determined based on eye tracking data from the eye tracking system and the 3D mapping of the display field of view by one or more processors of the personal A V apparatus executing the eye tracking software . In one example of determining gaze the eye tracking software executing on the one or more processors identifies a pupil position within each eye and models a gaze line for each eye extending from an approximated location of a respective fovea. The 3D scene mapping engine executing on the one or more processors determine a position in the display field of view where the gaze lines meet. This intersection is the point of gaze and it is within the Panum s fusional area for human eyes which is the area in which objects are in focus. Based on the 3D mapping of objects in the display field of view a current object or point in a location at which the gaze lines meet is a current object or point of focus.

The occlusion engine identifies spatial occlusions between objects and in particular between real and virtual objects based on spatial position data for recognized objects within a coordinate system as updated by the objection recognition engine and the scene mapping engine . For more information about occlusion processing see U.S. patent application Ser. No. 12 905 952 entitled Fusing Virtual Content into Real Content Flaks et al. and filed Oct. 15 2010 which is hereby incorporated by reference and see also U.S. patent application Ser. No. 13 443 368 entitled Realistic Occlusion for a Head Mounted Augmented Reality Display Geisner et al. and filed Apr. 10 2012 which is hereby incorporated by reference.

The 3D audio engine is a positional 3D audio engine which receives input audio data and outputs audio data for the earphones or other audio output devices like speakers in other embodiments. The received input audio data may be for a virtual object or be that generated by a real object. Audio data for virtual objects generated by an application or selected from a sound library can be output to the earphones to sound as if coming from the direction of the virtual object. An example of a positional 3D audio engine which may be used with an augmented reality system is disclosed in U.S. patent application Ser. No. 12 903 610 entitled System and Method for High Precision Dimensional Audio for Augmented Reality to Flaks et al. and filed Oct. 13 2010 which is hereby incorporated by reference.

Sound recognition engine of the 3D audio engine identifies audio data from the real world received via microphone for application control via voice commands and for environment and object recognition. Based on a sound library the engine can identify a sound with a physical object e.g. a crack of a bat a golf ball hit in the sweet spot a tennis ball hit with the rim of the racket etc. Additionally voice data files stored in user profile data or user profiles may also identify a speaker with whom a person object mapped in the environment may be associated.

An embodiment of a natural user interface NUI in one or more embodiments of the personal A V apparatus may include the outward facing capture devices and the gesture recognition engine for identifying a gesture which is an example of at least one user physical action of at least one body part. The eye tracking system and the eye tracking software for interpreting eye movements based on the data captured by the system may also be components in another embodiment of a natural user interface for the personal A V apparatus . Eye based actions like a blink sequence indicating a command a gaze pattern or gaze duration identified by the eye tracking software are also some examples of user input as one or more user physical actions of at least one body part. The microphone and sound recognition engine can also process natural user input of voice commands which may also supplement other recognized physical actions such as gestures and eye gaze.

The gesture recognition engine can identify actions performed by a user indicating a control or command to an executing application. The action may be performed by a body part of a user e.g. a hand or finger but also an eye blink sequence of an eye can be a gesture. In one embodiment the gesture recognition engine includes a collection of gesture filters each comprising information concerning a gesture that may be performed by at least a part of a skeletal model. The gesture recognition engine compares a skeletal model and movements associated with it derived from the captured image data to the gesture filters in a gesture library to identify when a user as represented by the skeletal model has performed one or more gestures. In some examples matching of image data to image models of a user s hand or finger during gesture training sessions may be used rather than skeletal tracking for recognizing gestures.

More information about the detection and tracking of objects can be found in U.S. patent application Ser. No. 12 641 788 Motion Detection Using Depth Images filed on Dec. 18 2009 and U.S. patent application Ser. No. 12 475 308 Device for Identifying and Tracking Multiple Humans over Time both of which are incorporated herein by reference in their entirety. More information about the gesture recognition engine can be found in U.S. patent application Ser. No. 12 422 661 Gesture Recognizer System Architecture filed on Apr. 13 2009 incorporated herein by reference in its entirety. More information about recognizing gestures can be found in U.S. patent application Ser. No. 12 391 150 Standard Gestures filed on Feb. 23 2009 and U.S. patent application Ser. No. 12 474 655 Gesture Tool filed on May 29 2009 both of which are incorporated by reference herein in their entirety.

The physics engine simulates the physics of motion of objects and the exchange of energy between objects as forces are applied to them based on rules governing a physical environment. In other words the physics engine helps make collisions between objects look real. The term collision in this specification is used in a physics sense of the word meaning a physical contact during which at least portions of different objects meet and each object exerts a force upon the other causing an exchange of energy. For example a handshake with a virtual person is a collision which can be modeled. A real object e.g. a hand collision with a virtual object and a virtual object collision with another virtual object can be modeled by the physics engine . In the illustrative examples discussed herein Newton s laws of physics are used as the illustrative rules for a physical environment. An application can define different physical environment rules. For example an environment having a different gravitational force than Earth s can be requested by inputting different environmental parameters.

Physics engine libraries are used by the physics engine in updating physics models and simulating actions and effects like collision effects sound effects and visual effects. Some examples of physics engine libraries are as follows. One or more materials lookup tables in the libraries can be referenced by the physics engine for identifying physics parameters like tensile strength and coefficients of friction for different types of materials. A pre collision events library includes data for representing events or actions for example a gesture which signal or trigger a collision. For example an object landing on a certain area in an environment may be a trigger for an explosion. The sport engine can register pre collision events with the physics engine .

An action simulator library includes software instructions for simulating movement of at least a part of an object based on input parameters of one or more physical properties. The sport engine can register simulated actions for avatars with the physics engine for use in an interactive avatar mode. A collision effects library comprises software routines for simulating a change in at least one physical property of an object during or resulting from a collision based on different input parameters. For example a collision effect may be a change in surface shape of an object to the collision. Other examples are different crack patterns or different breaking patterns for different materials or the same material in different orientations.

The sound library besides being a resource for command and object recognition based on sound may also store audio data for sound effects e.g. crack of a bat puck hitting a side of a rink to be played by the 3D audio engine and which may be linked with different simulated actions pre collision events and collision effects. Audio models for contact sounds and other sports related audio sounds may also have been registered by the sport engine with the physics engine as sound effects for types of virtual equipment. Similarly a visual effects library may store routines for animations highlighting and other types of visual enhancements which may also be associated with particular actions pre collision events and collision effects.

The libraries also store previously generated or stored virtual objects physics models and real objects physics models. Persistent object identifiers may be associated with the physics models so once a real object is recognized as a previously recognized object the physics model can be retrieved from storage rather than regenerated to save time. Similarly virtual objects previously registered by one or more applications for example via a software interface like an application programming interface API can be retrieved from the library as well. A plurality of physics models may be associated with an object. For example there may be a library of physics models for actions including sport performance movements associated with an avatar. For more information see U.S. patent application Ser. No. 13 458 800 to inventors Daniel J. McCulloch et al. entitled Displaying a Collision between Real and Virtual Objects filed on Apr. 27 2012 and which is incorporated by reference herein in its entirety.

An application like sport engine communicates data with the virtual data engine in order for the virtual data engine to display and update display of one or more virtual objects controlled by the application .

Virtual data engine processes virtual objects and registers the 3D position and orientation of virtual objects or imagery in relation to one or more coordinate systems for example in view dependent coordinates or in the view independent 3D map coordinates. The virtual data engine determines the position of virtual data in display coordinates for each display optical system . Additionally the virtual data engine performs translation rotation and scaling operations for display of the virtual data at the correct size and perspective. A virtual data position may be dependent upon a position of a corresponding object real or virtual to which it is registered. The virtual data engine can update the scene mapping engine about the positions of the virtual objects processed.

The following discussion describes some example processing for updating an optical see through augmented reality display to position virtual objects so that they appear realistically at 3D locations determined for them in the display. In one example implementation of updating the 3D display the virtual data engine renders the previously created three dimensional model of the display field of view including depth data for both virtual and real objects in a Z buffer. The real object boundaries in the Z buffer act as references for where the virtual objects are to be three dimensionally positioned in the display as the image generation unit displays the virtual objects but not real objects as the display device is an optical see through display device. For a virtual object the virtual data engine has a target 3D space position of where to insert the virtual object.

A depth value is stored for each display element or a subset of display elements for example for each pixel or for a subset of pixels . Virtual data corresponding to virtual objects are rendered into the same z buffer and the color information for the virtual data is written into a corresponding color buffer. The virtual data includes any modifications based on collision processing. In this embodiment the composite image based on the z buffer and color buffer is sent to image generation unit to be displayed at the appropriate pixels. The display update process can be performed many times per second e.g. the refresh rate . For a video see augmented reality display or operation of a see through display in a video see mode image data of the real objects is also written into the corresponding color buffer with the virtual objects. In a video see mode the opacity filter of each see through display optical system can be tuned so that light reflected from in front of the glasses does not reach the user s eye and the 3D image data of both the real and virtual objects is played on the display.

Device data may include an identifier for the personal apparatus a network address e.g. an IP address model number configuration parameters such as devices installed identification of the operating system and what applications are available in the personal A V apparatus and are executing in the personal A V apparatus etc. Particularly for the see through augmented reality personal A V apparatus the device data may also include data from sensors or sensing units or determined from the sensors or sensing units like the orientation sensors in inertial sensing unit the microphone and the one or more location and proximity transceivers in location sensing unit .

User profile data in a local copy or stored in a cloud based user profile has data for user permissions for sharing or accessing of user profile data and other data detected for the user like location tracking objects identified which the user has gazed at biometric data or determined states of being of the user. Besides personal information typically contained in user profile data like an address and a name physical characteristics for a user are stored as well. As discussed in more detail below physical characteristics include data such as physical dimensions some examples of which are height and weight width distance between shoulders leg and arm lengths and the like.

The virtual data provider system receives location data from personal A V apparatus in the location and tracks the location or position within a real physical location of one or more users with a user location and tracking module . Virtual data is made available for download by the apparatus based on one or more objects or areas around a user in a user s location. Optionally an authorization component is included to authenticate the user in the location.

The virtual data provider system includes a version of an image and audio data processing engine as a platform supporting the sport engine in servicing one or more personal A V apparatus which may be being used throughout the performance location. For example a scene mapping engine for the virtual data provider system tracking a view independent 3D mapping of objects including object identifiers for users in the location the real objects and the virtual objects and effects being displayed in the location. Additionally the virtual data engine of the provider system can provide processing resources to assist the personal A V apparatus update their display fields of view more quickly. For example the remote virtual data engine can format the virtual data in a standardized holographic format which is downloaded and readily displayed by the personal A V apparatus .

In this example a virtual data provider is communicatively coupled to a set of one or more sensors either directly or over a communication network like sensor . Some examples of sensors include video sensors depth image sensors heat sensors IR sensors weight sensors motion sensors etc. In this example the respective sensors are used to track the personal A V apparatus within the respective location for tracking where the user is and also capture image and depth data of the site throughout the day for tracking environmental conditions like weather and obstacles. Additionally there may be sensors included with the sports equipment e.g. ball club bat etc. for tracking a position in the location and orientation of the sports equipment with respect to one or more body parts of the user for providing data for analyzing an execution quality of a movement. In some examples either or both of the personal A V apparatus or a virtual data provider has a receiver e.g. wireless transceiver for receiving the sensors data of the user in the location from the sports equipment or both.

The technology may be embodied in other specific forms without departing from the spirit or essential characteristics thereof. Likewise the particular naming and division of modules routines applications features attributes methodologies and other aspects are not mandatory and the mechanisms that implement the technology or its features may have different names divisions and or formats.

For illustrative purposes the method embodiments below are described in the context of the system and apparatus embodiments described above. However the method embodiments are not limited to operating in the system and apparatus embodiments described above and may be implemented in other system and apparatus embodiments. Furthermore the method embodiments may be continuously performed while the personal A V apparatus is in operation and an applicable application is executing.

In step the scene mapping engine automatically identifies 3D space positions of at least one or more sport objects real or virtual in a sport performance area associated with the physical location based on a three dimensional mapping of objects in the sport performance area. In some embodiments the scene mapping engine notifies the sport engine of the identified 3D space positions of the one or more sport objects. An example of a sport object is sports equipment some examples of which are a football or baseball bat club goal post hockey net golf hole and flag. Another example of a sport object is another player. In other embodiments the sport engine is programmed to check the 3D mapping based on a programmed trigger like a time interval or data update notice. The sport engine database may have made accessible reference structures for one or more sports objects e.g. a basketball a basketball hoop and a backboard based on the sports applications supported by the engine. Besides sport objects the sport engine may also check for objects not related to the sport for purposes such as safety warnings due to objects which may pose obstacles outside the context of the sport and also to check environmental conditions and the confines of a sport performance area in a physical location. The sport performance area in the physical location may be a virtual representation of a sport performance area displayed by the AR display or a real sport performance area like a field court or course.

In one embodiment the sport engine may be executing software for providing recommendations for performing a physical sport movement. The recommendations may be provided automatically or responsive to user input requesting a recommendation. For example a user may have selected automatic recommendations by making a finger pointing gesture at a menu item in a menu displayed by the near eye AR display or may request a recommendation by audio command of saying recommendation which is captured by the microphone and processed by the speech recognition engine and forwarded to the sport engine. A recommendation may be personalized by factoring in the user s abilities as well as the context parameters e.g. distance of a ball from a goal of the current play. In step the sport engine accesses stored physical characteristics and skills data from user profile data stored for the user for example in network accessible user profile data accessible to the sport engine.

Physical characteristics include data such as physical dimensions some examples of which are height and weight width distance between shoulders. Some physical characteristics may have been input by the user like height and weight. A characteristic like height may be determined based on inertial sensing data from the inertial sensing unit and perhaps reference image data as the user wears the near eye display on his head. In some instances image data of the user captured by another user s apparatus or data sharing cameras like image sensors which may include 3D image capture devices in the real physical location may also be a source for identifying physical characteristics. Some other examples of physical characteristics are widths lengths and ratios for upper and lower legs widths lengths and ratios for upper and lower arm parts. Based on performance data stored for previous executions of sports movements a strength rating may be assigned for a user s arms and legs as part of their physical characteristics.

In step the sport engine determines a physical movement recommendation by a processor for the user performing the sport based on the accessed physical characteristics skills data and 3D space positions for at least the one or more sports objects. In some embodiments the 3D space position of an object which is not related to the sport may be a basis for the determination as well. As discussed below provides an example of a process for determining a physical movement recommendation.

In step 3D virtual data depicting one or more visual guides for assisting the user in performing the physical movement recommendation is displayed from a user perspective associated with a display field of view of the near eye AR display. A user moves around typically when playing a sport or sport objects real or virtual move with respect to the user. The scene mapping engine identifies a change in the display field of view and updates the displaying of 3D virtual data based on the change in the display field of view. Additionally the sport engine notifies the scene mapping engine of changes effecting the position of virtual data it generates in the course of the sport performance session like changes in direction and speed of objects and removal and appearance of visual guides. The scene mapping engine updates the 3D mapping based on these changes.

As mentioned above in the embodiment of the sport engine accesses stored rules for a sport. Some examples of rules are those by which points are scored minimum parameters for a sport performance area or sport performance location physical sport movements available to different player positions e.g. catcher pitcher quarterback etc. within different zones of the performance location and at different time periods in the course of a game e.g. overtime.

In step the sport engine includes a recommendation engine which processes the recommendation rules in the databases and assigns a sport objective score for each available physical sport movement based on a probability of success in achieving at least one sport objective. Some examples of sport objectives are advancing the runner increasing running speed and blocking an opponent s ball. Objectives are sport specific. In some examples a weighted value may be assigned to each available movement representing its probability of success. The different objectives may also be weighted based on the current context of the performance session as well and factored into the objective score for each movement.

In step the sport engine assigns a user execution score based on user skills data for each available physical sport movement. For example again weights may be assigned. A person may have good execution skills with a seven iron in golf but not with lower numbered irons and woods so a golf swing with a seven iron may receive a higher execution score than a golf swing with a three iron. The sport objective score however for the seven iron is lower than the three iron in an example context in which the distance to the hole is about 250 yards.

In optional step the sport engine may assign an environmental conditions score for each available physical sport movement representing an effect on the respective movement in achieving at least one sport objective. For example wind conditions may increase or decrease the projected speed of a ball like a football or a golf ball. In an example where the player is playing in a real sport performance area sensors may include sensors for detecting environmental conditions like weather conditions. In a virtual environment sport performance session using a virtual sport performance area a user may have selected conditions like rain or snow or wind to be implemented during the session.

In step the sport engine calculates a recommendation score for each available physical sport movement based on the respective sport objective score and the user execution score and optionally the environmental conditions score. For example in some embodiments the weighted scores may simply be added to calculate a recommendation score and the movement with the highest score selected as the physical movement recommendation. How recommendations are calculated is a matter of design choice. For example other steps may be included such as randomly deleting one recommendation score or one objective score to avoid repetitive recommendations in similar situations.

In step the sport engine selects a physical sport movement for the physical sport movement recommendation having a recommendation score satisfying a selection criteria. For example as mentioned in the discussion of step the selection criteria may be a highest combined score of the objective and user execution scores. In other examples the selection criteria may be different based on how the logic embodied in the sport engine assigns weights or otherwise designates priorities among factors.

Some examples of information a physical sport movement recommendation may include are a type of movement what piece of sports equipment to use a direction of aim a position on a field or court at which to perform the movement a suggested body movement e.g. duck run throw run and reach for a fly ball move back and bend down to catch a ball and a strength to use for a movement. The physical sport movement is sport dependent. For example a golf physical sport movement may include a club recommendation and a direction of aiming the golf ball which may not be directly at the hole depending on the obstacles like a pond or sand trap in the way and the user s abilities identified in his skills data.

In some basketball examples the recommendation may include one or more movements such as dribbling to a spot or zone of the court and taking a basketball shot from there with a medium strength. An alternative may be a jump shot in a zone farther from the basket with a high strength designation. For example a bunt swing in baseball is different from a baseball swing typically used to get a hit. A basketball dunk shot recommendation may be for one hand or for two hands and the hand or hands are near the top of the ball as the user is typically very near the basket. A recommendation for a shot a further distance from the basket may indicate hand placement near the middle of the ball and a recommendation of taking a shot at the basket from the opponent s court side may indicate or the hands are near the bottom to keep the ball in the air for the longer distance. In another example the angle at which the quarterback holds the ball is different for a long pass down the field than for a short lateral to a receiver or a hand off for a ground play.

As illustrated in later figures the recommendation may also include execution tips personal to the user in the form of displayed visual guides which help the user in executing one or movements by a body part designated in the physical movement recommendation. The visual guides may be image data of the user s own body part or parts which the user can place his or her one or more actual body parts over and follow for executing the movement.

In step the sport engine tracks a physical sport movement being performed by the user based on sensor data including image data captured by a natural user interface NUI of the personal A V apparatus and in step stores sports performance data representing the tracked physical sport movement in user profile data.

For example in the embodiment of the sport engine may track a physical sport movement being performed by the user. The sport engine may make filters available to the gesture recognition engine to use in identifying one or more physical sport movements associated with the sport and notifying the sport engine. In some embodiments the sport engine stores image data of the movement or a motion capture file derived from image data e.g. skeletal model of movement as sports performance data for the user. In an example where the user is using a real sport object the motion of the real sport object set in motion by the actual physical movement of the user can be tracked based on sensor data virtual data is displayed illustrating the tracked motion of the real sport object. The same motion tracking and display of the path of motion of a virtual sport object may also be displayed.

In step the sport engine stores sport performance data representing the tracked physical sport movement in user profile data and in step the sport engine analyzes the sport performance data in accordance with stored execution criteria for a type of the tracked physical sport movement.

In some examples execution criteria may be embodied as skeletal reference models or image data of skill level representative executions of the physical sport movement for the user s physical characteristics. The captured image or image derived data can be compared against the execution criteria models for a closest match in some examples. The differences of one or more body parts from a model for a movement can be measured and identified as falling within one or more ranges. In some examples the differences may be defined in terms of 3D distance angular rotation or 3D direction differences between the user s body part and the same type of body part represented in the model. Stored execution criteria can have predetermined difference ranges for one or more body parts. In some examples based on which body parts fall within which predetermined difference ranges an execution quality score and a skill level for the movement can be assigned. An overall skill level for the user may be assigned based on an accumulation of skill levels assigned for various movements associated with a sport. Some examples of skill levels are beginner intermediate and expert.

Particularly in embodiments in which the portable personal A V apparatus is operating without image data from other cameras at predetermined positions for capturing a user from angles outside the display field of view for example a 360 degree arrangement of cameras determining execution quality of a movement can be based on image data of the display field of view as well as non image sensed data like audio data gaze data orientation data the 3D mapping and sensor data on the sport equipment if available. For example in golf an example of a physical sport movement is a golf swing. A golfer tends to look at the golf ball. Anything a user looks at image capture devices captures as well at least in most sports situations. An inertial sensing unit can approximate how far the user s head is from the ball and such inertial data can also refine or update a distance determined from the front captured image data. Furthermore an orientation of the near eye augmented reality AR display can be determined for example based on date from the inertial sensing unit .

A user also naturally tends to look at her hands on the club so the sport engine can identify grip issues based on the image data. Gaze data can identify that the user took her eyes off the ball and when during the swing she did so. Golf in particular is a sport with audio sounds which identify how a club head met the ball in a swing. For example topping the ball as often occurs when one takes one s eyes off the ball has an identifiable sound as does hitting the ball in the sweet spot of the club. A microphone of the personal A V apparatus can capture a sound made in a physical location by a real sport object. The one or more processors are communicatively coupled to receive audio data representing the sound from the microphone and executes a sound recognition engine for identifying the sound as a sound related to the physical sport movement. The one or more processors compare the sound related to the physical movement against execution sound criteria for assigning a sound factor e.g. a weighting as part of execution criteria for analyzing the execution quality of a swing and assigning a skill level for the physical sport movement.

The sports equipment like the ball or puck and other equipment like a club bat or hockey stick may also have sensors which provide data on their positions and contact points with other real objects like clubs and bats used by the user or in a physical sport performance location. Such sensor data can also be provided as bases to the sport engine for analyzing the physical sport movement in identifying things effecting execution quality like strength and direction of a swing based on where the ball landed and where contact was made with a bat or club or backboard.

In step the sport engine assigns a skill level for the physical sport movement based on the execution criteria and in step the sport engine updates skills data in user profile data based on the assigned skill level for the movement. See the discussion above for examples. The skills data may be stored locally or in a network accessible memory like cloud based memory for storing user profile data . In step the sport engine may also perform steps and of for providing a recommendation.

In some embodiments the user may be using or playing with virtual sport objects like a virtual ball and virtual other players. The method embodiments discussed above may also comprise displaying from the user perspective motion of a virtual sport object responsive to the actual physical movement of the user. In other examples the method embodiments like those in may also include the sport engine displaying a virtual sport performance area for the user performing the sport and determining the physical movement recommendation by the processor for the user based on one or more features in the virtual sport performance area. For example a user may be playing with real or virtual sports equipment in his living room but based on the 3D virtual data displayed by his near eye AR display he is playing at a famous golf course where a well known annual tournament occurs.

Before discussing enhancing a sport experience with display of avatars audio data can also be used to provide feedback on execution quality when performing a sport with virtual sports equipment. The sport databases may also include a contact sound for a virtual sport object. The one or more processors e.g. or of companion processing module of the personal A V apparatus may determine audio parameters for a contact sound for a virtual sport object contacted in the physical sport movement based on the sports performance data captured and analyzed for the physical sport movement. The 3D audio engine can generate and play the contact sound related to the physical sport movement in accordance with the audio parameters determined for the contact sound through one or more audio output device e.g. earphones of the personal A V apparatus.

In step the sport engine or the gesture engine based on filters registered by the sport engine as mentioned above identifies a beginning and an end of a user physical sport movement. The different aspects of a physical movement may not all be captured by the outward facing capture devices . Image data from different perspectives and sensor data with different reference points may be available. Such data is used by the sport engine to transform 3D virtual data representing a sport object or a user body part in the movement to a coordinate system so the 3D virtual representation appear to the user from his or her perspective as approximated by the near eye AR display. Image data of a user s hands lower arms and golf club captured by capture devices in a first time period of a swing may be used in the visual feedback with transparent color data for a reference model. Image data from a camera opposite the front of the user may capture the club arm and shoulder movements outside the field of view of the devices . Such supplemental image data may be transformed to keep the motion of the club and body parts but from a user perspective to display the motion of the full swing from the user perspective associated with the display field of view. In step displays 3D virtual data of the user physical sport movement along with a reference model of execution for the movement from a user perspective associated with a display field of view of the near eye AR display being worn by the user. Such visual feedback will help a user with the form of her golf swing. The user can move her head and follow the motion of her swing and the visual guides depicting the reference model of execution from her perspective as if making the swing in her near eye AR display. The reference model of execution may be generated as visual guides based on a physical movement recommendation determined by the sport engine and selected by the user.

For example after a golf swing a user following her golf swing in her near eye AR display can see with her head and eyes discussed below also illustrates an example of post movement visual feedback for a golf swing.

Performing a sport with an avatar can definitely enhance a sport performing experience. For example playing against an avatar of oneself or even just seeing one s prior performance on a previous trek or golf course can be educational. Additionally while really playing in one s living room but virtually at a selected sport performance location or at an actual physical sport performance location which is where tournaments have taken place it may be fun to see the prior performance of a celebrity as presented by that person s avatar playing with virtual equipment or even just captured video data for the same context point in the performance session e.g. at the third hole. In some example a prior sport performance may be selected to be performed by an avatar even if the user s sport performance area or location is not the same area or location where the prior sport performance occurred. For example the user may be at a community basketball court and wishes to see a favorite celebrity s play.

It may even be more fun to interactively play with a celebrity. For example a user at a neighborhood baseball field may select a virtual sport performance area or location of Yankee Stadium in 1956 to be overlaid on the real baseball field. The user may select a position to play in a pivotal game of the 1956 World Series between the Yankees and the Brooklyn Dodgers and play that player s role using virtual equipment. As discussed above the sport engine may interface with a physics engine to cause avatars of the other players to perform the sports movements they were known to play in the game but also to perform a sport movement in response to the user s sports movements during the game. The user can see how the game would have been effected by his performance. In another example the user can assemble a virtual dream team or fantasy team to play on in a session against another such team on which a real friend is playing. The sport engine predicts the actions of the dream team members based on performance data skills data or both stored for the dream team avatars.

In some examples the avatar may be a virtual coach for whom audio and virtual data is displayed representing the coach as giving verbal and demonstrative instructions on physical movement recommendations.

In other examples one person may be playing at a remote location location. For example an avatar of a first player at a real course is displayed as 3D virtual data in the near eye AR display of a second player in his living room which second player is playing on a virtual version of the real sport performance location e.g. golf course or basketball court . In other examples the virtual version of the real sport performance location may be displayed on a display separate from the near eye AR display while visual guides for enhancing execution of a movement are displayed from the user perspective by the near eye AR display. Similarly an avatar of the second player may be displayed by the near eye AR display of the first player as if at the real sport performance location at a position in context with the game or performance e.g. at the position where the second player s ball would have landed on the real course which is analogous to where his virtual ball was determined to land on the virtual version of the course. Other cameras in the remote location such as a 3D image capture device like that associated with a game console and image capture devices in the real sport performance location like those which may be communicating with a virtual data provider may provide additional image data for enhancing the realism of the respective avatar mimicking the represented user s sports movements in real time.

In other examples an avatar not associated with a specific person may be selected. For example the sport engine may provide a selection e.g. a menu of avatars for a highly skilled player an intermediate player an advanced beginner player and a beginner player from which a user can select an avatar to play against or watch performing similar movements as the user for skill improvement purposes.

Joe holds a virtual basketball as he readies for a throw at the basket . Joe s holding of the basketball is a collision. Joe s hands apply forces on the ball and the ball has reactive forces. The basketball application executing in Joe s personal A V apparatus e.g. and provides a physics model for each of the virtual objects it generates like the basketball the basketball hoop and the backboard with box . The physics models for the real objects were derived from image data any user provided information during any inventory and object reference data stored in structure data .

As Joe contemplates making a throw the image and audio processing engine of the personal A V apparatus updates the 3D mapping of Joe s living room to indicate Joe s distance and direction to the virtual basketball hoop based on depth data captured by outwarding facing capture devices . The sport engine identifies that based on the distance and direction of Joe to the hoop a throw has an acceptable likelihood of increasing his score by making the basket based on for example recommendation rules as may be stored in sport databases .

The sport engine identifies actions or movements which make up a throw movement and execution criteria to be met in order for the success of the throw. Some of the actions or movements which make up the throw movement are 1 directing one s gaze at the target position of the throw 2 a distance to the target position 3 a direction to the target throw 4 position of the ball with respect to a reference point of the personal A V apparatus approximating a reference point to a user body part or parts and 5 hand positions on the ball. A recommendation of a throw at the basket is made with respect to Joe s physical characteristics and visual guides are generated and displayed to assist Joe in performing the recommended throw.

In many embodiments the sport engine updates a physical movement recommendation in real time based on a change in the sport context such as a user electing not to follow the recommendation another player has moved the ball or environmental conditions have changed such as an obstacle has entered the sport performance area. As in the grip example if the user rejects a club recommendation and chooses another club the sport engine updates its recommendation with updated grip advice for the user club selection which advice may be displayed visually using a visual guide.

Below is described an embodiment of a personal A V apparatus provide a personalized experience for the user while playing a sport. In one embodiment a user operating the personal A V apparatus will be provided with assistance during a game. For example in golf the personal A V apparatus can act like a virtual caddy that suggests shots suggests clubs advises for weather conditions provides strategy and automatically tracks the ball after being hit. In one embodiment the personal A V apparatus will also display the results of another player e.g. a friend or famous player for the same golf course so that the user can play against the other player. This technology can be used for sports other than golf. As described above some embodiments a user could actually play with the other player. A hologram of that player could appear on the course and tee up before or after the user. This may be previously captured data that has been uploaded and would then be specific to that course or as in the embodiment of the sport engine can communicate with the physics engine for coordinating interactive play.

In step Virtual Data Provider System accesses weather conditions including wind speed wind direction and precipitation information. In step data is accessed for the golf course or other type of field . This data will include the map of the field contours indications of traps etc. In step Virtual Data Provider System will access user profile data for the user who registered at step the information about the identity of the user was provided in step . In step Virtual Data Provider System determines the effects of weather e.g. wind rain . In step Virtual Data Provider System determines a high risk shot or other type of play for other sports based on the location of the personal A V apparatus the location of the ball weather conditions and the course information accessed in . Using the same data the system will determine a low risk shot play in step . Virtual Data Provider System determines the appropriate clubs to use for a shot in step . The manner for best addressing the ball is determined in step including where to stand and what orientation for the user to put his or her body.

In step the information determined above in steps can be adjusted based on the accessed user profile data. For example if the user is a particularly unskilled player or a novice the system will choose a recommendation that is easier to accomplish.

In step data for another player s game will also be accessed. For example the user may want to play against a friend who previously played the same course. Alternatively the use may want to play against a famous player such as a professional player who played the same course. Information for the other player for the same hole or same shot or same play will be accessed in step . In step the information determined in steps is sent back to personal A V apparatus .

In step the high risk shot play is reported to the user by displaying the information in the personal A V apparatus . In step personal A V apparatus will display the low risk shot play. In step effect of weather will be displayed. In step suggestion of which club to use will be displayed to the user. In step a suggestion of how to address the ball will be displayed in the personal A V apparatus. For example a diagram of where to stand and how to hit the ball can be displayed in the see through optical system of the personal A V apparatus in manner such that the user can see the actual ball unoccluded by any virtual data. In step personal A V apparatus will display the other player s results. For example the system can display a video of the other player can be shown an animation of what happened when the other player played the same course or text identifying the results for the other player. Note that the information displayed in steps will be displayed by the optical system within the personal A V apparatus as discussed above . In one embodiment the system can ghost the user with the user s last time played there.

After step it is assumed that the player will hit the ball. In step the personal A V apparatus will automatically track the ball so that when the balls lands the personal A V apparatus can render an arrow or other shape in the display field of view in the personal A V apparatus to show the user where the ball is. Additionally user profile data can be updated based on performance of the shot.

Many of the embodiments described herein include storing data about a user objects and places. This data is then used to augment reality when looking through a personal A V apparatus. To allow for the efficient storage of such data and exchange of such data it is contemplated to have a predetermined standard format for storing that data. An example of such a format is referred to as a holographic file format. Use of a holographic file format will allow for portability of data between platforms compressing the data use of smart objects and facilitating virtual representation of real world objects. For more information about some embodiments of a holographic file format see U.S. patent application Ser. No. 13 430 972 Geisner et al. entitled Personal Audio Visual System with Holographic Objects filed Mar. 27 2012 which is hereby incorporated herein by reference.

Device may also contain communications connection s such as one or more network interfaces and transceivers that allow the device to communicate with other devices. Device may also have input device s such as keyboard mouse pen voice input device touch input device etc. Output device s such as a display speakers printer etc. may also be included. These devices are well known in the art so they are not discussed at length here.

The example computer systems illustrated in the figures include examples of computer readable storage devices. A computer readable storage device is also a processor readable storage device. Such devices may include volatile and nonvolatile removable and non removable memory devices implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Some examples of processor or computer readable storage devices are RAM ROM EEPROM cache flash memory or other memory technology CD ROM digital versatile disks DVD or other optical disk storage memory sticks or cards magnetic cassettes magnetic tape a media drive a hard disk magnetic disk storage or other magnetic storage devices or any other device which can be used to store the information and which can be accessed by a computer.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. The specific features and acts described above are disclosed as example forms of implementing the claims.

