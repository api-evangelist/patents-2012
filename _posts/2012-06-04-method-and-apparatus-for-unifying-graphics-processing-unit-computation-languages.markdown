---

title: Method and apparatus for unifying graphics processing unit computation languages
abstract: A method and apparatus for unifying graphics processing unit (GPU) computation languages is disclosed. The method comprises identifying a GPU of a computer system; accessing a plurality of macros representing a difference in source code between a first GPU computation language and a second GPU computation language, expanding each macro in the plurality of macros based on the identified GPU and executing a kernel on the computer system using the expanded macro.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09275426&OS=09275426&RS=09275426
owner: ADOBE SYSTEMS INCORPORATED
number: 09275426
owner_city: San Jose
owner_country: US
publication_date: 20120604
---
Embodiments of the present invention generally relate to computer graphics processing and more particularly to a method and apparatus for unifying graphics processing unit computation languages.

A graphics processing unit GPU is a processor optimized for accelerating computer graphics. GPU computation languages have been developed for rendering the GPUs on specific hardware and or software platforms.

For example a GPU method for rendering computer graphics built on NVIDIA s Compute Unified Device Architecture CUDA technology can only run on the NVIDIA hardware platform while Open Computing Language OpenCL is a GPU computing language that may run on multiple platforms. In order to allow platform independent application and integration of GPUs having a platform dependency the GPU s computation language platform dependency must be overcome.

One solution which would allow platform independent application and integration of the CUDA and OpenCL computing languages for example is to maintain parallel OpenCL and CUDA code bases. However this solution requires a massive amount of duplicated code that becomes difficult to maintain. Another solution is to develop a new language or compiler that facilitates use of the functionality of both OpenCL and CUDA. However this requires a significant commitment of time and expense to develop and test the new language. Another solution is to support only OpenCL but that would require abandoning a significant amount of existing and well tested CUDA code. Also CUDA allows certain functionality on NVIDIA hardware which functionality is not available through OpenCL.

The present invention provides a method and apparatus for unifying graphics processing unit computation languages. In one embodiment the method comprises identifying a GPU on a computing device and accessing a plurality of macros representing a difference in source code between a first GPU computation language and a second GPU computation language. The method expands each macro based on the identified GPU and executes a kernel on the computing device using the expanded macro.

While the method and apparatus is described herein by way of example for several embodiments and illustrative drawings those skilled in the art will recognize that the method and apparatus for unifying graphics processing unit computation languages are not limited to the embodiments or drawings described. It should be understood that the drawings and detailed description thereto are not intended to limit embodiments to the particular form disclosed. Rather the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope of the method and apparatus for unifying graphics processing unit computation languages as defined by the appended claims. Any headings used herein are for organizational purposes only and are not meant to limit the scope of the description or the claims. As used herein the word may is used in a permissive sense i.e. meaning having the potential to rather than the mandatory sense i.e. meaning must . Similarly the words include including and includes mean including but not limited to. Additionally as used herein the word video refers generally to any multi media object or data and although in various embodiments of the invention may refer to only a video object still animated or moving in other embodiments may refer to a combination of both video and audio objects or an audio object alone.

Embodiments of the present invention comprise a method and apparatus for unifying GPU computation languages. The embodiments identify the GPU of a computer system. The method then accesses macros that represent a difference in source code between a first GPU computation language and a second GPU computation language. The embodiments expand the macros based on the identified GPU and execute a kernel using the expanded macros.

In the present invention a macro contains source code for both the first GPU computation language and the second GPU computation language along with instructions for when each piece of source code is to be used. A computer program also called a kernel contains tokens. Each token is a placeholder that corresponds to a macro. Macro expansion is a term which describes the process of substituting one or more lines of source code from a macro in place of the corresponding token in the kernel.

The following is a simple example to illustrate what happens in a macro expansion. A kernel is written to perform a function on a computer for example to print a poster. The poster can be printed in multiple languages. The title of the poster must be translated into one of the multiple languages before it is printed. The kernel may be written in such a way that it need not contain every translation. The kernel need only be written with a token that upon execution will access a macro. The macro will provide the appropriate substitution during macro expansion. For example the kernel contains a placeholder token named TRANSLATE TITLE. When the kernel is executed a TRANSLATE TITLE macro is accessed and the TRANSLATE TITLE macro is expanded. That is the token TRANSLATE TITLE in the kernel is replaced with a substitute local value defined in the expanded macro. For English the TRANSLATE TITLE token will be replaced with My Poster as the substitute value. For Spanish the TRANSLATE TITLE token will be replaced with Mi Cartel as the substitute value. However rather than simply replacing a token with a substitute value the present invention replaces the token with CUDA or OpenCL code.

Although the present disclosure describes the invention in terms of CUDA and OpenCL GPU Computation Languages the present invention may be used to unify two or more GPU computation languages derived from a common programming language such as C .

Because CUDA and OpenCL are both programming languages derived from the C programming language there are many similarities in the code bases. Embodiments of the present invention define a compatibility layer in the source code of a kernel. The compatibility layer of the source code contains tokens such as TRANSLATE TITLE in the previous example. Macros are defined which contain language specific source code that will replace these tokens in the compatibility layer. The macros represent the specific differences between the code bases of the two GPU computation languages. When the kernel is executed the macro is expanded. As described above macro expansion is a term which describes the process of substituting one or more lines of source code from a macro in place of the corresponding token in the kernel. The specific code for the appropriate GPU language which is contained in the macro is substituted into the place where the token had been located in the source code. Whether the macro is expanded for running in a CUDA environment and or expanded for running in an OpenCL environment the macro defines a specific behavior in the GPU.

Embodiments of the present invention provide a method and apparatus for unifying GPU computation languages. Advantageously there is no need to maintain separate code bases or separate compilers.

Various embodiments of an apparatus and method for unifying graphics processing unit computation languages are described. In the following detailed description numerous specific details are set forth to provide a thorough understanding of the claimed subject matter. However it will be understood by those skilled in the art that claimed subject matter may be practiced without these specific details. In other instances methods apparatuses or systems that would be known by one of ordinary skill have not been described in detail so as not to obscure claimed subject matter.

Some portions of the detailed description which follow are presented in terms of algorithms or symbolic representations of operations on binary digital signals stored within a memory of a specific apparatus or special purpose computing device or platform. In the context of this particular specification the term specific apparatus or the like includes a general purpose computer once it is programmed to perform particular functions pursuant to instructions from program software. Algorithmic descriptions or symbolic representations are examples of techniques used by those of ordinary skill in the signal processing or related arts to convey the substance of their work to others skilled in the art. As described herein an algorithm is generally considered to be a self consistent sequence of operations or similar signal processing leading to a desired result. In this context operations or processing involve physical manipulation of physical quantities. Typically although not necessarily such quantities may take the form of electrical or magnetic signals capable of being stored transferred combined compared or otherwise manipulated. It has proven convenient at times principally for reasons of common usage to refer to such signals as bits data values elements symbols characters terms numbers numerals or the like. It should be understood however that all of these or similar terms are to be associated with appropriate physical quantities and are merely convenient labels. Unless specifically stated otherwise as apparent from the following discussion it is appreciated that throughout this specification discussions utilizing terms such as processing computing calculating determining or the like refer to actions or processes of a specific apparatus such as a special purpose computer or a similar special purpose electronic computing device. In the context of this specification therefore a special purpose computer or a similar special purpose electronic computing device is capable of manipulating or transforming signals typically represented as physical electronic or magnetic quantities within memories registers or other information storage devices transmission devices or display devices of the special purpose computer or similar special purpose electronic computing device.

The computer comprises a CPU support circuits and a memory . The computer is a type of computing device e.g. a laptop a desktop a Personal Digital Assistant PDA and or the like . The CPU may comprise one or more commercially available microprocessors or microcontrollers that facilitate data processing and storage. The various support circuits facilitate the operation of the CPU and include a GPU one or more clock circuits power supplies cache input output circuits displays and the like. The memory comprises at least one of Read Only Memory ROM Random Access Memory RAM disk drive storage optical storage removable storage and or the like. The memory comprises an operating system OS a plurality of kernels a unifying agent a plurality of macros and a GPU identification .

According to some embodiments of the invention the operating system generally manages various computer resources e.g. network resources file processors and or the like . The operating system is configured to execute operations on one or more hardware and or software modules such as Network Interface Cards NICs hard disks virtualization layers firewalls and or the like. Examples of the operating system may include but are not limited to Linux Mac OSX BSD UNIX Microsoft Windows and the like.

According to some embodiments the unifying agent unifies GPU computational languages as described with respect to below. The unifying agent accesses macros. In some embodiments the macros represent a difference in source code between computation languages for example a difference between a first GPU computation language and a second GPU computation language. According to an embodiment the first GPU computational language is Compute Unified Device Architecture CUDA and the second GPU computational language is Open Computing Language Open CL . The unifying agent identifies the GPU. The GPU identification is obtained by determining whether the hardware of the computer is CUDA based or OpenCL based. In some embodiments CUDA runs NVIDIA hardware whereas OpenCL runs on non NVIDIA hardware. NVIDIA hardware does not understand a program that is written in OpenCL and non NVIDIA hardware does not understand a program that is written in CUDA.

The macros define differences in the source code between the GPU languages. For example a first GPU language may require a certain syntax to start a program for example syntaxA. A second GPU language may require a different syntax to start a program for example syntaxB. A macro will contain instructions for when to use syntaxA and when to use syntaxB. A kernel is a function that runs on the computer and accesses a macro . The kernel has a compatibility layer written into its source code. The compatibility layer contains tokens sometimes referred to as placeholders. For example a token may be called START PROGRAM . When the kernel is executed the unifying agent expands the macro START PROGRAM based on the identified GPU. As described above macro expansion is a term which describes the process of substituting one or more lines of source code from a macro in place of the corresponding token in the kernel. The macro provides instructions to replace the START PROGRAM token in the kernel with syntaxA in the first GPU environment and to replace the START PROGRAM token with syntaxB in the second GPU environment. Thus only one kernel needs to be written to perform a function in either GPU environment by expanding a macro to reconcile the differences in the GPU computation languages.

The method identifies the GPU on the computer and accesses macros created to resolve the language specific differences depending on what GPU is being targeted. The macros will be expanded based on the identified GPU. The macros represent a difference in source code between computation languages for example a difference between a first GPU computation language and a second GPU computation language. According to one embodiment the first GPU computational language is Compute Unified Device Architecture CUDA and the second GPU computational language is Open Computing Language Open CL .

The method starts at step and proceeds to step . At step the method identifies the GPU. The GPU is identified by determining for example whether hardware of the computer on which the application is running is CUDA based or OpenCL based. Determining the type of graphics card present on a computer identifies the GPU. In some embodiments the type of graphics card present is automatically detected. In some embodiments the type of graphics card present is determined by running a utility program that returns the type of graphics card that is present. The method proceeds to step .

At step the method accesses a plurality of macros. The macros define differences in the source code between the GPU languages and provide instructions for when each is to be utilized. Each macro corresponds to at least one token in the kernel. The method proceeds to step . At step the method expands the macros based on the identified GPU. The method expands the macros when the kernel is executed. As described above macro expansion is a term which describes the process of substituting one or more lines of source code from a macro in place of the corresponding token in the kernel. When the kernel is executed on the computer and the identified GPU is OpenCL all of the tokens in the kernels are substituted with OpenCL source code found in the macros. Once substitution is complete all of the kernels are now written in OpenCL. Conversely if the identified GPU is CUDA all of the tokens in the kernels are substituted with CUDA source code found in the macros resulting in all of the kernels now being written in CUDA source code.

As will be evident in the following description when executed in a CUDA environment a GF KERNEL macro is expanded using the part of the macro defined for CUDA. When compiling in an OpenCL environment the GF KERNEL macro is expanded using the part of the macro defined for OpenCL. The following GF KERNEL defines a point of entry in a kernel which executes on a GPU. It essentially is the term for start . However when starting a kernel in CUDA  global void denotes the meaning start and in OpenCL  kernel void denotes the meaning start . Although much of the rest of the source code is the same the start phrase must be reconciled based on the GPU in which it is to be compiled. When the macro is expanded for CUDA the GF KERNEL token is replaced by  global void . When the macro is expanded in OpenCL the GF KERNEL token is replaced by  kernel void .

The following is an example of a kernel for sharpening an image that uses the GF KERNEL macro from above 

Hence the line in the code above that reads GF KERNEL SharpenKernel will read  global void SharpenKernel in CUDA after macro expansion but will read  kernel void Sharpen Kernel in OpenCL after macro expansion.

Therefore only one kernel is written but the kernel runs different code depending on the specific GPU that is identified. In some instances where the code bases for the two GPU computation languages are so divergent and share little common code separate piece of code are maintained for each GPU language. In the example above the way to define texture in CUDA is so different from the way texture is defined in OpenCL that the GF CUDA TEXTURE macro and the GF OPENCL TEXTURE macro are maintained separately. When the GF CUDA TEXTURE macro is expanded CUDA source code will replace the GF CUDA TEXTURE token but the macro will not contain OpenCL source code thereby replacing the GF CUDA TEXTURE token with a blank or null value in OpenCL . Conversely when the GF OPENCL TEXTURE macro is expanded OpenCL source code will replace the GF OPENCL TEXTURE token but the macro will not contain CUDA source code thereby replacing the GF OPENCL TEXTURE token with a blank or null value in CUDA .

The method proceeds to step . At step the method executes the kernel using the expanded macros. Once the macro expansion from step is complete the method compiles the kernels thereby translating them into executable code and finally executes the compiled kernels on the computer. The method proceeds to step and ends.

The embodiments of the present invention may be embodied as methods apparatus electronic devices and or computer program products. Accordingly the embodiments of the present invention may be embodied in hardware and or in software including firmware resident software micro code etc. which may be generally referred to herein as a circuit or module . Furthermore the present invention may take the form of a computer program product on a computer usable or computer readable storage medium having computer usable or computer readable program code embodied in the medium for use by or in connection with an instruction execution system. In the context of this document a computer usable or computer readable medium may be any medium that can contain store communicate propagate or transport the program for use by or in connection with the instruction execution system apparatus or device. These computer program instructions may also be stored in a computer usable or computer readable memory that may direct a computer or other programmable data processing apparatus to function in a particular manner such that the instructions stored in the computer usable or computer readable memory produce an article of manufacture including instructions that implement the function specified in the flowchart and or block diagram block or blocks.

The computer usable or computer readable medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus device or propagation medium. More specific examples a non exhaustive list of the computer readable medium include the following hard disks optical storage devices a transmission media such as those supporting the Internet or an intranet magnetic storage devices an electrical connection having one or more wires a portable computer diskette a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber and a compact disc read only memory CD ROM .

Computer program code for carrying out operations of the present invention may be written in an object oriented programming language such as Java Smalltalk or C and the like. However the computer program code for carrying out operations of the present invention may also be written in conventional procedural programming languages such as the C programming language and or any other lower level assembler languages. It will be further appreciated that the functionality of any or all of the program modules may also be implemented using discrete hardware components one or more Application Specific Integrated Circuits ASICs or programmed Digital Signal Processors or microcontrollers.

The foregoing description for purpose of explanation has been described with reference to specific embodiments. However the illustrative discussions above are not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The embodiments were chosen and described in order to best explain the principles of the present disclosure and its practical applications to thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications as may be suited to the particular use contemplated.

Various embodiments of method and apparatus for unifying graphics processing unit computation languages as described herein may be executed on one or more computer systems which may interact with various other devices. One such computer system is computer system illustrated by which may in various embodiments implement any of the elements or functionality illustrated in . In various embodiments computer system may be configured to implement methods described above. The computer system may be used to implement any other system device element functionality or method of the above described embodiments. In the illustrated embodiments computer system may be configured to implement method as processor executable executable program instructions e.g. program instructions executable by processor s in various embodiments.

In the illustrated embodiment computer system includes one or more processors coupled to a system memory via an input output I O interface . Computer system further includes a network interface coupled to I O interface and one or more input output devices such as cursor control device keyboard and display s . In various embodiments any of components may be utilized by the system to receive user input described above. In various embodiments a user interface e.g. user interface may be generated and displayed on display . In some cases it is contemplated that embodiments may be implemented using a single instance of computer system while in other embodiments multiple such systems or multiple nodes making up computer system may be configured to host different portions or instances of various embodiments. For example in one embodiment some elements may be implemented via one or more nodes of computer system that are distinct from those nodes implementing other elements. In another example multiple nodes may implement computer system in a distributed manner.

In different embodiments computer system may be any of various types of devices including but not limited to a personal computer system desktop computer laptop notebook or netbook computer mainframe computer system handheld computer workstation network computer a camera a set top box a mobile device a consumer device video game console handheld video game device application server storage device a peripheral device such as a switch modem router or in general any type of computing or electronic device.

In various embodiments computer system may be a uniprocessor system including one processor or a multiprocessor system including several processors e.g. two four eight or another suitable number . Processors may be any suitable processor capable of executing instructions. For example in various embodiments processors may be general purpose or embedded processors implementing any of a variety of instruction set architectures ISAs such as the x96 PowerPC SPARC or MIPS ISAs or any other suitable ISA. In multiprocessor systems each of processors may commonly but not necessarily implement the same ISA.

System memory may be configured to store program instructions and or data accessible by processor . In various embodiments system memory may be implemented using any suitable memory technology such as static random access memory SRAM synchronous dynamic RAM SDRAM nonvolatile Flash type memory or any other type of memory. In the illustrated embodiment program instructions and data implementing any of the elements of the embodiments described above may be stored within system memory . In other embodiments program instructions and or data may be received sent or stored upon different types of computer accessible media or on similar media separate from system memory or computer system .

In one embodiment I O interface may be configured to coordinate I O traffic between processor system memory and any peripheral devices in the device including network interface or other peripheral interfaces such as input output devices In some embodiments I O interface may perform any necessary protocol timing or other data transformations to convert data signals from one components e.g. system memory into a format suitable for use by another component e.g. processor . In some embodiments I O interface may include support for devices attached through various types of peripheral buses such as a variant of the Peripheral Component Interconnect PCI bus standard or the Universal Serial Bus USB standard for example. In some embodiments the function of I O interface may be split into two or more separate components such as a north bridge and a south bridge for example. Also in some embodiments some or all of the functionality of I O interface such as an interface to system memory may be incorporated directly into processor .

Network interface may be configured to allow data to be exchanged between computer system and other devices attached to a network e.g. network such as one or more external systems or between nodes of computer system . In various embodiments network may include one or more networks including but not limited to Local Area Networks LANs e.g. an Ethernet or corporate network Wide Area Networks WANs e.g. the Internet wireless data networks some other electronic data network or some combination thereof. In various embodiments network interface may support communication via wired or wireless general data networks such as any suitable type of Ethernet network for example via telecommunications telephony networks such as analog voice networks or digital fiber communications networks via storage area networks such as Fibre Channel SANs or via any other suitable type of network and or protocol.

Input output devices may in some embodiments include one or more display terminals keyboards keypads touch pads scanning devices voice or optical recognition devices or any other devices suitable for entering or accessing data by one or more computer systems . Multiple input output devices may be present in computer system or may be distributed on various nodes of computer system . In some embodiments similar input output devices may be separate from computer system and may interact with one or more nodes of computer system through a wired or wireless connection such as over network interface .

In some embodiments the illustrated computer system may implement any of the methods described above such as the methods illustrated by the flowchart of . In other embodiments different elements and data may be included.

Those skilled in the art will appreciate that computer system is merely illustrative and is not intended to limit the scope of embodiments. In particular the computer system and devices may include any combination of hardware or software that can perform the indicated functions of various embodiments including computers network devices Internet appliances PDAs wireless phones pagers etc. Computer system may also be connected to other devices that are not illustrated or instead may operate as a stand alone system. In addition the functionality provided by the illustrated components may in some embodiments be combined in fewer components or distributed in additional components. Similarly in some embodiments the functionality of some of the illustrated components may not be provided and or other additional functionality may be available.

Those skilled in the art will also appreciate that while various items are illustrated as being stored in memory or on storage while being used these items or portions of them may be transferred between memory and other storage devices for purposes of memory management and data integrity. Alternatively in other embodiments some or all of the software components may execute in memory on another device and communicate with the illustrated computer system via inter computer communication. Some or all of the system components or data structures may also be stored e.g. as instructions or structured data on a computer accessible medium or a portable article to be read by an appropriate drive various examples of which are described above. In some embodiments instructions stored on a computer accessible medium separate from computer system may be transmitted to computer system via transmission media or signals such as electrical electromagnetic or digital signals conveyed via a communication medium such as a network and or a wireless link. Various embodiments may further include receiving sending or storing instructions and or data implemented in accordance with the foregoing description upon a computer accessible medium or via a communication medium. In general a computer accessible medium may include a storage medium or memory medium such as magnetic or optical media e.g. disk or DVD CD ROM volatile or non volatile media such as RAM e.g. SDRAM DDR RDRAM SRAM etc. ROM etc.

The methods described herein may be implemented in software hardware or a combination thereof in different embodiments. In addition the order of methods may be changed and various elements may be added reordered combined omitted modified etc. All examples described herein are presented in a non limiting manner. Various modifications and changes may be made as would be obvious to a person skilled in the art having benefit of this disclosure. Realizations in accordance with embodiments have been described in the context of particular embodiments. These embodiments are meant to be illustrative and not limiting. Many variations modifications additions and improvements are possible. Accordingly plural instances may be provided for components described herein as a single instance. Boundaries between various components operations and data stores are somewhat arbitrary and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of claims that follow. Finally structures and functionality presented as discrete components in the example configurations may be implemented as a combined structure or component. These and other variations modifications additions and improvements may fall within the scope of embodiments as defined in the claims that follow.

While the foregoing is directed to embodiments of the present invention other and further embodiments of the invention may be devised without departing from the basic scope thereof and the scope thereof is determined by the claims that follow.

