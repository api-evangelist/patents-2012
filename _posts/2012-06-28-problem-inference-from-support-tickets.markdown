---

title: Problem inference from support tickets
abstract: The described implementations relate to processing of electronic data. One implementation is manifested as a system that can include an inference engine and at least one processing device configured to execute the inference engine. The inference engine can be configured to perform automated detection of concepts expressed in failure logs that include unstructured data. For example, the inference engine can analyze text of support tickets or diary entries relating to troubleshooting of an electronic network to obtain concepts identifying problems, actions, or activities. The inference engine can also be configured to generate output that reflects the identified concepts, e.g., via a visualization or queryable programming interface.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09229800&OS=09229800&RS=09229800
owner: Microsoft Technology Licensing, LLC
number: 09229800
owner_city: Redmond
owner_country: US
publication_date: 20120628
---
Reliable datacenters and associated networks are useful to various applications including the delivery of cloud services. However mitigating a network failure can be a very challenging task as the root cause of a given network failure is not always apparent to network engineers. Conventionally network engineers may learn from experience or formal training how to deal with particular types of network failures. However even formally trained network engineers with years of experience may take a relatively long time e.g. days to diagnose and repair certain failures.

When working on a particular failure network engineers may use support tickets to track steps taken to diagnose and mitigate problems. However these tickets often include large amounts of relatively unstructured data including free form text device logs automated messages email conversations etc. Some tickets can grow very large and may even have on the order of one million words. In view of these characteristics of support tickets it can be very difficult for even skilled network engineers to discern from support tickets how to troubleshoot associated network failures.

This document relates to processing electronic data. One implementation is manifested as a technique that can include obtaining a plurality of documents. The plurality of documents can have corresponding text. The technique can also include performing phrase extraction on the text to obtain a plurality of phrases that appear in the text and filtering out some phrases from the plurality of phrases based on one or more criteria to obtain a subset of the plurality of phrases. The technique can also include mapping individual phrases from the subset of phrases to classes of an ontology model and storing the individual phrases in a knowledge base. The technique can also include analyzing an individual document by applying the classes of the ontology model to occurrences of the phrases in the individual document to infer at least one concept expressed by the individual document and generating output reflecting the at least one concept expressed by the individual document. The technique can also include performing incremental learning by updating the knowledge base with a new phrase identified in the individual document. At least the phrase extraction can be performed using a computing device.

Another implementation is manifested as one or more computer readable memory devices or storage devices that can include instructions which when executed by one or more processing devices cause the one or more processing devices to perform acts. The acts can include identifying occurrences of phrases in a support ticket wherein the support ticket includes text describing troubleshooting steps taken by one or more network engineers to resolve one or more network problems. The acts can also include inferring from the occurrences one or more concepts expressed by the text of the support ticket.

Another implementation is manifested as a system that can include an inference engine and one or more processing devices configured to execute the inference engine. The inference engine can be configured to perform automated detection of concepts expressed in failure logs. The failure logs can include unstructured data.

The above listed examples are intended to provide a quick reference to aid the reader and are not intended to define the scope of the concepts described herein.

This document relates to analysis of data and more particularly to performing automated inference of concepts expressed in documents. One specific implementation includes performing automated inference of concepts expressed in failure logs such as support tickets for troubleshooting problems on an electronic network. Generally an inference engine can evaluate failure logs to infer concepts associated with individual failures as well as trends across multiple failures. For example the inference engine can infer from an individual support ticket that a particular device component or configuration was a likely cause of a particular failure e.g. a faulty power supply for a load balancer or a faulty configuration of a router. The inference engine can also identify trends across multiple support tickets e.g. a particular brand or model of device may exhibit a relatively high failure rate relative to other comparable devices such as a specific brand and model of load balancer or router relative to other brands models of routers or load balancers. As another example of a trend that can be inferred from multiple support tickets a particular device configuration may be associated with particular types of failures.

As used the term document refers to a collection of one or more words and includes both electronic and physical documents. In the context of an electronic document the term can encompass the combination of human language words along with metadata e.g. as in the case of a word processing document or spreadsheet document. The term failure log refers to a particular type of document that includes information related to a failure of some type e.g. a device failure on a network. Examples of failure logs can include support tickets and associated diary entries network event logs e.g. identifying a link that is down traffic logs e.g. network traffic logged by a device system logs e.g. maintained by a server etc. Generally speaking a failure log can include device generated text such as automated logging messages by the device having the failure or another device e.g. a server computer affected by a failed router . A failure log can also include human generated text e.g. text written by a network engineer describing symptoms of a failure troubleshooting steps actions taken to mitigate a failure etc.

One particular example of a failure log is a network support ticket which is a document that can be used to track a failure in a network setting from when the problem is detected until the problem is resolved. Network support tickets can include certain automated fields that are auto filled by software that generates the tickets e.g. each field can be populated by the software with particular data values that relate to the failure. Network support tickets can also include unstructured data such as freeform text produced by humans communicating about the problem and or logging messages generated by one or more devices. As discussed in more detail below the disclosed implementations can be viewed as leveraging this unstructured data to automatically infer various concepts e.g. individual problems actions and activities inferred from an individual support ticket and trends associated with the network failures inferred using multiple support tickets.

For purposes of explanation consider introductory which shows an exemplary system that is consistent with the disclosed implementations. As shown in system includes a network connecting numerous devices such as a client device and a data center . Generally speaking data center can include one or more server devices that host cloud services e.g. cloud based applications provided by the data center to client devices such as client device . These cloud based applications can provide various services to client device such as storage email streaming media and personal information management e.g. calendars contacts etc. . Client device may access cloud services via one or more cloud interfaces e.g. one or more application programming interfaces APIs that make calls to the cloud services.

Network can also connect a server operations center to data center . Server operations center can generally include one or more server devices configured to monitor data center for network problems. For example the server operations center may allow network engineers to monitor status of the data center for various failures e.g. a failed router improper device configuration slow response times etc. As the network engineers address various problems in the data center they may generate support tickets . Individual support tickets may generally include information about problems within the data center from multiple sources. For example as mentioned above support tickets can include both structured data populated by the server devices as well as unstructured data such as text written by individual network engineers and or automated messages logged by one or more devices.

Network can also connect an inference server which may host an inference engine and a knowledge base . Generally speaking the inference engine can be configured to process support tickets to infer various concepts from the support tickets. For example the inference engine may identify concepts such as problems actions or activities expressed in the support tickets. The inference engine may also identify trends over multiple support tickets such as failures associated with a particular model of device or a particular device configuration. The inference engine may process the support tickets using information stored in the knowledge base as discussed in more detail below. The knowledge base may be implemented using various storage devices e.g. optical drives flash drives magnetic hard drives etc.

Network can also connect a client device which can include an inference engine interface configured to communicate with inference server . For example the inference engine interface may comprise a browser that receives and displays visualizations reflecting the inferences made by the inference server and client device may display the visualizations to a user of client device . Thus client device may be used by a network engineer to help troubleshoot various problems that may arise in data center . Inference engine interface may also be configured to operate on other e.g. non visual forms of output by querying the inference server using one or more application programming interfaces to obtain output from the inference engine.

Note that the various devices shown in system are illustrated with respect to logical roles that can be performed by the devices in operation of system . However the geographical locations of the various devices are not necessarily reflected by system . For example client device can be located on site at server operations center e.g. a desktop computer used by a network engineer during day to day operation of the server operations center. Alternatively as shown in client device can be embodied as a mobile device e.g. laptop tablet mobile phone etc. that can be used by a network engineer remotely from the server operations center. More generally the various functionality discussed herein with respect to each device shown in can be performed by other device classes e.g. server functionality can be performed by mobile devices and so on.

Also note that server operations center and data center are illustrated as multiple server devices whereas inference server and client devices and are illustrated as individual computing devices. This reflects one particular implementation and other implementations may provide inference server functionality and or client device functionality as discussed herein via multiple devices. Likewise server operations center and or data center functionality as discussed herein may be performed by individual devices.

Further note that in practice there may be additional instances of each computing device mentioned herein e.g. additional inference servers server operations centers client devices and data centers. As discussed in more detail below each of the computing device s and or shown in can include one or more processing devices such as computer processors executing instructions stored on one or more computer readable storage media such as volatile or non volatile memories optical disks hard drives flash drives etc.

Memory can be a volatile storage device such as a random access memory RAM or a non volatile storage device such as FLASH memory. Although not shown in inference server can also include various input output devices e.g. a keyboard a mouse a display a printer microphone for voice input touch screen for gesture or touch input etc. Furthermore the analysis server can include one or more non volatile storage devices such as a hard disc drive HDD optical compact disc digital video disc drive tape drive etc. for example knowledge base . Generally speaking any data processed by the inference server can be stored in memory and can also be committed to non volatile storage.

Memory of inference server can include various components that implement certain processing described herein. For example memory can include inference engine which can include various modules components. For example inference engine can include a phrase extraction component a phrase filtering component a mapping component an analysis component an output component and a learning component . The individual components can perform certain processing disclosed herein e.g. example methods and associated processing discussed in more detail below. For example the individual components can perform certain processing on support tickets obtained from server operations center to infer concepts from the support tickets. The inferred concepts can be used to identify root causes of individual problems and or problem trends within the data center .

Phrase extraction component can be configured to identify various phrases e.g. n grams that may appear in the support tickets. For example the phrases can be repeated patterns of language e.g. n grams of length one or more that appear in multiple instances in the support tickets. As discussed in more detail below the phrase extraction component can apply a two pass phrase extraction technique to obtain a frequency estimate of n grams in the support tickets.

Phrase filtering component can be configured to filter the extracted phrases to identify a subset of the phrases that have relatively significant meaning e.g. contribute to understanding the central topic of text. Generally speaking the phrase filtering component can apply filtering to the phrases identified by the phrase extraction component to eliminate certain phrases e.g. using the frequencies estimated by phrase extraction component or other criteria. The phrase filtering component can also apply information theoretic measures to perform filtering based on computed importance of the n grams.

Mapping component can be configured to map individual phrases to an ontology model. For example an individual such as a domain expert or network engineer may classify individual phrases into individual ontology classes and subclasses. The ontology model can be constructed to relate the various ontology classes and subclasses according to defined interactions and can be stored in the knowledge base .

Analysis component can be configured to identify concepts expressed in the support tickets. Analysis component can identify these concepts by identifying what problems were seen in individual tickets what actions were taken to mitigate the problems other related activities by the network engineers etc. For example using various grammar patterns concepts can be extracted based on the ontology classes mentioned above.

Output component can be configured to generate various forms of output that represent the processing by the analysis component . For example the output component can generate various visualizations e.g. graphical interfaces that reflect concepts inferred by the analysis component. The graphical interfaces may reflect root causes of problems of individual support tickets and or trends across multiple support tickets. As discussed in more detail below the output component may provide menu selections to filter by device type device model e.g. multiple models per device type or other fields to identify particular traits of devices that are obtained from the support tickets. The output component may also implement a queryable application programming interface API to allow other components e.g. third party software to access the output.

Generally speaking components and can include instructions stored in memory that can be read and executed by processing device . Components and can also be stored in non volatile storage and retrieved to memory to implement the processing described herein. As used herein the term computer readable media can include transitory and non transitory instructions. In contrast the term computer readable storage media excludes transitory instances and includes volatile or non volatile hardware memory devices and or hardware storage devices such as those discussed above with respect to memory and or other suitable storage technologies e.g. optical disk hard drive flash drive etc.

In some alternative configurations the techniques disclosed herein can be implemented on one or more computers that use a system on a chip SOC type design. In such a case functionality provided by various components e.g. and can be integrated on a single SOC or multiple coupled SOCs. In one such example individual computers can include shared resources and dedicated resources. One or more interfaces can facilitate communication between the shared resources and the dedicated resources. As the name implies dedicated resources can be thought of as including individual portions that are dedicated to achieving specific functionalities. Shared resources can be storage processing devices etc. that can be used by multiple functionalities.

Considering both support ticket and diary entry note that certain information is reflected in relatively less structured formats than other information. For example ticket data section of support ticket includes numerous specific fields that can be represented directly e.g. by corresponding database entries that compactly represent the knowledge included therein. In contrast ticket title section includes relatively less structured information including the sentence Load balancer failed and power supply was replaced. Similarly the diary description section includes relatively unstructured information e.g. freeform textual communication between multiple individuals.

To a trained human e.g. a network engineer the sentence Load balancer failed and power supply was replaced may convey a great deal of meaning about the root cause of the incident e.g. the power supply for a load balancer was found to be faulty and replacing the power supply resolved the incident. However this knowledge is represented in a relatively unstructured format. Generally speaking inference engine can process relatively unstructured data such as ticket title section and or diary description section to infer various concepts e.g. problems such as a load balancer failed actions such as replacing the power supply and activities such as reseating the power supply to diagnose the problem.

As mentioned diary description section also includes relatively unstructured information e.g. an instant messaging conversation between two individuals. Again a trained human such as a network engineer can read the instant messaging conversation and recognize that certain activities were performed e.g. J. Doe reseated two power supplies and powered the load balancer back up but the problem was not resolved until one of the power supplies was replaced. Using the disclosed techniques inference engine can process relatively unstructured data such as diary description section to infer certain problems actions and activities expressed in diary entry .

Method can begin at block where documents are obtained. For example inference server can obtain one or more of the support tickets from server operations center . In some implementations the inference server can also obtain one or more associated diary entries. For example the inference server can obtain one or more existing tickets that have already been opened and possibly closed .

Next at block phrase extraction can be performed on text in the documents to obtain a plurality of phrases that appear in the text of the documents. Block can also include determining a relative frequency with which each phrase appears in the documents. For example phrases that appear multiple times in the existing tickets and or diary entries can be extracted at block . Additional detail relating to block is discussed further below in the section entitled PHRASE EXTRACTION. 

Next at block the extracted phrases can be filtered using one or more criteria. For example a first filter can filter the phrases based on phrase length and or frequency a second filter can filter the phrases using part of speech patterns and a third filter can filter the phrases using one or more information theoretic measures. After the filtering the remaining phrases e.g. a subset of the extracted phrases can be processed further. Additional detail relating to block is discussed further below in the section entitled PHRASE FILTERING. 

Next at block individual phrases from the remaining subset can be mapped to classes of an ontology model. For example the ontology model can include various classes e.g. actions such as cleaning a fiber cable and or entities such as devices etc. The ontology model can be used to express relationships between the different classes which can in turn provide for concise expression of pertinent information included in the documents. The mapping of the phrases to the ontology model can be stored in the knowledge base . Additional detail relating to block is discussed further below in the section entitled PHRASE MAPPING. 

Next at block a document can be analyzed using the ontology model. For example an individual e.g. recent or new support ticket can be identified for analysis and processed to identify occurrences of the phrases that are included in the ontology. The ontological classes assigned to the phrases can be applied to the occurrences of the phrases to identify certain concepts reflected by the individual support ticket. Additional detail relating to block is discussed further below in the section entitled DOCUMENT ANALYSIS. 

Next at block output can be generated that reflects concepts identified by the analysis of the document. For example the output can be provided via one or more application programming interfaces APIs that allow querying for concepts on individual support tickets or for trends across multiple support tickets. As another example the output can be in the form of a visualization such as a concept tree representing the individual support ticket that can be created from the identified concepts. As another example of an output visualization a trait cloud can be generated that shows trends across multiple support tickets. Additional details relating to concept trees and trait clouds are discussed further below in the section entitled INFERENCE VISUALIZATION. 

As part of block the output can be provided to a user e.g. by displaying the output as a visualization or sending the output to another computing device for display or processing. For example output can be sent from inference server to a computing device such as client device e.g. either in the server operations center or at a remote location. Block can also include updating output responsive to various user selections e.g. different types of devices date ranges etc.

Next at block incremental learning can be performed. For example a network engineer may determine that the individual support ticket that has been analyzed has a particular phrase that while important to the central meaning of the individual support ticket is not reflected by the concept tree for the individual support ticket. This can occur for example when the particular new phrase has not yet been mapped to the ontology. Thus at block the network engineer can manually choose to map the new phrase to a class of the ontology. As a specific example the network engineer may choose to map the phrase memory module to the ontology model as an entity e.g. a physical replaceable entity. As new phrases are mapped to the ontology model they can be added to knowledge base for use in processing subsequent support tickets. For example when the phrase memory module is added to the knowledge base this phrase is now available for analyzing subsequent tickets that include the phrase memory module. 

Method can thus be viewed as an iterative procedure that provides for constant refinement of the ontology model over time. One way to conceptualize method is in various stages. First a knowledge building phase generally blocks can be applied to an existing group of support tickets that serve to bootstrap inference engine by building a knowledge base that maps a subset of phrases in the existing support tickets to classes of an ontology model. Next an operational phase generally blocks and can analyze additional support tickets e.g. new or incoming support tickets and generate outputs such as visualizations that reflect various concepts expressed in the new or incoming support tickets. Finally an incremental learning phase generally block can map additional phrases discovered in the new or incoming support tickets to the classes of the ontology model and these new mappings can be added to the knowledge base. Thus the inference engine can be refined over time by continually updating the knowledge base as new phrases are identified by users for inclusion in the knowledge base.

Method can begin at block where documents such as support tickets are obtained for processing by the method. For example support ticket and associated diary entry may be obtained for further processing. As mentioned above support tickets and associated diary entries can be obtained from server operations center .

Method can continue at phrase extraction stage . Generally phrase extraction stage can perform redundant phrase extraction on documents via word level compression. For example stage can identify what phrases appear in the support tickets and or diary entries. Stage can include blocks and discussed below.

Block can tokenize text in the documents into sentences. For example the support tickets and or diary entries can be parsed into individual unigrams e.g. words and delimiters such as periods can be used to identify sentences.

Block can use a compression algorithm to operate on the tokenized documents. For example some implementations may use a Lempel Ziv Welch LZW compression algorithm to build a dictionary of phrases e.g. sequences of unigrams from the tokenized support tickets and or diary entries although other compression algorithms may be used as well. Generally the compression algorithm can compress the text of the tokenized support tickets and or diary entries and output compressed text while building a dictionary of each phrase that appears in the compressed text. Note that some implementations can discard the compressed output e.g. a compressed binary while retaining the dictionary. The dictionary can include some or all sequences of phrases of any length e.g. applying the compression to just the phrase back with findings from diary entry can result in dictionary entries including back with findings back with with findings back with findings and so on for the entire diary entry and or support ticket. Note that some implementations may include only repeated phrases in the dictionary. Relatively infrequent phrases may use longer encoding since they will have less of an impact on the size of the compressed output.

Next at frequency estimation stage the documents and the dictionary are used to perform frequency estimation of redundant phrases. In some implementations the frequency estimation is performed at stage using a pattern matching technique such as the Aho Corasick algorithm. Frequency estimation stage can include blocks and discussed below.

Block can include constructing an ordered representation of the dictionary . For example a finite state automaton e.g. a Trie can be used to represent the dictionary with an empty string at the root unigrams at layer 1 bigrams at layer 2 and so on. Generally speaking the Trie can be used to look up individual phrases in the dictionary.

Block can include performing frequency estimation for individual phrases in the support tickets and or diary entries. For example the frequency estimation can be performed in a single pass over the support tickets and or diary entries to match phrases n grams in the support tickets and or diary entries to entries in the Trie. The output of block can be a frequency with which each entry in the dictionary appeared in the support tickets diary entries e.g. power 672 entries power supply 438 entries power supply unit 362 entries and so on.

Generally some phrases that appear frequently in the existing support tickets can be useful for inferring meaning from the support tickets e.g. the phrases power supply unit and load balancer can be central to understanding the meaning of a support ticket that indicates a power supply unit failed for a load balancer. In contrast other phrases may also appear frequently in the support tickets but these phrases may be less useful for understanding the meaning of the support tickets.

The following are several examples of frequently occurring phrases that may generally be less useful for inferring meaning from support tickets. Network engineers may use predetermined formats or templates to communicate with each other and thus phrases in these templates tend to appear relatively often in the support tickets diary entries. As another example devices may generate log messages during operation and these log messages may be appended to support tickets that are in the process of being resolved. As a third example email conversations may be appended to support tickets and it can be difficult to reconstruct the order of messages in an email conversation because email conversations tend to include many asynchronous replies. Also note that some email conversations may include emails that do not include any specific information on resolving the problem but rather discuss ancillary communication issues such as which individuals have been contacted which vendors or teams or involved etc. Also reply all messages can tend to duplicate prior conversations that may not have specific information related to problem resolution. Thus generally speaking phrases that appear in templates device logs and email conversations will tend to appear frequently in the support tickets but often do not contribute to the central meaning of the support tickets. One goal of method is to filter out such phrases automatically while retaining other phrases that do tend to contribute to the central meaning of the support tickets.

At block of method repeated phrases and their frequencies can be obtained e.g. from phrase extraction component . As mentioned the phrases can be n grams of varying lengths e.g. unigrams bigrams etc. up to arbitrary lengths. As discussed above the frequencies can be obtained from the output of a pattern matching algorithm such as Aho Corasick.

Next blocks generally can be viewed as individual filtering techniques to remove individual phrases from the set obtained from the phrase extraction component . Each block from blocks can remove some of the phrases based on particular criteria.

For example block can perform length and or frequency filtering to discard certain phrases. For example as a general proposition relatively long length phrases tend to be noisy n grams due to long repeated phrases e.g. automated phrases in the support tickets and or diary entries. As noted previously emails logging messages and frequently repeated templates can include such long phrases that appear often in the support tickets and or diary entries. Such phrases tend to be relative long in length e.g. many automated emails may include phrases such as if you need assistance outside of these hours please call the toll free number . . . and so on. Such phrases tend to be of relatively little value in inferring concepts from a support ticket or diary entry. Thus block may filter out relatively long repeated phrases e.g. over length irrespective of how many times the phrases appear in the support tickets and or diary entries.

Block can also filter out relatively low length phrases that appear with relatively low frequency e.g. less than a certain percentage e.g. 1 or number of times e.g. 

Next block can perform part of speech filtering to identify phrases remaining after block that match predetermined part of speech patterns. For example block can implement Justeson Katz collocation filtering to identify part of speech patterns such as . Adjective Noun . . Noun Noun . . Adjective Adjective Noun . . Adjective Noun Noun . . Noun Adjective Noun . . Noun Noun Noun . . Noun Preposition Noun . etc. Note the . can represent using a regular expression to match zero or more instances of any character other than a newline either to the left or right of the part of speech patterns. Phrases that do not match any of the part of speech patterns can be discarded as part of block .

Block can perform filtering based on one or more information theoretic measures. For example residual inverse document frequency RIDF and mutual information MI measures can be computed for each phrase that is remaining after block . For convenience of terminology the remaining phrases can fall into three groups e.g. 1 n grams that are filtered out by block 2 n grams that tend to be suited for building a domain specific dictionary and 3 n grams that tend to be useful for inference processing discussed in more detail below.

With respect to group 2 n grams with relatively low negative MI scores e.g. below a threshold of 10 tend not to be found in standard dictionaries e.g. they include technical words or terms used in unconventional ways. When n grams with strongly negative MI have relatively high RIDF scores e.g. above a threshold of 4 these terms can be used to build a domain specific dictionary. In other words phrases with strong negative MI and high RIDF tend to be domain specific phrases that do not appear in conventional dictionaries and can be added at block to a domain specific dictionary such as knowledge base for further processing.

With respect to group 3 block can also identify phrases with high RIDF e.g. above a threshold of 4 and positive MI e.g. above a threshold 0 to identify particularly relevant phrases from the support tickets and or diary entries. These phrases tend to include words that are part of a general vocabulary e.g. words found in a standard English dictionary . Such phrases tend to be included in sentences and can be useful for concept inference subject object differentiation and or other subsequent processing discussed herein. The phrases can be sorted by RIDF and then by MI for subsequent review by an individual such as a domain expert network engineer etc.

Block can output a filtered and sorted subset of phrases e.g. the n grams that are remaining after the three filtering blocks . Note that the number of phrases output at block can be a function of the particular thresholds that are used for MI and or RIDF at block . Thus for relatively small datasets i.e. relatively few tickets and or diary entries and or high availability of domain experts these thresholds can be set to allow a relatively higher percentage of phrases to pass through block . Conversely for relatively large datasets and or relatively low availability of domain experts these thresholds can be set to allow a relatively lower percentage of phrases to pass through block .

Generally speaking the following explanation can be viewed as a detailed explanation of block of method where phrases are mapped to classes or subclasses of an ontology model and added to knowledge base . More specifically some or all of the subset of phrases that are unfiltered by method can be assigned to classes of the ontology model by an individual such as a network engineer domain expert etc. Note that the phrases left over after the filtering may be more likely to be relatively important in the sense that they are more likely to have some meaning that is pertinent to understanding the support tickets diary entries from which they were extracted.

However certain remaining unfiltered phrases may not be suited for use within the ontology model because they may lack specificity relative to other phrases. For example consider the unigram slot and the bigram memory slot. The phrase slot may not be as useful as the phrase memory slot for the purposes of understanding a ticket because the phrase memory slot is more specific and provides more context for semantic interpretation.

For this reason in some implementations the individual can manually select from the subset of phrases left after filtering to identify phrases that are useful for mapping to the ontology model. For example the individual may choose to assign the bigram memory slot to a particular class and or subclass of the ontology model while dropping the unigram slot. Likewise the domain expert may choose to assign the bigram enterprise cloud to an ontology class and or subclass while dropping the unigram cloud. Note that this process is reasonable for manual efforts by an individual because the filtering by method can result in a manageable number of phrases in the subset. In some implementations it is this subset from which the individual chooses the phrases that go into the knowledge base .

Note also that the arrows show relationships between individual classes consistent with the ontology model. These relationships represent valid interactions between the ontology classes. As discussed more below an action taken on an entity is a valid interaction according to ontology model as shown by the arrow connecting action node to entity node . Conversely an action taken on a condition would not be a valid interaction according to the ontology model since there is no corresponding arrow illustrating such a relationship between the action node and the condition node .

Here the entity class generally includes phrases that represent a thing that exists. The entity class can include subclasses such as ReplaceableEntity VirtualEntity and or MaintenanceEntity. For example a ReplaceableEntity is a tangible object that can be created destroyed or replaced e.g. a device such as load balancer power supply unit router etc. A VirtualEntity is an intangible object that can be created destroyed or replaced e.g. a software configuration a port channel a kernel etc. A MaintenanceEntity is a tangible object that can act upon other entities e.g. a field technician network engineer etc.

The action class includes phrases that represent a behavior that is taken on an entity e.g. a power supply unit is an entity that can be replaced by a replacement action. Subclasses of the action class can include MaintenanceActions that interact with an entity and or alter the state of the entity such as checking a device cleaning a fiber cable deploying validating verifying etc. The actions class can also include a PhysicalActions subclass which includes creating replacing or destroying an entity e.g. replacing a device e.g. an RMA or return merchandise authorization decommissioning a device etc.

The condition class includes phrases that describe the state of an entity e.g. a power supply unit or software configuration can have a faulty condition. A condition can further be classified as a ProblemCondition subclass or a MaintenanceCondition subclass. A ProblemCondition describes a condition known to have a negative effect such as inoperative a reboot loop etc. A MaintenanceCondition describes a condition that requires maintenance e.g. a breakfix condition indicating a deployed entity needs maintenance.

The incident class includes phrases that occur upon an entity and can alter the state of an entity e.g. a power spike incident could cause a power supply unit to transition from an acceptable condition to a faulty condition. The incident class can include a FalsePositivelncident subclass e.g. a state known not to cause problems such as a false positive or false alert. The incident class can also include an Errorincident subclass e.g. a state known to cause a problem such as an error or exception.

The quantity class includes phrases that count or describe the quantity of an entity e.g. two power supply units an action e.g. replaced power supply unit twice or an incident e.g. second power spike . Some implementations may use LowQuantity MediumQuantity and HighQuantity subclasses to distinguish phrases that reflect relatively low moderate and high quantities depending upon context. A quantity can also be used to reflect severity e.g. LowQuantity can correspond to a relatively minor incident whereas HighQuantity can correspond to a relatively major incident.

The negation class includes phrases that negate another phrase e.g. did not replace power supply unit power supply unit is not faulty. The negation class can include a SyntheticNegations subclass that uses verbs or nouns to negate a condition incident or action e.g. phrases such as absence of declined denied etc. The negations class can also include AnalyticNegations subclass that uses not to negate a condition incident or action.

The sentiment class includes phrases that add strength or weakness to an action or incident phrase e.g. I confirm that the power supply unit is faulty. The sentiments class can include a PositiveSentiment subclass e.g. phrases such as confirm or affirmative. The sentiment class can also include a NeutralSentiment subclass e.g. phrases such as not sure. The sentiment class can also include a NegativeSentiment subclass e.g. phrases such as likely or potential. Note that phases in the NegativeSentiment class can be phrases that may not explicitly indicate negative sentiment but rather tend to be used by individuals when speculating about the nature of a network problem.

As mentioned above an individual can assign some or all of the phrases remaining after method to one or more classes or subclasses of ontology model . As the individual selects the class subclass assignments the mapping component can receive input from the individual and update the knowledge base with the assignments for use when processing other support tickets. Thus the knowledge base includes a mapping of various phrases in the support tickets to classes and or subclasses of the ontology model.

In further implementations the domain expert may also provide a custom built synonym dictionary to collapse various synonymous terms into a standardized term. For example the terms psu psus power supply and power supply unit may each mean power supply unit and a dictionary entry can map each of these terms to power supply unit. Likewise the terms gbic for Gigabit interface converter pic for port interface card fpc for flexible PIC concentrator and nic for network interface card may each be various ways of expressing network card and the dictionary can therefore provide this mapping as well. As a further example flash sdram for synchronous dynamic random access memory dram for dynamic random access memory and ddram for double data rate random access memory can each be various ways of expressing memory module and another dictionary entry can express this relationship.

Note that the first example dictionary entry power supply unit reflects direct synonyms whereas the second and third entries network card and memory module map individual types of network cards memory modules to a more general term. In some implementations the mapping component can process the existing support tickets diary entries to identify each instance of the synonyms generalizations and replace them with a corresponding standardized term.

Also note that in some implementations the individual may identify various phrases as antonyms or include certain phrases on a whitelist or blacklist. In some implementations the whitelist can identify phrases that the individual defines for inclusion in knowledge base irrespective of their occurrences in the support tickets or diary entries. Furthermore in some implementations the phrases on the whitelist are not mapped to synonyms or more general terms. Phrases on the blacklist may be excluded from the knowledge base irrespective of their occurrences in the support tickets or diary entries.

As mentioned above once the selected phrases have been classified using ontology model they are reflected in the knowledge base . Method can generally speaking use the knowledge base to infer certain concepts expressed in the support tickets diary entries. For example during the operational phase generally blocks and of method an incoming support ticket and or diary entry can be analyzed by the analysis component using method as follows.

Method can begin at block where occurrences of phrases are identified in a document. For example each occurrence of the phrases in the knowledge base in the individual support ticket diary entries being analyzed can be identified by the analysis component. In other words each phrase in the individual support ticket diary entries being analyzed can be matched to corresponding phrases in the knowledge base.

Next at block the identified occurrences of the phrases can be tagged according to corresponding classes of ontology model . For example text such as 

Next at block the tagged text of the documents is processed to identify valid interactions according to the ontology model. Each sentence with at least one valid interaction can be identified. For example some of the sentences in the individual support ticket and or diary entry may have valid interactions and some may not have any valid interactions.

Next at block the sentences of the document that do not have any valid interactions can be filtered out. Thus sentences in a support ticket diary entry that do not include at least one valid interaction can be filtered out from further processing when analyzing the support ticket diary entry leaving a subset of sentences for further processing.

Next at block concepts can be inferred from the remaining subset of sentences. For example the concepts can include Problems Actions and Activities. Problems can identify a particular network entity e.g. router link power supply unit and associated state condition or symptoms e.g. crash defective reboot as identified by a network engineer e.g. bad memory line card failure crash of a load balancer when the network engineer began troubleshooting. Activities can identify steps performed on the network entity during troubleshooting of a problem e.g. pinging a network device checking and cleaning cables verifying device configuration or other triage analysis steps performed before escalating the support ticket to higher levels of support. Actions can identify resolution actions performed on a network entity to mitigate a problem e.g. rebooting a switch replacing a line card reseating a power supply initiating an RMA for a load balancer etc. Note that the Action concept is distinguishable from the action class of the ontology e.g. the Action concept is an inferred meaning obtained from a given support ticket diary entry whereas the action class is a way of grouping certain phrases in the knowledge base .

In some implementations the analysis component can be configured to match certain grammar patterns to the tagged sentences to identify the concepts. For example the analysis component can use different grammar patterns for each of the types of concepts e.g. Problems Actions or Activities. The grammar patterns can be built using the classes or subclasses of the ontology model as follows.

In some implementations the analysis component can confine the analysis to unstructured data such as freeform text in the support tickets and or diary entries. Other implementations however may use structured data from the support tickets diary entries and or other data for analysis purposes. As one example structured data from a support ticket e.g. Problem Type from ticket data section or from a diary entry e.g. diary title can be appended to the freeform text and analyzed as discussed above.

In other implementations ticket titles problem types problem subtypes etc. can be extracted and used directly as concepts e.g Problem concepts from problem type subtype fields . Further implementations may validate concepts extracted in this manner relative to problems inferred from unstructured data. For example some implementations may compare concepts directly extracted from structured data to concepts inferred from freeform text to see if the concepts match.

Data external to the support tickets diary entries can also be leveraged during the analysis. For example data describing maintenance events configuration files for various devices network event logs system logs traffic logs etc. can also be processed during the analysis. For example a given faulty device configuration inferred by the analysis component can be correlated to a device configuration file.

During the operational phase of method a user may want to understand the significance of a particular support ticket and or associated diary entries. For example the user may be a network engineer reviewing open support tickets to try to resolve them as quickly as possible. As mentioned above block of method can include generating output that reflects the concepts expressed in an individual support ticket. For example inference engine can generate a visualization that reflects various concepts inferred from the support ticket. This can help the user quickly grasp the significance of the support ticket diary entry even when the support ticket diary entry is very long and difficult to read quickly.

Concept tree includes a root node at depth 1 that represents the particular ticket. Root node is connected to a problems node an actions node and an activities node . Tracing a path from problems node through node representing faulty and node representing device indicates that the support ticket was due to a particular problem e.g. a faulty device. Note that this path through nodes and fits the Problems heuristic pattern identified above.

Similarly tracing a path from actions node through node representing RMA and node representing load balancer indicates that a particular action was taken e.g. the load balancer was RMA d. Note that this path fits the Actions heuristic pattern identified above. Likewise tracing a path from activities node through node representing check and node representing device indicates that the network engineer checked the device as one activity associated with the support ticket. Likewise tracing a path from activities node through node representing clean and node representing fiber indicates that the network engineer cleaned the fiber as a second activity associated with the support ticket. Again note that these paths and fit the Activity heuristic pattern mentioned above.

Also note that the nodes within a given depth of concept tree can have certain commonalities. At depth 1 the root node represents the individual support ticket as a whole. At depth 2 the nodes in the concept tree represent Problems Activities or Actions. At depth 3 the nodes in the concept tree represent actions or conditions describing an entity. At depth 4 the nodes represent individual entities that are connected to the parent action condition at depth 3.

Concept tree can be presented as part of a graphical user interface that allows a user to interact with the concept tree. For example users can move the mouse cursor over a given node and the immediate parent and sibling nodes can be highlighted. This can be useful for example for helping users quickly grasp the significance of relatively complicated concept trees.

During the operational phase of method the user may also want to understand various trends across multiple support tickets. For example a network engineer may want to know what major problems exist in devices made by a particular manufacturer what problems dominate load balancer failures what actions are taken when a device becomes operational which manufacturer is causing the most maintenance costs or other similar questions. As mentioned above block of method can include generating output e.g. a visualization that reflects trends across multiple support tickets.

Interface can also include a device type field to specify a particular device type e.g. load balancer router etc. as well as a device model field to specify a particular device model e.g. models of load balancers routers etc. having different hardware specifications by model number . Property field can specify a particular device property e.g. software firmware version failing component etc . Trait field can specify a particular trait of the support tickets e.g. concepts such as problems actions or activities identified by the analysis component . Note that fields and are shown as text fields and fields are shown as drop down menus however there are many alternative implementations e.g. drop down calendars for dates radio buttons for traits etc. .

Interface also includes a trait cloud portion . Individual trait clouds in trait cloud portion are represented as circles sized relative to the number of instances individual concepts have been extracted from the group of support tickets. Generally speaking individual clouds on the left side of trait cloud represent a concept e.g. Problems Actions or Activities whereas individual clouds on the right side of trait cloud represent entities involved with the concept in the text of the analyzed documents. As shown in trait cloud portion identifies two actions RMA and reboot that appear in the group of support tickets represented by the trait cloud with RMA being somewhat more prevalent. This indicates that generally speaking load balancers were associated with more RMA actions than reboot actions during the specified time period. The specific entities upon which these actions were taken are reflected on the right side of trait cloud portion e.g. power supply units fans memory modules etc. Thus a user viewing interface can see that load balancer problems have generally been fixed with RMA or reboot actions and that power supply units fans memory modules etc have been the subject of these actions. Such a user might conclude therefore that power supply units cause a substantial number of problems with load balancers and often need to be RMA d to fix load balancer issues. The user might also conclude that memory module issues while still prevalent are less likely to be the cause of any particular failure associated with a load balancer.

Using the described implementations automated processing of support tickets can be leveraged to infer concepts expressed by the support tickets. Furthermore the automated processing can also be used to infer various trends across multiple support tickets. The concepts and trends can be output via various interfaces e.g. visualizations programming interfaces etc. The outputs can be used to enhance the ability of network engineers to readily diagnose network problems for new or incoming support tickets by leveraging an ontology model that classifies phrases from previous support tickets. Newly identified phrases in the new or incoming support tickets can be added to the ontology model over time to iteratively learn the new phrases for use as future support tickets arrive for processing.

Although techniques methods devices systems etc. pertaining to the above implementations are described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described. Rather the specific features and acts are disclosed as exemplary forms of implementing the claimed methods devices systems etc.

