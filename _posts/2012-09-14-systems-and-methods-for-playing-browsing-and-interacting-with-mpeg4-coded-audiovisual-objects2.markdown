---

title: Systems and methods for playing, browsing and interacting with MPEG-4 coded audio-visual objects
abstract: A number of novel configurations for MPEG-4 playback, browsing and user interaction are disclosed. MPEG-4 playback systems are not simple extensions of MPEG-2 playback systems, but, due to object based nature of MPEG-4, present new opportunities and challenges in synchronized management of independent coded objects as well as scene composition and presentation. Therefore, these configurations allow significantly new and enhanced multimedia services and systems. In addition, MPEG-4 aims for an advanced functionality, called Adaptive Audio Visual Session (AAVS) or MPEG-J. Adaptive Audio Visual Session (AAVS) (i.e., MPEG-AAVS, MPEG-Java or MPEG-J) requires, in addition to the definition of configurations, a definition of an application programming interface (API) and its organization into Java packages. Also disclosed are concepts leading to definition of such a framework.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09641897&OS=09641897&RS=09641897
owner: AT&T Intellectual Property II, L.P.
number: 09641897
owner_city: Atlanta
owner_country: US
publication_date: 20120914
---
The present application is a continuation of U.S. patent application Ser. No. 11 849 399 filed Sep. 4 2007 now U.S. Pat. No. 7 281 200 which is a continuation of U.S. patent application Ser. No. 10 662 831 filed Sep. 15 2003 which is a continuation of U.S. patent application Ser. No. 09 236 462 filed Jan. 25 1999 now U.S. Pat. No. 6 654 931 which claims the benefit of U.S. Provisional Application No. 60 072 923 filed on Jan. 28 1998 and which are incorporated herein by reference in their entirety.

This invention relates to systems and methods for decoding and presenting encoded audio and visual data. More specifically this invention relates to systems and methods for playing browsing and interacting with MPEG 4 coded scenes including one or more audio and or visual objects.

MPEG 1 and MPEG 2 encoding and decoding standards are frame based encoding and decoding techniques. That is in MPEG 1 and MPEG 2 audio visual data such as a video recording is organized into separate frames where each frame is a complete image. In MPEG 1 and MPEG 2 the human recognizable objects within each image are not distinguished from each other in encoding and decoding the data defining the image. Thus while each frame can be treated independently from any other frame each frame is itself a unitary element of the audio visual data. is an exemplary embodiment of an MPEG 2 playback system.

The Virtual Reality Modeling Language or VRML is a computer language that is used to create text descriptions defining three dimensional synthetic images. That is VRML is used to define the three dimensional objects that appear in a synthetic e.g. computer generated image including shapes and sizes of the objects the appearance of each object including material color shading and texture and the location of each objects including position and orientation. The objects are generally synthetic e.g. computer generated objects. VRML is also used to define the lighting in the synthetic image including the type and position of one or more light sources.

MPEG 4 is a new audio visual data encoding and decoding standard. In particular MPEG 4 in contrast to MPEG 1 and MPEG 2 is not a frame based encoding and decoding technique. MPEG 4 is an object based encoding and decoding technique. Objects can be synthetic or natural objects and further can be audio video or graphics objects. In MPEG 4 each frame is decomposed into a plurality of different objects and a scene description graph that indicates where each object appears in that frame. The object based nature of MPEG 4 along with requirements of flexible composition and user interactivity requires using some scene description mechanism.

Each object resides in its own video object plane that defines at least that object s shape motion opaqueness and color including surface texture. The scene description graph defines the spatial location of each object within the bounds of the frame. The scene description graph also defines the position of each object within the depth of the frame i.e. which objects are in front of which other objects.

These features allow new kinds of flexibilities not offered by simply decoding and presenting a video frame as in MPEG 2. MPEG 4 players can be flexible and the systems and methods for playing browsing and interacting with MPEG 4 coded scenes of this invention allows users the ability to browse two dimensional 2D or three dimensional 3D MPEG 4 scenes typically composed from synthetic and natural media elements. Furthermore the systems and methods for playing browsing and interacting with MPEG 4 coded scenes of this invention allow users the ability to interact with and customize such scenes. This invention further describes systems and methods for constructing MPEG 4 based multimedia players and browsers to facilitate these flexibilities such as programmatic control via JavaScript and Java and to enhance the user s experience while at the same time remaining compatible with the MPEG 4 standards.

These and other features and advantages of this invention are described in or are apparent from the following detailed description of the systems and methods according to this invention.

The following detailed description sets forth exemplary embodiments of a number of novel configurations of a system for playing back browsing and allowing user interaction with MPEG 4 encoded objects. As indicated above the MPEG 4 playback browsing and user interaction systems of this invention are not simple extensions of MPEG 2 playback systems. Rather due to the object based nature of MPEG 4 encoding decoding and presenting audio visual data encoded using MPEG 4 presents novel opportunities and challenges in synchronized management of the independently encoded objects that form a scene. The novel configurations of the system for playing back browsing and allowing user interaction with MPEG 4 encoded objects of this invention allow significantly new and enhanced multimedia services and systems.

In addition MPEG 4 also allows for an advanced functionality called adaptive audio visual session AAVS which requires in addition to definitions of configurations definitions of Application Programming Interfaces APIs and their organization into Java packages. It should be appreciated that the term AAVS has been changed to MPEG AAVS MPEG Java or MPEG J hereafter referred to as MPEG J in later versions of the MPEG 4 standard. The novel configurations of the system for playing back browsing and allowing user interaction with MPEG 4 encoded objects of this invention provide a definition of such a framework. Finally the novel configurations of the system for playing back browsing and allowing user interaction with MPEG 4 encoded objects of this invention provide for development tools and environments for AAVS i.e. MPEG J work.

It may appear that a MPEG 4 playback system can be derived simply as an extension of known MPEG 2 playback systems such as that shown in . However the MPEG 4 standard due to its object based structure is very different from the MPEG 2 standard. Thus the MPEG 4 standard presents new paradigms for synchronizing individual audio visual objects and for their composition. In turn the MPEG 4 standard also offers new opportunities in the sense of flexibilities such as browsing and user interaction with MPEG 4 coded scenes. The following detailed description of this invention outlines a number of MPEG 4 audio visual object browser systems of this invention that allow for various degrees of flexibilities for MPEG 4 playback browsing and scene control and interaction.

As indicated above shows a basic MPEG 2 playback system . The primary components of this basic MPEG 2 playback system include a program transport demultiplexer and depacketizer one or more MPEG 2 audio video decoders and a display process . The MPEG 2 coded data received from a storage device or from a node of a distributed network is fed to the program transport demultiplexer and depacketizer . Appropriate pieces of the demultiplexed and depacketized MPEG 2 coded data are fed to the respective ones of the one or more MPEG 2 audio video decoders . The decoded data output by the one or more MPEG 2 audio video decoders is sent to the display process .

U.S. patent application Ser. No. 09 055 929 filed Apr. 7 1998 incorporated herein by reference discloses various embodiments of a basic MPEG 4 encoder and a basic MPEG 4 decoder. However the MPEG 4 decoder disclosed in the 929 application does not permit any user interaction with the MPEG 4 coded data received by that MPEG decoder. U.S. patent application Ser. No. 09 055 934 filed Apr. 7 1998 incorporated herein by reference discloses various embodiments of various application programming interfaces APIs that provide decoding functionality and authoring capabilities to an MPEG 4 audio visual object playback system such as the MPEG 4 audio visual object playback systems according to this invention.

It should be appreciated that the MPEG 4 media decoder can include any known or later developed decoder. It should also be appreciated that the BIFS decoder is based on VRML but extends VRML beyond the static three dimensional objects normally supported by VRML. The BIFS decoder thus allows for two dimensional scenes video objects and audio objects that are normally not supported by conventional VRML. The incorporated 934 application describes various decoders that can be incorporated into either of the MPEG 4 media decoder and or the BIFS decoder .

As shown in the MPEG 4 coded data received from a storage device or from a node of a distributed network is fed to the delivery media integration framework and flexible demultiplexing layer DMIF and FlexDemux . The delivery media integration framework and flexible demultiplexing layer DMIF and FlexDemux outputs one or more flexmux protocol data units FlexMux PDUs . The FlexMux PDUs are input by the access layer which outputs unformatted access layer protocol data units AL PDUs . The unformatted access layer protocol data units AL PDUs are input by the MPEG 4 media decoder . In particular for each media type including but not limited to coded video streams coded facial and or animation streams coded audio streams or coded speech streams the unformatted access layer protocol data units AL PDUs are provided to corresponding one or ones of the video decoder the facial object animation decoder the image texture decoder the audio decoder the structured audio decoder or any other provided decoder. Also any unformatted access layer protocol data units AL PDUs that corresponding to scene description representation are input to the BIFS decoder .

 MPEG 4 Integrated Intermedia Format IIF Basic Specification A. Basso et al. ISO IEC SC29 WG11 MPEG98 M2978 International Standards Organization February 1998 MPEG 4 Integrated Intermedia Format IIF Extension Specification A. Basso et al ISO IEC SC29 WG11 MPEG98 M2979 International Standards Organization February 1998 U.S. patent application Ser. No. 09 055 933 filed Apr. 7 1998 and U.S. patent application Ser. No. 09 067 015 filed Apr. 28 1998 each incorporated herein by reference in its entirety outline various embodiments of the file organization and components of an MPEG 4 file that can be input as the received MPEG 4 coded data received by the MPEG 4 audio visual object playback system .

The output of the media decoders of the MPEG 4 media decoder as well as the output of the BIFS decoder is provided to the compositor and renderer . The compositor and renderer can also respond to minimal user provided control signals such as those provided by a selection device such as a mouse a trackball a touch pad and the like. The minimal user provided control signals will thus include mouse clicks and the like. The output of compositor and renderer is the scene for presentation and is provided to the display process for display to the user.

The MPEG 4 Native Iml is written in C or C contains a number of Native Iml methods and is shown as the native code that can be called from the Java methods . Furthermore the native Iml code methods can also call the Java methods . It should be appreciated that the MPEG 4 Native Iml is fully compiled to provide maximal computational efficiency. However this renders the compiled MPEG 4 Native Iml highly platform specific. In contrast the Java and JavaScript languages and are at most only partially compiled and are interpreted upon execution. This provides allows the same Java and JavaScript methods and to be used flexibly across many different platforms. However this flexibility requires more computational resources when executing the Java and JavaScript methods and .

In the linkage of the BIFS scene description graph interpreter to the media decoders compositor and renderer of is clarified. Depending on the contents of the scene description graph a number of different nodes of the BIFS scene description graph interpreter are invoked. For example in the exemplary embodiment shown in the contents of the scene description graph require invoking a VideoObject2D node an AudioSource node and an ImageTexture node of the BIFS scene description graph interpreter . The VideoObject2D node the AudioSource node and the Image texture node correspondingly deal with the video audio and image content by attaching the appropriate decoders of the media decoders compositor and renderer . The appropriate decoders of the media decoders compositor and renderer decode the corresponding encoded portions of the MPEG 4 encoded data and output the decoded data to a scene compositor of the media decoders compositor and renderer . It should be appreciated that the VideoObject2D node has been renamed as the MovieTexture node in later versions of the MPEG 4 standard. However the functionality of the MovieTexture node is substantially the same as that of the VideoObject2D node .

The BIFS scene description graph interpreter also includes the corresponding programmer interfaces between the nodes of the BIFS scene description graph interpreter and the corresponding decoders of the media decoders compositor and renderer . For example in the exemplary embodiment shown in the programmer interfaces of the BIFS scene description graph interpreter include a video object programmer interface i.e. a movie texture programmer interface an audio object programmer interface and an image object programmer interface .

Similarly in the exemplary embodiment shown in the appropriate decoders of the media decoders compositor and renderer include a video object decoder an audio object decoder and an image object decoder .

There has been much discussion during creation of the MPEG 4 standard for the need for a Script node in the scene description graph interpreter to support scripting such as the scripting used in VRML. is a functional block diagram illustrating the data flow between the components of a second exemplary embodiment of an MPEG 4 audio visual object browser system of this invention. In particular in this second exemplary embodiment of an MPEG 4 audio visual object browser system the MPEG 4 audio visual object browser system supports user local interaction through scripting.

As shown in the second exemplary embodiment of the MPEG 4 audio visual object browser system includes an MPEG audio visual objects demultiplexer and BIFS browser a BIFS scene description graph interpreter and a media decoders compositor and renderer . Basic user interaction directly with the MPEG audio visual objects demultiplexer and BIFS browser is possible through a user controlled input device such as a mouse a trackball a touch pad or the like. Additional user interaction i.e. local interaction can occur in the form of behavior programming via use of scripting. This local user interaction through scripting can be used to interact with either or both of the MPEG audio visual objects demultiplexer and BIFS browser and the BIFS scene description graph interpreter .

The BIFS scene description graph interpreter is capable of understanding the scene and invoking the needed media decoders of the media decoders compositor and renderer for interpreting the content. The decoded audio visual objects are composited and presented by the media decoders compositor and renderer to the MPEG audio visual objects demultiplexer and BIFS browser .

As shown in the linkage of the BIFS scene description graph interpreter to the media decoders compositor and renderer of clarified. Depending on the contents of the scene description graph a number of different nodes of the BIFS scene description graph interpreter are invoked. For example in the second exemplary embodiment shown in the contents of the scene description graph require invoking a VideoObject2D node i.e. a MovieTexture node an AudioSource node an ImageTexture node and a Proto node of the BIFS scene description graph interpreter . The VideoObject2D node the AudioSource node and the Image texture node correspondingly deal with the video audio and image content by attaching the appropriate decoders of the media decoders compositor and renderer . The appropriate decoders of the media decoders compositor and renderer decode the corresponding encoded portions of the MPEG 4 encoded data and output the decoded data to a scene compositor of the media decoders compositor and renderer .

The BIFS scene description graph interpreter also includes the corresponding programmer interfaces between the nodes of the BIFS scene description graph interpreter and the corresponding decoders of the media decoders compositor and renderer . For example in the exemplary embodiment shown in the programmer interfaces of the BIFS scene description graph interpreter include a video object programmer interface i.e. a movie texture programmer interface an audio object programmer interface and an image object programmer interface a native proto programmer interface . Similarly in the exemplary embodiment shown in the appropriate decoders of the media decoders compositor and renderer include a video object decoder an audio object decoder an image object decoder and a native proto implementation .

As described above with respect to including the proto node allows the scene description graph interpreter to use canned nodes as an extension to the nodes officially supported by the scene description graph interpreter . The new canned nodes can be defined by a DEF statement in the scene description graph and used by a USE statement in the scene description graph. The canned nodes can modify some aspects of a known node by programming its behavior. However it should be appreciated that the Proto node is not yet officially included in the BIFS standard for the scene description graph interpreter .

Additionally as shown in the nodes of the BIFS scene description graph interpreter also include a Script node while the programmer interfaces of the BIFS scene description graph interpreter also include an interpreter programmer interface . Similarly the appropriate decoders of the media decoders compositor and renderer include a JavaScript interpreter and a Java interpreter . The Script node offers local flexibility for behavior programming and can be considered as closely related to adaptive audio visual session AAVS or MPEG J if it is not pure adaptive audio visual session.

The programmer interfaces of the BIFS scene description graph interpreter also include a scripting interface . The scripting interface inputs the interpreted script commands from the JavaScript interpreter and or the Java interpreter . The output of the scripting interface is connected to the MPEG audio visual objects demultiplexer and BIFS browser and the scene compositor .

However it should be appreciated that the Script node like the Proto nodes and is not yet officially included in the BIFS standard for the scene description graph interpreter . However the Script node is supported by VRML. It should further be appreciated that adaptive audio visual session AAVS or MPEG J can be thought of as related to the VRML External Authoring Interface EAI .

As shown in the third exemplary embodiment of the MPEG 4 audio visual object browser system includes an MPEG audio visual objects demultiplexer and BIFS browser a BIFS scene description graph interpreter and a media decoders compositor and renderer . Local user interaction can occur in the form of behavior programming via use of scripting. This local user interaction through scripting can be used to interact with either or both of the MPEG audio visual objects demultiplexer and BIFS browser and the BIFS scene description graph interpreter However in contrast to the first and second exemplary embodiments of the MPEG 4 audio visual object browser systems and described above in the third exemplary embodiment of the MPEG 4 audio visual object browser system basic user interaction is not directly with the MPEG audio visual objects demultiplexer and BIFS browser . Rather basic user interaction is directly with an adaptive audio visual session AAVS module that sits between the basic user interaction and the MPEG audio visual objects demultiplexer and BIFS browser . Nonetheless basic user interaction remains possible through a user controlled input device such as a mouse a trackball a touch pad or the like.

The BIFS scene description graph interpreter is capable of understanding the scene and invoking the needed media decoders of the media decoders compositor and renderer for interpreting the content. The decoded audio visual objects are composited and presented by the media decoders compositor and renderer to the MPEG audio visual objects demultiplexer and BIFS browser .

The adaptive audio visual session AAVS module is invoked as an applet for controlling the MPEG audio visual objects demultiplexer and BIFS browser . The adaptive audio visual session AAVS module potentially supports a high degree of user interaction with the scene. This interaction is referred to as user global interaction rather than basic user interaction. The adaptive audio visual session AAVS module passes both the control information and data such as for example scene updates to the MPEG audio visual objects demultiplexer and BIFS browser . However it should be appreciated that the adaptive audio visual session AAVS module is conceptualized to just an additional layer interfacing the user with the scene.

The external applet interface includes an AAVS external interface a browser specific binding and a browser programmer interface . It should be appreciated that the browser specific binding can be implemented using Netscape Corp. s LiveConnect technology or Microsoft Corp. s ActiveX technology.

A user interacts with the MPEG 4 audio visual object browser system through user global interaction to program or reprogram the behavior of the scene using external scripting. This script is an AAVS external script . This AAVS external script is provided to the MPEG audio visual objects demultiplexer and BIFS browser through the AAVS external interface the browser specific binding and the browser programmer interface . It should be appreciated that this assumes that the MPEG audio visual objects demultiplexer and BIFS browser exists within the context of a browser. Further the AAVS external script is assumed to employ Java.

As clarified earlier AAVS i.e. MPEG J is an applet based system. In fact it should be possible to use AAVS i.e. MPEG J as an application rather than applet. clarifies this view. In particular is a functional block diagram illustrating the data flow between the components of a fourth exemplary embodiment of an MPEG 4 audio visual object browser system of this invention.

As shown in the fourth exemplary embodiment of the MPEG 4 audio visual object browser system includes an MPEG audio visual objects demultiplexer a BIFS Java 3D scene description graph interpreter a media decoders compositor and renderer and an adaptive audio visual session AAVS and presenter module that sits between the user global interaction and the MPEG audiovisual objects demultiplexer . Local user interaction can occur in the form of behavior programming via use of scripting. This local user interaction through scripting can be used to interact with the BIFS Java 3D scene description graph interpreter .

It should be appreciated that the MPEG 4 audio visual object browser system uses control from an application rather than an applet. For generality the MPEG 4 audio visual object browser system is shown to operate without a browser although the MPEG 4 audio visual object browser system supports both the user local interaction and the user global interaction. Further for generality not only a BIFS scene graph but also other forms of scene graph such as for example Java3D can be controlled.

The BIFS Java 3D scene description graph interpreter is capable of understanding the scene and invoking the needed media decoders of the media decoders compositor and renderer for interpreting the content. The decoded audio visual objects are composited and presented by the media decoders compositor and renderer to the adaptive audio visual session AAVS and presenter module .

The adaptive audio visual session AAVS and presenter module potentially supports a high degree of user global interaction with the scene. The adaptive audio visual session AAVS and presenter module rather than passing the control information and data such as for example scene updates to the MPEG audio visual objects demultiplexer and BIFS browser can choose to directly operate on the control information and data.

The above outlined MPEG 4 audio visual object browser systems allow the user to easily manipulate an MPEG 4 encoded scene. As described above in MPEG 4 each frame is decomposed into a plurality of different objects and a scene description graph that indicates where each object appears in that frame. Each object resides in its own video object plane that defines at least that object s shape motion opaqueness and color including surface texture. The scene description graph defines the spatial location of each object within the bounds of the frame. The scene description graph also defines the position of each object within the depth of the frame i.e. which objects are in front of which other objects.

Accordingly because each object is encoded independently of all other objects and the objects are related to each other to represent the scene only by the scene description graph any object or the scene description graph can be manipulated by the user to alter the resulting scene. That is by altering the scene description graph the user can modify the position of any object within the scene add new objects to the scene or delete currently included objects from the scene. In particular the added new objects can be synthetic objects created using VRML or BIFS.

For example the position of an object can be altered by modifying the scene description graph to change the defined location of that object. Thus when the compositor collects all of the decoded objects and composes them into the scene the compositor determines where to place each object in the scene according to the location defined in the scene description graph. By changing the location defined in the scene description graph for an object the position of that object in the composed scene changes.

Similarly the scene description graph can also be modified to change the relative position of an object relative to the depth of the scene. That is the relative depth of an object in the scene can be changed so that it appears behind rather that in front of another object or vise versa. Additionally in a three dimensional scene changing the z axis position of an object defined in the scene description graph assuming the x axis and y axis positions define the left to right and top to bottom position of the object modifies the position of the object in the scene.

Moreover an object can be added to the scene by adding a description of that object to the scene description graph. If the object is a synthetic object the scene description graph will be modified to fully describe the appearance of that object according to VRML or BIFS and to define the location of that object within the scene. If the object is an MPEG 4 encoded object the scene description graph will be modified to identify the VOP containing that object and to define the location of that object within the scene.

If an object is to be deleted from the scene the scene description graph will be modified to remove the nodes relating to that object from the scene description graph. This can be accomplished by either completely removing any mention of that object from the scene description graph or by preventing the MPEG 4 audio visual object browser systems from processing the nodes relating to that object. This can be accomplished by adding a code to each node referencing that object such as by adding a remark code to each such node. Accordingly because the nodes relating to that node are either removed or are not processed that object is not added to the scene when the compositor composes the scene from the scene description graph.

It should further be appreciated that these modifications to the scene description graph can be performed by the user either using user global interaction or user local interaction as described above. Modification of the scene using user global interaction can be programmatic such as via Java applets or JavaScript scripts to modify the scene description graph.

In modifying the scene using user local interaction a user can interactively manipulate a scene displayed using the MPEG 4 audio visual object browser systems . In particular the user can select an object within the displayed scene and drag that object to a different location within the scene. The user can also copy the selected object to a clipboard or cut it from the scene on to the clipboard. The user can then paste that object into a different scene or can paste a copy of that object into the first scene. The user can also paste any other object in the clipboard into the first scene at any arbitrary location.

Recently AAVS i.e. MPEG J in MPEG has generated much attention due to the promise of the enhanced user interactivity that is needed by many anticipated applications. Requirements have been collected for some applications while many other applications are envisaged but their requirements generally understood are difficult to crystallize. The work on collection of requirements should continue to help maintain the focus for AAVS i.e. MPEG J work. However the technology aspect of AAVS i.e. MPEG J has had similar uncertainty as well since some of the AAVS i.e. MPEG J work was started fairly early on in MPEG when even a fixed solution did not exist. Eventually BIFS reached a mature stage and MPEG has the needed key functionality. The AAVS i.e. MPEG J technology work was subsequently modified several times. In MPEG 4 the promise of AAVS i.e. MPEG J technology can finally be fulfilled but a clear direction is needed which minimizes overlap and adds increased value to systems tools offering.

There has been some debate whether a BIFS based solution needs to be made more flexible for example by incorporating Script nodes as described above. There has also been debate on which language may be appropriate for scripting. AAVS i.e. MPEG J and the relationship with scripting has been mentioned but not clearly understood mainly because many of the discussions take place in context of VRML which although very important for MPEG 4 is also inherently different at times.

As mentioned earlier in MPEG 4 there has been much debate about the similarities and differences between scripting and AAVS i.e. MPEG J and regarding the language to be used for scripting. To settle the later debate both JavaScript and Java have their roles for scripting JavaScript is needed for quick and dirty solutions while Java is needed for programming more complex behaviors. Further JavaScript is directly interpreted i.e. JavaScript does not need to be compiled has a relatively smaller footprint is relatively less secure is without graphics library support other than that supported via HTML and is without networking support other than that supported via the web browser. In comparison Java is compiled and then interpreted is more secure has graphics and networking support has a bigger footprint and is multimedia capable through JavaMediaFramework . Therefore it should be possible to support both the scripting languages in different profiles or at different levels of the same profile.

On the issue of scripting versus external AAVS i.e. MPEG J interface although there is potentially some area of overlap between the two they also differ in capabilities they introduce. While scripting can easily allow local behavior programming or control within a scene external interface allows global behavior programming or control. Thus again they offer somewhat different functionalities. This issue should also be handled via appropriate definition of systems profiles.

We now discuss the approach MPEG AAVS i.e. MPEG J could follow in defining its API. The rationale of the proposal is based on the apparent commonality of the AAVS i.e. MPEG J work with that of Liquid Reality. Liquid Reality is a platform independent VRML toolkit written in Java. In fact Liquid Reality is a set of developer tools. Liquid Reality supports the functionalities of parsing rendering and authoring VRML compliant files as well as interactively manipulating the scene graph. Liquid Reality is not just one API but a collection of several APIs and includes an API to manipulate VRML nodes level APIs to do 3D math rendering and more. Liquid Reality includes 11 Java packages dnx.geom dnx.ice dnx.util dnx.awtutil dnx.lr dnx.lr.field dnx.lr.node dnx.lr.app vrml vrml.field and vrml.node .

The dnx.geom package contains code for 3D math such as storing manipulating 2 3 and 4 components vector and matrices. The functionality of this package is also included in Java3D.

The dnx.ice package provides wrapper classes allowing access to low level rendering package such as OpenGL Direct3D and ICE.

The dnx.lr.node package contains a class for each of the nodes of VRML and 10 other Liquid Reality specific classes.

The dnx.lr.field package contains classes for each field type defined in VRML. That is the dnx.lr.field package is similar to but a superset of the vrml.field package.

Perhaps the MPEG 4 AAVS i.e. MPEG J packages can be patterned after Liquid Reality. However MPEG 4 may not need some of the packages of Liquid Reality and may alternatively add some new packages. This is a similar to the relationship of BIFS with VRML. As a more concrete example the following packages could be used for the AAVS i.e. MPEG J mpg.geom mpg.rend mpg.util mpg.aavs mpg.aavs. field mpg.aavs.node mpg.aavs.app mpg.aavs.ui and mpg.aavs.dev .

The mpg.geom package could be formed by taking the appropriate subset of Java3D and would support code for 3D math.

The mpg.rend package could provide wrapper classes allowing access to low level rendering package such as OpenGL and Direct3D or alternatively could provide access to Iml which in turn could call low level rendering packages.

The mpg.aavs.app package could contain classes that provide framework for MPEG based applet or application. Interfaces to JavaMediaFramework may also be necessary.

The mpg.aavs.dev package could contain classes that provide framework for device and networking interface 

In parallel to decision regarding the AAVS i.e. MPEG J API the development environment in MPEG may also need to be standardized to facilitate speedy development of AAVS i.e. MPEG J . In particular decisions are needed regarding the versions of the following.

Some of current AAVS i.e. MPEG J work by Sun is based on JDK1.1.4. A bug fixed newer version called JDK1.1.5 is also available as well as a significantly updated JDK2.0 which is in the Beta testing stage. In addition to JDK a software development environment such as for example Symanec Visual Cafe may also need to be standardized.

It should be appreciated that the MPEG 4 audio visual object browser systems and are preferably implemented on a programmed general purpose computer. However the MPEG 4 audio visual object browser systems and can also be implemented on a special purpose computer a programmed microprocessor or microcontroller and peripheral integrated circuit elements an ASIC or other integrated circuit a digital signal processor a hardwired electronic or logic circuit such as a discrete element circuit a programmable logic device such as a PLD PLA FPGA or PAL or the like. In general any device capable of implementing a finite state machine can be used to implement the MPEG 4 audio visual object browser systems and .

Accordingly it should be understood that each of elements of the MPEG 4 audio visual object browser systems and shown in can be implemented as portions of a suitably programmed general purpose computer. Alternatively each of elements of the MPEG 4 audio visual object browser systems and shown in can be implemented as physically distinct hardware circuits within an ASIC or using a FPGA a PDL a PLA or a PAL or using discrete logic elements or discrete circuit elements. The particular form each of the elements of the MPEG 4 audio visual object browser systems and shown in will take is a design choice and will be obvious and predicable to those skilled in the art.

While this invention has been described in conjunction with the specific embodiments outlined above it is evident that many alternatives modifications and variations will be apparent to those skilled in the art. Accordingly the preferred embodiments of the invention as set forth above are intended to be illustrative not limiting. Various changes may be made without departing from the spirit and scope of the invention.

