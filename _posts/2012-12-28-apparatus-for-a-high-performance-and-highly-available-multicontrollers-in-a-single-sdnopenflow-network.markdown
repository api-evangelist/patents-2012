---

title: Apparatus for a high performance and highly available multi-controllers in a single SDN/OpenFlow network
abstract: A method for distributing routing instructions to a plurality of nodes within an OpenFlow Software Defined Network (SDN) using a logically centralized multi-controller that comprises a plurality of controllers, wherein the method comprises receiving a plurality of incoming data packets, storing a plurality of current flow tables, queuing the incoming data packets, wherein the incoming data packets are queued based on the order received, processing the incoming data packets based on the order the incoming data packets were queued, wherein the incoming data packets that are first to be queued are the first to be processed, generating a plurality of flow tables by processing the incoming data packets, and transmitting the flow tables to the plurality of nodes when the flow tables have not been previously generated.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09065768&OS=09065768&RS=09065768
owner: Futurewei Technologies, Inc.
number: 09065768
owner_city: Plano
owner_country: US
publication_date: 20121228
---
Modern communication and data networks comprise network nodes such as routers switches bridges and other devices that transport data through the network. Over the years the telecommunication industry has made significant improvements to the network nodes to support an increasing number of protocols and specifications standardized by the Internet Engineering Task Force IETF . Creating and coupling the complex network nodes to form networks that support and implement the various IETF standards e.g. virtual private networks requirements has inadvertently cause modern networks to become labyrinth like and difficult to manage. As a result vendors and third party operators struggled to continually customize optimize and improve the performance of the interwoven web of network nodes.

Software defined networking SDN is an emerging network technology that addresses customization and optimization concerns within the convoluted networks. SDN simplifies modern networks by decoupling the data forwarding capability i.e. the data plane from routing resource and other management functionality i.e. the control plane previously performed in the distributed network nodes. To streamline the control plane architecture SDN implements a logically centralized system which may include one or more centralized controllers to manage the control plane. For example software in multiple servers that are separate from the network switching nodes may manage a network s control plane functions while the network switching nodes within the network are configured to govern the data plane. Currently the OpenFlow protocol was created to support SDN in the industry and provides communication between the data plane and the control plane. The OpenFlow protocol enables operators or other third parties to separate the control plane configurations and the data plane configurations for a variety of network switching nodes. Therefore the adoption of the Open Flow protocol with SDN enables vendors and third parties to easily create new network management and control applications that optimize and increase network performance.

Although the combination of SDN and the OpenFlow enables network customization and optimization the designation of a centralized system to manage all control plane functions inherently produces drawbacks for a network. For instance an SDN OpenFlow network is more susceptible to failures within the centralized systems which may cause computing and processing bottlenecks. Furthermore a centralized system may lack the dynamic ability to easily adapt and manage varying traffic volume across different network locations. Network scalability also becomes a concern when costly upgrades are necessary for the centralize system to manage newly provisioned network nodes and end services e.g. hosts .

To alleviate these concerns one design option has been to use a distributed control system that utilizes a cluster of central controllers. The cluster of central controllers may function as a logically centralize system without being physically centralized. However use of distributed control systems encounter load balancing and path rerouting issues caused by centralized controller failures e.g. single point of failures . Dedicated and inflexible control channels between every controller and all network nodes may be necessary to implement the control plane. Furthermore as the number of centralized controllers increase synchronization and other process delays between the centralized controllers and the network nodes may affect the managing capacity of the logically centralized system. Thus new technology is necessary to address the problems of implementing SDN and OpenFlow in large intricate networks.

In one example embodiment the disclosure includes an apparatus for receiving a plurality of data packets and providing routing instructions to a plurality of nodes using the plurality of data packets comprising an active controller ring comprising at least one active controller wherein the active controller ring is configured to receive a first data packet receive a second data packet process the first data packet with a first active controller while receiving the second data packet with a second active controller compute a plurality of generated flow tables based on processing the first data packet and the second data packet and transmit the plurality of generated flow tables via a plurality of outgoing packets wherein the generated flow tables dictate the routing behavior of the nodes when the outgoing packets are received by the nodes.

In yet another example embodiment the disclosure includes a logically centralized multi controller for providing routing instructions to a plurality of nodes via a control plane wherein the logically centralized multi controller comprises a plurality of input ports wherein each of the input ports are configured to receive an incoming frame a plurality of output ports wherein each of the output ports are configured to transmit an outgoing frame a controller in queue coupled to the plurality of input ports wherein the controller in queue is configured to temporarily store the incoming frames based on the order received an active controller ring coupled to the controller in queue wherein the active controller ring comprises a plurality of active controllers wherein the active controllers are configured to receive the incoming frames from the controller in queue and process the incoming frames to produce a plurality of routing instructions a dispatcher coupled to the active controller ring and to the plurality of output ports wherein the dispatcher is configured to transmit the plurality of routing instructions received from the active controllers via the output ports and a scheduler coupled to the active controller ring and to the controller in queue wherein the scheduler is configured to modify the number of active controllers within the active controller ring based on the number of incoming frames temporarily stored in the controller in queue.

In yet another example embodiment the disclosure includes a method for distributing routing instructions to a plurality of nodes within an OpenFlow Software Defined Network SDN using a logically centralized multi controller that comprises a plurality of controllers wherein the method comprises receiving a plurality of incoming data packets storing a plurality of current flow tables queuing the incoming data packets wherein the incoming data packets are queued based on the order received processing the incoming data packets based on the order the incoming data packets were queued wherein the incoming data packets that are first to be queued are the first to be processed generating a plurality of flow tables by processing the incoming data packets transmitting the flow tables to the plurality of nodes when the flow tables have not been previously generated modifying the number of controllers to process the incoming data packets and reusing a set of active controllers to continually process the incoming data packets.

These and other features will be more clearly understood from the following detailed description taken in conjunction with the accompanying drawings and claims.

It should be understood at the outset that although an illustrative implementation of one or more embodiments are provided below the disclosed systems and or methods may be implemented using any number of techniques whether currently known or in existence. The disclosure should in no way be limited to the illustrative implementations drawings and techniques described below including the exemplary designs and implementations illustrated and described herein but may be modified within the scope of the appended claims along with their full scope of equivalents.

Disclosed herein are a method apparatus and system to implement a centralized multi controller that manages the control plane of a network. The centralized multi controller may employ an active controller ring where a scheduler component may dynamically modify the number of active controllers within the active controller ring. Additional active controllers may be added to the controller ring from a pool of available controllers. Each active controller within the active controller ring may be configured as a master state a slave state or as equals. The centralized multi controller may designate one active controller as the current master controller and the remaining active controllers within the active controller ring as slave controllers. The current master controller may listen to a controller in queue for an incoming data packet received from a network node. When the current master controller receives the incoming data packet the current master controller may then switch to the slave state and proceeds to process the incoming data packet. Afterwards a slave controller may switch to the master state and listen for a new data packet from the shared controller in queue. Active controllers that received an incoming data packet may process the data packet to generate flow tables that may be used to update the flow tables in one or more network nodes.

The centralized multi controller may comprise a plurality of active controllers that are not shown in and may be configured to oversee the routing and other management functions e.g. the control plane of network . The centralized multi controller may manage the control plane for a plurality of network nodes via the control path . In one embodiment the centralized multi controller may be an SDN controller configured to use the OpenFlow protocol to access the forwarding plane of network nodes . Persons of ordinary skill in the art are aware that the centralized multi controller may use other Open Application Programming Interface API services to access the forwarding plane of network nodes 

The network nodes may include switching devices e.g. switches and routers that are compliant with OpenFlow protocols as defined in Rev. 1.2 for the OpenFlow specification of the OpenFlow Organization published December 2011 which is incorporated herein as if reproduced by its entirety. Network nodes may also include switching devices that are compliant with previous revisions of the OpenFlow protocols e.g. Rev. 1.0 and Rev. 1.1 . Network nodes may be coupled to each other via the data path while network nodes may be coupled to the centralized multi controller via the control path . Both the data path and the control path may be direct links such as fiber optic links electrical links and wireless links or indirect links such as logical connections or physical links with intervening nodes not shown in . Data path may comprise a single link a series of parallel links a plurality of interconnected network nodes not shown in or various combinations thereof to manage the forwarding plane within network . Control path may be substantially similar to data path except that the control path may be used to manage the control plane. also illustrates that network nodes may be coupled to a plurality of end nodes via the data paths . The end nodes may include hosts servers storage devices or other types of end devices that may originate data into or receive data from network .

In one embodiment method may receive a data packet from a network node and or an end node because of a new data flow. Using as an example network may create a new data flow from end node to end node using network nodes and . To obtain routing information for the new data flow the centralized multi controller may receive the first data packet transmitted by end node for the new flow via network node . Alternatively the centralized multi controller may receive data packets from network nodes and or end nodes when other changes occur within the network such as modifications to the network state e.g. node and link failures network application user priority and or quality of service QoS .

Method proceeds to block after receiving the incoming data packet. At block the incoming data packet may be placed into a controller in queue to temporarily store the incoming data packets. Block may then forward stored data packets out of the queue based on the order the controller in queue received the incoming data packets. Using as an example the controller in queue may receive a data packet from network node prior to receiving a second data packet from network node . Hence one embodiment of block may be a first in first out FIFO queue that forwards the data packet from node before forwarding the second data packet from node . Once the data packet is pushed to the front or top of the controller in queue stack method continues to block and forwards the incoming data packet from the controller in queue to the current master controller within an active controller ring. The current master controller may be the only active controller within the active controller ring to receive a data packet from the controller in queue. The active controller ring and active controller configurations will be discussed in more detail in .

The method may then proceed to block to determine whether the next active controller in the active controller ring is available to receive another incoming data packet at block . If the next active controller is not available the method may proceed to block to wait for the next active controller to become available. Method may continue to wait and check whether the next active controller is available to receive a message by looping between block and block . Once the next active controller in the active controller ring is available to receive a message the method may proceed to block .

Block may transition the current master controller to a slave controller after receiving a data packet from the controller in queue. When the current master controller switches to a slave controller the controller may no longer receive data packets from the in controller queue. Subsequently method continues to block and transitions the next active controller in the active controller ring from a slave controller to the current master controller. At this point method may then return back to block and reprocess blocks and using different active controllers. For example after the next active controller switches from a slave controller to the current master controller the active controller may now be ready to receive a data packet from the controller in queue. After the second active controller receives a data packet the active controller may transition back to a slave controller. A third active controller in the active controller ring may subsequently assume the current master controller. Method may continue to loop through blocks and in order to receive data packets from the controller in queue.

From block method continues to block to determine whether flow tables need to be updated for the network nodes. The flow tables generated in block may be compared with the existing flow tables for the network nodes. For example method may avoid producing unnecessary traffic by first determining whether an existing flow table has already been generated using a prior received data packet. If the existing flow table includes the same values as the generated flow table in block generated then the flow tables do not need to be updated. In one embodiment the flow tables may be multi flow table that are associated with more than one data flow within a network. Using as an example network may comprise a data flow from end node to end node and a second data flow from end node to end node . Block may generate one multi flow table that is associated with the two data flows. The flow tables may need to be updated when packets need to be rerouted flow entries exceed a pre specified time and become obsolete and or when other changes occur in the network environment.

If the flow tables do not need to be updated method may proceed to block . At block method may configure the slave controller to be ready to receive an additional data packet and thus may be available to transition to the current master controller. The method may end after block . Returning to block if block determines that the flow tables need to be updated method then moves to block to distribute the flow tables to network nodes within the corresponding data flow. Afterwards method may subsequently end.

Once the maximum and minimum pre specified limits are set method may then proceed to block to determine whether the controller in queue length exceeds the maximum specified limit. If the controller in queue length exceeds the maximum pre specified limit the method moves to block . At block one or more controllers from the available controller pool may be assigned to the active controller ring. After adding one or more controllers to the active control ring method may loop back to block to set a new maximum and minimum pre specified limits. Continuing with the prior example the maximum pre specified limit may be about 1 000 data packets within the controller in queue. When the length of the controller in queue reaches above 1 000 data packets e.g. about 1 300 data packets one or more available controllers e.g. two available controllers may be added to the active controller ring to process the data packets. The available controllers may be added one at a time to the active controller ring with a short delay between the additions of each available controller. Afterwards a new maximum e.g. about 1 400 data packets and minimum e.g. about 1 200 data packets pre specified limits may be set based on seven active controllers within the active controller ring. Method may also be applied when the utilization of current active controllers are used to increase or decrease the number of active controllers when incoming traffic flow fluctuates.

Returning back to block if the controller in queue length does not exceed the maximum pre specified limit then method moves to block . At block method determines whether the controller in queue length falls below the minimum pre specified limit. When the controller in queue length does not fall below the minimum pre specified limit then method proceeds to block to continue checking that the controller in queue does not exceed the maximum pre specified limit. However if the pre specified queue length is less than the minimum pre specified limit method then proceeds to block . At block one or more controllers may be removed from the active control ring and placed back into the pool for available controllers. Using the same example as discussed for block the minimum pre specified limit may be about 800 data packets within the controller in queue. When the length of the controller in queue is less than 800 data packets e.g. about 500 data packets one or more active controller e.g. two active controller may be removed from the active controller ring and allocated to the pool of available controllers. Similar to block block may then loop back to block to set a new maximum e.g. about 600 data packets and minimum e.g. about 400 data packets pre specified limits for three active controllers. Method may also be applied when the utilization of current active controllers are used to increase or decrease the number of active controllers when the incoming traffic flow fluctuates.

The centralized multi controller may receive one or more incoming data packets on one or more input ports. The incoming data packets may comprise header information that may be used by the active controllers to generate flow tables that produce routing information for network nodes within a network. In one embodiment the incoming data packet may be any Open Systems Interconnection OSI layer 2 or layer 3 encoded data packet such as an Ethernet frame or an IP packet. The header information may be encoded using a variety of protocols such as MPLS Asynchronous Transfer Mode ATM Ethernet Internet Protocol version 4 IPv4 Internet Protocol version 6 IPv6 etc. The header information may include a destination address encoded in an Ethernet frame multi protocol label switching MPLS IP packet or other similar types of data signals. Another embodiment of the header information may include a label used in various protocols such as a MPLS or data link connection identifier label DLCI in frame relay protocols.

After receiving the incoming data packets the centralized multi controller may place the incoming data packets into a Controller In queue component . The Controller In queue component may temporarily store the incoming data packets that may eventually be forwarded to the Active Controller Ring component . The Controller In queue component may be shared amongst the active controllers within the Active Controller Ring component . The incoming data packets may be stored in the Controller In queue component based on the order the multi controller receives the incoming data packets . In one embodiment the Controller In queue component may be a FIFO queue. As a FIFO queue the incoming data packets received first by the centralized multi controller may be placed first in the Controller In queue component . Hence the incoming data packets received first are forwarded to the Active Controller Ring component prior to any other incoming data packets received afterwards. For example in the Controller In queue component temporarily holds incoming data packets . Incoming data packet is at the front or top of the stack of the controller in queue while incoming data packet is queued behind the incoming data packet . The last incoming data packet stored in the Controller In queue component is incoming data packet . In this instance the centralized multi controller may have received incoming data packet prior to receiving incoming data packet while incoming data packet may be the last received data packet. Additionally because the incoming data packet is at the front of the queue stack incoming data packet may be the next incoming data packet to be forwarded to the Active Controller Ring component .

The NIB component may be coupled to the Scheduler component and to a plurality of active controllers within the Active Controller Ring component . NIB may maintain and provide necessary supporting information for resource utilization and traffic control. For instance the NIB component may provide multiple methods for the control logic to gain access to network nodes index all the of network nodes based on network node identifiers track state information of network nodes and network nodes features and capabilities. The NIB component may also provide routing information using flow tables such as NIB tables which may be analogous to routing information bases RIBs or forwarding information bases FIBs . The NIB tables may store a graph of the network nodes within a network topology instead of storing prefixes to destinations found in RIBs or FIBs. Moreover the NIB component may support logical elements e.g. overlay tunnels within a network. The NIB may also designate which flow entries or tables correspond to which network nodes. The active controllers within the Active Controller Ring component may use the routing information e.g. flow tables provided by the NIB component and header information from the incoming data packets to generate and compare flow tables.

The Active Controller Ring component may comprise a plurality of active controllers that may be used to process the incoming data packet . The active controllers may be logically coupled together to form a circular or a ring shaped Active Controller Ring component . The active controllers may comprise one or more central processor unit CPU chips network processor or other general purpose processors. The active controllers may be configured to implement several different logic states such as the master state the slave state or an equals state. The shape of the Active Controller Ring component and continuous circulation of a token may alternate which active controllers enter the master state. illustrates that one active controller C1 may be in the master state while the remaining active controllers C2 C4 within the Active Controller Ring component may be in the slave state. While in the master state the active controller C1 may be tasked with listening and waiting to receive an incoming data packet from the Controller In queue component . Active controllers C2 may be in the slave state and may be tasked with processing the incoming data packet to generate flow tables or waiting to transition to the master state. A token or some other special data packet may be passed from active controller C1 in the master state to the next in line active controller C2 in the Active Controller Ring component . The token may be passed when the active controller C1 is ready or about to receive an incoming data packet . When the next in line active controller C2 receives the token the next in line active controller C2 may transition from the slave state to the master state. The token used amongst the active controllers may be maintained centrally within the centralized multi controller . The equal state may represent when none of the active controllers are configured as the master state and or the slave state and every controller has full access to the network nodes and is equal to other controllers in the same role.

The state transitioning process as shown in may repeat itself for different active controllers e.g. active controller C4 until all active controllers have received an incoming data packet from the Controller In queue component . When this occurs all the active controllers may become temporarily unavailable to receive additional incoming data packets from the Controller In queue component . Referring back to block in an available controller may be added if the Controller in queue component s length exceeds a maximum pre specified limit or the utilization of all current active controllers exceed a pre specified limit while the active controllers are processing the incoming data packets .

After receiving an incoming data packet an active controller may process the incoming data packet by computing flow tables. Multi flow tables may be generated as described in the U.S. Provisional Patent Application No. 61 731 389 filed Nov. 29 2012 and entitled A Mechanism for Multiflow Table Design and Implementation under SDN OpenFlow which is incorporated herein by reference as if reproduced in its entirety. The flow entries in each flow table may use the header information and routing protocols. The active controller may also learn through feature inquiry and the structure of the flow table pipelines with attributes associated with each flow table as pre defined by vendors of the network nodes. The active controllers may be configured to process the different incoming data packets in a parallel fashion. In other words an active controller may not wait for other active controllers to finish processing incoming data packets before processing its own incoming data packet . Using as an example the current master active controller C1 may transfer the token to the new master active controller C2 receive an incoming data packet and process the incoming data packet . Afterwards the master active controller C2 may subsequently receive an incoming data packet and may process the received incoming data packet . As a result processing the two incoming data packets and may be implemented in parallel using two different active controllers

The Dispatcher component may be coupled to a plurality of output ports and a plurality of active controllers within the active controller ring. The Dispatcher component may be configured to transmit a plurality of outgoing data packets that comprise the flow tables generated by an active controller . The Dispatcher component may disseminate flow tables or flow entry modifications generated by the active controller to network nodes associated with a given data flow. The Dispatcher component may use a flow table or multi flow table pipeline to regulate when the flow tables may be distributed to network nodes . As discussed before the flow tables may be associated with more than one data flow within a network. In one embodiment the Dispatcher component may use the OpenFlow protocol to distribute the flow tables to the network nodes . After receiving the flow tables the network nodes may then use the new flow tables to forward data packets within the network to end nodes .

The processor may be implemented as one or more general purpose CPU chips or may be part of one or more application specific integrated circuits ASICs and or digital signal processors DSPs . The processor may comprise a central processor unit or CPU. The processor may be implemented as one or more CPU chips. The secondary storage is typically comprised of one or more disk drives or tape drives and is used for non volatile storage of data and as an over flow data storage device if RAM is not large enough to hold all working data. Secondary storage may be used to store programs that are loaded into RAM when such programs are selected for execution. The ROM is used to store instructions and perhaps data that are read during program execution. ROM is a non volatile memory device that typically has a small memory capacity relative to the larger memory capacity of secondary storage . The RAM is used to store volatile data and perhaps to store instructions. Access to both ROM and RAM is typically faster than to secondary storage .

At least one embodiment is disclosed and variations combinations and or modifications of the embodiment s and or features of the embodiment s made by a person having ordinary skill in the art are within the scope of the disclosure. Alternative embodiments that result from combining integrating and or omitting features of the embodiment s are also within the scope of the disclosure. Where numerical ranges or limitations are expressly stated such express ranges or limitations should be understood to include iterative ranges or limitations of like magnitude falling within the expressly stated ranges or limitations e.g. from about 1 to about 10 includes 2 3 4 etc. greater than 0.10 includes 0.11 0.12 0.13 etc. . For example whenever a numerical range with a lower limit R and an upper limit R is disclosed any number falling within the range is specifically disclosed. In particular the following numbers within the range are specifically disclosed R R k R R wherein k is a variable ranging from 1 percent to 100 percent with a 1 percent increment i.e. k is 1 percent 2 percent 3 percent 4 percent 7 percent . . . 70 percent 71 percent 72 percent . . . 97 percent 96 percent 97 percent 98 percent 99 percent or 100 percent. Moreover any numerical range defined by two R numbers as defined in the above is also specifically disclosed. The use of the term about means 10 of the subsequent number unless otherwise stated. Use of the term optionally with respect to any element of a claim means that the element is required or alternatively the element is not required both alternatives being within the scope of the claim. Use of broader terms such as comprises includes and having should be understood to provide support for narrower terms such as consisting of consisting essentially of and comprised substantially of. Accordingly the scope of protection is not limited by the description set out above but is defined by the claims that follow that scope including all equivalents of the subject matter of the claims. Each and every claim is incorporated as further disclosure into the specification and the claims are embodiment s of the present disclosure. The discussion of a reference in the disclosure is not an admission that it is prior art especially any reference that has a publication date after the priority date of this application. The disclosure of all patents patent applications and publications cited in the disclosure are hereby incorporated by reference to the extent that they provide exemplary procedural or other details supplementary to the disclosure.

While several embodiments have been provided in the present disclosure it should be understood that the disclosed systems and methods might be embodied in many other specific forms without departing from the spirit or scope of the present disclosure. The present examples are to be considered as illustrative and not restrictive and the intention is not to be limited to the details given herein. For example the various elements or components may be combined or integrated in another system or certain features may be omitted or not implemented.

In addition techniques systems subsystems and methods described and illustrated in the various embodiments as discrete or separate may be combined or integrated with other systems modules techniques or methods without departing from the scope of the present disclosure. Other items shown or discussed as coupled or directly coupled or communicating with each other may be indirectly coupled or communicating through some interface device or intermediate component whether electrically mechanically or otherwise. Other examples of changes substitutions and alterations are ascertainable by one skilled in the art and could be made without departing from the spirit and scope disclosed herein.

