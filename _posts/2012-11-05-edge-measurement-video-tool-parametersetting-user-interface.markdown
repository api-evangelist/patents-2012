---

title: Edge measurement video tool parameter-setting user interface
abstract: A user interface for setting parameters for an edge location video tool is provided. The user interface includes a multi-dimensional parameter space representation that allows a user to adjust a single parameter combination indicator in order to adjust multiple edge detection parameters at once. One or more edge feature representation windows may be provided which indicate the edge features detectable by the current configuration of the edge detection parameters.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08689127&OS=08689127&RS=08689127
owner: Mitutoyo Corporation
number: 08689127
owner_city: Kanagawa-ken
owner_country: JP
publication_date: 20121105
---
Precision machine vision inspection systems or vision systems for short can be utilized to obtain precise dimensional measurements of inspected objects and to inspect various other object characteristics. Such systems may include a computer a camera and optical system and a precision stage that is movable in multiple directions so as to allow the camera to scan the features of a workpiece that is being inspected. One exemplary prior art system that is commercially available is the QUICK VISION series of PC based vision systems and QVPAK software available from Mitutoyo America Corporation MAC located in Aurora Ill. The features and operation of the QUICK VISION series of vision systems and the QVPAK software are generally described for example in the QVPAK 3D CNC Vision Measuring Machine User s Guide published January 2003 and the QVPAK 3D CNC Vision Measuring Machine Operation Guide published September 1996 each of which is hereby incorporated by reference in their entirety. This product as exemplified by the QV 302 Pro model for example is able to use a microscope type optical system to provide images of a workpiece at various magnifications and move the stage as necessary to traverse the workpiece surface beyond the limits of any single video image. A single video image typically encompasses only a portion of the workpiece being observed or inspected given the desired magnification measurement resolution and physical size limitations of such systems.

Machine vision inspection systems generally utilize automated video inspection. U.S. Pat. No. 6 542 180 the 180 patent teaches various aspects of such automated video inspection and is incorporated herein by reference in its entirety. As taught in the 180 patent automated video inspection metrology instruments generally have a programming capability that allows an automatic inspection event sequence to be defined by the user for each particular workpiece configuration. This can be implemented by text based programming for example or through a recording mode which progressively learns the inspection event sequence by storing a sequence of machine control instructions corresponding to a sequence of inspection operations performed by a user with the aid of a graphical user interface or through a combination of both methods. Such a recording mode is often referred to as learn mode or training mode or record mode. Once the inspection event sequence is defined in learn mode such a sequence can then be used to automatically acquire and additionally analyze or inspect images of a workpiece during run mode. 

The machine control instructions including the specific inspection event sequence i.e. how to acquire each image and how to analyze inspect each acquired image are generally stored as a part program or workpiece program that is specific to the particular workpiece configuration. For example a part program defines how to acquire each image such as how to position the camera relative to the workpiece at what lighting level at what magnification level etc. Further the part program defines how to analyze inspect an acquired image for example by using one or more video tools such as edge boundary detection video tools.

Video tools or tools for short and other graphical user interface features may be used manually to accomplish manual inspection and or machine control operations in manual mode . Their set up parameters and operation can also be recorded during learn mode in order to create automatic inspection programs or part programs . Video tools may include for example edge boundary detection tools autofocus tools shape or pattern matching tools dimension measuring tools and the like.

Various methods are known for locating edge features in workpiece images. For example various algorithms are known which apply brightness gradient operators to images which include an edge feature to determine its location e.g. a Canny Edge detector or a differential edge detector. Such edge detection algorithms may be included in the machine vision inspection systems which also use carefully configured illumination and or special image processing techniques to enhance brightness gradients or otherwise improve edge location accuracy and repeatability.

Some machine vision systems e.g. those utilizing the QVPAK software described above provide edge location video tools which have adjustable parameters for an edge detection algorithm. In certain implementations the parameters may initially be determined for an edge on a representative workpiece during a learn mode operation and then utilized during a run mode operation to find the corresponding edge of a similar workpiece. When desirable edge detection parameters are difficult or impossible to determine automatically during the learn mode the user may choose to adjust the parameters manually. However certain edge detection parameters e.g. thresholds such as TH THR and THS outlined herein are considered to be difficult to understand for the majority of users e.g. relatively unskilled users and how their adjustment affects a particular edge detection operation is considered difficult to visualize particularly for a combination of parameters. The adjustments of the parameters may be further complicated by the variety of edge conditions and workpiece materials in part to part variations encountered when programming and using general purpose machine vision inspection system. An improved method and system that allows relatively unskilled users to adjust the parameters of edge location video tools so that they can be used to reliably inspect a variety of types of edges would be desirable.

A method for defining edge location parameters in a machine vision inspection system user interface is provided. In one embodiment a plurality of edge detection parameters for a region of interest ROI of an edge location video tool is defined. A multi dimensional parameter space representation is displayed which indicates possible combinations of the plurality of edge detection parameters. In one implementation the multi dimensional parameter space representation is a two dimensional grid with each dimension indicating possible values corresponding to one of the edge detection parameters. A parameter combination indicator e.g. including a parameter combination marker that can be selected and dragged in a user interface is located within the multidimensional parameter space representation which indicates a combination of the edge detection parameters based on its location. One or more edge feature representation windows are displayed which represent edge features located in the ROI of the edge location video tool. In one embodiment edge features detectable by the combination of edge detection parameters indicated by a current configuration of the parameter combination indicator are automatically updated in the one or more edge feature representation windows. It should be appreciated that the term window used herein includes previously known types of user interface windows and also refers more generally to unconventional elements of a user interface that may include one or more user interface characteristics such as they may include display elements made more compact than an entire display area and or may be hidden at some times e.g. as resized and or relocated and or hidden by a user they may focus on a particular class of information and or menus or selections related to a particular class of information and so on. Thus particular forms of windows illustrated herein are exemplary only and not limiting. For example in some embodiments a window may not have a well defined limiting boundary or the like it may have hyper link like behavior it may appear on a separate and or isolated display element and so on.

The edge feature representation windows may include representations of a scanline intensity and or scanline intensity gradient of the region of interest of the edge location video tool. Another edge feature representation window may include an image of the field of view of the machine vision inspection system. A representation of one or more of the edge features detectable by the combination of parameters indicated by a current configuration of the parameter combination indicator may be superimposed on the representation of the scanline intensity and or scanline intensity gradient and or the image of the field of view.

The edge feature representation windows and the multi dimensional parameter space representation may be synchronized such that a parameter adjustment or selection in one of the edge feature representation windows results in a corresponding adjustment or selection of the parameter indicator e.g. its position in the multi dimensional parameter space representation. The adjustment or selection in the edge feature representation window may comprise an adjustment or selection of a threshold level and the corresponding indication in the multi dimensional parameter space representation may comprise a movement of the parameter combination indicator to a location which corresponds to the selected threshold level.

Various embodiments of the invention are described below. The following description provides specific details for a thorough understanding and an enabling description of these embodiments. One skilled in the art will understand however that the invention may be practiced without many of these details. In addition some well known structures or functions may not be shown or described in detail so as to avoid unnecessarily obscuring the relevant description of the various embodiments. The terminology used in the description presented below is intended to be interpreted in its broadest reasonable manner even though it is being used in conjunction with a detailed description of certain specific embodiments of the invention.

Those skilled in the art will appreciate that the controlling computer system may generally consist of any computing system or device. Suitable computing systems or devices may include personal computers server computers minicomputers mainframe computers distributed computing environments that include any of the foregoing and the like. Such computing systems or devices may include one or more processors that execute software to perform the functions described herein. Processors include programmable general purpose or special purpose microprocessors programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such devices. Software may be stored in memory such as random access memory RAM read only memory ROM flash memory or the like or a combination of such components. Software may also be stored in one or more storage devices such as magnetic or optical based disks flash memory devices or any other type of non volatile storage medium for storing data. Software may include one or more program modules which include routines programs objects components data structures and so on that perform particular tasks or implement particular abstract data types. In distributed computing environments the functionality of the program modules may be combined or distributed across multiple computing systems or devices and accessed via service calls either in a wired or wireless configuration.

The vision measuring machine includes a moveable workpiece stage and an optical imaging system which may include a zoom lens or interchangeable lenses. The zoom lens or interchangeable lenses generally provide various magnifications for the images provided by the optical imaging system . The machine vision inspection system is generally comparable to the QUICK VISION series of vision systems and the QVPAK software discussed above and similar state of the art commercially available precision machine vision inspection systems. The machine vision inspection system is also described in commonly assigned U.S. Pat. Nos. 7 454 053 7 324 682 8 111 905 and 8 111 938 which are each incorporated herein by reference in their entireties.

The optical assembly portion is controllably movable along a Z axis that is generally orthogonal to the X and Y axes by using a controllable motor that drives an actuator to move the optical assembly portion along the Z axis to change the focus of the image of the workpiece . The controllable motor is connected to the input output interface via a signal line .

A workpiece or a tray or fixture holding a plurality of workpieces which is to be imaged using the machine vision inspection system is placed on the workpiece stage . The workpiece stage may be controlled to move relative to the optical assembly portion such that the interchangeable objective lens moves between locations on a workpiece and or among a plurality of workpieces . One or more of a stage light a coaxial light and a surface light e.g. a ring light may emit source light and or respectively to illuminate the workpiece or workpieces . The light source may emit light along a path including a mirror . The source light is reflected or transmitted as workpiece light and the workpiece light used for imaging passes through the interchangeable objective lens and the turret lens assembly and is gathered by the camera system . The image of the workpiece s captured by the camera system is output on a signal line to the control system portion . The light sources and may be connected to the control system portion through signal lines or busses and respectively. To alter the image magnification the control system portion may rotate the turret lens assembly along axis to select a turret lens through a signal line or bus .

As shown in in various exemplary embodiments the control system portion includes a controller the input output interface a memory a workpiece program generator and executor and a power supply portion . Each of these components as well as the additional components described below may be interconnected by one or more data control buses and or application programming interfaces or by direct connections between the various elements.

The input output interface includes an imaging control interface a motion control interface a lighting control interface and a lens control interface . The motion control interface may include a position control element and a speed acceleration control element although such elements may be merged and or indistinguishable. The lighting control interface includes lighting control elements and which control for example the selection power on off switch and strobe pulse timing if applicable for the various corresponding light sources of the machine vision inspection system .

The memory may include an image file memory portion an edge detection memory portion a workpiece program memory portion that may include one or more part programs or the like and a video tool portion . The video tool portion includes video tool portion and other video tool portions e.g. which determine the GUI image processing operation etc. for each of the corresponding video tools and a region of interest ROI generator that supports automatic semi automatic and or manual operations that define various ROIs that are operable in various video tools included in the video tool portion .

In the context of this disclosure and as known by one of ordinary skill in the art the term video tool generally refers to a relatively complex set of automatic or programmed operations that a machine vision user can implement through a relatively simple user interface e.g. a graphical user interface editable parameter windows menus and the like without creating the step by step sequence of operations included in the video tool or resorting to a generalized text based programming language or the like. For example a video tool may include a complex pre programmed set of image processing operations and computations which are applied and customized in a particular instance by adjusting a few variables or parameters that govern the operations and computations. In addition to the underlying operations and computations the video tool comprises the user interface that allows the user to adjust those parameters for a particular instance of the video tool. For example many machine vision video tools allow a user to configure a graphical region of interest ROI indicator through simple handle dragging operations using a mouse in order to define the location parameters of a subset of an image that is to be analyzed by the image procession operations of a particular instance of a video tool. It should be noted that the visible user interface features are sometimes referred to as the video tool with the underlying operations being included implicitly.

In common with many video tools the edge location and parameter setting subject matter of this disclosure includes both user interface features and underlying image processing operations and the like and the related features may be characterized as features of an edge location tool and corresponding parameter setting portion included in the video tool portion . The edge location tool may utilize an algorithm for determining edge locations. The algorithm may be governed by edge detection parameters which may be determined and programmed automatically in some cases during learn mode and or manually adjusted by a user e.g. thresholds such as TH THR and THS described in greater detail below. 

In one implementation in order that a user may manually set edge detection video tool parameters as outlined above the parameter setting portion provides a multi dimensional parameter space representation e.g. a 2 dimensional grid with TH on one axis and THS on the other axis . A parameter marker or indicator e.g. cursor is provided that can be moved within the parameter space representation by a user to adjust or select a desired parameter combination e.g. of TH and THS . One or more edge feature representation windows e.g. showing a scanline intensity and or a scanline intensity gradient and or a field of view of the machine vision system are provided which illustrate changes to the parameters and or the edge features that are detectable according to the current configuration as will be described in more detail below with respect to B A and B.

The signal lines or busses and of the stage light the coaxial lights and and the surface light respectively are all connected to the input output interface . The signal line from the camera system and the signal line from the controllable motor are connected to the input output interface . In addition to carrying image data the signal line may carry a signal from the controller that initiates image acquisition.

One or more display devices e.g. the display of and one or more input devices e.g. the joystick keyboard and mouse of can also be connected to the input output interface . The display devices and input devices can be used to display a user interface which may include various graphical user interface GUI features that are usable to perform inspection operations and or to create and or modify part programs to view the images captured by the camera system and or to directly control the vision system components portion . The display devices may display user interface features associated with the edge location video tool and parameter setting portion described in greater detail below.

In various exemplary embodiments when a user utilizes the machine vision inspection system to create a part program for the workpiece the user generates part program instructions by operating the machine vision inspection system in a learn mode to provide a desired image acquisition training sequence. For example a training sequence may comprise positioning a particular workpiece feature of a representative workpiece in the field of view FOV setting light levels focusing or autofocusing acquiring an image and providing an inspection training sequence applied to the image e.g. using an instance of one of the video tools on that workpiece feature . The learn mode operates such that the sequence s are captured or recorded and converted to corresponding part program instructions. These instructions when the part program is executed will cause the machine vision inspection system to reproduce the trained image acquisition and inspection operations to automatically inspect that particular workpiece feature that is the corresponding feature in the corresponding location on a run mode workpiece or workpieces which matches the representative workpiece used when creating the part program.

In the embodiment shown in the edge detection parameter window includes scan line intensity and scan line gradient windows and respectively as well as a multi dimensional parameter space representation of possible combinations of a plurality of edge detection parameters also referred to as edge characteristic parameters described further below. The scan line intensity window and scan line gradient window illustrate graphs of a scanline intensity profile IP and a scanline intensity gradient profile GP at pixel locations along the scanline direction for a representative scanline e.g. a central or average scanline or the like across the region of interest and each provides edge feature representations ER of edge features located in the region of interest of the edge location video tool . As used herein an edge feature representation is some form of representation that a user may understand as indicating an edge. In the scan line intensity window edge features are understood to be represented by significant changes in intensity over a relatively limited distance along the scan line. Typical rising edge and a falling edge feature representations ER one of several cases are indicated on the scanline intensity profile IP for example. In the scan line intensity gradient window edge features are understood to be represented by significant gradient changes over a relatively limited distance along the scan line and or gradient peaks or valleys . Typical positive and negative gradient edge feature representations ER one of several cases are indicated on the scanline intensity gradient profile GP for example. Of course in the field of view window edge features are understood to be represented by their image as indicated by the edge feature representations ER for example. Accordingly any or all of the windows or the like may be referred to as an edge representation window. The scanline intensity gradient profile GP may be understood to indicate the slope of the scanline intensity profile IP in this embodiment thus they will generally have corresponding edge feature representations at corresponding locations along their respective profiles.

In the embodiment shown in the multi dimensional parameter space representation includes a two dimensional graph showing potential combinations of edge detection parameters TH and THS with a current combination of the parameters indicated by the location of a parameter combination indicator PCI. The edge features detectable by the current combination of the edge detection parameters TH and THS are displayed and automatically updated in the windows and as described for one exemplary embodiment below.

The edge detection parameters TH and THS are edge detection parameters for an edge detection algorithm of the edge location video tool . In one embodiment these and other settings may be determined during a learn mode of the video tool and then utilized during a run mode for determining edges. When desirable settings are not able to be determined during the learn mode or when the edge points found by the video tool are determined to not be satisfactory the user may choose to adjust these settings manually. Some settings for video tools may be intuitive and readily adjustable. However other settings e.g. for the edge detection parameters TH and THS are sometimes considered to be relatively complicated and difficult to adjust particularly in combination.

The parameters may provide various functions in governing the algorithm. For example in some cases the parameters may provide a failsafe type function. That is a parameter that requires a minimum level of brightness change across an edge may prevent an edge detection video tool from returning an edge location in the case of unexpectedly low exposure e.g. due to a lighting failure or other anomalous condition. The parameter TH referred to herein defines a threshold that is related to a minimum level of brightness change required across an edge. In another case a parameter that requires a minimum level of brightness rate of change across an edge e.g. a gradient value which may characterize a width or sharpness of an edge may further characterize a particular instance of an edge and may prevent an edge detection video tool from returning an edge location in the case of an unexpected change in the form of an edge or its illumination e.g. an ambient illumination change or direction change or the focus of its image a blurry image broadens and softens an edge relative to the learn mode edge formation or illumination that was used for the initial training programming of the video tool. The parameter THS referred to herein defines a threshold that is related to a minimum brightness gradient required across an edge. It will be appreciated that each of the parameters outlined above and particularly their combination may be set to correspond to and or characterize a prototype instance of an edge during learn mode to increase the edge detection reliability and or specificity the detection of the expected edge using the expected imaging conditions . The parameters may be set discriminate for a particular edge or may cause the failure of the video tool when the expected conditions are not fulfilled or nearly fulfilled . In some embodiments a video tool may be set such that all the parameters are static resulting in video tool failure unless the expected conditions are strictly reproduced. In some embodiments a parameter THR referred to in the incorporated references may define a relationship between THS and TH and or a threshold value for that relationship such that video tool may set to adjust some of the parameters e.g. THS dynamically based on the actual brightness of an image provided that the brightness falls in a range deemed to provide a reasonable image for inspection resulting in a video tool that fails less often due to expected lighting variations and or part finish variations or the like.

In some cases a number of edges may be crowded together on a workpiece such that a target edge cannot be reliably isolated by the location and size of a video tool region of interest. In such cases the parameters outlined above and particularly their combination may be set at levels that are satisfied by a target edge including expected workpiece to workpiece variations and not satisfied by other nearby edges on the workpiece such that the video tool discriminates the target edge from the other edges during inspection and measurement operations. It should be appreciated that the inventive features disclosed herein are of particular value for setting a combination of parameters that are useful in this latter case as well as more generally providing improved ease of use and understanding for users.

The intensity window shows an intensity profile IP along a scanline of the edge detection video tool with an adjustable TH line . Similarly the gradient window shows a gradient profile GP along the same scanline of the edge detection video tool with an adjustable THS line . The windows and are configured to include operations wherein a user is able to select and adjust the parameter value of the TH line and the THS line graphically e.g. by dragging the lines without having to edit the TH and THS text boxes and respectively. This type of display and functionality may be particularly useful for experienced users for whom the adjustment may be easier and faster than utilizing the prior text box and methods. The location of the parameter combination indicator PCI and the TH and THS text boxes may be updated in real time in response to such a line adjustment. A disadvantage of only utilizing the adjustable lines and is that only one edge detection parameter may be adjusted at a time and less experienced users may not necessarily know how to interpret the raw intensity profile IP and the gradient profile GP illustrated in the edge feature representation windows and .

As illustrated in the windows and in order to increase user understanding of the edge discrimination effect of the TH and THS parameter values in one embodiment the windows and GUI are configured such that detectable edges DE may be indicated in those windows that is the corresponding detectable edge representations along the intensity profile IP and the gradient profile GP may be indicted . In the case shown in in the gradient window segments of the profile GP that fall below the threshold value for the gradient parameter THS are shaded out. The corresponding segments are shaded out in the intensity window as well. In addition the portions of the intensity profile that fall below the threshold value for the intensity parameter TH are shaded out in the intensity window . Thus three edge representations that satisfy the current combination of parameters are highlighted or indicated as detectable edges in the windows and by the unshaded areas marked as DE DE and DE in . Such visual indications assist users with understanding how changes in the edge detection parameters TH and THS separately and in combination affect the determination of edges and provide a real time indication of how the algorithm is working. In one embodiment not shown detectable edge indicators may also be superimposed on the corresponding edges in the field of view window as well. As shown in because the detectable edge number to select box is set to the default value of 1 meaning the edge to be located is the first detectable edge in the ROI along the scan line direction the edge selected ES is set for the video tool in the window to the edge corresponding to DE. The detectable edge DE may be marked as the selected edge in the window as well in some embodiments.

In contrast to the individual adjustments of the TH and THS lines and in the windows and the multi dimensional parameter space representation allows a user to adjust both of the thresholds TH and THS at the same time. In the graph the edge detection parameter TH is represented along the x axis while the edge detection parameter THS is represented along the y axis. The indicator PCI may be selected and dragged by the user to any location on the graph and the current location will define the current TH and THS values. Experiments have shown that by using various features of this user interface even relatively unskilled users can rapidly explore and optimize parameter combinations that reliably isolate particular edges using the various features outlined above or just as importantly help them understand that an edge cannot be reliably isolated without special conditions e.g. by selecting a particular detectable edge in the region of interest. 

As an illustrative example for the operation of the user interface in as shown in the indicator PCI in the graph has been moved to a new location e.g. by dragging in the user interface . In the embodiment of the windows and are synchronized with the multi dimensional parameter space representation such that an adjustment in the position of the indicator PCI results in a corresponding adjustment in the level of the TH line and the THS line and vice versa. Thus in accordance with the new location of the indicator PCI the TH line in the window and the THS line in the window have also been correspondingly adjusted. In any case as a result of the particular new levels defined for the parameters TH and THS an additional edge indicated by the reference number DE in the windows and is now indicated as a detectable edge. It will be understood by inspection of the figures that the detectable edge DE is a weaker edge than the edges DE DE and DE and cannot be isolated in the region of interest based solely on the parameters TH and THS. However it is readily observable that it is the second detectable rising edge along the scan line and so the user has set the detectable edge number to select box to a value of 2. Accordingly the parameters TH and THS isolate the relatively strong detectable edges DE DE while rejecting weaker edges and or noise and the detectable edge number selector further refines the desired edge to be located the second detectable edge along the scan line DE using the current set of parameters. In one embodiment at least one of the windows and and or the window and the detectable edge number e.g. as shown in the detectable edge number to select box are synchronized such that a user may select an indicated detectable edge in one of the windows and a corresponding parameter selection is automatically made for the number of the detectable edge along the scan line in the region of interest.

It will be appreciated that one advantage of the multi dimensional parameter space representation is that it allows the user to adjust multiple parameters e.g. edge detection parameters TH and THS at the same time to rapidly explore the detection margins and other detection reliability tradeoffs e.g. detection of an incorrect edge vs. the likelihood of tool failure associated with various combinations of settings. The user need not understand the functions of the various parameters because by adjusting the location of indicator PCI and observing the real time feedback of a correspond detectable edge indication the user intuitively feels the sensitivity of the edge detection results to the location indicator PCI and can intuitively set it in the best spot to produce the desired edge detection. Just as importantly the user may rapidly scan all combinations of parameters by simply sweeping the indicator PCI and learn that no particular combination isolates a target edge and determine that additional parameters e.g. the detectable edge number to select box may need to be set or the lighting may need to be changed or the region of interest adjusted or the like. In contrast it is impractical or impossible to make this same determination with the same efficiency and certainty using prior art methods and interfaces for setting edge detection parameters.

As previously indicated in the field of view window edge features are understood to be represented by their image as indicated by the edge feature representations ER for example. The multi dimensional parameter space representation includes a two dimensional graph showing potential combinations of edge detection parameters TH and THS with a current combination of the parameters indicated by the location of a parameter combination indicator PCI. In the embodiment shown in the scan line intensity window and scan line gradient window illustrate graphs of a scanline intensity profile IP and a scanline intensity gradient profile GP at pixel locations along the scanline direction for a representative scanline e.g. a central or average scanline or the like across the region of interest. The edge features detectable by the current combination of the edge detection parameters TH and THS the detectable edges DE are displayed and automatically updated in the windows and as previously described. In the field of view window detected edge points DEP that satisfy the current combination of parameters are indicated as described in greater detail below.

In the case shown in in comparison to a user has repositioned the parameter combination indicator PCI such that the edge detection parameter TH has a value 98.7. The three edge representations that satisfy the current combination of parameters are highlighted or indicated as detectable edges in the windows and by the unshaded areas marked as DE DE and DE in .

An important feature added in user interface display in comparison to the user interface display is that in the field of view window detected edge points DEP that satisfy the current combination of parameters are indicated. This provides more information that the representations in the user interface display . For example it may be seen in the scan line intensity window that the parameter TH is set such that the detectable edge DE of the representative scan line that is illustrated in the window barely exceeds the parameter TH. However importantly the detected edge points DEP in the field of view window indicate that along only a few of the scan lines does the detectable edge DE exceeds the parameter TH. Thus the detected edge points DEP in the field of view window also indicate that along some of the scan lines the first rising edge that exceeds the parameter TH correspond to the detectable edge DE. Such visual indications assist users with understanding how changes in the edge detection parameters TH and THS separately and in combination affect the determination of edges and provide a real time indication of how the algorithm is working. In the case shown in the detected edge points DEP clearly and immediately illustrate that the current parameters do not distinguish reliably between the edges DE and DE.

In contrast as shown in the indicator PCI in the graph has been moved to a new location e.g. by dragging in the user interface . In the embodiment of the windows and and are all synchronized with the multi dimensional parameter space representation such that an adjustment in the position of the indicator PCI results in a corresponding adjustment in the level of the TH line and the THS line as well as the detected edge points DEP. As a result of the particular new levels defined for the parameters TH and THS only the edge indicated by the reference number DE in the windows and is now indicated as a detectable edge. Perhaps more importantly it may be seen in the window that the detected edge points DEP are also now all located at the corresponding detectable edge DE conveying the information that the current set of parameters distinguish the edge DE from all other edges along all scan lines not just along the representative scan line in the windows and . In fact a user may perhaps drag the PCI and set the parameters more reliably by observing the corresponding detected edge points DEP than by observing the windows and in some embodiments. Thus in some embodiments where detected edge points DEP are illustrated in the field of view window the windows and may be optional or omitted and or hidden unless selected for display by the user.

From the foregoing it will be appreciated that specific embodiments of the invention have been described herein for purposes of illustration but that various modifications may be made without deviating from the scope of the invention. For example a multidimensional parameter space representation made include adding a third dimension to a two dimensional grid to form a volume e.g. represented isometrically and or rotatably or the like and locating parameter combination indicator in the volume. Or to a two dimensional grid may be augmented with a nearby linear parameter space representation for a third parameter or the like. Accordingly the invention is not limited except as by the appended claims.

