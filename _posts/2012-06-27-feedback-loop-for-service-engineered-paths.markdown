---

title: Feedback loop for service engineered paths
abstract: In general, techniques are described for providing feedback loops for service engineered paths. A service node comprising an interface and a control unit may implement the techniques. The interface receives traffic via a path configured within a network to direct the traffic from an ingress network device of the path to the service node. The control unit applies one or more services to the traffic received via the path and generates service-specific information related to the application of the one or more services to the traffic. The interface then sends the service-specific information to at least one network device configured to forward the traffic via the path so that the at least one network device configured to forward the traffic via the path is able to adapt the path based on the service-specific information.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09143557&OS=09143557&RS=09143557
owner: Juniper Networks, Inc.
number: 09143557
owner_city: Sunnyvale
owner_country: US
publication_date: 20120627
---
The invention relates to computer networks and more particularly to delivery of data within computer networks.

Recently network devices that form computer networks have been adapted to enable a form of networking referred to as software defined networking. In software defined networking the forwarding plane of a network switch router or other network device is made available via a communication protocol such that this forwarding plane may be configured via the communication protocol rather than a routing protocol. In many implementations the network devices may execute software to enable communications with other network devices in the network via this communication protocol so as to configure paths through the network. One example of a communication protocol that facilitates software defined networking is the so called OpenFlow communication protocol. OpenFlow is an open standard that allows users e.g. researchers to install software on the routers to run experimental or proprietary protocols to control packet routing or switching within a network.

The software controlled path setup may then orchestrate the configuration and deployment of paths on an as needed basis to suit a particular service. To illustrate orchestrating software that controls path setup may be manually configured with information identifying a network address translation NAT service operated by a first network device service node . A second network device that does not provide a NAT service may communicate with the orchestrating software learn of this NAT service and subscribe to this service whereupon the orchestrating software may configure a path through the network from the second network device to the first network device. The second network device may then push traffic requiring NAT service through the path where the first network device may apply the NAT traffic. When establishing the path the orchestrating software may configure one or more filters on the second network device controlling admission of network traffic into the path. These paths having been engineered for a particular service i.e. NAT service in this example having filters controlling admission to the path may be referred to as service engineered paths or SEPs . In this respect the orchestrating software may define these SEPs to suit a particular service in terms of defining filters tailored for that service.

While SEPs may provide for service sharing and enable other network devices to forward traffic via a path that meets the needs of the service application e.g. in terms of quality of service QoS provided bandwidth etc. SEPs may not generally be responsive to changes in the service application. In other words the control software managing the forwarding planes of the routers or switches along a SEP may be configured to accommodate network conditions through provisioning of paths that meet certain network requirements e.g. in terms of bandwidth quality of service etc. but may not be adapted to dynamically or automatically accommodate changes at service nodes.

In general the disclosure describes techniques for establishing a feedback loop for communicating service level information to any network device along a service engineered path SEP such as the ingress to the SEP a transit switch or node of the SEP intermediately positioned between the ingress network device and the end point service node or another service node which may occur in examples where two service nodes are along the SEP and both apply a service to the traffic . The techniques may enable a service node to communicate with these intermediate network devices along the SEP using a routing protocol such as an Intermediate System to Intermediate System IS IS routing protocol an open shortest path first OSPF routing protocol and a border gateway protocol BGP . The service level information may include any information related to a particular service provided by the service node. By providing this service level information the techniques may enable intermediate network devices along the SEP to adapt the SEP to accommodate changes in application of one or more services at the service node associated with the SEP.

In one embodiment a method comprises receiving with a service node of a network traffic via a path configured within the network to direct the traffic from an ingress network device of the path to the service node wherein the path includes filters that control admission of the traffic at the ingress network device to the path and applying with the service node one or more services to the traffic received via the path. The method also comprises generating with the service node service specific information related to the application of the one or more services to the traffic by the service node and sending with the service node the service specific information to at least one network device configured to forward the traffic along the path so that the at least one network device configured to forward the traffic along the path adapts the path based on the service specific information.

In another embodiment a service node comprises at least one interface that receives traffic via a path configured within a network to direct the traffic from an ingress network device of the path to the service node wherein the path includes filters that control admission of the traffic at the ingress network device to the path and a control unit that applies one or more services to the traffic received via the path and generates service specific information related to the application of the one or more services to the traffic. The at least one interface sends the service specific information to at least one network device configured to forward the traffic via the path so that the at least one network device configured to forward the traffic via the path is able to adapt the path based on the service specific information.

In another embodiment a non transitory computer readable medium comprises instructions that when executed cause one or more processors of a service node of a network to receive traffic via a path configured within the network to direct the traffic from an ingress network device of the path to the service node wherein the path includes filters that control admission of the traffic at the ingress network device to the path apply one or more services to the traffic received via the path generate service specific information related to the application of the one or more services to the traffic and send the service specific information to at least one network device configured to forward the traffic via the path so that the at least one network device configured to forward the traffic via the path is able to adapt the path based on the service specific information.

In another embodiment a method comprises configuring a path in a control unit of a network device by which to forward traffic from an ingress network device to a service node based on configuration data specified by an orchestrating device that computes and configures paths through the network and that does not forward any traffic through the network other than that used to compute and configure the paths through the network and forwarding the traffic along the path with the configured control unit to enable the service node to apply one or more services to the traffic. The method also comprises receiving with the network device service specific information related to the application of the one or more services by the service node to the traffic and re configuring the path in the control unit of the network device based on the received service specific information.

In another embodiment a network device comprises a control unit configured to forward traffic along a path from an ingress network device to a service node in accordance with configuration data specified by an orchestrating device that computes and configures paths through the network and that does not forward any traffic through the network other than that used to compute and configure the paths through the network and at least one interface that forwards the traffic along the path to enable the service node to apply one or more services to the traffic and receives service specific information related to the application of the one or more services by the service node to the traffic. The control unit re configures the path based on the received service specific information.

In another embodiment a non transitory computer readable medium comprises instructions that when executed cause one or more processors of a network device to configure a path in a forwarding plane of the network device by which to forward traffic from an ingress network device to a service node based on configuration data specified by an orchestrating device that computes and configures paths through the network and that does not forward any traffic through the network other than that used to compute and configure the paths through the network forward the traffic along the path with the configured forwarding plane to enable the service node to apply one or more services to the traffic receive service specific information related to the application of the one or more services by the service node to the traffic re configure the path in the forwarding plane of the network device based on the received service specific information.

In another embodiment a network system comprises a software defined network and an orchestrating device that computes and configures paths through the software defined network and that does not forward any traffic through the software defined network other than that used to compute and configure the paths through the software defined network. The software defined network includes a service node and an ingress network device. The ingress network device comprises a control unit configured to forward traffic along a path from the ingress network device to the service node in accordance with configuration data specified by the orchestrating device and at least one interface that forwards the traffic along the path to enable the service node to apply one or more services to the traffic. The service node comprises at least one interface that receives the traffic via the path and a control unit that applies the one or more services to the traffic received via the path and generates service specific information related to the application of the one or more services to the traffic. The at least one interface of the service node sends the service specific information to at least one network device configured to forward the traffic via the path. The control unit of the ingress network device unit re configures the path based on the received service specific information.

The details of one or more embodiments of the techniques are set forth in the accompanying drawings and the description below. Other features objects and advantages of the techniques will be apparent from the description and drawings and from the claims.

Enterprise network may represent a network owned and operated by an enterprise or business. Service node may represent a service card or so called service blade installed within or otherwise coupled to a network device such as network switch B a virtual network device executing within a network device or a separate physical network device. In any event service node may apply one or more services to network traffic such as Network Address Translation NAT or other firewall services intrusion detection and prevention IDP services anti virus services anti malware services parental control services or any other type of service. While described with respect to enterprise network establishing a path to service node the techniques may generally be applied with respect to any two network entities such as networks service nodes devices etc. and should not be limited to the examples described in this disclosure.

In any event a base network layer of network or base network includes network switches A B collectively network switches arranged in a physical topology. Network switches receive and forward packet data units PDUs for network flows according to forwarding information programmed into the switches by an administrator or external entity e.g. overlay controller or multi topology path computation element and or according to forwarding information learned by the switches whether by operation of one or more protocols e.g. interior gateway protocols IGPs or by recording information learned during PDU forwarding. Each of network switches may represent a router a layer three L3 switch a layer three L2 switch an L2 L3 switch or another network device that switches traffic according to forwarding information. Accordingly PDUs forwarded by network switches A may include for example L3 network packets e.g. Internet Protocol packets and or L2 packets e.g. Ethernet datagrams or Asynchronous Transfer Mode ATM cells . PDUs may be unicast multicast anycast and or broadcast.

An overlay network layer of network includes overlay switches A B collectively overlay switches arranged in a virtual topology over a physical topology defined by network switches . For example each of network switches may include a data plane and typical routing and or switching protocols for controlling packet forwarding but may also expose the data plane to other software installed on the device to control packet forwarding within this overlay network using propriety or third party protocols. Individual links of the virtual topology of the overlay network or overlay links may be established paths through the base network and or physical links connecting overlay switches . The overlay network may represent a virtual private network VPN an OpenFlow network consisting of one or more OpenFlow switches or an application layer network with selection functionality built in to endpoint devices for example. Accordingly each of overlay switches may represent a router or routing instance e.g. a virtual routing and forwarding VRF instance a Virtual Private Local Area Network LAN Service VPLS instance a dedicated L2 L3 or L2 L3 switch or a virtual or soft switch e.g. an OpenFlow switch implemented by a router or by a dedicated switch for example. Overlay switch A for instance represents a dedicated overlay switch i.e. a physical switch that performs switching and or routing in response to control plane software configured to control the data plane of the switch. Overlay switch B in contrast is implemented by network switch A and may represent for instance a soft switch that utilizes the underlying data plane of network switch A. Network may include multiple overlay network layers of different or similar types e.g. multiple VPNs and or OpenFlow networks .

Topology server receives topology information from network switches for the base network of multi topology network . For example topology server may execute one or more IGPs or Exterior Gateway Protocols e.g. the Border Gateway Protocol BGP to listen to routing protocol advertisements sent by network switches . Topology server collects and stores the base network topology information then provides the base network topology information to multi topology path computation element PCE in base topology update messages . Topology information may include traffic engineering information for the network links such as the links administrative attributes and bandwidth at various priority levels available for use by label switched paths LSPs . In some examples network switches may send topology update messages to topology server that specify L2 link information for L2 links connecting the network switches. In some examples topology server is a component of PCE .

Overlay controller receives topology information for the overlay network of multi topology network in topology update messages sent by overlay switches in respective communication sessions . Topology update messages sent by overlay switches may include virtual and physical switch port information PDUs and associated metadata specifying respective ports and or interfaces on which PDUs are received. In some examples overlay controller is a routing protocol listener that executes one or more routing protocols to receive routing protocol advertisements sent by overlay switches . Such routing protocol advertisements may be associated with one or more VRFs for instance. Overlay controller collects and stores the overlay topology information then provides the overlay topology information to PCE in overlay topology update messages . In some examples overlay controller is a component of PCE .

Network switches may be configured to or otherwise be directed to establish paths through the base network of multi topology network . Such paths may include for instance IP tunnels such as Generic Route Encapsulation GRE tunnels General Packet Radio Service GPRS Tunneling Protocol GTP tunnels LSPs or a simple route through the base network or a VPN identified by a static route with a route target for instance . Network switches provide path status information for paths established through the base network of multi topology network to PCE in communication sessions . Path status alternatively path state or LSP state information may include descriptors for existing operational paths as well as indications that an established path or path setup operation has failed. For example network switch A may attempt establish an LSP using a reservation protocol such as Resource reSerVation Protocol RSVP but fail due to insufficient network resources along a path specified by an Explicit Route Object ERO . As a result network switch A may provide an indication that the path setup operation failed to PCE in a communication session . PCE receives path status information and adds established paths through the base network of network as links in the overlay network topology.

PCE presents an interface by which clients A N collectively clients may request a dedicated path often for a specific time between any combination of network entities such as enterprise network and service node . Generally clients may request paths that conform to bandwidth host and time path parameters quality of service QoS path request parameters such as latency and jitter and may further specify additional associated classifiers to identify a flow between the specified endpoints. Example flow classifiers or parameters are provided below. Moreover PCE may present an interface by which client may request services to be applied to network traffic originating from networks owned and operated by respective ones of clients .

PCE uses base network topology information for network received from topology server overlay network topology information for network received from overlay controller and path status information received from network switches to compute and schedule paths between network entities through network that satisfy the parameters for the paths requested by clients . PCE may receive multiple path requests from clients that overlap in time. PCE reconciles these requests by scheduling corresponding paths for the path requests that traverse different parts of network and increase capacity utilization for example or by denying some of the path requests.

At the scheduled time for a scheduled path PCE installs forwarding information to network nodes e.g. overlay switches and network switches to cause the nodes to forward traffic in a manner that satisfies the requested path parameters. A requested path may traverse either or both domains of network . That is a requested path may traverse either or both of the base network and overlay network of multi topology network . For example a requested path for traffic may traverse only the base network domain as a simple network route for instance from network switch A to network switch B. However some paths may traverse multiple domains. For example any requested path for traffic between a network entity coupled to overlay switch B such as enterprise network and a network entity coupled to network switch B such as service node first traverses the overlay network domain and then traverses the base network domain.

PCE installs forwarding information to overlay switches using overlay controller . Overlay controller presents a programming interface by which PCE may add delete and modify forwarding information in overlay switches . Forwarding information of overlay switches may include a flow table having one or more entries that specify field values for matching PDU properties and a set of forwarding actions to apply to matching PDUs. A set of one or more PDUs that match a particular flow entries represent a flow. Flows may be broadly classified using any parameter of a PDU such as source and destination MAC and IP addresses a Virtual Local Area Network VLAN tag transport layer information a Multiprotocol Label Switching MPLS or Generalized MPLS GMPLS label and an ingress port of a network device receiving the flow. For example a flow may be all PDUs transmitted in a Transmission Control Protocol TCP connection all PDUs sourced by a particular MAC address or IP address all PDUs having the same VLAN tag or all PDUs received at the same switch port.

PCE invokes the programming interface of overlay controller by sending overlay network path setup messages directing overlay controller to establish paths in the overlay network of network and or steer flows from hosts onto established paths. Overlay controller responds to overlay network path setup messages by installing to overlay switches using communication sessions forwarding information that implements the paths and or directs flows received from hosts onto established paths.

PCE installs forwarding information to network switches using communication sessions . Each of network switches may present a programming interface in the form of a management interface configuration interface and or a path computation client PCC . PCE may invoke the programming interface of network switches to configure a tunnel e.g. an LSP install static routes configure a VPLS instance configure an Integrated Routing and Bridging IRB interface and to otherwise configure network switches to forward packet flows in a specified manner. In some instances PCE directs one or more of networks switches to signal a traffic engineered LSP TE LSP through the base network of network to establish a path. In this way PCE may program a scheduled path through network by invoking a programming interface of only the head network device for the path.

PCE may in some instances remove a path invoking the programming interfaces of network switches and overlay switches to remove forwarding information implementing the requested paths. In this way PCE frees resources for future paths.

Because PCE has an enhanced view of the current state of the network at both the overlay network layer and base network PCE may identify paths that are not visible to any one of network switches or overlay switches having a more limited view. PCE may additionally by virtue of having access to this enhanced view steer traffic to underutilized portions of network to increase capacity utilization of network . In addition centralizing the path computation and establishment with PCE may allow network operators to reconcile multiple possibly conflicting application path requests and may reduce first in time first in right access to network resources in favor of explicit centralized prioritization of application requests for dedicated paths. More information regarding path computation and establishment PCEs OpenFlow and other aspects related to the formation of paths within network by external orchestrating or coordinating devices may be found in U.S. patent application Ser. No. 13 339 983 entitled MULTI TOPOLOGY RESOURCE SCHEDULING WITHIN A COMPUTER NETWORK filed Dec. 29 2011 the entire contents of which are hereby incorporated by reference.

To illustrate an example path configured for enterprise network client A which may represent a network administrator of enterprise network or of a service provider network to which enterprise network subscribes to gain access to a public network e.g. the Internet may establish a path to service node in response to enterprise purchasing URL filtering services for enterprise network . Service node may in this example represent a service node that performs a URL filtering service to filter uniform resource locators URLs based on the content associated with requested URLs. Service node may advertise this URL filtering service to PCE via a routing protocol message that includes service discovery information identifying the URL filtering service. More information concerning automated discovery of services in computer networks can be found in U.S. Application Ser. No. 13 534 140 entitled AUTOMATED SERVICE DISCOVERY IN COMPUTER NETWORKS filed Jun. 27 2012 the entire contents of which are hereby incorporated by reference.

PCE may identify service node as providing the requested service and orchestrate path from overlay switch B through switch B to the identified service node i.e. service node in this example. PCE may orchestrate path by first computing path from overlay switch B through network switch B to service node and then configuring overlay switches B and network switch B to support delivery of traffic from enterprise network to service node via path . PCE may install filters that have admit and or deny criteria for traffic entering path via overlay switch B. In this example PCE may install filters associated with path that only admit and or deny traffic from enterprise network . The association of filters to control admission of traffic to path effectively limits admission of traffic to application of parental control services performed by service node . As a result these paths may be referred to as service engineered paths which is commonly abbreviated as SEP in the sense that these paths are engineered to steer traffic in an efficient manner so that one or more services may be applied to this traffic. In this sense path in this example is engineered to provide parental control services to traffic originated form enterprise network .

In this sense PCE acts as an orchestrating device and or software driving network which may be characterized as a software defined network in the sense that PCE performs path computation and directs configuration of the computed paths or SEPs rather than switches and themselves. That is network is configured externally via software executing on a device removed from network i.e. PCE in this example. In this respect the orchestrating software may define these SEPs to suit a particular service in terms of defining filters tailored for that service. In the previous example PCE configured SEP to promote utilization of service node . Typically switches or other network devices in order to provide a service included a service node that directly coupled to the switches or other network devices that also coupled to the network entity requiring the service application. Thus in the example of the service provider would have updated overlay switch B to include a service blade or node that performs parental control services in response to enterprise network requesting such services. However using software defined network PCE may configure SEP to service node coupled to network switch B which is a different switch to which enterprise network connects thereby promoting increased utilization of service node through service sharing.

While SEPs may provide for service sharing and enable other network devices to forward traffic via a path that meets the needs of the service application e.g. in terms of quality of service QoS provided bandwidth etc. SEPs may not generally be responsive to changes in the service application. In other words SEPs may be configured to accommodate network conditions through provisioning of paths that meet certain network requirements e.g. in terms of bandwidth QoS etc. but may not be adapted to dynamically or automatically accommodate changes at the level of service application.

To illustrate service node may analyze traffic received via SEP and determine that there are a number of requests for prohibited URLs where service node may drop packets requesting the prohibited URLs. The dropping of packets effectively wastes bandwidth reserved for SEP considering that these packets could have been preemptively dropped during admission of these packets by overlay switch B. In this sense service node has determined that these packets requested specific URLs are to be dropped but may be unable to communicate this service level information to overlay switch B. Overlay switch B not aware of the service level information continually sends packets that are effectively known to be prohibited and therefore dropped by the parental control service via SEP wasting bandwidth of SEP and the computing resources of switches B and service node .

In accordance with the techniques described in this disclosure service node may effectively establish a feedback loop for communicating service level information to 3party control plane software executing on any network device along the SEP such as the ingress to SEP i.e. overlay switch A in this example a transit switch or node i.e. network switch B in this example intermediately positioned between the ingress network device and the service node or another service node such as in examples where two service nodes are along a path and both perform services to the traffic . The techniques may enable service node to communicate with the overlay network control plane software executing on switches B along SEP using as one example a routing protocol such as an Intermediate System to Intermediate System IS IS routing protocol an open shortest path first OSPF routing protocol and a border gateway protocol BGP . One or more of these routing protocols may comprise a link state or flooding routing protocol such as the IS IS routing protocol and the OSPF routing protocol. The service level information may include any information related to a particular service provided by the service node. By providing this service level information the techniques may enable switches B along SEP to adapt SEP to accommodate changes in application of one or more services at service node .

In operation service node receives traffic via SEP configured within the network to direct the traffic from ingress switch B of the path to service node . As noted above SEP includes filters that control admission of the traffic at ingress switch B. Service node may apply one or more of the above noted services to the traffic received via SEP and generate service specific information related to the application of the one or more services to the traffic. Service node may then send the service specific information to at least one network device configured to forward the traffic via SEP i.e. any one of switches B in the example of but commonly ingress switch A so that any one of these switches B along SEP is able to adapt the path based on the service specific information.

To illustrate referring again to the URL filtering service service node may apply this URL filtering service to the traffic and identify URLs that are to be filtered effectively dropping any packets requesting or providing content associated with this URL. Service node may generate service specific information identifying this URL and send the URL to switch B via feedback loop . Switch B may receive this service specific information and update the filters associated with SEP to block any traffic associated with the URL which may be identified by a layer three address and a layer three port . In this respect switch B may adapt SEP to accommodate changes in application of the URL filtering service at service node .

Alternatively as another example service node may specify flows to which the URL filtering service is not to subsequently applied. That is service node may inspect certain flows and determine that these flows are requesting or providing content associated with URLs that are acceptable. Service node may then generate service specific information identifying these flows and indicate that the service application may be by passed for these flows. Service node may send this service specific information to ingress switch B via feedback loop which may adapt SEP based on this service specific information by updating the filters associated with SEP to indicate that traffic associated with these flows is to be sent to its intended destination and not pushed through SEP to service node .

The techniques may also be applied with respect to a number of other services to facilitate service level adaptation of SEPs such as SEP . For example service node may apply one or more services to flows associated with a particular set of one or more virtual private networks VPNs . The service in this context may be applied with respect to flows having a certain VPN membership scope corresponding to the set of VPNs. If service node is informed or otherwise determines that this VPN membership scope has changed meaning that one or more VPNs are added or removed from the set of VPNs service node may generate service specific information identifying the change in VPN membership scope and send this information to ingress switch B via feedback loop . Ingress switch A may then adapt SEP based on this service specific information by updating the filters to accommodate the change in VPN membership scope such that traffic associated with the removed VPNs are not forwarded via SEP and traffic associated with the new VPNs is forwarded via SEP .

Moreover the service specific information may include information defining execution results of the applied services and service state information between the at least one network device configured to forward the traffic along the path and the service node. The service state information may describe one or more of a current load and performance between the at least one network device configured to forward the traffic along the path and the service node. Service node may pass this service specific information to for example ingress switch B via feedback loop which may use this information to performing load balancing or other operations so as to better distribute traffic to service nodes so that service nodes are not overloaded.

In this manner service node may establish feedback loop so as to communicate service specific information to one or more network devices that support SEP i.e. switches and B in the example of . As a result these one or more network devices may adapt SEP to accommodate changes in application of the service by service node . By enabling the adaptation of SEP in this manner the techniques may improve network performance e.g. by not wasting bandwidth and utilization e.g. by load balancing traffic . While described above with respect to a number of different services and or application the SEP feedback loop techniques may generally enable any service specific information to be passed back to one or more network devices that support the SEP. The techniques should therefore not be limited to the example services and service specific information described in this disclosure.

PCE includes a control unit and a network interface not shown to exchange packets with other network devices. Control unit may include one or more processors not shown in that execute software instructions such as those used to define a software or computer program stored to a computer readable storage medium again not shown in such as non transitory computer readable mediums including a storage device e.g. a disk drive or an optical drive or a memory such as Flash memory or random access memory RAM or any other type of volatile or non volatile memory that stores instructions to cause the one or more processors to perform the techniques described herein. Alternatively or additionally control unit may comprise dedicated hardware such as one or more integrated circuits one or more Application Specific Integrated Circuits ASICs one or more Application Specific Special Processors ASSPs one or more Field Programmable Gate Arrays FPGAs or any combination of one or more of the foregoing examples of dedicated hardware for performing the techniques described herein.

Clients request paths through a network using client interface . In general a path request includes a requested date time a required bandwidth or other constraint and at least two endpoints. Client interface may be a command line interface CLI or graphical user interface GUI for instance. Client may also or alternatively provide an application programming interface API such as a web service. A user uses a client application to invoke client interface to input path request parameters and submit the request to PCE . Client interface receives path requests from clients and pushes the path requests to path request queue a data structure that stores path requests for computation distribution by path manager .

To compute and schedule paths through a network intelligently PCE receives topology information describing available resources at multiple layers of the network. Topology server interface illustrated as topology server IF executed by control unit of PCE communicates with a topology server to receive topology information for a base network layer of the network while overlay controller interface communicates with an overlay controller to receive topology information for an overlay network layer of the network. Topology server interface may include a routing protocol daemon that executes a routing protocol to receive routing protocol advertisements such as Open Shortest Path First OSPF or Intermediate System to Intermediate System IS IS link state advertisements LSAs or BGP UPDATE messages. Topology server interface may in some instances be a passive listener that neither forwards nor originates routing protocol advertisements.

In this example topology server interface receives topology information that includes traffic engineering TE information. Topology server interface may for example execute Intermediate System to Intermediate System with TE extensions IS IS TE or Open Shortest Path First with TE extensions OSPF TE to receive TE information for advertised links. Such TE information includes one or more of the link state administrative attributes and metrics such as bandwidth available for use at various LSP priority levels of links connecting routers of the domain. In some instances topology server interface executes Border Gateway Protocol to receive advertised TE information for inter AS and other out of network links. Additional details regarding executing BGP to receive TE info are found in U.S. patent application Ser. No. 13 110 987 filed May 19 2011 and entitled DYNAMICALLY GENERATING APPLICATION LAYER TRAFFIC OPTIMIZATION PROTOCOL MAPS which is incorporated herein by reference in its entirety.

Topology server interface may in some instances receive a digest of topology information collected by a topology server rather than executing a routing protocol to receive routing protocol advertisements directly. Topology server interface stores base network topology information with TE information in multi topology traffic engineering database illustrated as multi topology TED hereinafter MT TED which is stored by a computer readable storage medium of control unit for use in path computation. MT TED is described in further detail below.

Overlay controller interface illustrated as overlay controller IF represents a module that may implement a standardized interface such as OpenFlow to receive topology information from an overlay controller such as an OpenFlow controller that describes overlay network links connecting overlay switches. In general overlay network links are not advertised by network switches e.g. routers of the base network for the overlay network and so will not be described by topology information received by topology server interface . An overlay controller augments the base network topology with overlay network topology links by providing overlay network topology information to overlay controller interface which stores the overlay network topology information to MT TED . Overlay controller interface may receive topology information for multiple different overlay networks including VPNs and or OpenFlow networks. Different overlay networks may require different instances of overlay controller interface that communicate with network switches of the overlay network or with a topology server for example to receive overlay network topology information for respective overlay networks.

Multi topology traffic engineering database stores topology information for a base network layer and one or more overlay network layers of a network that constitutes a path computation domain for PCE . MT TED may organize topology information for respective network layers hierarchically with the base network topology information supporting the topology information for one or more overlay networks. Paths in a lower layer topology may appear as links in a higher layer topology. For example tunnels e.g. TE LSPs created in the base network layer can appears as links in an overlay network TE topology. PCE may then correlate overlay network links with paths established in the base network layer to efficiently compute paths that cross multiple overlay topologies. MT TED may include one or more link state databases LSDBs where link and node data is received in routing protocol advertisements received from a topology server and or discovered by link layer entities such as an overlay controller and then provided to PCE via overlay controller interface . In some instances an operator may configure traffic engineering or other topology information within MT TED via operator interface .

Topology server interface may also receive from a topology server or by execution of routing protocols to receive routing protocol advertisements that include reachability information endpoint information that describes endpoints reachable by specified nodes in any of the network topologies. Topology server interface may receive endpoint information for a base layer of the network as well as for one or more services e.g. VPNs provided by the network that may correspond to overlay networks of the network. Endpoint information may associate network address prefixes with a nodes of the multi topology network layers where network address prefixes may be e.g. IPv4 or IPv6. For example topology server interface may receive a BGP UPDATE message advertising a particular subnet as reachable from a particular node of the base network. As another example topology server interface may receive an Application Layer Traffic Optimization ALTO map that includes PIDs associating respective nodes of a multi topology network layer with network address prefixes reachable from the nodes. Endpoints that have network addresses that are members of the subnet are therefore reachable from the node and PCE may calculate paths for those endpoints to terminate i.e. begin or end at the node. Topology server interface stores endpoint information received for a layer to a corresponding one of endpoint databases A K illustrated as endpoint DB A K and collectively referred to as endpoint databases where K refers to a number of layers of the multi topology network that constitutes a path computation domain for PCE . Some of endpoint databases may therefore be associated with respective service instances e.g. respective VPNs that constitute overlay network layers of a multi topology network. PCE may therefore use endpoint databases to locate and validate endpoints specified in path requests received from clients.

Each of service path engines A K collectively SPEs compute requested paths through a layer of the multi topology network with which it is associated and for which it is responsible. Control unit may execute multiple SPEs concurrently e.g. as separate processes. Each of SPEs is associated with a corresponding one of generated path databases A K illustrated as generated path DB A K and collectively referred to as generated path databases . Path manager dequeues path requests from path request queue and assigns path requests to SPEs based on the layer of the multi topology network in which the endpoints reside as determined by path manager from endpoint databases . That is endpoints reachable by layers of a multi topology network that is a path computation domain for PCE are stored by at least one of endpoint databases and path manager determines the one or more endpoint databases that include endpoints specified for a dequeued path request.

Paths are unidirectional. If a client requests a bidirectional path path manager triggers two path requests for the requested path one for each direction. In some cases a path may cross multiple layers of the network e.g. at a gateway to the base layer that is implemented by one of the overlay network nodes or at a network node that participates in multiple overlay networks. In such cases multiple SPEs may cooperate to compute segments of the multi layer path that path manager stitches together at the gateway. Upon computing paths SPEs schedule the paths by storing the paths to respective generated path databases . A scheduled path stored in one of generated path databases includes path information used by path manager to establish the path in the network and may include scheduling information used by scheduler to trigger path manager to establish the path. As described in further detail below path scheduling may require locking generated path databases to perform path validation prior to committing the path.

When a servicing path request received from path manager an SPE may initially validate the request by determining from endpoint databases that the endpoints for the requested path whether expressed as logical interfaces or network addresses are known to PCE i.e. exist within the path computation domain of PCE . SPE may additionally validate flow classifiers to ensure that the flow classifiers specified for a requested path exist. If initial validation fails for either both of these reasons SPE rejects the requested path and path manager sends a path rejection message detailing the reasons to the requesting client via client interface .

To compute a requested path at a layer of a multi topology network a service path engine for the layer uses MT TED and the corresponding one of generated path databases for the layer to determine whether there exists a path in the layer that satisfies the TE specifications for the requested path for the duration of the requested time. SPEs may use the Djikstra constrained SPF CSPF and or the Bhandari Edge disjoint shortest pair for determining disjointed main and backup paths path computation algorithms for identifying satisfactory paths though the multi topology network. If a satisfactory computed path for the requested path exists the computing service path engine for the layer re validates the computed path and if validation is successful schedules the computed path by adding the computed path to the one of generated path databases for the layer. In addition the computing one of SPE adds the requested path start complete times to scheduler if any . A computed path added to one of generated path databases is referred to as a scheduled path until such time as path manager programs the scheduled path into the multi topology network whereupon the scheduled path becomes an active path. A scheduled or active path is a temporarily dedicated bandwidth channel for the scheduled time in which the path is or is to become operational to transport flows.

As noted above generated path databases store path information for scheduled and active paths. Path information may include an ERO that specifies a list of overlay or base network nodes for a TE LSP routes or tunnels to be configured in one or more overlay network or base network nodes forwarding information for overlay network nodes specifying respective sets of forwarding actions which may also be referred to as forwarding filters or filters to apply to PDUs inbound to the overlay network nodes and or any other information usable by any of topology node interfaces to establish and steer flows onto scheduled paths in a multi topology network.

SPEs compute scheduled paths based upon a current state or snapshot of the multi topology network as represented by MT TED and generated path databases . Because multiple SPEs execute simultaneously in this example to compute and schedule paths through the multi topology network multiple SPEs may attempt to update generated path databases simultaneously which could in some cases result in network resource oversubscription and failure by PCE to satisfy requested paths. One of SPEs may therefore having computed a path execute a transaction that conforms to the ACID properties atomicity consistency isolation durability or another type of atomic transaction to both re validate and update generated path databases with a scheduled path. That is the SPE may first lock generated path databases to prevent other SPEs from modifying generated path databases . The SPE may then validate the computed path against the locked generated path databases as well as MT TED . If the computed path is valid the SPE updates generated path databases by adding the computed path as a scheduled path. The SPE then unlocks generated path databases . In this way all affected links are updated in the same transaction and subsequent path validations by other SPEs account for the updates. SPEs may use any suitable data structure locking mechanism such as monitors mutexes or semaphores to lock generated path databases .

If the SPE fails to validate a previously computed path the SPE attempts to re compute the path. Upon identifying a satisfactory path against the current snapshot of the multi topology network the SPE again attempts to validate the computed path and update generated path databases .

In some cases SPEs may be unable to identify a path through an overlay network with which to satisfy a path request. This failure may be due to any of a number of factors. For example sufficient network resources with which to satisfy the path request may be unavailable for the scheduled time due for instance to previously scheduled paths that include one or more links of the base network layer for any possible paths between the endpoints of the path request at an overlapping time. In this example path computation fails. In other words one or more paths between the endpoints of the path request exist but the paths are already sufficiently subscribed to prevent the additional reservation of adequate resources for the requested path. As another example SPEs may be unable to identify any paths through an overlay network between the endpoints of the path request because the computation failed due to a missing link in the overlay network. In other words the computed overlay network graph after removing unusable edges unable to satisfy path request constraints includes two disjoint subgraphs of the overlay network. However in this case a suitable path may be generated by creating a tunnel through the base layer between the subgraphs for the overlay network.

Where path computation fails because sufficient network resources do not exist at the requested time the computing SPE may consider policies set by an operator via operator interface that establish priorities among clients of PCE and or among path request parameters including bandwidth hosts time and QoS parameters as well as flow classifiers. A policy of policies may prioritize the requested path for which path computation failed over and against one or more scheduled paths of generated path databases . In such instances the computing SPE may preempt one or more of these scheduled paths by removing again in accordance with policies the paths from generated path databases and scheduler . In addition the computing SPE in such instances enqueues the removed paths as path requests to path request queue . Components of PCE may then again attempt to compute satisfactory paths for the path requests corresponding to paths removed from generated path databases . Where SPEs are unable to identify a satisfactory path for such a path request SPEs direct path manager to send a path rejection message to a requesting client that issued the path request via client interface . In effect PCE revokes a grant of scheduled multi topology network resources made to the requesting client.

Where path computation fails due to a missing link between disjoint subgraphs of an overlay network each providing reachability to respective endpoints for a requested path the computing SPE requests one of tunnel managers A K collectively tunnel managers to establish a tunnel in a lower layer of the multi topology network. For example one of SPEs for an overlay network may request a tunnel in a lower layer overlay network or in the base network layer. Each of tunnel managers is associated with one of the layers of the multi topology network and with one of generated path databases . In other words each of tunnel managers manages tunnels for one of the topologies.

Tunnel managers operate as intermediaries between generated path databases and SPEs . A higher layer SPE of SPEs may request a lower layer one of tunnel managers to establish a tunnel between two nodes of the lower layer to create a link in the higher layer. Because a tunnel traverses two layers of the multi topology network each of the two nodes may straddle the two layers by having an ingress and egress interface coupling the two layers. That is a first one of the two nodes may be an ingress network switch having an ingress interface to the base network layer while a second one of the two nodes may be an egress network switch having an egress interface from the base network layer. The tunnel manager in response may enqueue a path request specifying the two nodes in the lower layer of the multi topology network to path request queue . If a lower layer SPE is able to schedule a path for the path request this path becomes a link in the lower layer generated path database and the lower layer SPE notifies the requesting one of tunnel managers with link tunnel information for the link. The tunnel manager propagates this tunnel information to MT TED which triggers the higher layer SPE that a new link is available in the higher layer topology and prompts the higher layer SPE to reattempt computing a satisfactory path for the original requested path. Tunnel managers may also validate tunnel setup at their respective layer of a multi topology network.

Scheduler instigates path setup by tracking scheduled start times for scheduled paths in generated path databases and triggering path manager to establish the scheduled paths at their respective start times. Path manager establishes each scheduled path using one or more of topology node interfaces including overlay controller interface device management interface and network switch interface . Different instances of PCE may have different combinations of topology node interfaces .

Path manager may invoke the overlay controller interface to sending overlay network path setup messages e.g. overlay network path setup messages of directing an overlay controller to establish paths in an overlay network and or steer flows from hosts onto established paths in accordance with path information for scheduled paths in generated path databases . In this way PCE may program paths according to a permanent virtual circuit PVC or hop by hop model by programming forwarding state in network and or overlay switches to execute the paths being programmed. As noted above one or more of these paths may include filters that admit or deny traffic to the path. SPEs may provide these filters to path manager which may install these filters in the ingress switches.

Device management interface may represent a Simple Network Management Protocol SNMP interface a Device Management Interface DMI a CLI or any other network device configuration interface. Path manager may invoke device management interface to configure network switches e.g. routers with static routes TE LSPs or other tunnels in accordance with path information for scheduled paths in generated path databases . Network switch interface establishes communication sessions such as communication sessions of with network switches to receive and install path state information and to receive path setup event information. Network switch interface may be a PCE protocol PCEP interface a DMI or SNMP interface for example.

Path manager may invoke device management interface and or network switch interface to configure and direct network switches to establish paths in a base network layer or overlay network layer of a multi topology network. For example path manager may first configure a TE LSP within a network switch at a network edge then direct the network switch to signal a path for the TE LSP using RSVP with traffic engineering extensions RSVP TE or another signaling protocol. In this way PCE may program paths including TE LSPs into the network according to a soft PVC SPVC model. In this model the network presents a programming interface that PCE invokes to dynamically set up the SPVCs. In some examples PCE may use a combination of PVC and SPVC models to program paths into a multi topology network.

Upon receiving confirmation from topology node interfaces that a scheduled path setup is successful path manager transitions a status of the scheduled path in generated path databases to active. At the scheduled end time if any for an active path scheduler notifies path manager to tear down the active path using topology node interfaces . After tearing down the path path manager removes the path from generated paths .

Some examples of router may not include the full functionality described and illustrated. For instance some examples of router may include different combinations of PCC B OpenFlow switch and IRB interface rather than all such components. Moreover while described with respect to a particular network device e.g. a router aspects of the techniques may be implemented by any network device or combination of network devices. The techniques should therefore not be limited to the exemplary embodiments described in this disclosure.

Router includes a control unit and interface cards A N collectively IFCs coupled to control unit via internal links. Control unit may include one or more processors not shown in that execute software instructions such as those used to define a software or computer program stored to a computer readable storage medium again not shown in such as non transitory computer readable mediums including a storage device e.g. a disk drive or an optical drive or a memory such as Flash memory random access memory or RAM or any other type of volatile or non volatile memory that stores instructions to cause the one or more processors to perform the techniques described herein. Alternatively or additionally control unit may comprise dedicated hardware such as one or more integrated circuits one or more Application Specific Integrated Circuits ASICs one or more Application Specific Special Processors ASSPs one or more Field Programmable Gate Arrays FPGAs or any combination of one or more of the foregoing examples of dedicated hardware for performing the techniques described herein.

In this example control unit is divided into two logical or physical planes to include a first control or routing plane A control plane A and a second data or forwarding plane B data plane B . That is control unit implements two separate functionalities e.g. the routing control and forwarding data functionalities either logically e.g. as separate software instances executing on the same set of hardware components or physically e.g. as separate physical dedicated hardware components that either statically implement the functionality in hardware or dynamically execute software or a computer program to implement the functionality.

Control plane A of control unit executes the routing functionality of router . In this respect control plane A represents hardware or a combination of hardware and software of control unit that implements routing protocols. In this example routing protocol daemon RPD is a process executed by control unit that executes routing protocols B illustrated as RPs B by which routing information stored in routing information base RIB and traffic engineering information stored in traffic engineering database TED may be determined. In addition RPD may establish peering sessions for one or more routing protocols B with another router route reflector or routing protocol listener e.g. an application layer traffic optimization ALTO server and send L3 topology and or traffic engineering in RIB and or TED to the peers.

Routing protocols B may include for example IGPs such as OSPF TE or IS IS TE and or exterior gateway protocols such as BGP TE. RIB and TED may include information defining a topology of a network such as the base network layer of multi topology network of . Routing protocol daemon may resolve the topology defined by routing information in RIB to select or determine one or more routes through the network. Control plane A may then update data plane B with these routes where data plane B maintains these routes as forwarding information .

Forwarding or data plane B represents hardware or a combination of hardware and software of control unit that forwards network traffic in accordance with forwarding information . RIB may in some aspects comprise one or more routing instances implemented by router with each instance including a separate routing table and other routing information. Control plane A in such aspects updates forwarding information with forwarding information for each of routing instances . In this respect routing instances each include separate forwarding information for use by data plane B in forwarding traffic in accordance with the corresponding routing instance. Further details of one example embodiment of a router can be found in U.S. patent application Ser. No. 12 182 619 filed Jul. 30 2008 and entitled STREAMLINED PACKET FORWARDING USING DYNAMIC FILTERS FOR ROUTING AND SECURITY IN A SHARED FORWARDING PLANE which is incorporated herein by reference.

Control plane A further includes management interface by which a network management system or in some instances an administrator using a command line or graphical user interface configures in VPN module one or more VPN instances for a network to interconnect combinations of L2 networks into a single Ethernet domain. For example an administrator may configure router as a participant in a particular VPN instance such as VPN instance . VPN module may perform auto discovery or other techniques to determine additional routers participating in a VPN instance and additionally performing signaling techniques to establish a full mesh of pseudowires between router and each of the additional routers.

Data plane B includes one or more forwarding units such as packet forwarding engines PFEs that provide high speed forwarding of network traffic received by interface cards via inbound links A N to outbound links A N. Integrated routing and bridging interface IRB interface of data plane B processes and forwards network traffic received on interfaces associated with the IRB interface . An administrator may configure IRB interface via management interface to map routing interface of IRB interface to one of routing instances of router . Routing interface may represent a next hop or other reference of a logical interface IFL of IRB interface for example. In some embodiments aspects of data plane B are distributed to a number of distributed forwarding units such as packet forwarding engines each associated with a different one or more IFCs . In these embodiments IRB interface may be may be distributed to the distributed forwarding units to enable high speed integrated routing and bridging within the data plane.

Router implements VPN instance associated with IRB interface to operate as a virtual switch to interconnect multiple L2 networks. VPN instance maps a gateway L2 address e.g. a gateway MAC address to routing interface which maps to one of routing instances . In this respect the gateway L2 address maps to the routing instance. IRB interface classifies L2 PDUs received on an interface associated with VPN instance and destined for a gateway L2 addresses of VPN instance as L3 packets for routing using the one of routing instances mapped to routing interface . In other words when router receives an L2 PDU on an interface associated with VPN instance IRB interface determines the destination L2 address of the L2 PDU. When the destination L2 address matches the gateway L2 address mapped to routing interface IRB interface classifies the L2 PDU as an L3 packet and provides the L2 PDU to the mapped one of routing instances for L3 forwarding by data plane B. IRB interface may decapsulate the L2 PDU of the L2 header and footer. When a destination L2 address of an L2 PDU does not match the gateway L2 address VPN instance may switch the L2 PDU according to a matching flow entry of flow table . As a result router may operate as a gateway between an L2 overlay network layer and an L3 base network layer of multi topology network . In some instances IRB interface performs a prior logical operation to classify L2 PDU as either routing traffic or bridging traffic and then bridges the traffic or provides the traffic to a routing interface based on the result of classification.

Router implements OpenFlow switch to control switching of L2 PDUs among the set of virtual and or physical interfaces of router that are associated with VPN instance . Such interfaces may include attachment circuits for attaching L2 networks to VPN instance . OpenFlow protocol interface IF of control plane A establishes an OpenFlow protocol session with an OpenFlow controller to provide L2 topology information and to receive forwarding information. OpenFlow protocol IF installs flow entries received in the OpenFlow protocol session to flow table to direct forwarding of PDUs received on interfaces associated with the VPN instance . In some instances VPN instance includes a L2 learning table and performs L2 learning with respect to interfaces of router associated with VPN instance .

A network management system or in some instances an administrator using a command line or graphical user interface may invoke management interface to configure label switched paths described in LSP database illustrated as LSP DB . LSP database includes LSP configuration data for example an LSP destination path e.g. a Reported Route Object and LSP attributes such as setup priority and hold priority number of hops the reserved bandwidth and or a metric that has been optimized for the LSP e.g. an IGP metric a TE metric or hop counts . LSP database may also include information designating zero or more attributes of each configured LSP as delegable parameters that may be set modified by a PCE using extended PCEP to modify the operation of the LSP when set up in the network. LSP attributes may be divided into three categories 1 non delegable parameters that RPD applies immediately using RSVP A and that are neither re signalled nor overridden by a PCE 2 delegable parameters that RPD applies when the LSP is re signaled due e.g. to LSP failure and 3 delegable parameters that may be overridden by a PCE and trigger re signaling by RPD . All delegable LSP parameters may include a configured default value that RPD applies when for example a PCEP session terminates the PCE otherwise becomes unavailable or the PCE returns a delegation.

RPD sets up LSP described in LSP database by executing a resource reservation protocol which in this instance is RSVP B that signals other routers in the network to reserve resources and provide MPLS forwarding information to RPD for use in forwarding MPLS packets. Various instances of router may also or alternatively use RSVP TE or another Label Distribution Protocol LDP to signal LSPs. In addition RPD executes RPs B to receive traffic engineering information that affects the state of LSPs such as failed links and preempted resources that may result in a down state for LSPs. RPD may associate such LSP state information with corresponding LSPs in LSP database and may further directs path computation client B to send one or more LSP state reports to a PCE in response as described in further detail below.

Path computation client PCC B of control plane A mediates communication between RPD and a path computation element e.g. PCE of or . PCC B includes a PCE interface not shown that implements PCE communication protocol PCEP extensions to receive and send extended PCEP messages. The PCE interface also implements functionality for the operation of conventional PCEP such as path computation request reply messages.

Path computation client B establishes extended PCEP sessions with a PCE and sends via the extended PCEP sessions LSP state reports that include up to date LSP state for LSPs described in LSP state information. LSP state reports may be included in PCRpt messages. In this way PCC B maintains strict LSP state synchronization between router and the PCE which the PCE may use when computing paths for an overlay network that make use of the LSPs.

In addition PCC B may advertise router as allowing modification of delegable parameters. As a result LSP state reports sent by PCC B may in some case include a delegation that provides access rights to a PCE to modify parameters of the target LSP. In some instances the delegation may specify the particular parameters of the target LSP that are exposed for modification. PCC B may after delegating LSPs receive LSP update requests that specify LSP parameter modifications for one or more of the LSPs. LSP update requests may be included in PCUpd messages. PCC B in response notifies RPD of new parameters for target LSPs identified in LSP update requests. RPD may re signal the target LSPs in turn and as new LSPs are established switch traffic over to the new LSPs and send a notification to PCC B that the new LSPs have been successfully signaled. PCC B provides this updated LSP state in LSP status reports to a PCE with which router has extended PCEP sessions. Router thus extends existing RSVP TE functionality with an extended PCEP protocol that enables a PCE to set parameters for a TE LSP configured within the router. In this way router may implement an SPVC like model to allow a PCE to signal computed paths through a multi topology network thereby dynamically setting up end to end paths as requested by clients.

For example PCE may configure router to forward traffic along a path from an ingress network device e.g. router in this example to a service node such as service node in accordance with configuration data specified by PCE which may represent an external orchestrating device that computes and configures paths through the network and that does not forward any traffic through the network other than that used to compute and configure the paths through the network. The configuration data may be stored to various databases such as RIB TED and LSP DB to provide a few examples. The configuration data may define filters and other parameters that admit or deny traffic to path as described above. These filters may then be installed in flow table of VPN instance as filters . Filters may be associated with individual entries or may represent catch all filters to be applied for making classification decisions as noted above. In this sense flow table may represent a collection of filters for making decisions with respect to flows as to whether to admit or deny flows to VPN instance . Data plane B which includes at least one interface e.g. IFCs then forwards the traffic along path to enable the service node to apply one or more services to the traffic.

While not shown explicitly in the example of router may also function outside of the context of a software defined network where RPS B may receive routing protocol messages such as IS IS messages OSPF messages and or BGP messages to provide a few examples. These messages may include routing information regarding the state of various links and or paths through network . RPS B may parse this routing information from the messages and store the routing information to RIB . RPS B may then resolve the routing information including any routing information received via the routing protocol messages to generate forwarding information for one or more routing instances. RPS B may then install this forwarding information as one or more routing instances . Data plane B may receive packets corresponding to these non software defined network routing instances as represented by one or more of routing instances and forward these packets in accordance with these ones of routing instances . Thus while described above as performing aspects related to the configuration and operation of a path in a software defined network router may also operate according to standard routing protocols where router resolves routing information itself and configure one or more routing instances of forwarding information as opposed to an orchestrating device external from router configuring one or more of routing instances .

Some examples of service node may not include the full functionality described and illustrated. For instance some examples of service node may include different combinations of PCC B OpenFlow switch B and IRB interface rather than all such components. Moreover while described with respect to a particular network device e.g. a router aspects of the techniques may be implemented by any network device or combination of network devices. The techniques should therefore not be limited to the exemplary embodiments described in this disclosure.

Service node includes a control unit and interface cards A N collectively IFCs coupled to control unit via internal links. Control unit may include similar to control unit of router one or more processors not shown in that execute software instructions such as those used to define a software or computer program stored to a computer readable storage medium again not shown in such as non transitory computer readable mediums including a storage device e.g. a disk drive or an optical drive or a memory such as Flash memory random access memory or RAM or any other type of volatile or non volatile memory that stores instructions to cause the one or more processors to perform the techniques described herein. Alternatively or additionally control unit may again similar to control unit of router comprise dedicated hardware such as one or more integrated circuits one or more Application Specific Integrated Circuits ASICs one or more Application Specific Special Processors ASSPs one or more Field Programmable Gate Arrays FPGAs or any combination of one or more of the foregoing examples of dedicated hardware for performing the techniques described herein.

In this example and different from router described above with respect to the example of control unit is divided into three logical or physical planes to include a first control or routing plane A control plane A a second data or forwarding plane B data plane B and a third service plane C. That is control unit implements three separate functionalities e.g. the routing control forwarding data and service functionalities either logically e.g. as separate software instances executing on the same set of hardware components or physically e.g. as separate physical dedicated hardware components that either statically implement the functionality in hardware or dynamically execute software or a computer program to implement the functionality.

Service plane C executes or otherwise provides one or more service modules . Service modules may each perform the same service or different services depending on the configuration of service plane C. These services may include a firewall service a NAT service an IDP service an URL filtering service an encryption service and any other type of service that may be performed in a network. In accordance with the techniques described in this disclosure control plane A includes a service monitoring module that monitors these service modules identifying changes to service state such as a change in the URL being filtered or allowed change in VPN membership scope etc. . Service monitoring module may also monitor execution of service modules to determine whether any of these service modules are reaching processing and or memory thresholds. Service monitoring module may therefore generally monitor service modules to identify service specific information related to the application of one or more services by service modules .

When monitoring service modules service monitoring module may determine which service modules are servicing traffic associated with different established SEPs. That is one or more of service modules may be mapped to one or more of the SEPs. Service monitoring module may perform lookups in RIB to identify these mappings or otherwise query routing protocol daemon and or management interface which may store configuration data to identify these mappings. Service monitoring module may then determine service specific information on a per SEP basis by monitoring specific ones of service modules associated with SEPs. Service monitoring module may then generate service specific information that identifies applications of services to traffic associated with a particular SEP. Service monitoring module may generate service specific information such that this information identifying the SEP to which the corresponding service information corresponds.

Upon determining or otherwise generating this service specific information service monitoring module may then pass this information to routing protocol daemon which may invoke one or more of RPS B. The invoked one or more of RPS B may process the service specific information generating a routing protocol message that includes this service specific information. The invoked one or more of RPS B may then forward often flooding these messages to other routers and or switches that also implement the corresponding routing protocol thereby effectively creating a path specific feedback loop e.g. feedback loop to any device that supports the corresponding path.

Referring back to the example of router may receive this routing protocol message via a corresponding one of RPS B and parse the service specific information. RPS B may inspect the service specific information to determine whether router is associated with a SEP to which the service specific information corresponds. Assuming the service specific information corresponds to SEP operating over VPN instance the corresponding one of RPS B passes the service specific information which may also be referred to as feedback to feedback module executing within control unit . Feedback module may process the service specific information and interface with VPN module path computation client B OpenFlow protocol IF RIB and or TED to adapt SEP based on the received service specific information.

To illustrate feedback module may receive service specific information defining filter by pass states for various flows sent via SEP operating over VPN instance to service node which again may represent a more detailed example of service node shown in the example of . Feedback module may interface with VPN module to configure filter of VPN instance so as to redirect these by pass flows to their intended destination rather than to send these flows via VPN instance over SEP to service node .

As another example feedback module may receive service specific information identifying a change in VPN membership scope and in response interface with VPN module to update VPN instance to adapt SEP and more specifically filters to add or remove flows from VPN instance .

As yet another example feedback module may receive service specific information indicating some operational constraint on application of the service to flows being sent via SEP . These operational constraints may comprise a lack of physical resources to process these flows such as processor capacity or memory resources. The operational constraints may also comprise limits imposed on the operation of service node such as a maximum number of flows that service node may be permitted to process concurrently. In any event feedback module may be configured to interface with path computation client B and or OpenFlow protocol IF to request from PCE that a new path be established to another service node that performs a same or similar service as that performed by service node . In some instances feedback module may query an ALTO server to identify this other service node that performs the same or similar service as that performed by service node . Upon receiving the network identifier e.g. IP address for this other service node feedback module may interface with path computation client B and or OpenFlow protocol IF to request a new SEP to this other service node identified by the network identifier. Once this new path is configured in the manner described above feedback module may redirect one or more of the flows via this new SEP to the other service node updating filters and filters associated with the other SEP to implement this redirection of flows. Feedback module may continue to receive service related information from service node and may redirect flows until the operational conditions at service node are alleviated or within threshold levels.

In this sense feedback module may adapt SEP to accommodate changing service level conditions occurring at service node . By enabling adaption of SEP the techniques may promote more efficient network operation enabling more efficient bandwidth usage and potentially load balancing based on service level conditions in addition to network level conditions rather than strictly on network level conditions.

While described above with respect to an ingress router to SEP and a service node in many instances the techniques may be deployed such that feedback loop has multiple termination points. That is feedback loop may not necessarily terminate with ingress overlay switch B of SEP but any network devices positioned between ingress overlay switch B and service node may receive and respond to service specific feedback information. One example may be where two or more service nodes are daisy chained together with the first service node applying a first service and the second service node applying a second service. This second service may be different that the first service. To illustrate the first service may comprise a firewall service while the second service may comprise an IDP service. The second IDP service node may apply the IDP service and determine that one or more flows are malicious and should be dropped. The second IDP service node may utilize a feedback loop similar to feedback loop to communicate that these malicious flows should be dropped. The first firewall service node may then block these malicious flows at the firewall and possibly identify any derivative flows associated with the source address of the infected packets that should also be blocked. Additionally this feedback loop may extend back to the ingress device of the SEP which may also utilize this information to discard the particular malicious flows. Accordingly the techniques should not be limited to the examples provided above but may be implemented such that feedback loop provides feedback to any device that supports a SEP to which this feedback corresponds.

Moreover while not shown explicitly in the example of service node may also function outside of the context of a software defined network where RPS B may receive routing protocol messages such as IS IS messages OSPF messages and or BGP messages to provide a few examples. These messages may include routing information regarding the state of various links and or paths through network . RPS B may parse this routing information from the messages and store the routing information to RIB . RPS B may then resolve the routing information including any routing information received via the routing protocol messages to generate forwarding information for one or more routing instances. RPS B may then install this forwarding information as one or more routing instances . Data plane B may receive packets corresponding to these non software defined network routing instances as represented by one or more of routing instances and forward these packets in accordance with these ones of routing instances . Thus while described above as performing aspects related to the configuration and operation of a path in a software defined network router may also operate according to standard routing protocols where router resolves routing information itself and configure one or more routing instances of forwarding information as opposed to an orchestrating device external from router configuring one or more of routing instances .

In any event once configured router may receive traffic via IFCs where IRB interface may apply filters to admit or deny traffic to SEP and forward any admitted traffic via SEP to service node in the manner described above . Service node receives the traffic via SEP and invokes one or more of service module to apply one or more services to the traffic received via SEP . Service monitoring module executing on control unit of service node may monitor the application of the one or more services to the traffic generating service specific information in the manner described above . Service monitoring module may pass the service specific information to RPs B executing on control unit of service node which may generate a routing protocol message that includes the service specific information . RPs B may forward often by flooding the routing protocol message to the ingress network switch i.e. router in this example .

Router receives the routing protocol message and parses the service specific information from the routing protocol message . Router may invoke feedback module to determine whether this service specific information corresponds to a SEP for which router has been configured to forward traffic. Assuming for purposes of illustration that this service specific information corresponds to SEP feedback module may adapt SEP based on the service specific information in any of the number of ways described above .

PDU length field L stores a value indicating the entire length of the PDU including variable fields in octets. Remaining lifetime field M stores a value indicating a number of seconds before the link state protocol data unit LSP is considered expired. LSP ID field N stores a value identifying the system ID of the source of the LSP. Sequence number field O stores a sequence number of the LSP. Checksum field P stores a value defining a checksum of the contents of the LSP from the source ID to the end. P ATT LSPDBOL IS type fields Q T store a number of bits that are used to signal various functionalities and attributes of the source of the routing protocol message. More information regarding IS IS and IS IS PDUs is described in more detail in International Standard ISO IEC 10589 entitled Information Technology Telecommunications and Information Exchange Between Systems Intermediate System to Intermediate System Intra domain Routeing Information Exchange Protocol for Providing the Connectionaless Mode Network Service ISO 8473 dated Nov. 15 2002 the entire contents of which are hereby incorporated by reference.

While IS IS provides for extensions to the PDU in the form of variable length fields specified in accordance with a type length value TLV field format these extensible fields are reserved for communicating information related to the IS IS routing protocol. An extension to the IS IS routing protocol adapts IS IS for communicating generic information GENINFO that is not directly related to the operation of the IS IS protocol. This GENINFO extension is defined in a Network Working Group Internet Draft entitled Advertising Generic Information in IS IS dated Nov. 10 2010 the entire contents of which are hereby incorporated by reference. Using this extension to IS IS as one example the service specific information described above may be encoded in a TLV denoted as service specific information field . Service specific information field may include sub fields defining flags an application identifier and application IP address information as well as the service specific information. The flags subfield identifies how the service specific information is to be flooded as one example. The application identifier sub field identifies the identifier assigned to the application. The Application IP address information sub field identifies either or both the IPv4 or IPv6 address associated with the application. Using this information routers switches and other network devices may determine to which application the service specific information field corresponds.

While described above with respect to IS IS routing protocol the techniques may be implemented with respect to other routing protocols that provide for TLV or other fields to which service specific information may be stored. These other routing protocols may include OSPF and BGP to name a few examples. In this respect the techniques should not be limited to the IS IS routing protocol described above.

Various embodiments of the invention have been described. These and other embodiments are within the scope of the following claims.

