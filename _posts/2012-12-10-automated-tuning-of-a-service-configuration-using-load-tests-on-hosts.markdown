---

title: Automated tuning of a service configuration using load tests on hosts
abstract: Methods and systems for automated tuning of a service configuration are disclosed. An optimal configuration for a test computer is selected by performing one or more load tests using the test computer for each of a plurality of test configurations. The performance of a plurality of additional test computers configured with the optimal configuration is automatically determined by performing additional load tests using the additional test computers. A plurality of production computers are automatically configured with the optimal configuration if the performance of the additional test computers is improved with the optimal configuration.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09053070&OS=09053070&RS=09053070
owner: Amazon Technologies, Inc.
number: 09053070
owner_city: Reno
owner_country: US
publication_date: 20121210
---
Tuning the configuration of a service on a computer system to handle a desired load is typically a manual process. In other words a tuning process typically involves a user manually tweaking different attributes of the service or the underlying system in the hopes of improving the performance of the system running the service. The attributes to be tweaked may be as specific as the numbers of threads in different thread pools but may include any aspect of the service that might affect performance or throughput. Multiple attributes may need to be manually modified many times in an effort to improve the performance significantly especially if performance is dependent on multiple interrelated attributes. For heterogeneous multi host web services that have specific targets in terms of throughput latency or stability the tuning process may be especially complex and time consuming.

A typical approach to this manual tuning process involves trial and error. A user may making some initial guesses on optimal values put the service into production based on the guesses and manually analyze the load it can handle. The user may then tweak the values even further again based on guesswork. In some circumstances parts of the system will change dramatically over time thus making the original estimates outdated. However because this approach to tuning is manual and time consuming the tuning may not be performed on a regular basis. As a result outdated and inefficient settings may remain in place until they have significantly adverse effects on performance. When performance estimates are outdated or entirely absent hardware resources may be wasted on systems that are not operating optimally. For example in a fleet of 10 000 hosts a 10 improvement in throughput can mean a savings of 1000 hosts as well as savings in resources such as network bandwidth and power consumption.

While embodiments are described herein by way of example for several embodiments and illustrative drawings those skilled in the art will recognize that embodiments are not limited to the embodiments or drawings described. It should be understood that the drawings and detailed description thereto are not intended to limit embodiments to the particular form disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope as defined by the appended claims. The headings used herein are for organizational purposes only and are not meant to be used to limit the scope of the description or the claims. As used throughout this application the word may is used in a permissive sense i.e. meaning having the potential to rather than the mandatory sense i.e. meaning must . Similarly the words include including and includes mean including but not limited to. 

Various embodiments of methods and systems for automated tuning of a service configuration are described. Using the systems and methods described herein intelligent and automated tuning of a service may be performed to determine an optimal configuration on test computers before the optimal configuration is put into production. The optimal configuration may be rolled back if it adversely affects the performance of production computers. In one embodiment the automated tuning may be applied to any numerically configurable parameter that may affect performance throughput or stability. The optimal configuration may be determined such that a user specified performance goal is met and the user need not be aware of the specific configurable parameters that are tuned.

The load generator module may generate a plurality of test loads for use in the load testing of a service . For example if the service is associated with an electronic commerce e commerce merchant then the service may be configured to perform one or more suitable operations such as generating a web page e.g. a product description page for a product offered for sale by the merchant completing a sale or other transaction between the merchant and a customer verifying a payment presented by the customer etc. Each test load may comprise data associated with a plurality of transactions or other operations that are processed by the service . The test loads may vary in transaction frequency e.g. transactions per second . The test loads may be generated by sampling actual production transactions or by generating synthetic transactions. The functionality of the load generator module is discussed in greater detail below with respect to .

Turning back to the automated configuration tuning system may be communicatively coupled to a plurality of other computer systems any of which may be implemented by the example computing device illustrated in over one or more network s . The automated configuration tuning system may be communicatively coupled to a test host pool comprising a plurality of test computer systems. For purposes of illustration the test host pool is depicted in as including three test hosts and . However it is contemplated that any suitable number of test hosts may be used. Software implementing the service may be installed on and executed by each of the test hosts and .

The test host pool may be used to determine an optimal configuration for the service . The load testing module may use the test loads to perform load tests using one or more of the computer systems in the test host pool . In one embodiment a first set of load tests may be performed using a single one of the test hosts e.g. test host . Each of many test configurations of the service such as configurations and may be subjected to load tests using the test loads . In one embodiment a plurality of the test hosts and or may each be used for simultaneous and independent load testing of different test configurations. Each configuration of the service may comprise a different set of values for one or more configurable parameters of the service. An example of configurable parameters is discussed below with respect to . Turning back to in one embodiment each test configuration may be subjected to test loads of increasingly greater transactions per second. Based on the results of the load testing of the different test configurations the load testing module may select an optimal configuration from among the test configurations. The optimal configuration may comprise optimal values for the one or more configurable parameters of the service .

In one embodiment the load testing module may further validate the optimal configuration by performing additional load tests on a plurality of test hosts and or using the optimal configuration. The additional load tests may determine whether the optimal configuration is scalable from one host to many hosts. If the optimal configuration adversely affects the performance of the test host pool then the individual hosts in the test host pool may be reverted to an earlier configuration of the service . The functionality of the load testing module is discussed in greater detail below e.g. with respect to .

Turning back to the automated configuration tuning system may also be communicatively coupled to a production host pool comprising a plurality of production computer systems. For purposes of illustration the production host pool is depicted in as including three production hosts and . However it is contemplated that any suitable number of production hosts may be used. Software implementing the service may be installed on and executed by each of the production hosts and . The production hosts and may comprise computer systems used in conducting the primary or real world operations of an entity. For example if the entity is an e commerce merchant then the production hosts and may be used to process real world transactions with customers.

Once the optimal configuration has been selected and validated by the load testing module the optimal configuration may be deployed to the production hosts and . In deploying the optimal configuration the configurable parameters of the service may be set to the optimal values in each host in the production host pool . The performance monitoring module may then monitor the performance of the service with the optimal configuration in the production host pool . In one embodiment the performance monitoring module may receive performance data from a performance monitoring agent running on each production host and then analyze the performance data. If the performance monitoring module determines that the optimal configuration is adversely affecting the performance of the production host pool then the individual hosts in the production host pool may be reverted to an earlier configuration of the service .

As shown in a plurality of test loads may be generated. The test loads may be associated with data processed by the service whose performance is sought to be tuned. The generation of the test loads is discussed in greater detail below with respect to .

Turning back to as shown in a baseline performance of the service may be determined. In one embodiment the baseline performance may be determined by running baseline load tests of the service on one or more of the provisioned test hosts using an existing configuration of the service. The baseline performance may be determined for a single test host and also for a plurality of test hosts. The baseline load tests may use the test loads generated in . In another embodiment the baseline performance may be determined by monitoring the real world performance of production hosts processing production transactions. The baseline performance may measure any suitable performance attribute s such as memory usage processor usage network throughput network latency response time etc. In one embodiment the performance attribute s measured for the baseline performance may be relevant to a performance goal specified by a user.

As shown in one or more load tests may be performed on a single test host for each test configuration. The load tests may use the test loads generated in . Each test load may comprise data associated with a plurality of transactions or other operations that are processed by the service . For each test configuration the test loads may increase in transaction frequency e.g. transactions per second for each successive load test. In one embodiment the duration of each test may be user configured. The load testing is discussed in greater detail below with respect to .

Turning back to as shown in the optimal configuration may be selected from among the configurations tested in . In one embodiment the optimal configuration may be determined such that a user specified performance goal is met and the user need not be aware of the specific configurable parameters that are tuned. The optimal configuration may comprise optimal values for the one or more configurable parameters of the service .

The optimal configuration may then be deployed to the entire set of provisioned test hosts. In deploying the optimal configuration the configurable parameters of the service may be set to the optimal values in the other test hosts. As shown in one or more additional load tests may be performed on a plurality of the provisioned test hosts with the optimal configuration. The additional load tests may also use the test loads generated in . In performing the additional load tests a pre production performance of the test hosts may be determined. The pre production performance may measure any suitable performance attribute s such as memory usage processor usage network throughput network latency response time etc. that are relevant to the user specified performance goal.

As shown in the pre production performance of the test hosts with the optimal configuration may be compared to the baseline performance. The comparison may involve the numerical values measured for one or more specified performance goals. If the pre production performance is not better than the baseline performance then the tuning method may end and the optimal configuration determined in may be discarded.

If however the pre production performance is better than the baseline performance then as shown in the optimal configuration may be deployed to the production hosts. In deploying the optimal configuration the configurable parameters of the service may be set to the optimal values in the production hosts. Using the production hosts the service may then operate with the optimal configuration to process production traffic.

As shown in the performance of the production hosts may be monitored after the optimal configuration has been deployed. As shown in the performance of the production hosts with the optimal configuration may be compared to a previous performance of the production hosts with a previous configuration. The comparison may involve the numerical values measured for one or more specified performance goals. If the current performance is not better than the earlier performance then as shown in the production hosts may be reverted to the previous configuration. On the other hand if the performance of the production hosts is improved by the optimal configuration then the production hosts may be left to operate with the optimal configuration pending another tuning operation at a later time.

In one embodiment the method shown in may be performed automatically and according to a schedule or on a periodic basis. In one embodiment a cron like daemon or other suitable scheduling component may be used to invoke the automated service configuration tuning process at appropriate times. The schedule or periodic basis may be established by a user. A performance goal for the tuning operation may also be established by a user. In one embodiment any of the operations shown in may be performed automatically e.g. without manual instruction from a user or otherwise independent of direct user control and or programmatically e.g. by execution of suitable program instructions .

The Production Data Provider may sample real world production data e.g. data associated with production transactions . The sampled production data may be saved to a Test Data Repository TDR by a batch process. When test loads are generated the Production Data Provider may fetch the next transaction out of the TDR . Accordingly the Production Data Provider may be used in conjunction with the getNextTransaction method when real world transaction patterns are desired for use with the automated tuning.

The Distribution Probability Provider may generate synthetic test loads and may thus be used for modeling transaction patterns that are not easily found in current production data. In configuring the Distribution Probability Provider the user may define different operation types and the desired percentage distribution of the operations. In the example shown in the Distribution Probability Provider is configured such that transaction type X is called 20 of the time transaction type Y is called 70 of the time and transaction type Z is called 10 of the time. The load generator module may distribute the calls accordingly and the user may supply any suitable plug ins or other service specific program code to perform the calls. For example if the system is a RESTful web service that is currently getting 10 POST requests and 90 GET requests but it is expected that it will later receive 30 POST requests and 70 GET requests then the Distribution Probability Provider may more closely model the future pattern than the Production Data Provider .

In one embodiment the automated configuration tuning system may automatically detect the configurable parameters of a service along with the current parameter values and the range of potential parameter values i.e. the maximum and minimum values . The values of the parameters in the test configurations may then be assigned within the appropriate range. To implement this auto discovery functionality the automated configuration tuning system may include an administrative application programming interface API to modify the configurable parameters. In one embodiment each service being tuned may expose a debug hook that includes the following calls getAllVariables and setVariable variable value . The getAllVariables call may return a set of one or more variables e.g. parameters that can be tuned and the setVariable variable value call may set the indicated variable to the supplied value.

Each variable returned by getAllVariables may include the following data a unique ID a data type a minimum and maximum value to try and a priority value. The unique ID describes the particular variable e.g. ThreadPool DoingWorkX or ThreadPool DoingWorkY. The data type indicates a suitable data type of the parameter value such as an integer or a double precision floating point value. The priority value may allow parameters to be separated into tiers such as priority 1 and priority 2 where the automated tuning system may favor priority 1 variables to be optimized over priority 2 variables all other things being equal. The variables returned by getAllVariables and set by setVariable variable value may include any configurable parameter such as for example a number of threads in a thread pool a number of elements to process before stopping a percentage of records to sample a buffer size a number of database connections or any other parameter with a range that could potentially affect performance.

As shown in baseline load tests may be run based on current settings. As shown in the baseline performance and current variables may be stored. As shown in load tests may be performed for each priority and for each configurable variable. As shown in load tests may be run to determine the best value for the variable. The load testing process may start at a desired low transactions per second TPS run at that TPS for a specified amount of time measuring the latency of each individual call and gradually increase the TPS. The load testing process may compute percentile metrics and give the user the ability to specify complex requirements. For example the user may specify a requirement such as run until it is no longer the case that the P50 latency is 

As shown in the performance and best value for each variable may be stored. As shown in the best value may be selected. As shown in the current settings may be modified based on the selection of the best value for a particular variable. As shown in the load testing process may stay in the loop indicated in until the difference in performance is sufficiently small or until a timeout condition is reached. As shown in validation load tests may then be run based on the new settings. As shown in the optimized performance and new variables may be stored.

Some configurable parameters may be completely independent but others may affect one another. For example let the configurable parameters be x and y where originally x 10 and y 7. On the first pass the load testing process may determine that for y 7 the best value for x is 13. However the load testing process may then decide that for x 13 the best value for y is 9. The load testing process may then iterate through all the variables a second time and determine that for y 9 the best value for x is 21. If the solution never converges the load testing process will timeout as shown in .

Because some parameters may not scale linearly from a single host to multiple hosts the optimal configuration determined for a single host may be validated for multiple hosts before putting the optimal configuration into production. illustrate examples of performance variations for different values of a configurable parameter according to one embodiment. In particular illustrates an example graph indicating how the number of threads affects the latency per call. illustrates an example graph indicating how the number of threads affects the total throughput in transactions per second . In the example of if the user specified performance goal is that the P90 i.e. 90percentile latency is below 210 ms then the automated tuning system may recommend 8 threads. On the other hand if the user specified performance goal is to maximize overall throughput then the automated tuning system may recommend 11 threads for a maximum of 42 transactions per second but with a P90 of 260 ms .

In at least some embodiments a computer system that implements a portion or all of one or more of the technologies described herein such as the automated tuning configuration system may include a general purpose computer system that includes or is configured to access one or more computer readable media. illustrates such a general purpose computing device . In the illustrated embodiment computing device includes one or more processors coupled to a system memory via an input output I O interface . Computing device further includes a network interface coupled to I O interface .

In various embodiments computing device may be a uniprocessor system including one processor or a multiprocessor system including several processors e.g. two four eight or another suitable number . Processors may include any suitable processors capable of executing instructions. For example in various embodiments processors may be general purpose or embedded processors implementing any of a variety of instruction set architectures ISAs such as the x86 PowerPC SPARC or MIPS ISAs or any other suitable ISA. In multiprocessor systems each of processors may commonly but not necessarily implement the same ISA.

System memory may be configured to store program instructions and data accessible by processor s . In various embodiments system memory may be implemented using any suitable memory technology such as static random access memory SRAM synchronous dynamic RAM SDRAM nonvolatile Flash type memory or any other type of memory. In the illustrated embodiment program instructions and data implementing one or more desired functions such as those methods techniques and data described above are shown stored within system memory as code i.e. program instructions and data .

In one embodiment I O interface may be configured to coordinate I O traffic between processor system memory and any peripheral devices in the device including network interface or other peripheral interfaces. In some embodiments I O interface may perform any necessary protocol timing or other data transformations to convert data signals from one component e.g. system memory into a format suitable for use by another component e.g. processor . In some embodiments I O interface may include support for devices attached through various types of peripheral buses such as a variant of the Peripheral Component Interconnect PCI bus standard or the Universal Serial Bus USB standard for example. In some embodiments the function of I O interface may be split into two or more separate components such as a north bridge and a south bridge for example. Also in some embodiments some or all of the functionality of I O interface such as an interface to system memory may be incorporated directly into processor .

Network interface may be configured to allow data to be exchanged between computing device and other devices attached to a network or networks such as other computer systems or devices as illustrated in for example. In various embodiments network interface may support communication via any suitable wired or wireless general data networks such as types of Ethernet network for example. Additionally network interface may support communication via telecommunications telephony networks such as analog voice networks or digital fiber communications networks via storage area networks such as Fibre Channel SANs or via any other suitable type of network and or protocol.

In some embodiments system memory may be one embodiment of a computer readable i.e. computer accessible medium configured to store program instructions and data as described above with respect to for implementing embodiments of the corresponding methods and apparatus. However in other embodiments program instructions and or data may be received sent or stored upon different types of computer readable media. Generally speaking a computer readable medium may include non transitory storage media or memory media such as magnetic or optical media e.g. disk or DVD CD coupled to computing device via I O interface . A non transitory computer readable storage medium may also include any volatile or non volatile media such as RAM e.g. SDRAM DDR SDRAM RDRAM SRAM etc. ROM etc that may be included in some embodiments of computing device as system memory or another type of memory. Further a computer readable medium may include transmission media or signals such as electrical electromagnetic or digital signals conveyed via a communication medium such as a network and or a wireless link such as may be implemented via network interface . Portions or all of multiple computing devices such as that illustrated in may be used to implement the described functionality in various embodiments for example software components running on a variety of different devices and servers may collaborate to provide the functionality. In some embodiments portions of the described functionality may be implemented using storage devices network devices or special purpose computer systems in addition to or instead of being implemented using general purpose computer systems. The term computing device as used herein refers to at least all these types of devices and is not limited to these types of devices.

Various embodiments may further include receiving sending or storing instructions and or data implemented in accordance with the foregoing description upon a computer readable medium. Generally speaking a computer readable medium may include storage media or memory media such as magnetic or optical media e.g. disk or DVD CD ROM volatile or non volatile media such as RAM e.g. SDRAM DDR RDRAM SRAM etc. ROM etc. In some embodiments a computer readable medium may also include transmission media or signals such as electrical electromagnetic or digital signals conveyed via a communication medium such as network and or a wireless link.

The various methods as illustrated in the figures e.g. and described herein represent exemplary embodiments of methods. The methods may be implemented in software hardware or a combination thereof. In various of the methods the order of the steps may be changed and various elements may be added reordered combined omitted modified etc. Various of the steps may be performed automatically e.g. without being directly prompted by user input and or programmatically e.g. according to program instructions .

Various modifications and changes may be made as would be obvious to a person skilled in the art having the benefit of this disclosure. It is intended to embrace all such modifications and changes and accordingly the above description is to be regarded in an illustrative rather than a restrictive sense.

