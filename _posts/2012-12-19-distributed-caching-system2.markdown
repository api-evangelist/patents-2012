---

title: Distributed caching system
abstract: Embodiments of a distributed caching system are disclosed that cache data across multiple computing devices on a network. In one embodiment, a first cache system serves as a caching front-end to a distributed cluster of additional cache systems. The first cache system can distribute cache requests to the additional cache systems. The first distributed caching system can also serve as a cache server itself, by storing data on its own internal cache. For example, the first cache system can first attempt to find a requested data item on the internal cache, but, if the lookup results in a cache miss, the first cache system can search the additional cache systems for the data. In some embodiments, the first cache system is configured to identify additional requests for the first data item and duplicate the requested data item in order to respond to the additional requests, which allows for greater responsiveness to requests.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09621399&OS=09621399&RS=09621399
owner: Amazon Technologies, Inc.
number: 09621399
owner_city: Seattle
owner_country: US
publication_date: 20121219
---
Caching is computer technology that allows computer processes to be accelerated. Generally a computer uses multiple levels of memory types of varying sizes and speeds with cheaper memory being generally slower and bigger in storage size and more expensive memory being generally faster and smaller in storage size. As faster memory is generally small in storage size only a limited amount of data can be stored on the faster memory types in the computer. Generally described caching attempts to anticipate what data items are needed by the computer in the future and attempts to keep those data items in the limited amounts of the faster memory types in order to improve performance by reducing access times to the data items. These data items can be maintained in a cache data structure in the computer memory.

Generally described a cache e.g. an application cache includes a data structure that transparently stores data so that future requests for that data can be served faster. The data that is stored within a cache may include values that have been computed earlier or duplicates of original values that are stored elsewhere. If requested data is contained in the cache e.g. cache hit the request can be served by reading the cache instead of calculating the data or retrieving the data from elsewhere. If reading from the cache is faster than calculating or retrieving the data then the request is served faster. Otherwise e.g. cache miss the data has to be recomputed or fetched from its original storage location which can be comparatively slower. Generally the greater the number of requests that can be served from the cache the faster the overall system performance.

Generally cache sizes are small relative to bulk computer storage. Nevertheless caches have proven themselves useful in many areas of computing because access patterns in typical computer applications have locality of reference. For example references exhibit temporal locality if data is requested again that has been recently requested already. In another example references exhibit spatial locality if data is requested that is physically stored close to data that has been requested already. Thus caches can be beneficial despite being able to fit only a portion of data stored in the bulk computer storage.

Accessing slower memory types can cause computer processor unit CPU delay as the CPU waits for data items from the slower memory types to become available. This delay can be characterized as an expense that slows down the performance of a first computing system. One possible solution to this problem involves caching data items on faster memory types. However as storage sizes of the faster memory types are limited some searches for data items in the cache can result in a cache hit with the data item found and some searches can result in a cache miss where the data item is not found. Thus increasing the chances of the data item being found in the cache sometimes referred to as the hit rate can improve the performance of the computing system by reducing delays. For example the hit rate can be increased by making available to the first computing system caches found on other computing systems on a network thereby effectively increasing the storage size of the cache available to the first computing system and increasing the chance of a data item being found in the expanded distributed cache.

Embodiments of a distributed caching system DCS are disclosed that cache data items across multiple computing devices on a network. In one embodiment a first cache system sometimes referred to herein as a caching system of the DCS serves as a caching front end or client interface to a distributed cluster of the other cache systems in the DCS where the first cache system can receive and distribute cache requests to the additional cache systems as needed. The first cache system can also serve as a cache server itself by storing data items on its own internal cache. For example the first cache system can first attempt to find a requested data item on the internal cache but if the lookup results in a cache miss the first cache system can search the additional cache systems for the data item. In some embodiments the first cache system is configured to multiplex requests to each additional cache system over a single Transmission Control Protocol TCP socket which allows for network efficiencies and faster detection of failure. In some embodiments the first cache system is configured to identify additional requests for the first data item and duplicate the requested data item in order to respond to the additional requests which allows for greater responsiveness to requests.

In some embodiments the distributed caching system is configured to store session state identifiers in a networked cache while dynamically allocating requests to servers. Client devices can then resume secure sessions even if assigned to new servers as the new servers can obtain the session state identifiers from the distributed caching system. In at least some cases the client device can be authenticated without the server having to perform a full authentication thereby reducing the workload of the server and decreasing latency as the server can respond faster.

In some embodiments the cache systems of the distributed caching systems are spread over multiple data centers. In one embodiment cache writes at a cache system in one data center are distributed to other data centers. By propagating the cache writes across multiple data centers the caches at the different data centers include more recently accessed data thereby increasing the likelihood of cache hits.

Various aspects of the disclosure will now be described with regard to certain examples and embodiments which are intended to illustrate but not to limit the disclosure. Nothing in this disclosure is intended to imply that any particular feature or characteristic of the disclosed embodiments is essential. The scope of protection of certain inventions is defined by the claims.

Generally caches include copies of data items that are kept temporarily in the cache but that are primarily stored more persistently elsewhere on a primary data storage device or devices. Caches are usually significantly smaller than these primary data storage devices and thus cannot fit all the data items from the primary data storage device. Caches are also usually operating on cache memory that is significantly faster than the primary data storage device. Therefore caches typically attempt to include the most used data items from the primary data storage in order to improve the performance of a computing system.

In some embodiments the distributed caching system is logically positioned between various computing services and one or more client devices connected via a network . For example the distributed caching system may provide caching for a web server or other computing service that processes request from the client devices. The distributed caching system can receive data item requests from the client devices for example on a network interface e.g. Ethernet or other network interface card NIC 802.11a b g n receiver etc. and process those requests. In one example the client devices operate web browsers that request web page data items from the distributed caching system . In one embodiment the distributed caching system requests and or pre processes web pages on behalf of the web browser users. The distributed caching system may be part of a cloud computing service provider that hosts many distinct web sites of many different entities and the distributed caching system caches data items for those distinct web sites.

The distributed caching system can cache data items from a variety of primary data sources where the data items are primarily stored. For example data items can be primarily stored on one or more mass storage devices on the distributed caching system. Such mass storage devices can include mechanical drives e.g. hard disk drives optical disk tape solid state drives SSDs that use integrated circuit assemblies as memory to store data items persistently or combinations of the above e.g. hybrid drives . Cached data items can also be primarily stored on external systems. In some embodiments the distributed caching system is connected via a network to one or more servers databases and or other data repositories where the primary copies of the data items are stored. For example in the case of a cache miss the distributed caching system can search for the data items on other computing devices on the network

In addition to caching individual computers used by the distributed caching system can also be used to provide other functions and services such as running virtual machine instances providing computing resources such as computing power and storage running one or more web servers and or providing other computing services. For example the same computer may provide caching services as well as web server services.

In one embodiment a front end system for the distributed caching system accepts requests on behalf of the distributed caching system attempting to fulfill the request and or finding an external cache e.g. located on a separate computing device that contains the requested data items. The front end system can be connected via a first network e.g. the Internet to one or more client devices . The front end system can also be connected via a second network e.g. a data center or private network to one or more external caches operating on one or more back end cache systems 

In some embodiments multiple or each of the computing devices of the distributed caching system can act as a front end system . For example a load balancer or work distribution module may lie between the distributed caching system and the client devices . The load balancer may receive the requests from the client devices and assign them to a selected one of the computing devices of the distributed caching system through a selection algorithm e.g. round robin random hashing etc. . Once the selected computing device receives the request it can attempt to fulfill the request using its internal cache and if the data items are not on the cache the selected computing device can search on the network for a computing device that contains the requested data items. describes such embodiments in additional detail.

The distributed caching system can support multiple levels of cache lookup. For example the distributed caching system can provide multiple levels of application caches to applications running on application servers. In one two level embodiment the first level involves the front end system performing a local lookup of the requested data items. If the lookup results in a cache miss the front end system can proceed to the second level by searching on the network at the additional external caches for the data items. In some embodiments the first cache system is configured to multiplex requests to each external cache system over a single TCP socket or other negotiated streaming protocol connection e.g. Stream Control Transmission Protocol SCTP Unix domain sockets or other inter process communication IPC socket Synchronous Link Control SLC or the like which allows for network efficiencies and faster detection of failure. In one embodiment requests are multiplexed over a first connection protocol e.g. Unix domain sockets or IPC sockets for local communications in the front end system and a second connection protocol e.g. TCP for network communications.

As discussed above the front end system can act as a cache server responding to requests using data items from its own internal cache. The front end system may also act as a cache proxy receiving and or managing requests on behalf of the distributed caching system . In one embodiment the front end system distributes requests among remaining systems in the distributed caching system .

In one embodiment the front end system uses a hash function to consistently select a particular back end cache system to handle a given data item. For example the front end system can determine or generate a key from the request. It can then apply a hash function to the key to determine a particular back end cache system to send the request to. By applying the hash function the front end system can consistently identify the particular back end cache system whenever the same data items are requested. For example if a client device makes a first request for a first data item the front end system can select a first back end system to handle the request. If the client device or another client device makes a second request for the same first data item the front end system can select the same first back end system to handle the request. As the first back end system handled the first request the first back end will likely have the first data item in its cache and will not have to obtain the data item from primary storage thereby speeding up the response time.

Consistent hashing is one example hashing algorithm that can be used by the distributed caching system to map keys or data items to cache systems. In consistent hashing the mapping of keys to slots e.g. cache systems remains largely static even if the number of slots changes increased or decreases . For example when the number of slots changes on average only K n keys need to be remapped where K is the number of keys and n is the number of slots. Consistent hashing is based on mapping each object to a point on the edge of a circle or equivalently mapping each object to a real angle . Each available machine or other storage slot is mapped to many pseudo randomly distributed points on the edge of the same circle. To find where an object should be placed a system using consistent hashing finds the location of that object s key on the edge of the circle then walks around the circle until falling into the first slot it encounters or equivalently the first available slot with a higher angle . The result is that each slot contains all the resources located between its point and the next slot point. Consistent hashing is useful for the distributed caching system as the number of cache systems can change based on failures and or recoveries of the cache systems. Consistent hashing allows the distributed caching system to account for those changes efficiently.

Referring back to the embodiment with a dynamically determined front end system described above where the multiple systems of the distributed caching system are capable of acting as a front end in one embodiment each front end candidate system uses the same hashing function to determine which back end system is assigned to handle a request. For example if a first front end system receives a first request for the first data item it selects a first back end cache system. If a different front end system receives a second request for the same first data item it selects the same first back end cache system. By consistently selecting back end cache systems the hit rate of the distributed caching system will likely increase because the same back end cache system is handling the same requested data item regardless of from which front end system on the distributed caching system the request is received.

In some embodiments TCP is used by the distributed caching system to communicate between its components. TCP includes flow control and congestion control properties that are desirable in a distributed caching architecture given the large number of hosts that can be sharing a single instance e.g. 2 000 . Flow control can be an important part of high throughput request reply systems and can enable graceful slowdown in the face of overload. Lack of proper end to end flow control is a common cause for complete failure in the face of high traffic. For example in the absence of flow control a few misbehaving clients may have the ability to take down an entire fleet of cache systems or otherwise saturate the network . While the distributed caching system uses TCP in some embodiments in other embodiments the distributed caching system can use other protocols such as User Datagram Protocol UDP for its communications.

In one embodiment the distributed caching system implements a dead end point detection mechanism based on the notion of inactivity. For example a connection end point may be considered inactive if no new traffic is received after a specified amount of time. This allows the distributed caching system to shut down inactive TCP end points thereby increasing the number of TCP end points the distributed caching system can handle before running into TCP connection limits. One benefit of such an embodiment is more robustness in the case of SYN synchronize message floods such an embodiment can shut down connections thereby making it more difficult for the SYN flood to saturate the network.

In one embodiment the determination of inactivity of a TCP end point is based at least partly on how the TCP connection is used. For example if a TCP connection is being used to request or respond to a large data request the time out before endpoints of that TCP connection are considered inactive may be longer than if the TCP connection is being used for a small data request.

Various embodiments of the distributed caching system may implement additional features. For example the distributed caching system may allow randomization of retries of transactions. This can reduce contention or collisions in the network. In one embodiment the distributed caching system implements a stronger flow control process between the client and the responding system. For example clients read and then write. Servers write and then read. The TCP windows allow both the clients and servers to behave in a pipelined fashion thereby increasing network efficiency.

In one embodiment each computing system or some of the systems in the distributed caching system uses a caching structure to provide an in memory cache to users of each computer. In one embodiment the in memory cache is configured to provide high memory efficiency. Typically random access memory RAM is a limited resource on computing devices particularly for computing devices that provide services to multiple users e.g. multiple web server instances . A more memory efficient in memory cache can result in fewer network calls compared to a less memory efficient cache. For example higher memory efficiency can allow more data items to be stored on the in memory cache increasing the hit rate of the cache and thereby reducing the need to search external caches on the network for the data items. Typically local memory access is significantly faster accessing memory over the network thus avoiding network calls tends to increased performance.

The caching structure can be maintained on the memory of the computing system. The memory can include processor cache e.g. L1 L2 L3 etc. and or main memory e.g. DDR RAM SDRAM other types of RAM etc. . In some embodiments the caching structure uses multiple levels of caches with small fast caches backed up by larger slower caches. For example the caching structure may operate by checking a smallest level 1 L1 cache first if it hits the processor proceeds at high speed. If the smaller cache misses the next larger cache L2 is checked and so on before main memory is checked. In some embodiments the caching structure uses only a single cache memory type.

While some embodiments use a slab allocation technique or a best fit allocation technique to assign storage space in one embodiment the caching structure utilizes small fixed size storage blocks for storing data items. By using small fixed size storage blocks more data items can be stored on the same size of cache memory through higher efficiency. Generally if a stored data item does not fill a storage block the extra space is not used and is considered wasted. For example if a first cache uses 128 KB storage blocks and a second cache uses 1024 KB cache blocks then if the caches store a 128 KB block the first cache will have 0 128 KB 128 KB 0 wasted space 100 efficiency while the second cache will have 896 KB 1024 KB 128 KB 896 KB of wasted space 12.5 efficiency . The efficiency advantage of using smaller storage blocks can also extend to larger stored data items. Generally a data item can be subdivided into chunks equal to the block size with any remainder data taking up a last storage block. If the last storage block is not filled then space on the last block is wasted. Going back to the above example if the caches stores a 600 KB data item the first cache uses five storage blocks with 40 KB 5 128 KB 600 KB 40 KB of wasted space 93.75 efficiency while the second cache uses one storage block with 400 KB 1024 KB 600 KB 400 KB of wasted space 58.6 efficiency .

However using smaller blocks can increase access times for the cache. For example referring to the above example for the 600 KB data item five blocks need to be accessed from the first cache while only one block needs to be accessed on the second cache. Assuming the access times are roughly similar for different size blocks the access time for the first cache for the 600 KB data item may be up to 5 times longer than the second cache. In some cases accessing multiple blocks can be even more expensive if the blocks are stored in the memory cache in non contiguous blocks. Nevertheless the first cache may still be faster on average even if the access times are longer per operation if the cache hit rate of the first cache is sufficiently higher than the cache hit rate of the second cache. For example assume the first cache has an access time of 5X and a hit rate of 90 while the second cache has an access time of X and a hit rate of 80 and the cost of a cache miss is 100X where X is some arbitrary period of time. Then for 100 requests the first cache on average will take 14.5X to serve each request 5X 90 100X 10 14.5X . Meanwhile the second cache on average will take 20.8X X 80 100X 20 20.8X . Thus depending on the circumstances higher cache access times from using smaller block sizes can be more than offset by higher hit rates because of the generally significantly higher costs of accessing the slower primary storage device where the data items are primarily stored.

The caching structure can implement various cache algorithms also called replacement algorithms or replacement policies to manage a cache of information stored on the computer. When the cache is full the algorithm chooses which items to discard to make room for the new data items.

In some embodiments the caching structure can use a cache algorithm to determine which data items to discard from the cache. Some example cache algorithms include least recently used LRU most recently used MRU pseudo LRU random replacement segmented LRU SLRU 2 way set associative direct mapped cache adaptive replacement cache clock with adaptive replacement and multi queue caching.

As will be recognized the arrangement shown in represents just one of many possible ways that the distributed caching system may be arranged in a network. For example the illustrated networks may be different networks or part of the same network e.g. the Internet . In one embodiment the first network is a public network while the second network is a private network or vice versa. For ease of reference the disclosure generally uses the term network to refer to either or both networks 

The client computing devices may include for example personal computers PCs personal digital assistants PDAs cellular telephones laptops tablets e book readers and other types of devices that support web browsing or otherwise access online computing services. For example the client devices may be used by users to connect to various types of web sites such as sites related to shopping news organizations sports games product reviews or the like. These web sites may be hosted by various web servers.

As discussed above data items can be primarily stored and cached from a variety of data repositories that can be local to components of the distributed caching system or can be on networked or distributed systems. The data repositories may be implemented using any type or types of physical computer storage. For example such data repositories can include magnetic hard drives solid state drives or memory optical disc and or the like. Various data structures can be used to store the data items such as electronic files databases or other data structures.

In one embodiment the front end system includes a server TCP write lock for managing access to connections between client devices and server threads on the front end system. In one embodiment a lock also called a mutex is a synchronization mechanism for enforcing limits on access to a resource in an environment where there are many threads of execution. A lock can be used to enforce a mutual exclusion concurrency control policy. In one embodiment reading ends of a server TCP end point are acted on by only a single event loop while writing end points may be acted on by multiple loops or threads.

In some embodiments the server threads receive cached data from the distributed caching system and communicate the requested cached data to the client devices . The front end system may be running multiple event loops or threads to provide these caching services. In addition in some embodiments the front end system can provide various services to users in addition to caching services. For example such services can include storage of data performing computations serving web pages or the like. Some of the server threads may be used to provide these services. The services may be different services or instances of the same service.

In some embodiment the event loops are I O event driven loops. In one embodiment the front end system stripes established connections over a small set of threads each thread running its own private event loop. For example the system can associate a set of multiple threads to multiple network connections. When a request arrives on one of the network connections the request can be assigned to one of the associated multiple threads. This design reduces the number of wakeups needed to process a single request though it may increase the number of wakeups that the operating system kernel needs to perform for a small number of concurrent requests. When processing a large number of concurrent requests however the number of wakeups would be lower with the above configuration. Another advantage of the above configuration is that the state machine that represents a single client is not required to be thread safe. Buffers also do not migrate between threads which depending on the memory allocator in use can result in significant reductions in heap growth and allocation time.

In one embodiment the front end system uses a custom event dispatch loop. In one embodiment each event dispatch loop is bound to a thread and sockets hashed to one of many loops. Well behaving handlers have the ability to yield the event loop to prevent starvation of requests from other sockets. In some embodiments the custom event dispatch loop is configured to handle socket events signals and or timers.

In some embodiments the services are performed by server threads or other server process e.g. an application or virtual machine instance . In one embodiment in order to minimize memory contention server event handlers are only invoked via a single thread. Additionally run time statistics can be maintained using thread local counters. In one embodiment the server and client interaction are designed to maximize or increase flow control effectiveness using the techniques described above. The server process can include command parsers configured to minimize or reduce buffer allocation and copies. For each client the server process can dedicate a small user space queue that helps mitigate socket contention on the writing end. If the queue is full then the event handler for the socket can yield so that other clients may be serviced. A yielded event handler can be restarted after all other events have been processed.

As discussed above the front end system can include an internal in memory cache for storing data items on the front end system. When server threads need access to data items to fulfill a request they can check the cache to see if the data items have already been stored there. In the case of a cache miss the system can check the back end cache systems of the distributed caching system for the data items. As even networked caches are generally faster than accessing a primary storage device e.g. a hard drive or database the system can improve performance by attempting to use resources from the external caches if available.

In one embodiment the front end system includes a DCS client that handles requests that resulted in a cache miss in the internal cache . The DCS client can identify a backend cache system on which the requested data items may be stored. In one embodiment the DCS client generates or determines a key based on the requested data item and uses a hash function to identify a candidate backend system that may contain the requested data item. In one embodiment the DCS client includes or has access to a list of backend cache systems on the distributed caching system including the network location e.g. IP addresses of the backend cache systems.

In some cases the candidate backend system may not store the data item because of data time outs if the data item has not been requested before if the data item was displaced from the cache by newer data items etc. In those cases the network cache request results in another cache miss and the front end system can obtain the requested data item from a primary storage device. However if the candidate backend system does include the data item then the candidate backend system can return the data item from its internal cache e.g. a cache hit to the front end system.

The DCS client may take advantage of the separation between the reading and writing halves of a TCP socket when it dispatches and or receives response from the backend cache systems. As discussed above in one embodiment any requests that cannot be fulfilled by the local cache are delegated to the locally running DCS client . The front end system can continue to serve requests that have arrived after the delegated request. Once a reply arrives the DCS client can directly write the reply to the writing half of the originating client s TCP socket as well as save the reply it to the local cache . In one embodiment if the originating client disconnects the system still saves the result to the local cache so that it can be available for future transactions such as if the originating client reconnects.

In one embodiment the DCS client uses a single TCP socket per end point. For example the distributed caching system can multiplex multiple requests to a back end cache system over the single TCP socket. For example when the client is processing a first request it can determine if other requests are pending for the same back end cache system and aggregate that request over the single TCP socket. By doing so the DCS client can reduce the number of active connections any single front end or back end cache system would need to handle. As concurrent requests to the same end point can be aggregated this can also reduce the number of outbound network packets reducing load on the network. In addition using a single TCP socket can result in faster TCP ramp up and potentially offers better network utilization by allowing more aggressively pipelined requests on the fly. In cases where the system provides virtual machine instances having a single TCP socket per end point frees users of the virtual machine instances from having to manually tune the size of the connection pool.

The DCS client can use a callback model to notify callers of completion of requests. In some cases aggregated requests over a single connection may be received and processed in a different order than when sent. Requests can be written directly to the socket by the DCS client if the socket is writable and an event loop is not concurrently writing. This can save the delay incurred from waking an event loop to write the request to the TCP socket connected to the requesting client device.

The front end system can also include a client TCP write lock for controlling access to TCP connections between the backend cache systems and the front end system . In one embodiment the front end system shuts down TCP connections e.g. to client devices or backend systems that are deemed inactive in order to control the amount of active TCP connections existing concurrently. Many operating systems have limits for the number of active TCP connections and by managing the connections the front end system can enhance its responsiveness to requests e.g. by not having to wait for TCP connections to become available if the limit is reached .

As discussed above in some embodiments in order improve throughput a set of event loops are assigned or striped to a particular client device . When a data item is received from the distributed caching system that needs to be sent to the client device one of the event loops can be dynamically assigned to process the data item. For example if other event loops are busy servicing other clients the least busy event loop may be selected. The even loop can then acquire the server lock associated with the client device so that another event loop does not attempt to send the same data item to the client device . The use of the server write locks can ensure that effort between the event loops are not duplicated. In some embodiments the client TCP write locks of also serve a similar purpose for connections with the back end cache systems 

Beginning at block the front end system receives a first request for a first data item. For example the first request may come from a client device or a server thread handling a request from the first client device with the request received on an inter process communication interface e.g. a syscall API etc. . In some cases the request is received on a networking interface of the front end system as part of network packets sent over the network by the client device . A thread can then process the request and provide the request to the inter process communication interface. One example transaction is a web page or web content request where a browser on the client device is requesting a web page and the distributed caching system provides caching for a web server. The web server or a thread associated with the web server after receiving the request can request that the front end system provide the first data item. Other types of transactions may also be handled by the distributed caching system . As discussed above caching data items can speed up a wide variety of data transactions.

At block the front end system determines whether the first data item is stored in local in memory cache sometimes referred to herein as an internal cache or a local cache . In one embodiment the front end system identifies a first key corresponding to the first data item. For example the distributed caching system can obtain or derive some type of identifier e.g. session ID user ID URL filename etc. from the first request. The front end system can then determine whether the key and a value associated with the key are in the cache. For example the cache can include a hash table or similar structure and the system checks if the hash table includes an entry for the key by applying a hashing function to the key.

At block based on the determination the front end system proceeds to block or block . In some cases the first data item is in the local in memory cache e.g. a cache hit and the system proceeds to block . In some cases the first data item is not in the local in memory cache e.g. a cache miss and the system proceeds to block .

At block the front end system identifies from a plurality of external caches an external cache designated to store the first data item. In one embodiment the system applies a hashing function e.g. consistent hashing or other deterministic function to the data item request or an identifier derived or generated from the data item request to identify the external cache. In one embodiment the same data item request maps to the same external cache thus if the first data item has previously been requested the external cache may still have the first data item cached.

At block the front end system determines whether the external cache is storing the first data item. In some cases the first data item may not be on the external cache. For example the first data item may not have been requested before thus the first data item has not yet been loaded onto the external cache from permanent storage. If the data item is not on the external cache the routine proceeds to block . If the data item is on the external cache the routing proceeds to block .

At block the front end system obtains the first data item from the external cache assuming the external cache has the data item. In some embodiments if the external cache does not have the first data item stored the backup cache system on which the external cache resides or the front end system can obtain the first data item from a primary data storage. In some cases the primary data storage may be a mass storage device on the same computing system or computer as the front end system . In some cases the primary data storage may be a storage device located on the network . For example the primary data storage may be part of an external database or server. In other embodiments the front end system provides a notice of a cache miss rather than obtaining the first data item from the primary data storage.

At block the front end system stores the first data item in its in memory cache. In some cases the cache may be full and the front end system identifies storage blocks in the cache that can be replaced with the first data item. For example the system may identify the least recently used blocks and then select those blocks for replacement. However as discussed above may different cache algorithms can be used to identify which storage blocks on the cache to replace. The routine can then proceed back to block and perform the functions described above for block .

At block the front end system provides the results e.g. first data item or a cache miss result . For example the front end system can provide the first data item to the source of the request e.g. a client device a web server a requesting thread another service on same computer or the like if the first data item was found on the internal cache or the external cache. If the first data item was not found either on the internal cache or the external cache the front end system can inform the requestor e.g. a requesting thread that the cache request resulted in a cache miss. The requesting thread can then try to obtain the data from the primary data storage.

At block the front end system optionally provides the first data item to other requestors. In many situations there can be multiple requestors waiting for the same data item. For example if the first data item is a news article hosted on a web site many users of the web site may have requested the news article particularly if the article is popular. In such situations the front end system can in response to receiving the first data item determine if other requestors are waiting for the first data item. For example the front end system can obtain or generate a key e.g. using a hash function associated with the first request. The system can then determine if additional outstanding requests are associated with the same key for example by generating keys for those requests and checking if those keys match the key for the first request. The front end system can then provide the first data item to all or some of the outstanding requests. By checking if the first data item is responsive to other requests the front end system can reduce the latency of responses to those other requests because the other requestors can piggyback off the response to the first requestor. The routine can then end.

For illustrative purposes an example scenario describes one embodiment of a response duplication process in additional detail. In the scenario the front end system obtains a first data item from a first cache e.g. internal or external cache the first data item responsive to a first request from a first requestor e.g. a client device or a thread . Meanwhile a second request for the first data item is received from a second requestor e.g. a client device or a thread before the first data item is retrieved. When the first data item is received the front end system can determine that the second request is pending and can subsequently send the first data item to the first requestor and the second requestor.

In some embodiments the front end system in response to receiving the second request sends out a second cache request for the first data item if the first cache request responsive to the first request is still outstanding. That is there may be two or more if additional requests are received outstanding cache requests. In such cases the front end system can process whichever cache response for the first cache request or second cache request is received back first and provide the first data item to the first and second requestors. When the trailing cache response arrives the front end system can discard that cache response since the first data item has already been provided.

Referring back to block if the in memory cache includes the first data item the routine proceeds to block . At block the front end system obtains the first data item from the cache. The routine can then proceed to block and block perform the operations described above and then end.

In the illustrated embodiment the request distributor is configured to load balance or otherwise distribute workload to the proxy fleet . In one embodiment load balancing is a computer networking methodology to distribute workload across multiple computers or a computer cluster network links central processing units disk drives or other resources to achieve efficient resource utilization increase throughput reduce response time and avoid overload. In addition using multiple components with load balancing instead of a single component may increase reliability through redundancy. The request distributor can include dedicated software or hardware such as a computing system multilayer switch or a Domain Name System server.

In one embodiment the proxy systems are similar to or include at least some of the functionality of the front end systems of . In the illustrated embodiment the cache systems include a local cache and a services module for providing user services such as processing Secure Sockets Layer SSL or TSL connection requests. For example the proxy system may include an application or virtual machine instance configured to provide some service such as storage computation and or web hosting. As discussed above the proxy fleet can include physical computing systems and or virtual computing systems operating on physical computing systems. For example the proxy fleet in one embodiment can be implemented as a group of virtual machine instances operating on computers in a data center.

Transport Layer Security TLS is a cryptographic protocol that provides secure communications over the Internet. TLS encrypts the segments of network connections at the Application Layer for the Transport Layer using asymmetric cryptography for key exchange symmetric encryption for confidentiality and message authentication codes for message integrity.

TLS is the successor to SSL another cryptographic protocol. Several versions of the TLS protocols are in use in applications such as web browsing electronic mail Internet faxing instant messaging and or voice over IP VoIP . Transport Layer Security formerly SSL is an Internet Engineering Task Force IETF standards protocol first introduced as IETF Request for Comments RFC 2246 in January 1999. Since that time there have been numerous informational and standards track RFC s introducing new versions options and extensions including but not limited to RFC 2818 3436 3546 4346 4347 5077 5216 5246 5487 6066 6091. A specific embodiment herein is related to TLS session re use as introduced in RFC 5077.

The following describes the operation of an embodiment of the TLS protocol in which a client is authenticated via TLS using certificates exchanged between both server and the client during a handshake process. In TLS a server proves its identity to the client. The client may also need to prove its identity to the server. Public key infrastructure PKI the use of public private key pairs is the basis of this authentication. The exact method used for authentication is determined by the cipher suite negotiated.

In one embodiment of TLS the client and server exchange random numbers and a special number called the Pre Master Secret. These numbers are combined with additional data permitting client and server to create their shared secret called the Master Secret. The Master Secret is used by client and server to generate the write MAC secret which is the session key used for hashing and the write key which is the session key used for encryption. The client and server can then exchange application data over the secured channel they have established. All messages sent from client to server and from server to client are encrypted using session key.

Authentication can be computationally complex. For example authentication can include public key operations e. g. RSA that are relatively expensive in terms of computational power. The TLS protocol provides a secure shortcut in the handshake mechanism to avoid at least some operations. In an ordinary full handshake a session ID or other session state identifier is generated for the session and stored by the server. In one embodiment the client associates this session ID with the server s IP address and TCP port so that when the client connects again to that server it can use the session ID to enable a shorter handshake process. In the server the session ID maps to the cryptographic parameters previously negotiated and or other session data from the previous session such as the master secret . In one embodiment both sides must have the same master secret otherwise the resumed handshake will fail beneficially this can prevent an eavesdropper from using a session ID . This type of resumed handshake is sometimes called an abbreviated handshake or a restart handshake.

Referring back to the distributed caching system in one embodiment the distributed caching system acts as a shared caching system for the proxy fleet enabling TLS sessions or other secured sessions to be resumed or session data to be otherwise reused. By utilized a shared caching system client devices can be transparently transferred to other proxy systems allowing different proxy systems to serve client requests even if a particular client device authenticated with a first proxy system but then is subsequently assigned to a second different proxy system e.g. during a second session . By allowing different proxy systems to handle subsequent requests from a particular client device the distributed services system can better manage resources for example by allowing subsequent requests to be assigned to less busy proxy systems .

For example after the user is authenticated with the first proxy system the first proxy system can save the session ID into a shared cache system of the distributed caching system so that when the second proxy system handles another transaction from the client it can obtain the session ID from a shared cache in the distributed caching system . Generally TLS authentication is computationally complex such that reading the session ID from a cache instead of re authenticating can improve system performance and reduce latency.

Starting at event the client device begins a first TLS transaction and sends a request to the request distributor . At event the request distributor determines which proxy system to assign the request to for processing. For this example the request distributor assigns the request to a first proxy system . As the client device is not resuming an existing TLS session in this example it goes through TLS authentication with the first proxy system . During the authentication a session ID is generated. At event the first proxy system identifies a designated cache system from the distributed caching system for storing the session ID. As discussed above in the designated cache system can be selected based on a hashing function e.g. consistent hashing . For purposes of this example the designated cache system is cache system . However in some cases the designated cache system could be any cache system of the distributed cache system . At event after the authentication is completed the first proxy system can provide the requested service and complete the first transaction. In some embodiments the first proxy system responds directly to the client device while in some embodiments the first proxy system provides a response to the request to the request distributor which then provides the response to the client device .

At event the client device begins a new second transaction. Some time may have passed since the first transaction and the request distributor at event reassigns the second transaction to the second cache system . The second cache system can attempt to speed up the authentication process by checking whether a session ID is stored for the client device indicating the device was recently authenticated. At event the second proxy system determines if the session ID is cached in the distributed caching system . In one embodiment the second proxy system checks its local cache for the session ID and if not found there it identifies the designated cache and requests the session ID from the external designated cache system . In one embodiment the second proxy system checks the local and external caches by performing a routine that is similar to the routine described in . At event the second proxy system receives the session ID from the cache system assuming the session ID is still stored by the system . The second proxy system can then complete authentication of the client device . At event the cache system performs the requested function and completes the second transaction. In some implementations the second proxy system responds directly to the client device while in other implementations the second proxy system provides a response to the request to the request distributor which then provides the response to the client device .

Beginning at block the proxy system receives a TLS connection request from a first client device. For example the first request may come from a client device with the request transmitted over the network received and routed by a request distributor and sent to a networking interface or another interface e.g. an application programming interface or application of the proxy system . Assuming the client device was previously authenticated a session state identifier e.g. a session ID or other identifier associated with session state data may still be stored on the distributed caching system . The proxy system can search for the cached session ID rather than performing a re authentication. By doing so the system can improve performance and reduce latency.

At block the proxy system determines whether a session state identifier and or session data is stored on the system s local cache . For example the distributed caching system can obtain or derive some type of key or identifier e.g. user ID URL filename etc. from the first request. It can then determine whether the key and a value associated with the key are in the cache. For example the cache can include a hash table or similar structure and the proxy system checks if the hash table includes an entry for the key by applying a hashing function to the key.

At decision block based on the determination the proxy system proceeds to block or block . In some cases the session state identifier is in the cache e.g. a cache hit and the system proceeds to block . In some cases the session state identifier is not in the cache e.g. a cache miss and the system proceeds to block .

At block the proxy system identifies from a plurality of external caches an external cache e.g. a cache system of the distributed caching system designated to store the first data item. In one embodiment the cache system applies a hashing function e.g. consistent hashing to the data item request or an identifier derived or generated from the data item request to identify the external cache. In one embodiment the same data item request maps to the same external cache thus if the first data item has previously been requested the external cache may still have the first data item cached.

At decision block the proxy system determines whether the session state identifier is stored on the external cache. In some cases the session state identifier is in the external cache e.g. a cache hit and the system proceeds to block . In some cases the session state identifier is not in the cache e.g. a cache miss and the system proceeds to block .

At block the proxy system obtains the session state identifier from the external cache. In some cases the session state identifier is associated with additional session data and in some embodiments the proxy system obtains the session data with the session state identifier from the external cache. In some cases the session state identifier is sufficient to allow the proxy system to authenticate the client device without additional session data.

At block the proxy system stores the session state identifier in the system s local cache. In some embodiments the proxy system stores additional session data on the local cache.

At block the proxy system authenticates the TLS connection. The proxy system may then securely communicate with the client device and can receive and fulfill requests from the client device. The routine can then end.

Referring back to block if the local cache includes the session state identifier the routine proceeds to block . At block the proxy system obtains the session state identifier from the local cache. The proxy system may also obtain additional session data associated with the session state identifier. The routine can then proceed to block perform the operations described above and then end.

Referring back to block if the external cache does not store the session state identifier the routine proceeds to block . At block the proxy system performs a full handshake or otherwise re authenticates the client device. For example the proxy system can treat the request as a new request and go through the complete TLS connection setup process. The routine can then end.

This technique can be particularly beneficial in systems such as the disturbed services system of where a server is dynamically selected from a group of servers to respond to requests from client devices. In such a system a client reconnecting to the distributed services system can be assigned to a different server than it was originally connected to before the initial handshake transaction was interrupted. However by saving the negotiation data to the distributed caching system a newly assigned second server can obtain the saved negotiation data and transparently resume the handshake operation even if the secure transportation protocol does not support a resumption feature.

In event the client sends a client hello message to server . In event sever then responds to the client hello message with a server hello message. In event server also transmits the handshake data received and or sent by server to the distributed caching system .

In event server sends a certificate to the client. Such certificates can be used the calculation of encryption keys. At event server updates the handshake data saved in the distributed caching system . At this point an interruption event occurs. For example packets sent between the client and server may have been dropped or lost causing a time out. Other possible interruptions include crashes restarts or other failure events.

In event the client sends a certificate or otherwise responds to the previous message from server . For example the client can send the next message required by the protocol or the client can resend the previous message if no response was received from server due to the interruption event.

In the illustrated scenario the client is assigned to server subsequent to the interruption event. For example server may have crashed and is still not available to resume handling transactions. Server retrieves the handshake data stored by server from the distributed caching system. In event the client and server complete the handshake.

In another scenario not shown the client is again assigned to server after the interruption event. However in the scenario server lost any local copies of the handshake data after the interruption event. For example server may have crashed or rebooted losing non permanently stored data. In this scenario server can still retrieve the handshake data from the distributed caching system and complete the handshake transaction with the client.

One scenario where spreading cache writes can be beneficial is where a web site is hosted by a distributed services system operating on multiple data centers. For example the web site may be hosted on data centers on the U.S. east coast and on the U.S. west coast in order to reduce latency and better serve visitors of the web site across the country. As the east coast and west coast are different geographic regions on different time zones visitors on the east coast will likely begin visiting the web site earlier than visitors on the west coast. However if cache updates caused by requests on the east coast data center which is on an earlier time zone are propagated to the west coast data center then when visitors begin requesting data items from caching systems on the west coast data center the caching systems will already be pre populated with useful data items thereby increasing the chances of cache hits occurring in the caching systems in the west coast data centers.

In one embodiment while cache writes are propagated across multiple data centers cache reads remain local to a particular data center. Requesting data from a different data center can be expensive as network packets may have to be transmitted over significant distances over multiple networks increasing the chance that packets can be lost. By keeping reads local to the data center high responsiveness to client requests can be maintained. Cache writes meanwhile can be performed separately from responding to client requests thus propagating cache writes does not necessarily affect responsiveness to client requests.

Starting at event a first client device requests a first data from a first cache system in a first data center . The first cache system can then check caches e.g. local caches or external caches associated with the first cache system in the first data center. Assuming the first data item is not in the caches or if the first data item is not up to date the first cache system can retrieve the first data item from primary storage.

At event the first cache system can then update the one or more caches e.g. the local cache in the first data center with the first data item. For example the first cache system can load the first data item into the one or more caches or it can update a stale entry for the first data item in the one or more caches.

At event the first cache system can then provide the first data item to the client device in response to the original request. At event and event the first cache system can transmit the cache update for the first data item to a second data center and a third data center respectively in order to cause caches at those data centers to be updated. As discussed above the cache writes can be performed separately from responding to client requests thus in some embodiments event and or are performed before event and vice versa. Referring back to event a second cache system at the second data center receives the cache update and updates one or more caches associated with the second cache system. Likewise referring back to event a third cache system at the third data center receives the cache update and updates one or more caches associated with the third cache system.

At event the second cache system receives a second request for the first data item from a second client device . As cache s in the data center have been updated as a result of the cache update propagation the second cache system can find the first data item in the cache s at the second data center . At event the second cache system provides the first data item retrieved from the cache s to the second client device

While the above disclosure has discussed specific embodiments of the distributed caching system many variations of the distributed caching system are possible. For example different data structures can be used instead of or in addition to the data structures described above. For example rather than a hash table the in memory cache may use a b tree b tree binary search tree skip list or other data structure. In another example data stored in the distributed caching system may be encrypted or unencrypted.

As described above the distributed caching system can be implemented with one or more physical servers or other computing machines such as several computing machines interconnected via a network. Thus each of the components depicted in the distributed caching system can include hardware and or software for performing various features. In one embodiment the distributed caching system is implemented on a computing system that hosts a web site or collection of web sites that the system monitors.

The distributed caching system can include one or more servers for receiving and responding to requests from a network such as requests to process session records. The one or more servers can include web servers application servers database servers combinations of the same or the like.

The processing of the various components of the distributed caching system can be distributed across multiple machines networks and other computing resources. The various components of the distributed caching system can also be implemented in one or more virtual machines rather than in dedicated servers. Likewise data repositories can include represent physical and or logical data storage including for example storage area networks or other distributed storage systems. Moreover in some embodiments the connections between the components shown represent possible paths of data flow rather than actual connections between hardware. While some examples of possible connections are shown any of the subset of the components shown can communicate with any other subset of components in various implementations.

In some embodiments the distributed caching system may be configured differently than illustrated in the figures above. For example various functionalities provided by the illustrated modules can be combined rearranged added or deleted. In some embodiments additional or different processors or modules may perform some or all of the functionalities described with reference to the example embodiment illustrated in the figures above. Many implementation variations are possible.

Other types of interactions additionally or alternatively between the distributed caching system and the users and or user systems are possible in addition to those described above. For example a distributed caching system interaction can be received directly from a user or administrator e.g. via an interactive console web browser or other GUI provided by the distributed caching system or from an executing program. In some embodiments users may interact with the distributed caching system using other types of interfaces and in other ways.

In some embodiments the distributed caching system and its components are executed or embodied by one or more physical or virtual computing systems. For example in some embodiments a server computing system that has components including a central processing unit CPU input output I O components storage and memory may be used to execute some or all of the components of the distributed caching system . The I O components can include a display a network connection to the network a computer readable media drive and other I O devices e.g. a keyboard a mouse speakers etc. . In some embodiments the distributed caching system may be configured differently than described above.

An embodiment of the distributed caching system can be stored as one or more executable program modules in the memory of the server and or on other types of non transitory computer readable storage media and the distributed caching system can interact with computing assets over the network . In one embodiment the distributed caching system may have additional components or fewer components than described above. For example the distributed caching system may be built on top of existing software development or testing systems and designed to coordinate the actions of the existing systems.

In an example embodiment a system for distributed data caching comprises 1 an inter process communication interface for receiving requests the requests including a first request for a first data item from a first client 2 computer memory for storing a local cache and one or more processors configured to determine whether the first data item is stored in the local cache in response to determining that the first data item is not on the local cache determine a first external cache from a plurality of external caches the first external cache designated to store the first data item transmit the first request for the first data item to the external cache the first request sent with a second request for a second data item the second request determined to be designated for the external cache the first request and the second request transmitted over a single negotiated streaming protocol connection socket receive the first data item from the first external cache and provide the first data item to the first client.

In another example the above system wherein the one or more processors are configured to detect inactive connections and close the inactive connections.

In an additional example the above system wherein the single negotiated streaming protocol connection socket is a transmission control protocol TCP socket or an inter process communication IPC socket.

In another example the above system wherein the one or more processors are configured to associate multiple threads that handle requests from clients to a plurality of network connections to the clients and dynamically assign a request received on one of the plurality of network connections to one of the associated multiple threads.

In an additional example the above system further comprising a distributed caching client configured to obtain the first data item from the external cache and transmit the first data item directly on a network connection to the first client without waking a thread assigned to the first client.

In another example the above system wherein the one or more processors are configured to determine a first external cache from a plurality of external caches by obtaining an identifier based at least partly on the first request and applying a deterministic algorithm to the identifier such that the deterministic algorithm identifies the first external cache.

In an additional example the above system wherein the first data item is received prior to the second data item.

In another example the above system wherein the first data item is received after the second data item.

In an example embodiment a method of caching data comprising receiving on a first caching system a first request for a first data item from a requestor identifying a first external cache designated to store the first data item from a plurality of external caches transmit the first request for the first data item with a second request for a second data item the second request determined to be designated for the external cache the first request and the second request aggregated over a single protocol connection to the external cache receiving the first data item from the first external cache and providing the first data item to the requestor.

In another example the above method wherein the single protocol connection comprises a single negotiated streaming protocol connection. Further in one example the above method wherein the negotiated streaming protocol connection is a Transmission Control Protocol TCP connection. Additionally in another example the above method wherein the negotiated streaming protocol connection uses a UNIX domain socket.

In an additional example the above method further comprising detecting inactive connections to one or more external caches and attempting to connect to one or more external caches using a background thread of execution.

In another example the above method further comprising associating multiple threads that handle requests from requestors to a plurality of network connections to the requestors and dynamically assigning a request received on one of the plurality of network connections to one of the associated multiple threads.

In an additional example the above method wherein identifying the first external cache from a plurality of external caches comprises obtaining an identifier based at least partly on the first request and applying a deterministic function to the identifier wherein the deterministic function identifies the first external cache.

In another example the above method wherein the requestor is a service module configured to handle requests received from a plurality of client devices.

In an additional example the above method wherein the local cache stores data using fixed size storage blocks.

In another example the above method wherein the local cache stores data using a slab allocation technique or a best fit allocation technique.

In an additional example the above method wherein identifying the first external cache designated to store the first data item from a plurality of external caches further comprises determining whether the first data item is stored in a local cache of the first caching system and in response to determining that the first data item is not on the local cache identifying the first external cache from the plurality of external caches. Further in one example the above method further comprising storing the first data item on the local cache.

In an example embodiment non transitory computer storage having stored thereon instructions that when executed by a computer system cause the computer system to perform operations comprising identifying a first external cache designated to store a first data item from a plurality of external caches the first data item requested in a first request identifying a second request for a second data item destined for the first external cache the first external cache designated to store the second data item transmitting the first request and the second request together over a single connection socket to the external cache and receiving the first data item or the second data item from the first external cache.

In another example the above non transitory computer storage wherein the first request and the second request are multiplexed over a single transmission control protocol TCP socket or an inter process communication IPC socket.

In an additional example the above non transitory computer storage wherein the first request and the second request are multiplexed over a first connection protocol for local communications in the computer system and a second connection protocol for network communications.

In another example the above non transitory computer storage wherein the first request is received from a first client device from a plurality of computing devices. Further in one example the above non transitory computer storage wherein the instructions further cause the computer system to perform operations comprising detecting inactive connections to one or more external caches and attempting to connect to the one or more external caches using a background thread of execution. In addition in one example the above non transitory computer storage wherein the instructions further cause the computer system to perform operations comprising associating multiple threads that handle requests from the client devices to a plurality of network connections to the client devices and assigning a request received on one of the plurality of network connections to one of the associated multiple threads.

In an additional example the above non transitory computer storage wherein identifying the first external cache comprises obtaining an identifier based at least partly on the first request and applying a deterministic function to the identifier wherein the deterministic function identifies the first external cache.

In another example the above non transitory computer storage wherein either the first data item or the second data item is received by the computing system first.

In an example embodiment a distributed system for reusing secure sessions using non locally cached data comprises 1 a plurality of proxy systems configured to provide services to users each proxy system having a local cache for storing data 2 a request distribution module configured to receive requests for secure communications over a network the requests sent by a plurality of client devices associated with the users the requests including a first request from a first client device and assign the first request for secure communications to a first proxy system from the plurality of proxy systems and 3 the first proxy system comprising a local cache the first proxy system configured to determine whether the local cache includes a session state identifier associated with the first request in response to determining that the local cache does not include the session state identifier identify a first cache system from a plurality of shared cache systems the first cache system designated to store data items associated with the first request determine whether the first cache system includes the session state identifier associated with the first request in response to determining that the second computing device includes the session state identifier obtain the session state identifier from the first cache system wherein the session state identifier was stored on the first cache system as a result of a prior transaction with the first client device and establish a secure connection with the first client device based at least in part on the session state identifier.

In another example the above distributed system wherein the secure connection uses Transport Layer Security TLS protocol.

In an additional example the above distributed system wherein the session state identifier was stored on the first cache system as a result of the prior transaction with the first client device the prior transaction occurring between the first client device and a second proxy system different from the first proxy system.

In another example the above distributed system wherein the session state identifier was stored on the first cache system as a result of the prior transaction with the first client device the prior transaction occurring between the first client device and the first proxy system.

In an additional example the above distributed system wherein the first computing device is configured to establish a secure connection with the first client device based at least in part on the session state identifier by obtaining cryptographic parameters associated with the session state identifier and performing an abbreviated hand shake operation using at least the cryptographic parameters.

In another example the above distributed system wherein session state identifier comprises a session ID.

In an example embodiment a method comprises receiving on a first computing system a request for secure communications the request sent by a first client device wherein a previous request from the first client device was sent during a first session identifying a second computing system having a cache the second computing system designated to store data associated with the request in the cache the second computing system accessible to the first computing system over a network obtaining a session state identifier from the second computing system and establishing from the first computing system a secure connection with the first client device based at least in part on the session state identifier.

In an additional example the above method wherein obtaining the session state identifier from the second computing system comprises determining whether the cache of the second computing system includes the session state identifier associated with the request the session state identifier generated during the first session and in response to determining that the cache of the second computing system includes the session state identifier obtaining the session state identifier from the second computing system.

In another example the above method wherein the secure connection uses Transport Layer Security TLS protocol.

In an additional example the above method wherein establishing the secure connection with the first client device comprises resuming the first session.

In another example the above method wherein establishing the secure connection with the first client device based at least in part on the session state identifier comprises obtaining session state data associated with the session state identifier and performing an abbreviated hand shake operation using at least the session state data.

In an additional example the above method further comprising determining that the session state identifier is not stored on a local cache of the first computing system.

In another example the above method wherein the first session was between the first client device and a third computing system different from the first computing system and the second computing system the third computing system configured to transmit the session state identifier to the second computing system for storage on the cache of the second computing system.

In an additional example the above method wherein the first session was between the first client device and the first computing system the first computing system configured to store the session state identifier generated during the first session on the cache of the second computing system.

In an additional example the above method wherein identifying the second computing system designated to store data associated with the request comprises applying a consistent hashing function to an identifier related to the request. Further in the above method the identifier includes an IP address.

In an additional example the above method wherein the second computing system is part of a shared caching system.

In another example the above method wherein the session state identifier is reused from a prior secure connection.

In an additional example the above method wherein the secure connection is resumed from a prior network transaction.

In an example embodiment a non transitory computer storage having stored thereon instructions that when executed by a computer system cause the computer system to perform operations comprising receiving on a first computing system a request for secure communications the request sent by a first client device wherein a previous request from the first client device was sent during a first session determining whether an external cache includes a session state identifier associated with the request the session state identifier generated during the first session in response to determining that the external cache includes the session state identifier obtaining the session state identifier from the external cache and establishing from the first computing system a secure connection with the first client device based at least in part on the session state identifier.

In another example the above non transitory computer storage wherein establishing a secure connection with the first client device based at least in part on the session state identifier comprises reusing negotiation data from the first session.

In an additional example the above non transitory computer storage wherein the first session was between the first client device and a second computing system different from the first computing system the second computing system configured to store the session state identifier in the external cache.

In another example the above non transitory computer storage wherein the external cache is a first external cache of a shared cache system comprising a plurality of external caches.

In an additional example the above non transitory computer storage further comprising identifying the first external cache designated to store the session state identifier from the plurality of external caches of the shared cache system.

In an example embodiment a distributed system comprises a first proxy system comprising one or processors and a local cache the first proxy system configured to in response to determining that the local cache does not include a session state identifier associated with a request for a secure connection from a client device identify a first cache system from the plurality of shared cache systems the first cache system designated to store data items associated with the first request in response to determining that the first cache system includes the session state identifier obtain the session state identifier from the first cache system wherein the session state identifier was stored on the first cache system as a result of a prior transaction with the client device and establish a secure connection with the client device based at least in part on the session state identifier.

In an example embodiment a system for distributed data caching comprises 1 a first caching system in a first partition group the first caching system configured to in response to receiving a first cache update for a first data item update a first cache in the first partition group with the first cache update and transmit the first cache update for the first data item to a second caching system in a second partition group and 2 the second caching system in the second partition group the second caching system configured to in response to receiving the first cache update transmitted by the first caching system update a second cache in the second partition group with the first cache update for the first data item in response to receiving a request for the first data item from a requestor obtain the first data item from the second cache and provide the first data item to the requestor.

In another example the above system wherein the first partition group and the second partition group are located in different time zones.

In an additional example the above system wherein the first partition group and the second partition group are located in different geographic areas.

In another example the above system wherein the first cache system and the second cache system are part of a distributed caching system that spans multiple partition groups.

In an example embodiment a method of caching data in a disturbed system the method comprising receiving at a first caching system at a first partition group an update for a first data item updating a first cache associated with the first caching system with the update for the first data item the first cache in the first partition group and transmitting the cache update for the first data item to a second caching system in a second partition group thereby causing the second caching system to update a second cache on the second partition group with the cache update for the first data item.

In another example the above method further comprising in response to receiving a request for the first data item at the first caching system retrieving the first data item from the first cache in the first partition group.

In an additional example the above method further comprising in response to receiving a request for the first data item at the second caching system retrieving the first data item from the second cache in the second partition group.

In another example the above method wherein the first partition group and the second partition group are located in different time zones.

In an additional example the above method wherein the first partition group and the second partition group are located in different geographic areas.

In another example the above method wherein the first cache system and the second cache system are part of a distributed caching system that spans multiple partition groups.

In an additional example the above method wherein receiving the update for the first data item comprises receiving a request for the first data item and in response to determining that the first caching system does not store the first data item retrieving the first data item from primary storage wherein the first cache update for the first data item comprises adding the first data item.

In another example the above method wherein updating the first cache associated with the first caching system comprises loading the first data item into the first cache.

In an additional example the above method wherein updating the first cache associated with the first caching system comprises finding an entry in the first cache associated with the first data item and updating the entry in the first cache.

In another example the above method wherein the first partition group is a data center and the second partition group is a data center.

In an additional example the above method wherein the first partition group comprises multiple virtual machine instances on the same computer.

In an example embodiment non transitory computer storage having stored thereon instructions that when executed by a computer system cause the computer system to perform operations comprising receiving at a first caching system at a first partition group an update for a first data item updating a first cache associated with the first caching system with the update for the first data item the first cache in the first partition group and causing one or more additional caching systems located in one or more additional partition groups to update caches located at the one or more additional partition groups.

In another example the above non transitory computer storage wherein the first partition group and the one or more additional partition groups are located in different time zones.

In an additional example the above non transitory computer storage wherein the first partition group and the one or more additional partition groups are located in different geographic areas.

In another example the above non transitory computer storage wherein the first cache system and the one or more additional cache systems are part of a distributed caching system that spans multiple partition groups.

In an additional example the above non transitory computer storage wherein updating the first cache associated with the first caching system comprises loading the first data item into the first cache.

In another example the above non transitory computer storage wherein updating the first cache associated with the first caching system comprises finding an entry in the first cache associated with the first data item and updating the entry in the first cache.

In an additional example the above non transitory computer storage wherein the first partition group comprises a data center and the second partition group comprises a data center.

In another example the above non transitory computer storage wherein the first partition group comprises multiple virtual machine instances on the same computer.

In an example embodiment a system for distributed data caching comprises 1 an inter process communication interface for receiving requests the requests including a first request for a first data item from a first requestor 2 computer memory for storing a local cache and 3 one or more processors configured to determine whether the first data item is stored in the local cache in response to determining that the first data item is not on the local cache determine a first external cache from a plurality of external caches the first external cache designated to store the first data item transmit the first request for the first data item to the external cache receive the first data item from the first external cache determine whether a second requestor has requested the first data item in response to determining that a second requestor has requested the first data item provide the first data item to the first requestor and the second requestor and in response to determining that a second requestor has not requested the first data item provide the first data item to the first requestor.

In another example the above system wherein the first requestor is a first client device and the second requestor is a second client device.

In an additional example the above system wherein the first requestor is a first thread and the second requestor is a second thread.

In another example the above system further comprising a distributed caching client configured to obtain the first data item from the external cache and transmit the first data item directly on a network connection to the first client without waking a thread assigned to the first client.

In an additional example the above system wherein the one or more processors are configured to determine a first external cache from a plurality of external caches by obtaining an identifier based at least partly on the first request and applying a deterministic algorithm to the identifier such that the deterministic algorithm identifies the first external cache.

In another example the above system wherein the one or more processors are further configured to identify additional requestors of the first data item and in response to receiving the first data item providing the first data item to the additional requestors.

In an example embodiment a method of caching data the method comprising receiving on a first caching system a first request for a first data item from a first requestor identifying a first cache designated to store the first data item the first cache selected from a local cache and a plurality of external caches obtain the first data item from the first cache and in response to determining that a second requestor has requested the first data item provide the first data item to the first requestor and the second requestor.

In another example the above method further comprising identifying additional requestors of the first data item and in response to receiving the first data item providing the first data item to the additional requestors.

In an additional example the above method wherein the first cache is one of the plurality of external caches.

In another example the above method wherein identifying the first cache from the local cache and the plurality of external caches comprises determining whether the first data item is stored in the local cache and in response to determining that the first data item is not on the local cache determining a first external cache from the plurality of external caches the first external cache designated to store the first data item wherein the first cache is the first external cache. Further in one example the above method wherein determining the first external cache from the plurality of external caches comprises obtaining an identifier based at least partly on a key associated with the first request and applying a deterministic function to the identifier wherein the deterministic function identifies the first external cache. Additionally in one example the above method wherein the first request and additional requests identical to the first request are associated with a same key. Further in one example the above method wherein the deterministic function is a consistent hashing function.

In an additional example the above method wherein the first requestor is a first client device and the second requestor is a second client device.

In another example the above method wherein the first requestor is a first thread and the second requestor is a second thread.

In an example embodiment non transitory computer storage having stored thereon instructions that when executed by a computer system cause the computer system to perform operations comprising receiving a first request for a first data item from a first requestor obtaining the first data item from a first cache and in response to determining that a second requestor has requested the first data item provide the first data item to the first requestor and the second requestor.

In another example the above non transitory computer storage wherein obtaining the first data item from the first cache comprises selecting the first cache from a local cache and a plurality of external caches.

In another example the above non transitory computer storage wherein selecting the first cache from the local cache and the plurality of external caches comprises determining whether the first data item is stored in the local cache and in response to determining that the first data item is not on the local cache determining a first external cache from the plurality of external caches the first external cache designated to store the first data item wherein the first external cache is the first cache. Further in one example the above non transitory computer storage wherein determining the first external cache from the plurality of external caches comprises obtaining an identifier based at least partly on a key associated with the first request and applying a deterministic function to the identifier wherein the deterministic function identifies the first external cache.

In another example the above non transitory computer storage further comprising identifying additional requestors of the first data item and in response to receiving the first data item providing the first data item to the additional requestors.

In an example embodiment a distributed system for resuming security handshakes using non locally cached data the system comprising 1 a plurality of server systems configured to provide services to users each server system configured to store security handshake data on a distributed caching system 2 the distributed caching system comprising one or more caching systems the distributed caching system configured to receive handshake data for a first handshake transaction between a first client and a first server from the plurality of server systems store the handshake data in the one or more caching systems in response to an interruption event that interrupts the first handshake transaction and causes the first client to be assigned to a second server from the plurality of server systems receive a request for the handshake data from the second server and provide the second server with the handshake data to enable the first handshake transaction to be resumed.

In another example the above system wherein the security handshake is for Transport Layer Security TLS protocol.

In an additional example the above system wherein the security handshake is for Secure Sockets Layer SSL protocol.

In an additional example the above system wherein the handshake data includes data sent in an initial message from the first client and data sent in an initial message from the first server.

In an example embodiment a method for resuming security handshakes using non locally cached data the method comprising receiving handshake data for a first handshake transaction between a first client and a first server from a plurality of server systems storing the handshake data in one or more caching systems in response to an interruption event that interrupts the first handshake transaction and causes the first client to be assigned to a second server from the plurality of server systems receiving a request for the handshake data from the second server and providing the second server with the handshake data to enable the first handshake transaction to be resumed.

In another example the above method wherein the security handshake is for Transport Layer Security TLS protocol.

In an additional example the above method wherein the security handshake is for Secure Sockets Layer SSL protocol.

In an additional example the above method wherein the handshake data includes data sent in an initial message from the first client and data sent in an initial message from the first server.

In another example the above method wherein the handshake data includes one or more of a public key an encryption key cryptographic material and a security certificate.

In an example embodiment non transitory computer storage having stored thereon instructions that when executed by a computer system cause the computer system to perform operations comprising receiving handshake data for a first handshake transaction between a first client and a first server from a plurality of server systems storing the handshake data on a caching system accessible from each of the plurality of server systems receiving a request for the handshake data from a second server of the plurality of the server systems and providing the second server with the handshake data to enable the first handshake transaction to be resumed by the second server wherein the first handshake transaction between the first client and the first server was interrupted.

In another example the above non transitory computer storage wherein the security handshake is for Transport Layer Security TLS protocol.

In an additional example the above non transitory computer storage wherein the security handshake is for Secure Sockets Layer SSL protocol.

In another example the above non transitory computer storage wherein the handshake data includes cryptographic parameters.

In an additional example the above non transitory computer storage wherein the handshake data includes data sent in an initial message from the first client and data sent in an initial message from the first server.

In another example the above non transitory computer storage wherein the handshake data includes one or more of a public key an encryption key cryptographic material and a security certificate.

In an example embodiment a method comprising receiving handshake data for a first handshake transaction between a first client and a first server storing the handshake data in a caching system in response to an interruption event that interrupts the first handshake transaction and causes the first server to lose locally stored handshake data receiving a request for the handshake data from the first server and providing the first server with the handshake data to enable the first handshake transaction to be resumed.

Each of the processes methods and algorithms described in the preceding sections may be embodied in and fully or partially automated by code modules executed by one or more computers computer processors or machines configured to execute computer instructions. The code modules may be stored on any type of non transitory computer readable storage medium or tangible computer storage device such as hard drives solid state memory optical disc and or the like. The processes and algorithms may be implemented partially or wholly in application specific circuitry. The results of the disclosed processes and process steps may be stored persistently or otherwise in any type of non transitory computer storage such as e.g. volatile or non volatile storage.

The various features and processes described above may be used independently of one another or may be combined in various ways. All possible combinations and subcombinations are intended to fall within the scope of this disclosure. In addition certain method event state or process blocks may be omitted in some implementations. The methods and processes described herein are also not limited to any particular sequence and the blocks or states relating thereto can be performed in other sequences that are appropriate. For example described tasks or events may be performed in an order other than that specifically disclosed or multiple may be combined in a single block or state. The example tasks or events may be performed in serial in parallel or in some other manner. Tasks or events may be added to or removed from the disclosed example embodiments. The example systems and components described herein may be configured differently than described. For example elements may be added to removed from or rearranged compared to the disclosed example embodiments.

Conditional language used herein such as among others can could might may e.g. and the like unless specifically stated otherwise or otherwise understood within the context as used is generally intended to convey that certain embodiments include while other embodiments do not include certain features elements and or steps. Thus such conditional language is not generally intended to imply that features elements and or steps are in any way required for one or more embodiments or that one or more embodiments necessarily include logic for deciding with or without author input or prompting whether these features elements and or steps are included or are to be performed in any particular embodiment. The terms comprising including having and the like are synonymous and are used inclusively in an open ended fashion and do not exclude additional elements features act operations and so forth. Also the term or is used in its inclusive sense and not in its exclusive sense so that when used for example to connect a list of elements the term or means one some or all of the elements in the list. Conjunctive language such as the phrase at least one of X Y and Z unless specifically stated otherwise is otherwise understood with the context as used in general to convey that an item term etc. may be either X Y or Z. Thus such conjunctive language is not generally intended to imply that certain embodiments require at least one of X at least one of Y and at least one of Z to each be present.

While certain example embodiments have been described these embodiments have been presented by way of example only and are not intended to limit the scope of the inventions disclosed herein. Thus nothing in the foregoing description is intended to imply that any particular feature characteristic step module or block is necessary or indispensable. Indeed the novel methods and systems described herein may be embodied in a variety of other forms furthermore various omissions substitutions and changes in the form of the methods and systems described herein may be made without departing from the spirit of the inventions disclosed herein.

