---

title: Firewalls in logical networks
abstract: Some embodiments provide a method for configuring a logical firewall in a hosting system that includes a set of nodes. The logical firewall is part of a logical network that includes a set of logical forwarding elements. The method receives a configuration for the firewall that specifies packet processing rules for the firewall. The method identifies several of the nodes on which to implement the logical forwarding elements. The method distributes the firewall configuration for implementation on the identified nodes. At a node, the firewall of some embodiments receives a packet, from a managed switching element within the node, through a software port between the managed switching element and the distributed firewall application. The firewall determines whether to allow the packet based on the received configuration. When the packet is allowed, the firewall the packet back to the managed switching element through the software port.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09015823&OS=09015823&RS=09015823
owner: Nicira, Inc.
number: 09015823
owner_city: Palo Alto
owner_country: US
publication_date: 20121115
---
This application claims the benefit of U.S. Provisional Application 61 560 279 entitled Virtual Middlebox Services filed Nov. 15 2011. U.S. Application 61 560 279 is incorporated herein by reference.

Many current enterprises have large and sophisticated networks comprising switches hubs routers middleboxes e.g. firewalls servers workstations and other networked devices which support a variety of connections applications and systems. The increased sophistication of computer networking including virtual machine migration dynamic workloads multi tenancy and customer specific quality of service and security configurations require a better paradigm for network control. Networks have traditionally been managed through low level configuration of individual network components. Network configurations often depend on the underlying network for example blocking a user s access with an access control list ACL entry requires knowing the user s current IP address. More complicated tasks require more extensive network knowledge forcing guest users port 80 traffic to traverse an HTTP proxy requires knowing the current network topology and the location of each guest. This process is of increased difficulty where the network switching elements are shared across multiple users.

In response there is a growing movement towards a new network control paradigm called Software Defined Networking SDN . In the SDN paradigm a network controller running on one or more servers in a network controls maintains and implements control logic that governs the forwarding behavior of shared network switching elements on a per user basis. Making network management decisions often requires knowledge of the network state. To facilitate management decision making the network controller creates and maintains a view of the network state and provides an application programming interface upon which management applications may access a view of the network state.

Some of the primary goals of maintaining large networks including both datacenters and enterprise networks are scalability mobility and multi tenancy. Many approaches taken to address one of these goals results in hampering at least one of the others. For instance one can easily provide network mobility for virtual machines within an L2 domain but L2 domains cannot scale to large sizes. Furthermore retaining user isolation greatly complicates mobility. As such improved solutions that can satisfy the scalability mobility and multi tenancy goals are needed.

Some embodiments provide a system that allows several different logical firewalls to be specified for several different logical networks through one or more shared firewall elements. In some embodiments the system distributes the logical firewall for a particular logical network across several different physical machines that also host virtual machines of the particular logical network. At each of the different physical machines a firewall element operates that may be virtualized into several different firewall instances each implementing a different set of firewall rules for a different logical firewall. In addition each of these physical machines also operates a managed switching element to perform logical forwarding functionalities for one or more logical forwarding elements of the logical networks.

Such a firewall may be incorporated into a logical network in various different topologies in some embodiments. For instance a firewall may be specified to connect to a logical forwarding element e.g. a logical router out of the direct flow through the logical network may be located between two logical forwarding elements or between a logical forwarding element and an external network within the logical network topology.

The firewall of some embodiments analyzes data packets sent to it to determine whether or not the packets should be allowed through. The firewall stores a set of rules e.g. entered by a user that determine whether or not the firewall drops i.e. discards or allows the packet through or in some cases rejects the packet by dropping the packet and sending an error response back to the sender . In some embodiments the firewall is a stateful firewall that keeps track of transport e.g. TCP and or UDP connections and uses the stored state information to make faster packet processing decisions.

When the firewall is not located on a logical wire i.e. between two logical network components packets will not be sent to the firewall unless routing policies are specified e.g. by a user such as a network administrator for the logical forwarding element to which the firewall connects. Some embodiments enable the use of policy routing rules that forward packets based on data beyond the destination address e.g. destination IP or MAC address .

In addition the user specifies rules for the firewall packet processing. While the logical topology and policy routing rules are set up through the network controller API some embodiments include a separate firewall specific API e.g. specific to the particular firewall deployed through which the user configures the logical firewall. In some embodiments these rules specify conditions which if matched result in either dropping or allowing the packet similar to access control list ACL table entries . As an example a user might specify that packets from a particular external IP address or domain of IP addresses are always dropped when destined for a particular subnet. In addition the rules may relate to the existence of an established L4 e.g. TCP connection between two IP addresses.

In some embodiments the logical network is implemented in a distributed virtualized fashion. The various end machines i.e. workloads may be implemented as virtual machines hosted on numerous different physical host machines e.g. servers in a grid of physical nodes although in other embodiments some or all of the machines in the network are physical machines themselves.

In addition some embodiments implement the logical switching and routing elements in a distributed virtualized fashion. That is rather than using physical switches to implement the logical forwarding elements the forwarding responsibilities are spread across managed switching elements distributed throughout the network. For instance some embodiments include switching software within the physical host machines. This switching software implements the logical forwarding elements of the logical networks in some embodiments.

In addition to distributing and virtualizing the end machines and the logical forwarding elements some embodiments also distribute and or virtualize the firewalls and other middleboxes . That is in some embodiments the firewall is implemented across various host machines within the network rather than as a single centralized machine some embodiments may alternatively or conjunctively include centralized firewalls . Some embodiments implement a firewall within the host machines at which virtual machines are located that may send and or receive packets through the firewall. Furthermore in some embodiments the firewall module s running on the hosts may be sliced so as to implement multiple different firewalls either several different firewalls for the same logical network or firewalls for several different networks . That is within the same module several separate firewall instances may run implementing different packet processing instructions.

When the packets are sent to the firewall from the managed switching element some embodiments include a slice identifier or tag to identify to which of the several firewalls the packet is being sent. When multiple firewalls are implemented for a single logical network the slice identifier will need to identify more than just the logical network to which the packet belongs in order to be sent to the correct firewall instances. Different embodiments may use different slice identifiers for the firewalls.

In some embodiments a network control system is utilized to configure both the managed switching elements and the firewalls. The network control system of some embodiments includes a logical controller that receives the logical network topology e.g. as logical control plane data information regarding the physical implementation of the network e.g. the physical locations of the virtual machines of the network and the firewall configuration. In order to configure the firewalls the logical controller identifies the host machines on which to implement the firewalls and provides the configuration data for distribution to those machines. In some embodiments the configuration data is distributed through a set of physical controllers with each physical host machine managed by a particular physical controller. Thus the logical controller distributes the firewall configuration data to a set of physical controllers which subsequently distribute the firewall configuration to the firewall modules on the host machines. In some embodiments the physical controllers additionally assign the slice identifiers for the firewall instances and a particular physical controller distributes the slice identifier to both the firewall instance and the managed switching element at its assigned host machine.

The preceding Summary is intended to serve as a brief introduction to some embodiments of the invention. It is not meant to be an introduction or overview of all inventive subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly to understand all the embodiments described by this document a full review of the Summary Detailed Description and the Drawings is needed. Moreover the claimed subject matters are not to be limited by the illustrative details in the Summary Detailed Description and the Drawing but rather are to be defined by the appended claims because the claimed subject matters can be embodied in other specific forms without departing from the spirit of the subject matters.

In the following detailed description of the invention numerous details examples and embodiments of the invention are set forth and described. However it will be clear and apparent to one skilled in the art that the invention is not limited to the embodiments set forth and that the invention may be practiced without some of the specific details and examples discussed.

Some embodiments provide a system that allows several different logical firewalls to be specified for several different logical networks through one or more shared firewall elements. In some embodiments the system distributes the logical firewall for a particular logical network across several different physical machines that also host virtual machines of the particular logical network. At each of the different physical machines a firewall element operates that may be virtualized into several different firewall instances each implementing a different set of firewall rules for a different logical firewall. In addition each of these physical machines also operates a managed switching element to perform logical forwarding functionalities for one or more logical forwarding elements of the logical networks.

In addition the firewall connects to the L3 router in order to process some or all packets that enter the logical router . One of ordinary skill in the art will recognize that the network architecture represents just one particular logical network architecture into which a firewall may be incorporated. In various embodiments the firewall may be located directly between two other components e.g. as shown below in directly between the gateway and logical router e.g. in order to monitor and process all traffic entering or exiting the logical network or in other locations in a more complex network.

The firewall of some embodiments analyzes data packets sent to it to determine whether or not the packets should be allowed through. The firewall stores a set of rules e.g. entered by a user that determine whether or not the firewall drops i.e. discards or allows the packet through or in some cases rejects the packet by dropping the packet and sending an error response back to the sender . In some embodiments the firewall is a stateful firewall that keeps track of transport e.g. TCP and or UDP connections and uses the stored state information to make faster packet processing decisions.

In the architecture shown in the firewall is not located within the direct traffic flow either from one domain to the other or between the external world and the domain. Accordingly packets will not be sent to the firewall unless routing policies are specified e.g. by a user such as a network administrator at the logical router that determine which packets should be sent to the firewall for processing. Some embodiments enable the use of policy routing rules that forward packets based on data beyond the destination address e.g. destination IP or MAC address . For example a user might specify e.g. through a network controller application programming interface API that all packets with a source IP address in the logical domain or all packets that enter the network through the gateway destined for the second logical domain should be directed to the firewall for processing.

In addition the user specifies rules for the firewall packet processing. While the logical topology and policy routing rules are set up through the network controller API some embodiments include a separate firewall specific API e.g. specific to the particular firewall deployed through which the user configures the logical firewall. In some embodiments these rules specify conditions which if matched result in either dropping or allowing the packet similar to access control list ACL table entries . As an example a user might specify that packets from a particular external IP address or domain of IP addresses are always dropped when destined for the domain . In addition the rules may relate to the existence of an established L4 e.g. TCP connection between two IP addresses. For example a particular rule or set of rules might specify that packets sent from the first logical domain are not allowed into the second logical domain unless a TCP connection initiated by the destination machine in the second domain exists.

As stated above illustrates a particular logical topology for the network . as stated conceptually illustrates a different logical topology for a network architecture of some embodiments that also includes a firewall . In this case the structure of the network without the firewall is the same as that for network with a logical L3 router linking two logical L2 switches and which each perform switching for machines on two logical domains and . In this case however the firewall is located on the logical wire between the first logical switch and the logical router .

Such a setup is simpler for the network administrator because no routing policies need to be configured as compared to the need for routing policies specifying when packets are sent to the firewall from logical router in the network but offers less flexibility. Instead all traffic between the logical switch and the logical router will be sent to the firewall . From a logical perspective the firewall is transparent to the switch and the router . The flows for the managed switching elements that implement these logical forwarding elements however will need to be configured to send packets to the firewall rather than to the other one of the two logical forwarding elements.

In some embodiments such a logical network e.g. either network or network is implemented in a distributed virtualized fashion. The various machines in the logical domains and may be implemented as virtual machines hosted on numerous different physical host machines e.g. servers in a grid of physical nodes although in other embodiments some or all of the machines in the network are physical machines themselves.

In addition some embodiments implement the logical switching and routing elements collectively referred to as logical forwarding elements in a distributed virtualized fashion. That is rather than using physical switches to implement the logical forwarding elements the forwarding responsibilities are spread across managed switching elements distributed throughout the network. For instance some embodiments include switching software within the physical host machines e.g. running on top of or within a hypervisor on the host . This switching software e.g. open virtual switch OVS implements the logical forwarding elements of the logical networks in some embodiments.

In addition to distributing and virtualizing the end machines and the logical forwarding elements some embodiments also distribute and or virtualize the middleboxes such as the firewall . That is in some embodiments the firewall is implemented across various host machines within the network rather than as a single centralized machine some embodiments may alternatively or conjunctively include centralized firewalls . Some embodiments implement a firewall within the host machines at which virtual machines are located that may send and or receive packets through the firewall e.g. running on top of or within a hypervisor on the host . Furthermore in some embodiments the firewall module s running on the hosts may be sliced so as to implement multiple different firewalls either several different firewalls for the same logical network or firewalls for several different networks . That is within the same module several separate firewall instances may run implementing different packet processing instructions.

In addition each of the hosts includes a managed switching element MSE . The managed switching elements of some embodiments are software forwarding elements that implement logical forwarding elements for one or more logical networks. For instance the MSEs in the hosts include flow entries in forwarding tables that implement the logical forwarding elements of network . Specifically the MSE on the first host implements the logical switch the logical switch and the logical router . In some embodiments because only virtual machines from the second domain are located on the second host the MSE on the second host implements only the second logical switch and the router but not the first logical switch .

Similarly the firewall modules and running on the hosts and both implement the firewall policies assigned to the firewall . When a packet is sent from one of the virtual machines of the logical domain residing on the host the managed switching element performs the logical L2 processing of switch . When the destination address for the packet is located in the second logical domain the flow entries implementing the L2 processing forward the packet to the logical router . The managed switching element then applies the flow entries for the logical router to the packet. These flow entries may implement routing policies that send the packet to the logical firewall . In such cases the packet is sent to the firewall module still on the host for processing. If the firewall allows the packet through then some embodiments send the packet back to the managed switching element as a new packet in some such embodiments for additional processing by the forwarding tables implementing the logical router . These flow entries send the packet to the logical L2 switch the forwarding tables for which are also stored in the MSE . The L2 flow entries identify the destination machine and send the packet to that machine which may be located on the same host or a different one of the hosts .

In some cases the MSE located on a particular one of the hosts may not include forwarding tables for some of the logical forwarding required for a packet due to the distributed nature of the forwarding elements . When the particular MSE does not store the necessary flow entries some embodiments send the packet to a pool node for additional processing. The pool nodes allow the various MSEs at different hosts to only store a description of subsections of a network thereby limiting the amount of data required for each MSE.

The firewalls and as well as firewall modules on other hosts that contain machines in the logical network store packet processing rules in order to implement the logical firewall . In addition if one of the other virtual machines on one of the hosts is in a network with a distributed firewall the firewall module operating on that host will store packet processing rules to implement the second firewall. These will effectively operate as two or more separate firewall instances such that the firewall module is sliced into several virtual firewalls. In addition when the packets are sent to the firewall from the managed switching element some embodiments include a slice identifier or tag to identify to which of the several firewalls the packet is being sent. When multiple firewalls are implemented for a single logical network the slice identifier will need to identify more than just the logical network to which the packet belongs in order to be sent to the correct firewall instances. Different embodiments may use different slice identifiers for the firewalls.

The above illustrates examples of the implementation of logical firewalls in a network of some embodiments. Several more detailed embodiments are described below. Section I describes the network control system of some embodiments for configuring a network in order to implement a logical network that includes a firewall. Section II describes the distributed firewall implementation of some embodiments. Next Section III describes packet processing in a node that includes a distributed firewall. Section IV then describes the migration of firewall state information along with a migrating virtual machine. Finally Section V describes an electronic system with which some embodiments of the invention are implemented.

In order to implement a logical network such as that shown in some embodiments configure the managed switching elements and distributed or centralized firewalls as well as other middleboxes through one or more network controllers of a network control system. illustrates a network control system of some embodiments for configuring managed switching elements and firewall modules in order to implement logical networks.

As shown the network control system includes an input translation controller a logical controller physical controllers and hosts . As shown the hosts include managed switching element and firewall components. One of ordinary skill in the art will recognize that many other different combinations of the various controllers and hosts are possible for the network control system .

In some embodiments each of the controllers in a network control system has the capability to function as an input translation controller logical controller and or physical controller. Alternatively in some embodiments a given controller may only have the functionality to operate as a particular one of the types of controller e.g. as a physical controller . In addition different combinations of controllers may run in the same physical machine. For instance the input translation controller and the logical controller may run in the same computing device with which a user interacts.

Furthermore each of the controllers illustrated in and subsequent is shown as a single controller. However each of these controllers may actually be a controller cluster that operates in a distributed fashion to perform the processing of a logical controller physical controller or input translation controller.

The input translation controller of some embodiments includes an input translation application that translates network configuration information received from a user. For example a user may specify a network topology such as that shown in which includes a specification as to which machines belong in which logical domain. This effectively specifies a logical data path set or a set of logical forwarding elements. For each of the logical switches the user specifies the machines that connect to the logical switch i.e. to which logical ports are assigned for the logical switch . In some embodiments the user also specifies IP addresses for the machines. The input translation controller translates the entered network topology into logical control plane data that describes the network topology. For example an entry might state that a particular MAC address A is located at a particular logical port X.

In some embodiments each logical network is governed by a particular logical controller e.g. logical controller . The logical controller of some embodiments translates the logical control plane data into logical forwarding plane data and the logical forwarding plane data into universal control plane data. Logical forwarding plane data in some embodiments consists of flow entries described at a logical level. For the MAC address A at logical port X logical forwarding plane data might include a flow entry specifying that if the destination of a packet matches MAC A forward the packet to port X.

The universal physical control plane data of some embodiments is a data plane that enables the control system of some embodiments to scale even when it contains a large number of managed switching elements e.g. thousands to implement a logical data path set. The universal physical control plane abstracts common characteristics of different managed switching elements in order to express physical control plane data without considering differences in the managed switching elements and or location specifics of the managed switching elements.

As stated the logical controller of some embodiments translates logical control plane data into logical forwarding plane data e.g. logical flow entries then translates the logical forwarding plane data into universal control plane data. In some embodiments the logical controller application stack includes a control application for performing the first translation and a virtualization application for performing the second translation. Both of these applications in some embodiments use a rules engine for mapping a first set of tables into a second set of tables. That is the different data planes are represented as tables e.g. n Log tables and the controller applications use a table mapping engine to translate between the data planes.

Each physical controller is a master of one or more managed switching elements e.g. located within host machines . In this example the first and second physical controllers and are masters of three managed switching elements each while the third physical controller is the master of two managed switching elements. In some embodiments a physical controller receives the universal physical control plane information for a logical network and translates this data into customized physical control plane information for the particular managed switches that the physical controller manages. In other embodiments the physical controller passes the appropriate universal physical control plane data to the managed switch which includes the ability e.g. in the form of a chassis controller running on the host machine to perform the conversion itself.

The universal physical control plane to customized physical control plane translation involves a customization of various data in the flow entries. For the example noted above the universal physical control plane would involve several flow entries. The first entry states that if a packet matches the particular logical data path set e.g. based on the packet being received at a particular physical input port and the destination address matches MAC A then forward the packet to logical port X. This flow entry will be the same in the universal and customized physical control planes in some embodiments. Additional flows are generated to match the physical ingress port to a logical ingress port on the particular logical data path set as well as to match logical port X to a particular egress port of the physical managed switch e.g. a virtual interface of the host machine . However these physical ingress and egress ports are specific to the host machine containing the managed switching element. As such the universal physical control plane entries include abstract physical ports while the customized physical control plane entries include the actual physical ports involved.

In some embodiments the network control system also disseminates data relating to the firewall of a logical network. The network control system may disseminate firewall configuration data as well as attachment data relating to the sending and receiving of packets to from the firewall at the managed switches and to from the managed switches at the firewall. For instance the flow entries propagated through the network control system to the managed switches will include entries for sending the appropriate packets to the firewall e.g. flow entries that specify for packets that have a source IP address in a particular subnet to be forwarded to the firewall . In addition the flow entries for the managed switch will need to specify how to send such packets to the firewall. That is once a first entry specifies a logical egress port of the logical router to which the firewall is bound additional entries are required to attach the logical egress port to the firewall.

When the firewall is a centralized appliance these additional entries will match the logical egress port of the logical router to a particular physical port of the host machine e.g. a physical network interface through which the host machine connects to the firewall. In addition the entries include encapsulation information for sending the packet to the centralized firewall appliance via a tunnel between the host machine and the firewall.

When the firewall is distributed the packet does not have to actually leave the host machine in order to reach the firewall. However the managed switching element nevertheless needs to include flow entries for sending the packet to the firewall module on the host machine. These flow entries again include an entry to map the logical egress port of the logical router to the port through which the managed switching element connects to the firewall. However in this case the firewall attaches to a software abstraction of a port in the managed switching element rather than a physical or virtual interface of the host machine. That is a port is created within the managed switching element to which the firewall attaches. The flow entries in the managed switching element send packets to this port in order for the packets to be routed within the host machine to the firewall.

As opposed to the tunneling information required for a centralized firewall in some embodiments the managed switching element adds slicing information to the packet. Essentially this slicing information is a tag that indicates to which of the potentially several instances being run by the firewall module the packet should be sent. Thus when the firewall module receives the packet the tag enables the module to use the appropriate set of packet processing rules in order to determine whether to drop allow etc. the packet. Some embodiments rather than adding slicing information to the packet define different ports of the managed switching element for each firewall instance and essentially use the ports to slice the traffic destined for the firewall.

The above describes the propagation of the forwarding data to the managed switching elements. In addition some embodiments use the network control system to propagate configuration data downward to the firewalls and other middleboxes . conceptually illustrates the propagation of data through the network control system of some embodiments. On the left side of the figure is the data flow to the managed switching elements that implement a logical network while the right side of the figure shows the propagation of both firewall configuration data as well as network attachment and slicing data to the firewall modules.

On the left side the input translation controller receives a network configuration through an API which is converted into logical control plane data. This network configuration data includes a logical topology such as those shown in or . In addition the network configuration data of some embodiments includes routing policies that specify which packets are sent to the firewall. When the firewall is located on a logical wire between two logical forwarding elements as in network architecture then all packets sent over that wire will automatically be forwarded to the firewall. However for an out of band firewall such as that in network architecture the logical router will only send packets to the firewall when particular policies are specified by the user.

Whereas routers and switches are typically configured to forward packets according to the destination address e.g. MAC address or IP address of the packet policy routing allows forwarding decisions to be made based on other information stored by the packet e.g. source addresses a combination of source and destination addresses etc. . For example the user might specify that all packets with source IP addresses in a particular subnet or that have destination IP addresses not matching a particular set of subnets should be forwarded to the firewall. For the purposes of this figure an example of Send packets with a source IP address in the subnet B to firewall will be used.

As shown the logical control plane data is converted by the logical controller specifically by the control application of the logical controller to logical forwarding plane data and then subsequently by the virtualization application of the logical controller to universal physical control plane data. For the example routing policy the logical forwarding plane data is a flow entry of If source IP is contained in B then forward to Port K where Port K is the logical port of the logical router that corresponds to the firewall. For this example the logical router implements a logical data path set LDPS Q with two additional ports Port M for packets received from a first logical switch and Port N for packets received from a second logical switch.

The universal physical control plane data for this example includes several flow entries. A first flow entry for the forwarding adds a match over the L3 logical data path specifying If match L3 logical data path set Q and source IP is contained in B then forward to Port K . Furthermore the universal physical control plane data will include an ingress port integration entry for packets received from the first logical switch that performs switching over logical subnet B specifying If received from Port X of L2 switch then mark ingress port as Port M of LDPS Q . A similar entry for packets received from the firewall will state If received from software port abstraction mapping to firewall mark ingress port as Port K of LDPS Q . The universal physical control plane data also includes an egress port integration flow entry that enables packets to actually be sent to the firewall specifying If sent to Port K run egress pipeline then send to software port abstraction mapping to firewall. In some embodiments at the universal physical control plane level the control plane data does not specify the actual port because this may differ between host machines. Instead an abstract indicator of the port that maps to the firewall is used.

The physical controller one of the several physical controllers as shown translates the universal physical control plane data into customized physical control plane data for the particular managed switching elements that it manages. In the continuing example the flow entries requiring translation are the port integration entries relating to the firewall which must be configured to specify the ports appropriate for the particular configuration on the different host machines. This port might be a virtual NIC if the firewall runs as a virtual machine on the host machine or the previously described software port abstraction within the managed switching element when the firewall runs as a process e.g. daemon within the hypervisor on the virtual machine. In some embodiments for the latter situation the port is an IPC channel or TUN TAP device like interface. In some embodiments the managed switching element includes one specific port abstraction for the firewall module and sends this information to the physical controller in order for the physical controller to customize the physical control plane flows.

In addition in some embodiments the physical controller adds flow entries specifying slicing information particular to the firewall implementation on the host machines. For instance for a particular managed switching element the flow entry may specify to add a particular tag e.g. a VLAN tag or similar tag to a packet before sending the packet to the particular firewall. This slicing information enables the firewall module to receive the packet and identify which of its several independent firewall instances should process the packet.

The managed switching element one of several MSEs managed by the physical controller performs a translation of the customized physical control plane data into physical forwarding plane data. The physical forwarding plane data in some embodiments are the flow entries stored within a switching element either a physical router or switch or a software switching element against which the switching element actually matches received packets.

The right side of illustrates two sets of data propagated to the distributed firewalls in the host machines rather than the managed switching elements. The first of these sets of data is the actual firewall configuration data which includes various packet filtering rules. This data may be received at the input translation controller or a different input interface through an API particular to the firewall implementation. In some embodiments different firewall modules will have different interface presented to the user i.e. the user will have to enter information in different formats for different particular firewalls . As shown the user enters a firewall configuration which is translated by the firewall API into firewall configuration data. The user may enter rules that packets with particular source subnets or specific IP addresses should be blocked discarded allowed etc. particular rules to apply to transport sessions e.g. TCP connections which may depend on the IP address or subnet of the source and or destination for the packets as well as various other firewall rules. In some embodiments the firewall configuration data is a set of records with each record specifying a particular firewall matching rule. These records in some embodiments are similar to the flow entries e.g. ACL entries propagated to the managed switching elements. In fact some embodiments use the same applications on the controllers to propagate the firewall configuration records as for the flow entries and the same table mapping language e.g. n Log for the records.

The firewall configuration data in some embodiments is not translated by the logical or physical controller while in other embodiments the logical and or physical controller perform at least a minimal translation of the firewall configuration data records. As the firewall packet processing rules operate on the IP address or TCP connection state of the packets and the packets sent to the firewall will have this information exposed i.e. not encapsulated within the logical port information the firewall configuration does not require translation from logical to physical data planes. Thus the same firewall configuration data is passed from the input translation controller or other interface to the logical controller to the physical controller .

In some embodiments the logical controller stores a description of the logical network and of the physical implementation of that physical network. The logical controller receives the one or more firewall configuration records and identifies which of the various nodes i.e. host machines will need to receive the firewall configuration information. In some embodiments the entire firewall configuration is distributed to firewall modules at all of the host machines so the logical controller identifies all of the machines on which at least one virtual machine resides whose packets require use of the firewall. This may be all of the virtual machines in a network e.g. as for the firewall shown in or a subset of the virtual machines in the network e.g. when a firewall is only applied to traffic of a particular domain within the network . Some embodiments make decisions about which host machines to send the configuration data to on a per record basis. That is each firewall rule may apply only to a subset of the virtual machines and only hosts running these virtual machines need to receive the record.

Once the logical controller identifies the particular nodes to receive the firewall records the logical controller identifies the particular physical controllers that manage these particular nodes. As mentioned each host machine has an assigned master physical controller. Thus if the logical controller identifies only first and second hosts as destinations for the firewall configuration data the physical controllers for these hosts will be identified to receive the data from the logical controller and other physical controllers will not receive this data . In order to supply the firewall configuration data to the hosts the logical controller of some embodiments pushes the data using an export module that accesses the output of the table mapping engine in the logical controller to the physical controllers. In other embodiments the physical controllers request configuration data e.g. in response to a signal that the configuration data is available from the export module of the logical controller.

The physical controllers pass the data to the firewall modules on the host machines that they manage much as they pass the physical control plane data. In some embodiments the firewall configuration and the physical control plane data are sent to the same database running on the host machine and the managed switching element and firewall module retrieve the appropriate information from the database.

In some embodiments the firewall at the host machine translates the firewall configuration data. The firewall configuration data will be received in a particular language to express the packet processing rules. The firewall module of some embodiments compiles these rules into more optimized packet classification rules. In some embodiments this transformation is similar to the physical control plane to physical forwarding plane data translation. When the firewall module receives a packet the firewall applies the compiled optimized packet processing rules in order to efficiently and quickly classify and process the packet.

In addition to the firewall configuration rules the firewall modules receive slicing and or attachment information in order to receive packets from and send packets to the managed switching elements. This information corresponds to the information sent to the managed switching elements. As shown in some embodiments the physical controller generates the slicing and attachment information for the firewall i.e. this information is not generated at the input or logical controller level of the network control system .

The physical controllers in some embodiments receive information about the software port of the managed switching element to which the firewall connects from the managed switching element itself then passes this information down to the firewall. In other embodiments however the use of this port is contracted directly between the firewall module and the managed switching element within the host machine so that the firewall does not need to receive the attachment information from the physical controller. In some such embodiments the managed switching element nevertheless transmits this information to the physical controller in order for the physical controller to customize the universal physical control plane flow entries for receiving packets from and sending packets to the firewall.

The slicing information generated by the physical controller in some embodiments consists of an identifier for the firewall instance to be used for the particular logical network. In some embodiments as described the firewall module operating on the host machine is used by multiple logical networks. When the firewall receives a packet from the managed switching element in some embodiments the packet includes a prepended tag e.g. similar to a VLAN tag that identifies a particular one of the firewall instances i.e. a set of rules within the firewall to use in processing the packet.

As shown in the firewall translates this slicing information into an internal slice binding. In some embodiments the firewall uses its own internal identifiers different from the tags prepended to the packets in order to identify states e.g. active TCP connections within the firewall. Upon receiving an instruction to create a new firewall instance and an external identifier that used on the packets for the new firewall instance some embodiments automatically create the new firewall instance and assign the instance an internal identifier. In addition the firewall module stores a binding for the instance that maps the external slice identifier to the module s internal slice identifier.

A first user inputs a first network configuration i.e. for a first logical network into the logical controller and a second user inputs a second network configuration i.e. for a second logical network into the logical controller . Each of these logical networks includes a distributed firewall as well as several virtual machines with two of the virtual machines from each of the logical networks assigned to the host . In this example the network configurations additionally include the configuration i.e. specification of the packet processing rules for the firewalls within the networks.

Thus the first logical controller converts the first network configuration into a first universal physical control plane and sends this to the physical controller along with the first firewall configuration. One of ordinary skill in the art will recognize that the logical controller will in fact send this information or relevant portions thereof to several physical controllers that manage switching elements on machines that host virtual machines in the first logical network. Similarly the second logical controller converts the second network configuration into a second universal control plane and sends this information to the physical controller along with the second firewall configuration.

The same physical controller is used to convey data for both logical networks to the managed switching element and the firewall on the host . As shown the physical controller converts the first universal physical control plane into a first customized physical control plane and the second universal physical control plane into a second customized physical control plane and transfers both of these sets of flow entries to the managed switching element . This customized physical control plane data includes attachment information in the flow entries that directs packets for the firewall to a particular software port of the managed switching element.

In addition the physical controller conveys both the first and second firewall configurations to the firewall module operating on the host . These configurations specify the packet processing rules that the firewall uses to determine whether to allow drop etc. packets sent to it by the managed switching element. As shown the firewall stores a first set of configuration rules and a second set of configuration rules .

In addition the physical controller transmits slicing data to the firewall for both of the logical networks. As stated this slicing data for a particular one of the networks includes a tag that the managed switching element prepends to packets that are part of the particular network before sending the packets to the firewall . The firewall then knows whether to use the first set of configuration rules or the second set of configuration rules .

While this figure is drawn showing the first and second configurations being sent to the physical controller at the same time and the physical controller passing the first and second configurations to the managed switching element and firewall together one of ordinary skill in the art will recognize that the more likely situation is that the users input the network configurations at different times though this does not mean the two logical controllers are precluded from both accessing the physical controller at the same time .

The above figures illustrate various physical and logical network controllers. illustrates example architecture of a network controller e.g. a logical controller or a physical controller . The network controller of some embodiments uses a table mapping engine to map data from an input set of tables to data in an output set of tables. The input set of tables in a controller include logical control plane LCP data to be mapped to logical forwarding plane LFP data LFP data to be mapped to universal physical control plane UPCP data and or UPCP data to be mapped to customized physical control plane CPCP data. The input set of tables may also include middlebox configuration data to be sent to another controller and or a distributed middlebox instance. The network controller as shown includes input tables a rules engine output tables an importer an exporter a translator and a persistent data storage PTD .

In some embodiments the input tables include tables with different types of data depending on the role of the controller in the network control system. For instance when the controller functions as a logical controller for a user s logical forwarding elements the input tables include LCP data and LFP data for the logical forwarding elements. When the controller functions as a physical controller the input tables include LFP data. The input tables also include middlebox configuration data received from the user or another controller. The middlebox configuration data is associated with a logical datapath set parameter that identifies the logical switching elements to which the middlebox to be is integrated.

In addition to the input tables the control application includes other miscellaneous tables not shown that the rules engine uses to gather inputs for its table mapping operations. These miscellaneous tables include constant tables that store defined values for constants that the rules engine needs to perform its table mapping operations e.g. the value 0 a dispatch port number for resubmits etc. . The miscellaneous tables further include function tables that store functions that the rules engine uses to calculate values to populate the output tables .

The rules engine performs table mapping operations that specifies one manner for converting input data to output data. Whenever one of the input tables is modified referred to as an input table event the rules engine performs a set of table mapping operations that may result in the modification of one or more data tuples in one or more output tables.

In some embodiments the rules engine includes an event processor not shown several query plans not shown and a table processor not shown . Each query plan is a set of rules that specifies a set of join operations that are to be performed upon the occurrence of an input table event. The event processor of the rules engine detects the occurrence of each such event. In some embodiments the event processor registers for callbacks with the input tables for notification of changes to the records in the input tables and detects an input table event by receiving a notification from an input table when one of its records has changed.

In response to a detected input table event the event processor 1 selects an appropriate query plan for the detected table event and 2 directs the table processor to execute the query plan. To execute the query plan the table processor in some embodiments performs the join operations specified by the query plan to produce one or more records that represent one or more sets of data values from one or more input and miscellaneous tables. The table processor of some embodiments then 1 performs a select operation to select a subset of the data values from the record s produced by the join operations and 2 writes the selected subset of data values in one or more output tables .

Some embodiments use a variation of the datalog database language to allow application developers to create the rules engine for the controller and thereby to specify the manner by which the controller maps logical datapath sets to the controlled physical switching infrastructure. This variation of the datalog database language is referred to herein as n Log. Like datalog n Log provides a few declaratory rules and operators that allow a developer to specify different operations that are to be performed upon the occurrence of different events. In some embodiments n Log provides a limited subset of the operators that are provided by datalog in order to increase the operational speed of n Log. For instance in some embodiments n Log only allows the AND operator to be used in any of the declaratory rules.

The declaratory rules and operations that are specified through n Log are then compiled into a much larger set of rules by an n Log compiler. In some embodiments this compiler translates each rule that is meant to address an event into several sets of database join operations. Collectively the larger set of rules forms the table mapping rules engine that is referred to as the n Log engine.

Some embodiments designate the first join operation that is performed by the rules engine for an input event to be based on the logical datapath set parameter. This designation ensures that the rules engine s join operations fail and terminate immediately when the rules engine has started a set of join operations that relate to a logical datapath set i.e. to a logical network that is not managed by the controller.

Like the input tables the output tables include tables with different types of data depending on the role of the controller . When the controller functions as a logical controller the output tables include LFP data and UPCP data for the logical switching elements. When the controller functions as a physical controller the output tables include CPCP data. Like the input tables the output tables may also include the middlebox configuration data. Furthermore the output tables may include a slice identifier when the controller functions as a physical controller.

In some embodiments the output tables can be grouped into several different categories. For instance in some embodiments the output tables can be rules engine RE input tables and or RE output tables. An output table is a RE input table when a change in the output table causes the rules engine to detect an input event that requires the execution of a query plan. An output table can also be an RE input table that generates an event that causes the rules engine to perform another query plan. An output table is a RE output table when a change in the output table causes the exporter to export the change to another controller or a MSE. An output table can be an RE input table a RE output table or both an RE input table and a RE output table.

The exporter detects changes to the RE output tables of the output tables . In some embodiments the exporter registers for callbacks with the RE output tables for notification of changes to the records of the RE output tables. In such embodiments the exporter detects an output table event when it receives notification from a RE output table that one of its records has changed.

In response to a detected output table event the exporter takes each modified data tuple in the modified RE output tables and propagates this modified data tuple to one or more other controllers or to one or more MSEs. When sending the output table records to another controller the exporter in some embodiments uses a single channel of communication e.g. a RPC channel to send the data contained in the records. When sending the RE output table records to MSEs the exporter in some embodiments uses two channels. One channel is established using a switch control protocol e.g. OpenFlow for writing flow entries in the control plane of the MSE. The other channel is established using a database communication protocol e.g. JSON to send configuration data e.g. port configuration tunnel information .

In some embodiments the controller does not keep in the output tables the data for logical datapath sets that the controller is not responsible for managing i.e. for logical networks managed by other logical controllers . However such data is translated by the translator into a format that can be stored in the PTD and is then stored in the PTD. The PTD propagates this data to PTDs of one or more other controllers so that those other controllers that are responsible for managing the logical datapath sets can process the data.

In some embodiments the controller also brings the data stored in the output tables to the PTD for resiliency of the data. Therefore in these embodiments a PTD of a controller has all the configuration data for all logical datapath sets managed by the network control system. That is each PTD contains the global view of the configuration of the logical networks of all users.

The importer interfaces with a number of different sources of input data and uses the input data to modify or create the input tables . The importer of some embodiments receives the input data from another controller. The importer also interfaces with the PTD so that data received through the PTD from other controller instances can be translated and used as input data to modify or create the input tables . Moreover the importer also detects changes with the RE input tables in the output tables .

As described above the firewall of some embodiments is implemented in a distributed fashion with firewall modules operating in some or all of the host machines on which the virtual machines and managed switching elements of a logical network are located. While some embodiments use a centralized firewall implementation e.g. a single virtual machine or a single physical appliance a cluster of firewall resources etc. this section describes the distributed firewall implementation of some embodiments within a host machine.

As illustrated in the host includes hardware kernel user space and VMs . The hardware may include typical computer hardware such as processing units volatile memory e.g. random access memory RAM non volatile memory e.g. hard disk drives flash memory optical discs etc. network adapters video adapters or any other type of computer hardware. As shown the hardware includes NICs and which in some embodiments are typical network interface controllers for connecting a computing device to a network.

As shown in the host machine includes a kernel and a user space . In some embodiments the kernel is the most basic component of an operating system that runs on a separate memory space and is responsible for managing system resources e.g. communication between hardware and software resources . In contrast the user space is a memory space where all user mode applications may run.

The kernel of some embodiments is a software abstraction layer that runs on top of the hardware and runs below any operating system. In some embodiments the kernel performs virtualization functionalities e.g. to virtualize the hardware for several virtual machines operating on the host machine . The kernel is then part of a hypervisor in some embodiments. The kernel handles various management tasks such as memory management processor scheduling or any other operations for controlling the execution of the VMs and operating on the host machine.

As shown the kernel includes device drivers and for the NICs and respectively. The device drivers and allow an operating system e.g. of a virtual machine to interact with the hardware of the host . In this example the device driver allows interaction with the NIC while the driver allows interaction with the NIC . The kernel may include other device drivers not shown for allowing the virtual machines to interact with other hardware not shown in the host .

The virtual machines and are independent virtual machines running on the host machine using resources virtualized by the kernel . As such the VMs run any number of different operating systems. Examples of such operations systems include Solaris FreeBSD or any other type of Unix based operating system. Other examples include Windows based operating systems as well.

As shown the user space which in some embodiments is the user space of the hypervisor includes the firewall daemon the OVS daemon and the OVS DB daemon . Other applications not shown may be included in the user space as well including daemons for other distributed middleboxes e.g. load balancers network address translators etc. . The OVS daemon is an application that runs in the user space . Some embodiments of the OVS daemon communicate with a network controller in order to receive instructions as described above in the previous section for processing and forwarding packets sent to and from the virtual machines and . The OVS daemon of some embodiments communicates with the network controller through the OpenFlow protocol while other embodiments use different communication protocols for transferring the physical control plane data. Additionally in some embodiments the OVS daemon retrieves configuration information from the OVS DB daemon after the network controller transmits the configuration information to the OVS DB daemon.

In some embodiments the OVS DB daemon is also an application that runs in the user space . The OVS DB daemon of some embodiments communicates with the network controller in order to configure the OVS switching element e.g. the OVS daemon and or the OVS kernel module . For instance the OVS DB daemon receives configuration information from the network controller and stores the configuration information in a set of databases. In some embodiments the OVS DB daemon communicates with the network controller through a database communication protocol. In some cases the OVS DB daemon may receive requests for configuration information from the OVS daemon . The OVS DB daemon in these cases retrieves the requested configuration information e.g. from a set of databases and sends the configuration information to the OVS daemon .

The OVS daemon includes an OpenFlow protocol module and a flow processor . The OpenFlow protocol module communicates with the network controller to receive configuration information e.g. flow entries from the network controller for configuring the software switching element. When the module receives configuration information from the network controller it translates the configuration information into information understandable by the flow processor .

The flow processor manages the rules for processing and routing packets. For instance the flow processor stores rules e.g. in a storage medium such as a disk drive received from the OpenFlow protocol module . In some embodiments the rules are stored as a set of flow tables that each includes a set of flow entries. The flow processor handles packets for which integration bridge described below does not have a matching rule. In such cases the flow processor matches the packets against its stored rules. When a packet matches a rule the flow processor sends the matched rule and the packet to the integration bridge for the integration bridge to process. This way when the integration bridge receives a similar packet that matches the generated rule the packet will be matched against the generated exact match rule in the integration bridge and the flow processor will not have to process the packet.

In some embodiments the flow processor may not have a rule to which the packet matches. In such cases some embodiments of the flow processor send the packet to another managed switching element e.g. a pool node for handling packets that cannot be processed by an edge switching element. However in other cases the flow processor may have received from the network controller a catchall rule that drops the packet when a rule to which the packet matches does not exist in the flow processor .

As illustrated in the kernel includes a hypervisor network stack and an OVS kernel module . The hypervisor network stack is an Internet Protocol IP network stack in some embodiments. The hypervisor network stack processes and routes IP packets that are received from the OVS kernel module and the PIF bridges and . When processing a packet that is destined for a network host external to the host the hypervisor network stack determines to which of the physical interface PIF bridges and the packet should be sent.

The OVS kernel module processes and routes network data e.g. packets between VMs running on the host and network hosts external to the host e.g. network data received through the NICs and . In some embodiments the OVS kernel module implements the forwarding tables of the physical control plane for one or more logical networks. To facilitate the processing and routing of network data the OVS kernel module communicates with OVS daemon e.g. to receive flow entries from the OVS daemon . In some embodiments the OVS kernel module includes a bridge interface not shown that allows the hypervisor network stack to send packets to and receive packets from the OVS kernel module .

The integration bridge processes and routes packets received from the hypervisor network stack the VMs and e.g. through VIFs and the PIF bridges and . In some embodiments the integration bridge stores a subset of the rules stored in the flow processor and or rules derived from rules stored in the flow processor that the integration bridge is currently using or was recently using to process and forward packets.

In some embodiments the flow processor of some embodiments is responsible for managing rules in the integration bridge . In some embodiments the integration bridge stores only active rules. The flow processor monitors the rules stored in the integration bridge and removes the active rules that have not been access for a defined amount of time e.g. 1 second 3 seconds 5 seconds 10 seconds etc. . In this manner the flow processor manages the integration bridge so that the integration bridge stores rules that are being used or have recently been used.

Although illustrates one integration bridge the OVS kernel module may include multiple integration bridges. For instance in some embodiments the OVS kernel module includes a separate integration bridge for each logical switching element that is implemented across a managed network to which the software switching element belongs. That is the OVS kernel module has a corresponding integration bridge for each logical switching element that is implemented across the managed network.

The above description relates to the forwarding functions of the managed software switching element of some embodiments. Just as the software switching element includes a user space component that implements the control plane the OVS daemon and a kernel component that implements the data plane the OVS kernel module the firewall of some embodiments includes a control plane component operating in the user space the firewall daemon and a data plane component operating in the kernel the firewall kernel module .

As shown the firewall daemon includes a firewall configuration receiver and a firewall configuration compiler . The firewall configuration receiver communicates with the network controller in order to receive the configuration of the firewall e.g. the packet processing rules for the firewall as well as slicing information. As described above the slicing information assigns an identifier to a particular firewall instance to be performed by the distributed firewall. In some embodiments the identifier is bound to a particular logical firewall in a particular tenant s logical network. That is when a particular logical network includes several different firewalls with different processing rules the firewall daemon will create several firewall instances. Each of these instances is identified with a different slice identifier on packets sent to the firewall. In addition in some embodiments the firewall daemon assigns a particular internal identifier for each of these instances which the firewall uses in its internal processing e.g. in order to keep track of active TCP connections that it is monitoring .

The firewall daemon also includes a firewall configuration compiler . In some embodiments the firewall configuration compiler receives the firewall configuration the packet processing rules for a particular firewall instance in a first language and compiles these into a set of rules in a second language more optimized for the internal processing of the firewall. The firewall configuration compiler sends the compiled packet processing rules to the firewall processor of the firewall kernel module .

The firewall kernel module processes packets sent from and or to VMs running on the host in order to determine whether to allow the packets through drop the packets etc. As shown the firewall kernel module includes a firewall processor to perform these functions. The firewall processor receives translated firewall rules for a particular firewall instance from the firewall configuration compiler . In some embodiments these translated firewall rules specify a packet processing pipeline within the firewall that includes flow entries similar to the ACL tables specified by the managed switching element.

In order to receive packets from the managed switching element the firewall processor of some embodiments connects to a software port abstraction on the integration bridge of the OVS kernel module. Through this port on the integration bridge the managed switching element sends packets to the firewall and receives packets from the firewall after processing by the firewall unless the firewall drops the packet . As described these packets include a slice identifier tag used by the firewall processor to determine which set of compiled packet processing rules to apply to the packet.

The architectural diagram of the distributed firewall and software switching element illustrated in is one exemplary configuration. One of ordinary skill in the art will recognize that other configurations are possible. For instance in some embodiments the firewall processor that applies the compiled packet processing rules is located in the user space rather than the kernel . In such embodiments the kernel exposes the network interfaces and for full control by the user space so that the firewall processor can perform its functions in the user space without a loss of speed as compared to the kernel.

The above sections describe the distributed firewall of some embodiments as well as the configuration of such a firewall in order for the firewall to process packets. Once a logical network has been configured the machines e.g. virtual machines on that network will send and receive packets which requires the use of the packet processing functions of both the managed switching elements and the firewall that reside on the hosts along with the virtual machines.

The logical router has three logical ports each of which corresponds to a set of IP addresses. In this case the virtual machines 1 and 2 connected to the L2 switch have IP addresses in the 1.1.1.0 24 subnet while the virtual machines 3 and 4 connected to the L2 switch have IP addresses in the 1.1.2.0 24 subnet. For the purposes of this example the routing policies at the L3 level state that packets sent from the 1.1.1.0 24 subnet should be sent to the firewall . Furthermore the firewall configuration rules allow packets from this subnet to be sent to the 1.1.2.0 24 subnet so long as certain conditions are met for the TCP connection between the two addresses.

The right side of the figure illustrates the processing of a packet sent from VM 1 to VM 4. As shown VM 1 and VM 3 reside on a first host while VM 2 and VM 4 reside on a second host . Both of these hosts include a managed switching element that implements each of the logical switching elements of the network both logical switches as well as the logical router .

In this example the packet originates from the virtual machine VM 1 which sends the packet to the managed switching element in the first host . The managed switching element begins processing the packet by applying the flow entries for the logical switch i.e. L2 processing . The first such entry maps the ingress port of the managed switch on which the packet was received that corresponds to VM 1 to a logical ingress port i.e. Port 1 of the switch which by default maps the packet to the logical switch itself . Next the L2 processing performs any ingress ACL entries that may drop allow etc. the packet based on its layer 2 information. Assuming the packet is not dropped the L2 processing makes a L2 forwarding decision. In this case the decision forwards the packet to Port 3 of the logical switch which connects to the logical router. In some embodiments before beginning the L3 processing the L2 processing includes egress ACL processing.

Next as shown the managed switching element performs L3 processing on the packet. The L3 processing includes an L3 ingress ACL that operates on the layer 3 information of the packet and then a forwarding decision. In some embodiments the router as implemented in the managed switching element includes a flow entry to forward packets with a destination IP address in subnet 1.1.2.0 24 to L2 switch via Port 2 of the router. However doing so would bypass the firewall given the setup of logical network . Accordingly the L3 router also includes a higher priority flow entry to route packets with a source IP address in subnet 1.1.1.0 24 and which are received at Port 1 i.e. have an L3 ingress context of logical Port 1 of the logical router to the firewall via Port 3 of the router. Thus the forwarding decision made by the L3 processing will route the packet to the logical firewall .

At this point although the firewall is contained within the same physical machine the packet processing by the managed switch is effectively done and any logical context stored on the packet is stripped off. Furthermore the L3 router includes a flow entry for adding the firewall slice identifier to the packet which it adds before sending the packet via a software port to the firewall.

The packet then reaches the firewall processing performed by the firewall component s within the host machine . The firewall receives the packet through the previously mentioned software port then first identifies which of potentially several firewall instances should process the packet. The firewall strips off the slice identifier from the packet and matches this slice identifier to one of its own internal instance identifiers. The firewall also determines whether the packet matches any of its previously created state identifiers for ongoing TCP connections monitored by the firewall by using packet data e.g. source and destination IP etc. as well as the internal firewall instance ID to find a match in its stored set of states. When the packet matches a state the firewall processes the packet e.g. drops allows etc. according to the state. For instance if the state is an ongoing allowed TCP connection between the IP address of VM 1 and the IP address of VM 4 then the packet will be allowed based on this state. When no state is matched the firewall applies its firewall processing rules to determine whether to allow the packet and creates a state for the TCP connection between the IP address of VM 1 and the IP address of VM 4.

Assuming the packet is allowed through the firewall the firewall sends the packet back to the managed switching element. In some embodiments the firewall actually sends out a new packet and the managed switching element treats it as such. The packet is received at the managed switching element through its software port that connects to the firewall which is mapped during L3 processing to ingress Port 3 of the logical router . The managed switching element then performs its usual L3 processing to send the packet to Port 2 based on the destination IP address which connects to the logical switch along with any L3 ACL entries . Because the ingress context for the packet is no longer Port 1 the policy routing flow entry to send the packet to the firewall is not matched and the packet does not end up in a never ending loop with the firewall.

The managed switching element on host contains flow entries to implement the L2 switch as well so the L2 processing is also contained in the first host as well. The L2 processing at the switch maps the packet to the port corresponding to VM 4 Port 2 based on the destination MAC address on the packet then performs egress context mapping to map this to a physical port i.e. Port 6 of the host that connects to the host at which VM 4 is located. In addition the L2 processing adds a tunnel encapsulation to the packet in order to transmit the packet across the physical network to the host . Once the packet reaches the second host the managed switching element on the host identifies the L2 egress context performs any egress ACL processing and forwards the packet to the destination VM 4.

In the above situation the firewall is located hanging off of the L3 router and therefore routing policies not based strictly on packet destination are required in order to send packets to the firewall. on the other hand conceptually illustrates a logical network in which the firewall is located between one of the switches and the L3 router. As such all packets sent between the logical switch and the logical router will be sent through the firewall.

The right side of again illustrates the processing for a packet sent from VM 1 connected to logical switch and located at the first host to VM 4 connected to logical switch and located at the second host . The packet originates from VM 1 which sends the packet to the managed switching element in the first host . The managed switching element begins processing the packet by applying flow entries for the logical switch i.e. L2 processing . This processing maps the physical port Port 4 to the logical port Port 1 of the logical switch then makes an L2 forwarding decision to send the packet to the L3 router via the firewall . Thus the flow entry specifies to actually send the packet to the firewall module in the host .

In some embodiments with the in line firewall shown in the managed switching element creates multiple ports for connecting to the firewall module one of the ports corresponding to the logical switch and the other corresponding to the logical router . When the firewall receives a packet via a first one of the ports it stores instructions to send out a new version of the packet assuming that the packet is not dropped via the other of the two ports to the managed switching element. In other embodiments the same software port is used but different slicing identifiers are added to the packet depending on whether the packet comes from the L3 router or the L2 switch .

Thus the firewall within the host receives the packet via Port Y strips off the slicing identifier and performs the firewall processing . The firewall processing may contain similar rules and state maintenance as described above with respect to . Assuming the packet is allowed through the firewall the firewall sends the packet via Port W to the L3 router in the managed switching element. As in the previous case the packet sent out from the firewall is essentially a new packet. The managed switching element performs ingress mapping to map the received packet via the software port from the firewall to Port 1 of the L3 router . The router performs any ACLs and L3 forwarding to forward the packet to Port 2 of the router.

The managed switching element then performs the L2 processing on the packet. However in some cases the managed switching element at host may not have a flow entry for forwarding the packet based on its destination MAC address i.e. does not have a flow for matching the MAC address of VM 4. In such a case the managed switching element defaults to forwarding the packet to a pool node not shown which then performs the logical switching to direct the packet towards the host . This may also occur in the case of but may be more likely in this situation because the host does not contain any of the VMs on the logical switch .

Assuming no pool node is involved because the forwarding tables at the host include an entry for matching the destination MAC address of the packet the packet is sent to the second host via a tunnel. Once the packet reaches the second host the managed switching element on the host identifies the L2 egress context performs any egress ACL processing and sends the packet to the destination VM 4.

As shown the actual firewall processing is the same irrespective of how the firewall is incorporated into the network i.e. out of band as in or in line as in but the flow entries required to send packets to the firewall are different. In the first case the user specifies routing policies that identify which packets go to the firewall whereas in the second case the user simply specifies the network topology and flow entries are automatically generated to send the packets on the logical wire between two logical forwarding elements to the firewall. As described in some embodiments the second situation also requires the creation of two software ports between the managed switching element and the firewall in the host.

As mentioned above the distributed firewall module of some embodiments creates states for each connection e.g. TCP connection for which it processes packets in order to process packets based on these states. In many situations where numerous virtual machines are implemented on a grid of nodes a virtual machine may be migrated i.e. moved from one host machine to a different host machine e.g. for resource optimization in order to perform maintenance on the physical node etc. . In such situations some embodiments migrate the active states of the distributed firewall that relate to the migrated virtual machine along with the virtual machine.

The firewall element on the first host stores states for each of these ongoing TCP connections TCP A and TCP B . In some embodiments each of these connections is identified by 1 an identifier matching the state with the particular instance of the firewall and 2 a connection identifier. The firewall instance ID in some embodiments is an internal identifier that maps to a slicing identifier appended by the managed switching element in the host to packets destined for the particular firewall instance. The connection identifier in some embodiments uses a set of data unique to the connection to identify the connection. For instance some embodiments use a 5 tuple of source IP address destination IP address source port destination port transport protocol e.g. TCP UDP etc. . Thus the two states stored by the firewall would have different identifiers due to different destination IP addresses and different destination ports. In some embodiments the states store statistics such as number of packets in each direction for a particular connection packet rate etc. as well as timing information e.g. the time of the last packet sent through the connection in order to determine when to time out.

The second stage occurs after the host e.g. a hypervisor on the host receives a command to migrate the virtual machine . In some embodiments a virtual machine configuration manager on the host informs the firewall that the virtual machine will be migrated. This VM configuration manager in some embodiments is a process running within the hypervisor on the host machine. The configuration manager retrieves or is automatically sent all of the state relating to the particular virtual machine being migrated. In some embodiments this state is identified by the firewall instance identifier and all states for that particular instance are sent to the configuration manager.

When multiple virtual machines from the same network that use the same firewall instance are located on the same host some embodiments retrieve all of the state relating to any of these virtual machines because the same identifier is used irrespective of to which of the virtual machines the state relates. In other embodiments the identifier appended to the packet is specific to the source virtual machine i.e. the physical controller assigns several identifiers to a particular firewall instance each identifier signifying a specific VM .

In some embodiments the physical controller assigns both 1 an instance identifier for the particular firewall instance and 2 a VM specific identifier for each VM that sends packets through the particular firewall instance. The packets sent from the managed switching element in some such embodiments then include the instance identifier in the packet and the VM specific identifier is provided outside the packet to the firewall instance. In other embodiments the managed switching element adds a VM specific identifier to the packet e.g. via the attachment flow entries . Because the managed switching element stores the virtual interface from which a packet is received it has the ability to identify the source VM for the packet. In both cases the packets are still sent to the firewall over a single logical port with the flow entries for the attachment after the logical pipeline is completed handling the necessary slice insertion. As the logical network forwarding rules in the logical lookup tables do not differ based on individual source VMs logical forwarding is based on destination MAC or IP unless policy routing rules are defined the attachment entries must use the stored virtual interface information to add the slice information to the packet.

In some embodiments in which VM specific identifiers are used only the state relating to the migrating VM is retrieved from the firewall. However the extraction of state is dependent upon the API of the particular firewall implementation so in some embodiments the network control system may assign identifiers for each particular VM e.g. as an additional identifier to the slice ID but the firewall API may only extract state on a per slice basis.

The VM configuration manager stuns the virtual machine and retrieves or is sent any change to the state since the initial retrieval as the state may change rapidly with each new packet affecting the states to some extent . As shown the firewall state and the virtual machine are migrated to the new location. In this example the host is the new location for virtual machine . Some embodiments migrate this information through the network control system e.g. using the network of physical and logical controllers as a single data blob or as two data blobs. This information in this example is sent to the VM configuration manager on the third host .

The third stage illustrates the three hosts after the migration. The host no longer includes firewall because no virtual machines belonging to the particular logical network remain on the host. While this indicates that the particular firewall instance is no longer present on the host in some embodiments the firewall element still remains present. In addition the VM configuration manager has installed the virtual machine on the host and provided the migrated state to the firewall on that host. The TCP connections between the VM and the other VMs have resumed and are monitored by the firewall .

Many of the above described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium also referred to as computer readable medium . When these instructions are executed by one or more computational or processing unit s e.g. one or more processors cores of processors or other processing units they cause the processing unit s to perform the actions indicated in the instructions. Examples of computer readable media include but are not limited to CD ROMs flash drives random access memory RAM chips hard drives erasable programmable read only memories EPROMs electrically erasable programmable read only memories EEPROMs etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.

In this specification the term software is meant to include firmware residing in read only memory or applications stored in magnetic storage which can be read into memory for processing by a processor. Also in some embodiments multiple software inventions can be implemented as sub parts of a larger program while remaining distinct software inventions. In some embodiments multiple software inventions can also be implemented as separate programs. Finally any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments the software programs when installed to operate on one or more electronic systems define one or more specific machine implementations that execute and perform the operations of the software programs.

The bus collectively represents all system peripheral and chipset buses that communicatively connect the numerous internal devices of the electronic system . For instance the bus communicatively connects the processing unit s with the read only memory the system memory and the permanent storage device .

From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of the invention. The processing unit s may be a single processor or a multi core processor in different embodiments.

The read only memory ROM stores static data and instructions that are needed by the processing unit s and other modules of the electronic system. The permanent storage device on the other hand is a read and write memory device. This device is a non volatile memory unit that stores instructions and data even when the electronic system is off. Some embodiments of the invention use a mass storage device such as a magnetic or optical disk and its corresponding disk drive as the permanent storage device .

Other embodiments use a removable storage device such as a floppy disk flash memory device etc. and its corresponding drive as the permanent storage device. Like the permanent storage device the system memory is a read and write memory device. However unlike storage device the system memory is a volatile read and write memory such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some embodiments the invention s processes are stored in the system memory the permanent storage device and or the read only memory . From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of some embodiments.

The bus also connects to the input and output devices and . The input devices enable the user to communicate information and select commands to the electronic system. The input devices include alphanumeric keyboards and pointing devices also called cursor control devices cameras e.g. webcams microphones or similar devices for receiving voice commands etc. The output devices display images generated by the electronic system or otherwise output data. The output devices include printers and display devices such as cathode ray tubes CRT or liquid crystal displays LCD as well as speakers or similar audio output devices. Some embodiments include devices such as a touchscreen that function as both input and output devices.

Finally as shown in bus also couples electronic system to a network through a network adapter not shown . In this manner the computer can be a part of a network of computers such as a local area network LAN a wide area network WAN or an Intranet or a network of networks such as the Internet. Any or all components of electronic system may be used in conjunction with the invention.

Some embodiments include electronic components such as microprocessors storage and memory that store computer program instructions in a machine readable or computer readable medium alternatively referred to as computer readable storage media machine readable media or machine readable storage media . Some examples of such computer readable media include RAM ROM read only compact discs CD ROM recordable compact discs CD R rewritable compact discs CD RW read only digital versatile discs e.g. DVD ROM dual layer DVD ROM a variety of recordable rewritable DVDs e.g. DVD RAM DVD RW DVD RW etc. flash memory e.g. SD cards mini SD cards micro SD cards etc. magnetic and or solid state hard drives read only and recordable Blu Ray discs ultra density optical discs any other optical or magnetic media and floppy disks. The computer readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code such as is produced by a compiler and files including higher level code that are executed by a computer an electronic component or a microprocessor using an interpreter.

While the above discussion primarily refers to microprocessor or multi core processors that execute software some embodiments are performed by one or more integrated circuits such as application specific integrated circuits ASICs or field programmable gate arrays FPGAs . In some embodiments such integrated circuits execute instructions that are stored on the circuit itself. In addition some embodiments execute software stored in programmable logic devices PLDs ROM or RAM devices.

As used in this specification and any claims of this application the terms computer server processor and memory all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification the terms display or displaying means displaying on an electronic device. As used in this specification and any claims of this application the terms computer readable medium computer readable media and machine readable medium are entirely restricted to tangible physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals wired download signals and any other ephemeral signals.

While the invention has been described with reference to numerous specific details one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. Thus one of ordinary skill in the art would understand that the invention is not to be limited by the foregoing illustrative details but rather is to be defined by the appended claims.

