---

title: Interactive reality augmentation for natural interaction
abstract: Embodiments of the invention provide apparatus and methods for interactive reality augmentation, including a 2-dimensional camera () and a 3-dimensional camera (), associated depth projector and content projector (), and a processor () linked to the 3-dimensional camera and the 2-dimensional camera. A depth map of the scene is produced using an output of the 3-dimensional camera, and coordinated with a 2-dimensional image captured by the 2-dimensional camera to identify a 3-dimensional object in the scene that meets predetermined criteria for projection of images thereon. The content projector projects a content image onto the 3-dimensional object responsively to instructions of the processor, which can be mediated by automatic recognition of user gestures.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09158375&OS=09158375&RS=09158375
owner: APPLE INC.
number: 09158375
owner_city: Cupertino
owner_country: US
publication_date: 20121223
---
This application is a continuation in part of PCT Patent Application PCT IB2011 053192 filed Jul. 18 2011 which claims the benefit of U.S. Provisional Application No. 61 365 788 filed Jul. 20 2010. This application is related to another U.S. patent application filed on even date entitled Adaptive Projector . All of these related applications are incorporated herein by reference.

This invention relates generally to natural interaction systems. More particularly this invention relates to adaptive reality augmentation and 3 dimensional input interfaces.

Natural user interfaces are gaining momentum in the entertainment and computer industry. Gesture controls are supplementing or replacing more conventional and less natural interfaces such as keyboard and mouse game controller and remote control. The user interactions however continue to relate largely to the computer monitor thus limiting applicability and ease of use of such interfaces. Some of the gesture controls rely on optical 3 dimensional mapping.

Various methods are known in the art for optical 3 D mapping i.e. generating a 3 dimensional profile of the surface of an object by processing an optical image of the object. This sort of profile is also referred to as a depth map or depth image and 3 D mapping is also referred to as depth mapping.

Some methods are based on projecting a laser speckle pattern onto the object and then analyzing an image of the pattern on the object. For example PCT International Publication WO 2007 043036 whose disclosure is incorporated herein by reference describes a system and method for object reconstruction in which a coherent light source and a generator of a random speckle pattern project onto the object a coherent random speckle pattern. An imaging unit detects the light response of the illuminated region and generates image data. Shifts of the pattern in the image of the object relative to a reference image of the pattern are used in real time reconstruction of a 3 D map of the object. Further methods for 3 D mapping using speckle patterns are described for example in PCT International Publication WO 2007 105205 whose disclosure is incorporated herein by reference.

The present invention in certain embodiments thereof seeks to provide an improved content projection device which is aware of objects in its field of view recognizing such objects as suitable for projection of content thereon. The projection device may adapt to the geometry and character of the objects by controlling scale distortion focus of the projected content and varying the projected content itself. Additionally or alternatively the projection device may adapt the projected content according to the relationship of the viewer to the projected content such as its gaze vector distance from the surface onto which content is projected and other similar parameters. The 2D 3D input device used to analyze the geometry for projection can also be used to interact with the projected content.

According to disclosed embodiments of the invention methods and apparatus are provided for the projection of content such as the input device interface using a 3 dimensional input device as means of determining the optimal objects to serve as substrate for such content projection.

There is provided according to embodiments of the invention an apparatus for processing data including a sensing element for acquiring a scene including a 2 dimensional camera and a 3 dimensional camera a processor linked to the 3 dimensional camera and the 2 dimensional camera and programmed to produce a depth map of the scene using an output of the 3 dimensional camera and to coordinate the depth map with a 2 dimensional image captured by the 2 dimensional camera to identify a 3 dimensional object in the scene that meets predetermined criteria for projection of images thereon and a content projector for establishing a projected image onto the 3 dimensional object responsively to instructions of the processor.

According to an aspect of the apparatus coordinating the depth map includes identifying a position of the 3 dimensional object with six degrees of freedom with respect to a reference system of coordinates wherein the content projector is operative to compensate for scale pitch yaw and angular rotation of the 3 dimensional object.

According to a further aspect of the apparatus coordinating the depth map includes referencing a database of 3 dimensional object definitions and comparing the 3 dimensional object with the definitions in the database.

An aspect of the apparatus includes a wearable monitor wherein the content projector is operative to establish the projected image as a virtual image in the wearable monitor or in a virtual space. The sensing element the processor and the content projector may be incorporated in the wearable monitor.

According to a further aspect of the apparatus the content projector is operative to establish the projected image onto a virtual surface for user interaction therewith.

According to yet another aspect of the apparatus the processor is operative for controlling a computer application responsively to a gesture and wherein the projected image includes a user interface for control of the computer application.

In another embodiment an apparatus for processing data includes a projector which is configured to project content onto at least a part of a scene and a processor which is configured to detect a location of an eye of a person in the scene and to control the projector so as to reduce an intensity of the projected content in an area of the eye.

Other embodiments of the invention provide methods for carrying out the function of the above described apparatus.

In the following description numerous specific details are set forth in order to provide a thorough understanding of the various principles of the present invention. It will be apparent to one skilled in the art however that not all these details are necessarily always needed for practicing the present invention. In this instance well known circuits control logic and the details of computer program instructions for conventional algorithms and processes have not been shown in detail in order not to obscure the general concepts unnecessarily.

As used herein the term content projection may encompass establishment of an image of the content onto a wearable transparent monitor such as see through eyeglasses and thus invisible to anyone other than the person wearing the glasses or onto a physical object that is visible to anyone interacting with the object. The term is not limited to the above examples. It may encompass forming an image by many means including retinal projection projection onto see through glasses projection of the image into a virtual space for example as a hologram and other techniques for creating augmented reality.

Turning now to the drawings reference is initially made to which is a schematic pictorial illustration of an interactive three dimensional video display system which is constructed and operative in accordance with a disclosed embodiment of the invention. The system incorporates a 3 dimensional 3 D camera which may include an infra red IR projector and corresponding CMOS CCD camera open for the projector band. The terms 3 dimensional camera and 3 D camera as used herein refer to an imaging device used in forming a 3 D map also referred to as a depth map of a scene i.e. an array of 3D coordinates comprising a depth Z coordinate value of the body surface at each point X Y within a predefined area. The 3 D camera captures 3 D information that may includes the body or at least parts of the body of the user tangible entities wielded or operated by the user for controlling a computer application and other objects in the field of view of the 3 D camera . Details of a 3 D imaging assembly of this sort are described for example in PCT International Publication WO 2010 004542 and U.S. Patent Application Publication No. 2009 0183125 which are herein incorporated by reference. The 3 D camera typically operates in the near infra red spectrum. However the principles of the invention are equally applicable to modifications that enable the 3 D camera to capture electromagnetic energy outside the near infra red spectrum for example far infrared or ultraviolet energy. The system may also include a 2 dimensional 2 D camera which operates in the visible spectrum and can acquire a scene with sufficient resolution to allow automatic interpretation of written information in the scene and typically produces a Red Green Blue RGB output signal.

The 3 D camera and the 2 D camera are cooperative with a content projector all under the control of a processor such as a computer .

A suitable unit for use in the system that bundles the 3 D camera and the 2 D camera is the PrimeSensor Reference Design available from PrimeSense Corporation 104 Cambay Conn. Cary N.C. 27513 U.S.A. The content projector may be the PicoP display engine available from MicroVision Inc. 6222 185th Ave NE Redmond Wash. 98052. In some embodiments the 3 D camera and the 2 D camera may be integral with the content projector as a modification of the PrimeSensor Reference Design. In one embodiment the 3 D camera is an integrated module that includes an IR projector which projects a pattern of spots onto the object and captures an image of the projected pattern. Alternatively the IR projector may be embodied as a separate module not shown . The IR projector may be realized according to the teachings of U.S. Provisional Applications 61 372 729 filed Aug. 11 2010 and 61 425 788 filed Dec. 22 2010 as well as in PCT International Publication WO 2010 020380 all of which are herein incorporated by reference. These provisional and PCT applications also teach how to reuse the scanning hardware to project both the IR required for depth mapping and the visible content.

The processor may analyze the scene using the teachings of commonly assigned copending U.S. Patent Application Publication 2011 0293137 entitled Analysis of Three Dimensional Scenes which is herein incorporated by reference.

The computer may comprise a general purpose computer processor which is programmed in software to carry out the functions described hereinbelow. The software may be downloaded to the processor in electronic form over a network for example or it may alternatively be provided on non transitory tangible storage media such as optical magnetic or electronic memory media. Alternatively or additionally some or all of the image functions may be implemented in dedicated hardware such as a custom or semi custom integrated circuit or a programmable digital signal processor DSP . Although the computer is shown in by way of example as a separate unit from the 3 D camera some or all of the processing functions of the computer may be performed by suitable dedicated circuitry associated with or within the housing of the 3 D camera and the 2 D camera . As will be seen from the discussion below elements of the system may be miniaturized and incorporated in a wearable monitor to enable the user to move about and more freely interact with the scene in near real time. In any case the 3 D camera and the 2 D camera function as a sensor component which observes a scene users and their surroundings . The computer functions as a perception component which comprehends the scene and user interaction within these surroundings as mediated or stimulated by information provided by the content projector .

The computer may execute programs such as Nite Middleware available from PrimeSense in cooperation with the PrimeSensor Reference Design. For example the PrimeSensor Reference Design supplies an application layer in the computer with control widgets thereby providing an application programming interface API that translates user gestures or postures into known deterministic application inputs. The Middleware performs image processing operations on data generated by the components of the system including the 3 D camera with its IR projector and the 2 D camera in order to reconstruct 3 dimensional maps of a user and acquired scenes. The term 3 dimensional map refers to a set of 3 dimensional coordinates representing the surface of a given object. One form of 3 dimensional map is referred to as a depth image or depth map in which each pixel has a value indicating the distance from the camera to the corresponding point in the scene rather than the brightness and color of the point as in a 2 dimensional image. The computer then computes the three dimensional coordinates of points on the surface of the control entity by triangulation based on transverse shifts of the spots in the pattern.

In typical applications information captured by the 3 D camera is processed by the computer which drives the content projector . The computer may operate according to a program that is designed to create a natural or contrived experience for the user. As shown in the system has recognized a book in the scene and has projected a sale offer onto the book Buy at 75.99 . The user is reacting to the offer by a hand gesture 26 which acts as an input to the computer . Gesture control of a computing device is known for example from commonly assigned U.S. Patent Application Publication No. 2009 0183125 which is herein incorporated by reference and which also teaches methods of projection of scenes into a virtual image space. Gesture control is included in the functionality of the Nite Middleware which may interpret gestures of the user for example in response to the sale offer that are acquired by the 3 D camera and the 2 D camera .

Furthermore as the interaction of the user with the book and the sale offer evolves for example by the user grasping the book a gaze identification module executing in the computer may recognize that the user is looking at the book . By processing the acquired 2 D images the book title may be recognized and interpreted in the system . Then computing optimal projection parameters a book review may be projected onto the book . The user could scroll and interact with the projected book review as if he were viewing it on a display screen. In this way the system cooperatively with the user converts the book in an ad hoc fashion into a virtual information screen for the benefit of the user a.

The system optionally includes a display screen and conventional input devices such as a keyboard and mouse which may present a user interface for administrative use e.g. system configuration and for operational control of the system by the user .

Reference is now made to which is a block diagram of the system in accordance with an embodiment of the invention. A scene is acquired concurrently by two cameras a 2 D camera and a 3 D camera which may be separate units or integral as a combined unit. Alternatively the scene can be captured by the 3 D camera only or by the 2 D camera only image analysis performed on the images acquired in any case. As noted above these cameras may be realized as the PrimeSensor Reference Design. Data output by the 2 D camera and a 3 D camera are input to a processor which executes middleware for example the above mentioned Nite Middleware. The Middleware places the scenes captured by the two cameras in registration. The middleware includes an object analysis module which identifies objects in the scene and determines their suitability for content projection thereon. A projector control module another component of the Middleware converts coordinates and characteristics of objects in the scene for example an object and prepares an image for projection. The module issues suitable instructions for a projector such that the image typically containing information content is projected onto the object . The instructions may contain corrections for distortion attributable to the scale attitude and configuration of the object . Additionally or alternatively the projector may include its own mechanisms to compensate for such distortion.

The position and attitude of the user may be taken into consideration when computing projection parameters. For example as noted above the gaze vector toward the projected content may vary as the user moves about in the scene. The projection parameters may be accordingly adjusted to compensate for such variations e.g. by adjusting for scale parallax and similar distortions so as to simulate a realistic experience for the user. One example of such adjustment is a correction for the fact that 3 dimensional objects appear differently when viewed from different directions i.e. different sides of the object or different 2 D projections of the object become apparent to the observer. The projection content can be adjusted as a function of the gaze vector and user position relative to virtual object thus creating a realistic experience of the object actually being in the presence of the observer. Gaze direction can be determined by methods known in art. For example in the case of a device embedded in see through glasses head position orientation is obtainable by rigid registration of the world relative to the device. Gaze direction can also be measured for example using eye tracking products available from Tobii Technology Inc. 510 N Washington Street Suite 200 Falls Church Va. 22046. Gaze may then be translated into object coordinates using 3D information obtained by the sensor.

Techniques for identifying and tracking body parts are known from commonly assigned U.S. Patent Application Publication No. 2011 0052006 entitled Extraction of Skeletons from 3 D Maps which is herein incorporated by reference. Essentially this is accomplished by receiving a temporal sequence of depth maps of a scene containing a humanoid form. A digital processor processes at least one of the depth maps so as to find a location of a designated body part such as the head or hand estimates dimensions of the humanoid form based on the location. The processor tracks movements of the humanoid form over the sequence using the estimated dimensions. These teachings are employed in the above mentioned Nite Middleware and may be enhanced by linking other known recognition routines by those skilled in the art.

For example in the case of identifying the head of the body the processor may segment and analyzes a 3 dimensional form to identify right and left arms and then search the space between the arms in order to find the head. Additionally or alternatively recognition techniques may be used. The depth maps may be registered with 2 dimensional images of the head or other object. The processor may apply a pattern or face recognition technique to identify the face of a humanoid form in a 2 dimensional image. The face location in the 2 dimensional image is then correlated with the location of the head of the 3 dimensional form. Using the same techniques an entire scene may be analyzed segmented and known categories of objects identified as candidates for projection of images thereon.

In one embodiment which is shown in upon recognizing the head in an area in which an image is being projected the processor may instruct the projector to reduce the intensity of the light that is projected in the area of the head or turn it off entirely in order to avoid projecting bright light into the eyes which can be uncomfortable and even hazardous.

Reference is now made to which is a block diagram that schematically shows functional elements of a portion of an exemplary processing device which is a component of the processor and which is constructed and operative in accordance with an embodiment of the invention. The processing device may be fabricated as a dedicated integrated circuit on a single semiconductor substrate with a USB port to an optional host computer . Device may include other interfaces as well including an object analyzer . The object analyzer is linked to a database which holds a library containing descriptions of objects to be recognized and evaluated by the object analyzer . It will be appreciated that alternative configurations of the processing device can be constructed by those skilled in the art. As noted above the operations of the processing device may be controlled by middleware residing in instruction memory and data memory

A depth processor processes the information captured by the 3 D camera in order to generate a depth map. Depth processor uses dedicated memory space in a memory . This memory can also be accessed by a controller which is described hereinbelow but typically not by the host computer . Rather depth processor may be programmed by the host computer via an application program interface API .

Depth processor receives input IR data from 3 D camera via a depth CMOS interface . The depth processor processes the video data in order to generate successive depth maps i.e. frames of depth data. The depth processor loads these data into a depth first in first out FIFO memory in a USB FIFO unit .

In parallel with the depth input and processing operations a color processing block receives input color video data from the 2 D camera via a color CMOS sensor interface . The block converts the raw input data into output frames of RGB video data and loads these data into a RGB FIFO memory in the unit . Alternatively the block may output the video data in other formats such as YUV or Bayer mosaic format.

The unit acts as a buffer level between the various data suppliers and a USB controller . The unit packs and formats the various data types according to different classes such as a USB video class and a USB audio class and also serves to prevent data loss due to USB bandwidth glitches. It arranges the data into USB packets according to the USB protocol and format prior to transferring them to the USB controller.

A high bandwidth bus such as an Advanced High performance Bus AHB matrix is used to carry data between the components of the processing device and specifically for conveying data from the unit to the USB controller for transfer to the host computer . AHB is a bus protocol promulgated by ARM Ltd. of Cambridge England. When there are packets ready in the unit and space available in the internal memory of USB controller the USB controller uses direct memory access DMA to read data from memory memory and an audio FIFO memory via an AHB slave module and the matrix . The USB controller multiplexes the color depth and audio data into a single data stream for output via the USB port to the host computer .

For the purpose of USB communications they processing device comprises a USB physical layer interface PHY which may be operated by the USB controller to communicate via a suitable USB cable with a USB port of the host computer . The timing of the USB PHY is controlled by a crystal oscillator and a phase locked loop PLL as is known in the art.

Alternatively USB controller may optionally communicate with the host computer via a USB 2.0 Transceiver Macrocell Interface UTMI and an external PHY .

Various external devices may connect with the processing device cooperatively with the host computer including a projector control module which accepts instructions from the processing device and the host computer to effect a desired image projection onto specified coordinates in space.

The controller is responsible for managing the functions of the processing device including boot up self test configuration power and interface management and parameter adjustment.

The controller may comprise a digital signal processor DSP core and an AHB master for controlling data movement on the matrix . Typically controller boots from a boot read only memory and then loads program code from a flash memory not shown via a flash memory interface into instruction random access memory and data memory . The controller may in addition have a test interface such as a Joint Test Action Group JTAG interface for purposes of debugging by an external computer .

The controller distributes configuration data and parameters to other components of the processing device via a register configuration interface such as an Advanced Peripheral Bus APB to which the controller is connected through the matrix and an APB bridge .

Further details of the processing device are disclosed in the above noted PCT International Publication WO 2010 004542.

Continuing to refer to the object analyzer evaluates data developed by the depth processor in cooperation with the block and the unit to evaluate a scene captured by the 3 D camera .

The algorithm executed by the object analyzer may be dictated by an application program in the host computer . For example the object analyzer may be instructed to search for and report one or more known objects in the scene that are specified in the database . The host computer may thereupon instruct the content projector to project images on the selected object or objects. Additionally or alternatively the object analyzer may be instructed to identify and report objects meeting predefined criteria without resort to the database .

The data communicated by the object analyzer with respect to an identified object typically includes the size and location of the object as well as its orientation preferably with six degrees of freedom including scale pitch yaw and angular rotation with respect to a reference system of coordinates. This information allows the projector to compensate for distortions by suitably scaling and contorting a projected image so as to be project it onto the selected object such that the viewer sees an image that is substantially distortion free. Configuration of a projected image is known e.g. from U.S. Patent Application Publication No. 20110081072 entitled Image Processing Device Image Processing Method and Program . The image may be configured in software in order to avoid the expense of complex optical arrangements and to more easily achieve freedom from such effects as off axis image distortion Alternatively As noted above commercially available projects may provide their own compensation for distortion control.

Reference is now made to which is an exemplary flow chart of a method of identifying 3 dimensional objects in a scene in accordance with an embodiment of the invention. For convenience of presentation the method is disclosed in conjunction with the apparatus shown in and but it is applicable to apparatus configured differently. The process steps are shown in a particular linear sequence in for clarity of presentation. However it will be evident that many of them can be performed in parallel asynchronously or in different orders. Those skilled in the art will also appreciate that a process could alternatively be represented as a number of interrelated states or events e.g. in a state diagram. Moreover not all illustrated process steps may be required to implement the process. Furthermore many details may vary according to the dictates of the host computer and the requirements of its application program.

Assume that the viewer is located in a bookshop. At initial step an application program executing in the host computer would like to identify an open book displaying textual information. This is a 3 dimensional object having a known definition in the database that includes at least one generally light colored planar surface. The 3 D camera is enabled and a 3 dimensional scene captured in the processing device . The object analyzer evaluates the scene locates and identifies objects in 3 dimensional space.

Control now proceeds to decision step where it is determined if the planar surface meets criteria for a book. The criteria may involve inter alia size proximity to certain other objects and geometric details corresponding to a closed or open book.

If the determination at decision step is affirmative then control proceeds to final step . The coordinates and orientation of the book are reported by the object analyzer to the controller which instructs the projector control module cooperatively with the host computer to display an application determined image MENU 1 on the identified book. The image may contain for example options to purchase the item or obtain additional details for example book reviews and popularity ratings. Indeed if the 3 D camera was successful in capturing the title of the book the additional details may be included in the projected image. It is assumed that the host computer has access to a local or distributed database or can make automatic inquiries via the Internet.

The coordinates and other characteristics of the book or of any other object onto which an image is to be projected can also be used in controlling projection parameters such as the intensity of light projected in the image. Thus for example the projector may increase the intensity of the projected light when the object is relatively far from the projector and decrease it for nearby objects. Additionally or alternatively the reflectivity of the object may be assessed using image data from camera for example and the intensity of the projected light may be increased when projected onto less reflective objects and decreased for more reflective objects.

If the determination at decision step is negative then control proceeds to decision step . A determination is made if more objects are present in the scene for processing.

If the determination at decision step is negative then a second state of the method commences. It is assumed that the application program falls through to a secondary option in which an image is projected on the user s hand if visible to the 3 D camera .

Control now proceeds to decision step where it is determined if a body part is present in the scene. This may be accomplished using the teachings of the above noted U.S. Patent Application Publication No. 2011 0052006.

If the determination at decision step is affirmative then control proceeds to decision step where it is determined if the body part is a hand.

If the determination at decision step is affirmative then control proceeds to final step which is similar to final step . However a different menu MENU 2 is now projected on the hand which may include for example control options for the governing computer application. In both final step and final step the image is configured so as to create a natural feeling on the part of the user when interacting with the content.

Alternatively or additionally the object analyzer may determine whether the body part in question is a head and if so may instruct the projector to reduce or turn off the projected intensity in the area of the head. This option is described in greater detail hereinbelow with reference to .

If the determination at decision step is negative then control proceeds to decision step . A determination is made if more objects are present in the scene for processing.

If the determination at decision step is affirmative then control returns to decision step . Otherwise control passes to final step in which a conventional menu display is presented on a display screen. Final step represents a failure to identify a suitable external object for projection of an image thereon. It will be appreciated that the method shown in can be varied and elaborated as required to comply with the specifications of the governing application program. Recognition and prioritization of various objects and images may be programmed so as to accommodate the configuration of a particular scene and the needs of the program itself.

This embodiment is similar to the first embodiment except a convenient virtual surface is provided for projection of images and for access by the user. Reference is now made to which illustrates a screen typically of a mobile information device such as a cellular telephone e.g. a smart phone that is projected onto a virtual surface in accordance with an embodiment of the invention. Such devices are too small for convenient interaction and media consumption. The screen incorporates a miniature projector and sensing device which have the same functions as the 3 D camera and content projector in the embodiment of . Projectors suitable for this purpose are available for example from Microvision. In this embodiment the projector projects an image onto a virtual projection surface which is enlarged relative to the screen .

In one mode of operation the projector may create an enlarged version of information displayed on the screen .

In another mode of operation the sensing device captures an external scene. The mobile information device is configured to perform the method of scene analysis described above with reference to . In this example an open book was identified in the external scene. An application program executing in the mobile information device has caused the projector to project an image of the book onto the projection surface and to superimpose a menu onto the image . The menu invites the user to purchase the book at a sales price of 75.99 or to cancel the display.

In the first embodiment images have been described as projections onto a physical object e.g. a book or a hand. In this embodiment the projector may be embodied as a device that projects content onto a wearable monitor such as eye glasses. In this embodiment final step and final step are modified in the method of .

Reference is now made to which illustrates an interactive three dimensional video display system having a wearable monitor in accordance with an embodiment of the invention. The system is configured to project the respective images onto the wearable monitor rather than the object themselves Such devices offer possibilities of allowing a computer generated image produced by the method described with reference to to be generated and optionally superimposed on a real world view. Such devices may operate by projecting the computer generated image through a partially reflective mirror while viewing an external scene. Alternatively the device may mix the computer generated image and real world view electronically.

In the example of a user employs a wearable monitor which is capable of displaying stereoscopic imagery. The wearable monitor is provided with or interfaced with components similar to those of the system . Like the system the wearable monitor is adapted to analyze an external scene. In this example it identifies the book and generates an image containing the same information as the image . The wearable monitor may be a separate unit or may incorporate other elements of the system . In the embodiment of the wearable monitor includes a miniature projector and a sensing element . Additionally or alternatively the wearable monitor may communicate with an external processor or sensing device via a wireless link. Suitable wearable helmet mounted displays and see through eyewear displays for use as the wearable monitor are available as the Madison line of Novero novero.com or from Lumus Ltd. 2 Bergman Street Rehovot 76705 Israel.

While the image is actually established within the wearable monitor in some embodiments it may be perceived by the user as being superimposed in an external region of space as shown in . The wearable monitor in such embodiments may be equipped with positioning head tracking and eye tracking subsystems.

As shown in a beam combiner such as a dichroic reflector aligns the IR beam from a radiation source with a visible beam from a visible light source . Source may be monochromatic or polychromatic. For example source may comprise a suitable laser diode or LED for monochromatic illumination or it may comprise multiple laser diodes or LEDs of different colors not shown whose beams are modulated and combined in order to project the desired color at each point in the field of view. For this latter purpose combiner may comprise two or more dichroic elements not shown in order to align all of the different colored and IR beams.

A scanning mirror or a pair of scanning mirrors not shown scans the beams from sources and typically in a raster pattern over the field of view of camera . While the beams are scanned projector control in processor modulates sources and simultaneously Source is modulated to generate the desired pattern for 3 D mapping at each point in the field while source is modulated according to the pixel value intensity and possibly color of the visible image that is to be projected at the same point which may be based on the 3 D map of the scene at that point . Because the visible and IR beams are optically aligned and coaxial the visible image will be automatically registered with the 3 D map. Alternatively in place of camera projector may also contain another sort of sensing element such as an IR detector not shown whose field of view is scanned so as to coincide with the projection scan. Such detection schemes are described for example in the above mentioned PCT International Publication WO 2010 020380. Additionally or alternatively the projector may contain also contain a detector or detectors for visible light in order to form a color image of the scene.

The projector shown in is particularly useful in adjusting the projected image to the characteristics of the scene since it enables the projected pattern to be modified on the fly pixel by pixel in perfect registration with the 3 D map that provides the scene information. As a particular example when the presence of person is detected in the scene by suitably segmenting and analyzing the 3 D map the intensity of source may be decreased possibly to the point of turning off the source altogether in the area of the person s head or at least in the area of the eyes. In this manner projector avoids shining bright light into the person s eyes which could otherwise cause discomfort and even eye damage.

This principles of this embodiment may be applied using other types of imaging and projection devices and are not limited to the particular sort of scanning projector and mapping device that are described above. For example other types of mapping and imaging devices as well as other image analysis techniques which may operate on either a 2 D image captured by a suitable capture device or a 3 D map may be applied in identifying the area of the eyes for this purpose. Similarly substantially any suitable type of electronically driven projector including standard video projectors can be controlled in this manner to reduce intensity in the area of the eyes as long as an image or map of the area onto which the projector casts its beam is registered in the frame of reference of the projector. Thus when the location of the head and or eyes that is found in the image or map the corresponding part of the projected beam can be dimmed accordingly.

It will be appreciated by persons skilled in the art that the present invention is not limited to what has been particularly shown and described hereinabove. Rather the scope of the present invention includes both combinations and sub combinations of the various features described hereinabove as well as variations and modifications thereof that are not in the prior art which would occur to persons skilled in the art upon reading the foregoing description.

