---

title: Computer action detection
abstract: A tool for performing a computer action based on a user's touch on a touch screen. The user's touch is characterized by certain parameters which are compared to predefined parameters which correspond to the hand that was used to touch the screen and a specific computer action. If the parameters match, within a specific error tolerance, the computer action is performed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09001058&OS=09001058&RS=09001058
owner: International Business Machines Corporation
number: 09001058
owner_city: Armonk
owner_country: US
publication_date: 20120305
---
The present invention relates generally to user input on a computing device and more specifically to associating a user input on a touch screen with a specific computer action.

Portable computing devices such as smart phones tablet computers and satellite navigation systems are becoming more prevalent. Comprehensive applications are being created to utilize the capabilities of these portable computing devices. Portable computing devices typically include a touch screen and a multitude of other sensors that allow for interaction between a user and the device.

Touch screens allow a user to make selections or move a cursor by touching the touch screen via a finger or stylus. In general touch screens can recognize the size shape and position of the touch and output this information to a host device. The host device may be a handheld computer tablet computer or smart phone. Some touch screens recognize single touches while others can recognize multiple simultaneous touches.

Touch screens typically include a touch panel a display screen and a controller. The touch panel is a clear panel with a touch sensitive surface. The touch panel is positioned in front of the display screen so that the touch sensitive surface covers the viewable area of the display screen. The touch panel registers touches and sends these signals to the controller. The controller processes these signals into data and sends the data to the host device. Any device that houses a touch screen generally provides an Application Programming Interface API that programs can call to utilize the data.

Proximity sensors are one type of sensor that can be included on a portable computing device to allow for user interaction with the device. For example a proximity sensor can detect if a user is touching or is close to touching the proximity sensor. The maximum distance that a proximity sensor can detect is defined as the nominal range. The nominal range is sensor specific and may be adjustable. Generally data from a proximity sensor is accessed through an API provided with the portable computing device. The data is typically a Boolean value yes or no indicating whether the user is within the nominal range of the sensor.

Accelerometers and gyroscopes are other types of sensors that can be included on a portable computing device. Accelerometers and gyroscopes can allow a user to interact with the portable computing device by tilting rotating or otherwise moving the device. Accelerometers can detect a device s linear acceleration along the x y and z axes. Gyroscopes can detect a device s angular acceleration around the x y and z axes permitting precise calculation of pitch yaw and roll. In general data from an accelerometer and a gyroscope can be accessed by a program calling an API that is provided with the portable computing device. The data can be raw data from the sensors or data processed by the host device in the form of information regarding the position rotation rate direction of gravity and acceleration of the portable computing device.

Aspects of an embodiment of the present invention disclose a method system and a program product for performing a computer action based on a user input on a touch screen of a computer. The method comprises a computer receiving the user input on the touch screen and determining the parameters characterizing the user input. The method further comprises the computer determining which hand was used to enter the user input. The method further comprises the computer determining that the parameters characterizing the user input match predefined parameters corresponding to the hand used to enter the user input and a specific computer action within a specific error tolerance. The method further comprises the computer performing the specific computer action.

The present invention will now be described in detail with reference to the figures. depicts a diagram of a computing system in accordance with one embodiment of the present invention. provides only an illustration of one embodiment and does not imply any limitations with regard to the environments in which different embodiments may be implemented.

In the depicted embodiment computing system is a handheld computing device such as a tablet computer personal digital assistant PDA smart phone or a satellite navigation system. In general computing system may be any handheld electronic device capable of accepting user input on touch screen and executing computer program instructions. Computing system includes touch screen user interface user input interpretation program shape store computer action setup program touch screen API and sensor API .

Computing system also includes a plurality of proximity sensors integrated around the edge of the computing system as shown in . The plurality of proximity sensors are configured to detect if a user is touching the sensors. Generally data from the plurality of proximity sensors is accessed by a program calling an Application Programming Interface API sensor API provided with computing system . The data is typically a Boolean value yes or no indicating whether the user is touching the sensor.

Computing system also includes along with the plurality of proximity sensors a gyroscope and an accelerometer not shown integrated with the computing system. The gyroscope and accelerometer are configured to detect the linear acceleration and angular acceleration around the x y and z axes of computing system and to send data to the computing system. Generally data from the gyroscope and the accelerometer can be accessed by a program calling an API sensor API provided with computing system . The data can be raw data from the sensors or data processed by the host device in the form of information regarding the position rotation rate direction of gravity and acceleration of the portable computing device.

Touch screen is integrated with computing system . Touch screen is configured to receive input from a user s touch and to send parameters characterizing the input from the user s touch to computing system . Generally the parameters from touch screen can be accessed by a program calling an API touch screen API provided with computing system . The parameters can be raw data from touch screen or information on the size shape and position of the touch. In one embodiment the parameter characterizing the input from the user s touch is the shape of the input from the user s touch. The shape of the input from the user s touch can be defined as an ordered list of points describing the path that forms the bounds of the input and or an ordered list of two dimensional geometric segments describing the path that forms the bounds of the input. An ordered list of points or segments may be referred to as a polyline.

User interface operates on computing system and works in conjunction with touch screen to visualize content such as icons and application material and allows a user to select a specific location on the touch screen. User interface may comprise one or more interfaces such as an operating system interface and application interfaces. User interface receives the data characterizing the input from the user s touch on touch screen from touch screen API and reports the data to user input interpretation program or computer action setup program .

User interface also receives a Boolean value yes or no from sensor API for each sensor in the plurality of proximity sensors indicating whether the user is touching one or more of the sensors during the user s touch on touch screen . User interface then reports the Boolean value to user input interpretation program or computer action setup program .

User interface also receives gyroscope and accelerometer data from sensor API indicating the computing system s position rotation rate direction of gravity and acceleration during the user s touch on touch screen . User interface then reports this information to user input interpretation program or computer action setup program .

Computer action setup program operates on computing system to associate specific computer actions with the parameters characterizing input from a user s touch for use with user input interpretation program . In one embodiment the user identifies the specific computer action to set up. For example the user can choose to define parameters for a right hand right click which would mimic the right click of a mouse. After the user chooses the computer action the user will make a desired input on touch screen . For example for a right hand right click computer action the user may touch the touch screen with the middle finger of their right hand. The parameters of the touch for the right hand right click computer action are then stored for use with user input interpretation program . In one embodiment the parameter characterizing the input from the user s touch is the shape of the input from the user s touch and the shape of the input is stored in shape store .

In another embodiment computer action setup program operates on computing system to associate specific computer actions with the parameters characterizing an input from a user s touch for use with user input interpretation program . In one embodiment the user identifies the specific computer action to set up. For example the user can choose to define parameters for a right hand right click which with computing system at a certain position would mimic the right click of a mouse. After the user chooses the computer action the user will make a desired input on touch screen with computing system at the certain position see .

Shape store is a file that may be written to by computer action setup program and read by user input interpretation program . Shape store operates to store the shape of the input from the user s touch on touch screen received from computer action setup program . In other embodiments shape store may be a database such as an Oracle database or an IBM DB2 database.

User input interpretation program operates on computing system to perform a computer action based on input from a user s touch on touch screen and which hand was used to enter the user s touch. In other embodiments other body parts such as a foot may be used to enter the user s touch on touch screen .

In step user input interpretation program receives from user interface the shape of the input from the user s touch on touch screen . The shape of the input from the user s touch can be defined as an ordered list of points describing the path that forms the bounds of the input and or an ordered list of two dimensional geometric segments describing the path that forms the bounds of the input. An ordered list of points or segments may be referred to as a polyline.

User input interpretation program determines the hand used to enter the user input on touch screen step . In one embodiment user input interpretation program receives from user interface a Boolean value yes or no for each sensor in the plurality of proximity sensors indicating whether the user is touching the sensor during the user s touch on touch screen . User input interpretation program also receives from user interface an indication of the orientation of computing system during the user s touch on touch screen . The orientation of computing system can be described as portrait portrait upside down landscape left or landscape right. Generally a computing system will have a standard top edge designated by a button or a headphone jack that is used as a reference when describing other orientations of the computing system. A standard bottom edge corresponds to the edge opposite to the standard top edge.

User input interpretation program determines the hand used to enter the user input on touch screen using the information received from user interface . For example if computing system has an orientation of landscape right the standard top of the computing system is to the right and one or more proximity sensors on the standard bottom edge of the computing system have a Boolean value of yes user input interpretation program determines that the right hand entered the user input on touch screen see .

In step user input interpretation program determines that the shape of the input from the user s touch on touch screen matches a predefined shape corresponding to the hand used to enter the user input and a specific computer action within a specific error tolerance. The specific error tolerance can be determined by multiple samples of the shape of the user s touch on touch screen during multiple iterations of the set up process shown in for a specific computer action. The multiple samples set a range of acceptable shapes. The shape of the input from the user s touch must be within this acceptable range. In one embodiment user input interpretation program queries shape store to fetch the predefined shapes corresponding to the hand used to enter the user input. For example if user input interpretation program determines that the right hand enters the user input on touch screen the user input interpretation program queries shape store and fetches the predefined shapes corresponding to the right hand for comparison.

If user input interpretation program determines that the shape of the input from the user s touch on touch screen matches a predefined shape corresponding to the hand used to enter the user input and a specific computer action within a specific error tolerance then user input interpretation program performs the specific computer action step .

Step and step are equivalent to step and step in . User input interpretation program receives data describing the position of computing system during the user s touch on touch screen step . In one embodiment the data describing the position of computing system during the user s touch on touch screen received by user input interpretation program from user interface is data from a gyroscope and accelerometer in the form of the angle of rotation around the x axis with respect to gravity. In one embodiment the x axis passes through computing system from side to side as shown in .

In step user input interpretation program determines that the shape of the input from the user s touch on touch screen matches a predefined shape corresponding to the hand used to enter the user input the data describing the position of computing system and a specific computer action within a specific error tolerance. In one embodiment user input interpretation program queries shape store to fetch the predefined shapes corresponding to the hand used to enter the user input and the data describing the position of computing system . For example if user input interpretation program determines that the right hand entered the user input on touch screen and it receives an angle of rotation where the top of computing system is rotated 15 degrees away from the user during entry of the user input then the user input interpretation program queries shape store and fetches the predefined shapes corresponding to the right hand and the angle of rotation of 15 degrees away from the user for comparison.

If user input interpretation program determines that the shape of the input from the user s touch on touch screen matches a predefined shape corresponding to the hand used to enter the user input the data describing the position of computing system and a specific computer action within a specific error tolerance then user input interpretation program performs the specific computer action step .

Given that computing system is in landscape right orientation and the user is in contact with proximity sensors during touch user input interpretation program determines the user s right hand was used to input touch . User input interpretation program fetches predefined shapes that correspond to the user s right hand and a specific computer action and determines if the shape of touch matches a predefined shape. In this example the shape of touch matched the predefined shape corresponding to the user s right hand and the specific computer action of a right click.

Because the shape of touch matched the predefined shape corresponding to the user s right hand and the specific computer action of a right click user input interpretation program proceeds with instructions of the right click. The instructions of the right click indicate to computing system that a right click is being inputted by the user. In this example a right click in user interface displays a right click menu with different actions the user may select.

In step computer action setup program receives from user interface an indication that the user is requesting setup of a computer action. In one embodiment step involves the user selecting a computer action setup function in user interface upon which the user interface sends the indication to computer action setup program .

In response to receiving the indication of the computer action setup request computer action setup program sends user interface a request for the user to choose the computer action to set up step . User interface displays on touch screen a list of specific computer actions available for set up. Custom actions can be entered by the user for use with certain programs. For example the user can choose to define parameters for a right hand right click computer action which mimic the right click of a mouse by selecting that computer action from the list displayed on touch screen .

Computer action setup program receives from user interface an indication of the computer action the user selected to set up step . In response to receiving the indication of the computer action selected to set up computer action setup program sends to user interface a prompt for the user to enter a user input on touch screen step . For example for a right hand right click computer action the user may touch touch screen with the middle finger of their right hand.

In step computer action setup program receives from user interface the parameters characterizing the user input on touch screen corresponding to the hand used to enter the user input and a specific computer action. In one embodiment the parameter characterizing the user input is the shape of the input from the user s touch as discussed previously. For example the shape of the user input for the right hand right click computer action is the shape of the part of the middle finger of the user in contact with touch screen .

Computer action setup program stores the shape of the user input on touch screen corresponding to the hand used to enter the user input and a specific computer action in shape store step . User input interpretation program may access the stored shapes when carrying out step of and step of .

Computing system includes one or more processors one or more computer readable RAMs and one or more computer readable ROMs on one or more buses and one or more operating systems and one or more computer readable tangible storage devices . The one or more operating systems user interface user input interpretation program computer action setup program shape store touch screen API and sensor API are stored on one or more of computer readable tangible storage devices for execution by one or more of processors via one or more of RAMs which typically include cache memory . In the embodiment illustrated in each of the computer readable tangible storage devices is a magnetic disk storage device of an internal hard drive. Alternatively each of the computer readable tangible storage devices is a semiconductor storage device such as ROM EPROM flash memory or any other computer readable tangible storage device that can store a computer program and digital information.

Computing system also includes a R W drive or interface to read from and write to one or more portable computer readable tangible storage devices such as a CD ROM DVD memory stick magnetic tape magnetic disk optical disk or semiconductor storage device. User interface user input interpretation program computer action setup program shape store touch screen API and sensor API can be stored on one or more of portable computer readable tangible storage devices read via R W drive or interface and loaded into hard drive .

Computing system also includes a network adapter or interface such as a TCP IP adapter card. User interface user input interpretation program computer action setup program shape store touch screen API and sensor API can be downloaded to computing system from an external computer via a network and network adapter or interface . From the network adapter or interface user interface user input interpretation program computer action setup program shape store touch screen API and sensor API are loaded into hard drive . The network may comprise copper wires optical fibers wireless transmission routers firewalls switches gateway computers and or edge servers.

Computing system includes a touch screen and sensors . Sensors includes a gyroscope an accelerometer and a plurality of proximity sensors. Also included in computing system are device drivers to interface to touch screen and sensors . The device drivers R W drive or interface and network adapter or interface comprise hardware and software stored in storage device and or ROM .

User interface user input interpretation program computer action setup program shape store touch screen API and sensor API can be written in various programming languages such as Java C including low level high level object oriented or non object oriented languages. Alternatively the functions of user interface user input interpretation program computer action setup program shape store touch screen API and sensor API can be implemented in whole or in part by computer circuits and other hardware not shown .

Based on the foregoing a computer system a method and a program product have been disclosed for performing a computer action based on input from a user s touch on a touch screen. The description above has been presented for illustration purposes only. It is not intended to be an exhaustive description of the possible embodiments. One of ordinary skill in the art will understand that other combinations and embodiments are possible. Therefore the present invention has been disclosed by way of example and not limitation.

