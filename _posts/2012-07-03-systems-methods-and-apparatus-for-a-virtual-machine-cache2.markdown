---

title: Systems, methods and apparatus for a virtual machine cache
abstract: A virtual machine cache provides for maintaining a working set of the cache during a transfer between virtual machine hosts. In response to the transfer, a previous host retains cache data of the virtual machine, which is provided to the new host of the virtual machine. The cache data may be transferred via a network transfer.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09612966&OS=09612966&RS=09612966
owner: SanDisk Technologies LLC
number: 09612966
owner_city: Plano
owner_country: US
publication_date: 20120703
---
The embodiments described herein relate to the management of data input output I O operations in a computing environment and in particular to cache warming.

The embodiments described herein relate to the management of data input output I O operations in various computing environments including virtualized computing environments. However although particular examples and or embodiments disclosed herein relate to virtualized environments the disclosure is not limited in this regard and could be applied to non virtualized bare metal environments.

In some embodiments the described systems and methods intercept I O operations in the virtualized environment to dynamically allocate resources such as cache resources across multiple virtual machines in the virtualized environment. This management of data I O operations improves the performance of the virtual machines and reduces the number of I O operations handled by the primary storage system. Additionally the management of I O operations is transparent to other components in the virtualized environment and can be implemented without modification to existing application software or existing data storage systems. Thus operating systems that currently exist will be oblivious to the operations of the embodiments described herein which will cooperate with the basic operation characteristics of virtual operating systems and not disrupt them while better optimizing the operations of virtual machines resident in hosts.

Specific systems and methods described herein utilize a cache memory constructed with various memory devices such as flash memory devices or RAM random access memory that may or may not be block oriented. The systems and methods described herein do not differentiate between Flash memory RAM or other types of memory and further envision new types of memory developed in the future that will utilize various embodiments described herein. The described systems and methods may utilize any type of memory device regardless of the specific type of memory device shown in any figures or described herein. Particular systems and methods described herein may generally be referred to as an I O hypervisor due to its management of I O operations in a virtualized environment.

The systems and methods described herein relate to the management of data input output I O operations in a computing environment. Although particular examples discussed herein relate to virtualized environments the same systems and methods are applicable to any type of computing environment. In particular implementations the described systems and methods intercept I O operations in the virtualized environment to dynamically allocate resources such as cache resources across multiple virtual machines in the virtualized environment. This management of data I O operations improves the performance of the virtual machines and reduces the number of I O operations handled by the primary storage system. Additionally the management of I O operations is transparent to other components in the virtualized environment and can be implemented without modification to existing application software or existing data storage systems. Thus operating systems that currently exist will be oblivious to the operations of the embodiments described herein which will cooperate with the basic operation characteristics of virtual operating systems and not disrupt them while better optimizing the operations of virtual machines resident in hosts.

Specific systems and methods described herein utilize a cache memory constructed with various memory devices such as flash memory devices or RAM random access memory that may or may not be block oriented. The systems and methods described herein do not differentiate between Flash memory RAM or other types of memory and further envision new types of memory developed in the future that will utilize various embodiments described herein. The described systems and methods may utilize any type of memory device regardless of the specific type of memory device shown in any figures or described herein. Particular systems and methods described herein may generally be referred to as an I O hypervisor due to its management of I O operations in a virtualized environment.

Each virtual machine A N may be configured to implement a different operating system. The host may comprise one or more computing devices capable of hosting multiple virtual machines A N and supporting the applications executed by the virtual machines and the functions associated with those applications. The host may include for example one or more processors memory devices persistent storage devices communication devices I O interfaces and related components. Although three virtual machines A N are shown within the host the disclosure is not limited in this regard and the host may include any number of virtual machines.

The virtualization kernel may be configured to manage the operation of the virtual machines A N operating on the host as well as other components and services provided by the host . For example the virtualization kernel may be configured to handle various I O operations associated with a primary storage system or other storage devices. They primary storage system may be shared among the multiple virtual machines A N and may be shared by multiple hosts. The primary storage system may comprise multiple disk drives or other storage devices such as one or more storage arrays e.g. RAID JBOD or the like .

The host may further comprise a virtual machine cache which may be configured to provide caching services to the virtual machines A N deployed on the host computing device . The virtual machine cache may comprise a cache provisioner module and a cache . The cache may comprise a cache storage device which may include but is not limited to one or more memory devices such as solid state memory devices Random Access Memory RAM devices or the like. As used herein a solid state memory device refers to a non volatile persistent memory that can be repeatedly erased and reprogrammed. Accordingly a solid state memory device may comprise a solid state storage device and or solid state storage drive SSD e.g. a Flash storage device . The cache provisioner module may be configured to provision resources of the cache to the virtual machines A N which may comprise dynamically provisioning cache storage and or I O operations IOPS to the virtual machines A N. The cache provisioner module may be configured to provide for sharing resources of the cache between multiple virtual machines A N. The cache provisioner module may be further configured to protect and or secure data stored within the cache to prevent more than one virtual machine A N from accessing the same cache data. For example in some embodiments the cache provisioner module is configured to associate cached data with a virtual machine identifier via a VLUN mapping as described below in conjunction with which may be used to control access to the cache data. Additional details regarding the operation of cache provisioner module and cache are discussed herein.

The virtual machines A N may comprise an I O driver and a cache management system . As used herein the cache management system CMS may also be referred to as a Cache File System CFS . The I O driver may be configured to intercept I O operations of the associated virtual machine A N and to direct the I O operations to the virtual machine cache for processing.

In some embodiments and as depicted in an I O driver may be implemented within the virtual machines A N. Accordingly the I O driver may be in close proximity to the source of I O operations and data associated with the I O operations e.g. the I O driver does not have to access the virtualization kernel and or cross a virtual machine boundary to access the I O operations .

In some embodiments the I O driver may comprise and or be implemented as a device driver e.g. a device driver of respective guest operating systems of the virtual machines A N . The I O driver may comprise a generic component that forms part of an operating system and a device specific component. The I O driver may leverage I O Application Programming Interfaces APIs published by the operating system e.g. may be in the I O path of the virtual machines A N . Accordingly in some embodiments the I O driver may comprise a filter driver configured to operate above standard device drivers in an I O stack of a virtual machine A N.

In some embodiments the virtual machines A N may be configured to be transferred and or relocated from the host to other host computing devices. The virtualization kernel or other virtualization layer may be configured to prevent virtual machines that reference local resources of the host such as local disk storage or the like from being transferred. Accordingly virtual machines A N may be configured to access the virtual machine cache as if the virtual machine cache were a shared storage resource and or in a way that does not prevent the virtual machines A N from being transferred between hosts .

In some embodiments the cache management module is configured to access the virtual machine cache through a virtual disk which the virtualization kernel treats as a shared device and or a device that does not prevent virtual machines A N from being transferred between hosts . The virtual disk may be provided in a Virtual Machine Disk Format VMDK supported by the host and or virtualization kernel . The I O driver may comprise an I O filter that is configured to monitor I O operations of the virtual machine A N intercept I O operations directed to the virtual disk e.g. the VLUN disk described below and to forward the I O operations and other related data to the virtual machine cache . The I O filter may operate above a SCSI and or vSCSI level of the virtual machine A N storage stack. The I O filter may provide for passing I O requests and responses between the cache management system of the virtual machines A N and the cache management system . The I O filter may further provide for communicating other data such as configuration command and or control data e.g. performing the handshake protocol described above . The virtual disk may be represented as a Virtual Logical Unit Number VLUN disk implemented according to the VMDK format of the host and or virtualization kernel . The virtual disk may be very small e.g. a few megabytes since the virtual disk is not used for storage but as a conduit for communication between the virtual machine and the virtual machine cache in the virtualization kernel .

The virtual machines A N may be configured to emulate shared storage in other ways. For example in some embodiments the virtual machines A N may be configured to replicate one or more shared VLUN disks across a plurality of hosts such that to the hosts the VLUN disks appear to be shared devices. For instance the VLUN disks may share the same serial number or other identifier. The host and or the virtualization kernel may therefore treat the VLUN disks as shared devices and allow virtual machines A N to be transferred to from the host . The VDMK approach described above may provide advantages over this approach however since a smaller number of shared disks need to be created which may prevent exhaustion of limited storage references e.g. a virtual machine may be limited to referencing storage devices .

The cache provisioner module may be configured to manage the storage capacity of cache by for example allocating cache space among the virtual machines A N as discussed herein. The allocation information associated with a particular virtual machine e.g. virtual machine 1 A may be communicated to the corresponding virtual machine cache management system . Additional details regarding the operation of the I O driver and the cache provisioner module are provided below.

In some embodiments the cache management system is configured to request cache storage from the cache provisioner module . The request may be transmitted in response to a cache management system initialization operation e.g. cache warm up . As used herein cache initialization refers to one or more operations to prepare the cache management system for operation. The cache management system may be configured to perform one or more initialization operations in response to the virtual machine A N being powered on restarted transferred to a new host e.g. in a VMotion operation or the like.

A cache initialization operation may comprise a handshake protocol with the virtual machine cache which may comprise identifying the virtual machine A N assigning a virtual machine identifier VMID to the virtual machine A N allocating cache storage to the virtual machine A N and so on. The handshake protocol and or cache initialization may further comprise the cache provisioner module allocating cache storage capacity to the virtual machine A N as described above. The virtual machine cache may maintain list or other data structure that identifies the virtual machines A N deployed on the host . The virtual machines A N may be identified by VMID or other identifier s . The virtual machine cache may identify a transferred virtual machine A N based at least in part on the VMID of the virtual machine A N. For example the virtual machine cache may determine that the virtual machine A N is a new virtual machine on the host in response to the VMID not appearing in the list of deployed virtual machines and may determine that the virtual machine A N was transferred to the host based upon a host identifier of the VMID and or a separate host identifier maintained by the virtual machine A N .

Alternatively or in addition the virtual machine cache may be configured to actively identify a transferred virtual machines . For example the virtual machine cache may be configured to periodically query the virtual machines for a VMID and or current host id which as discussed above may be used to determine whether the virtual machine was transferred in from a remote host . In another example the cache management system of a virtual machine may be configured to periodically interrogate the virtual machine cache which may initiate a handshake protocol as discussed above before the virtual machine performs any I O requests.

In some embodiments the cache provisioner module is configured to maintain mappings between the virtual machines A N and respective cache storage locations allocated virtual machines A N e.g. as depicted in . The mappings may be used to secure cache data of the virtual machines A N e.g. by limiting access to the virtual machine A N mapped to the cached data and or to provide for retaining and or transferring cache data of one or more virtual machines A N transferred from the host to other remote hosts as described below.

The cache management system may be configured to maintain a set of cache tags in accordance with the cache storage that has been allocated to the virtual machine A N by the cache provisioner module . As used herein a cache tag refers to an association between an identifier and a cache resource e.g. a page or other cache storage location in the cache . Accordingly the cache tags may represent cache resources that have been allocated to a particular virtual machine A N by the cache provisioner module . As used herein an identifier of a cache tag refers to an identifier used by the virtual machine A N to reference data that has been or will be stored in the cache . A cache tag identifier may include but is not limited to an address e.g. a memory address physical storage address logical block address etc. such as an address on the primary storage system a name e.g. file name directory name volume name etc. a logical identifier a reference or the like.

The cache tags may be stored within the respective virtual machine A N e.g. in volatile memory allocated to the virtual machine A N by the host . In some embodiments the cache tags may represent a working set of cache data of the virtual machine A N. As used herein a working set of cache tags refers to a set of cache that that has been admitted and or retained in the cache by the management system through inter alia the application of one or more cache policies such as cache admission policies cache retention and or eviction policies e.g. cache aging metadata cache steal metadata least recently used LRU hotness and or coldness and so on cache profiling information file and or application level knowledge and the like. Accordingly the working set of cache tags may represent the set of cache data that provides optimal I O performance for the virtual machine A N under certain operating conditions.

In some embodiments the cache management system may be configured to preserve and or maintain the cache tags which may comprise persisting the cache tags in a non volatile storage medium such as the primary storage system persistent cache storage device e.g. cache or the like. As used herein a snapshot refers to the working set of the cache at a particular time. A snapshot may comprise all or a subset of the cache tags and or related cache metadata . In some embodiments a snapshot may further comprise pinning data in the cache device which may cause data referenced by the one or more cache tags to be retained in the cache . Alternatively the snapshot may reference only the data identifiers and may allow the underlying data to be removed e.g. evicted from the cache . The cache management system may be configured to load a snapshot from persistent storage and to use the snapshot to populate the cache tags . A snapshot may be loaded as part of an initialization operation e.g. cache warm up and or in response to configuration and or user preference. For example the cache management service may be configured to load different snapshots that are optimized for particular application s and or service s . Loading a snapshot may further comprise requesting cache storage from the cache provisioner module as described above. In some embodiments the cache management service may load a subset of a snapshot if the virtual machine A N cannot allocate sufficient cache space for the full snapshot.

The cache management system may be further configured to retain the cache tags in response to relocating and or transferring the virtual machine A N to another virtual machine host e.g. in a VMotion operation as described below . Retaining the cache tags may comprise maintaining the cache tags in the memory of the virtual machine A N and or not invalidating the cache tags . Retaining the cache tags may further comprise requesting cache storage from the cache provisioner module of the new host in accordance with the retained cache tags and or selectively adding and or removing cache tags in response to being allocated more or less cache storage on the new host . In some embodiments the cache management system may retain the cache tags despite the fact that the cache data referenced by the cache tags does not exist in the cache of the new host. As described below the virtual machine cache may be configured to populate the cache with cache data from a previous host of the virtual machine A N e.g. via a network transfer and or from a shared primary storage system.

In some embodiments one or more of the virtual machines A N may comprise a virtual desktop such as a desktop environment associated with a particular user. One or more users may accesses the desktop environment via a terminal or other system or device. This type of virtual desktop environment is commonly referred to as Virtual Desktop Infrastructure VDI . Thus a single host can replace many individual desktop computing systems. Alternatively or in addition one or more of the virtual machines A N may provide one or more server side applications. Accordingly a single host can replace any number of individual software or application servers each operating in a separate virtual computing environment.

The system may further comprise a primary storage system which may be shared among the hosts A N and or the virtual machines . The primary storage system may comprise any suitable persistent storage device and or storage system including but not limited to one or more magnetic disks e.g. hard drives a redundant array of inexpensive disks RAID storage area network SAN or the like. The hosts A N may be configured to access the primary storage system via the network .

In some embodiments each virtual machine may be assigned a respective VMID. The VMID may be assigned when the virtual machine is instantiated e.g. loaded on a host A N e.g. during a handshake protocol described above . The VMID may comprise a process identifier thread identifier or any other suitable identifier. In some embodiments the VMID may uniquely identify the virtual machine on a particular host A N and or within a within a group of hosts A N. For example the hosts A N may operate within the same namespace such as a cluster and the VMID of each virtual machine may be unique within the namespace of the cluster unique across the virtual machines deployed on hosts A N in the cluster . In some embodiments the VMID may comprise a host identifier such as a Media Access Control MAC address network address distinguished name or the like. Accordingly in some embodiments a VMID may uniquely identify a virtual machine in a particular namespace and may identify the host A N upon which the virtual machine is current deployed or was previously deployed . Alternatively or in addition each virtual machine may be configured to maintain a current host identifier and a previous host identifier.

In some embodiments one or more of the virtual machines may be capable of being relocated and or transferred between the hosts A N. For example a virtual machine X may be transferred from the host A to the host B e.g. in a VMotion or similar operation . The virtual machine cache B may be configured to identify the transfer in response to receiving a request from the cache management system of the transferred virtual machine X via the I O driver as described above . The request may comprise the VMID of the transferred virtual machine X from which the virtual machine cache B may determine that the virtual machine X is new to the host B e.g. requests comprising the VMID have not been received before . In response the virtual machine cache B may initiate a handshake protocol with the virtual machine X as described above. The virtual machine cache B may determine that the virtual machine X was transferred to the host B based at least in part on a host identifier of the VMID and or host identifier s maintained by the virtual machine X. The host identifier of the virtual machine X may reference the host A whereas the host identifier of a newly powered on virtual machine may reference the host B or be blank . Alternatively or in addition the virtual machine X may comprise a separate host identifier which may reference host A and may be accessed in the handshake protocol with the virtual machine cache B.

The cache provisioner module may be configured to allocate storage for the virtual machine X in the cache B as described below. The cache provisioner module may be configured to determine how much cache storage to provision based at least in part upon the size of the cache storage allocated to the virtual machine X on the previous host host A . As discussed above the cache management system of the virtual machine X may be configured to retain the working set of the cache e.g. retain the cache tags after the transfer to host B. The cache provisioner module B may attempt to allocate sufficient cache storage in the cache B to support the retained cache tags . If sufficient cache storage cannot be allocated the cache management system may be configured to selectively remove the retained cache tags in accordance with the new cache storage allocation on host B. Alternatively if excess cache storage is available the cache management system may be configured to add new tags to the retained cache tags .

The virtual machine cache A may comprise a retention module A which may be configured to retain cache data of the virtual machine X after the virtual machine X is transferred from the host A. The cache data may be retained for a retention period and or until the virtual machine cache A determines that the retained cache data is no longer needed. The retention module A may determine whether to retain the cache data and or determine the cache data retention period based upon various retention policy considerations including but not limited to availability of cache A availability of cache B relative importance of the retained cache data as compared to cache requirements of other virtual machines whether the cache data is backed up in the primary storage system and so on.

The cache management system of the virtual machine X may be configured to retain the working state of the cache the cache tags despite the fact that the cache B does not comprise the cache data to which the cache tags refer. As discussed below the virtual machine cache B may be configured to populate the cache B with cache data transferred from the cache A of host A and or the primary storage system to reconstruct the working set of the transferred virtual machine X.

The virtual machine cache B may comprise a cache transfer module B which may be configured to access cache data of the virtual machine X stored at the previous host A. The cache transfer module B may be configured to identify the previous host A by use of the VMID and or by interrogating the virtual machine X e.g. accessing a previous host identifier maintained by the virtual machine X . The cache transfer module B may use the host identifier to issue one or more requests for the cache data to the virtual machine cache of the host A via the network . In some embodiments the cache transfer module B is configured to determine and or derive a network address or network identifier of the host A from the host identifier.

The virtual machine cache A may comprise a cache transfer module A that is configured to selectively provide access to retained cache data of the transferred virtual machine X. In some embodiments the cache transfer module A is configured to secure the retained cache data. For example the cache transfer module A may be configured to verify that the requesting entity e.g. the virtual machine cache B is authorized to access the cache data of the transferred virtual machine X which may comprise verifying that the virtual machine X is deployed on the host B. For example the cache transfer module A may request a credential associated with the transferred virtual machine X such as the VMID or the like. Alternatively or in addition the cache transfer module A may implement a cryptographic verification which may comprise verifying a signature generated by the transferred virtual machine X or the like.

The cache transfer module B may be configured to transfer the cache data by one or more demand paging transfers prefetch transfers and or bulk transfers. A demand paging transfer may comprise transferring cache data in response to requests for the cache data from the virtual machine X e.g. on demand . The transferred data may be used to service the I O request. In addition the transferred data may be admitted into the cache B of the new host B. Alternatively the transferred data may be admitted at a later time not not at all in accordance with a cache policy. A prefetch transfer may comprise transferring data according to a prefetch cache policy e.g. by proximity or the like . The amount and or extent of cache data to prefetch may be determined by inter alia cache metadata of the virtual machine cache management system e.g. cache aging metadata hotness and so on . Accordingly in some embodiments the cache transfer module B may be configured to query the cache management system to identify the cache data to prefetch if any . A bulk transfer may comprise transferring cache data in bulk independent of storage requests from the virtual machine X. A bulk transfer may comprise transferring populating the entire cache storage allocated to the virtual machine X. Alternatively a bulk transfer may comprise populating a subset of the cache which as discussed above may be selected based upon cache metadata of the virtual machine cache management system .

The cache storage module B may be configured to store cache data transferred from the cache A of the host A or acquired from other sources such as the primary storage system in the cache B. The cache storage module B may be configured to store the cache data at cache storage locations that have been allocated to the transferred virtual machine X by the cache provisioning module B. The cache data may be stored at the same cache storage location e.g. same offset with the cache storage as in the original cache A such that the references in the retained cache tags remain valid. Additional details regarding various embodiments of transferable cache tag mappings are provided below in conjunction with .

The cache transfer module A is configured to identify the requested cache data in the cache A using inter alia the VMID of the transferred virtual machine X. The cache transfer module A may then transfer the requested cache data if available to the cache transfer module B via the network .

The cache transfer module B may be configured to access cache data from either the previous host A the primary storage system and or other sources of the data e.g. other persistent storage systems hosts N or the like . The cache transfer module B may select the source of the cache data based upon various policy considerations e.g. a cache transfer policy which may include a network policy a bandwidth policy a host resource policy a primary storage resource policy and the like. For example in response to determining that the network is highly congested the cache transfer module B may be configured to reduce the amount data to transfer defer a bulk transfer and or transfer the cache data from another source that is independent of the network . Similarly the cache transfer module B may direct requests to the host as opposed to the primary storage system in response to determining that the primary storage system is heavily loaded and or has limited available bandwidth.

The cache data retained on the host A may represent cache resources that cannot be used by the other virtual machines operating on the host A. As such the cache retention module A may be configured to selectively remove the retained cache data when the data is no longer needed and or according to a retention policy. The retention policy may be determined based upon the retention policy factors described above. In some embodiments the cache transfer module B is configured to inform the previous host A of cache data that has been transferred to the host B from other sources so that the retention module A can remove the corresponding data from the cache A. The cache transfer module B may be further configured to inform the host A of other conditions in which the cache data no longer needs to be retained such as when the cache data is overwritten deleted e.g. trimmed evicted from the cache B or the like.

In some embodiments the cache transfer module A may be configured to push cache data of the virtual machine X to the new host B. Pushing cache data may comprise transferring retained cache data of the virtual machine X to the cache transfer module B without receiving a request for the cache data independent of requests for the cache data . The cache transfer module A may determine the host identifier of the new host B through user configuration the verification process described above through active polling by the cache transfer module A through a call back implemented by the transferred virtual machine X or the like. In some embodiments the virtual machine cache of the new host B may identify that the virtual machine X was transferred from the host A in response to receiving cache data pushed from the host A as described above.

As discussed above a virtual machine X may be transferred between hosts A N e.g. from host A to host B as described above . Alternatively or in addition the virtual machine X may be transferred to more than one host B N in an 1 N transfer. As shown in the virtual machine X may be transferred from host A to both hosts B and N. As such each host B and N may comprise a respective instantiation of the virtual machine X labeled as Y and Z respectively . In response the transfer the virtual machine cache B and N may identify the transferred virtual machines Y and Z provision cache storage for the transferred virtual machines Y and Z and may populate the cache storage allocated thereto the cache B and N as described above. The virtual machine cache N may comprise a cache transfer module N and a cache storage module N that are configured to transfer cache data from the host A and or primary storage system and store the transferred the transferred data in the cache N. The retention module A may be configured to retain the cache data of the virtual machine A until the data is transferred to both caches B and N. Alternatively the retention module A may be configured to retain the cache data until the cache data is transferred to either one of the caches B or N. Subsequent requests for the cache data may result in a miss may be directed to primary storage or one of the caches B and or N.

In another example the transfer may comprise retaining the virtual machine X on the host A e.g. creating clones of the virtual machine X on the other hosts B and N . In a cloning example the retention module A may be configured to retain a copy of the cache data of the virtual machine X as of the time the virtual machine X was transferred to the other hosts B and N from which the cache transfer modules B and N may transfer cache data. Alternatively the retention module A may not copy the cache data of virtual machine X and the cache transfer module A may provide access to the working cache data as described above. In some embodiments the cache of virtual machine X may be locked e.g. treated as static and or read only until the cache data has been transferred to the other hosts B and N and or the cache transfer modules B and or N have obtained the cache data from one or more other sources.

The address space translator may be configured to correlate logical identifiers e.g. addresses in a primary storage system with cache storage locations e.g. cache addresses cache pages etc. .

The cache tag manager may be configured to manage the cache tags allocated with the cache management system as described herein which may comprise maintaining associations between virtual machine identifiers e.g. logical identifiers address etc. and data in the cache .

The clock sweep module may be configured to determine and or maintain cache aging metadata using inter alia one or more clock hand sweep timers as discussed herein. The steal candidate module may be configured to identify cache data and or cache tags that are candidates for eviction based upon inter alia clock sweep metadata or other cache policy.

The cache page management module may be configured to manage cache resources e.g. cache page data and related operations. The valid unit map module may be configured to identify valid data stored in a cache and or a primary storage system. The page size management module may be configured to perform various page size analysis and adjustment operations to enhance cache performance as described herein. The interface module may be configured to provide one or more interfaces to allow other components devices and or systems to interact with the cache management system .

The cache tag retention module may be configured to retain cache tags in response to transferring the cache management system to a different host. As described above the cache tags may represent a working set of the cache which may be developed through the use of one or more cache admission and or eviction policies e.g. the clock sweep module and or steal candidate module and in response to the I O characteristics of the virtual machine and or the applications running on the virtual machine . The cache tag retention module may be configured to retain the cache tags after the virtual machine is transferred to a new host e.g. transferred from host A to host B in inter alia a VMotion operation despite the fact that the underlying cache data to which the cache tags refer may not be available on the cache storage device of the new host. The virtual machine cache described herein however may be configured to populate the cache at the new host such that the cache management system can continue to use the working set of cache tags .

As described above data of the retained cache tags may be transferred to the new host from the previous host and or from the primary storage system or other source . The cache data may be transferred via a demand paging model which may comprise populating the cache on demand as the cache data of various retained cache tags is requested by the virtual machine . Alternatively or in addition cache data may be prefetched and or transferred in a bulk transfer operation which may comprise transferring cache data independent of requests for the cache tag data. In some embodiments data may be selectively prefetched based upon a cache transfer policy which may be based at least in part on the cache aging metadata of the clock sweep module and or steal candidate module and or other cache policy metadata e.g. hotness coldness least recently used etc. .

The cache tag snapshot module may be configured to maintain one or more snapshots of the working set of the cache e.g. the cache tags . As described above a snapshot refers to a set of cache tags at a particular time. The snapshot module may be configured to store a snapshot of the cache tags on a persistent storage medium and or load a stored snapshot as described above.

At the cache management system may determine whether the requested data is available in the cache . Step may comprise determining whether the cache management system comprises a cache tag corresponding to the read request e.g. whether the cache management system comprises a cache tag having an identifier corresponding to an identifier of the read request . If the data is determined to be in the cache block the procedure branches to block where the request is forwarded to the cache management system and the requested data is retrieved from the cache . If the data is not available in the cache the procedure branches to block where the requested data is retrieved from a primary storage system discussed above. After retrieving the requested data the procedure determines whether to write the retrieved data to the cache block to improve the storage I O performance of the virtual machine . This determination is based on various cache policies and other factors.

The cache management system may develop and or maintain a working set for the cache using inter alia a file system model. As described above the working set of the cache may be embodied as the set of cache tags maintained by the cache management system . The cache may comprise one or more solid state storage devices which may provide fast read operations but relatively slow write and or erase operations. These slow write operations can result in significant delay when initially developing the working set for the cache. Additionally the solid state storage devices comprising the cache may have a limited lifetime a limited number of write erase cycles . After reaching the write lifetime of a solid state storage device portions of the device become unusable. These characteristics may be taken into consideration by the cache management system in managing the cache .

The cache may be shared between a plurality of virtual machines on a host. A cache chunk may be assigned or allocated to a particular one of the virtual machines based upon inter alia the cache needs of the virtual machine and or the cache needs of other virtual machines. The number of chunks assigned to a particular virtual machine can change over time as the cache needs of the virtual machine s change. The number of chunks assigned to a specific virtual machine may determine the cache capacity of that virtual machine. For example if two 256 MB chunks are assigned to a specific virtual machine that virtual machine s cache capacity is 512 MB. The assignment of chunks to particular virtual machines is handled by the cache provisioner such as the cache provisioner described above.

Cache tags are used in mapping storage I O addresses in a virtual machine to cache pages e.g. physical addresses in the cache . The cache tags can cache data associated with any storage device assigned to a virtual machine. The cache tags may therefore be used to perform translations between identifiers in the cache tags e.g. address of blocks on a primary storage system and a cache address. In some embodiments cache tags may be organized linearly in RAM or other memory. This allows the address of the cache tag to be used to locate a physical cache page because of the algorithmic assumption that each cache tag has a linear 1 1 correspondence with a physical cache page . Alternatively or in addition cache tags may be organized into another data structure such as a hashtable tree or the like.

As shown in cache tags associated with a particular virtual machine A N are stored within that virtual machine A N. The cache tags contain metadata that associates storage I O addresses to specific cache pages in the cache. In a particular embodiment each cache tag is associated with a particular page in the cache.

Referring back to in some embodiments a thin provisioning approach is used when allocating cache chunks to the virtual machines . In this embodiment each virtual machine is allocated a particular number of cache chunks . However the entire cache capacity is published to each of the virtual machines . For example if the total cache size is 1 TB each virtual machine reports that it has access to the entire 1 TB of storage space . However the actual allocation of cache chunks may be considerably smaller e.g. 256 MB or 512 MB based on the current needs of the virtual machine . The allocated cache chunks represent a specific range of cache addresses available within the cache . The cache provisioner module dynamically changes these cache chunk allocations as each virtual machine s working set requirements change and or virtual machines are transferred to from the host . Regardless of the number of cache chunks actually allocated to a particular virtual machine that virtual machine reports that it has access to the entire 1 TB cache. Accordingly the guest OS of the virtual machine may operate with a virtual disk of size 1 TB. By using a thin provisioning approach the actual storage space allocated to the virtual machine can be changed dynamically without the guest operating system indicating an error condition.

If the decision is to write the retrieved data to the cache the cache management system uses the memory address of the cache tag to determine a physical cache address associated with the data to be written. The data is then written to the cache using the physical cache address associated with the data.

If the requested data is in the cache block the cache management system uses the memory address of the cache tag to determine a physical cache address associated with the requested data block . The requested data is then retrieved from the cache using the physical cache address associated with the requested data block .

Storing the cache tags and other cache metadata within the associated virtual machine allows the virtual machine to easily determine where the data is stored physically in the cache without having to access a different system or process. Instead the systems and methods described herein allow each virtual machine to quickly access cache tags which increases the speed and efficiency of the I O operations. Additionally the virtual machine typically understands the data it is processing better than other systems. For example the virtual machine understands the nature and context of the data it is processing. This understanding of the data enhances the development and management of an effective working set of cache tags . Other systems that are external to the virtual machine may simply see the data as raw data without any context or other understanding. Thus having the cache tags stored locally in the virtual machine enhances the operation of the virtual machine and the I O operations.

Next the virtual machine writes the data associated with the data write operation to the cache using the physical cache address block . The virtual machine also simultaneously writes the data associated with the data write operation to the primary storage system block in a write through operation. The original data write operation is completed when the primary storage system acknowledges a completed write operation block .

In a particular implementation the cache discussed herein is a write through cache. This type of cache writes data to both primary storage and the cache . A write completion is acknowledged after the write operation to the primary storage system is completed regardless of whether a corresponding write operation to the cache has completed. In specific embodiments cache write operations can be queued and completed as the cache speed allows. Thus a cache with a slow write speed or a queue of pending write operations does not degrade performance of the overall system. Cache tags associated with incomplete or queued write operations are identified as pending. After the write operation completes the associated cache tag is identified as valid . When the cache tag is identified as pending any attempted read of the data associated with the cache tag results in a cache miss causing retrieval of the requested data from the pending memory buffer associated with the I O or from the primary storage system.

As mentioned above each cache tag stored in a virtual machine is associated with a particular cache page . Additionally the systems and methods described herein are capable of dynamically allocating cache resources e.g. cache chunks to the virtual machines in a virtualized environment. Therefore the number of cache tags associated with a particular virtual machine can be increased beyond the number of cache pages actually associated with the virtual machine . In certain embodiments a user changes the configuration of the cache management system and cache allocations by increasing the number of cache tags allocated such that a determination can be made whether a given number of cache tags will provide for efficient use of the cache by a particular virtual machine . This increase in cache tags allows the cache management system to determine whether increasing the number of cache pages allocated to the particular virtual machine will likely improve the cache hit rate for that virtual machine .

The procedure increases the number of cache tags of the virtual machine without increasing the cache size block . For example the procedure may increase the number of cache tags by an amount that corresponds to assigning an additional cache chunk to the virtual machine . However the additional cache chunk is not actually assigned to the virtual machine at this point in the evaluation procedure. Next the procedure monitors the cache hit rate using the increased number of cache tags block . After monitoring the cache hit rate with the increased number of cache tags for a period of time the procedure determines whether the cache hit rate has improved block . If the cache hit rate has improved as a result of the additional cache tags the procedure returns to block to further increase the number of cache tags associated with the virtual machine .

The process of increasing the number of cache tags and monitoring the results continues until the increase in cache tags does not improve the cache hit rate. At this point procedure determines the minimum number of cache tags that provide improved cache performance block . In an alternate embodiment the procedure determines an optimal number of cache tags that provide optimal cache performance. The procedure then adjusts the cache size allocated to the virtual machine based on the number of cache tags that provide improved cache hit rate performance block . Dynamic addition of cache chunks or capacity to a virtual machine is based on both the hit rate and other policy that handles cache resource provisioning to other virtual machines . The hit rate IOPS improvements and cache capacity are also adjusted using policy that can be controlled by the user or implemented algorithmically based on rules specified by the user.

In a particular embodiment the number of cache tags added at block is substantially the same as the number of the cache pages in a particular cache chunk . Thus allocating additional cache resources to the virtual machine is performed by allocating a number of cache chunks that corresponds to the minimum number of cache tags that provide improved cache performance.

Each clock hand has a different time interval. In the example of one clock hand has a time interval of ten minutes and the other clock hand has an interval of one hour. The time interval associated with each clock hand indicates the frequency with which the clock hand sweeps the clock hand data bits. For example a clock hand with a time interval of ten minutes clears one of the two clock hand data bits every ten minutes. Each time a cache page is accessed a cache hit all clock hand bits associated with the cache page are reset to a value of 1 .

As shown in all clock hand bits are initially set to 1 e.g. at time 00 00 . After the first ten minute clock sweep Bit of clock hand is cleared to 0 . The clock hand bits associated with the one hour clock hand are unchanged because the one hour clock sweep has not yet occurred. In this example the ten minute clock sweep occurs at time 00 08 which is less than ten minutes. This occurs because the initial time 00 00 is not necessarily aligned with a clock sweep time.

After a second ten minute clock sweep without any access of the cache page the Bit of clock hand is cleared leaving a clock hand value of 00 . At this time the cache page associated with this example is identified as a steal candidate i.e. the cache page is a candidate for removal from the cache due to a lack of access of the cache page . A separate table or other listing is maintained for cache pages in which both clock hands have been cleared. Cache pages with both clock hands cleared are top candidates for steal prior to cache pages with only one clock hand cleared.

As shown in if a cache page data access occurs at time 00 22 all clock hand bits are set to 1 . At time 00 31 the one hour clock hand sweeps causing the clearing of Bit of clock hand . That bit is set along with setting all other clock hand bits at time 01 04 due to a cache page data access. Although the particular example of uses two clock hands with ten minute and one hour intervals alternate embodiments may use any number of clock hands each having any time interval.

Finally the cache tag data structure may include a valid unit map field which is a dynamic field that identifies which units in a page are cached. An example of a unit within a cache page is a sector. For example a particular cache page may have one or more sectors that are missing or no longer valid. The valid unit map identifies the status of all units associated with a particular cache page to prevent accessing data in units that is not valid.

The cache may support multiple page sizes. Different applications executing in the virtual environment may require different page sizes to function properly. For example some applications always perform 32K data I O operations. For these applications it is desirable to use a large cache page size such as 16K or 32K to minimize the number of data I O operations necessary to handle the 32K of data. For example if the cache page size is 4K and the application performs a 32K data I O operation eight cache pages must be accessed to read or write the 32K of data. Performing eight separate I O operations to accommodate the 32K of data is a burden on system resources and dramatically increases the number of I O operations that must be processed by the system. In contrast if the cache page size is 16K only two I O operations are required to process the 32K of data. Thus the larger cache page size reduces I O operations and the corresponding burden on system resources.

Using larger cache page sizes also reduces the number of cache tags thereby reducing the memory space required to store the cache tags. For example in a one terabyte cache having 4K cache pages 256M cache tags are necessary to provide a single cache tag for each cache page. In the same system using 16K cache pages 64M cache tags are needed. Thus the larger cache page size reduces the number of cache tags and the memory resources needed to store the cache tags.

Although larger cache page sizes can reduce I O operations and reduce the number of cache tags in certain situations a larger cache page size can result in underutilized cache resources. For example if a system is using a 32K cache page size and an application performs a 4K I O operation only a small fraction of the 32K page is used 28K of the page is not needed . This situation results in significant unused cache resources. Therefore the systems and methods described herein support multiple cache page sizes to improve utilization of system resources such as I O resources and cache storage resources.

Different applications have different data storage characteristics. Applications can be characterized as having sparse address spaces or dense address spaces . Sparse address spaces tend to have scattered data with significant gaps between different groupings of data. In contrast dense address spaces tend to have data that is more compact with fewer or smaller gaps between different groupings of data. When selecting cache page sizes for a particular virtual environment it is important to consider the data storage characteristics e.g. sparse or dense address spaces associated with applications executing in the virtual environment. There can be exceptions where a sparse address space may comprise groups of contiguous data where the groups are sparsely located. In such cases one can use large pages even though the address space is sparse.

In a particular embodiment data associated with existing applications can be analyzed prior to implementing a system or method of the type described herein. This prior analysis allows the system to be tuned based on typical application data. After the systems and methods are implemented the dynamic nature of the system allows for adjustments to cache page sizes cache allocations system resources and other parameters based on changes in the operation of the application.

In a particular implementation a cache is divided into multiple sections such that each section supports different cache page sizes. Because application I O workloads can vary a particular cache page size for one application may be more efficient than for another application. One objective in using different cache page sizes is to minimize the number of I O requests that cross over a cache page boundary in order to make the I O operations as efficient as possible. For example a cache may be divided into four sections two of which support 4K cache pages one that supports 16K cache pages and one that supports 32K cache pages. The cache pages in these different sections are allocated to different virtual machines and or applications based for example on the data storage characteristics of the applications.

In one embodiment a different hash table is used for each different cache page size. Each hash table has its own associated hash function that identifies a particular hash slot in the table based on an address provided to the hash function. When using multiple hash tables such as a 4K hash table and a 16K hash table the systems and methods perform a lookup operation for each hash table. Performing a lookup in both hash tables is necessary because a 4K address could be contained within a 16K entry in the 16K hash table. To enhance the lookup process the systems and methods described herein apply one or more algorithms based on a percentage of cache hits associated with different cache page sizes a success rate associated with different hash tables and other factors to weight the lookup between the different hash tables and thereby improve the lookup efficiency.

In a particular implementation an algorithm uses both the percentage of cache hits associated with cache page sizes and the success rate associated with different hash tables to search for data in a cache.

In other embodiments the systems and methods use a single hash table associated with the smallest cache page size such as 4K and still presents the feature of a virtual machine using multiple different page sizes. Although the cache supports multiple cache page sizes the hash table uses a 4K page size exclusively. This approach eliminates the need to perform a lookup in multiple hash tables associated with different cache page sizes. In this scheme a 16K page I O would require four hash table lookups in the single has table and groups of cache tags are managed as one.

In certain situations it is desirable to prevent one or more cache pages from being stolen or usurped by another virtual machine. This is accomplished in the systems and methods discussed herein by pinning the cache tags associated with the cache pages that are to be protected from being stolen. Cache tags are pinned by setting the state bit to pinned state in the cache tag .

Pinning cache tags may be used in a variety of situations. For example a user can pin specific data within the cache to prevent the data from being replace modified evicted or the like. The user may know that the specified data is critical to the operation of the virtual machine and wants to ensure that the data is always available in the cache .

Pinning cache tags may also be used to lock a range of addresses in the cache . In certain situations a portion of data associated with a read operation is available in the cache but a portion is not available or not valid in the cache . This condition is referred to as a partial cache hit or a partial cache miss. In these situations the system must decide whether to retrieve all of the data from the primary storage system or retrieve a portion from the cache and the remainder from the primary storage system . The decisions involving what s available in the cache can result in more than one I O to primary or shared storage which may be more efficient when doing sequential I Os .

As discussed above the cache management system may be configured to snapshot a group of cache tags which may comprise storing the cache tags on a persistent storage device. Later when the virtual machine warms up e.g. reboots power cycles etc. the stored cache tags are retrieved from the persistent storage device the actual cache data is read back from the primary or shared storage thereby recreating the working set. This allows the virtual machine to resume operation immediately with a fully functioning working set rather than taking a significant period of time recreating the working set of cache tags . Similarly the working set of cache tags may be retained when a virtual machine is transferred to a different host .

In certain embodiments the cache management system is configured to manage a partial cache miss as efficiently as possible to minimize the number of I O requests forwarded on to the primary storage. In addition to managing partial cache miss I O requests the cache management system mitigates the amount of fragmentation of I Os to primary storage based on I O characteristics of the I O requests. Fragmentation of I Os also known as I O splitting refers to an I O request that crosses a cache page boundary or is divided between data that resides in the cache and data that resides on the primary storage. The I O characteristics may include whether the I O is contiguous the size of the I O request the relationship of the I O request size to the cache page size and the like. In affectively managing partial cache hits and fragmentation of I O requests the cache management system may coalesce I O requests for non contiguous address ranges and or generate additional I O requests to either the cache or the primary storage.

In a particular embodiment a checksum is calculated for each cache page. When calculating the checksum the system only performs the calculation on the valid data based on a valid unit map e.g. the valid data sectors . When a write operation is performed that increases the number of valid data sectors the checksum is recalculated to include the new valid data sectors.

Computing device includes one or more processor s one or more memory device s one or more interface s one or more mass storage device s one or more Input Output I O device s and a display device all of which are coupled to a bus . Processor s include one or more processors or controllers that execute instructions stored in memory device s and or mass storage device s . Processor s may also include various types of computer readable media such as cache memory.

Memory device s include various computer readable media such as volatile memory e.g. random access memory RAM and or nonvolatile memory e.g. read only memory ROM . Memory device s may also include rewritable ROM such as Flash memory.

Mass storage device s include various computer readable media such as magnetic tapes magnetic disks optical disks solid state memory e.g. Flash memory and so forth. As shown in a particular mass storage device is a hard disk drive . Various drives may also be included in mass storage device s to enable reading from and or writing to the various computer readable media. Mass storage device s include removable media and or non removable media.

I O device s include various devices that allow data and or other information to be input to or retrieved from computing device . Example I O device s include cursor control devices keyboards keypads microphones monitors or other display devices speakers printers network interface cards modems lenses CCDs or other image capture devices and the like.

Display device includes any type of device capable of displaying information to one or more users of computing device . Examples of display device include a monitor display terminal video projection device and the like. Interface s include various interfaces that allow computing device to interact with other systems devices or computing environments. Example interface s include any number of different network interfaces such as interfaces to local area networks LANs wide area networks WANs wireless networks and the Internet. Other interfaces include a user interface and a peripheral device interface .

Bus allows processor s memory device s interface s mass storage device s and I O device s to communicate with one another as well as other devices or components coupled to bus . Bus represents one or more of several types of bus structures such as a system bus PCI bus IEEE 1394 bus USB bus and so forth.

For purposes of illustration programs and other executable program components are shown herein as discrete blocks although it is understood that such programs and components may reside at various times in different storage components of computing device and are executed by processor s . Alternatively the systems and procedures described herein can be implemented in hardware or a combination of hardware software and or firmware. For example one or more application specific integrated circuits ASICs can be programmed to carry out one or more of the systems and procedures described herein.

In another embodiment an issue of compatibility that occurs within virtual systems is addressed. In certain virtual systems some of the processes make certain assumptions about the environment in order to properly operate.

In a single host there typically will be multiple virtual machines operating in the host. Each virtual machine will have its own separate I O drivers and also separate cache management module to manage local storage operations from the perspective of each particular virtual machine. Each virtual machine needs to share the local storage cache and each virtual machine will have its own unique demand for space on the local storage cache during its operation. Multiple virtual disks may be created on the local cache storage and these can be exposed to the local virtual machines. During operation of the various virtual machines the demand can vary among the different virtual machines. As a result capacity in the local cache may not be efficiently utilized by the virtual machines and cache capacity may be wasted.

In one example a thin provisioned storage is provided such as a thin provisioned cache for dynamic allocation of storage space among multiple virtual machines within a host. Since virtual machines are dynamic in nature their demand for storage space may vary. If they share actual storage space with other virtual machines the use of the storage space by a group of virtual machines may conflict. For example if one or more virtual machines experience a higher than normal I O traffic rate their operations may become bogged down causing lags in output. Other machines may experience a lower than normal I O traffic rate at the same time leaving their allocated storage space unutilized. Thus in some cases the higher I O virtual machines use of actual storage space may be unnecessarily restricted by rigid or inefficient allocation schemes. Virtual machines may be transferred from one host to another may become inactive or offline for some period of time may power down or rest on a host that needs to power down or its demand for storage space may change up or down during operation. Thus it would be useful if the storage space allocated to the group of virtual machines could be dynamically allocated and balanced where actual storage space allocated to any one machine can be apportioned more intelligently. As such dynamic allocation of storage space could serve to reduce lag time for virtual machines that demand more space and I O transfers by provisioning more space when other virtual machines associated with the same storage demand less space. The embodiment provides such solutions in an elegant manner.

In typical virtual machine environments shared storage is utilized among multiple hosts that have equal access to the common storage space. The shared storage may be a clustered file system a virtual machine file system VMFS where the system provides correctness and consistency among the various virtual machine hosts using file based locking and other methods.

Referring back to one or more virtual machines may be configured to transfer between hosts A N. For example VMWare provides the VMotion product that enables virtual machines to transfer operations from one host A N to another where storage of the transferred virtual machine e.g. virtual machine X is maintained on storage that is shared between the source and destination hosts e.g. hosts A and B . The transferred virtual machine X may be a live operating virtual machine X located on one host A and the desire is to be able to move the virtual machine X to another host B without interruption. This is possible because the multiple hosts A and B see and share the common data storage system e.g. shared primary storage system . Thus the virtual machine may move from one host to another without shutting down or rebooting the virtual machine X the move is transparent to the moving virtual machine X.

When a virtual machine boots up and begins to run the virtual machine is configured to identify available resources such as storage devices network devices etc. which may comprise sending out Small Computer System Interface SCSI inquiries to connected storage devices to determine what resources are available. The storage available to the virtual machine is virtual storage that is encapsulated in a file. The encapsulated file is the main storage space for the virtual machine. Thus the storage for the virtual machine is now instantiated in a file and becomes a virtual hard drive. In prior art devices this file is stored in the common data storage system shared among multiple hosts.

According to one embodiment it is desired to store the virtual disk of the virtual machines hosted on a particular host A in local storage such as the cache storage A. However as discussed above the virtualization kernel may prevent virtual machines that reference local storage resources from being transferred between hosts A N. In some embodiments the virtual machine cache may be configured to emulate a shared storage device by use of a VMDK virtual disk and vSCSI filter as described above. Alternatively the virtual machine cache may emulate shared storage by use of other types of virtual disks VLUN disks .

Once provisioned each virtual machine expects to have access to predetermined and contiguous cache storage space for which it has the cache tags discussed above . In one embodiment a dynamic provisioning approach is provided to divide the cache storage into cache chunks that can be dynamically provisioned to the separate virtual machines . According to one embodiment the cache provisioner module comprises a VLUN driver configured to manage the chunks of cache data that is allocated to each virtual machine . According to one embodiment a VLUN disk is a virtual storage resource allocated to a virtual machine . Since multiple virtual machines will typically be operating on a single host the chunks of cache storage that come available will likely be located in different physical areas of the cache . The VLUN driver may be configured to create a VLUN disk that is assigned to the virtual machine .

In some embodiments the virtual machines may be configured for use with a fixed amount of storage space. The virtual machine may react adversely or may not operate properly if there is sudden atypical change to storage space. Accordingly the cache provisioner module may be configured to expose cache storage resources that appear to the virtual machine to have a fixed size. In some embodiments the cache provisioner module VLUN driver allocates a limited amount of cache storage to the virtual machines on an as needed basis while exposing a fixed storage capacity through a virtual disk such as a VLUN disk as described above. To avoid potential conflicts the cache storage space allocated to the virtual machine appears to have a fixed size. The fixed size may be equal to a set amount of space that a virtual machine s operating system expects to detect. Thus the dynamically provisioned cache storage may appear to be constant.

The cache provisioner may be configured to manage the dynamic allocation of the available cache chunks to the virtual machines . The storage is thus physically managed in chunks by the VLUN driver of the cache provisioner module which provides each virtual machine with the abstraction of contiguous chunks of storage space.

The VLUN driver may be configured to translate allocations from a virtual space exposed through the VLUN disk into the underlying physical chunks allocated to each virtual machine within the cache . As a result the embodiment allows the cache provisioner module to allocate cache storage as chunks which can be allocated on the fly to the various virtual machines . In operation the cache provisioner module maintains mappings between the virtual space of each virtual machine and the actual physical storage space located in the cache e.g. cache addresses within the cache storage device s of the cache .

The cache may comprise one or more persistent storage devices such as one or more solid state and or Flash storage devices. The cache provisioner may be configured to dynamically allocate different amounts of cache storage to the virtual machines A N e.g. by use of the VLUN Driver as described above . The virtual machines A N may be allocated varying amounts of cache space in accordance with different cache requirements of the virtual machines A N.

The cache provisioner may comprise a map module configured to map virtual storage resources exposed to the virtual machines A N to physical addresses in the cache . As described above the VLUN driver may be configured to present fixed sized contiguous cache storage allocations to the virtual machines A N. The map module may be configured to map the virtual storage allocations to physical cache addresses e.g. cache chunks and or pages . For example the cache storage provisioned to the Virtual Machine 1 A is illustrated diagrammatically as space in the cache . The virtual space allocated to this virtual machine A is two terabytes 2 TB and the physical cache storage space that is actually allocated to the virtual machine A is four gigabytes 4 GB . The VLUN driver may expose the entire fixed sized 2 TB virtual disk to the virtual machine A and may dynamically allocation portion s of the physical storage space to the virtual machine A as needed 4 GB in this example .

Furthermore the chunks of storage space allocated to a particular virtual machine may be disbursed within the physical address space of the cache in an arbitrary manner the chunks may be discontiguous . However the virtual cache storage presented to the virtual machine A may be contiguous.

The cache provisioning module may be configured to dynamically shift cache storage allocations between the virtual machines A N in response to changing cache requirement and or as virtual machines A N are transferred to from the host .

The virtual cache storage allocated to the virtual machine 1 VM 1 A is depicted as a contiguous range of cache chunks VM 1 VM 1 VM 1 VM 1. The physical cache storage actually allocated to VM 1 A is depicted as a discontiguous set of chunks VM 1 VM 1 VM 1 VM 1within the physical address space of the cache . As depicted in the chunks in the physical address space of the cache may be discontiguous and or interleaved with chunks allocated to other virtual machines B N. Although the illustration in shows some of the different locations in a physical order the cache chunks allocated to the VM 1 A may be located in a random order in accordance with the availability of physical cache resources e.g. available chunks .

The VLUN mapping implemented by the map module may be configured to map virtual cache storage allocations to physical cache allocations. In some embodiments the VLUN mapping may comprise an any to any index of associations between the virtual cache storage allocations exposed to the virtual machines A N and the physical address space of the cache .

In some embodiments the virtual machine cache may leverage the VLUN mapping to secure data stored in the cache . For example the virtual cache manager may use the VLUN mapping as a form of access control wherein access to physical cache chunks is restricted to the virtual machine to which the physical cache chunk is mapped. For example the cache chunk labeled VM 1may only be accessible to the virtual machine A N to which the chunk is mapped in the VLUN mapping e.g. VM 1 A . Moreover by virtual of the VLUN mapping layer virtual machines A N may be incapable of referencing and or addressing physical cache chunks of other virtual machines A N.

The VLUN mapping may be configured to map virtual cache storage using the VMID of the corresponding virtual machine. Accordingly when a virtual machine is transferred from the host to another host the VLUN mapping may remain valid to reference data of the virtual machine e.g. given the VMID the retained cache data of the corresponding virtual machine A N may be identified and accessed . Therefore in some embodiments the cache transfer module A may be configured to determine the cache address of retained cache data of a transferred virtual machine using the VMID and or the VLUN mapping .

The cache provisioner module of the virtual machine cache may comprise a VLUN driver configured to represents to the operating system of the virtual machine A that a large fixed amount of space is allocated to the virtual machine A even though a dynamically provisioned lesser amount is actually allocated to any one virtual machine A. Although the cache storage appears to the fixed the actual cache storage allocated to the virtual machine A may be dynamically provisioned by a VLUN driver .

Each of the virtual machines A N may comprise a respective SCSI filter that is incorporated into a storage stack of the virtual machine A N e.g. OS SCSI stack . The virtual machines A N may further comprise a windows driver comprising the cache management system described above. The CMS may be configured to manage data transfers between the virtual machine A and various storage devices. An input output I O filter cooperates with the CMS to service I O requests directed toward primary storage either directly from the primary storage or from the virtual machine cache of the host . The primary storage may comprise a physical storage device located within the host device or a virtual disk defined on a virtual and or shared storage.

The virtual disk may be available only to a single virtual machine A while the primary storage may be accessible by a number of virtual machines A N e.g. all of the virtual machines A N deployed on the host . The SCSI filter may be configured to manage transfers between the CMS and the I O filter and the various storage devices. The SCSI filter may be configured to manage the transfer of data among physical and virtual entities e.g. primary storage VLUN disk and or the virtual machine cache . Within the virtual machine A the SCSI filter is configured to identify the VLUN disk and to manage capacity changes implemented by inter alia the cache provisioning module and or VLUN driver as described above. The VLUN disk is a virtual disk which provides raw storage capacity for the CMS . As described above the VLUN disk may be configured to report a larger fixed storage capacity than the actual physical cache capacity allocated to the virtual machine A such that the cache provisioner can dynamically provision cache storage to from the VLUN disk without adversely affecting the virtual machine A. Alternatively the SCSI filter may be configured to manage the actual physical capacity of the VLUN disk which may be hidden from other applications and or operating systems of the virtual machine host A. In one embodiment the VLUN disk is presented to the virtual machine A as a read only storage device. Consequently the guest operating system prevents other applications of the guest operating system from writing data to the VLUN disk .

As discussed above in operation though the actual storage space that is allocated to the virtual machine A is one value another fixed value is represented to the virtual machine operating system. For example the virtual machine A may be allocated 4 GB of actual cache storage but may appear to the operating system to comprise 2 TB cache storage.

The cache provisioner module may report the actual physical cache storage allocated to the virtual machine A via a communication link . The communication link may operate separately from I O data traffic between the VLUN driver and the SCSI filter . Thus asynchronous out of band messages may be sent between the VLUN driver and the SCSI filter to inform the Windows driver of the actual cache space allocated to the virtual machine A in the cache . The SCSI filter may report the allocation information to the CMS which may use the allocation information to determine the number of cache tags available to the virtual machine A. Accordingly the cache may be thinly provisioned with respect to the virtual machines A N. The communication path allows the ability to inform the Windows driver particularly CFS of cache storage capacity changes when actual cache storage space that is allocated to the virtual machine changes. Thus in underlying operation each virtual machine A N is dynamically actually allocated cache storage space in accordance with the cache requirements of the virtual machine A N which may vary over time in response to changing operating conditions power on off events virtual machine transfers and so on.

According to one embodiment the virtual machine A may be provisioned different amounts of cache storage as its needs change. Referring to a process flow chart of a change in allocation of cache storage is shown and will act as a process guide in conjunction with the system diagram of to illustrate how cache storage space is dynamically provisioned.

In the example the virtual machine A may be dynamically allocated additional cache storage e.g. an increase from 4 GB to 8 GB . As described above the cache provisioner module may be configured to monitor and or manage cache storage for the virtual machines A N by use of the VLUN driver . The cache provisioner module may be further configured to dynamically provision cache storage and to communicate allocation changes to the virtual machines A N via the SCSI filter . Step may comprise the VLUN driver resizing the cache storage allocated to the virtual machine A from 4 GB to 8 GB.

In step the VLUN driver instructs the SCSI filter to stop sending I O data traffic relating to the virtual machine cache during the dynamic provisioning or re provisioning of cache storage. The SCSI filter communicates the this directive to the CMS . Alternatively or in addition the directive may be communicated via the path . In one embodiment while the CMS stalls applications running on the virtual machine A may continue to perform I O operations with the primary storage via path . Accordingly applications such as Iometer Microsoft Office SQL Server and other applications can continue to operate and the I O traffic directed to the primary storage may continue. The CMS may be configured to invalidate selected cache tags in response to write operations that occur while the CMS is stalled. Step may further comprise flushing any outstanding I O requests directed to the virtual machine cache e.g. waiting for any outstanding I O data traffic to and from the virtual machine cache to complete .

In step the CMS notifies the VLUN driver that the pending I Os are complete. In response the CMS is stalled as described above. The VLUN driver initiates the resizing from 4 GB to 8 GB in step and instructs the SCSI filter that the new allocation of cache storage space is 8 GB in step . In step the SCSI filter instructs CMS to resize the allocation of cache storage to 8 GB. In one embodiment the CMS resizing the cache storage allocation by adding new cache tags to an existing working set of cache tags . Thus the CMS may maintain the existing working set of cache tags and associated metadata corresponding to the original 4 GB during the resizing operation.

In step control is returned to the SCSI filter and in step the SCSI filter instructs the VLUN driver that the change of cache storage for virtual machine A is complete. In step the VLUN driver instructs the SCSI filter to resume operations e.g. unstall the CMS . In step the SCSI filter instructs CMS to resume operations. In step and I O data traffic resumes to the virtual machine cache and cache and the virtual machine A can continue to send I O data traffic to either the cache or the shared storage as described above.

Thus a thin provisioned cache device is provided where the limitation of a fixed disk capacity requirement in conventional virtual systems has been addressed. Thus the operating system can essentially be deceived into thinking that a fixed amount of cache storage has been allocated so that applications in the operating system have no impact. And the actual cache storage space allocated to any virtual machine may be resized on the fly without impacting other system operations. The result is an intelligent and optimized utilization of cache storage where the available cache storage space is more efficiently utilized. Multiple virtual machines are dynamic in nature and their data flow and cache storage needs change dynamically. A virtual machine substantially reduces its demand for cache storage in different modes or circumstances. For example it may power off or go into sleep mode it may stall while moving from one host to another and its needs will necessarily change when these operational changes occur. A virtual machine may alternatively increase its demand for cache storage in other modes or circumstances such as when it wakes up from a sleep mode arrives at a new host after moving or simply experiences an upsurge in usage operations. This embodiment gives the host system the flexibility to dynamically change and optimizes the use of cache storage at the same time. Accordingly the amount of cache storage designed in a host system can be minimized substantially saving costs in a host system or device. The cache device which is commonly implemented in expensive flash memory is itself virtualized in this embodiment and its operations are intelligently managed in a way that optimizes the use of its storage space allocating cache storage to the various virtual machines according to their needs.

One fundamental precept of virtual systems is that shared storage must be secured among the different virtual machines. This is important because the different virtual machines may store confidential information in the various storage chunks in cache storage that possibly could be accessed by other virtual machines in the dynamic provisioning process. For example a person s confidential financial and identity information may be stored by one virtual machine in one chunk of allocated cache data storage and that machine s allocated cache storage may be resized as a result of low demand. The virtual machine may then give up allocated cache storage space to another machine in the dynamic allocation process also giving the second virtual machine that acquires the data chunk having the person s confidential information stored in that chunk. This is thus a security risk and the dynamic allocation process that has been designed to optimize the use of the cache storage may cause a problem when resizing cache space of particular machines and allocating cache storage chunks from one virtual machine to another. One embodiment of the invention addresses this security risk in an elegant manner without substantial impact to the work flow within the system while dynamically provisioning cache storage chunks.

The virtual machine cache may be further configured to secure cache data as cache chunks are dynamically reallocated between the virtual machines A N. The individual chunks must be properly handled when being allocated between the virtual machines A N to ensure that the virtual machine to which a chunk has been allocated cannot access cache data stored thereon by the previous owner of the chunk .

In some embodiments the virtual machine cache may be configured to erase chunks as they are provisioned between virtual machines A N. This approach may not be efficient however due to erase latencies and or wear characteristics of some types of solid state storage media used to implement the cache . Alternatively the virtual machine cache may be configured to TRIM chunks e.g. logically invalidate the data stored on the transferred chunks . In other embodiments chunks may be monitored to ensure that a virtual machine A N does not attempt to perform a read before write operation. Chunks that were erased prior to being transfer and before being used to store data of another virtual machine A N may not require monitoring. As used herein such that is erased before being transferred and or before use to store data by another virtual machine A N is referred to as an unused chunk. By contrast a chunk that was used to store data of another virtual machine A N and was not erased or TRIMed is referred to as a used or dirty chunk which may be monitored to prevent read before write security hazards.

The monitoring state of a chunk may be persisted for use after a power cycle event. In some embodiments the VLUN driver monitors used chunks after allocation to a different virtual machine A N and or during a current power cycle of the virtual machine cache . In one such embodiment a bit mask indicator is used to prevent read before write hazards on the dirty chunk . In one embodiment each 4 kb sub portion of a used chunk is monitored to determine whether there has been a corresponding write in the 4 kb sub portion. This is determined at the time the subsequent virtual machine A N accesses the used chunk and is performed only when necessary to prevent the reading of old data. After provisioning a used chunk to a new virtual machine A N each sub portion of the chunk may be tested prior to any read operation on the used chunk until the respective sub portions are overwritten and or erased .

The process then proceeds to step where it is determined whether all pages within the chunk were written erased and or TRIMed at least once by the second virtual machine N and in particular whether this has occurred since the chunk was provisioned to the second virtual machine N. If yes then the need for the security test is obviated and the reads to this chunk by the second virtual machine N may resume in step without further testing otherwise the flow continues to step for further testing on an as needed basis until each sub portion cache page is written erased and or TRIMed at least once and while the second virtual machine N continues to be allocated the chunk continues to initiate reads to the chunk .

At step if the sub portion testing indicates a read before write hazard e.g. the particular sub portion has not been written by the second virtual machine A the read may be rejected at step . Otherwise if no security hazard exists the read may be allowed at step . The determination of step may comprise accessing a bitmap comprising on or off logic 1 or logic 0 for example. The bits may be recorded in a table such as table . This may be in the form of a type of bit mask that can be stored in cache or other storage location. The indication of whether a page is entirely written over may be indicated by a single bit that is toggled to a binary 1 when a full write over occurs. When a partial write occurs to the page of a chunk the VLUN driver converts the partial write to a full write by filling in zeros for the data space that is not covered by the partial write from the CMS . Any other write to a page while the very first write is in progress will be returned with error. If a read occurs before a write then a test would show that a full write has not occurred and the binary bit should be 0 . As the chart shows the table may start out as all logical 0 indicating that the individual pages have not been fully written over since being allocated to the second virtual machine. As full write over occurs in each page the full write indicator bits eventually become more populated across the array eventually ending up over time with all logical 1 bits indicating that each and every page has been written over by the second virtual machine at least once.

The VLUN Manager may comprise a user space daemon that configures the provisioning of the portions of the cache storage among the different virtual machines A N e.g. to parcel out cache storage fairly and efficiently among the virtual machines A N . When a management module may establishing relative proportions allocated among the various virtual machines A N. Shares of storage space may be defined for each virtual machine A N for example 

These shares are used as an abstract definition of proportions of cache storage that is allocated to particular virtual machines A N independent of the actual size of the cache . The shares can be divided up and allocated among multiple virtual machines A N as they are added or subtracted from the host as further cache capacity is added and or as the system changes over time. The shares allow for a relative dynamic percentage for each virtual machine A N in response to changing demands. Thus for each virtual machine A N the amount of cache storage it receives can be calculated as Capacity of VM 1 shares VM 1 shares total active VM shares Cache Capacity where the total active VM shares are the total number of shares allocated to total number of powered on virtual machines A N. Thus for virtual machines A N that are not up and running their shares are not accounted for in the capacity equation. Thus for the example in and given the allocated shares set forth above since only Virtual Machine 1 A and Virtual Machine 2 B are active and given the example of a 100 GB cache storage capacity the following capacities may be calculated 

The different virtual machines A N may be powering on and off vMotionting migrating away and back to the host so the capacity allocations can change over time. At the time Virtual Machine 3 N comes on line the capacity of each host would be calculated as follows 

Thus the current percentage may be calculated based on current allocations. In performing this transition of VM3 N online and being allocated its percentage or shares of cache storage VM3 N must be allocated its percentage shares and virtual machines VM1 A and VM2 B must relinquish storage space. This is accomplished by the methods discussed above in connection with . Each virtual machine A N must stall operations change capacity and then resume operations. Thus for each machine VM2 B must shrink from 75 to 50 VM1 A must shrink from 25 to 17 and VM3 N can then be given its 33.3 which is taken from the relinquished storage space from VM1 A and VM2 B. Thus the embodiment provides a dynamic provisioning of cache using a virtual disk approach.

Additionally to the extent virtual machines can be provisioned storage space according to the shares concept IOPS capacity can also be allocated among the virtual machines. Thus for each machine 

In one embodiment the VLUN driver manages the cache such that each VM A N receives its allocated shares of IOPS capacity. Typically a cache device operates at a single IOPS rate for each request that it services. Consequently the VLUN driver in one embodiment manages IOPS shares amongst VM1 A VM2 B and VM3 N by giving each an opportunity to use the cache device in a given time period. In other words each VM A N gets a time slice within a given time period to use the cache device . In this manner the IOPS capacity between VMs A N can be managed. In some embodiments the IOPS rate of a VM A N may be throttled to allow other VMs A N to access the cache in accordance with IOPS share allocation therebetween.

As discussed above virtual machines may be transferred between hosts without powering down or taking the virtual machine offline in the process. In conventional systems since hosts are usually connected to shared storage this process is well defined and seamless. However in systems configured according to the various embodiments described above that utilize local cache storage rather than shared storage for certain virtual system operations such as a thin provisioned cache there are conflicts that result from exercising certain features common in virtual systems such as moving virtual machines from one host to another.

In conventional virtual systems a virtual machine may be moved from one host to another by utilizing and or emulating shared storage. However moving virtual machines from one host to another while utilizing the various embodiments described herein problems would occur with the transfer and critical data and virtual systems operations may be compromised. In some virtual systems the move simply would not be allowed such as by VMWare virtual system products for example. As discussed above the systems and methods for emulated a shared storage device may address these issues.

Referring back to shared storage may be emulated by use of an I O filter operating within the virtual machines A N. The I O filter may be configured to monitor I O operations directed to particular virtual machine disks VMDKs . The virtualization kernel may treat VMDKs as shared storage therefore any virtual machine A N that maps to a VMDK may be available for to be transferred e.g. in a VMotion operation .

In other examples shared storage is emulated by creating shared VLUN disks that are configured to appear to the virtualization kernel to the shared devices e.g. by replicating a serial number or other identifier . depicts one embodiment of a configuration for emulating shared storage. The embodiment A includes two hosts Host 1 and Host 2 where Host 1 includes instances of two virtual machines VM 1 and VM 2 that have VLUN disks and respectively. Host 2 includes virtual machine VM 3 having VLUN disk 3 . Host 1 further includes virtualization kernel and VLUN driver instantiated therein as well as cache storage for storing cache data from the virtual machines . Each cache storage may be divided up into chunks as discussed above where the chunks are identified as holding either VM 1 or VM 2 data in Host 1 and VM 3 data in Host 2. Host 2 includes its own virtualization kernel VLUN driver and cache storage . In one embodiment the system is configured to allow a transfer of one or any virtual machine such as VM 1 for example from Host 1 to Host 2 and to do so substantially seamlessly without the need to completely shut down.

In conventional virtual systems shared storage may store the instances of the primary virtual disks of the virtual machines located among different hosts. These primary virtual disks are accessible to virtual machines operating on hosts that have shared access to the shared storage . In order to enable the transfer of virtual machines from one host to another the virtualization kernel requires that the source host e.g. Host 1 and the destination host e.g. Host 2 both have shared access to each storage device of the transferring virtual machine.

Embodiments of the present invention allow transfer of virtual machines between hosts even though each host does not have access to all physical storage devices of the transferring virtual machine. For example Host 1 and Host 2 both have access to shared physical storage but Host 2 does not have access to the physical storage device serving as the cache device . Similarly Host 1 does not have access to the physical storage device serving as the cache device .

According to one embodiment virtual machine transfers from one host to another is accomplished by instantiating the VLUN disk A in an active state on Host 1 and also instantiating a corresponding VLUN disk B in a dormant state on Host 2. In certain embodiments these instantiations are performed before the virtual machines power on. In some embodiments during a configuration phase the VLUN driver instantiates a dormant VLUN disk C N on each host a user may desire to use for transferring of virtual machines for example each host in a cluster of hosts.

The VLUN disk A N having the same serial number either active or dormant on each host satisfies the requirements of the virtualization kernel requires that the source host e.g. Host 1 and the destination host e.g. Host 2 both have shared access to each storage device of the transferring virtual machine.

For example an instantiation of VLUN disk 1 A having a serial number of naa.200.cd123. An identical instantiation may be made in on Host 2 including the same serial number but it is dormant where VM 1 does not actively use the copy B but rather uses it as a type of holding place for VM 1 when and if VM 1 transfers from Host 1 to Host 2. In response to the transfer the naa.200.cd123 disk on Host 1 becomes dormant and the corresponding disk on Host 2 becomes active.

In a system of multiple host computers that each have multiple virtual machines multiple VLUN disks may be instantiated on the different host computers to aid in transferring virtual machines from one host to another with the VLUN disk of the source host transitioning to a dormant state and the VLUN disk of the destination host transitioning to an active state.

It has been observed that in typical computing systems with peripheral and other system devices such as virtual computing systems for example SCSI operations serve as interfaces for devices within a system and can be utilized to fool the virtualization kernel into believing that the cache storage devices located in individual host devices are actually accessible by each host in the cluster. When an operating system communicates to components within the system and discovers devices within the purview of operating system such as storage disks VLUN disks and other devices it initiates queries when a device is found to learn the device s identity and relevant operating information. It questions who the manufacturer is what the model number is what the capacity is and importantly for this embodiment what the serial number is. The serial number is configured to be globally unique within the system. Thus in a virtual system the operating system queries discovered devices such as disks to identify them and to derive a serial number that will be used by the operating system to identify the storage device. For virtual machines the operating system in conventional virtual systems identifies shared storage devices and derives a unique serial number to identify it within the virtual system. Once the virtual machines are created the conventional virtual systems identify each virtual machine as a shared storage device by using this unique serial number assigned to the shared storage.

According to the embodiments discussed herein however cache storage devices are not shared among different hosts but are local to the hosts and shared among virtual machines within the hosts. In operation conventional virtual systems require that the virtual machines are assigned to shared storage in order to enable a transfer of a virtual machine from one host to another. According to one embodiment fictitious shared storage is created and exported to the host as a Fibre channel or SAS device. Thus the Fibre channel or SAS device is artificially recognized as a shared storage device with a unique serial number and is instantiated when a VLUN disk is created. VLUN disk devices are fictitious shared storage spaces that are associated with actual storage space in the local cache storage devices. Once created these VLUN disks are treated as actual devices from the perspective of the operating system. The unique serial numbers for VLUN disks instantiated within the local cache devices such as naa.200.cd123 are derived by the virtualization kernel from the serial number of the shared local storage and each are unique and associated with a particular VLUN disk. Thus when the VLUN disk is created it is created with the unique serial number and these are recognize by the operating system as legitimate entities but are fictitious shared storage. This derived serial number is also used to create another VLUN disk in Host 2 such as VLUN disk 1 B so that a virtual machine such as VM 1 will have a corresponding VLUN disk in the other host to communicate to and continue its I O data traffic after being transferred to Host 2.

While running in Host 1 prior to moving VM 1 through CFS believes it has some amount of data stored in the cache storage having chunks designated VM 1 in this illustration and these chunks of data storage are not resident in Host 2 after the move. Prior to the move and referring together with a flow chart showing general steps of a transfer process in step CFS is operating it is actively caching data and issuing I O data transfers to and from the cache storage in normal operation. CFS is doing read operations to the designated chunks of data storage prior to the move. Once the move is initiated in step and then in step the hypervisor first completes VM 1 s initiated I O transfers to the cache and any shared storage and then stops these I O transfers for a small period of time prior to the transfer of the virtual machine. In step the VM 1 then stops operating and essentially disappears from Host 1 and then reappears on Host 2 and begins operations.

Once a virtual machine moves from one host to another the data is left resident on the prior host Host 1 for example and when the virtual machine arrives at the destination host Host 2 in this example the data is left behind. Again this breaks the design assumption of conventional virtual systems that requires and assumes the existence of having the I O data transfers associated with the virtual machine to be available to the virtual machine when it transfers which is typically located on remote shared storage that is shared among different hosts. Having copies of the VLUN disks of the different virtual machines is an approach used to essentially deceive existing virtual systems into believing that each virtual machine is storing I O data transfers in remote shared storage.

In conventional systems the hypervisor stalls I O transfers to the remote storage device prior to transferring from one host to another. The virtual machine is then transferred to another host instantiated on that host and operations resume. In this embodiment however there is not only remote shared storage for operations but also local storage. After the transfer in step VM 1 is associated with VLUN driver and Cache which does not have the former cached data and VM 1 has essentially zero capacity in the local cache . Referring to the post move system is illustrated with VM 1 appearing in Host 2 VLUN disk associating with VLUN driver and VLUN disk 1 now designated as B and being in the active state with identical serial number naa.200.cd123. After the move CFS of VM 1 still registers that it has 4 GB of data and that it has data chunks stored in the cache but is now located in Host 2 without access to that cache storage with that capacity and also without access to that stored data that is still resident in cache storage . Thus VM 1 is essentially not aware that the move has occurred. Referring back to prior to the move Host 2 has only VM 3 that has the entire capacity of Cache Note In a typical system multiple virtual machines VM 1 VM 2 . . . VM n exists in a host and there is a complex mapping of shared cache storage. For simplicity of discussion and to avoid obscuring the description of the embodiments only these three virtual machines are illustrated . When VM 1 arrives in Host 2 VM 3 has substantially the entire capacity of Cache and VM 1 needs to get acclimated and acquire capacity in resident cache storage . shows an example acclamation process .

After arriving in Host 2 in step CFS will continue to send I O requests to the VLUN driver in step the new VLUN driver. The VLUN driver will fail the I O requests in step with errors traveling up the storage stack to the CFS that cache addresses assigned to VM 1 are out of VM 1 s range it has no capacity. The error code is interpreted and recognized by the SCSI filter within VM 1. In some embodiments the SCSI filter may fail the I O requests to the CFS and request that CFS invalidate the cache tags associated with the I O requests. Thus there is a small period of time after the transfer from Host 1 to Host 2 when there is no cache storage capacity of Host 2 being used by VM 1. The small number of I O requests that are issued are failed and the cache tags are invalidated.

Alternatively in response to the error code the virtual machine cache may instruct the CFS to retain the cache tags via the SCSI filter . The cache tags may be retained by the cache tag retention module described above. In addition the virtual machine cache may further comprise a cache transfer module e.g. module B that is configured to transfer cache data from Host 1 to Host 2 as described above. Similarly Host 1 may comprise a cache retention module A configured to retain cache data of the virtual machine as described above. Accordingly the working set of cache tags in the transferred virtual machine may be retained.

The CFS will then reissue the I O requests that failed to primary virtual disk storage typically stored on shared storage in step . Later the VLUN Manager recognizes the arrival of VM 1 in Host 2 and the VLUN driver provisions cache storage capacity for VM 1 in step according to an allocation of shares as discussed above. In response the cache transfer module described above may begin transferring cache data to from Host 1 to Host 2 as described above.

Subsequent IO requests will benefit from local cache storage once CFS acquires capacity in local cache storage . The VLUN driver stalls CFS as discussed above to perform capacity allocation.

In step the VLUN driver instructs the CFS to retain cache tags . Step may comprise resizing the cache tags in accordance with the cache capacity allocated to the virtual machine on Host 2 . Step may further comprise initiating cache transfer s form the Host 1 and or shared storage as described above.

In step CFS resumes operation with the retained potentially resized set of cache tags . Also the allocation will utilize the bit mask processes described above to protect against VM 1 reading any old data that may have been written by VM 3 or any other virtual machine. Thus VM 1 would need to write to the chunks of allocated cache storage space before it can read or the read requests will get a fail error.

Step may comprise determining the remote host of the transferred virtual machine X. Step may comprise inspecting the VMID for an included host identifier querying the transferred virtual machine X receiving pushed cache data from the virtual machine or the like as described above.

Step may comprise populating at least a portion of the cache storage allocated to the virtual machine X with cache data retained at the remote host A as described above. Step may comprise requesting the cache data verifying that the host B is authorized to access the cache data receiving the cache data in a push operation or the like. Cache data may be requested transferred and or pushed according to a cache transfer policy as described above. In some embodiments step further comprises populating the allocated cache storage with data accessed from primary storage or another source . Step may further comprise informing the remote host A that the retained cache data of the virtual machine X no longer needs to be retained as described above.

Step comprises determining an address of the retained cache data based at least in part on a VMID of the transferred virtual machine e.g. within an index of mappings maintained by the map module described above .

Step comprises providing retained cache data as described above. Step may comprise responding to requests for the cache data pushing the cache data or the like.

The embodiments disclosed herein may involve a number of functions to be performed by a computer processor such as a microprocessor. The microprocessor may be a specialized or dedicated microprocessor that is configured to perform particular tasks according to the disclosed embodiments by executing machine readable software code that defines the particular tasks of the embodiment. The microprocessor may also be configured to operate and communicate with other devices such as direct memory access modules memory storage devices Internet related hardware and other devices that relate to the transmission of data in accordance with various embodiments. The software code may be configured using software formats such as Java C XML Extensible Mark up Language and other languages that may be used to define functions that relate to operations of devices required to carry out the functional operations related to various embodiments. The code may be written in different forms and styles many of which are known to those skilled in the art. Different code formats code configurations styles and forms of software programs and other means of configuring code to define the operations of a microprocessor in accordance with the disclosed embodiments.

Within the different types of devices such as laptop or desktop computers hand held devices with processors or processing logic and also possibly computer servers or other devices that utilize the embodiments disclosed herein there exist different types of memory devices for storing and retrieving information while performing functions according to one or more disclosed embodiments. Cache memory devices are often included in such computers for use by the central processing unit as a convenient storage location for information that is frequently stored and retrieved. Similarly a persistent memory is also frequently used with such computers for maintaining information that is frequently retrieved by the central processing unit but that is not often altered within the persistent memory unlike the cache memory. Main memory is also usually included for storing and retrieving larger amounts of information such as data and software applications configured to perform functions according to various embodiments when executed by the central processing unit. These memory devices may be configured as random access memory RAM static random access memory SRAM dynamic random access memory DRAM flash memory and other memory storage devices that may be accessed by a central processing unit to store and retrieve information. During data storage and retrieval operations these memory devices are transformed to have different states such as different electrical charges different magnetic polarity and the like. Thus systems and methods configured disclosed herein enable the physical transformation of these memory devices. Accordingly the embodiments disclosed herein are directed to novel and useful systems and methods that in one or more embodiments are able to transform the memory device into a different state. The disclosure is not limited to any particular type of memory device or any commonly used protocol for storing and retrieving information to and from these memory devices respectively.

Embodiments of the systems and methods described herein facilitate the management of data input output operations. Additionally some embodiments may be used in conjunction with one or more conventional data management systems and methods or conventional virtualized systems. For example one embodiment may be used as an improvement of existing data management systems.

Although the components and modules illustrated herein are shown and described in a particular arrangement the arrangement of components and modules may be altered to process data in a different manner. In other embodiments one or more additional components or modules may be added to the described systems and one or more components or modules may be removed from the described systems. Alternate embodiments may combine two or more of the described components or modules into a single component or module.

