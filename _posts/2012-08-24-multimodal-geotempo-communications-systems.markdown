---

title: Multi-modal, geo-tempo communications systems
abstract: Disclosed is a flexible, multi-modal system useful in communications among users, capable of synchronizing real world and augmented reality, wherein the system is deployed in centralized and distributed computational platforms. The system comprises input devices to generate signals representing speech, gestures, pointing direction, and location of a user, and transmit the same to a multi-modal interface. A plurality of agents and one or more databases are integrated into the system, where at least some of the agents receive signals from the multi-modal interface, translate the signals into data, compare the same to a database, generate signals representing meanings as defined by the database, and transmit the signals to the multi-modal interface. Finally, a plurality of output devices are associated with the system to receive and process signals from the multi-modal interface, some of said signals representing messages to the user to be communicated by means of an output device.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08880606&OS=08880606&RS=08880606
owner: Applied Research Associates, Inc.
number: 08880606
owner_city: Albuquerque
owner_country: US
publication_date: 20120824
---
The present invention regards systems for facilitating crucial communications among mobile users by means of multi modal geo referenced and time referenced messages as well as communications to and from and remote control of stationary and mobile objects. The systems allow users to create annotate transmit retrieve and review voice iconic visual tactile and text messages and or in some embodiments to control apparatus and systems. Each message may include a geographic marker based upon its place of origination e.g. GPS coordinates and or area of dissemination and further may include a time reference regarding the time of creation and or duration of transmission receipt or review. In some embodiments the present invention allows users to tag or otherwise identify certain structures or geographical features at a location and send messages regarding the tagged structure or feature to other individuals who might have the structure or geographical feature in their field of view or near their geographical location.

The systems of the present invention are designed to support human interactions occurring in reduced geographical locations including environments like battlefields first response operations and daily life social networking and provide dynamic communications systems where mobile users may improve their performance by communicating in association with their physical surroundings. Components of the systems are designed to provide flexibility to cope with environments where different profiles roles levels of expertise and knowledge are present among the mobile users. Furthermore some components are designed to support basic human analogical communication such as gestures tactile sensing visual icons and verbal commands thereby providing users with an intuitive way to support and extend their communications. In some embodiments the devices and systems augment the real world perceived by users by incorporating geo tempo registered data overlaid on see through displays.

The systems further aim to support Stigmergy a form of communication where the environment plays a fundamental role in the communication process. By supporting the annotation of geo tempo registered multi modal messages the systems will facilitate the coordination communication and awareness among mobile users who most likely have limited access to both global information and information processing capabilities.

Furthermore the systems preferably include an agent based architecture to address scalability high performance and fault tolerance features and may be used in distributed or centralized computing environments.

Although the applications of the present invention are focused primarily on enabling communication through and with highly dynamic environments they can be extended to other domains like video games and virtual reality and for everyday applications.

Embodiments of the systems of the present invention are generally depicted in and comprise various hardware components and other input and output devices as well as computer software and database components which are designed to work together to facilitate the generation transmission processing review dissemination receipt and view of multi modal and geo tempo registered messages. As shown in this figure a plurality of sensor input software and hardware multi modal output software and hardware and communications software and hardware interface with a centralized processing command unit however as hereinafter described preferably the system of the present invention comprises a plurality of processing command units in a de centralized configuration.

Most generally the systems of the present invention comprise a plurality of input processing and receiver output devices. By this system and using an application interface the user generates a command or message via the input devices an event this command or message is interpreted by the processing software and compared to the related databases to determine whether it is a command for the same or another device or a message for transmission to other users. The system then transmits control or messaging signals to the output devices to control the same or to disseminate a message.

The present system contemplates a series of input devices that generate electronic signals representing user speech gestures location pointing direction and in some embodiments range finders. Further input devices include for purposes of system output devices that generate electronic signals representing the user s look vector. Associated with or integrated into many of these input devices are translation or recognition agents capable of converting raw electronic signals into meaningful data.

The system further comprises an application interface a plurality of software agents and databases and the hardware and physical engines associated therewith and devices that generate and transmit signals to control objects or communicate messages. The agent based architecture in the system enables the system to be deployed in centralized and distributed computational platforms and further enables flexibility and adaptation of the application in run time to adjust to environmental mission and other issues.

The architecture of the system of the present invention is designed to be flexible enough to be able to provide its communication and control services and operate under either or both a centralized and a distributed manner. This characteristic allows the person deploying the system to adapt it to her own needs and computational resources. Thereby the system could deploy all of the services at one or more servers which perform the calculations associated with user requests demands and consummate related actions in a centralized manner. Alternatively each of the calculations may be performed on processing and memory resources local to the user e.g. running all the system services on a wireless enabled handheld device . A third and preferred option is that the services are deployed at nodes with each node having processing and memory resources to perform some calculations and where any node may be a client user as well as a server. In this distributed manner dedicated server nodes are not required. In any of these designs the system provides the application of its services in a completely transparent way from the application s point of view.

As described generally above the present invention contemplates a series of input devices that generate electronic signals representing speech gestures and pointing direction generated by a user the location and look vector of the user the distance from the user to an object. More specifically audio and gesture input devices receive a user s speech or gestures and initially convert the speech or gestures into electronic signals location input devices determine and transmit the location of the user in coordinates longitude latitude and altitude one or more pointer input devices determine and transmit the direction of the pointer used by a user azimuth pitch and roll and range finders determine and transmit the distance from a user to an object the user is pointing to. All of this information can be used by the present invention to control objects transmit information tag structures or locations or generate a user s look vector the user s view through goggles or glasses determined from information from the location input device and a pointer input device .

Suitable speech input devices for use with the present invention include any commercial off the shelf COTS computer microphones such as 3.5 mm stereo headsets USB Microphones and USB webcams including a microphone. Typical brands for these components are Dynex Logitech Belkin Plantronics and others. These microphones have means to receive and translate user speech into electronic signals representing the user utterance but do not presently have integrated speech recognition software.

Suitable gesture recognition input devices for use with the present invention include pointer devices such as the Air Mouse from Gyration and the Wii Remote from Nintendo. These input devices have means to receive and translate gestures into digital signals and have integrated gesture recognition software. The core of these products includes three gyroscopes and three accelerometers to implement inertial navigation systems. With this combination of elements the pointer devices are capable of transmitting the relative trajectory of the device held and or worn by the user and processing the trajectories to translate the gestures made by the user into pre programmed messages corresponding to such gestures. Algorithms may be modified or implemented on the gesture recognition input device to process data from the gyroscopes and accelerometers along a timeline including machine state and neural network algorithms.

The pointing and gesturing input capabilities may be embodied in a single component or may be separate components e.g. pointing with the head while generating gestures with a device secured to or held by a user s hand . Pointer orientation sensors suitable for use in the present invention for purposes of determining a user s intentional pointing direction and a user s look vector include those based on electronic magnetic compasses EMC like Honeywell HMR3300 and HMR3000 digital inertial movement units IMU based on gyroscopes and accelerometers like the one included within Honeywell Dead Reckoning Module DRM4000 among others. Specifically EMC may be useful in contexts where no significant magnetic noise is present in the surroundings. IMU units are not only an alternative technology to EMC but also a good complement candidate given their resiliency to magnetic fields. shows an EMC based pointer in both hand held and helmet versions.

GPS devices such as GlobalSat MR 350P USB RS232 Garmin 18 USB and Pharos IGPS 500 among many others can be used as input devices in association with the present invention to track the location of a user. These devices transmit the coordinates of a user to the processors of the present invention at pre programmed intervals further they can be programmed to transmit the coordinates upon receipt of a request from the processors of the present invention. Thus for example the processors of the present invention may generate and transmit a digital signal to GPS input devices to obtain the coordinates of a transmitting user or a group of users upon receipt of any message or a particularly categorized message.

In some embodiments a range finder is also used to determine the location of geographical features or structures to be tagged. These are especially helpful where no 3 D model of the environmental surroundings is accessible to the processor. The range finder will determine the distance from the user to a target which will allow a general area of space in which the target is located to be tagged as hereinafter described. Like the other input devices this device is capable of generating a digital signal representing the range to an object and transmitting the same to the processors of the present invention. Range finders suitable for use in the present invention include Opti Logic RS400 Industrial Laser Rage Finder x RS 232 Serial Port. This device requires simple calibration and gives range readings up to 400 yards with 1 yard precision.

The input devices transmit electronic signals representing the information they are charged with collecting voice gestures location pointing direction and range to target to the sensor interface of the present invention. Some of this information is translated into data by the software on the input device itself for example some of the gesture input devices hereinabove described range finders and pointing devices transmit data rather than raw electronic signals . This data may be further translated into data meaningful to the system. Other input devices presently available in the marketplace only generate electronic signals representing the user input and to be meaningful to the present invention must be translated into data for further processing by the present invention. Thus one or more translation agents are preferably integrated into the system of the present invention. These agents may be located on any node cluster or server within the system including without limitation a light Personal Digital Assistant on the user.

Speech recognition software agents are typically coupled with commercial speech input devices. These agents receive electronic signals representing user utterances from the speech input device wherein the electronic signals from the input device are first transformed into digital signals by software drivers included in computer operating systems like Windows. The resulting digital signals are further processed and transformed into data by the speech recognition software of the agent like Windows Speech Recognition available at Windows Vista . Other suitable speech recognition software products include Microsoft Speech Server AT T Natural Voices Conversay and DECtalk. Services differ between manufacturers and most of them do not offer either an application program interface API or a documented API for the integration of their solutions with the system of the present invention. However the system of the present invention is independent in the sense of being able to work with different underlying services of the speech recognition technology used. Specifically the system acts as an overlay service for the application level which in turn uses the services of several underlying services or engines. Thus the speech services as well as the location services are considered interchangeable services for the system and the designer of the system may choose whatever the best technology is at a given time for a given application. In testing of the system interface concept of the present invention Microsoft Speech Server performed best given its superior quality signals. These products also provide text to speech capabilities and can be used for audibly communicating text messages in the system of the present invention.

Data from the other input devices may need to be translated into the language of the present system thus additional translation or recognition agents may be incorporated into the system of the present invention.

The internal space of the present invention provides an efficient and effective model to synchronize real world physical surroundings with augmented reality. Preferably the geographical internal space of the system of the present invention uses relative local coordinates based upon relation to a point set south east of any point to be mapped rather than true location coordinates or the location technology that may be provided with any third party products e.g. Ekahau uses a local 3 D Cartesian coordinates system and GPS uses a global angular system . Furthermore because the system is targeted to local contexts the geographical internal space of the system is based upon the false assumption that the earth is round discarding the effect on the calculations from the fact that the earth is fat around the equator as a result of the centrifugal force it gets from spinning on its axis . Extensions regarding the earth s dimensions are not significant in the local context and can therefore be ignored. Thus two points with the same altitude are modeled as being in a plane horizontal to the X Y plane in a Euclidian space.

In determining whether two dimensional or three dimensional mapping is appropriate the purposes for the system should be considered. In a traffic control application aimed to provide police officers the ability to control traffic lights depending on ongoing traffic conditions 2 D mapping is sufficient. In this environment the police officer points at a traffic light and gives a voice command through a speech and wireless enabled handheld device these input signals are transmitted to the sensor interface of the present invention which will process the information and transmit an appropriate signal to the traffic light thereby allowing the officer to better manage real time traffic conditions due to an accident for example. However for a theme park battlefield game application aimed to give the users a realistic feeling of what targets are shot when they pull the trigger of wireless and speech enabled weapons a 3 D space would be preferable. The latter example is similar to true battlefield maneuvers and its demands for a realistic scenario dictate a 3 D space in order to enhance the performance strategies and skills of the users involved in such maneuvers.

The configuration file of the geographical internal space of the system contains information about of the space 3 D or 2 D i.e. Z 0 and true latitude longitude and altitude coordinates A B and C of the point of origin. A point of origin is positioned at ground level south east of the location and thus all valid points in the system space will satisfy x 0 y 0 and z 0 where the x axis represents relative latitude the y axis represents relative longitude and the z axis represents relative altitude .

The system space data file includes the relative location of all passive stationary do not have the capacity to directly interact with other objects do not generate events for the system objects that can be controlled by means of the system and all active objects mobile have the capacity to interact by means of the system with other objects that transmit location information to the geographical space of the system. All of the objects mapped in the system are associated to one and only one input and or output device however one device can be associated with one or more other devices e.g. plurality of lights in a room plurality of devices on a user . Every object has identification location and is modeled by the space it occupies area in 2 D volume in 3 D and radius and height for a cylinder or width length and depth for a rectangle with a square area. Thus in 3 D every stationary object comprises a set one or more of polygons with each polygon represented by a set of points where the first and the last point are the same. In 2 D the stationary objects may be represented by a single polygon a sequence of points where the last and the first point are the same a set of polygons or simply by a sequence of points where the last point and the first point in the sequence are not the same.

All mobile objects including users whose locations are tracked in the space of the system deliver coordinates or other similar location information to the space processors for mapping user look vector and or collision detection.

Since the location coordinates transmitted to the system are provided in true coordinates this information must be translated into system relative coordinates x y z by a location translation agent using one of the following formulas. In these formulas the true coordinates of the point of origin are A B C where A and B represent latitude and longitude respectively in radians and C represents altitude in meters. The true coordinates of the point of location transmitted to the system are D E F where D and E represent latitude and longitude respectively in radians and F represents altitude in meters.

Thus for any given GPS point the corresponding point in the system space is cos sin cos sin sin sin where r is the radius at a given geodesic latitude 

In this example the length of the arc segments assuming that these arcs are a straight line between the respective points A B C and D E F along the latitude and longitude are the values for x and y in the system space respectively. Using the formula for calculating the length of an arc segment with r being the radius of the sphere earth and being the angle formed by the relevant points in the circle latitude or longitude at the center of the sphere this approach gives us the following formulas for x and y in the space where r is the average of the equator and polar radii

Thus the internal model of the system space is mapped loaded with an initial layout list of objects and locations of the physical context to be modeled this space may be maintained with the addition removal or update of objects.

Objects within the system may be organized in order to improve the searching process especially for a system having a large number of passive objects. This structure has complexity O logn where m is the number of children per node in the space partition tree hereinafter described and n is the number of items in the space. If the objects are not organized the collision searching algorithm is the na ve algorithm O n . For the fastest search capability the objects are organized into a search tree with m 2. This feature of the system allows the system to improve its performance in searching collision detection provided that passive objects are mainly stationary. As active objects in some embodiments carry their own processing units each one of them may use its own processing power to resolve collisions with a remote pointer.

A location service engine facilitates this mapping and tracking of objects of the system of the present invention. No specific location service engine is preferred or necessary any location service suitable for use with the application should suffice. However an example of a suitable location service is the Ekahau environment where movement paths are defined for Wi Fi enabled users and a 3 D representation of the model is built in the Ekahau environment using the Quake game engine. The location service runs preferably as one or more agents at a lower layer than the application interface as hereinafter described which in turn runs at a lower layer than the application service layer .

With objects mapped the system is able to support a user s tagging of a geographical feature building or object or a point in the system space or determine each user s look vector . Much of this technology is facilitated by collision detection agents where an event is tied to a specific structure or geographical feature pre defined in the system space . For any of these to occur the system must translate the location of the user as hereinabove described and receive the direction of a pointing device. If there is insufficient information or no pre programmed map of the surroundings or geographical features structures then the system will also retrieve the range measurement from the range finder to geo reference a location in the system space.

When a user desires to tag certain geographical features and or buildings in the user s environment a six degrees of freedom 6DoF vector associated with the user is determined by the geographic coordinates of the user and the orientation of the pointer worn carried by the user and if relevant the relative location of the pointer on the user for example her wrist finger head or shoulder . Thus the geographic coordinates of the user latitude longitude and altitude are received from the GPS input device and translated into the system s relative space and the orientation of the pointer azimuth pitch and roll are generated and transmitted by the pointer input device. The pointing by the user can be tracked over time so that for example an entire building may be outlined or the perimeter of an area defined. The relative location of the pointer in relation to the GPS device may be input by the user prior to beginning the mission may be pre programmed into the system with the identification of the object or may be programmed on the pointer device for transmission to the processor of the systems based upon the intended location thereof e.g. if it is designed to be worn on a helmet the device will be pre programmed to transmit a marker to the processing software that it is located on the head of the user . Knowing where the user is pointing from by GPS and pointer device information and pointing to by orientation of the pointer and in some cases using information from a range finder the processor is able to determine the 6DoF vector. Using the collision detection services as hereinafter described the user may precisely reference an object in the system space e.g. a window on the second floor of a building an object at ground level etc. 

Each user and some active objects of the present invention are mapped as a vector in the space of the present invention these vectors are used to facilitate standardize the calculations made by services like the collision detection service.

Collision detection in the present system are based upon and extended or customized from one of the several engines developed within the gaming development community. The general process for collision detection is to locate a vector in the space in the present system the 6DoF Vector projecting a ray through it and determining which objects polygons are intersected by the ray. However searching for the objects intersected by a ray in a large scale system is problematic even using improved algorithms. Therefore in order to reduce the impact of a large number of objects on a modeling effort the searching process may be supported by a Binary Space Partition BSP Tree. BSP is a tool widely used in the game industry to structure the space in a manner which improves the performance of rending and collision detection functions O log n and is therefore incorporated into the processors of the present invention for large scale systems although BSP may be avoided in low scale settings where a linear search O n may accomplish appropriate performance . Using BSP the main functions of the collision detection agent of the present invention are described as 

The algorithm to split the region into two sub regions box into sub boxes selects the larger side of the box and divides through that side. At least from a qualitative perspective by using this criterion the size of the smallest boxes the ones that actually contain the polygons will tend to be homogeneous in their dimensions thereby obtaining a more balanced collision process response times .

In 2 D collision detection wherein the calculations are already in the planes of the polygon the only condition to determine a collision is if the ray vector intersects at least one of the segments of the polygon or sequence of points. If so the collision detection engine returns the Id of the object s intersected by the ray.

In order for a functional game to be complete a physics engine able to determine semantics for the intersection of two objects must be used. Quake 2 Quake 3 Doom III well suited for indoor environments but comparatively poor for large outdoor spaces and Far Cry are engines suitable for use in embodiments of the present invention.

Finally to allow information to be overlayed on a user s visual screen a user look vector is generated by information from an EMC an IMU or a group of them on the user positioned in relation to the user s glasses goggles and transmitted to the processors of the present invention with the latitude longitude and altitude of the user received from the GPS device. This vector like the user pointing vector hereinabove described aggregates information regarding the azimuth pitch and roll of the user s head and the latitude longitude and altitude coordinates of the user. With this information the algorithms of the present invention are able to determine the field of view of the user and overlay information onto the viewing screen regarding tagged features structures or areas or geo specific information. The user s field of view is modeled as a rectangle aligned with the user s azimuth axis. Horizontal and vertical angles scopes are pre defined based upon the visual screen device configuration so that the range of analysis also pre defined by the system will provide geo tempo registered marks in the surrounding environment. Once the geo location of a tagged structure location or icon is retrieved from the system for display on the visual screen it is mapped to the user view using traditional 3 D to 2 D transformation scaling algorithms. Furthermore when critical tagged information is not in the field of view of the user but is in the range of analysis information may still be transmitted to the viewing screen by an icon and or text or other similar information.

With the various input devices translation recognition agents and mapping and collision agents as hereinabove described the system of the present invention further comprises a multimodal application interface a matrix database and a physical engine that receives and compares the input and generates signals to cause objects associated with the system of the present invention to act as requested by users through their voice and gesture commands as programmed into the matrix database . As with the agents each of these components may be centrally located or more preferably distributed among a plurality of system nodes including any computer device on the user.

The multimodal interface allows total flexibility for the developer and the final user to manipulate the natural actions to interact voice and gestures and enables an adaptable and customizable interface between a the user and the system during the input process and b the system and the user during the output process. This interface comprises a sensor input interface which is programmed to detect and recognize or disregard voice and or movement patterns of the audio and gesture input devices after processing by the translation recognition agents associated therewith as hereinabove described . Other input information that is received at the sensor interface and transmitted to the system map and in some instances the collision detection agent are pointer coordinates user look vectors and user location coordinates. This information is frequently tagged to gesture and voice input commands and data and mapped features and messages so that the commands and messages become location specific either in transmission action or storage.

Generally each multi modal interface is in idle mode until it receives and detects a valid input from an input device in which case it enters a triggering events mode. For this process to occur a hardware status alert controller is permanently monitoring the data gathered from the sensors. The controller is coupled with an N 1 syntax semantics matrix database which suffices to implement this feature. This database comprises rows corresponding to syntactical elements i.e. symbolic representation strings for utterances and the pointer s movement patters or any combination thereof while columns correspond to the semantics business logics command etc assigned for the syntactical element. Thereby the sensor interface is able to detect when a voice or gesture input corresponds to a message or command for the system for further processing.

Once the sensor interface receives and recognizes a voice or gesture message command the same is compared to a distributed or centralized database of the present invention. This database comprises an application specific language matrix defining the relationship between symbols input commands and meanings output commands which controls and allows the customization of the present invention. Thereby the system of the present invention is applicable in a variety of contexts e.g. theme parks battlefield situations and simulations transportation settings and social networking allowing variations between the objects commands and their meanings as appropriate for any particular application.

The input commands defined in the matrix include all possible objects designed and configured to send signals to the sensor interface including the users preferably defined as a distinct category of objects . Thereby all objects are distinguished in the database as well as the commands applicable to those objects. In most embodiments speech detected by the audio input device and movement patterns of the gesture input device are the primary input commands received by the sensor interface. Thus the language matrix is typically defined as an m n l language matrix with m the number of speech patterns the system is able to deal with plus the null element meaning no speech pattern n the number of pointer gesture movement patterns the system is able to deal with plus the null element meaning no speech pattern and l the number of meanings effects consequences for the system. Each cell of the language matrix may hold either a 0 or a 1 signaling that if a cell i j k also referred to as a tuple holds a 1 such combination of causes effect is a valid tuple for the system otherwise it is an invalid tuple and the system treats it a no meaning situation sending a corresponding message back to the application. This database may be located at a central command center for the system or distributed among several nodes including even a partial database at certain nodes the data thereof being the data necessary to process commands of a certain type or from a particular input object .

Once an utterance or gesture message is detected and recognized by the voice and or movement pattern time and space pattern an event at the sensor input interface the same is transmitted to and classified by the language matrix database using a physical engine. Incorporating modeling elements commonly used in the linguistics community creates a more robust reliable and flexible syntaxes to semantics transformation which in turn makes the interface more natural and intuitive for the end user. Thereby the physical engine and database operate as an object control of the system processing gesture and voice information received from the input devices or the recognition agents associated therewith and translating the same into system operational commands or transmission messages based upon the correspondence between the input and a pre defined command all as set forth in the database.

For instance if a user in a simulated battlefield pulls the trigger of his her rifle the command event in that case is the action of pulling the trigger. The system of the present invention translates this input into one or more meanings defined in the database matrix. The meanings may be a command to obtain additional information from other input devices. Thus when the user pulls the trigger and if additional information regarding the impact of the shot is desired the system of the present invention will seek the direction of the rifle using the location and pointing input and 6DoF vector as hereinabove described . This directional and location input is transmitted to the collision detection agent described above so that the system will determine whether it is feasible that an object would likely be hit by the projectile from the rifle. Depending on where the object is hit as determined by the collision agent the system of the present invention by means of a database would decide whether the object is killed injured or destroyed for example although various outputs could be programmed into the database matrix of the present invention . As one might expect if the user shot a grenade the meaning of the shot may be categorized completely differently than a rifle shot.

As discussed above in some embodiments the database may comprise a plurality of databases wherein certain preliminary or basic functions may be performed based upon user input without comparison to the primary database. These secondary databases and associated physical engines may be located on a user or at other distributed nodes of the system. A particular secondary database would be the N 1 database associated with the sensor input interface. When databases are distributed in this configuration the secondary databases and their associated physical engines may work to gather information as necessary to transmit the same to a principal database or to independently take action based upon the command received.

The physics associated with determining the effect of various user actions is provided by a physics service algorithms describing user object properties their interactions and consequences of such interactions incorporated with the present invention. The Physics Service may model for example radial explosions and detection of the objects affected by it. This service can be extended and customized to support the mechanics and dynamics of specific application domains based upon physics services available in the gaming and simulations world and a customizable cause effect service along with the 3 D mapping and modeling made through the systems of the Present invention. Thus with this physics service the system is able to mediate the interactions of mobile users in varied domains. In addition the systems of the present invention provide a mechanism for the application developers to define cause effect relationships so they can make the system aware of such cause effect relationships during runtime.

This matrical representation to process the inputs perceived by the sensor inputs of the present invention allows the system to be easily extended for other uses. This matrical representation is useful in modeling dynamic relationships between inputs and outputs i.e. between symbols and meanings which is a useful modeling feature for the development of consensus in populations of artificial users.

By means of the matricial representation where each tuple a b m represents a cause effect relationship the system s physical engine can easily resolve the appropriate action of the system given a pre defined input detected by it. The main purpose of this representation is to give enough functionality to the system s physical engine to articulate the information without necessarily being aware of the particular context of applicability. Thereby the system as a whole is designed to be flexible enough to perform in a variety of contexts language matrix where the semantics for a particular context is define by the application itself and not by the systems of the present invention.

As hereinabove described input data includes commands for input devices commands for object control messages for transmission to other users and user look vectors for receipt of messages or information. Once a command for an input device is detected after comparison to a database the physical engine generates an electronic signal representing the command corresponding to the input data in the database and transmits the same to the appropriate input device for receipt and action by the input device. Similarly upon any command for object control once identified in a database the physical engine generates an electronic signal representing the command and transmits the same to the specified object in some cases determined by the 6DoF vector of the pointing device using the collision detection agent to control the same at its control location. Finally if the input comprises a part of a message for transmission to other users as determined by comparison to a database the database physical engine alone or in combination with other databases gathers the remaining information for the message and transmits the message including modifications thereto in the form prescribed by the user or the database as hereinafter discussed to other users.

Thus for example the microphone may capture a voice command indicating that a gesture command is about to begin upon receipt translation and recognition of this voice command the physical engine using an associated secondary database would determine that the voice command indicates that a gesture command is about to commence and may send a signal to the gesture input device related software agent to cause the gesture input agent to review signals generated by the input device and determine whether a gesture command is being generated by the user. Similarly a voice command may be used to designate the end of the gesture command which may after comparison to the secondary database determine that the gesture is complete and available either for on board translation and processing or transmit the same to another node of the system for processing. If the input data is not a command for other objects of the system but rather information to be processed by the principal or another secondary database physical engine then the same is transmitted to that database.

Some of the input components may generate periodic signals and transmit the same to the mapped space or generate and transmit signals at the request of a physical engine database without input from the user. For example the GPS sensor may transmit a periodic signal regarding the coordinates of users for mapping within the space of the present invention or for processing by an agent of the present invention designed to track movements and actions of the users to determine whether global user movements actions might designate a retreat or indicate a threat or determine that a battle is occurring or otherwise that could be represented by a message of the present invention alternatively the GPS input device may generate these signals periodically and transmit them to the sensor interface . When pre programmed situations are detected by the system of the present invention output signals or messages may be modified in mode or new messages may be automatically transmitted to users based upon the situation detected by the user movement agent and the pre programmed response within the primary database or a secondary database of the system if a secondary database this database may be designed and limited to determining user movements and adjusting and or transmitting system messaging based upon certain movement patterns . In such an embodiment for example messages that would normally be transmitted as retrievable voice message may be transmitted by a tactile sensation a computer generated voice command or otherwise to minimize visual interruption allowing the user to continue monitoring his immediate surroundings. In other embodiments this battlefield assessment may occur when the sensor interface receives and recognizes a message or control input command a triggering event or a cue whereby the database matrix causes a local remote or centralized physical engine to send electronic signals seeking information from the generating user and or other users or objects to know if the scenario is routine or high threat for example.

The database physical engine preferably returns some messages or commands to the creating demanding user for confirmation prior to dissemination to other users or to control objects in the environment. This confirmation is helpful especially when geographical features or structures are being tagged to ensure that there is not an error in the collision model in the objects mapped in the system or a miscalibration in the input devices. If there is an error the user may re input or otherwise adjust the location he had intended to tag before sending the message to other users. Confirmation or rejection may be by voice gestures or other input device which is received translated and compared to the database s of the present invention for action. Thus the output interface and associated physical engines prepare and distribute commands and messages to be provided back to the user to object control processors and to some or all of the team members thereby allowing a user to control objects in his environment correct his input task mission plan describe the result of his command and or share awareness communication with the selected subset or all of the team members. This feedback mechanism allows users to control and correct the effectiveness of the communication process. This mechanism further enables the implementation of learning algorithms within the system to adapt to unique user gestures or accents.

Some or all of the messages or commands created by users voice gesture or pointing may be marked on a corresponding time scale with the geographical coordinates of the user. This information may be transmitted with the message to other databases physical engines to the recipients and or stored in a repository database.

Furthermore messages may be combined for transmission by the systems of the present invention. Thus voice text color icons visual tagged or tactile messages may be associated or linked with one another or with a tagged structure formation or location. These combinations are typically pre set in the database s however the language matrix of the database may be designed to allow a user to override these defaults prior to or during a mission. Furthermore the processes may be designed so that the defaults change based upon detected environmental conditions importance of the information and mission and other factors that can be programmed into the databases prior to consummation of the mission or based upon information received from some sensor inputs of the present invention during the mission. In some embodiments users of the system may pre set their own preferences regarding the delivery announcement and mode of messages they send and or receive.

The matrix of the user movement agent and or the primary database or any other agent of the system generating messages for other users may be pre programmed to determine the scope of dissemination of messages transmitted through the system in other words what users should receive the information based upon the situation the role of the participants and the appropriate geographic scope of the message. Further considerations regarding the scope of dissemination of the information can include the location where the information is generated the level of impact of the information for example strategic tactical operational or close friends acquaintances general public and the priority threat need level of the information. The determination of scope of messaging is preferably pre programmed into one or more agents or databases of the system and may comprise its own filtering agent database. Subject to override one or more databases of the system will further determine the redundancy in some cases dependent on the activities situation in which the receptors of the information are involved and the timing and persistence related to the data so that information is delivered quickly but does not persist remain associated with a geo registered location beyond an applicable period of time.

To provide system services in a transparent and flexible manner the system borrows the concept of shared space found in distributed shared memory systems Tanenbaum 1995 . The idea behind distributed shared memory systems is to provide every node participating in such distributed system the illusion of using a global memory while such memory is really the collection of the contributions made by each node. The present invention builds on this concept distinctly providing a single system application programming interface receiving application requests by users for system services and transmitting the same to objects agents responsible for locating and invoking locally or remotely the requested service.

In most embodiments the system architecture is an agent based system. In particular the functionality encapsulated in every lower level box presented in is implemented by a software agent as shown in . An agent based approach allows full modularity and functionality. Each service speech recognition location orientation etc. is managed by an agent. Because agents run independently of the application they can provide services to several applications simultaneously if required. Furthermore as application independent units agents may be designed and configured to detect environmental conditions which could affect the dynamics of the applications such as a monitoring capability about network reliability to decide which connection oriented or connectionless message delivery services can be best facilitated by the agents. This thereby enables self configuration of the systems of the present invention. In addition agents are conceptually hardware independent so they may reside on servers desktop PCs or handheld devices. Thus the agent based architecture allows the system to be deployed in a centralized distributed or preferably mixed computational platform.

The interaction among the major components of the system of the present invention can be described through the example shown in which is a broad introduction to the sequence of actions triggered after an event has been detected by the system interface. In general terms there are two kinds of events 1 the user produces an utterance and or 2 the user or the object generates a movement of the pointer device. Thus a natural interface exploiting verbal and gestural communications is created ad hoc for environments where users cannot spend time writing text messages. command to generate an event may be composed of only a verbal instruction only a movement of the pointer or both depending only on the relationship defined between such inputs into the system and the meanings those inputs could have. For example if a user says turn off the light in a room that the application is aware because the system database has been informed that there is only one light switch the system will not inquire or determine where the pointer is pointing. On the other hand if the user says Turn off that light and this utterance is produced by an user located in a room with several light switches then the system under instructions from the databases will inquire and determine the direction of the pointer and using the collision detection agent as hereinabove described will determine what object the user is pointing at and will proceed with generating the requested effect on the intended object by transmission of a digital signal representing the spoken command to the processing control center for such subject .

One or more butler software agents coordinate and mediate the interaction between the services and the other agents transmitting information and commands to and among the input interface various agents of the system and the output interface. For example function calls made from at the application interface and or a database may be mapped to the actual local and or remote agents providing the services associated with such function calls e.g. location orientation logging etc. . Thereby the location of the agents services local or remote is totally transparent to the application. The butler may receive one message with one or a list of candidate intersecting objects or several messages with one or several objects in each message. The butler will tie these messages together and send them back bundled to the system application interface. A plurality of butlers may be implemented into the design of the system of the present invention. When a butler agent is an independent agent always local to the application it allows the butler to deal with information relevant to the independent application such as for example monitoring network reliability during some period of time before the application is launched so that the butler may determine whether it is better for the agents objects and systems of the present invention to communicate in a connection oriented manner or using connectionless communication protocols TCP vs UDP . This design is preferred over butlers designed as libraries loaded with the application although the latter is a simpler programming endeavor. Either design is transparent to the system as a whole.

A memory channel can be used for communication between the butler and other agents running on the same device for remote agents sockets may be used. Preferably this adaptation is made automatically by the butler agent. When the butler or agents are located on PDAs Windows Mobile 2003 and above OpenNETCF Application Blocks are used to communicate processes applications through memory in the case of PCs P Invoke calls to c memory mapped files functionality is used to communicate between butlers and agents.

As hereinabove described the databases of the present invention include meanings for each input gesture or auditory command or combination thereof. The physical engine associated with the database then transmits commands through the sensors control to the output devices used in association with the present invention to cause the device to take action in accordance with the meaning in the database. Output devices include auditory listening devices hereinabove described with headset microphones visual viewing screens and tactile devices.

Auditory outputs may include a message from another user either directly transmitted to users or made available upon prompting from a user or a computer generated auditory message pre stored sounds verbal descriptions and speech generated from text based upon directions from a user or the detection by agents of the system of general environmental conditions such as an urgent retreat from an area detected by the agents of the present invention .

Visual outputs sent to a user viewing screen include visual icons that may indicate a message or other information is available or transmit a message such as danger by text or symbol. Visual outputs also include maps that may show the location of other team members determined by GPS coordinates input into the system and mapped to the space of the system and tagged geographical features. Visual outputs available to a user are typically based upon their user look vector determined as hereinabove described using a 6DoF vector. Thus inputs from the input devices gathering such information may define the visual output of the system to a particular user.

As noted previously the systems of the present invention support mobile users conducting activities in dynamic environments within a geographical region such as battlefields and first response operations and everyday life social networking. A manner in which the system does this is illustrated in . shows a first responder s view on how geo tempo registered messages could be represented as a layer on the see through display. This display is synchronized with the observed real world. See through visual displays able to exhibit such capability are currently available or under development such as wearable displays from microvision.com and bae.com . In this display several buildings are marked by a transmitting user as potentially dangerous one of which has a voice message associated with it another having a text message. Furthermore a map message is incorporated into the top of the view. An iconic message is also incorporated showing that there is a structure a defined distance to the right of the field of view of the user which structure has a message appended to it. Once this latter structure is in the field of view of the user the structure will be marked and the message will be displayed.

Tactile devices used for feedback include commercial off the shelf COTS capability like vibration included in cell phone technology and technology primarily developed for hearing and visually impaired persons like skin vibration devices which transform sound waves into vibrations that can be felt by the skin. These capabilities may be incorporated into one of the other components of the present invention such as any hand held pointing device or a PDA or may be an independent component.

A user s output components may be controlled by the user during operation of the system for example the darkness of the screen may be changed based upon user command or upon other input information from additional input objects e.g. light sensors located on a user or otherwise indicating the light in the environment of a user or generally of all users at a particular time.

Other components of the user s output may further be controlled and customized by the user pre mission or during a mission by sending control signals to the sensors input of the present invention. For example if a user desires that all auditory messages play immediately regardless of the situation and not be otherwise retrievable the same may be programmed by the user into the database by a series of commands wherein the database is programmed to activate such a preference upon receipt of such commands . Thereby once the message reaches a team member by direct on line communication or because the geographical location of the message is within or near their field of view the multi modal module will set the message delivery mode or content in whatever form the user s preferences or the system s override dictates.

By the foregoing the sensors interface of the present invention receives messages from the sensor hardware and transmits processed information to multi modal output devices including see through displays tactile effectors and headset earphones. Further a network interface is provided to facilitate the receipt and transmission of information to and from the sensors interface the interface being integrated with communications hardware including radios ad hoc networking multicast broadcast unicast message delivery and distributed systems.

To better understand the present invention embodiments and processing of messages using the present invention are set forth in the following examples. In one example if a primary or secondary database is programmed to receive and translate gestures after the vocal utterance into a microphone by a user of a term such as start the user intending to generate a gesture message would first utter the term start this term would be received by the microphone translated into a digital signal and further translated into data by the speech recognition software. The translated data is received at the multi modal interface sensor interface and compared to a database using a processor. In the proposed embodiment the database would be pre populated so that the term start would correspond to the start of a gesture input and upon receipt of the translated input data for the term start the physical engine associated with the database would generate a digital signal and communicate the same to for example a the gesture input device to begin generating digital signals representing gestures by movement of the user s gesture input device b to the receiver of the gesture recognition translation software to begin translating signals received from the user s gesture input device and communicate the same to the multi modal sensor interface or c to the multi modal sensor interface or an agent designed and configured to receive and process inputs from the gesture input device by translating the signals if necessary and comparing the same to an applicable database to determine the user s message and take any corresponding action. Until the system transmits a signal to begin detecting receiving translating or recognizing gesture input signals or translated signals some or all of these agents or devices may be idle ignoring signals that may be transmitted thereto. However because the system of the present invention is intended to receive messages from users as utterances or gestures or both the processors and agents preferably receive and translate all audio and gesture signals from input devices if the utterance gesture translates into pre defined data within the translation software or the sensor interface then the same translated data is forwarded to a database agent and compared to the database to determine whether a command has been entered and if so what action must be taken by the system.

Similarly if a user desires to transmit a voice message he may inform the systems of the present invention to record and transmit the message by another term such as speak. In this example the user intending to generate a voice message would utter the term speak this term would be received by the microphone and translated into a digital signal by the microphone s driver further translated into data by the speech recognition software with the input data then being transmitted to the multi modal sensor interface and compared to a database. The database would be pre populated so that the term speak would correspond to the start of a voice message and upon receipt of the input data for the term speak the systems of the present invention would generate a digital signal to begin receiving a voice message communicated by the user by means of the microphone. This digital signal would be transmitted to the microphone the audio translation software and or the processor of the database to begin recording receiving and or processing for transmission the voice message.

In some embodiments a user may desire to control a mobile or inanimate object such as a light in a room. In this case the user may produce an utterance or generate a movement of the pointer device as in the examples hereinbefore described commands may be composed of only a verbal instruction only a movement of the pointer and or the gesture input device or any combination thereof depending only on the relationship defined between such inputs into the systems of the present invention and the meanings those inputs could have as defined in the databases . In the case of object control the location of the user within the system space is determined from the GPS coordinates of the user transmitted to the system of the present invention by means of the GPS input device using collision detection algorithms as hereinabove described. For example if a user says turn off the light in a room that only has one light switch as reflected in the programming of the database the system of the present invention will determine what room the user is in and upon determining that there is only one light switch will not go further to receive or process information regarding the pointer orientation or if it does it will discard this information . On the other hand if the user says turn off that light and this utterance is produced by a user located in a room with several light switches controllable by the systems of the present invention then the system of the present invention will determine the location of the user and the applicable light switches will retrieve and or receive the information about the 6DoF vector of the pointer as hereinabove described and using collision detection algorithms will determine the light the user has requested be turned off. The system will then transmit a digital signal to the object controlling the light switch to turn off the corresponding light.

Similar to the starting of gesture or voice messages the user may designate the conclusion of a message with another voice or gesture command which is received by the input device transmitted to the processors and compared to the pre populated database so that a digital signal may be generated and transmitted to the input device or another device within the communications chain to designate that the message is complete and no additional utterances or gestures need be received or processed.

Another component of the multi modal application interface is the message manager. This module agent is responsible for the logging and distribution of messages derived and generated through the interaction by or among users mediated by the interface and the systems and agents of the present invention. This module may use any communications or radio system available to send receive messages to and from other team members.

This agent server receives a copy of every message transmitted through and facilitated by the system and monitors the processes triggered by any object user flows. For this purpose the message server agent preferably utilizes threshold times for each step of each process which are defined parametrically. Furthermore this message server agent allows the system to keep record of every action taken for each event occurring in the natural context of the object users and thereby the system will be able to reproduce the dynamics generated through the systems of the present invention. This feature allows the applications running on top of the systems of the present invention to be useful for training and or educational purposes by reconstructing the application runs .

Voice messages are typically stored in the databases of the present invention with a visual tactile or auditory announcement being transmitted to other users with a link to the voice message. However a voice message could be transmitted to users for immediate play or for storage in any media storage device on their person. To retrieve these voice messages the user may give another verbal command or a gesture which will be transcribed transmitted to the processor and compared in a database to a command to playback the voice message a digital signal is then generated and sent to the media storage device to cause the message to play.

