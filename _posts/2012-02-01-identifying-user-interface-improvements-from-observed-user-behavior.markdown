---

title: Identifying user interface improvements from observed user behavior
abstract: Disclosed are various embodiments for identifying user interface improvements from observed user behavior. A use case description is obtained from a developer of an application. Usability metrics are collected that observe interaction of a plurality of users with a user interface of the application. The usability metrics are analyzed in conjunction with the use case description to identify at least one aspect of the user interface for improvement. The aspect includes a page or an actionable element. Information is provided to the developer about the at least one aspect of the user interface identified for improvement.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08924942&OS=08924942&RS=08924942
owner: Amazon Technologies, Inc.
number: 08924942
owner_city: Seattle
owner_country: US
publication_date: 20120201
---
Developers may offer applications for purchase or download through an electronic application marketplace. Usability tests may reveal a user s dissatisfaction with the user interface of the application and or inefficiencies in using the application to complete a task.

The various embodiments described herein relate to programmatically identifying improvements to the user interface of an application based on usability metrics collected from users interacting with the application and on use case descriptions provided by application developers. The use case description describes the desired behavior contemplated by the developer while the usability metrics show actual user behavior. An analytics engine examines the collected usability metric and determines whether user behavior corresponds to the developer s expectations. If the analytics engine finds a discrepancy between actual and expected behaviors the analytics engine identifies a particular aspect of the user interface that is linked to the discrepancy in behavior and designates the identified aspect as an area for improvement. The identified improvements are communicated to the application developer and the developer can then use this information as feedback to modify the application.

In one example scenario the application is an electronic commerce marketplace and usability metrics are collected which show the navigation paths which various users take through the user interface in order to purchase an item in the electronic catalog. The developer provides a use case description which specifies purchase of any item as the goal and further specifies a navigation path top sellers list to item detail to purchase to achieve the goal. Usability metrics such as navigation path and task completion time metrics are collected from multiple users running the application. These collected usability metrics show that while users are reaching the item detail page from the top sellers list page these users are then spending time in the review area of the detail page before clicking on the Buy button on the detail page. More specifically in this example the metrics show that users are spending time locating and then reading negative reviews before buying. Under these conditions the analytics engine may identify the review section of the item detail page as aspect of the user interface to be improved. The analytics engine may further recommend to the developer a change to the layout of the item detail page so that several negative reviews are presented at the top of the review pane thus reducing the amount of time and effort needed to read a negative review from the detail page. The analytics engine may also recommend to the developer a change to the layout of the review page to include a Buy button thus eliminating the need to return to the item detail page to make a purchase.

While the particular aspect identified in this particular example was placement of user interface components on a page the disclosure applies to other user interface aspects as well. Such aspects include but are not limited to navigation paths through the user interface additional pages or paths presence of particular user interface components on a page and presence of particular components in the application. In this manner developers can be notified of various deficiencies in the user interface such as unused features dead end paths and error prone paths. In the following discussion a general description of the system and its components is provided followed by a discussion of the operation of the same.

With reference to shown is a networked environment according to various embodiments. The networked environment includes one or more computing devices in data communication with one or more devices and with one or more client devices by way of a network . The network includes for example the Internet intranets extranets wide area networks WANs local area networks LANs wired networks wireless networks or other suitable networks or any combination of two or more such networks.

A computing device may comprise for example a server computer or any other system providing computing capability. Alternatively a plurality of computing devices may be employed that are arranged for example in one or more server banks or computer banks or other arrangements. A plurality of computing devices together may comprise for example a cloud computing resource a grid computing resource and or any other distributed computing arrangement. Such computing devices may be located in a single installation or may be distributed among many different geographical locations. For purposes of convenience the computing device is referred to herein in the singular. Even though the computing device is referred to in the singular it is understood that a plurality of computing devices may be employed in various arrangements.

Various applications and or other functionality may be executed in the computing device according to various embodiments. The components executed on the computing device include for example an electronic commerce service a developer portal a metric collection service and an analytics engine . The components executed on the computing device may also include other applications services processes systems engines or functionality not discussed in detail herein. These components may communicate with each other using various mechanisms including but not limited to any type of middleware framework. Although the electronic commerce service the developer portal the metric collection service and the analytics engine are shown as separate components in the example embodiment of in other embodiments these components may be integrated as should be appreciated.

Also various data is stored in a data store that is accessible to the computing device . The data store may be representative of a plurality of data stores as can be appreciated. The data stored in the data store for example is associated with the operation of the various applications and or functional entities of the computing device as described herein. The data stored in the data store includes for example an application marketplace applications usability metrics metric generation code and use case descriptions .

The electronic commerce service is executed in order to facilitate the online purchase of applications by customers from one or more application marketplaces over the network . The electronic commerce service also performs various backend functions associated with the online presence of an application marketplace in order to facilitate the online purchase of applications as will be described. For example the electronic commerce service may generate network pages such as for example web pages and or other types of network content that are provided to devices for the purposes of promoting and selecting applications for purchase rental download lease or any other forms of consumption. To this end the electronic commerce service may include a commercially available hypertext transfer protocol HTTP server such as for example Apache HTTP Server Apache Tomcat Microsoft Internet Information Services IIS and so on.

An application marketplace corresponds to a network site that offers a multitude of applications for sale download purchase lease etc. by users. The applications correspond to various applications that may be executed in the client device . The applications are offered through the application marketplace by multiple developers. In one embodiment developers may self register for accounts with the application marketplace and self manage various offerings of applications in the application marketplace by way of the developer portal .

The developer portal is configured to obtain applications from client devices and to store the applications in the data store of the computing device for access by the application marketplaces . The developer portal may include a commercially available hypertext transfer protocol HTTP server such as for example Apache HTTP Server Apache Tomcat Microsoft Internet Information Services IIS and so on.

Before making an application available through an application marketplace the developer portal is configured to instrument an application with metric generation code . To this end the developer portal identifies locations in the application where usability metrics are to be collected and modifies the application to include code that generates and then reports the usability metrics to the metric collection service . The metric collection service is configured to collect and store these usability metrics .

As should be appreciated a usability metric measures some aspect of a user s interaction with the user interface of an application . The analytics engine is configured to process the usability metrics and potentially other data to generate information for the developer regarding possible improvements to the user interface of the application . This analysis process will be described in further detail below. The developer portal may be configured to present these possible improvements to the developer. Such information may be presented to the developer via network pages email messages mobile applications text messages and or other reporting media.

Having discussed the computing device the client device and the client device will now be considered. The client device and the client device are each representative of a plurality of client devices that may be coupled to the network . The client device and the client device may comprise for example a processor based system such as a computer system. Such a computer system may be embodied in the form of a desktop computer a laptop computer a personal digital assistant a cellular telephone a smartphone a set top box a television a music player a video player a media player a web pad a tablet computer system a game console an electronic book reader or other devices with like capability. The client device may include a display while the client device may include a display . The display may comprise for example one or more devices such as cathode ray tubes CRTs liquid crystal display LCD screens gas plasma based flat panel displays LCD projectors or other types of display devices etc.

The client device may be configured to execute various applications such as a browser . The browser may be executed in the client device for example to access and render network pages such as web pages or other network content served up by the computing device and or other servers thereby generating a rendered content page on a display of the client device . The client device may also be configured to execute an application obtained from the computing device where the application has been modified to include metric generation code for generating and reporting usability metrics to the metric collection service . The user interface of the application is rendered on the display . The client device may be configured to execute applications beyond the browser such as for example email applications messaging applications and or other applications.

The client device may be configured to execute various applications such as a browser . The developer may use the browser to interact with the developer portal executing in the computing device . The client device may be configured to execute applications beyond the browser such as an integrated development environment developer tools and or other applications.

Next a general description of the operation of the various components of the networked environment is provided. To begin a developer at a client device creates an application . The application may be designed for execution in a general purpose computing device or in a specialized device such as for example a smartphone a video game console a handheld game device an arcade game device etc. A non limiting list of application types includes games photo manipulation applications video editing applications and office productivity applications.

The developer transfers the application to the computing device by way of the developer portal . In some embodiments the developer specifies during the transfer process whether the application is to be modified to collect usability metrics . For instance some applications may operate with private and or confidential information that the developer may not want modified due to security concerns. In other embodiments the application may be modified regardless of any developer input. It is understood that the applications may be stored at the computing device as binary files and the source code to the applications may be unavailable to the computing device .

As part of the transfer process the developer may also specify those locations in the application where usability metrics are to be generated. These locations may be specified for example by way of an application programming interface API . Additionally the developer may specify what data is to be included in a particular usability metric . A non limiting list of types of usability metrics include measures of the user s performance on a task carried out using the application the user s engagement with various aspects of the application the user s satisfaction with various aspects of the application the user s preferences with regard to various aspects of the application the user s recall of various aspects of the application the user s recognition of various aspects of the application and the user s physiological reactions while interacting with the application . A non limiting list of example usability metrics includes a task completion time a task success rate a task error rate a task error recovery rate and a navigation path. A usability metric may include various data about the current interaction between the user and the application such as the current page the actionable element with which the user most recently interacted the current execution time and the current location in the application .

The developer also interacts with the developer portal to provide one or more use case descriptions for the application . As described herein usability metrics describe actual interactions of users with the application in various scenarios. In contrast a use case description refers to a specific set of user interactions with application contemplated by the developer. In other words a use case description describes a particular use case envisioned by the developer while usability metrics describe what actually happened. In some embodiments a use case description includes a goal or task to be completed by the user. Discrepancies between what a developer expects to happen and what actually happened can indicate areas for improvement in the application user interface . The analytics engine uses the use case descriptions and the usability metrics to identify such areas for improvement. As one example if the developer expects the user to take a particular path through the pages of the application user interface in order to complete a task yet observed interactions show that most users take a different path the developer may consider restructuring the navigation through the application user interface .

After receiving from the developer an application and an indication that application is to be modified with metric generation code the developer portal instruments the application with the metric generation code for example using an instrumentation engine. Once instrumented a modified application reports the usability metrics back to the metric collection service that is executed in the computing device .

The developer portal stores the instrumented application in the data store and makes the application available to users in an application marketplace . Users may access the application marketplace and place an order for the application through the electronic commerce service . Upon consummating the order the electronic commerce service facilitates a transfer of the modified application to the client device . In various embodiments the application may be transferred in digital form over the network . In other embodiments the application may be physically delivered to the user in the form of a computer readable medium.

A user installs the modified application on one or more devices which may or may not be the same client device through which the application was obtained. Other users may also obtain and install the same application through the application marketplace . In this manner various users execute instances of the application in the devices . Various versions of the same application may be available through the application marketplace . Application developers can then apply use cases to determine the usability of one version as compared to another version. User may not be aware of the differences between versions or even that different versions exist.

As described above the instances of the modified application are configured to generate and report various usability metrics to the metric collection service through the metric generation code . The usability metrics may be reported to the metric collection service in an encrypted format or by way of an encrypted channel over the network . The metric collection service may be configured to track and store the usability metrics at the user level session level or device level. The metric collection service may be configured to aggregate the usability metrics at any of these levels.

The analytics engine analyzes the collected usability metrics in conjunction with use case descriptions provided by the developer via the developer portal . This analysis may be performed periodically after a predefined number of usability metrics are collected upon request by a developer or may be triggered by some other action. During the analysis the analytics engine identifies one or more differences or discrepancies between a developer contemplated use case description and actual user behavior as captured by usability metrics . The analytics engine then identifies which aspect or aspects of the application user interface is responsible for these differences and provides the identified user interface aspect s to the developer as a possible improvement to the application . The analysis process will be described in further detail below.

The user interface aspects identified by the analytics engine may include aspects related to pages of the interface and aspects related to the flow of the interface between these pages. Within a particular page the analytics engine may identify particular components which can be improved for example improvements to the visual design of the component or the page or improvements to the placement of the components on the page. As a user interacts with the user interface the user progresses or flows from one page to another by interacting with particular user interface elements. Some of these may be actionable elements while others may be inactionable elements.

Turning now to shown is a flowchart that provides one example of the operation of portion s of the developer portal and the metric collection service according to various embodiments. It is understood that the flowchart of provides merely an example of the many different types of functional arrangements that may be employed to implement the operation of portion s of the analytics engine as described herein. As an alternative the flowchart of may be viewed as depicting an example of steps of a method implemented in the computing device according to one or more embodiments.

Beginning at box the developer portal obtains from a developer and through the network an application to be offered through the application marketplace . The developer portal also obtains instructions regarding instrumentation of the application with code for generating and reporting usability metrics . As noted above the instrumentation instructions may indicate particular locations within the application for instrumenting as well as specifying what data is to be included in a particular usability metric .

Next at box the developer portal instruments the application with code to generate and report usability metrics . The reporting may happen as the application generates each usability metric after a batch of usability metrics is generated periodically relative to an event or at another time. In one embodiment the application may delay reporting based on the user client device system load network load and or network status.

To instrument the application the developer portal may profile the code which makes up the application . The profiling may involve parsing the application to search for stored signatures bytecode profiles source code profiles predefined file offsets etc. The developer portal may automatically insert or inject code into the application that corresponds to portions of the metric generation code for the purpose of generating and reporting usability metrics . In some embodiments the developer may flag certain code in the application or make certain library and or function calls in order to indicate where the metric generation code is to be inserted. That is the instrumentation instructions from the developer may be provided as part of the application itself.

In some embodiments developer portal may modify the application such that the metric generation code generates a specific usability metric referred to herein as a navigation path metric any time a different page of the user interface is depicted on the client device . For example suppose the user interface includes a start page a search results page a browse page an item detail page and a transaction page. In that case an application session in which the user finds a product through search would report the start page the search results page and the item detail page. Similarly an application session in which the user buys a product after browsing the electronic catalog would report the home page one or more browse pages the product detail page and one or more transaction pages.

In some embodiments developer portal may modify the application such that the metric generation code generates a navigation path metric any time an actionable element of the application user interface appears on a page of the user interface . For example suppose the start page of the user interface includes a search box a search button a browse tree a list of top selling items and a list of recently viewed items. In that case when the start page is depicted on the client device the metric generation code generates a navigation path metric for the search box search button each of the nodes in the browse tree each of the items in the top seller list and each of the items in the recently viewed list.

In some embodiments developer portal may modify the application such that the metric generation code generates a navigation path metric any time the user selects or otherwise engages an actionable element of the application user interface . To continue with the earlier example of an application session if user types a phrase into search box and then clicks the search button the metric generation code generates a navigation path metric indicating interaction with the search box and a navigation path metric indicating interaction with the search button.

In some embodiments instead of reporting navigation path metrics to the metric collection service the metric generation code may instead use the navigation path data to derive other metrics such as task completion time task error rate task completion rate or other suitable metrics. Moreover the choice of communicating navigation path metrics to the computing device or processing them locally at the client device may be made at run time depending on the relative capabilities of the server and the client and or the characteristics of the network .

Next at box the developer portal obtains from the developer a use case description for the application . The use case description includes a description of a task which the user completes by interacting with the user interface of the application . One example of a task might be purchasing a product from an electronic catalog. Another example of a task might be using a product from an electronic catalog for a trial period. The use case description also includes a goal for the task. The goal may have a single term for example task completion in ninety seconds. Alternatively the goal may be compound for example task completion in ninety seconds by 80 of the users who started the task. The use case description may also include a specific path to complete the task.

The use case description may specify one or more usability metrics used to determine whether or not the goal is met. A non limiting list of usability metrics to evaluate a goal includes a task completion time a task success rate a conversion rate and a task error rate. A goal may include more than one usability metric for example 80 of users complete the task in three minutes and with no more than one task error.

At box the electronic commerce service inserts the instrumented application into one or more application marketplaces where users can purchase lease or otherwise consume the application . As discussed above after obtaining the application through the application marketplace a user installs and executes the application on a client device . In some embodiments the instrumented application is selectively offered to users in the application marketplaces and the developer is given some control of the selection. For example the developer may be able to specify through the developer portal one or more classes of users and or classes of client devices.

Next at box the metric collection service collects usability metrics from the instrumented instances of a particular application . The collection may be driven by the metric collection service or by the metric generation code in the application . That is the metric collection service may request the usability metrics from the application or the application may instead push the usability metrics to the metric collection service . If the metric collection service drives the collection the usability metrics may perform the collection periodically or upon request from the analytics engine . Although depicts the process as being complete after box it should be appreciated that the collection functionality of box is ongoing.

The metric collection service may also aggregate usability metrics . For example if one of the usability metrics indicates that a user completed a particular task within the application the metric collection service may keep a count of the total number of users that complete the task. The metric collection service may also perform statistical calculations on usability metrics . For example the metric collection service may use a task time metric from individual users or sessions to derive an average time to complete a task. The metric collection service may also use one metric to derive another. For example the metric collection service may use the number of users that complete a task along with the number of users that attempt the task to compute a task success rate. As another example the metric collection service may compute a conversion rate from the number of users who viewed a particular item in a list and the number of users who reached the detail page for that particular item.

Moving on to shown is a flowchart that provides one example of the operation of portion s of the analytics engine according to various embodiments. It is understood that the flowchart of provides merely an example of the many different types of functional arrangements that may be employed to implement the operation of portion s of the analytics engine as described herein. As an alternative the flowchart of may be viewed as depicting an example of steps of a method implemented in the computing device according to one or more embodiments.

The process of performs analysis on usability metrics for a particular application and for a particular use case. The particular application and use case may be identified in analysis request presented to the analytics engine by for example the developer portal . Beginning at box the analytics engine examines the collected usability metrics and identifies those metrics which have been reported by the particular application for example from an application identifier stored in or in association with the usability metric . The functionality of box may be appropriate in embodiments where the metric collection service handles collection for multiple applications .

Next at box the analytics engine obtains the goal for the identified use case from the use case description . The analytics engine identifies which of the application s usability metrics are relevant for the goal. In some embodiments the goals in a use case description are limited to a set of predefined types and the analytics engine performs a mapping from goal type to a set of relevant usability metrics . In some embodiments the use case description may itself specify one or more usability metrics used to evaluate the completion of the goal.

At box the analytics engine evaluates the goal in light of the identified usability metrics . If the metrics indicate that the developer s goal has been achieved then no further processing is necessary for the requested use case and the processing of is complete. On the other hand if the evaluation at box indicates a discrepancy between the use case goal and user behavior as captured in the identified usability metrics then processing continues at box . In determining a discrepancy between a goal and a particular usability metric the analytics engine may perform aggregation or a statistical calculation with respect to the usability metric . For example if the collected usability metrics included 100 instances of a task completion time the analytics engine may compute the average of the task completion time and compare the average to the goal.

Next at box the analytics engine extracts references or tags from the identified usability metrics . Each tag or reference is associated with a particular aspect or element of the user interface . This association between user interface element and metric was embedded in the application in some manner so that the metric generation code includes the tag along with the metric that is reported to the metric collection service . For example the developer may embed within the application function calls to an application programming interface provided by the developer portal where each of these function calls link a user interface element to a reported metric. When the application executes in the client device these API calls inform the developer portal of the mapping between user interface element and metrics. The developer portal then stores these mappings in the data store .

At box the analytics engine maps the tag to a user interface element using the stored mapping mentioned above. Finally at box the analytics engine designates the user interface element as an aspect of the user interface for possible improvement and provides the developer with information about the aspect designated for improvement. For example the analytics engine may provide the developer with a uniform resource locator URL or an image or a text description for a page or user interface element to be improved. The analytics engine may also provide the developer with an identifier for the use case description and or the application . The analytics engine may further provide the developer with information about the users for which usability metrics were collected. The improvement information communicated to the developer may take various forms such as a list a table a report or a chart. The information may be communicated to the developer through the developer portal upon log on to the developer s account. After providing the developer with information about possible improvements the process of is then complete.

Several non limiting examples of identified improvements will now be discussed. As one example of an identified improvement the analytics engine may determine from a low conversion score in the usability metrics that users are failing to reach the item detail page for items shown on the very top or the very bottom of a top sellers page in the user interface . Under such conditions the analytics engine might identify the layout of the top sellers page as an aspect of the user interface to be improved.

As another example of an identified improvement the analytics engine may determine from the usability metrics that 40 of users complete a task associated with the developer s goal by taking a primary navigation path through the user interface and 60 of users complete the task by taking a secondary navigation path. Under such conditions the analytics engine might identify the secondary navigation path as an aspect of the user interface to be improved. The analytics engine might further indicate to the developer that improving this aspect would improve the efficiency of the application .

As yet another example of an identified improvement the analytics engine may determine that the usability metrics show 40 of users completing a task associated with the developer s goal by taking a primary navigation path through the user interface and 60 of users completing the task by taking a secondary navigation path. The analytics engine might then identify the secondary navigation path as an aspect of the user interface to be improved. The analytics engine might further indicate to the developer that improving this aspect would improve the efficiency of the application .

In some embodiments the analytics engine offers specific recommendations to improve the user interface in addition to identifying aspects of the user interface for improvement. For example the analytics engine may recommend an alternate layout for a page of the user interface or an alternate navigation path to complete a task or goal. In other embodiments the specific suggestions for improvement are provided instead of a more general recommendation of aspects to be improved.

In some embodiments the analytics engine may identify multiple aspects for improvement and may rank or prioritize the aspects. In determining a discrepancy between the goal and the user behavior captured by the usability metrics the analytics engine may generate a score for the desired behavior expressed in the use case description and another score for the observed user behavior. The analytics engine may then use differences between these scores as criteria for ranking or prioritizing among multiple improvements.

In some embodiments the analytics engine may compare user behavior as captured through usability metrics against a goal associated with a class of applications rather than a particular developer goal. For example the analytics engine may compare performance of a developer s application to a performance of a representative group of electronic commerce applications. The analytics engine may then communicate to the developer how well the developer s application performed relative to the representative group e.g. the top 25 percentile.

After receiving information about possible improvements to the user interface of the application the developer may provide another version of the application to be made available to users through the application marketplace . In some embodiments multiple versions of the application are stored by the computing device and analytics engine is notified of new versions and or updates to applications . The analytics engine may then determine which version is most appropriate for a particular user and may communicate this information to the electronic commerce service so that the application marketplace can offer the appropriate version. The determination by the analytics engine as to which version is most appropriate may be made for example by using behavior metrics to classify users into profiles. Such techniques are described in the commonly assigned patent application entitled Behavioral Characterization Ser. No. 13 276 370 filed Oct. 19 2011 which is hereby incorporated herein in its entirety. In some embodiments the determination made by the analytics engine considers which version is optimized for the user s behavior pattern. For example two versions of the application may differ in the layout used on a particular page of the user interface where one layout is optimized for a small screen such as that used by a mobile device and another layout is optimized for a larger screen such as that used by a desktop computer.

Described above are various types of improvements which may be recommended by the analytics engine . One specific example of such an improvement will now be discussed the placement of an actionable button on the page. With reference now to shown are drawings of example user interface screens and which differ in the placement of a Buy Now button. The screen illustrated in includes two portions a visible portion and a hidden portion where the hidden portion becomes visible when the user scrolls down to the bottom of the screen. Screen represents a first user interface in which the Buy Now button is placed at the bottom of the screen in the hidden portion . In one scenario the analytics engine determines from the collected usability metrics that users took longer than expected to discover and act on the Buy Now button . The analytics engine may then recommend as an improvement moving the button to the top of the screen so that the button is rendered in the visible portion of the screen and is thus immediately visible to the user viewing the page. This improvement can be seen on screen in where the Buy Now button appears at the top of the screen

In these examples the screens correspond to network pages rendered by a browser application executing on a client device in the networked environment according to various embodiments of the present disclosure. However in other embodiments a user interface is rendered directly by a client application outside of a browser context.

With reference now to shown is a schematic block diagram of the according to an embodiment of the present disclosure. The computing device includes at least one processor circuit for example having a processor and a memory both of which are coupled to a local interface . To this end the computing device may comprise for example at least one server computer or like device. The local interface may comprise for example a data bus with an accompanying address control bus or other bus structure as can be appreciated.

Stored in the memory are both data and several components that are executable by the processor . In particular stored in the memory and executable by the processor are the analytics engine the electronic commerce application and potentially other applications. Also stored in the memory may be a data store and other data. In addition an operating system may be stored in the processor and executable by the processor . While not illustrated the client device and the client device also include components like those shown in whereby the browser and or the application are stored in a memory and executable by a processor.

It is understood that there may be other applications that are stored in the memory and are executable by the processor as can be appreciated. Where any component discussed herein is implemented in the form of software any one of a number of programming languages may be employed such as for example C C C Objective C Java JavaScript Perl PHP Visual Basic Python Ruby Delphi Flash or other programming languages.

A number of software components are stored in the memory and are executable by the processor . In this respect the term executable means a program file that is in a form that can ultimately be run by the processor . Examples of executable programs may be for example a compiled program that can be translated into machine code in a format that can be loaded into a random access portion of the memory and executed by the processor source code that may be expressed in proper format such as object code that is capable of being loaded into a random access portion of the memory and executed by the processor or source code that may be interpreted by another executable program to generate instructions in a random access portion of the memory and executed by the processor etc. An executable program may be stored in any portion or component of the memory including for example random access memory RAM read only memory ROM hard drive solid state drive USB flash drive memory card optical disc such as compact disc CD or digital versatile disc DVD floppy disk magnetic tape or other memory components.

The memory is defined herein as including both volatile and nonvolatile memory and data storage components. Volatile components are those that do not retain data values upon loss of power. Nonvolatile components are those that retain data upon a loss of power. Thus the memory may comprise for example random access memory RAM read only memory ROM hard disk drives solid state drives USB flash drives memory cards accessed via a memory card reader floppy disks accessed via an associated floppy disk drive optical discs accessed via an optical disc drive magnetic tapes accessed via an appropriate tape drive and or other memory components or a combination of any two or more of these memory components. In addition the RAM may comprise for example static random access memory SRAM dynamic random access memory DRAM or magnetic random access memory MRAM and other such devices. The ROM may comprise for example a programmable read only memory PROM an erasable programmable read only memory EPROM an electrically erasable programmable read only memory EEPROM or other like memory device.

Also the processor may represent multiple processors and the memory may represent multiple memories that operate in parallel processing circuits respectively. In such a case the local interface may be an appropriate network that facilitates communication between any two of the multiple processors between any of the processors and any of the memories or between any two of the memories etc. The local interface may comprise additional systems designed to coordinate this communication including for example performing load balancing. The processor may be of electrical or of some other available construction.

Although the electronic commerce service the developer portal the metric collection service the analytics engine and other various systems described herein may be embodied in software or code executed by general purpose hardware as discussed above as an alternative the same may also be embodied in dedicated hardware or a combination of software general purpose hardware and dedicated hardware. If embodied in dedicated hardware each can be implemented as a circuit or state machine that employs any one of or a combination of a number of technologies. These technologies may include but are not limited to discrete logic circuits having logic gates for implementing various logic functions upon an application of one or more data signals application specific integrated circuits having appropriate logic gates or other components etc. Such technologies are generally well known by those skilled in the art and consequently are not described in detail herein.

The flowchart of shows the functionality and operation of an implementation of portions of the analytics engine . If embodied in software each block may represent a module segment or portion of code that comprises program instructions to implement the specified logical function s . The program instructions may be embodied in the form of source code that comprises human readable statements written in a programming language or machine code that comprises numerical instructions recognizable by a suitable execution system such as one of the processors in a computer system or other system. The machine code may be converted from the source code etc. If embodied in hardware each block may represent a circuit or a number of interconnected circuits to implement the specified logical function s .

Although the flowchart of shows a specific order of execution it is understood that the order of execution may differ from that which is depicted. For example the order of execution of two or more blocks may be scrambled relative to the order shown. Also two or more blocks shown in succession in the flowchart of may be executed concurrently or with partial concurrence. Further in some embodiments one or more of the blocks shown in the flowchart of may be skipped or omitted. In addition any number of counters state variables warning semaphores or messages might be added to the logical flow described herein for purposes of enhanced utility accounting performance measurement or providing troubleshooting aids etc. It is understood that all such variations are within the scope of the present disclosure.

Also any logic or application described herein including the electronic commerce service the developer portal the metric collection service and the analytics engine that comprises software or code can be embodied in any non transitory computer readable medium for use by or in connection with an instruction execution system such as for example the processor in a computer system or other system. In this sense the logic may comprise for example statements including instructions and declarations that can be fetched from the computer readable medium and executed by the instruction execution system. In the context of the present disclosure a computer readable medium can be any medium that can contain store or maintain the logic or application described herein for use by or in connection with the instruction execution system. The computer readable medium can comprise any one of many physical media such as for example magnetic optical or semiconductor media. More specific examples of a suitable computer readable medium would include but are not limited to magnetic tapes magnetic floppy diskettes magnetic hard drives memory cards solid state drives USB flash drives or optical discs. Also the computer readable medium may be a random access memory RAM including for example static random access memory SRAM and dynamic random access memory DRAM or magnetic random access memory MRAM . In addition the computer readable medium may be a read only memory ROM a programmable read only memory PROM an erasable programmable read only memory EPROM an electrically erasable programmable read only memory EEPROM or other type of memory device.

It should be emphasized that the above described embodiments of the present disclosure are merely possible examples of implementations set forth for a clear understanding of the principles of the disclosure. Many variations and modifications may be made to the above described embodiment s without departing substantially from the spirit and principles of the disclosure. All such modifications and variations are intended to be included herein within the scope of this disclosure and protected by the following claims.

