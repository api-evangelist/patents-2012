---

title: Apparatus, system, and method for managing solid-state storage reliability
abstract: A storage controller may be configured to assess the reliability of a solid-state storage medium. The storage controller may be further configured to project, forecast, and/or estimate storage reliability at a future time. The projection may be based on a currently reliability metric of the storage and a reliability model. The portions or sections of the solid-state storage media may be retired in response the projected reliability metric failing to satisfy a reliability threshold. The reliability threshold may be based on data correction and/or reconstruction characteristics. The projected reliability metrics of a plurality of erase blocks of a storage division may be combined, and one or more of the erase blocks may be retired in response to determining that the combined reliability metric projection fails to satisfy the reliability threshold.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09170897&OS=09170897&RS=09170897
owner: SANDISK TECHNOLOGIES, INC.
number: 09170897
owner_city: Plano
owner_country: US
publication_date: 20121221
---
This Application claims priority to U.S. Provisional Patent Application No. 61 652 745 entitled Apparatus System and Method for Managing Storage Division Retirement filed on May 29 2012 which is hereby incorporated by reference in its entirety.

This disclosure relates to apparatus systems and methods for managing a solid state storage medium and in particular to managing the retirement of portions of a solid state storage medium.

A storage controller may be configured to manage a solid state storage medium comprising a plurality of storage units. As used herein a storage unit refers to one or more physical storage units and or storage locations of a solid state storage medium. A storage unit may refer to any unit of storage including but not limited to a page a group collection or set of pages e.g. a logical page a sector a block or the like. The storage controller may be configured to manage storage divisions of the solid state storage medium. As used herein a storage division refers to a particular portion or section of a solid state storage medium which may include a group collection and or set of storage units. Accordingly a storage division may refer to one or more of an erase block a group collection and or set of erase blocks e.g. logical erase block or the like.

The solid state storage medium may have a limited lifetime and may be subject to failure conditions. These conditions may result in data errors as data is written to and or read from the solid state storage medium. Such errors may arise due to a number of factors which may include but are not limited to wear over programming read disturb write disturb erase disturb programming errors charge gain charge loss charge leaking de trapping and so on. The probability of data errors may quantified in a reliability metric. As used herein a reliability metric quantifies a probability likelihood assurance guarantee or the like that data stored on the solid state storage medium can be successfully obtained therefrom. Accordingly in some embodiments a reliability metric may correspond to a bit error rate BER and or raw bit error rate RBER . BER and or RBER metrics may be derived at least in part from the number of errors encountered during one or more storage operations as compared to the total amount of data transferred to and or from the solid state storage medium. For example an RBER of a read operation may correspond to the number of bit errors encountered in a read operation divided by the total number of bits transferred in the read operation. The reliability metric may incorporate other factors such as the probability of failure e.g. based on a current and or projected wear level operating conditions profiling information manufacturer specifications testing and experience and so on.

In some embodiments the storage controller may comprise a reliability module that is configured to identify portions or sections e.g. storage divisions of the solid state storage medium that are no longer sufficiently reliable to remain in service and as such should be retired. As used herein storage that is out of service OOS or retired refers to a storage resources that are no longer in use to store data. The reliability module may periodically scan the solid state storage medium to identify storage media that should be taken OOS. Alternatively or in addition the reliability module may identify portions or sections of the solid state storage medium that should be retired by monitoring storage operations as they occur and or by accessing error profiling data pertaining to ongoing storage operations.

In some embodiments the storage controller may be configured to perform storage operations on logical storage units. As used herein a logical storage unit refers to a group of two or more physical storage units such as a group of physical pages. The storage controller may be configured to perform storage operations on the two or more physical storage units in parallel. In some embodiments the storage controller may be configured to store data structures such as data segments packets ECC codewords or the like on two or more of the physical storage units of a logical storage unit. The reliability characteristics of such storage operations may therefore incorporate the reliability characteristics of different sections of the solid state storage medium e.g. two or more erase blocks . The reliability module may be configured to combine the reliability metrics of the different sections of the solid state storage medium and may determine whether to retire the different portions based upon the combined reliability metric.

The reliability module may be configured to determine reliability information and or manage storage retirement of arbitrarily designated sections or portions of the solid state storage medium e.g. storage divisions . Different sections of the solid state storage medium may have different reliability characteristics these differences may be due to various factors including but not limited to layout of the solid state storage medium e.g. signal paths architecture etc. different wear levels in different sections of the media access and or use patterns e.g. read and or write disturb characteristics manufacturing characteristics e.g. manufacturing defects etc. and the like.

Many of the factors that contribute to data errors worsen over time. Accordingly in certain embodiments the reliability of a storage division may decrease the longer data remains on the storage division e.g. the error rate may increase over time . In some embodiments the reliability module may be configured to project or forecast changes in the reliability metric these projections may be based on a reliability model of the non volatile storage media and or storage divisions . As used herein a reliability model refers to a model for projecting forecasting and or estimating changes in the reliability metric of portions of a non volatile storage medium over time e.g. changes in the BER and or RBER over time . The reliability model may incorporate any number of factors which may include but are not limited to operating conditions operating temperature wear level s e.g. erase cycle count program or write cycle count read cycle count and so on manufacturer specifications operating voltage testing and experience and so on.

The storage controller may be configured to provide a data retention guarantee such that data stored on the solid state storage medium is reasonably guaranteed to be retained on and or readable from the solid state storage medium for the duration of a predetermined time e.g. a data retention period even in the absence of power. In support of this guarantee the reliability module may be configured to project the reliability metric of the solid state storage medium to a future time such as the end of the data retention period and may retire portions of the storage medium that are projected to be unreliable at the end of the data retention period e.g. are projected to be insufficiently reliable to reasonably provide for accessing the retained data at the end of the data retention period . In some embodiments projecting the reliability metric comprises multiplying a current error rate e.g. RBER by a time based scaling factor. Portions of the solid state storage medium that are projected to have a reliability metric that does not satisfy a reliability threshold may be retired. The reliability threshold may be based at least in part on an error correction strength which may correspond to the number of data errors the storage controller is capable of detecting and or correcting in data stored on the non volatile storage medium. For example data may be encoded in an error correcting code ECC capable of correcting a pre determined number of errors and the reliability threshold may be set such that the number of probable errors can be corrected by the ECC encoding. Therefore in some embodiments the reliability threshold may be based at least in part upon the strength of an ECC data encoding.

As disclosed above the reliability module may be configured to determine a projected reliability metric of different portions or sections of the solid state storage medium in accordance with the granularity of storage operations performed thereon. In some embodiments the reliability module is configured to determine the reliability metric of storage divisions which may comprise groups collections and or sets of storage units such as erase blocks logical erase blocks or the like. Determining a reliability metric of a storage division may therefore comprise accumulating and or combining the projected reliability metrics of different portions of the solid state storage medium such as a group of erase blocks of one or more logical storage units e.g. by performing one or more test read operations on the logical storage units . The reliability module may be configured to retire portions of the storage division if the projected reliability metric fails to satisfy a reliability threshold. In some embodiments portions of the storage division may be retired until the projected reliability metric of the portions of the storage division satisfies the reliability threshold. The data retention period may be 90 days.

The reliability of data retained on the solid state storage medium may degrade over time. Reliability testing performed on stale data may yield inaccurate results. Accordingly the storage controller may defer retirement of storage divisions that comprise aged data and fail to satisfy one or more reliability thresholds . The storage controller may instead mark such storage divisions for post write reliability testing. Post write reliability testing may comprise evaluating the storage division for retirement after grooming and or re programming the storage division.

Disclosed herein are embodiments of an apparatus comprising a reliability metric module configured to determine a reliability metric of a section of a solid state storage medium based on a storage operation performed on the section a projection module configured to project the reliability metric to a time in the future and a reliability module configured to retire the storage division in response to the projected reliability metric failing to satisfy a reliability threshold. The apparatus may further comprise a groomer module configured to relocate data stored on the retired storage division.

The reliability metric may be based on an error rate of one or more storage operations performed on storage units in the section. The storage operations may be performed by a scan module configured to perform test storage operations on storage divisions of the solid state storage medium. The scan module may be configured to perform the test read operations according to a predetermined scanning pattern.

The reliability threshold may be based on one or more of an error correcting code strength of data stored on the storage division data reconstruction characteristics and or parity characteristics.

The apparatus may further comprise a projection module that is configured to project forecast and or estimate storage division reliability at a future time. In some embodiments the projection module is configured to project the reliability metric to the end of a data retention period. The projection module may determine the reliability projection forecast and or estimate by use of a reliability model which may comprise a reliability scaling factor a linear reliability model an exponential reliability model a polynomial reliability model a spline reliability model an artificial neural network model a radial basis function model a combination of models or the like.

The section may comprise a storage division which may include one or more storage units such as pages erase blocks or the like. The section may be adapted according to the granularity of storage operations performed on the solid state storage medium. The section may comprise a plurality of erase blocks and the projected reliability of the section may be based on reliability projections of the erase blocks. The reliability metric of an erase block may be based on storage operation errors attributed to the erase block. The projected reliability metric of the section may be a combination of the reliability metrics of the erase blocks comprising the section e.g. an average of the erase block reliability metrics .

The reliability module may be configured to retire one or more of the erase blocks in response to the projected reliability metric of the section failing to satisfy the reliability threshold. The retired erase blocks may be replaced by other erase blocks. An updated projected reliability metric of the section may be determined based on the projected reliability of the replacement erase blocks.

Disclosed herein are embodiments of a method for managing solid state storage media. The methods disclosed herein may be embodied at least in part as instructions on a machine readable storage medium. The instructions may be configured for execution by use of one or more computing device components which may include but are not limited to processors co processors special purpose processors general purpose processors programmable and or configurable processing components input output components communication components network interfaces memory components storage components and the like.

Embodiments of the methods disclosed herein may comprise forecasting reliability of a portion of a solid state storage medium at a future time wherein the reliability forecast is based on a current reliability metric of the portion and removing the portion from service in response to the reliability forecast of the storage division failing to satisfy a data retention guarantee. Removing the storage division from service may further comprise relocating data stored on the storage division.

The data retention guarantee may be based on one or more of an error correcting code strength and a data reconstruction capability for data stored on the solid state storage medium. The reliability metric may be based on a raw bit error rate of one or more test read operations performed on the storage division.

The portion may comprise plurality of erase block such as a logical erase block or portion of a logical erase block. Forecasting the reliability of the portion may comprise combining reliability forecasts of each erase block. The disclosed methods may further comprise removing one or more of the erase blocks from service in response to the reliability forecast of the portion failing to satisfy the data retention guarantee.

The disclosed methods may further comprise attributing errors of one or more test read operations to respective erase blocks wherein the reliability forecasts of the physical erase blocks are derived from errors attributed to the respective physical erase blocks.

Disclosed herein are embodiments of a system comprising means for measuring storage division reliability by use of one or more test read operations performed on the storage division means for estimating storage division reliability at an end of a data retention period based on the measured storage division reliability and a reliability model and means for retiring the storage division in response to the estimated reliability failing to satisfy a reliability threshold. The storage division may comprise a plurality of erase blocks and the disclosed systems may further comprise means for attributing errors of the one or more test read operations to respective erase blocks means for determining a respective reliability metric for each erase block and means for estimating reliability of the storage division based on the reliability metrics of the erase blocks and a reliability model wherein the means for retiring the storage division comprises means for retiring one or more of the erase blocks in response to the estimated probability of the storage division satisfy the reliability threshold.

The solid state storage media may comprise non volatile solid state storage media such as flash memory nano random access memory nano RAM or NRAM nanocrystal wire based memory silicon oxide based sub 10 nanometer process memory graphene memory Silicon Oxide Nitride Oxide Silicon SONOS Resistive Random Access Memory RRAM Programmable Metallization Cell PMC Conductive Bridging RAM CBRAM Magneto Resistive RAM MRAM Dynamic RAM DRAM Phase change RAM PRAM or the like. The solid state media controller s may be configured to write data to and or read data from the solid state storage media via a bus . The bus may comprise a storage I O bus for communicating data to from the solid state storage media and may further comprise a control I O bus for communicating addressing and other command and control information to the solid state storage media .

The storage controller may comprise and or be implemented on a computing device . In some embodiments portions of the storage controller may be internal to the computing device for example portions of the storage controller and or solid state storage media may be connected using a system bus such as a peripheral component interconnect express PCI e bus a Serial Advanced Technology Attachment serial ATA bus or the like. The disclosure is not limited in this regard in some embodiments components of the storage controller may be external to the computing device and may be connected via a universal serial bus USB connection an Institute of Electrical and Electronics Engineers IEEE 1394 bus FireWire an external PCI bus Infiniband or the like.

The computing device may comprise a processor volatile memory and or persistent storage . The processor may comprise one or more general and or special purpose processing elements. The processor may be configured to execute instructions loaded into the volatile memory from the persistent storage . Portions of one or more of the modules of the storage controller may be embodied as machine readable instructions stored on the persistent storage . The instructions may be configured for execution by the processor to implement one or more of the modules and or methods described herein.

One or more storage clients may access storage services provided by the storage controller through a storage interface . The storage interface may comprise a block device interface a virtual storage interface VSL or other suitable storage interface and or Application Programming Interface API . The storage controller may further comprise a logical to physical translation layer to map and or associate identifiers of the storage client with physical storage locations e.g. physical addresses on the solid state storage media . The logical to physical translation layer may provide for any to any mappings between logical identifiers and physical storage locations such that data may be written and or updated out of place on the solid state storage media . As used herein a physical address refers to an address or other reference capable of referencing a particular storage location on the solid state storage media . Accordingly a physical address may be a media address. 

The storage controller may be configured to maintain metadata pertaining to solid state storage media including but not limited to an index comprising the any to any mappings between logical identifiers and physical storage locations of the solid state storage media a reverse index pertaining to the contents of the solid state storage media one or more validity bitmaps reliability testing and or status metadata and so on. The metadata may be stored on the volatile memory and or may be periodically stored on a persistent storage medium such as the persistent storage and or solid state storage media .

In some embodiments the solid state storage media may comprise a plurality of solid state storage elements an array of solid state storage elements . As used herein a solid state storage element refers to a solid state storage package chip die plane or the like. Groups or banks of solid state storage elements may be communicatively coupled to the media controller and or solid state media controller s in parallel forming one or more logical storage elements . As used herein a logical storage element refers to a set of two or more solid state storage elements that are capable of being managed in parallel e.g. via an I O and or control bus . A logical storage element may comprise a plurality of logical storage units such as logical pages logical erase blocks or the like. As used herein a logical storage unit refers to a logical construct combining two or more physical storage units each physical storage unit on a respective solid state storage element each solid state storage element being accessible in parallel . A logical erase block refers to a set of two or more physical erase blocks. In some embodiments a logical erase block may comprise erase blocks within respective logical storage elements and or banks. Alternatively a logical erase block may comprise erase blocks within a plurality of different logical storage elements and or may span multiple banks of solid state storage elements.

The reliability module may be configured to identify portions of the solid state storage media that should be retired or taken OOS. As used herein retiring a portion of the solid state storage media refers to indicating that the portion should not be used to store data. Portions of the solid state storage media may be taken out of service in response to various conditions including but not limited to the reliability module determining that the portion is not sufficiently reliable or is projected to become unreliable within a pre determined time a failure condition partial failure inaccessibility unacceptable performance e.g. long read program and or erase times programming errors read errors wear or the like.

The reliability module may be configured to determine a reliability metric for different portions of the solid state storage media e.g. storage divisions . As disclosed above a storage division may refer to any portion of the solid state storage medium including but not limited to one or more pages one or more logical pages an erase block a group collection and or set of erase blocks e.g. a logical erase block or the like. The storage divisions may be configured in accordance with the partitioning of the solid state storage media and or the granularity of storage operations performed on the solid state storage media . The reliability metric of a storage division may therefore quantify the reliability of storage operations performed on the solid state storage media by the storage controller e.g. may correspond to the probability of errors in data written to and or read from the solid state storage media . The reliability metric of a storage division may comprise a bit error rate BER a raw bit error rate RBER time to failure estimate wear level read and or write cycle count or the like. In some embodiments storage divisions may be retired based on a projected reliability metric. As used herein a projected reliability metric refers to a projection estimate and or forecast of the reliability of a storage division at future time e.g. after a pre determined time period such as the data retention period described above and or one or more other event s . For example the projected reliability metric of a storage division after a 90 day data retention period refers to a projection of the reliability of the storage division 90 days into the future.

The reliability module may be configured to calculate a reliability metric of the storage division e.g. based on one or more test operations to project forecast and or estimate the reliability metric at the end of a pre determined time period and determine whether to retire the storage division based on the projected reliability metric. Retiring a storage division may comprise storing an indication that the storage division or portions thereof is to be taken OOS e.g. no longer used to store data . The indication may be stored in storage metadata on the solid state storage media on the computer readable storage or the like.

The OOS management module may be configured to avoid portions of the solid state storage medium that have been taken OOS. Avoiding an OOS storage location may comprise replacing OOS storage resources with replacement resources e.g. remapping masking OOS storage resources e.g. mapping nonce and or padding data to the OOS storage location a hybrid approach combining remapping and masking or the like.

The storage controller may further comprise a groomer module which is configured to perform grooming operations on the solid state storage media . Grooming operations may include but are not limited to reclaiming storage resources erasure wear leveling refreshing data stored on the solid state storage media and so on. The groomer module may operate outside of the path for servicing other higher priority storage operations and or requests. Therefore the groomer module may operate as an autonomous background process which may be suspended and or deferred while other storage operations are in process. Alternatively the groomer module may operate in the foreground while other storage operations are being serviced. The groomer may wear level the non volatile storage media such that data is systematically spread throughout different storage locations which may improve performance data reliability and avoid overuse and or underuse of particular storage locations thereby lengthening the useful life of the solid state storage media . Grooming an erase block or logical erase block may comprise relocating valid data if any to other storage locations erasing the erase block and or initializing the erase block for storage operations e.g. marking the erase block with a sequence indicator sequence number timestamp or the like . The groomer module may operate within a driver of the storage controller . Alternatively or in addition portions of the groomer module may be implemented on the solid state media controller e.g. as hardware components firmware programmable hardware components or the like .

The solid state media controller may comprise a request module configured to receive storage requests from the storage controller and or other storage clients . The request module may be configured to perform storage operations on the solid state storage media in response to the requests which may comprise transferring data to from the storage controller and or storage clients . Accordingly the request module may comprise one or more direct memory access DMA modules remote DMA modules bus controllers bridges buffers and the like.

The solid state media controller may comprise a write pipeline that is configured to process data for storage on the solid state storage media . In some embodiments the write pipeline comprises one or more data processing stages which may include but are not limited to compression encryption packetization media encryption error encoding and so on.

Error encoding may comprise encoding data packets or other data containers in an error correcting code ECC using inter alia the ECC write module . ECC encoding may comprise generating ECC codewords each of which may comprise a data segment of length N and a syndrome of length S. For example the ECC write module may be configured encode data segments into 240 byte ECC chunks each ECC chunk comprising 224 bytes of data and 16 bytes of ECC data. In this embodiment the ECC encoding may be capable of correcting more bit errors than the manufacturer of the solid state storage media requires. In other embodiments the ECC write module may be configured to encode data in a symbolic ECC encoding such that each data segment of length N produces a symbol of length X. The ECC write module may encode data according to a selected ECC strength. As used herein the strength of an error correcting code refers to the number of errors that can be detected and or corrected by use of the error correcting code. In some embodiments the strength of the ECC encoding implemented by the ECC write module may be adaptive and or configurable. In some embodiments the strength of the ECC encoding may be selected according to the reliability and or error rate of the solid state storage media .

The ECC write module may be further configured to calculate parity data for one or more data segments or other data structures . The parity data may be used with or in place of the ECC encoding described above. Parity data may be used to detect and or correct errors in data stored on the solid state storage medium e.g. using parity substitution as described below .

The write pipeline may be configured to store data in a contextual format on the solid state storage media . As used herein a contextual format refers to a data format in which a logical interface of a data segment is associated with the data segment on the solid state storage media . For example a contextual packet format may include a packet header comprising one or more logical identifiers of a data segment or the like. The contextual format may be used to reconstruct the logical to physical translation layer and or storage metadata of the storage controller in the event storage metadata e.g. forward index of the storage controller is lost or corrupted.

The write buffer may be configured to buffer data for storage on the solid state storage media . In some embodiments the write buffer may comprise one or more synchronization buffers to synchronize a clock domain of the solid state media controller with a clock domain of the solid state storage media and or bus .

The log storage module may be configured to select media storage location s for data storage and or may provide addressing and or control information to the non volatile storage media via the bus . Accordingly the log storage module may provide for storing data sequentially at an append point within the physical address space of the solid state storage media . The physical address at which a particular data segment is stored may be independent of the logical interface e.g. logical identifier of the data segment. The logical to physical translation layer may be configured to associate the logical interface of data segments e.g. logical identifiers of the data segments with the physical address es of the data segments on the solid state storage media . In some embodiments the logical to physical translation layer may comprise storage metadata which may include a forward index comprising arbitrary any to any mappings between logical identifiers and media addresses. The storage metadata may be maintained in volatile memory such as the volatile memory . In some embodiments the storage controller is configured to periodically store portions of the storage metadata on a persistent storage medium such as the solid state storage media persistent storage or the like.

The solid state media controller may further comprise a read pipeline that is configured to read data from the solid state storage media in response to requests received via the request module . The requests may comprise and or reference the logical interface of the requested data such as a logical identifier a range and or extent of logical identifiers a set of logical identifiers or the like. The physical addresses associated with data of the request may be determined based at least in part upon the logical to physical translation layer and or storage metadata maintained by the storage controller . Data may stream into the read pipeline via the read buffer and in response to addressing and or control signals provided via the bus . The read buffer may comprise one or more read synchronization buffers for clock domain synchronization as described above.

The read pipeline may be configured to process data read from the non volatile storage media and provide the processed data to the storage controller and or a storage client . The read pipeline may comprise one or more data processing stages which may include but are not limited to error correction media decryption depacketization decryption decompression and so on. Data processed by the read pipeline may flow to the storage controller and or storage client via the request module and or other interface or communication channel e.g. the data may flow directly to from a storage client via a DMA or remote DMA module of the storage controller .

The read pipeline may comprise an ECC read module configured to detect and or correct errors in data read from the solid state storage media using inter alia the ECC encoding of the data e.g. as encoded by the ECC write module parity data e.g. using parity substitution and so on. The ECC encoding may be capable of detecting and or correcting a pre determined number of bit errors in accordance with the strength of the ECC encoding. The ECC read module may be capable of detecting more bit errors than can be corrected.

The ECC read module may be configured to correct any correctable errors using the ECC encoding. In some embodiments the ECC read module may attempt to correct errors that cannot be corrected using the ECC encoding using other techniques such as parity substitution or the like. Alternatively or in addition the ECC read module may attempt to recover data comprising uncorrectable errors from another source. For example in some embodiments data may be stored in a RAID configuration. In response to detecting an uncorrectable error the ECC read module may attempt to recover the data from the RAID or other source of redundant data e.g. a mirror backup copy or the like .

In some embodiments the ECC read module may be configured to generate an interrupt in response to reading data comprising uncorrectable errors. The interrupt may comprise a message indicating that the requested data is in error and may indicate that the ECC read module cannot correct the error using the ECC encoding. The message may comprise the data that includes the error e.g. the corrupted data . The interrupt may be caught by the storage controller or other process.

In some embodiments the storage controller may correct errors in corrupted data using alternative error correction techniques such as parity substitution or the like. Parity substitution may comprise iteratively replacing portions of the corrupted data with a parity mask e.g. all ones until a parity calculation associated with the data is satisfied. The masked data may comprise the uncorrectable errors and may be reconstructed using other portions of the data in conjunction with the parity data. Alternatively the storage controller may replace the corrupted data with another copy of the data such as a backup or mirror copy and then may use the replacement data of the requested data packet or return it to the read pipeline . In another embodiment the storage controller stores data in a RAID configuration from which the corrupted data may be recovered as described above.

Further embodiments of apparatus systems and methods for detecting and or correcting data errors are disclosed in United States Patent Application Publication No. 2009 0287956 Ser. No. 12 467 914 entitled Apparatus System and Method for Detecting and Replacing a Failed Data Storage filed May 18 2009 which is hereby incorporated by reference in its entirety. The solid state media controller may further comprise a multiplexer that is configured to selectively route data and or commands between the write pipeline and read pipeline and solid state storage media . In some embodiments solid state media controller may be configured to read data while filling the write buffer and or may interleave one or more storage operations on one or more banks of solid state storage elements as described below in conjunction with . Further embodiments of write and or read pipelines are disclosed in United States Patent Application Publication No. 2008 0141043 Ser. No. 11 952 091 entitled Apparatus System and Method for Managing Data using a Data Pipeline filed Dec. 6 2007 which is hereby incorporated by reference in its entirety.

As discussed above the groomer module may be configured to reclaim storage resources on the solid state storage media . The groomer module may operate as an autonomous background process which may be suspended and or deferred while other storage operations are in process. The groomer module may manage the solid state storage media so that data is systematically spread throughout media addresses of the solid state storage media which may improve performance data reliability and avoid overuse and underuse of any particular storage locations thereby lengthening the useful life of the solid state storage media e.g. wear leveling etc. .

In some embodiments the groomer module may interleave grooming operations with other storage operations and or requests. For example reclaiming a storage resource such as an erase block or logical erase block e.g. set of two or more erase blocks may comprise relocating valid data stored on the logical erase block to other storage locations on the solid state storage media . The groomer write and groomer read bypass modules and may be configured to allow data packets to be read into the read pipeline and then be transferred directly to the write pipeline without being routed out of the storage media controller .

The groomer read bypass module may coordinate reading data to be relocated from the storage resource that is being reclaimed. The groomer module may be configured to interleave the relocation data with other data being written to the non volatile storage media via the groomer write bypass . Accordingly data may be relocated without leaving the solid state media controller . In some embodiments the groomer module may be configured to fill the remainder of the write buffer with relocation data which may improve groomer efficiency while minimizing the performance impact of grooming operations.

The solid state storage elements may be embodied on separate chips packages die or the like. Alternatively or in addition one or more of the solid state storage elements may share the same package and or chip e.g. be separate die and or planes on the same chip . The solid state storage elements comprise respective erase blocks each comprising a plurality of storage units e.g. pages . However the disclosure could be adapted to use different types of solid state storage media comprising different media partitioning schemes and as such should not be read as limited in this regard.

The storage controller may be configured to perform storage operations on logical storage units and or logical erase blocks of the logical storage element . In the embodiment each logical erase block comprises an erase block of a respective storage element 0 through 24 and each logical page comprises a physical page of a respective storage element 0 through 24. Accordingly each logical erase block may comprise as many as twenty five 25 erase blocks and each logical page may comprise as many as twenty five 25 physical pages . Although the logical erase block of includes erase blocks within a single logical storage element the disclosure is not limited in this regard in some embodiments described below the logical erase block may span a plurality of logical storage elements and or banks of storage elements .

The storage controller may be configured to perform storage operations on logical storage element which may operate across the constituent solid state storage elements an operation to read a logical page comprises reading from as many as twenty five 25 physical pages e.g. one storage unit per solid state storage element an operation to program a logical page comprises programming as many as twenty five 25 physical pages an operation to erase a logical erase block comprises erasing as many as twenty five 25 erase blocks and so on. Accordingly the effective read write bandwidth of the logical storage element may be proportional to the number of solid state storage elements included therein.

Arranging solid state storage elements into logical storage elements may be used to address certain properties of the solid state storage media . For example the solid state storage media may have asymmetric properties it may take ten 10 times as long to program data on a solid state storage element as it takes to read data from the solid state storage element . Moreover in some cases data may only be programmed to erase blocks that have been initialized e.g. erased . An erase operation may take ten 10 times as long as a program operation and by extension one hundred 100 times or more longer than a read operation .

The arrangement of the solid state storage elements into logical storage elements and or interleaved banks as described herein may allow the storage controller to address the asymmetric properties of the solid state storage media . In some embodiments the asymmetry in read program and or erase operations is addressed by performing these operations on many elements in parallel e.g. on a logical storage element . In the embodiment programming asymmetry may be addressed by programming twenty five 25 physical pages in a logical page in parallel. Performing multiple program operations in parallel may increase the effective write or programming bandwidth. The effective program bandwidth of the logical storage element depicted in may be as much as twenty five 25 times that of the program bandwidth of the same twenty five 25 solid state storage elements in serial. The increase to programming bandwidth may be used to mask the asymmetry between write program and read operations. Erase operations may be performed on a multiple erase blocks e.g. logical erase blocks . Erasing a logical erase block may therefore comprise erasing twenty five 25 separate erase blocks in parallel. Like the logical programming operations described above implementing erase operations on logical erase blocks in parallel may allow the storage controller to manage asymmetry between erase program and read operations.

In some embodiments a certain portion of a logical storage element may be configured to store error detection and or recovery data. For example one of the storage elements denoted in may be used to store parity data. In this embodiment the effective capacity and or bandwidth of the logical storage element may be reduced e.g. reduced from twenty five 25 physical pages to twenty four 24 physical pages the first twenty four 24 physical pages are used to store data and physical page is dedicated to storing parity data. As used herein effective capacity and or bandwidth refers to the number of storage units or divisions that are available to store data and or the total amount of data that can be stored and or read in parallel. The operational mode described above may be referred to as a 24 1 configuration denoting that twenty four 24 physical storage units are available to store data and one 1 of the physical storage units is used for parity data. The logical storage element could be configured to operate in any number of operational modes in which any proportion of the solid state storage elements are used to store error detection and or recovery data and as such the disclosure should not be read as limited in this regard.

As illustrated above the storage controller may be configured to perform storage operations on logical storage units logical pages of the solid state storage media each of which may comprise as many as twenty five erase blocks . The reliability module may be configured to track reliability metrics of the solid state storage medium at a corresponding level of granularity. Accordingly the reliability module may be configured to determine reliability characteristics of storage sections that correspond to the logical erase blocks which as disclosed herein may comprise combining the reliability metrics of individual erase blocks .

Although particular embodiments of logical storage elements as disclosed herein the disclosure is not limited in this regard and could be adapted to incorporate logical storage elements of differing sizes and or configurations. The size and number of erase blocks pages planes or other logical and physical divisions within the solid state storage elements are expected to change over time with advancements in technology it is to be expected that many embodiments consistent with new configurations are possible and are consistent with the embodiments disclosed herein.

Referring back to as described above the storage controller may be configured to continue operating when storage units in the solid state storage media fail and or are taken out of service e.g. are retired . The reliability module may be configured to identify portions of the solid state storage media that should be taken OOS e.g. pages erase blocks die planes chips etc. . In some embodiments the solid state media controller may maintain profiling information pertaining to the solid state storage media the profiling information may include but is not limited to error information e.g. RBER performance wear levels and so on. In some embodiments the profiling information may be maintained in the storage metadata and or may be accessible to the reliability module which may use the profiling information to identify storage resources that should be retired. Alternatively or in addition the reliability module may be configured to actively scan and or test the solid state storage media to identify storage resources that should be retired.

The OOS management module may be configured to track storage resources that have been taken out of service. In some embodiments the OOS management module tracks OOS conditions in the solid state storage media using OOS metadata . OOS conditions may be detected and or tracked at varying levels of granularity OOS conditions may be tracked and or maintained by page logical page erase block logical erase blocks die chips planes and or according to other storage partitions or divisions. The storage divisions may be configured to reflect the reliability characteristics of storage operations performed on the solid state storage media . The reliability module may be configured to maintain reliability information for storage divisions comprising a plurality of erase blocks in accordance with the logical storage element and or logical pages of . The disclosure should not be read as limited in this regard however and could be applied to any size and or organization of non volatile storage media . The storage controller may be configured to manage OOS conditions using one or more of a remapping approach masking approach hybrid approach or the like.

In some embodiments the storage controller is configured to manage OOS conditions using a remapping approach in which the bus includes addressing information for each solid state storage element in the logical storage element e.g. each storage element may receive a respective physical address via the bus . The storage controller may leverage the separate addressing information to remap replacements for one or more OOS storage resources from other portions of the solid state storage media . The OOS management module may use remapping to prevent a few OOS erase blocks from taking an entire logical erase block out of service.

The OOS management module may be configured to manage OOS conditions using an masking approach. in which OOS conditions are managed by masking physical storage units that are OOS if any . As used herein masking an OOS storage location such as an erase block may comprise configuring the write pipeline to inject padding data into the write buffer such that the padding data is mapped to the OOS storage locations on the bus during programming operations. Masking may further comprise configuring the read pipeline to ignore or otherwise avoid data read from OOS storage locations during read operations. Masking OOS storage units may reduce the storage capacity and or effective bandwidth of portions of the logical storage element while allowing the remaining in service storage divisions to continue in operation. As used herein padding or masking data refers to any data that is used in place of valid data. Accordingly padding data may be actively added as a particular data pattern e.g. ones zeros or other patterns or may be added passively by reusing whatever data is on the bus or write pipeline allowing portions of the bus to float or the like.

In some embodiments the OOS management module is configured to manage OOS conditions using a hybrid approach in which OOS conditions are managed by masking the OOS storage units if any as described above. The masking approach may be used until the number of OOS storage locations reaches a threshold. When the threshold is reached the storage controller may be configured to implement the bad block remapping approach to replace one or more of the OOS physical storage units from other portions of the solid state media as described above. OOS storage units for which there are no available replacements may continue to be managed using the masking approach. Further embodiments of apparatus systems and methods for managing OOS conditions are disclosed in U.S. patent application Ser. No. 13 354 215 entitled Apparatus System and Method for Managing Out of Service Conditions filed Jan. 19 2011 which is hereby incorporated by reference in its entirety.

In the embodiment the solid state media controller may comprise an OOS write module configured to manage OOS conditions in the write pipeline e.g. remap and or mask OOS storage resources . During write operations the OOS write module may be configured to identify storage resources that are OOS using inter alia the OOS metadata . The OOS write module may access the OOS metadata from the OOS management module an internal metadata storage unit driver storage controller or the like. Alternatively or in addition the OOS management module may be configured to push OOS metadata to the solid state media controller via the request receiver module e.g. OOS metadata may be included with storage requests .

The OOS write module may be configured to manage OOS conditions using one or more of a remapping approach masking approach hybrid approach or the like as described above. The OOS write module or other command and control module may be configured to implement a remapping approach to replace OOS storage resources with other available storage resources. The remapping approach may comprise identifying other available storage resources and modifying one or more addresses and or command signals on the bus to replace OOS storage resources with the identified replacement resources e.g. using the log storage module . The OOS write module may be further configured to implement a masking approach which may comprise injecting padding data into the write buffer or other portions of the write pipeline such that the padding data is mapped to the OOS storage resources identified by the OOS metadata . The OOS write module may be further configured to implement a hybrid approach in which the OOS write module masks a threshold number of OOS storage resources and then implements bad block remapping where available thereafter.

The OOS read module may be configured to manage OOS conditions in the read pipeline using one or more of a remapping approach masking approach hybrid approach or the like as described above. In a bad block remapping approach the OOS read module may be configured to identify the replacement addresses for OOS storage resources if any and set addressing and or control signals on the bus accordingly e.g. by use of the log storage module . In a masking approach the OOS read module may be configured to strip or otherwise ignore data read corresponding to OOS storage resources e.g. strip padding data from the read buffer before the data is processed through the rest of the read pipeline . In a hybrid approach the OOS read module may be configured to selectively remap storage resources and or strip data from the read buffer in accordance with the OOS metadata and as described above.

The OOS write module may comprise a write padding module configured to selectively replace incoming data with padding data such that the padding data is mapped to storage locations of OOS erase blocks as identified by a corresponding OOS metadata entry A N. The logical erase blocks A N may comprise twenty five erase blocks 0 24 each on a respective solid state storage element . As illustrated in the OOS metadata entry A of logical erase block A may indicate that erase blocks and are OOS. Accordingly when writing data to logical erase block A the write padding module may be configured to map padding data to erase blocks and e.g. by use of the cross point switch or other mechanism . Outbound data may stream from the cross point switch to a the write buffer which may be configured to buffer 24 bytes one byte for each solid state storage element . The write parity module may be configured to calculate a parity byte corresponding to the data the padding data may be ignored by the write parity module and or the padding data may be configured to not affect the parity calculation . The output data and parity byte may stream to program buffers of the solid state storage elements via the bus such that the padding data is streamed to elements and .

As illustrated in OOS metadata entry N of logical erase block N may indicate that erase block is OOS e.g. the erase block on storage element has been retired . In response the write padding module may be configured to mask erase block with padding data such that the padding data streams to the program buffer of solid state storage element as described above.

In some embodiments the OOS write module may implement a remapping approach to managing OOS conditions. The OOS write module may therefore comprise a remapping module which may be provided in place of or in addition to the OOS padding module . The remapping module may be configured to maintain remapping metadata which may comprise addresses of replacement erase blocks that can be used to replace erase blocks that are OOS. The replacement erase blocks may be maintained in one or more dedicated spare areas may be taken from other logical erase blocks A N or the like. The remapping module may be configured to remap one or more of the replacement erase blocks of to replace one or more OOS erase blocks in a logical erase block A in accordance with the OOS metadata as described above e.g. per the OOS metadata entries A N . For example when writing data to logical erase block N the remapping module may be configured to remap erase block B to replace the OOS erase block N. Remapping the erase block B may comprise providing different addressing information to the solid state storage element via the bus configured to cause storage element to program the data to erase block B rather than erase block N. Accordingly the data may flow to each solid state storage element of logical erase block N without padding data . However when the data is streamed and or programmed to the logical erase block N the solid state storage element may be provided with different addressing information than the other solid state storage elements .

The OOS write module may comprise both the write padding module and a remapping module . The remapping module may be configured to replace OOS erase blocks when possible. The write padding module may be configured to mask OOS erase blocks for which there are no replacements available with padding data as described above. For example when writing data to logical erase block A the remapping module may be configured to remap a replacement for the OOS erase block on solid state storage element and may mask the erase block of solid state storage element with padding data. The read pipeline may comprise similar modules configured to strip and or ignore data from OOS solid state storage elements and or read data from remapped solid state storage elements .

The reliability module may be configured to identify storage resources within the banks A N that should be taken out of service and the OOS management module may be configured to track OOS conditions across each of the banks A N. In some embodiments the reliability module is configured to monitor storage operations on the banks A N access profiling data pertaining to storage operations on the banks A N and or scan and or test the banks A N as described above. The OOS management module may be configured to track OOS conditions using OOS metadata which may comprise entries pertaining to OOS conditions on each of the banks A N. The OOS write module and the OOS read module may be configured to manage OOS conditions within each bank A N in accordance with the OOS metadata as described above.

Some operations performed by the storage controller may cross bank boundaries. For example the storage controller may be configured to manage the solid state storage media using logical erase blocks that span banks A N. Each logical erase block may comprise a group of erase blocks A on each of the banks A N. The groups of erase blocks A N in the logical erase block may be erased together e.g. in response to a single erase command and or signal or in response to a plurality of separate erase commands and or signals . Performing erase operations on larger groups of erase blocks A N may further mask the asymmetric properties of the solid state storage media as disclosed above.

The storage controller may be configured to perform some storage operations within bank boundaries e.g. within the boundaries of particular banks A N . In some embodiments the storage controller may be configured to read write and or program logical pages A N within the respective banks A N. As depicted in the logical pages A N may not span banks A N e.g. each logical page A N may be contained within the logical storage element A N of a respective bank A N . The log storage module and or bank interleave module may be configured to interleave such storage operations between the banks A N.

The bank interleave module may be configured to program data to logical pages A N within the banks A N in accordance with an interleave pattern. In some embodiments the interleave pattern is configured to sequentially program data to logical pages A N of the banks A N. In some embodiments the interleave pattern may comprise programming data to a first logical page LP  of bank A followed by the first logical page of the next bank B and so on until data is programmed to the first logical page LP  of each bank A N. As depicted in data may be programmed to the first logical page LP  of bank A in a program operation A. The bank interleave module may then stream data to the first logical page LP  of the next bank B. The data may then be programmed to LP  of bank B in a program operation B. Data may be streamed to and programmed on the first logical page LP  of bank B in a program operation B. The program operation B may be performed concurrently with the program operation A on bank A the storage controller may stream data to bank B and or issue a command and or signal for the program operation B while the program operation A is being performed on bank A. Data may be streamed to and or programmed on the first logical page LP  of the other banks C N following the same interleave pattern e.g. after data is streamed and or programmed to LP  of bank B data is streamed and or programmed to LP  of bank C in program operation C and so on . Following the programming operation N on LP  of the last bank N the bank interleave controller may be configured to begin streaming and or programming data to the next logical page LP  of the first bank A and the interleave pattern may continue accordingly e.g. program LP  of bank B followed by LP  of bank C through LP  of bank N followed by LP  of bank A and so on .

Interleaving programming operations as described herein may increase the time between concurrent programming operations on the same bank A N which may reduce the likelihood that the storage controller will have to stall storage operations while waiting for a programming operation to complete. As disclosed above programming operations may take significantly longer than other operations such as read and or data streaming operations e.g. operations to stream the contents of the write buffer to a logical storage element A N via the bus A N . The interleave pattern of avoids consecutive program operations on the same bank A N programming operations on a particular bank bank A may be separated by N 1 programming operations on other banks e.g. programming operations on bank A are separated by programming operations on banks B N . Since the interleave pattern of programming operation separates programming operations on A by programming operations on banks B N the programming operation on bank A is likely to be complete before another programming operation needs to be performed on the bank A.

As depicted in the interleave pattern for programming operations may comprise programming data sequentially across logical pages A N of a plurality of banks A N. As depicted in the interleave pattern may result in interleaving programming operations between banks A N such that the erase blocks of each bank A N erase block groups EBG  N are filled at the same rate. The interleave pattern programs data to the logical pages of the first erase block group EBG  in each bank A N before programming data to logical pages LP  through LP N of the next erase block group EBG  and so on e.g. wherein each erase block comprises 0 N pages . The interleave pattern continues until the last erase block group EBG N is filled at which point the interleave pattern continues back at the first erase block group EBG .

The erase block groups of the banks A N may therefore be managed as logical erase blocks A N that span the banks A N. Referring to a logical erase block may comprise groups of erase blocks A N on each of the banks A N. As disclosed above erasing the logical erase block may comprise erasing each of the erase blocks A N comprising the logical erase block . In the embodiment erasing the logical erase block A may comprise erasing EBG  of each bank A N erasing a logical erase block B may comprise EBG  of the banks A N erasing logical erase block C may comprise erasing EBG  of the banks A N and erasing logical erase block N may comprise erasing EBG N of the banks A N. Other operations such as grooming recovery and the like may be performed at the granularity of the logical erase blocks A N recovering the logical erase block A may comprise relocating valid data if any stored on EBG  of the banks A N erasing the erase blocks of each EBG  of the banks A N and so on. Accordingly in embodiments comprising four banks A N each bank A N comprising a logical storage element A N formed of twenty five storage elements A N erasing grooming and or recovering a logical erase block comprises erasing grooming and or recovering one hundred erase blocks . Although particular multi bank embodiments are described herein the disclosure is not limited in this regard and could be configured using any multi bank architecture comprising any number of banks A N having logical storage elements A N comprising any number of solid state storage elements A N.

The reliability module may be configured to track reliability metrics and or storage retirement in accordance with the granularity of the storage operations performed on the solid state storage media . The granularity of storage operations may differ from other operations and or partitioning schemes used to manage the solid state storage media . As described above the storage controller may be configured to erasure grooming recovery and other operations at the granularity of logical erase blocks which span multiple banks A N. As illustrated in however the storage controller may be configured to perform storage operations on logical pages A N that are defined within respective bank boundaries storage operations on the logical page A are performed within bank A e.g. erase blocks A N of group A storage operations on the logical page B are performed within bank B e.g. erase blocks A N of group B and so on.

As disclosed above the reliability module may be configured to maintain reliability metrics and or retirement metadata in accordance with arbitrarily defined storage divisions A N of the solid state storage media . The storage divisions A N may be configured in accordance with the granularity of the storage operations performed on the solid state storage media e.g. read write and or program operations such that the reliability metrics of the storage divisions A N accurately reflect the reliability characteristics of storage operations performed on the solid state storage medium . Therefore in the embodiment the storage divisions A N may be defined within bank boundaries in accordance with the boundaries of the logical pages A N the reliability metric of the storage division A may quantify the reliability of the erase blocks A of bank A and comprising the logical page A the reliability metric of the storage division N may quantify the reliability of the erase blocks N of bank N and comprising the logical page N and so on. The difference in granularity between the storage divisions A N used by the reliability module and the logical erase blocks may enable the reliability module to accurately characterize and or quantify the reliability of storage operations performed on the logical pages A N. For example if the reliability module were to use a storage division at the granularity of the logical erase blocks the corresponding reliability metric would incorporate the reliability metric of the erase blocks within each bank A N which would include reliability characteristics of erase blocks not involved in read write and or program operations of individual logical pages A N e.g. the reliability metric of operations on a logical page A would incorporate reliability characteristics of erase blocks in groups B N which are not involved in storage operations on the logical page A and could result in inaccurate reliability assessments e.g. poor reliability of erase blocks B would affect the reliability metric of operations performed on the logical page A despite the fact that such operations do not involve bank B. However in embodiments in which storage operations span multiple banks A N such that the logical pages A N comprise storage units on multiple banks A N the reliability module may configure the storage divisions A N accordingly e.g. adapt the storage divisions A N to span multiple banks A N in accordance with the granularity of the multi bank logical pages .

Referring to the reliability module may be configured to identify portions of the solid state storage media that should be retired by use of reliability metrics pertaining to storage divisions which may be configured in accordance with the granularity of storage operations performed on the solid storage media . The OOS management module may also be configured to maintain and or manage OOS conditions at different levels of granularity. The OOS management module is configured to manage OOS conditions at the same granularity as the reliability module which may comprise managing OOS conditions in accordance with the storage divisions . Alternatively the OOS management module may be configured to manage OOS conditions at a different level of granularity. In some embodiments for example the OOS management module is configured to manage OOS conditions at a higher level of granularity such as logical erase blocks .

As described above the storage controller may be configured to interleave storage operations between a plurality of banks A N. The storage operations may be performed on logical pages A N within respective banks A N. Other operations however may span multiple banks A N. In some embodiments the storage controller is configured to perform erasure recovery and or grooming operations on logical erase blocks A N each of which may comprise groups of erase blocks on multiple banks A N logical erase block A may comprise erase block group of banks A N   A N logical erase block N may comprise erase block group N of banks A N  N A N and so on.

In some embodiments the OOS management module may be configured to manage OOS conditions at the granularity of the logical erase blocks A N. The OOS metadata entry A tracks OOS conditions within the logical erase block A. As such retiring an erase block in any of the banks A N results in treating the corresponding erase block s of the other banks A N as being OOS. For example retiring erase block of bank A OOS results in retiring erase block of banks B N even if the erase blocks in the other banks B N are still sufficiently reliable to remain in service. When writing data to logical pages A N within the logical erase block A N the OOS write module may be configured to apply the same set of OOS conditions to each interleaved storage operations e.g. the OOS conditions apply to each bank A N . Managing OOS conditions at the granularity of logical erase blocks A N may result in lower OOS management overhead since such entries A apply to relatively large portions of the solid state storage medium and the OOS write module applies the same set of OOS indications to each interleaved storage operation within the logical erase block A.

The OOS management module may be configured to manage OOS conditions at other levels of granularity. The OOS management module may maintain a metadata entry N configured to track OOS conditions at the level of granularity of the storage divisions used by the reliability module as described above. Accordingly the entry N may comprise indications A N that correspond to OOS conditions within each bank A N in the logical erase block N the indications A correspond to erase block group  N A of bank A B corresponds to erase block group  N B of bank B C corresponds to erase block group  N C of bank C N corresponds to erase block group  N N of bank N and so on. Accordingly the OOS write module may be configured to apply a different set of OOS conditions to each interleaved storage operation within the logical erase block N storage operations on each different bank A N may have different OOS conditions erase block is treated as OOS when performing storage operations on banks A and B but is not when performing operations on banks C and N erase block is only treated as OOS when performing storage operations on bank C and erase blocks is only treated as OOS when performing storage operations on bank N. Managing OOS conditions at the lower level of granularity may therefore enable more efficient use of the solid state storage media . Although particular examples of OOS metadata and or granularities for tracking and or managing OOS conditions are disclosed herein the disclosure is not limited in this regard and could be adapted to track and or manage OOS conditions at any suitable level of granularity on the solid state storage medium .

The reliability module may be configured to determine the reliability metric of a storage division A N using any suitable measurement and or monitoring technique. As described above in some embodiments the media controller may be configured to maintain error profiling data pertaining to the solid state storage media and may provide the profiling data to the reliability module . The error profiling data may comprise information pertaining to errors detected and or corrected by the ECC read module and or by parity substitution or other reconstruction technique as described above. The reliability module may use the error profiling data to determine the reliability metric of one or more of the storage divisions A N.

In some embodiments the reliability module may comprise a scan module configured to perform periodic test read operations on the solid state storage media . A test operation may comprise reading one or more packets and or data segments from particular storage divisions A N. The test operations may further comprise determining whether the operation s resulted in an error e.g. errors detected and or corrected using the ECC correction module a parity module or the like . The reliability module may use error metrics of the test operations to determine and or estimate the RBER for particular storage divisions A N. The scan module may be configured to perform test operations independently of other storage operations and may be configured to avoid impacting other storage requests e.g. may be performed as low priority autonomous background operations . The scan module may be configured to periodically test the storage divisions A N to identify portions of the solid state storage media that should be retired e.g. perform test operations every five seconds and or schedule test operations such that each storage division A N is scanned within a 24 hour period . In some embodiments the scan module is configured to scan near an append point within the solid state storage media to reduce the time differential between the time the storage division A N is programmed and the time at which the reliability metric of the storage division A N is determined.

In some embodiments the scan module may perform test read operations according to a scan pattern. The scan pattern may be configured to alternate read locations within the storage divisions A N. For example a first test operation may comprise reading a first packet or first page of a storage division A a second test operation may comprise reading a second packet or second page in another storage division B and so on until the read locations wrap back to the first packet or page .

In some embodiments the scan module may comprise scan policy which may configure and or control the operation of the scan module . The scan policy may define a scan pattern to be implemented by the scan module determine a scanning schedule of the scan module determine conditions for triggering the scan module and or scheduling scan operations and so on. For example the scan policy may configure the scan module to scan through the storage divisions A N of the solid state storage media at a predetermined scanning period such that each storage division A N is scanned at least once during the scanning period e.g. at least once per 24 hours . Alternatively or in addition the scanning policy may be adaptive in accordance with operating conditions of the storage controller and or state of the solid state storage media . For example the scan policy may be configured to increase the frequency of scanning operations in response to determining that the solid state storage media is becoming less reliable e.g. overall reliability of the storage divisions A N reaches a particular threshold has reached a pre determined wear level or the like. The scan policy may be stored in volatile memory with other storage metadata storage the solid state storage media or in another storage location. The scan policy may be configured according to user preferences testing and experience or the like.

The reliability module may further comprise a reliability metric module configured to determine a reliability metric of a storage division A N based at least in part on the test read operations performed by the scan module e.g. based on a raw bit error rate of the test read operations . The reliability metric may also incorporate error profiling data provided by the media controller and so on. The reliability metric module may be configured to determine the reliability metric of a storage division using any suitable error modeling technique and or mechanism e.g. RBER or the like . For example in some embodiments the reliability metric module incorporates other factors into the reliability metric of a storage division including but not limited to the wear level of the storage division A N e.g. program erase cycle count performance of the storage division A N e.g. time to program and or erase retry count e.g. number program retries required and so on.

In some embodiments the storage controller may be configured to guarantee data availability for a pre determined data retention period. As used herein a data retention period refers to the time for which data stored on the solid state storage media is reasonably guaranteed to be retained even in the absence of power to the solid state storage media . The data retention period may vary depending on user requirements and or the capabilities of the solid state storage media . For example the data retention period may be 90 days meaning that data stored on the solid state storage media is reasonably guaranteed to be readable 90 days after being written thereto even in the absence of power to the solid state storage media .

The reliability module may be configured to manage retirement of storage divisions A N and or portions thereof of the solid state storage media to ensure that the retention guarantee can be fulfilled. In some embodiments the reliability module comprises a projection module configured to determine a projected reliability metric of the storage division A N. As used herein a projected reliability metric refers to a projection estimate forecast and or prediction of the reliability of a storage division A N at some time in the future. Accordingly a projected reliability metric may comprise the projected RBER of the storage division A N at the end of a 90 day data retention period. Projecting a reliability metric may therefore comprise extrapolating a current reliability metric of particular storage division C into some time T in the future such as the end of the data retention period. In some embodiments the projection may be linear and as such calculating a projected reliability metric R may comprise scaling a current reliability metric R of the storage division C by a time based T reliability scaling factor P 

The projection module may be configured to calculate the projected reliability metric using a reliability model which may be configured to model changes in storage division reliability in response to various factors and or characteristics which may include but are not limited to time the age of the solid state storage media operating temperature erase cycle count program cycle count read count manufacturer specifications testing and experience and so on. For example the projected reliability metric of a particular storage division C after the data retention period T may be projected based on the retention period guaranteed by the storage controller e.g. 90 days the number of times the storage division C has been erased the number of times the storage division C has been programmed the number of times the storage division C has been read manufacturer specifications regarding wear testing and experience regarding wear and so on. The projection module may be configured to combine various reliability modeling factors in a weighted combination e.g. certain factors may weigh more heavily in the projected reliability metric than others . The weights may be determined based on testing and experience internal feedback loops and or metrics manufacturer specifications and so on. Accordingly the projection module may be configured to project reliability metrics using a reliability model RelModel that incorporates any number of reliability projection factors which as described above may include but are not limited to media age Age temperature Temp erase cycle count ErC program cycle count PrC read count RdC manufacturer specifications MS testing and experience TE and so on such that the reliability the storage division C projected to a time T is RelModel Age Temp ErC PrC RdC MS TE 

Moreover in some embodiments the projected reliability of a storage division A N may be related to e.g. a function of a current reliability of the storage division A N for example the rate at which the reliability of the storage divisions A N is projected to change may increase in proportion to the current reliability of the storage divisions A N. Therefore the reliability model RelModel may incorporate the current reliability of the storage division RelModel Age Temp ErC PrC RdC MS TE 

In some embodiments the reliability of a storage division A N may be modeled using an exponential decay function such as 

The initial value R of the exponential delay function may be the current measured reliability of the storage division A N. The decay factor may be modeled according to testing and experience e.g. curve fitting observed reliability data and or a reliability model as described above. In another example reliability may be projected by use of a polynomial spline or other curve fitting and or modeling function e.g. a numerical model an artificial neural network model a radial basis function model or the like as described above.

The reliability module may be configured to retire storage divisions A N that have a projected reliability metric that does not satisfy a reliability threshold Where is a data retention period .

In some embodiments the reliability threshold R is based on the error correction strength of the storage controller . As used herein error correction strength quantifies the ability of storage controller to detect and or correct errors in the data stored on the solid state storage media. The error correction strength may be related to the strength ECC encoding implemented by the storage controller e.g. the ECC strength data reconstruction capabilities of parity data associated with data on the solid state storage media and so on. For example and as described above the ECC read module may be capable of correcting a pre determined number of errors in data read from a storage division A N of the solid state storage media . If the projected RBER of a particular storage division C exceeds the number of errors that can be corrected fails to satisfy the reliability threshold the reliability module may retire the storage division C take the storage division C out of service .

The plot further includes a reliability threshold which as described above may correspond to a minimum reliability metric at which data can be reliably read from the storage division A N e.g. the minimum reliability metric at which a data retention guarantee can be reasonably provided . Accordingly the reliability threshold may correspond to an ECC strength parity strength and or other data reconstruction and or recovery mechanisms used to protect data stored on the storage division A N. The plot further depicts a predetermined retention period T which may correspond to a data retention guarantee.

The reliability module may be configured to retire storage divisions A N that have a projected reliability metric A that does not satisfy the reliability threshold . Such storage divisions A N may be removed from service since their use is forecast to fail to satisfy the data retention guarantee e.g. violate the guarantee that data stored on the storage division A N will be readable at the end of the data retention period . This condition is illustrated in at the intersection A of the reliability projection A and the end of the reliability period T along the time axis . The storage division A N corresponding to the reliability projection A satisfies the reliability threshold since the reliability projection A at T is projected to exceed the reliability threshold . If however the data retention period T were increased by T to time e.g. increased from 90 days to 110 days the projected reliability B of the storage division A N would fail to satisfy the reliability threshold and the storage division A N may be retired .

Referring back to taking a particular storage division C out of service may comprise identifying the retired storage division C to the OOS management module so that the storage division C is no longer used to store data. The OOS management module may be configured to prevent the media controller from storing data on the storage division C which may comprise marking the storage division C as out of service in the OOS metadata remapping a replacement s for the storage division C masking the storage division C and so on as described herein. In addition the groomer module may be configured to relocate data stored on the OOS storage division C to other storage division s on the solid state storage media in a grooming operation as described above.

The reliability module may be configured to identify portions of the storage media that should be retired. As disclosed above the reliability module may be configured to determine the reliability of storage divisions which may be configured in accordance with the granularity of storage operations performed by the storage controller . In the embodiment the storage divisions correspond to the logical erase blocks .

The reliability metric of a storage division may be based on the results of test read operations performed by the scan module profiling data and so on as described above. Determining the reliability metric of a storage division may comprise determining the reliability metric of the erase blocks A N comprising the storage division e.g. the erase blocks A N of the logical erase block . The scan module may be configured to perform one or more test read operations on logical pages A N within the storage division . The reliability metric module and or the scan module may be configured to attribute errors encountered in the one or more test read operations and or profiling data to respective erase blocks A N e.g. identify which storage division caused which error s from which a reliability metric of the erase block A N may be determined.

As described above the storage controller may be configured to encode data into ECC codewords for storage on the solid state storage medium e.g. by use of the ECC write module . As illustrated in an ECC codeword may be stored on a logical page A which may comprise storing portions of the ECC codeword on each of a plurality of erase blocks A N. Accordingly errors in the ECC codeword may be attributable to one or more different erase blocks A N. In some embodiments the reliability module is configured to determine the source of each test read error e.g. identify which storage division caused the error such that reliability metrics for the individual erase blocks A N can be accurately determined. For example in some embodiments the test read operations of the scan module may comprise identifying and or correcting read errors in the ECC codeword using inter alia the ECC read module as described above. The reliability module may use the ECC error identification and or correction information to attribute test read errors to particular erase blocks A N.

In some embodiments the ECC codeword may be associated with parity data . The parity data may be used to reconstruct portions of the ECC codeword that cannot be corrected by use of the ECC read module e.g. via parity substitution or other parity reconstruction technique as described above . The reliability module may be configured to determine the source of uncorrectable ECC errors if any based on the parity correction.

In another example the storage controller may be configured to encode data using a symbolic ECC encoding in which data is encoded as a plurality of ECC symbols . The ECC symbols may be configured to be stored within pre determined storage boundaries of the respective erase blocks A N e.g. each ECC symbol may be configured to be stored on a page of a respective one of the erase blocks A N . Accordingly the source of ECC read errors may be determined as ECC symbol errors if any are detected and or corrected. ECC symbol data may further comprise parity data which may be used to reconstruct ECC symbols comprising unrecoverable errors as described above e.g. using parity substitution or the like . As described above parity reconstruction using the parity data may comprise identifying the source of the unrecoverable error s .

The reliability metric module may be configured to use the results of the test read operations and or other profiling data to calculate respective reliability metrics of the erase blocks A N of the storage division . Accordingly the reliability metric module may be configured to identify errors attributable to each erase block A N based on ECC and or parity processing as described above and to calculate a respective reliability metric for each erase block A N based on the errors attributed thereto.

The projection module may be configured to project the reliability metric of each erase block A N to the end of a data retention period T as described above.

The reliability module may further comprise an accumulation module configured to determine the projected reliability metric of the storage division based on the individual projected reliability metrics of the erase blocks A N. As described above reading data from a logical page A N e.g. storage division may comprise reading data from each of the erase blocks A N comprising the storage division . Accordingly the projected reliability metric of the storage division R may be based on the projected reliability metrics of each of the erase blocks A N in the storage division Rthrough R . In some embodiments the projected reliability metric of the storage division R may comprise an average of the projected reliability metrics of the erase blocks A N Rthrough R as follows 1

The reliability module may determine whether the projected reliability of the storage division satisfies the reliability threshold which as discussed above may be based on the error correcting strength of the solid state storage controller . If the projected reliability metric fails to satisfy the reliability threshold e.g. R

In some embodiments the reliability module may take the storage division out of service if more than a threshold number of erase blocks A N must be removed to satisfy the reliability threshold and or if the reliability threshold cannot be satisfied by removing erase blocks A N . Alternatively or in addition the reliability module may be configured to replace one or more of the erase blocks A N to improve the projected reliability metric of the storage division e.g. in a remapping approach as described above . In the remapping approach an updated projected reliability metric of the storage division may be calculated by incorporating projected reliability metrics of the replacement erase blocks into the accumulated projected reliability metric as described above.

In some embodiments the reliability module may retire one or more erase blocks A N even if the accumulated projected reliability metric of the storage division satisfies the reliability threshold . For example the reliability metric of a particular storage division may be below a retention threshold. The retention threshold may be a minimal threshold for inclusion in a storage division . The retention threshold may be less stringent than the reliability threshold described above. For example a particular erase block E of the storage division may have an RBER that is very high e.g. indicates that the storage division D has failed or is about to fail . However the other erase blocks A C and E N may have high reliability metrics such that the accumulated reliability metric of the storage division satisfies the reliability threshold . Notwithstanding inclusion of the unreliable erase block D may adversely affect performance storage operations involving the erase block D may require time consuming ECC correction and or parity substitution operations. As such the reliability module may be configured to retire the erase block D in response to determining that the erase block D fails to satisfy the minimal retention threshold despite the fact that the accumulated projected reliability metric of the storage division satisfies the reliability threshold .

The reliability module may identify and or mark storage division s and or individual erase blocks A N that should be retired. The reliability module may be configured to identify such storage divisions to the OOS management module which may be configured to take the identified storage divisions OOS as described above. As disclosed above the OOS management module may be configured to maintain OOS metadata e.g. bad block metadata that identifies OOS storage divisions such that the OOS storage divisions can be avoided ignored and or remapped replaced by use of one or more of a remapping approach masking approach hybrid approach or the like. Although particular examples of mechanisms for managing OOS storage divisions are disclosed herein the disclosure is not limited in this regard and could be adapted to manage OOS storage divisions using any suitable mechanism and or technique. Retiring a storage division may further comprise configuring the groomer module to relocate valid data from the OOS stored division s if any as described above.

The reliability of the solid state storage media may decrease over time. Moreover reliability may be affected by operations performed on neighboring portions of the solid state storage media such as inter alia operating conditions e.g. read disturb write disturb erase disturb media characteristics e.g. charge gain charge loss de trapping etc. and so on. As such determining the reliability metric for a storage division when aged or stale data is on the storage division may result in inaccurate results a storage division comprising aged data may appear to be less reliable than a storage division comprising data that was recently programmed e.g. programmed to the storage division within the age threshold . Therefore it may be more accurate to evaluate the reliability of a storage division when the storage division satisfies an age threshold e.g. the storage division was recently programmed . As used herein the data age refers to the length of time data has remained on a storage division e.g. the time that has passed since the data was programmed onto the storage division . As used herein an age threshold refers to a time threshold pertaining to data age and or the last programming time of a storage division . Accordingly an age threshold Tmay relate to the time differential Tbetween the programming time Tof the storage division and the time that the reliability of the storage division was measured T e.g. the time at which the test read operations were performed on the storage division to determine the reliability metric of the storage division . 

As used herein an aged storage division refers to a storage division having programming time differential Tthat that exceeds the age threshold T such that data stored on the storage division exceeds the age threshold T e.g. is older than the age threshold age threshold TAT . A non aged storage division refers to a storage division having a programming time differential Tthat is less than the age threshold T such that the data has been stored on the storage division for less than the age threshold T e.g. is younger than the age threshold T Aged or Aged 

The age threshold Tmay be set in accordance with the reliability model of the storage division testing an experience or the like. In some embodiments the age threshold Tis 24 hours such that storage divisions programmed more than 24 hours before reliability testing would be considered to be aged storage divisions . The age of a storage division may be based on one or more of a last program time of a storage division a sequence indicator and or timestamp a log order of a storage division a system clock indicator storage metadata or the like.

The projection module may be configured to determine the reliability projection D for the storage division based on the reliability metric determined at T. As depicted in the reliability projection D indicates that the storage division is projected to satisfy the reliability threshold at the end of the data retention period T. Based on the reliability projection D the reliability module may keep the storage division in service.

As disclosed above the reliability module may be configured to scan the solid state storage media in a particular scan pattern and or by use of background test read operations. As such time may pass between the programming time T and the time the reliability metric for the storage division is determined e.g. the time the test read operations and or other scanning operations are performed during which the reliability of the storage division may degrade as described above. Accordingly an aged data reliability metric determined at time T may not accurately reflect the reliability of the storage division the aged data reliability metric may incorporate reliability degradation that occurred during T which may include the time between the program time T and the time T the reliability metric was determined e.g. the time the test read operations used to determine the reliability metric were performed . The aged data reliability metric may also reflect non deterministic reliability degradation due to inter alia read disturb write disturb and so on which may vary depending upon usage patterns during T. The aged data reliability metric is depicted on the reliability projection D to illustrate one embodiment of an aged data reliability differential due to reliability degradation during T. Although indicates that the reliability projection C accurately predicted the aged data reliability metric the aged data reliability metric falls on the reliability projection D this may not always be the case due to inter alia non deterministic and or other factors affecting storage division reliability has described herein.

Using an aged data reliability metric to determine whether to retire the storage division may yield inaccurate results. depicts an aged data reliability projection E based on the aged data reliability metric . The aged data reliability projection E indicates that the storage division should be retired the reliability projection E indicates that the storage division is not projected to satisfy the reliability threshold at the end of the data retention period T. However as described above the poor aged data reliability projection E may be due to the time differential T as opposed to actual reliability issues with the storage division . For example in the embodiment the aged data reliability projection E is tantamount to projecting the reliability beyond the data retention threshold T e.g. time shifting the reliability projection to span from T to T T .

In some embodiments the projection module may be configured to correct the aged data reliability projection E which may comprise time shifting and or curve fitting the reliability projection E in accordance with the reliability model and the time differential T. The correction may result in generating the reliability projection D based on the aged data reliability metric . As depicted in the corrected reliability projection may comprise fitting the aged data reliability metric to the reliability projection E.

Alternatively or in addition the reliability module may be configured to include data age when evaluating storage divisions for retirement. The reliability module may be configured to evaluate aged storage divisions differently than non aged storage divisions. In some embodiments the reliability module is configured to defer retirement decisions pertaining to aged storage divisions involving aged data reliability metrics and or projections E or the like. In some embodiments the reliability module may be configured to incorporate an age threshold T into retirement evaluation. As disclosed above an aged storage division refers to a storage division comprising data older than the age threshold T e.g. the differential Tbetween the programming time Tof the storage division and the time that the reliability of the storage division was measured T exceeds the age threshold T .

The age threshold T may be based on one or more of the reliability model configuration user preferences testing and experience or the like. The age threshold T may be configured such that reliability measurements made within the time window defined by the age threshold T the time window from the programming time T and the age threshold T satisfy one or more statistical and or stability criteria e.g. do not deviate by more than a threshold . For example as depicted in the age threshold T may be configured to include a stable portion of the reliability projection E and to cut off before reaching less stable higher rate of change portions.

In some embodiments the reliability module may be configured to retire non aged storage divisions based on a reliability metric and or reliability projection E as disclosed herein. The reliability module may be configured to defer retirement decisions pertaining to aged storage divisions . Accordingly an aged storage division may not be retired even if the storage division is projected to fail the reliability threshold . Aged storage divisions that exhibit poor reliability may be marked for subsequent testing which may comprise grooming the storage division writing data to the storage division and determining a post write reliability metric of the storage division . A storage division having an aged data reliability projection E that fails to satisfy an aged data reliability threshold A and or is projected to fail an aged data reliability threshold B may be marked for post write reliability testing. The aged data reliability threshold B may be the same as the reliability threshold or as depicted in may differ from the reliability threshold .

The reliability module may be configured to evaluate storage divisions A N for retirement based on inter alia data age characteristics. As used herein the storage divisions A N may be individual erase blocks and or groups collections and or sets of erase blocks A N e.g. logical erase blocks portions of one or more logical erase blocks or the like .

As described above reliability metrics of storage divisions A N comprising aged data may be inaccurate e.g. aged data reliability metrics derived from operations performed against data older than an age threshold T . Therefore the reliability module may be configured to evaluate storage divisions A N for retirement based on data age characteristics of the storage divisions A N. Non aged storage divisions A N e.g. storage divisions A N comprising data that is younger than the age threshold T may be evaluated for retirement as described herein. Aged storage divisions A N that exhibit poor reliability characteristics may be scheduled for post write reliability testing.

In some embodiments the reliability module comprises an age module that is configured to determine the age of data stored on the storage divisions A N e.g. as part of one or more test read operation s as described above . Determining the age of the data may comprise determining a last program time of the storage division A N e.g. the last program time of the erase block s comprising the storage divisions A N accessing metadata pertaining to the storage division A N which may include but is not limited to a reverse index a validity bitmap a sequence indicator of the storage division A N a timestamp a sequence number or other sequence indicator storage division metadata maintained by the storage controller time indicator s system clock or the like. For example the storage controller may be configured to store data in a sequential log based format which may comprise marking sequence information on the solid state storage media . The age of data on a storage division A N may be derived from such sequence information. Alternatively or in addition the storage controller may be configured to append data to the solid state storage media at an append point. The age of a storage division A N may be determined based on the sequential order of the storage division A N relative to a current append point.

A storage division A N may be identified as an aged storage division in response to the age of data on the storage division A N exceeding an age threshold T e.g. the differential T between the programming time T of the storage division A N and the time that the reliability of the storage division A N was determined T exceeds the age threshold T . As disclosed above the age threshold T may be based on the reliability model configuration user preferences testing and experience or the like. In some embodiments the age threshold T is 24 hours. When scanning a particular storage division C the reliability module may be configured to determine whether the storage division C is aged based at least in part on the age of the data stored thereon as indicated by the age module . If the storage division C satisfies the age threshold data on the storage division C is younger than the age threshold T the storage division C may be evaluated for retirement as described herein e.g. by determining the reliability metric of the storage division C projecting the reliability metric based on the data retention period Tpp accumulating the reliability metric s of erase blocks A N comprising the storage division C and so on .

The reliability module may be configured to defer retirement decisions pertaining to aged storage divisions A N. In some embodiments the reliability module is configured to evaluate aged storage divisions A N for post write reliability testing. Post write reliability testing may comprise determining a post write reliability metric of the storage division A N which may include grooming the storage division A N programming data onto the storage division A N and evaluating the reliability of the storage division A N within the age threshold T e.g. determining the reliability metric of the storage division such that the differential Tbetween the programming time and the time the reliability metric is determined is less than the age threshold T . The reliability module may determine whether to perform post write reliability testing based on the aged data reliability metric and or aged data reliability projection D of the storage division A N. For example the reliability module may mark a storage division C for post write reliability testing in response to the time differential T of the storage division C exceeding the age threshold T and the aged data reliability metric of the storage division C failing to satisfy an aged data reliability threshold A and or aged data reliability projection failing to satisfy an aged data reliability threshold B. The aged data reliability thresholds A and or B may be different than the reliability threshold used to determine whether to retire non aged storage divisions A N. For example the aged data reliability threshold s A and or B may be more stringent than the reliability threshold since failure to satisfy these thresholds only results in further testing as opposed to retiring a storage resource.

In some embodiments the post write reliability test may comprise grooming the storage division C reprogramming the storage division C e.g. storing data on the storage division C subsequent to grooming as part of normal user and or application workload storage operations and or in one or more test operations and determining a post write reliability metric of the storage division C. Determining the post write reliability metric may comprise performing one or more test read operations using the scan module calculating a reliability metric using the reliability metric module projecting the reliability metric using the projection module and or accumulating the projected reliability metric as described herein.

The reliability module may comprise a marking module configured to mark storage divisions C for post write reliability testing in response to inter alia the reliability module determining that the storage division C comprises aged data and determining that the reliability metric of the storage division C fails to satisfy one or more aged data reliability thresholds A and or B. Marking the storage division C may comprise updating storage metadata such as a forward index reverse index or the like in the logical to physical translation layer to indicate that the storage division C is marked for post write reliability testing.

In some embodiments the marking module may be configured to store a persistent note on the solid state storage media pertaining to the post write reliability test. As used herein a persistent note refers to metadata e.g. a metadata note that is stored on a persistent storage medium such as the solid state storage media . The persistent note may identify the storage division C that is subject to the post write reliability test. The note may further comprise a sequence indicator or other timing metadata associated with the post write reliability test or the storage division C. The storage controller may be configured to access the persistent note in response to grooming and or storage operations which may trigger a post write reliability test on the storage division C as described herein. Storing a persistent note may ensure that the post write reliability test is crash safe such that the post write reliability test will be performed even if volatile metadata is lost due to an unclean shutdown or crash.

The groomer module may be configured to prepare the storage division C for post write reliability testing by grooming the storage division C which may comprise relocating valid data if any on the storage division C to other storage locations and erasing the storage division C so that new data may be programmed thereon. The groomer module may prioritize grooming the storage division C which may reduce the chance that valid data stored on the storage division C will become unrecoverable due to inter alia further reliability degradation excessively high error rate and or other failure conditions. The groomer module may be configured to prioritize grooming the storage division C over other grooming operations e.g. cause the storage division C to be groomed immediately before grooming other storage divisions A N and or over other storage requests e.g. other foreground storage operations .

The storage division C may be reprogrammed after being groomed. The storage division C may be reprogrammed in response to storage requests from one or more storage clients . In some embodiments the data may be mirrored on other storage location s on the solid state storage media or other storage resources to prevent data loss. Alternatively or in addition the reliability module may be configured to program the storage division C with test data as opposed to data of the storage clients . In some embodiments only a portion of the storage capacity of the storage division C may be reprogrammed. Alternatively the storage division C may be fully programmed before the post write reliability metric is calculated.

A post write reliability metric of the storage division C may be determined in response to reprogramming the storage division C. Accordingly grooming the storage division C and or programming the storage division C may be referred to as trigger events for post write reliability testing. In some embodiments the reliability module comprises a trigger module configured to identify storage divisions A N that are ready for post write reliability testing which may comprise identifying storage divisions A N that are marked for post write reliability testing e.g. in the storage metadata and or one or more persistent notes on the solid state storage medium and that have satisfied one or more trigger events for the post write reliability test e.g. have been groomed and or have had a sufficient amount of data programmed thereon . The trigger module may identify such storage divisions A N by monitoring the operation of the storage controller which may include monitoring changes to the storage metadata monitoring the operation of the groomer module e.g. to detect grooming of the marked storage division C monitoring the solid state media controller s e.g. to detect storage of data on the marked storage division C subsequent to grooming and so on. The trigger module may be configured to invoke post write reliability testing in response to identifying completion of the one or more trigger conditions on one or more marked storage divisions A N.

Triggering post write reliability testing may comprise configuring the reliability module to evaluate the storage division C for retirement which may comprise determining a post write reliability metric of the storage division C e.g. determining a reliability metric by use of the reliability metric module based on test read operations performed by the scan module calculating a projected reliability metric based on a reliability model and or determining to retire the storage division C or portions thereof based on the projected reliability metric as described herein. The reliability module may retire the storage division C in response to determining that storage division C fails to satisfy a post write reliability threshold. In some embodiments the post write reliability threshold may be the same as the reliability threshold . Alternatively the post write reliability threshold may differ from the reliability threshold . For example the post write reliability metric may be more stringent to account for the relative recency at which the storage division C was programmed in the post write reliability testing scenario. If the storage division C fails to satisfy the post write reliability threshold the storage division C and or portions thereof may be retired as described above. If the storage division C satisfies the post write reliability threshold the storage division C may remain in service continue to be used to store data . The marking module may be configured to remove the post write reliability test marking s from the storage division C which may comprise removing one or more entries in the storage metadata invaliding one or more persistent notes on the non volatile storage media and or storage metadata and so on. Alternatively or in addition the marking module may be configured to update the marking s associated with the storage division C to indicate that the post write reliability test is complete.

Step may comprise determining a reliability metric of a storage division e.g. the RBER of the storage division . The reliability metric may be based on one or more storage operation s performed on the storage division e.g. one or more read operations . Accordingly step may comprise measuring storage division reliability by use of one or more test storage operations e.g. performed by the scan module the reliability of the storage division may be based on an error rate of the one or more test storage operations. Step may further comprise accessing and or referencing error profiling data obtained by a media controller or the like as described above. The reliability metric may comprise an RBER of the storage division error rate of the one or more test storage operations . The reliability metric may comprise and or incorporate any number of factors pertaining to storage division reliability including but not limited to RBER of the storage division the wear level of the storage division e.g. program erase cycle count performance of the storage division e.g. time to program and or erase retry count e.g. number program retries required and so on. The reliability metric of step may represent the current reliability metric of the storage division e.g. quantify the reliability of the storage division at the time the test and or other operation s upon which the reliability metric is based were performed .

Step may comprise projecting forecasting and or estimating a reliability metric of the storage division at the end of a data retention period T. Step may comprise using a reliability model to determine the projected reliability metric based on the reliability metric of step and or a reliability model as described herein. The projection forecast and or estimate may be based on inter alia the length of the data retention period T temperature erase cycle count program cycle count read count manufacturer specifications testing and experience and so on. The projected reliability metric may comprise a projected RBER of the storage division at the end of a data retention period. Accordingly step may comprise forecasting and or estimating the reliability metric in accordance with a data retention guarantee. Step may comprise one or more of scaling the reliability metric of step by a time based scaling factor applying a reliability model of the storage division and or solid state storage media . As described above the reliability model may include but is not limited to a linear model e.g. a time based scaling function an exponential model a polynomial model a spline model a numerical model an artificial neural network model a radial basis function model a combination of models or the like as described above. Applying the reliability model may comprise incorporating one or more operational factors described above such as temperature storage division wear characteristics and so on.

Step may comprise determining whether the projected reliability metric of the storage division satisfies a reliability threshold and or whether the storage division is forecast or projected to satisfy the data retention guarantee e.g. whether it can be reasonably guaranteed that data stored on the storage division will be readable at the end of the data retention period . The reliability threshold may be based on the number of errors that can be corrected in the data stored on the storage division e.g. the number of errors that can be corrected by the ECC read module by use of an ECC coding of the data or the like data reconstruction data e.g. parity data data mirroring characteristics user configuration testing and experience and or the like. If the projected reliability metric fails to satisfy the reliability threshold the flow may continue to step otherwise the flow may end.

Step may comprise retiring the storage division which may comprise removing the storage division from service e.g. taking the storage division out of service . Step may comprise configuring the storage controller and or OOS management module to avoid the storage division and or to stop using the storage division to store data. Step may comprise updating storage metadata OOS metadata configuring a media controller to remap and or replace the storage division and so on. Step may further comprise grooming the storage division to relocate data stored thereon if any to other storage locations on the solid state storage media and or other storage resource s .

Step may comprise determining a reliability metric of a storage division . In the embodiment the storage division may comprise a plurality of erase blocks A N the storage division may correspond to a logical erase block a bank of solid state storage elements A N or other group collection and or set of erase blocks A N. Step may comprise determining the reliability metric of the two or more erase blocks A N comprising the storage division . Step may comprise accessing and or referencing error profiling data performing test operations on the storage division and so on as described herein. Step may further comprise attributing errors if any detected in the test operations to particular erase blocks A N. The reliability of the particular erase blocks A N may be based on the errors attributed thereto.

Step may comprise projecting forecasting and or estimating the reliability of the two or more erase blocks A N of the storage division at the end of a data retention period T as described herein. The projected reliability metrics may be based on a reliability model which may incorporate various factors including but not limited to time e.g. the data retention period T operating conditions wear levels usage patterns user configuration feedback testing and experience and so on. Step may therefore comprise applying a time based reliability model to the current reliability metrics of the two or more erase blocks A N to determine a projected forecast and or estimated reliability of the respective erase blocks A N in accordance with a data retention guarantee.

Step may comprise projecting forecasting and or estimating the reliability of the storage division at the end of the data retention period T. Step may comprise combining aggregating and or averaging the individual erase block storage division reliability metrics of steps and or e.g. averaging the projected reliability metrics of the two or more erase blocks A N .

Step may comprise determining whether the projected reliability metric determined at step satisfies a reliability threshold and or satisfies a data retention guarantee. As discussed above the reliability threshold may be based on various factors including but not limited to an error correction strength ECC encoding data reconstruction factors data mirroring factors user configuration testing and experience and so on. If the projected reliability metric of step fails to satisfy the reliability threshold the flow may continue to step otherwise the flow may end.

Step may comprise retiring portions of the storage division which may comprise retiring one or more of the erase blocks A N comprising the storage division . Step may comprise retiring erase blocks A N with the poorest projected reliability metric e.g. highest projected RBER . Retiring an erase block A N may comprise remapping a replacement for the erase block A N masking the OOS erase block A N or the like by use of the OOS management module and or OOS write module as described herein. Step may comprise retiring erase blocks A N until the projected reliability metric of the storage division satisfies the reliability threshold e.g. the average RBER of the remaining erase blocks A N satisfies the reliability threshold . If more than a threshold number of erase blocks A N must be retired to satisfy the reliability threshold and or the projected reliability metric of the storage division cannot satisfy the reliability threshold by removing replacing erase blocks A N step may comprise retiring the entire storage division e.g. all erase blocks A N in the storage division may be taken OOS .

Step may comprise determining a reliability metric of a storage division as described above. The storage division of step may comprise one or more erase blocks . Step may comprise performing one or more test read operations on the storage division determining a reliability metric of the storage division and or accumulating the reliability metrics of a plurality of erase blocks A N as described herein. Step may further comprise projecting forecasting and or estimating a projected reliability metric of the storage division at the end of a data retention period T.

Step may comprise determining whether the storage division is aged e.g. determining whether the storage division comprises aged data . Step may comprise determining the age of the data stored on the storage division by use of the age module as described above. Step may further comprising comparing the age of the data to an age threshold e.g. 24 hours which may comprise determining the differential Tbetween the programming time Tof the storage division and the time that the reliability of the storage division was measured and comparing the time differential Tto an age threshold TAT. The storage division may be considered to be aged if the time differential Texceeds the age threshold TAT. If the storage division was programmed within the age threshold T e.g. within 24 hours of determining the reliability metric at step the flow continues at step otherwise the flow continues at step .

Step may comprise evaluating the storage division for retirement as described above. Step may comprise retiring the storage division and or portions thereof in response to determining that the storage division fails to satisfy the reliability threshold . Step may comprise determining a projected forecast and or estimated reliability metric of the storage division at the end of a data retention period T by use of a reliability model. Step may further comprise accumulating reliability metrics of each of a plurality of erase blocks A N and retiring erase block s A N until the accumulated projected reliability metric of the storage division satisfies the reliability threshold or the entire storage division is retired as described herein.

Step may comprise determining whether the storage division should be marked for post write reliability testing. Step may comprise comparing the reliability metric and or projected reliability metric determined at step to an aged data reliability threshold A and or B. The aged data reliability threshold s A and or B may be the same or different than the reliability threshold of step e.g. may be more or less stringent . The storage division may be selected for post write reliability testing in response to failing to satisfy the aged data reliability threshold s A and or B and the flow may continue to step .

Step may comprise marking the storage division for post write reliability testing. Marking the storage division may comprise updating storage metadata storing a persistent note on the solid state storage medium or the like as described herein. Step may further comprise grooming the storage division . The grooming operation may be prioritized over other grooming operations and or other storage operations and may comprise relocating valid data stored on the storage division to other storage locations and or erasing the storage division . Step may further comprise storing data on the storage division after the storage division is groomed e.g. reprogramming the storage division .

In some embodiments step comprises triggering a post write reliability test in response to determining that the storage division has been groomed and or reprogrammed. Triggering may comprise monitoring one or more modules of the storage controller such as the storage metadata groomer media controller or the like. In response to triggering a post write reliability test the reliability module may implement a post write reliability test of the storage division .

Step may comprise triggering a post write reliability test of a storage division . As described herein the storage division may comprise a single erase block or a group collection and or set of erase blocks A N e.g. a logical erase block or portion thereof . Step may comprise determining that a marked storage division is ready for post write reliability testing e.g. the storage division has been groomed and or reprogrammed . Step may be implemented by a trigger module configured to monitor one or more modules of the storage controller such as the storage metadata groomer media controller or the like to identify storage divisions that are ready for post write reliability testing.

As described above data may be stored on a storage division that has been marked for post write reliability testing in response to storage requests from storage clients and or in response to test write operations of the reliability module . Accordingly step may comprise monitoring and or receiving an indication from the solid state media controller in response to storing data to the marked storage division and or reprogramming the marked storage division . Alternatively or in addition the trigger of step may comprise accessing a persistent note stored on the solid state storage medium which as described above may identify the storage division as subject to post write reliability testing.

Step may comprise determining a post write reliability metric of the storage division . The post write reliability metric may be based on one or more test read operations performed after the storage division was groomed and or reprogrammed. Step may comprise projecting the reliability metric to the end of a data retention period T and or accumulating the projected reliability metrics of one or more erase blocks A N of the storage division .

Step may comprise determining whether the projected reliability metric of step satisfies a post write reliability threshold. The post write reliability threshold may be the same as the reliability threshold . Alternatively the post write reliability threshold may differ from the reliability threshold for example the post write reliability threshold of step may be more stringent to account for the recency at which the storage division was programmed. If the projected reliability metric fails to satisfy the post write reliability threshold the flow continues to step otherwise the flow ends.

Step may comprise retiring the storage division and or retiring portions of thereof e.g. one or more erase blocks A N until the accumulated reliability metric of the storage division satisfies the post write reliability threshold of step or the entire storage division is retired as described herein. Step may comprise updating OOS metadata of the OOS management module to ignore avoid and or remap the retired storage division s . Step may further comprise grooming the storage division to relocate valid data stored thereon if any to other storage locations.

This disclosure has been made with reference to various exemplary embodiments. However those skilled in the art will recognize that changes and modifications may be made to the exemplary embodiments without departing from the scope of the present disclosure. For example various operational steps as well as components for carrying out operational steps may be implemented in alternate ways depending upon the particular application or in consideration of any number of cost functions associated with the operation of the system e.g. one or more of the steps may be deleted modified or combined with other steps . Therefore this disclosure is to be regarded in an illustrative rather than a restrictive sense and all such modifications are intended to be included within the scope thereof. Likewise benefits other advantages and solutions to problems have been described above with regard to various embodiments. However benefits advantages solutions to problems and any element s that may cause any benefit advantage or solution to occur or become more pronounced are not to be construed as a critical a required or an essential feature or element. As used herein the terms comprises comprising and any other variation thereof are intended to cover a non exclusive inclusion such that a process a method an article or an apparatus that comprises a list of elements does not include only those elements but may include other elements not expressly listed or inherent to such process method system article or apparatus. Also as used herein the terms coupled coupling and any other variation thereof are intended to cover a physical connection an electrical connection a magnetic connection an optical connection a communicative connection a functional connection and or any other connection.

Additionally principles of the present disclosure may be reflected in a computer program product on a machine readable storage medium having machine readable program code means embodied in the storage medium. Any tangible machine readable storage medium may be utilized including magnetic storage devices hard disks floppy disks and the like optical storage devices CD ROMs DVDs Blu Ray discs and the like flash memory and or the like. These computer program instructions may be loaded onto a general purpose computer special purpose computer programmable computer e.g. FPGA or other processing device to produce a machine such that the instructions that execute on the computer or other programmable data processing apparatus create means for implementing the functions specified. These computer program instructions may also be stored in a machine readable memory that can direct a computer or other programmable data processing apparatus to function in a particular manner such that the instructions stored in the machine readable memory produce an article of manufacture including implementing means that implement the function specified. The computer program instructions may also be loaded onto a computer or other programmable data processing apparatus to cause a series of operational steps to be performed on the computer or other programmable apparatus to produce a computer implemented process such that the instructions that execute on the computer or other programmable apparatus provide steps for implementing the functions specified.

While the principles of this disclosure have been shown in various embodiments many modifications of structure arrangements proportions elements materials and components that are particularly adapted for a specific environment and operating requirements may be used without departing from the principles and scope of this disclosure. These and other changes or modifications are intended to be included within the scope of the present disclosure.

