---

title: Processor having multiple cores, shared core extension logic, and shared core extension utilization instructions
abstract: An apparatus of an aspect includes a plurality of cores and shared core extension logic coupled with each of the plurality of cores. The shared core extension logic has shared data processing logic that is shared by each of the plurality of cores. Instruction execution logic, for each of the cores, in response to a shared core extension call instruction, is to call the shared core extension logic. The call is to have data processing performed by the shared data processing logic on behalf of a corresponding core. Other apparatus, methods, and systems are also disclosed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09582287&OS=09582287&RS=09582287
owner: Intel Corporation
number: 09582287
owner_city: Santa Clara
owner_country: US
publication_date: 20120927
---
Embodiments relate to processors. In particular embodiments relate to processors having multiple cores.

In some processors each of the SIMD execution logic may represent a relatively large amount of logic. For example this may be the case when each of the SIMD execution logic is to process wide SIMD operands. Some processors are able to process vector or packed data operands having relatively wide widths such as for example 128 bit operands 256 bit operands 512 bit operands 1024 bit operands or the like. Commonly the SIMD execution logic needed to process such wide operands tends to be relatively large to consume a relatively large amount of die area to increase the cost of manufacturing the processor and to consume a relatively large amount of power during use. Replicating the relatively large SIMD execution logic per core tends to exacerbate such problems. Moreover in many applications or workload scenarios the replicated SIMD execution logic per core tends to be underutilized at least some of the time. If the number of cores continues to increase in the future such problems may become even more significant.

Still further in the prior art processor of each of the cores also has conventional flow control logic. In particular core 0 has flow control logic core 1 has flow control logic and core M has flow control logic M. Commonly the flow control logic may be designed or optimized to cover a wide range of usage models for example introducing speculative execution. However this generally tends to have a relatively small benefit for SIMD and various other high throughput computations but tends to be accompanied by relatively high power consumption.

Disclosed herein are embodiments of processors having multiple cores and shared core extension logic that is shared by the multiple cores e.g. is operable to perform data processing for each of the cores . Also disclosed herein are shared core extension utilization instructions processors to execute the shared core extension utilization instructions methods performed by the processors when processing or executing the shared core extension utilization instructions and systems incorporating one or more processors to process or execute the shared core extension utilization instructions.

In the following description numerous specific details are set forth such as particular micro architectural details particular command register formats particular shared core extension utilization instruction functionalities particular groups of shared core extension utilization instructions particular types and interrelationships of system components and particular logic partitioning integration details. However it is understood that embodiments of the invention may be practiced without these specific details. In other instances well known circuits structures and techniques have not been shown in detail in order not to obscure the understanding of this description.

The processor has multiple cores . The illustrated processor has a core 0 through a core M M. By way of example there may be two four seven ten sixteen thirty two sixty four one hundred twenty eight or more cores or any other reasonably appropriate number of cores that is desired for the particular implementation. In some embodiments each of the cores may be able to operate substantially independently of the other cores. Each of the cores is able to process at least one thread. As shown in the illustration core 0 has a thread 0 and may optionally include up to a thread P P. Similarly core M has a thread 0 and may optionally include up to a thread P P. The number of threads P may be any reasonably appropriate number of threads. The scope of the invention is not limited to any known number of cores or any known number of threads that those cores are able to process.

The processor may be any of various complex instruction set computing CISC processors various reduced instruction set computing RISC processors various very long instruction word VLIW processors various hybrids thereof or other types of processors entirely. In some embodiments the cores may be general purpose cores of a general purpose processor of the type used in desktop laptop server and like computer systems. In some embodiments the cores may be special purpose cores. Examples of suitable special purpose cores include but are not limited to graphics processor cores digital signal processor DSP cores and network processor cores to name just a few examples. In some embodiments the processor may be a System on Chip SoC having multiple general purpose or special purpose cores and one or more of a graphics unit a media block system memory integrated on chip with the cores.

The processor also includes an embodiment of shared core extension logic . The shared core extension logic is shared by each of the cores e.g. is operable to perform data processing for each of the cores . The shared core extension logic includes shared data processing logic that is operable to perform the data processing for each of the cores. The shared core extension logic and the cores are coupled with one another by one or more busses or other interconnects of the processor. The cores and the shared core extension logic include corresponding interface logics to allow one or more physical threads on each of the cores and the shared core extension logic to interface or interact with one another e.g. for the threads of the cores to call the shared core extension logic to have data processing performed to check on the status of the data processing to abort data processing to synchronize virtual memory attributes on context switches to route page faults occurring during the data processing etc. . The computational tasks executed by the shared core extension logic on behalf of each physical thread may run under the logical process of that specific physical thread. As will be described further below the context used for the interface may be provided per physical thread.

In particular core 0 includes an embodiment of shared core extension interface logic including at least some logic specific to thread 0 on core 0 and at least some logic specific to thread P on core 0. Likewise core M includes an embodiment of shared core extension interface logic M including at least some logic specific to thread 0 on core M and at least some logic specific to thread P on core M. Each of the other cores if any may similarly include such shared core extension interface logic. The shared core extension logic includes an embodiment of corresponding core interface logic . Each core may interface or interact through its corresponding shared core extension interface logic with the core interface logic of the shared core extension logic . In some embodiments the shared core extension interface logic and the core interface logic may provide an architectural interface e.g. new architectural macroinstructions and new architectural registers as well as a micro architectural interface or hardware mechanism e.g. data processing scheduling logic memory management unit MMU synchronization logic page fault routing logic etc. to allow the cores to share the shared core extension logic e.g. share data processing by the shared data processing logic . Detailed example embodiments of the shared core extension interface logic and the core interface logic will be discussed further below.

The shared data processing logic may represent different types of data processing logic in various different embodiments. As previously discussed above in the background section certain types of data processing logic e.g. certain wide SIMD execution units have conventionally been replicated per core. As mentioned before often this replicated logic tends to be relatively large. Moreover often this replicated logic is underutilized at least some of the time for many common workload scenarios. The replication of such logic generally tends to consume a relatively large amount of die area to increase the cost of manufacturing and to consume a relatively large amount of power. In some embodiments such relatively large and or commonly underutilized data processing logic which is conventionally replicated per core may be extracted from the multiple cores into the shared core extension logic as a single shared copy of the data processing logic. Moreover the shared core extension logic may employ flow control logic that is desired or optimized for high throughput as opposed to being designed or optimized to cover a wide range of usage models for example introducing speculative execution as was the case for the conventional flow control logic of the cores of . This generally tends to provide a higher level of power performance efficiency for throughput oriented algorithms.

In various embodiments the shared data processing logic may represent throughput oriented hardware computation function logic a high throughput computation engine matrix multiplication logic matrix transpose logic finite filter logic sum of absolute difference logic histogram computation logic gather scatter instruction implementation logic transcendental vector execution logic or the like. In some embodiments the shared data processing logic may include execution units such as for example SIMD execution units e.g. potentially relatively wide SIMD execution units . In some embodiments the shared core extension logic may interact with shared core extension data structures e.g. matrixes tables etc. for example in the memory .

Advantageously as compared to replicating logic the shared core extension logic may help to reduce one or more of the overall die area needed to implement the logic the cost of manufacturing the logic and or the power consumed by the logic. That is the shared core extension logic may allow multiple cores to share common data processing function evaluation hardware resources without incurring the generally high integration costs of replicating such resources per core. For clarity it is not required that the particular shared data processing logic be large although the greatest benefits of size cost and power reductions will often be achieved when relatively large logic is shared by the cores instead of being replicated per core. Moreover the greatest benefits will often be achieved when the shared logic would otherwise if it had been replicated per core have been relatively underutilized since the sharing may tend to increase the utilization of the logic whereby underutilized or unnecessary logic may be consolidated to reduce die area and manufacturing cost. As a further advantage the shared core extension logic may also potentially be used to allow the cores to be customized or optimized for one type of processing e.g. for scalar workloads performance power and area while allowing the shared core extension logic to be customized or optimized for another type of processing e.g. for throughput oriented workload performance power and area .

Each of one or more physical threads running on the core 0 may use the shared core extension interface logic to interface with the shared core extension logic . The shared core extension interface logic includes shared core extension utilization instructions of an instruction set of the core 0. The instruction set is part of an instruction set architecture ISA of the core. The ISA represents the part of the architecture of the core related to programming. The ISA commonly includes the native instructions architectural registers data types addressing modes memory architecture interrupt and exception handling and the like of the processor. The ISA is distinguished from the micro architecture which generally represents the particular design techniques selected to implement the ISA. Processors or cores with different micro architectures may share a common ISA. The instructions of the instruction set including the shared core extension utilization instructions represent machine instructions macroinstructions or higher level instructions e.g. instructions provided to the core for execution as opposed to microinstructions micro ops or lower level instructions e.g. those which result from decode logic decoding machine instructions or macroinstructions .

The shared core extension interface logic also includes a core 0 thread 0 set of shared core extension command registers SCECRs . Each physical thread may have a set of SCECRs registers associated with it as part of its context to be saved and restored irrespective of the progress of other threads. In some embodiments for the core 0 the there may be multiple sets of SCECRs provided per thread for each of one or more physical threads that run on the core 0. For example in the illustrated embodiment the core 0 thread 0 SCECRs may belong to a thread 0. Similarly each physical thread running on the core 0 may have a set of core 0 thread specific SCECRs to interface with the shared core extension logic . Alternatively there may be a single set of core 0 SCECRs for the core 0. In such cases there may be time sharing of the SCECRs between the physical threads at the hardware level. Context may be swapped out of the core 0 SCECRs on context switches and saved and restored.

In the illustration an SCECR 0 through an SCECR N N are shown. That is there are N 1 registers. The number N 1 may be any desired number such as two four eight sixteen thirty two sixty four or some other number. There is no requirement for the number N 1 to be a power of two although this generally tends to provide efficient register addressing. A given one of these registers is generically represented herein as SCECR x where the x may represent any one of register SCECR 0 through SCECR N.

In some embodiments the shared core extension command registers may be architecturally visible registers of the ISA of the core and or the processor. The architectural registers generally represent on die processor storage locations. The architectural registers may also be referred to herein simply as registers. Unless otherwise specified or apparent the phrases architectural registers and registers are used herein to refer to a registers that are visible to the software and or programmer e.g. software visible and or the registers that are specified by macroinstructions. These registers are contrasted to other non architectural or non architecturally visible registers in a given microarchitecture e.g. temporary registers used by instructions reorder buffers retirement registers etc. .

The shared core extension utilization instructions are used to submit monitor and abort calls to the shared core extension logic for data processing to be performed. By way of example the shared core extension utilization instructions may be used for parallel programming and may be included in an instruction set e.g. as an extension of the instruction set to increase the efficiency and or throughput of parallel programming workloads. The shared core extension utilization instructions may explicitly specify e.g. through bits or one or more fields or otherwise indicate e.g. implicitly indicate a shared core extension command register SCECR x of the core 0 shared core extension command registers . The shared core extension registers may provide an architectural hardware interface of the processor to the shared core extension logic.

In the illustrated embodiment the shared core extension utilization instructions include a shared core extension SCE call instruction that has a format SCE call SCECR x parameters . The SCECR x indicates one of the core 0 shared core extension command registers and the parameters indicate one or more parameters associated with the call which will be discussed further below. The illustrated shared core extension utilization instructions also include an SCE read instruction having a format SCE read SCECR x . Another shared core extension utilization instructions is an SCE abort instruction having a format SCE abort SCECR x . Yet another shared core extension utilization instructions is an SCE wait instruction having a format SCE wait SCECR x . Each of these instructions may include an operation code or opcode e.g. a plurality of bits or one or more fields that is operable to identify the instruction and or the operation to be performed. The functionality of each of these illustrative shared core extension utilization instructions will be discussed further below.

It is to be appreciated that this is just one illustrative example of a suitable set of shared core extension utilization instructions. For example in other embodiments some of the illustrated instructions may optionally be omitted and or additional instructions may optionally be added to the shared core extension utilization instructions. Moreover other shared core extension utilization instructions and sets of them are contemplated and will be apparent to those skilled in the art and having the benefit of the present disclosure.

One of the physical threads running on the core 0 may issue one of the shared core extension utilization instructions . The shared core extension utilization instruction issued by that thread may indicate an appropriate core 0 shared core extension command registers . The appropriate core 0 shared core extension command registers may correspond to the thread e.g. thread 0 and provide context per thread.

Referring again to the core 0 includes decode logic . The decode logic may also be referred to as a decoder or decode unit. The decode logic may receive and decode higher level machine instructions or macroinstructions and output one or more lower level micro operations micro code entry points microinstructions or other lower level instructions or control signals that reflect and or are derived from the original higher level instruction. The one or more lower level control signals may implement the operation of the higher level instruction through one or more lower level e.g. circuit level or hardware level operations. The decoder may be implemented using various different mechanisms including but not limited to microcode read only memories ROMs look up tables hardware implementations programmable logic arrays PLAs and other mechanisms used to perform instruction decoding known in the art. Moreover in some embodiments an instruction emulator translator morpher interpreter or other instruction conversion logic may be used either instead of and or in addition to the decode logic.

SCE instruction execution logic is coupled with the decode logic and with the core 0 shared core extension command registers . The shared core extension instruction execution logic may receive from the decoder one or more micro operations micro code entry points microinstructions other instructions or other control signals which reflect or are derived from the shared core extension utilization instructions. The shared core extension instruction execution logic is operable to perform actions in response to and or as specified by the shared core extension utilization instructions e.g. in response to the control signals from the decoder . In some embodiments the shared core extension instruction execution logic and or the processor may include specific or particular logic e.g. circuitry or other hardware potentially combined with software and or firmware operable to execute and or process the shared core extension utilization instructions and perform actions in response to and or as specified by the shared core extension utilization instructions.

In the illustrated embodiment the shared core extension instruction execution logic is included within shared core extension control logic . The shared core extension control logic is coupled with the shared core extension command registers the decode logic and a memory management unit which will be discussed further below. The shared core extension control logic may assist with various control management coordination timing and related implementation aspects of the shared core extension interface logic .

As mentioned above the instruction set of the core 0 includes the SCE call instruction . The SCE call instruction may be used to submit a call to the shared core extension logic to have data processing performed on behalf of the core e.g. on behalf of a thread running on the core . By way of example a physical or logical thread running on the core 0 may issue an SCE call instruction in order to send a call or command for data processing to be performed to the shared core extension logic. In some embodiments the call or command may be passed through one or more of the shared core extension command registers to the shared core extension logic. For example the shared core extension call instruction of an embodiment may specify or otherwise indicate one of the core 0 shared core extension command registers e.g. SCECR x . That is the shared core extension command registers may be accessible from the thread s on the cores using the new SCE call macroinstruction. In some embodiments the SCE call instruction may also specify or otherwise indicate one of more parameters to further specify qualify or define the data processing that is to be performed. Data may be written or stored in the indicated shared core extension command register e.g. SCECR x based on the SCE call instruction e.g. based on the one or more parameters of the SCE call instruction . If a current SCE call is made to an shared core extension command register that is already dedicated to or occupied by a previous SCE call then the current SCE call may be blocked until the occupied shared core extension command register is released e.g. when the associated call completes or is aborted . Subsequently the shared core extension logic may access the indicated shared core extension command register e.g. SCECR x including the data written or stored therein and may implement the call or command e.g. perform the requested data processing .

A SCE call instruction is received within a core of a processor having a plurality of cores at block . In various aspects the SCE call instruction may be received at the core from an off core source e.g. from a main memory a disc or a bus or interconnect or may be received at a portion of the core e.g. at decode logic scheduling logic etc. from other logic within the core e.g. an instruction cache queue scheduling logic etc. . The SCE call instruction is to cause the core to call shared core extension logic to have data processing performed. The shared core extension logic is shared by the plurality of cores. The SCE call instruction indicates a shared core extension command register and also indicates one or more parameters. The one or more parameters specify the data processing that is to be performed by the shared core extension logic.

In some embodiments the one or more parameters may provide one or more of a pointer e.g. explicit virtual memory pointers to a command attribute data structure in memory having command attributes associated with the call one or more pointers e.g. one or more explicit virtual memory pointers to one or more input data operands in memory upon which data processing is to be performed and one or more pointers e.g. one or more explicit virtual memory pointers to one or more output data operands in memory where results of the data processing are to be stored. For example in some embodiments the one or more parameters may provide information to be stored in and or used to derive the fields shown in which will be discussed further below. Alternatively in other embodiments one or more fields may have direct encodings of opcodes and arguments instead of memory pointers.

The shared core extension logic is called in response to the SCE call instruction to have the data processing performed at block . In some embodiments calling the shared core extension logic may include writing or otherwise storing data in the shared core extension command register indicated by the instruction based on the one or more parameters indicated by the instruction.

The status field may be used to provide a status of the call corresponding to the shared core extension command register. Examples of such status include but are not limited to the call is valid e.g. it is in progress the call has been completed the call has an error or the like. By way of example two bits may be used to specify any of the aforementioned three status conditions. In another example a single bit may be used to encode either of two status conditions such as valid and invalid. The valid may represent that the call is currently in progress. The invalid may indicate that an error has occurred.

The progress field may be used to provide a progress of the call corresponding to the shared core extension command register. The progress may represent a level of completion progress or how far the call or command has progressed toward completion. The progress field may effectively implement a counter of sorts that counts the amount of work completed so far in executing the call. In some embodiments the progress may be represented by atomic commit points. For example the counter may be incremented whenever an atomic sub operation is completed by the SCE logic. The atomic sub operation may vary from one type of data processing to another e.g. in one example when a certain number of cache lines of data have been processed . In some embodiments the progress field may be used to provide progress atomicity with respect to the data processing of the shared core extension logic and an ability to pre empt and re schedule a running command on the shared core extension logic. When execution of a call is interrupted e.g. on a context switch from one thread to another or on a fault the progress field may be saved. Later the progress field may be restored and the data processing associated with the call resumed e.g. when the thread resubmits . Restoring the progress field may allow the data processing to resume where it left off. This is useful especially when the amount of data processing to be performed by the SCE logic is relatively large and or takes a relatively large amount of time to complete.

The command pointer field may be used to provide a pointer that points to call or command attribute information of the call corresponding to the shared core extension command register. In some embodiments the call attribute information may be included in a call attribute data structure. In some embodiments the call attribute information may be stored at one or more memory locations in a memory . In some embodiments the pointer may be an explicit virtual memory pointer. The call attribute information may further specify qualify define or characterize the attributes of the call. For example the call attribute information may further specify qualify define or characterize the precise type of data processing that is to be performed by the shared core extension logic. In some embodiments the commands attributes may describe processing that represents relatively simple or short processing routines or functions such as for example operations to transpose a matrix operations to generate a histogram operations to perform a filter or the like. The command attributes may describe a sequence of operations to perform on one or more input data operands e.g. one or more input data structures to produce one or more output data operands e.g. one or more output data structures . In some embodiments they may be any of various such relatively simple algorithms or routines typically performed in hardware accelerators or graphics processing units or the like.

The input data operand s pointer s field may be used to provide one or more pointers that point to one or more input data operands. The input data operands are those on which data processing is to be performed by the shared core extension logic. In some embodiments the one or more input data operands may represent one or more data structures such as for example matrices tables etc. As shown in some embodiments the pointer s may point to input data operand s in memory location s in the memory . In some embodiments the pointer s may be explicit virtual memory pointer s . In other embodiments the pointers may point to one or more input data operands in one or more registers or other storage locations.

The output data operand s pointer s field may be used to provide one or more pointers that point to one or more output data operands. The output data operands are those used to convey results of the data processing that has been performed by the shared core extension logic at the completion of the call. In some embodiments the one or more output data operands may represent one or more data structures such as for example matrices tables etc. As shown in some embodiments the pointer s may point to output data operand s in memory location s in the memory. In some embodiments the pointer s may be explicit virtual memory pointer s . In other embodiments the pointers may point to one or more output data operands in one or more registers or other storage locations.

It is to be appreciated that this is just one example embodiment of a suitable format for a shared core extension command register. Alternate embodiments may omit some of the illustrated fields or may add additional fields. For example one or more of the fields may be provided through an implicit location that need not be explicitly specified in the shared core extension command register. As another example an input data operand storage location may be reused as an output data operand storage location such that it need not be specified twice but one of the specifications may be implicit. As yet another example one or more fields may have direct encodings of opcodes and arguments instead of memory pointers. Moreover the illustrated order arrangement of the fields is not required but rather the fields may be rearranged. Furthermore fields need not include contiguous sequences of bits as suggested in the illustration but rather may be composed of non contiguous or separated bits.

Referring again to after the execution of the SCE call instruction an shared core extension command register indicated by the SCE call instruction e.g. SCECR x may store data corresponding to the SCE call instruction. After the thread or core submits the task or call the thread or core may proceed to prepare and submit additional calls or tasks to the shared core extension logic before the earlier submitted calls or tasks complete. Additionally the thread or core may proceed to perform other processing while the previously submitted calls or tasks complete. The shared core extension command registers together with a scheduler which will be discussed further below may help to provide fine grain control flow which may allow multiple threads and or multiple cores to submit tasks or calls and then proceed to submit other tasks or calls or perform other processing while and until the tasks or calls complete on the shared core extension logic.

The shared core extension logic includes core interface logic to access the core 0 shared core extension command registers . The core interface logic may also be used to access the core M shared core extension command registers as well as for any other cores if any . That is in some embodiments the shared core extension logic and or the core interface logic may access a separate set of shared core extension command registers for each of the cores.

The shared core extension logic may use the shared core extension command registers . For example the shared core extension logic may access the command attribute information pointed to by the command field e.g. field may access the input data operands pointed to by the input data operands field e.g. field may update progress as a result of data processing in the progress field e.g. field when the operation is done or encounters an error may update the status field e.g. field to reflect complete or an error and in the event of completion without an error may access the output data operands through the pointer in the output data operands field e.g. field .

To facilitate the description the shared core extension logic is shown as having a copy of the core 0 thread 0 shared core extension command registers. However the shared core extension command registers of the shared core extension logic are shown in dashed lines to indicate that there may not actually be two sets of the core 0 thread 0 shared core extension command registers. Rather both the core 0 and the shared core extension logic may logically view the same set of core 0 thread 0 shared core extension command registers. Similarly the shared core extension logic may view the corresponding shared core extension command registers of other threads of other processors through potentially a core M thread P set . Also for clarity the physical core 0 thread 0 shared core extension command registers may be located in the core 0 in the shared core extension logic in a location outside the core 0 and outside the shared core extension logic or in a combination of different locations.

The shared core extension logic includes an embodiment of a scheduler . The scheduler may be implemented in hardware software firmware or some combination. In one aspect the scheduler may be a hardware scheduler. The scheduler may be operable to access the core 0 shared core extension command registers through the core M shared core extension command registers and schedule data processing associated with calls conveyed through these registers on the shared data processing logic . In some embodiments the scheduler may represent a programmable hardware scheduler or programmable hardware scheduling logic to schedule the data processing for the cores according to a programmable scheduling algorithm or objective. In some embodiments the hardware scheduler may be implemented as a state machine that is operable to rotate between command registers and between physical threads. Arbitration policies may potentially be exposed to software through a set of machine specific registers MSRs . In other embodiments the hardware scheduler may be implemented as a firmware block for example incorporating both fixed read only memory ROM and patchable random access memory RAM domains. This may potentially allow the hardware scheduler to use more elaborate scheduling algorithms which may rely on operating system directives application programming interfaces APIs run time compiler directives real time hardware signals or a combination of such controls. By way of example the scheduling may be a fair scheduling algorithm a weighted scheduling algorithm for some of the cores over others e.g. based on core load time criticality of the thread or data being processed thread priority or according to other objectives . Many different types of scheduling algorithms known in the arts are suitable for different implementations depending upon the particular objectives of those implementations. The scheduler may also monitor the completion of the calls or tasks scheduled on the shared data processing logic.

The shared core extension logic also includes status and or progress update logic . The status and or progress update logic may monitor the status and or progress of the calls being handled by the shared data processing logic . The status and or progress update logic may also update shared core extension command registers corresponding to the calls based on the monitored status and or progress. For example the status field and the progress field of may be updated. By way of example when a call completes on the shared core extension logic the status may be updated to reflect completed or when processing of a call on the shared core extension logic encounter an error the status may be updated to reflect an error condition. As another example throughout the data processing associated with a call the status and or progress update logic may update the progress of the completion of the call e.g. may update atomic commit points in the progress field .

In some embodiments an operating system may use a state save state restore functionality e.g. xsave xrestore in Intel Architecture to manage the state of shared core extension command registers on context switches. Calls or commands that have not yet been completed by the shared core extension logic may be saved and then restored and re launched by the physical thread on a context switch. In some embodiments to support context switch and operating system pre emption the shared core extension command registers may have the aforementioned progress field to record e.g. atomic progress of the data processing task being handled by the shared core extension logic. The progress field may be saved on context switch as part of the thread context and used for task resumption when the operating system reschedules the thread.

The shared core extension logic also includes shared core extension control logic . The shared core extension control logic is coupled with the scheduler the shared data processing logic the status progress update logic the core 0 M shared core extension command registers and a shared core extension memory management unit MMU which will be discussed further below. The shared core extension control logic may assist with various control management coordination timing and related implementation aspects of the shared core extension logic .

Refer again to the SCE call instruction of and or the SCE call instruction of the method of in some embodiments the SCE call instruction may be a non blocking SCE call instruction. In some embodiments the non blocking SCE call instruction may be sent non speculatively from a thread e.g. a physical thread and may retire at a core on which the issuing thread is running after the non blocking SCE call instruction has been accepted for execution at the shared core extension logic e.g. stored in the SCE command register .

In other embodiments the SCE call instruction may be a blocking SCE call instruction. In some embodiments the blocking SCE call instruction may be sent non speculatively from a thread e.g. a physical thread and may retire at a core on which the issuing thread is running after execution of the call or task has completed at the shared core extension logic e.g. when the status field of the shared core extension command register is updated to reflect completed . In some embodiments both non blocking and blocking variants of SCE call instructions may be included in the instruction set.

In some embodiments a blocking SCE call instruction may specify or otherwise indicate a timeout value e.g. a number of cycles to wait for a shared core extension command register release. For example this number of cycles or other timeout value may be specified in one of the parameters of the SCE call instruction. In some embodiments a failure fault error or the like may be returned in response to the call if the timeout value is reached without the shared core extension command register being released.

Following the retirement of an SCE call instruction the shared core extension logic may modify memory state according to the assigned task or call. In a multi threaded environment software synchronization may be performed to maintain cache coherency and memory ordering between logical threads that may use the shared core extension and have shared operands. Alternatively hardware synchronization may also optionally be performed.

A shared core extension SCE read instruction is received within a core of a processor having a plurality of cores at block . In various aspects the SCE read instruction may be received at the core from an off core source e.g. from a main memory a disc or a bus or interconnect or may be received at a portion of the core e.g. at decode logic scheduling logic etc. from other logic within the core e.g. an instruction cache queue scheduling logic etc. . The SCE read instruction to cause the core to read a status of a previously made call to shared core extension logic. The shared core extension logic is shared by the plurality of cores. The SCE read instruction indicates a shared core extension command register.

The status of the previously made call to the shared core extension logic is read in response to the SCE read instruction at block . In some embodiments reading the status may include reading data from the shared core extension command register indicated by the instruction. In some embodiments the status may include completion status. For example a status field e.g. the status field in may be read. In some embodiments the read status may be selected from completed error valid although the scope of the invention is not so limited.

In other embodiments the SCE read instruction may read other information from the indicated shared core extension command register. Examples of such information include but are not limited to progress e.g. from progress field of an output data operand or a portion thereof e.g. as indicated by field and command attribute information e.g. as indicated by field . In some embodiments the shared core extension command register corresponds to a previous call to the shared core extension logic to have data processing be performed on behalf of the core receiving the SCE read instruction.

A shared core extension SCE abort instruction is received within a core of a processor having a plurality of cores at block . In various aspects the SCE abort instruction may be received at the core from an off core source e.g. from a main memory a disc or a bus or interconnect or may be received at a portion of the core e.g. at decode logic scheduling logic etc. from other logic within the core e.g. an instruction cache queue scheduling logic etc. . The SCE abort instruction is to cause the core to abort a previously made call to shared core extension logic. The shared core extension logic is shared by the plurality of cores. The SCE abort instruction indicates a shared core extension command register.

The previously made call to the shared core extension logic is aborted in response to the SCE abort instruction at block . In some embodiments aborting the call may include stopping data processing by the shared core extension logic that corresponds to the previously made call and or that corresponds to the indicated shared core extension command register. In some embodiments aborting the call may also include releasing the occupied shared core extension command register indicated by the SCE abort instruction.

In some embodiments a blocking SCE call instruction may specify or otherwise indicate a timeout value e.g. a number of cycles to wait for a SCECR release and the call may return a failure if the timeout elapses. The failure may occur either if the timeout is reached without the release and or if the timeout is reached during in progress command execution that has not completed prior to the expiration of the timeout. For non blocking call a SCE wait instruction may be used to block on shared core extension execution. The SCE wait instruction may similarly include a timeout value e.g. a number of cycles to wait for a shared core extension command register release. A failure error or the like may be returned if the timeout elapses without the shared core extension command register release. In some embodiments the timeout value of the blocking SCE call instruction and or the SCE wait instruction may be encoded as a variable parameter that the instruction may specify. In other embodiments the timeout may be a fixed implicit value. In some embodiments the SCE wait instruction may be used in conjunction with a non blocking SCE call instruction to reduce power consumption. For example when a blocking SCE call instruction blocks and or when an SCE wait instruction blocks the physical thread may optionally be halted and put to sleep assuming there is no other work that is desired to be done until the shared core extension logic wakes it on the relevant SCE call being completed. However this is optional and not required. Moreover other methods for aborting a call or command that runs for an unexpected or undesired long duration are also contemplated besides the aforementioned approach of indicating a timeout value through a blocking SCE call instruction and or a SCE wait instruction.

In some embodiments the SCE logic may operate on the same virtual memory as the core 0. Referring again to the core 0 has a memory management unit MMU . The MMU includes shared core extension MMU interface logic . The MMU may be substantially conventional except for the shared core extension MMU interface logic . The shared core extension logic has a shared core extension MMU . The SCE MMU may maintain the page mapping of the core 0 e.g. cache or preserve the translations from virtual or linear memory to system memory that are cached or preserved by the core 0 . In addition to maintaining TLB entries corresponding to those of the TLB of core 0 the SCE MMU may also maintain TLB entries for each of the cores. The shared core extension MMU has core MMU interface logic . The shared core extension MMU interface logic and the core MMU interface logic interface with one another to perform synchronization between the MMU and the shared core extension MMU . In some embodiments the shared core extension MMU interface logic and the core MMU interface logic may represent a hardware mechanism or hardware support for synchronization of the MMU and the shared core extension MMU .

In some embodiments synchronization between the MMU and the SCE MMU may be performed to maintain consistency in this page mapping. For example when a page is invalidated by the core 0 the core 0 may invalidate a corresponding TLB entry of the core 0 MMU. In some embodiments synchronization may also be performed between the core 0 and the SCE logic in which a corresponding TLB entry on the SCE MMU of the SCE logic may also be correspondingly invalidated. By way of example a physical thread running on the core 0 may use the hardware interface provided by the shared core extension MMU interface logic and the core MMU interface logic to signal the SCE logic to invalidate the corresponding TLB of the SCE MMU through bus cycles on the processor. That is in some embodiments the synchronization of the shared core extension MMU may be performed by hardware from within a physical thread running on the core 0. As another example if a thread is swapped by the operating system e.g. a context switch then the SCE logic may be signaled and or notified of the context switch so that the context associated with the thread may be saved so that they can be later restored. In some embodiments such synchronization signaling may be at the hardware level e.g. through bus cycles or bus transactions through a hardware mechanism . That is the synchronization may be performed at the hardware level e.g. through the hardware of the MMU and SCE MMU and bus transactions rather than through software involvement e.g. without involvement of the operating system .

In some embodiments the MMU and the shared core extension MMU may also interact through the interface logic to route or communicate page faults that occurring when the shared core extension logic is processing calls for the core 0. In some embodiments the shared core extension MMU may use the core 0 to notify the operating system of a page fault that has occurred while processing a call from the core 0. Similarly the shared core extension MMU may notify other cores of page faults that occur while processing calls from these other cores. The cores may notify the operating system of the page faults. The operating system may not have any reason to know that the page fault actually originated at the SCE logic rather than at the core that provided the page fault. In some embodiments for a non blocking SCE call instruction the instruction pointer on the core specifying the fault may be arbitrary. In some embodiments for a blocking SCE call instruction the instruction pointer for the faulting shared core extension logic may point to the SCE call instruction corresponding to the call that faulted on the calling thread.

The shared core extension logic offers a number of advantages over other approaches for offloading processing known in the arts. Conventionally with hardware accelerators e.g. graphics processing units and the like a software based paradigm is used to interact with the hardware accelerators. The hardware accelerators are commonly managed by software device drivers. System calls are used by applications to utilize the processing of the hardware accelerators. Intervention of software e.g. the operating system is often needed to provide fair utilization of hardware accelerator by different threads running on the cores. As compared to such hardware accelerators the shared core extension logic may allow a traditional programming paradigm of the cores utilizing the shared core extension logic e.g. general purpose cores without shifting to the software paradigm of driver based hardware accelerator access. Moreover in embodiments where the SCE logic operates on the same virtual memory as associated physical threads it can be utilized without an accompanying overhead of data copying and or data marshaling. Furthermore as compared to a hardware accelerator the shared core extension logic generally involves a smaller amount of open pages for making forward progress. In addition as compared to a hardware accelerator the shared core extension logic generally tends to reduce the latency overhead of submitting a command substantially to approximately the latency of a non speculative core bus cycle. Also the SCE logic may use a scheduling unit in hardware or other logic on processor to provide fair or distributed utilization among different threads running on cores rather than through intervention of software e.g. the operating system .

In the description above for simplicity of illustration and description embodiments have shown and described a single instance of shared core extension logic e.g. logic logic etc. . However in some embodiments there may be more than one shared core extension logic. Each of the shared core extension logic may be shared by multiple cores which may be either the same cores or different cores and which may be either all of the cores or some of the cores. In some embodiments different types of shared core extension logic e.g. to perform different types of data processing may be included and shared among the cores. In other cases multiple instances of the same general type of shared core extension logic may be included and shared among either all of the cores e.g. their threads or each of the shared core extension logic may be shared by a subset of the total number of cores e.g. a different subset . Various arrangements are contemplated as will be appreciated by those skilled in the art and having the benefit of the present disclosure.

Components features and specific details described for may optionally be used with those of or . The features and or details described herein for an apparatus also optionally apply to the methods described herein which are performed by and or with an apparatus. For example components features and specific details described for may optionally be used with those of or .

Processor cores may be implemented in different ways for different purposes and in different processors. For instance implementations of such cores may include 1 a general purpose in order core intended for general purpose computing 2 a high performance general purpose out of order core intended for general purpose computing 3 a special purpose core intended primarily for graphics and or scientific throughput computing. Implementations of different processors may include 1 a CPU including one or more general purpose in order cores intended for general purpose computing and or one or more general purpose out of order cores intended for general purpose computing and 2 a coprocessor including one or more special purpose cores intended primarily for graphics and or scientific throughput . Such different processors lead to different computer system architectures which may include 1 the coprocessor on a separate chip from the CPU 2 the coprocessor on a separate die in the same package as a CPU 3 the coprocessor on the same die as a CPU in which case such a coprocessor is sometimes referred to as special purpose logic such as integrated graphics and or scientific throughput logic or as special purpose cores and 4 a system on a chip that may include on the same die the described CPU sometimes referred to as the application core s or application processor s the above described coprocessor and additional functionality. Exemplary core architectures are described next followed by descriptions of exemplary processors and computer architectures.

In a processor pipeline includes a fetch stage a length decode stage a decode stage an allocation stage a renaming stage a scheduling also known as a dispatch or issue stage a register read memory read stage an execute stage a write back memory write stage an exception handling stage and a commit stage .

The front end unit includes a branch prediction unit coupled to an instruction cache unit which is coupled to an instruction translation lookaside buffer TLB which is coupled to an instruction fetch unit which is coupled to a decode unit . The decode unit or decoder may decode instructions and generate as an output one or more micro operations micro code entry points microinstructions other instructions or other control signals which are decoded from or which otherwise reflect or are derived from the original instructions. The decode unit may be implemented using various different mechanisms. Examples of suitable mechanisms include but are not limited to look up tables hardware implementations programmable logic arrays PLAs microcode read only memories ROMs etc. In one embodiment the core includes a microcode ROM or other medium that stores microcode for certain macroinstructions e.g. in decode unit or otherwise within the front end unit . The decode unit is coupled to a rename allocator unit in the execution engine unit .

The execution engine unit includes the rename allocator unit coupled to a retirement unit and a set of one or more scheduler unit s . The scheduler unit s represents any number of different schedulers including reservations stations central instruction window etc. The scheduler unit s is coupled to the physical register file s unit s . Each of the physical register file s units represents one or more physical register files different ones of which store one or more different data types such as scalar integer scalar floating point packed integer packed floating point vector integer vector floating point status e.g. an instruction pointer that is the address of the next instruction to be executed etc. In one embodiment the physical register file s unit comprises a vector registers unit a write mask registers unit and a scalar registers unit. These register units may provide architectural vector registers vector mask registers and general purpose registers. The physical register file s unit s is overlapped by the retirement unit to illustrate various ways in which register renaming and out of order execution may be implemented e.g. using a reorder buffer s and a retirement register file s using a future file s a history buffer s and a retirement register file s using a register maps and a pool of registers etc. . The retirement unit and the physical register file s unit s are coupled to the execution cluster s . The execution cluster s includes a set of one or more execution units and a set of one or more memory access units . The execution units may perform various operations e.g. shifts addition subtraction multiplication and on various types of data e.g. scalar floating point packed integer packed floating point vector integer vector floating point . While some embodiments may include a number of execution units dedicated to specific functions or sets of functions other embodiments may include only one execution unit or multiple execution units that all perform all functions. The scheduler unit s physical register file s unit s and execution cluster s are shown as being possibly plural because certain embodiments create separate pipelines for certain types of data operations e.g. a scalar integer pipeline a scalar floating point packed integer packed floating point vector integer vector floating point pipeline and or a memory access pipeline that each have their own scheduler unit physical register file s unit and or execution cluster and in the case of a separate memory access pipeline certain embodiments are implemented in which only the execution cluster of this pipeline has the memory access unit s . It should also be understood that where separate pipelines are used one or more of these pipelines may be out of order issue execution and the rest in order.

The set of memory access units is coupled to the memory unit which includes a data TLB unit coupled to a data cache unit coupled to a level 2 L2 cache unit . In one exemplary embodiment the memory access units may include a load unit a store address unit and a store data unit each of which is coupled to the data TLB unit in the memory unit . The instruction cache unit is further coupled to a level 2 L2 cache unit in the memory unit . The L2 cache unit is coupled to one or more other levels of cache and eventually to a main memory.

By way of example the exemplary register renaming out of order issue execution core architecture may implement the pipeline as follows 1 the instruction fetch performs the fetch and length decoding stages and 2 the decode unit performs the decode stage 3 the rename allocator unit performs the allocation stage and renaming stage 4 the scheduler unit s performs the schedule stage 5 the physical register file s unit s and the memory unit perform the register read memory read stage the execution cluster perform the execute stage 6 the memory unit and the physical register file s unit s perform the write back memory write stage 7 various units may be involved in the exception handling stage and 8 the retirement unit and the physical register file s unit s perform the commit stage .

The core may support one or more instructions sets e.g. the x86 instruction set with some extensions that have been added with newer versions the MIPS instruction set of MIPS Technologies of Sunnyvale Calif. the ARM instruction set with optional additional extensions such as NEON of ARM Holdings of Sunnyvale Calif. including the instruction s described herein. In one embodiment the core includes logic to support a packed data instruction set extension e.g. AVX1 AVX2 thereby allowing the operations used by many multimedia applications to be performed using packed data.

It should be understood that the core may support multithreading executing two or more parallel sets of operations or threads and may do so in a variety of ways including time sliced multithreading simultaneous multithreading where a single physical core provides a logical core for each of the threads that physical core is simultaneously multithreading or a combination thereof e.g. time sliced fetching and decoding and simultaneous multithreading thereafter such as in the Intel Hyperthreading technology .

While register renaming is described in the context of out of order execution it should be understood that register renaming may be used in an in order architecture. While the illustrated embodiment of the processor also includes separate instruction and data cache units and a shared L2 cache unit alternative embodiments may have a single internal cache for both instructions and data such as for example a Level 1 L1 internal cache or multiple levels of internal cache. In some embodiments the system may include a combination of an internal cache and an external cache that is external to the core and or the processor. Alternatively all of the cache may be external to the core and or the processor.

The local subset of the L2 cache is part of a global L2 cache that is divided into separate local subsets one per processor core. Each processor core has a direct access path to its own local subset of the L2 cache . Data read by a processor core is stored in its L2 cache subset and can be accessed quickly in parallel with other processor cores accessing their own local L2 cache subsets. Data written by a processor core is stored in its own L2 cache subset and is flushed from other subsets if necessary. The ring network ensures coherency for shared data. The ring network is bi directional to allow agents such as processor cores L2 caches and other logic blocks to communicate with each other within the chip. Each ring data path is 1012 bits wide per direction.

Thus different implementations of the processor may include 1 a CPU with the special purpose logic being integrated graphics and or scientific throughput logic which may include one or more cores and the cores A N being one or more general purpose cores e.g. general purpose in order cores general purpose out of order cores a combination of the two 2 a coprocessor with the cores A N being a large number of special purpose cores intended primarily for graphics and or scientific throughput and 3 a coprocessor with the cores A N being a large number of general purpose in order cores. Thus the processor may be a general purpose processor coprocessor or special purpose processor such as for example a network or communication processor compression engine graphics processor GPGPU general purpose graphics processing unit a high throughput many integrated core MIC coprocessor including 30 or more cores embedded processor or the like. The processor may be implemented on one or more chips. The processor may be a part of and or may be implemented on one or more substrates using any of a number of process technologies such as for example BiCMOS CMOS or NMOS.

The memory hierarchy includes one or more levels of cache within the cores a set or one or more shared cache units and external memory not shown coupled to the set of integrated memory controller units . The set of shared cache units may include one or more mid level caches such as level 2 L2 level 3 L3 level 4 L4 or other levels of cache a last level cache LLC and or combinations thereof. While in one embodiment a ring based interconnect unit interconnects the integrated graphics logic the set of shared cache units and the system agent unit integrated memory controller unit s alternative embodiments may use any number of well known techniques for interconnecting such units. In one embodiment coherency is maintained between one or more cache units and cores A N.

In some embodiments one or more of the cores A N are capable of multi threading. The system agent includes those components coordinating and operating cores A N. The system agent unit may include for example a power control unit PCU and a display unit. The PCU may be or include logic and components needed for regulating the power state of the cores A N and the integrated graphics logic . The display unit is for driving one or more externally connected displays.

The cores A N may be homogenous or heterogeneous in terms of architecture instruction set that is two or more of the cores A N may be capable of execution the same instruction set while others may be capable of executing only a subset of that instruction set or a different instruction set.

Referring now to shown is a block diagram of a system in accordance with one embodiment of the present invention. The system may include one or more processors which are coupled to a controller hub . In one embodiment the controller hub includes a graphics memory controller hub GMCH and an Input Output Hub IOH which may be on separate chips the GMCH includes memory and graphics controllers to which are coupled memory and a coprocessor the IOH is couples input output I O devices to the GMCH . Alternatively one or both of the memory and graphics controllers are integrated within the processor as described herein the memory and the coprocessor are coupled directly to the processor and the controller hub in a single chip with the IOH .

The optional nature of additional processors is denoted in with broken lines. Each processor may include one or more of the processing cores described herein and may be some version of the processor .

The memory may be for example dynamic random access memory DRAM phase change memory PCM or a combination of the two. For at least one embodiment the controller hub communicates with the processor s via a multi drop bus such as a frontside bus FSB point to point interface such as QuickPath Interconnect QPI or similar connection .

In one embodiment the coprocessor is a special purpose processor such as for example a high throughput MIC processor a network or communication processor compression engine graphics processor GPGPU embedded processor or the like. In one embodiment controller hub may include an integrated graphics accelerator.

There can be a variety of differences between the physical resources in terms of a spectrum of metrics of merit including architectural microarchitectural thermal power consumption characteristics and the like.

In one embodiment the processor executes instructions that control data processing operations of a general type. Embedded within the instructions may be coprocessor instructions. The processor recognizes these coprocessor instructions as being of a type that should be executed by the attached coprocessor . Accordingly the processor issues these coprocessor instructions or control signals representing coprocessor instructions on a coprocessor bus or other interconnect to coprocessor . Coprocessor s accept and execute the received coprocessor instructions.

Referring now to shown is a block diagram of a first more specific exemplary system in accordance with an embodiment of the present invention. As shown in multiprocessor system is a point to point interconnect system and includes a first processor and a second processor coupled via a point to point interconnect . Each of processors and may be some version of the processor . In one embodiment of the invention processors and are respectively processors and while coprocessor is coprocessor . In another embodiment processors and are respectively processor coprocessor .

Processors and are shown including integrated memory controller IMC units and respectively. Processor also includes as part of its bus controller units point to point P P interfaces and similarly second processor includes P P interfaces and . Processors may exchange information via a point to point P P interface using P P interface circuits . As shown in IMCs and couple the processors to respective memories namely a memory and a memory which may be portions of main memory locally attached to the respective processors.

Processors may each exchange information with a chipset via individual P P interfaces using point to point interface circuits . Chipset may optionally exchange information with the coprocessor via a high performance interface . In one embodiment the coprocessor is a special purpose processor such as for example a high throughput MIC processor a network or communication processor compression engine graphics processor GPGPU embedded processor or the like.

A shared cache not shown may be included in either processor or outside of both processors yet connected with the processors via P P interconnect such that either or both processors local cache information may be stored in the shared cache if a processor is placed into a low power mode.

Chipset may be coupled to a first bus via an interface . In one embodiment first bus may be a Peripheral Component Interconnect PCI bus or a bus such as a PCI Express bus or another third generation I O interconnect bus although the scope of the present invention is not so limited.

As shown in various I O devices may be coupled to first bus along with a bus bridge which couples first bus to a second bus . In one embodiment one or more additional processor s such as coprocessors high throughput MIC processors GPGPU s accelerators such as e.g. graphics accelerators or digital signal processing DSP units field programmable gate arrays or any other processor are coupled to first bus . In one embodiment second bus may be a low pin count LPC bus. Various devices may be coupled to a second bus including for example a keyboard and or mouse communication devices and a storage unit such as a disk drive or other mass storage device which may include instructions code and data in one embodiment. Further an audio I O may be coupled to the second bus . Note that other architectures are possible. For example instead of the point to point architecture of a system may implement a multi drop bus or other such architecture.

Referring now to shown is a block diagram of a second more specific exemplary system in accordance with an embodiment of the present invention Like elements in bear like reference numerals and certain aspects of have been omitted from in order to avoid obscuring other aspects of .

Referring now to shown is a block diagram of a SoC in accordance with an embodiment of the present invention. Similar elements in bear like reference numerals. Also dashed lined boxes are optional features on more advanced SoCs. In an interconnect unit s is coupled to an application processor which includes a set of one or more cores A N and shared cache unit s a system agent unit a bus controller unit s an integrated memory controller unit s a set or one or more coprocessors which may include integrated graphics logic an image processor an audio processor and a video processor an static random access memory SRAM unit a direct memory access DMA unit and a display unit for coupling to one or more external displays. In one embodiment the coprocessor s include a special purpose processor such as for example a network or communication processor compression engine GPGPU a high throughput MIC processor embedded processor or the like.

Embodiments of the mechanisms disclosed herein may be implemented in hardware software firmware or a combination of such implementation approaches. Embodiments of the invention may be implemented as computer programs or program code executing on programmable systems comprising at least one processor a storage system including volatile and non volatile memory and or storage elements at least one input device and at least one output device.

Program code such as code illustrated in may be applied to input instructions to perform the functions described herein and generate output information. The output information may be applied to one or more output devices in known fashion. For purposes of this application a processing system includes any system that has a processor such as for example a digital signal processor DSP a microcontroller an application specific integrated circuit ASIC or a microprocessor.

The program code may be implemented in a high level procedural or object oriented programming language to communicate with a processing system. The program code may also be implemented in assembly or machine language if desired. In fact the mechanisms described herein are not limited in scope to any particular programming language. In any case the language may be a compiled or interpreted language.

One or more aspects of at least one embodiment may be implemented by representative instructions stored on a machine readable medium which represents various logic within the processor which when read by a machine causes the machine to fabricate logic to perform the techniques described herein. Such representations known as IP cores may be stored on a tangible machine readable medium and supplied to various customers or manufacturing facilities to load into the fabrication machines that actually make the logic or processor.

Such machine readable storage media may include without limitation non transitory tangible arrangements of articles manufactured or formed by a machine or device including storage media such as hard disks any other type of disk including floppy disks optical disks compact disk read only memories CD ROMs compact disk rewritable s CD RWs and magneto optical disks semiconductor devices such as read only memories ROMs random access memories RAMs such as dynamic random access memories DRAMs static random access memories SRAMs erasable programmable read only memories EPROMs flash memories electrically erasable programmable read only memories EEPROMs phase change memory PCM magnetic or optical cards or any other type of media suitable for storing electronic instructions.

Accordingly embodiments of the invention also include non transitory tangible machine readable media containing instructions or containing design data such as Hardware Description Language HDL which defines structures circuits apparatuses processors and or system features described herein. Such embodiments may also be referred to as program products.

In some cases an instruction converter may be used to convert an instruction from a source instruction set to a target instruction set. For example the instruction converter may translate e.g. using static binary translation dynamic binary translation including dynamic compilation morph emulate or otherwise convert an instruction to one or more other instructions to be processed by the core. The instruction converter may be implemented in software hardware firmware or a combination thereof. The instruction converter may be on processor off processor or part on and part off processor.

In the description and claims the terms coupled and or connected along with their derivatives have be used. It should be understood that these terms are not intended as synonyms for each other. Rather in particular embodiments connected may be used to indicate that two or more elements are in direct physical or electrical contact with each other. Coupled may mean that two or more elements are in direct physical or electrical contact. However coupled may also mean that two or more elements are not in direct contact with each other but yet still co operate or interact with each other. For example an execution unit may be coupled with a register or a decoder through one or more intervening components. In the figures arrows are used to show couplings and or connections.

In the description and claims the term logic may have been used. As used herein the term logic may include hardware firmware software or various combinations thereof. Examples of logic include integrated circuitry application specific integrated circuits analog circuits digital circuits programmed logic devices memory devices including instructions etc. In some embodiments the logic may include transistors and or gates potentially along with other circuitry components e.g. embedded in semiconductor materials .

In the description above specific details have been set forth in order to provide a thorough understanding of the embodiments. However other embodiments may be practiced without some of these specific details. The scope of the invention is not to be determined by the specific examples provided above but only by the claims below. All equivalent relationships to those illustrated in the drawings and described in the specification are encompassed within embodiments. In other instances well known circuits structures devices and operations have been shown in block diagram form or without detail in order to avoid obscuring the understanding of the description. Where multiple components have been shown in some cases they may be integrated into a single component. Where a single component has been shown and described in some cases this single component may be separated into two or more components.

Certain methods disclosed herein have been shown and described in a basic form although operations may optionally be added to and or removed from the methods. In addition a particular order of the operations may have been shown and or described although alternate embodiments may perform certain operations in different order combine certain operations overlap certain operations etc.

Certain operations may be performed by hardware components and or may be embodied in a machine executable instruction that may be used to cause and or result in a hardware component e.g. a processor potion of a processor etc. programmed with the instruction performing the operations. The hardware component may include a general purpose or special purpose hardware component. The operations may be performed by a combination of hardware software and or firmware. The hardware component may include specific or particular logic e.g. circuitry potentially combined with software and or firmware that is operable to execute and or process the instruction and perform an action in response to the instruction e.g. in response to one or more microinstructions or other control signals derived from the instruction .

Reference throughout this specification to one embodiment an embodiment one or more embodiments some embodiments for example indicates that a particular feature may be included in the practice of the invention but is not necessarily required to be. Similarly in the description various features are sometimes grouped together in a single embodiment Figure or description thereof for the purpose of streamlining the disclosure and aiding in the understanding of various inventive aspects. This method of disclosure however is not to be interpreted as reflecting an intention that the invention requires more features than are expressly recited in each claim. Rather as the following claims reflect inventive aspects lie in less than all features of a single disclosed embodiment. Thus the claims following the Detailed Description are hereby expressly incorporated into this Detailed Description with each claim standing on its own as a separate embodiment of the invention.

The following clauses and or examples pertain to further embodiments. Specifics in the clauses and or examples may be used anywhere in one or more embodiments.

In one embodiment a first apparatus includes a plurality of cores and a shared core extension logic coupled with each of the plurality of cores. The shared core extension logic has shared data processing logic that is shared by each of the plurality of cores. The first apparatus also includes instruction execution logic for each of the cores that in response to a shared core extension call instruction is to call the shared core extension logic. The call is to have data processing performed by the shared data processing logic on behalf of a corresponding core.

Embodiments include the first apparatus further including a plurality of shared core extension command registers coupled with the instruction execution logic and the shared core extension logic where the shared core extension call instruction is to indicate one of the shared core extension command registers and a plurality of parameters.

Embodiments include any of the above first apparatus in which the instruction execution logic in response to the shared core extension call instruction is to store data in the indicated shared core extension command register based on the indicated parameters.

Embodiments include any of the above first apparatus in which the instruction execution logic in response to the shared core extension call instruction is to store in the indicated shared core extension command register a pointer in a call attribute pointer field to point to call attribute information a pointer in an input data operand pointer field to point to an input data operand and a pointer an output data operand pointer field to point to an output data operand.

Embodiments include any of the above first apparatus in which the shared core extension logic is to store based on data processing associated with the call in the indicated shared core extension command register a status field to provide a status of the call and a progress field to provide a progress of the call.

Embodiments include any of the above first apparatus in which the shared core extension call instruction includes a macroinstruction of an instruction set of the cores.

Embodiments include any of the above first apparatus in which the shared data processing logic includes at least one vector execution unit.

Embodiments include any of the above first apparatus in which the shared data processing logic includes data processing logic that is not found in the plurality of cores.

Embodiments include any of the above first apparatus in which the instruction execution logic in response to the shared core extension call instruction is to call the shared core extension logic to have data processing performed on at least one input data structure in memory according to a routine to produce at least one output data structure in memory.

Embodiments include any of the above first apparatus further including a memory management unit MMU of a first core of the plurality a shared core extension MMU of the shared core extension logic and a hardware interface between the MMU of the first core and the shared core extension MMU to exchange synchronization signals in hardware to synchronize the MMU of the first core and the shared core extension MMU.

Embodiments include any of the above first apparatus further including a memory management unit MMU of a first core of the plurality a shared core extension MMU of the shared core extension logic and an interface between the MMU of the first core and the shared core extension MMU to route a page fault which corresponds to a call from the first core from the shared core extension MMU to the MMU of the first core.

Embodiments include any of the above first apparatus further including hardware scheduling logic on die with the shared core extension logic to schedule calls from the plurality of cores on the shared data processing logic.

In one embodiment a first method includes receiving within a core of a processor having a plurality of cores a shared core extension call instruction. The shared core extension call instruction to cause the core to call a shared core extension logic which is shared by the plurality of cores. The call is to have data processing performed. The shared core extension call instruction indicates a shared core extension command register and indicates a plurality of parameters that specify the data processing to be performed. The shared core extension logic is called in response to the shared core extension call instruction to have the data processing performed. Calling the shared core extension logic includes storing data in the shared core extension command register indicated by the instruction based on the parameters indicated by the instruction.

Embodiments include the first method in which receiving the instruction includes receiving a non blocking shared core extension call instruction and further including retiring the non blocking shared core extension call instruction at the core after the shared core extension logic has accepted the data processing to be performed.

Embodiments include the first method in which receiving the instruction includes receiving a blocking shared core extension call instruction and further including retiring the blocking shared core extension call instruction at the core after the shared core extension logic has completed the data processing.

Embodiments include the first method in which receiving the instruction includes receiving a blocking shared core extension call instruction where the blocking shared core extension call instruction indicates a timeout value for a release of the indicated shared core extension command register.

Embodiments include any of the above first methods in which the shared core extension call instruction includes a macroinstruction of an instruction set of the core and where the shared core extension command register comprise an architectural register.

Embodiments include any of the above first methods in which storing the data in the indicated shared core extension command register based on the parameters includes storing a pointer in a call attribute pointer field to point to call attribute information storing a pointer in an input data operand pointer field to point to an input data operand and storing a pointer an output data operand pointer field to point to an output data operand.

Embodiments include any of the above first methods further including the shared core extension logic storing data in the indicated shared core extension register based on data processing associated with the call the storing of the data including storing a status in a status field of the indicated register to provide a status of the call and storing a progress in a progress field of the indicated register to provide a progress of the call.

Embodiments include any of the above first methods in which calling includes calling the shared core extension logic to have data processing performed on at least one input data structure in memory according to a routine to produce at least one output data structure in memory.

Embodiments include any of the above first methods further including synchronizing a memory management unit MMU of the core and a shared core extension MMU of the shared core extension logic by exchanging synchronization signals in hardware between the MMU and the shared core extension MMU.

Embodiments include any of the above first methods further including routing a page fault corresponding to the call from a shared core extension memory management unit MMU to an MMU of the core.

Embodiments include any of the above first methods further including before receiving the shared core extension call instruction receiving a shared core extension abort instruction indicating the shared core extension command register and stopping data processing corresponding to the shared core extension command register indicated by the shared core extension abort instruction in response to the shared core extension abort instruction and releasing the shared core extension command register.

Embodiments include any of the above first methods further including after receiving the shared core extension call instruction receiving a shared core extension read instruction indicating the shared core extension command register and reading a data processing completion status from the shared core extension command register indicated by the shared core extension read instruction in response to the shared core extension read instruction.

In one embodiment a machine readable storage medium stores one or more instruction that if executed by a machine cause the machine to performing any of the above first methods.

Embodiments include a first system including a processor and a dynamic random access memory DRAM coupled with the processor. The processor includes a plurality of cores and shared core extension logic coupled with each of the plurality of cores. The shared core extension logic has shared data processing logic that is shared by each of the plurality of cores. The processor also includes instruction execution logic for each of the cores that in response to a shared core extension call instruction is to call the shared core extension logic. The call is to have data processing performed by the shared data processing logic on behalf of a corresponding core.

Embodiments include the first system in which the shared core extension call instruction includes a macroinstruction of an instruction set of the cores.

Embodiments include any of the above first systems further including a plurality of architectural shared core extension command registers coupled with the instruction execution logic and the shared core extension logic where the shared core extension call instruction is to indicate one of the shared core extension command registers and a plurality of parameters.

