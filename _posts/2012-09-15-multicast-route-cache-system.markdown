---

title: Multicast route cache system
abstract: Techniques for organizing and grouping memory contents related to multicast routing so as to enable more efficient multicast operations. For PIM multicast routing, techniques are provided for organizing and grouping multicast routing information into data structures according to a plurality of dimensions such that multicast routing cache entries are accessible when performing a multicast routing operation by traversing the one or more data structures according to at least two of the dimensions.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09143335&OS=09143335&RS=09143335
owner: Brocade Communications Systems, Inc.
number: 09143335
owner_city: San Jose
owner_country: US
publication_date: 20120915
---
The present application is a non provisional of and claims the benefit and priority under 35 U.S.C. 119 e of U.S. Provisional Application No. 61 535 901 filed Sep. 16 2011 entitled MULTICAST ROUTE CACHE SYSTEM the entire contents of which are incorporated herein by reference for all purposes.

The present disclosure relates to networking technologies and more particularly to techniques for performing efficient multicast operations.

Multicast routing protocols are used to distribute data to multiple recipients. IP multicasting enables a sender device or sender host to send a packet to a set of recipients. The set of recipients is referred to as a multicast group and is represented by an IP address referred to as the multicast address. A multicast address thus corresponds to or represents a group of IP hosts that have joined the multicast group and want to receive packets whose destination address is the multicast address. By specifying a multicast address as the destination address for a packet referred to as a multicast packet or multicast IP datagram the packet is then delivered to the zero or more members receivers of the multicast group.

The membership of a multicast group is dynamic hosts may join and leave multicast groups at any time. There is typically no restriction on the location or number of members in a multicast group. An IP host may be a member of more than one multicast group at a time. A host need not be a member of a group to be able to send multicast packets. Internet Group Management Protocol IGMP is an example of a protocol that facilitates formation and management of multicast groups. Hosts may use IGMP to join or leave multicast groups. Hosts may also use IGMP to advertise their membership in a multicast group.

Forwarding of multicast packets from senders to receivers is performed by a fabric of network devices e.g. routers switches that execute a multicast routing protocol. For example multicast routing may be performed using Protocol Independent Multicast PIM which is a collection of multicast routing protocols including protocols such as PIM Sparse Mode PIM dense Mode Bi directional PIM and others. PIM and its variants provide a set of protocols that can be used by network devices such as routers providing multicast routing services to distribute information about multicast group membership.

Network devices such as routers that are configured to perform multicast routing are also referred to as multicast routers. A multicast router typically maintains multicast state information also referred to as multicast information that is used by the router to forward a multicast packet to its multicast group receivers. The multicast information can include PIM mcache information also referred to as a multicast routing table with multiple forwarding entries referred to as mcache entries that are cached by the router and are used to determine how a multicast packet is to be forwarded by the router.

For some networks there may be tens of thousands of mcache entries. PIM mcache information is conventionally structured with hash tables and linked lists such that all mcache entries that hash to the same hash bucket is further organized as a linked list. Because of this structure with conventional multicast routing techniques when an event occurs e.g. a link goes down all of the mcache entries need to be traversed to determine which particular mcache entries are affected by the event. This need to traverse all mcache entries can lead to significant inefficiency if the number of mcache entries affected is very small relative to the number of entries.

Certain embodiments of the present invention provide techniques for organizing and grouping memory contents related to multicast routing so as to enable more efficient multicast operations and scaling. In one embodiment for PIM multicast routing techniques are provided for grouping mcache entries according to multiple dimensions to enable heursitic searches during multicast operations.

In one embodiment a network device comprises a memory and a processor. The memory is adapted to store multicast routing information including information about multicast routing cache entries. The processor is adapted to generate a set of data structures based upon the multicast routing information. The data structures enable the multicast routing information to be organized along multiple dimensions or views. The processor is further adapted to perform a multicast routing operation relating to one or more multicast routing cache entries. Performing the operation includes traversing one or more data structures in the set of data structures. One or more multicast routing cache entries may be identified by traversing the one or more data structures according to at least two of the multiple dimensions.

In certain embodiments a data structure may represent a node of a tree structure that is based upon the multicast routing information. In certain embodiments a data structure may represent one of a multicast routing cache entry a multicast source a multicast group a rendezvous point RP and a reverse path forwarding RPF neighbor. The multiple dimensions along which the multicast routing information may be organized may include one or more of the multicast source the multicast group the RP and the RPF neighbor.

The multicast routing operation performed may be of different types. For example it may be a search for one or more multicast routing cache entries a traversal of all multicast routing cache entries or processing of a multicast route change. In certain embodiments the multicast routing operation performed is the processing of an RP down event or an RP up event in other embodiments the multicast routing operation performed is the processing of an RPF neighbor down event or an RPF neighbor up event. In still other embodiments the multicast routing operation is sending or processing of a Join Prune message.

In performing the operation the data structures may be traversed according to ascending order of addresses of one of the multicast source the multicast group the RP and the RPF neighbor. In other embodiments performing the operation may involve traversing the data structures to perform a heuristic binary search for one or more of a multicast routing cache entry a multicast source a multicast group an RP and an RPF neighbor.

The foregoing together with other features and embodiments will become more apparent upon referring to the following specification claims and accompanying drawings

Attached as Appendix A are example data structures for memory cache used for multicast routing that may be implemented in accordance with an embodiment of the present invention.

Attached as Appendix B are example application programming interfaces APIs for memory cache used for multicast routing that may be implemented in accordance with an embodiment of the present invention.

It should be understood that the specific embodiments described in Appendices A and B are not limiting examples of the invention and that some aspects of the invention might use the teachings of Appendices A and B while others might not. It should also be understood that limiting statements in Appendices A and B may be limiting as to requirements of specific embodiments and such limiting statements might or might not pertain to the claimed inventions and therefore the claim language need not be limited by such limiting statements.

In the following description for the purposes of explanation specific details are set forth in order to provide a thorough understanding of certain embodiments of the invention. However it will be apparent that various embodiments may be practiced without these specific details. The figures and description are not intended to be restrictive.

Certain embodiments of the present invention provide techniques for organizing and grouping memory contents related to multicast routing so as to enable more efficient multicast operations and scaling. In particular this description provides an architecture an organization data structure layouts algorithms and APIs for memory cache used for multicast routing also referred to as Multicast Route Cache mcache or cache that enables various multicast routing protocol operations to be optimized even when a large number of route entries e.g. 64 000 and beyond are present in the cache. The optimizations enable access either directly or through heuristic binary searches to mcache entries that are affected by the multicast routing protocol operations. Although embodiments are described with reference to PIM the systems and techniques discussed are not limited to PIM and may also apply to other multicast routing protocols for example Distance Vector Multicast Routing Protocol DVMRP and IGMP. Generally the concept is applicable to a data set with entries that can be organized by multiple dimensions or views such that each dimension or view represents the entries i.e. a subset of the entries in the entire data set that will be affected by a common event and each entry may belong to more than one dimension or view.

A multicast router typically maintains multicast information that is used by the router to forward a multicast packet to its multicast group receivers. The multicast information may be generated by a multicast routing protocol such as PIM that is executed by a processor such as a management processor of the router. Multicast packets are forwarded in a network using a multicast distribution tree. A multicast packet is replicated at each fork in the distribution tree such that only one copy of the packet is transmitted through any particular link even if that link leads to multiple receivers. A multicast protocol such as PIM may be used to set up multicast distribution trees such that data packets from senders to a multicast group reach all receivers who have joined the group.

There are generally two types of multicast distribution trees a source multicast distribution tree and a shared multicast distribution tree. A source multicast distribution tree is rooted at the sender i.e. the source of a multicast packet and the receivers are located at the ends of the branches of the tree. The branches of a source multicast distribution tree form a Shortest Path Tree through the network from the sender to the one or more receivers. A separate source multicast distribution tree is built for each sender host sending data to a multicast group. An S G notation is used to represent forwarding entries based upon a source distribution tree with each active source having an S G entry where S represents the IP address of the source and G represents the multicast group address to which the packet is to be sent.

Shared multicast distribution trees use a single common root placed at some chosen node in the network. In PIM the common root is referred to as the Rendezvous Point RP . The RP is the point at which receivers join to learn of active sources. Multicast traffic sent from a source to a group is carried by the network to the RP for that group. When receivers join a multicast group on a shared tree the root of the tree is always the RP and multicast traffic is transmitted from the RP down toward the receivers. Therefore the RP acts as a go between for the sources and receivers. Multicast forwarding entries for a shared tree use the notation G with the representing that all sources for a particular group share the same tree and G representing the multicast group address.

The multicast information can include PIM mcache information with mcache entries that are cached by the network device and are used to determine how a multicast packet is to be forwarded by the network device. The mcache entries include S G or G entries with each entry identifying incoming interface e.g. input port information and associated one or more outgoing interfaces e.g. output ports information. The incoming interface information associated with a mcache entry identifies an interface of the network device over which a multicast packet is received. The outgoing interface information associated with the mcache entry identifies for the multicast packet received via the incoming interface identified by the entry one or more interfaces of the network device to be used for forwarding the multicast packet from the network device.

Multiple protocol and system command line interface CLI events may cause mcache entries to be created updated deleted or searched. Identified and described below are various ways in which information and entries stored in the mcache may be organized or grouped so as to enable efficient PIM operations. The reasons for the organization are provided. Also described are several methods of traversing the mcache which may be needed or triggered by the various protocol and CLI events. Examples of mcache traversals include the following 

 1 All entries in the mcache are traversed in ascending order of group address sorted by ascending order of source addresses within each group. This may be used for displaying the mcache in response to a command such as Show ip pim mcache from the CLI.

 2 All distinct groups in the mcache are traversed. This may be used for efficiently assembling a Join Prune message which includes a list of groups and a list of joined and pruned sources for each group.

 3 All sources in the mcache of a given group are traversed. This also may be used for efficiently assembling a Join Prune message.

 4 All groups in the mcache that hash to the same RP are traversed. This may be used when an RP goes down and all the groups that used to map to that RP needs to be rehashed into one or more different RPs.

 5 All entries in the mcache that have the same Reverse Path Forwarding RPF neighbor are traversed. This may be used for efficiently processing RPF neighbor events.

 6 All entries in the mcache with the same source are traversed. This may be used for efficiently processing route change events toward sources.

 7 Entries in the mcache are traversed to search for a specific S G entry or G entries. This may be used for efficiently processing incoming join prune messages state machine etc.

Described below are various ways in which information and entries stored in the mcache may be organized or grouped so as to enable efficient PIM operations. Techniques and example data structures are provided for grouping mcache entries according to multiple dimensions to enable efficient and heursitic searches and traversals during multicast operations. In one embodiment these data structures are maintained in network devices e.g. routers .

In addition to the left and right pointers a node representing a particular group in the Group Tree also points to a root of a tree of nodes representing mcache entries with that same particular group. The tree of nodes representing mcache entries with that same particular group may be referred to as a Group S G Tree. Each node of a Group S G Tree represents a mcache entry. For example in the root node of Group Tree representing group G points to the root node SG of the Group S G Tree comprising S G nodes representing mcache entries SG SG and SG. Each Group S G Tree may be an AVL tree with nodes organized based upon the addresses of the respective source S .

In addition to the left and right pointers a node representing a particular source in the Source Tree also points to a root of a tree of nodes representing mcache entries with that same particular source. The tree of nodes representing mcache entries with that same particular source may be referred to as a Source S G Tree. Each node of a Source S G Tree represents a mcache entry. For example in the root node of Source Tree representing source S points to the root node SG of the Source S G Tree comprising S G nodes representing mcache entries SG SG and SG. Each Source S G Tree may be an AVL tree with nodes organized based upon the addresses of the respective group G .

In one embodiment each mcache node is part of two AVL trees. depicts an example of a mcache entry node e.g. SG being part of two AVL trees according to an embodiment of the present invention. The Group S G Tree is based on the group address and the root node of this Group S G Tree is pointed to by the group node for that group. The Source S G Tree is based on the source address and the root node of this S G Tree is pointed to by the source node for that source. For example SG which is assumed to be the root node of both the Source S G Tree for the source S and the Group S G Tree for the group G in this example is pointed to by Source Tree node S and Group Tree node G neither of which is shown in . Each non root node of a Group S G Tree is only pointed to by its parent S G node in the same Group S G Tree and not from S G nodes in other Group S G Trees. Similarly each non root node of a Source S G Tree is only pointed to by its parent S G node in the same Source S G Tree and not from S G nodes in other Source S G Trees.

As described above mcache entries are represented by nodes in the Group S G Trees and Source S G Trees. A node representing a mcache entry is thus a part of two trees namely a Group S G Tree and a Source S G Tree. This overlap between the Group S G Tree and Source S G Tree is depicted in . Such an organization of mcache entries information enables various operations to be performed efficiently. For example given an mcache entry S G the data structures can be used to efficiently determine another mcache entry with the same group G and a different source S i.e. with next higher or next lower source address or another mcache entry with the same source S and a different group G i.e. with next higher or next lower group address . For example given a mcache entry SG the data structures depicted in can be used to determine the mcache entry with the next lower source address and same group address i.e. SG by traveling the left pointer of the node representing SG in the Group S G Tree Left For Group and the mcache entry with the next higher source address and same group address i.e. SG can be reached by traveling the right pointer of the node representing SG in the Group S G Tree Right For Group . Similarly from mcache entry node SG the mcache entry with the next lower group address and same source address i.e. SG can be reached by traveling the left pointer of the node representing SG in the Source S G Tree Left For Source and the mcache entry with the next higher group address and same source address i.e. SG can be reached by traveling the right pointer of the node representing SG in the Source S G Tree Right For Source .

Storing data structures representing each mcache entry node or S G node as being part of the Source Tree and Group Tree divides the whole mcache information along a source S dimension and a group G dimension and enables efficient querying of the mcache information based upon these dimensions or a combination thereof. The mcache entries are divided into various subsets by their source addresses and their group addresses. This allows traversal of the mcache entries by their group addresses or by their source addresses.

In one embodiment data structures are provided for organizing information related to RPs that are hashed to by each group in the mcache by their address. depicts an example of data structures for organizing multicast groups and RPs according to an embodiment of the present invention. In the embodiment depicted in the data structures are AVL trees although other data structures may be used in alternative embodiments.

In addition to the left and right pointers a node representing a particular RP in the RP Tree also points to a root of a tree of nodes representing all the groups that hash to the particular RP. The tree of nodes representing all the groups that hash to the particular RP may be referred to as a RP Group Tree. Each node of an RP Group Tree represents a group. For example in the root node of RP Tree representing RP points to the root node G of the RP Group Tree comprising group nodes representing groups G G and G. Each RP Group Tree may be an AVL tree with nodes organized based upon the addresses of the comprised groups.

In one embodiment each group node is part of two AVL trees. depicts an example of a group node e.g. G being part of two AVL trees according to an embodiment of the present invention. The Group Tree includes all groups in the mcache is based on the group address and is rooted globally. The RP Group Tree includes groups that hash to the same RP and the root node of this RP Group Tree is pointed to by the RP node. Representing each group node as being part of the Group Tree and an RP Group Tree allows traversal of the groups by their group addresses or by the RPs to which they hash.

As described above groups are represented by nodes in the Group Trees and RP Group Trees. A node representing a group is thus a part of two trees namely the Group Tree and an RP Group Tree. This overlap between the Group Tree and RP Group Tree is depicted in . Such an organization of group information enables various operations to be performed efficiently. For example given a group G the data structures can be used to efficiently determine another group with the next higher or next lower group address or another group with the next higher or next lower group address that also hashes to the same RP. For example given a group G the data structures depicted in can be used to determine the group with the next lower group address i.e. G by traveling the left pointer of the node representing G in the Group Tree Left By Address and the group with the next higher group address i.e. G by traveling the right pointer of the node representing G in the Group Tree Right By Address . From group node G the group with the next lower group address i.e. G that hashes to the same RP to which G hashes can be reached by traveling the left pointer of the node representing G in the RP Group Tree Left By RP and the group with the next higher group address i.e. G that hashes to the same RP to which G hashes can be reached by traveling the right pointer of the node representing G in the RP Group Tree Right By RP .

In one embodiment data structures are provided for organizing information related to RPF neighbors toward each source in the mcache by their address. depicts data structures for organizing multicast sources and RPF neighbors according to an embodiment of the present invention. In the embodiment depicted in the data structures are AVL trees although other data structures may be used in alternative embodiments.

In addition to the left and right pointers a node representing a particular RPF neighbor in the Neighbor Tree also points to a root of a tree of nodes representing all the sources that use this particular neighbor as its RPF neighbor. The tree of nodes representing all the sources that use this particular neighbor as its RPF neighbor may be referred to as a Neighbor Source Tree. Each node of a Neighbor Source Tree represents a source. For example in the root node of Neighbor Tree representing N points to the root node S of the Neighbor Source Tree comprising source nodes representing sources S S and S. Each Neighbor Source Tree may be an AVL tree with nodes organized based upon the addresses of the comprised sources.

In one embodiment each source node is part of two AVL trees. depicts an example of a source node e.g. S being part of two AVL trees according to an embodiment of the present invention. The Source Tree includes all sources in the mcache is based on the source address and is rooted globally. The Neighbor Source Tree includes sources that have the same RPF neighbor and the root node of this Neighbor Source Tree is pointed to by the neighbor node. Representing each source node as being part of the Source Tree and a Neighbor Source Tree allows traversal of the sources by their source addresses or by their RPF neighbors.

As described above sources are represented by nodes in the Source Trees and Neighbor Source Trees. A node representing a source is thus a part of two trees namely the Source Tree and a Neighbor Source Tree. This overlap between the Source Tree and Neighbor Source Tree is depicted in . Such an organization of source information enables various operations to be performed efficiently. For example given a source S the data structures can be used to efficiently determine another source with the next higher or next lower source address or another source with the next higher or next lower source address that also uses the same RPF neighbor. For example given a source S the data structures depicted in can be used to determine the source with the next lower source address i.e. S by traveling the left pointer of the node representing S in the Source Tree Left By Address and the source with the next higher source address i.e. S by traveling the right pointer of the node representing S in the Source Tree Right By Address . From source node S the source with the next lower source address i.e. S that routes through the same RPF neighbor through which S routes can be reached by traveling the left pointer of the node representing S in the Neighbor Source Tree Left By Nbr and the source with the next higher source address i.e. S that routes through the same RPF neighbor through which S routes can be reached by traveling the right pointer of the node representing S in the Neighbor Source Tree Right By Nbr .

In one embodiment an RP node is a node of one AVL tree. depicts an example of RP nodes being part of one AVL tree. The RP Tree is based on the RP addresses and is rooted globally.

In one embodiment a RPF neighbor node is a node of one AVL tree. depicts an example of RPF neighbor nodes being part of one AVL tree. The Neighbor Tree is based on the RPF neighbor addresses and is rooted globally.

In one embodiment the mcache is divided according to multiple dimensions or views for example two or more of the following source group RP and RPF neighbor. depicts an example of a comprehensive view of the mcache including all the trees mentioned above i.e. Source Tree Group Tree Source S G Tree Group S G Tree RP Tree Neighbor Tree RP Group Tree and Neighbor Source Tree . The example of the comprehensive view of the mcache illustrates the various trees for the mcache entry SG. Mcache entry SG is pointed to by Group Tree node G which is pointed to by group node G of an RP Group Tree the comprised groups of which hash to RP of the RP Tree. Mcache entry SG is also pointed to by Source Tree node S which is pointed to by source node S of a Neighbor Source Tree the comprised sources of which use N of the Neighbor Tree as their RPF neighbor.

Appendix A provides example data structures for memory cache used for multicast routing in accordance with some embodiments of the present invention. Example data structures are included for implementations of mcache entry node group node source node RP node and RPF neighbor node. Specific embodiments described in Appendix A are not limiting examples of the invention.

Designs techniques and example data structures for implementing multi dimensional tree structures are provided for organizing multicast information and mcache entries according to multiple dimensions. Details of how these designs and techniques enable efficient heursitic searches and traversals needed to support PIM protocol operations on the stored data are provided below. Also described are several procedures for traversing the mcache which may be needed or triggered by the various protocol and CLI events.

Appendix B provides example APIs for memory cache used for multicast routing that may be implemented in accordance with an embodiment of the present invention. Example APIs are included for creating deleting and looking up mcache entries various ways of traversing the mcache and updating RP for a group or RPF neighbor for a source. Specific embodiments described in Appendix B are not limiting examples of the invention.

One example of a heuristic search enabled by the described designs is a search or look up for a particular mcache S G entry. According to PIM protocol a network device e.g. a router receives from each of its downstream neighbors at least every 60 seconds a Join Prune message which includes a list of groups and a list of joined and pruned sources for each group. Given a Join Prune message the network device needs to search for the particular mcache S G entries identified by the listed groups and sources. A particular mcache S G entry may be found using two binary searches one on the Group Tree and the other on the Group S G Tree. For example the Group Tree may be searched first and then the Group S G Tree may be searched.

Thus a particular mcache S G entry can be found using only two binary searches with a complexity of O log n where n is the number of nodes in the tree prior to the operation. The binary searches heuristically identify the particular S G node without having to do an exhaustive search unlike with conventional multicast routing techniques.

The multi dimensional tree structure designs also enable traversal of all mcache entries in numerical order which may be used for displaying the mcache in response to a command such as Show ip pim mcache from the CLI. This is in contrast to conventional multicast routing techniques in which mcache entries are essentially traversed in random order and cannot be sorted into numerical order due to their structure through hash tables and linked lists. With the multi dimensional tree structure designs mcache entries may be traversed in numerical order by using one of four procedures.

The first procedure traverses the mcache entries first by group and then by source. The mcache entries are traversed in ascending order of groups according to ascending order of the sources within each group.

The second procedure traverses the mcache entries first by source and then by group. The mcache entries are traversed in ascending order of sources according to ascending order of the groups for each source.

The third procedure traverses the mcache entries first by RP then by group and finally by source. The mcache entries are traversed in ascending order of RPs according to ascending order of the groups that hash to each RP according to ascending order of the sources within each group.

The fourth procedure traverses the mcache entries first by RPF neighbor then by source and finally by group. The mcache entries are traversed in ascending order of RPF neighbors according to ascending order of the sources that use each RPF neighbor according to ascending order of the groups for each source.

Another PIM protocol operation that benefits from the multi dimensional tree structure designs is the processing when an RP goes down. Because PIM mcache information is conventionally structured with hash tables and linked lists when using conventional multicast routing techniques and an RP goes down all of the mcache entries need to be traversed to determine which particular mcache entries are affected by the event. However with the described multi dimensional tree structure designs only the groups in the mcache that hash to the affected RP are traversed.

As mentioned above processing when an RP goes down using conventional multicast routing techniques results in all of the mcache entries being traversed. However with the above procedure the groups that are affected by the RP down event are reached directly without needing to traverse any groups that are not affected by the event. Once a group that has been affected is identified the Group S G Tree containing all S G entries for this group can be reached directly without needing to traverse any S G entries that are not affected by the event. All the S G entries affected by the event i.e. the S G entries that were using the affected RP are implicitly migrated to new RP s without these S G entries needing to be moved individually.

Similarly the processing when an RP goes up also improves in efficiency as a benefit from the multi dimensional tree structure designs. As with the RP down event an RP up event also requires traversal of all mcache entries when using conventional multicast routing techniques.

Even though all the groups in the mcache are traversed in this event all S G entries that are affected can be reached without needing to traverse any S G entries that are not affected by the event. This offers significant savings over conventional designs which would require traversal of all mcache entries. All the S G entries affected by the event i.e. the S G entries that now hash to the new RP are implicitly migrated to the new RP without these S G entries needing to be moved individually.

Another PIM protocol operation that benefits from the multi dimensional tree structure designs is the processing when an RPF neighbor goes down. For this event all entries in the mcache that have the same Reverse Path Forwarding RPF neighbor are traversed. As with the RP down event and the RP up event an RPF neighbor down event also requires traversal of all mcache entries when using conventional multicast routing techniques.

As mentioned above processing when an RPF neighbor goes down using conventional multicast routing techniques results in all of the mcache entries being traversed. However with the above procedure the sources that are affected by the RPF neighbor down event are reached directly without needing to traverse any sources that are not affected by the event. Once a source that has been affected is identified the Source S G Tree containing all S G entries for this source can be reached directly without needing to traverse any S G entries that are not affected by the event. All the S G entries affected by the event i.e. the S G entries whose routes used to go through the affected RPF neighbor are implicitly migrated to new RPF neighbor s without these S G entries needing to be moved individually.

Similarly the processing when an RPF neighbor goes up also improves in efficiency as a benefit from the multi dimensional tree structure designs. As with the RPF neighbor down event an RPF neighbor up event also requires traversal of all mcache entries when using conventional multicast routing techniques.

Even though all the sources in the mcache are traversed in this event all S G entries that are affected can be reached without needing to traverse any S G entries that are not affected by the event. This offers significant savings over conventional designs which would require traversal of all mcache entries. All the S G entries affected by the event i.e. the S G entries that now route through the new neighbor are implicitly migrated to the new neighbor without these S G entries needing to be moved individually.

Another PIM protocol operation that benefits from the multi dimensional tree structure designs is the processing of PIM Join Prune messages. For this event entries in the mcache are traversed to search for the S G entries listed in the Join Prune message. With conventional multicast routing techniques processing of a Join Prune message requires an exhaustive search for the listed S G entries in the worst case if the distribution of source and group addresses are such that they all map to the same hash bucket.

Since the format of the PIM Join Prune message lists the entries as a list of random i.e. in no particular order groups with a list of random i.e. no particular order sources within each group this procedure provides efficient lookup for processing Join Prune messages.

Outgoing PIM Join Prune messages are also efficiently assembled using procedures based on the multi dimensional tree structure designs. For this event all distinct groups in the mcache are traversed and all sources in the mcache of a given group are traversed.

Since the format of the PIM Join Prune message lists the entries as a list of random i.e. in no particular order groups with a list of random i.e. no particular order sources within each group this procedure provides efficient assembling for outgoing Join Prune messages.

Similar to the processing of RPF neighbor up or down events processing of Route Table Manager RTM route changes also benefits from the multi dimensional tree structure designs discussed above. A network device e.g. a router can receive notice of an RTM route change through a Route Program Message RPM or the unicast route table. For this event all entries in the mcache with the same source are traversed.

As an example a router may receive notice through the RPM or the unicast route table of an RTM route change for prefixes starting at 10.0.0.0 but less than 11.0.0.0. In this case all sources within the mcache with addresses starting with 10 i.e. within the prefix range can be found and a route lookup can be performed to determine if that source now routes through a different RPF neighbor. If the route for the source has changed the node for the source will be moved from the old neighbor to the new neighbor and PIM operations will be handled for that source and all mcache entries with that source.

By performing a heuristic binary search rather than an exhaustive search on the Source Tree a smaller number of sources are likely traversed before identifying all sources that may have been affected by the event. For example with a given prefix range the Source Tree is searched in order of ascending addresses until all the sources with addresses within the prefix range have been traversed. Unaffected sources with addresses greater than the prefix range would not be traversed with this procedure whereas at least some of these unaffected sources with addresses greater than the prefix range could be and likely would be traversed in an exhaustive search performed with conventional multicast routing techniques. In addition with the above procedure all S G entries that are affected can be reached without needing to traverse any S G entries that are not affected by the event. This offers significant savings over conventional designs. All the S G entries affected by the event i.e. the S G entries that now route through a new neighbor are implicitly migrated to the new neighbor without these S G entries needing to be moved individually.

At a memory of a network device stores multicast routing information including information about multicast routing cache entries i.e. mcache entries . In one embodiment the multicast routing information includes addresses of sources multicast groups RPs and RPF neighbors for the multicast routing cache entries.

At a processor of the network device generates a set of data structures based upon the multicast routing information. The data structures enable the multicast routing information to be organized and queried along multiple dimensions or parameters. This organization provides multiple ways to view or relate the entries within the multicast routing cache. For example as discussed above the multicast routing information may be organized along but are not limited to one or more of the multicast source the multicast group the RP and the RPF neighbor.

In one embodiment one or more data structures represent a node of a tree structure based upon the multicast routing information. For example the data structures can be used in the manner discussed above and structured as depicted in any of or as provided in Appendix A. In one embodiment a data structure represents one of a multicast routing cache entry a multicast source a multicast group a RP and a RPF neighbor.

At the processor performs a multicast routing operation in response to an event affecting routing for one or more multicast routing cache entries. The processor performs the multicast routing operation by traversing one or more data structures in the set of data structures to identify the one or more multicast routing cache entries affected by the event without checking any multicast routing cache entries not affected by the event. The one or more multicast routing cache entries affected by the event are identifiable by traversing the one or more data structures according to at least two dimensions of the multiple dimensions.

In one embodiment the multicast routing operation can be processing a multicast route change processing a RP up event processing a RP down event processing a RPF neighbor up event processing a RPF neighbor down event and sending a Join Prune message. Each of these multicast routing operations directly or indirectly affects e.g. searches for creates updates or deletes one or more multicast routing cache entries. In other embodiments the multicast routing operation can be searching for one or more multicast routing cache entries traversing all multicast routing cache entries or processing a Join Prune message.

Performing the multicast routing operation can include traversing the data structures according to ascending order of addresses of one or more of the multicast source the multicast group the RP and the RPF neighbor. In addition or alternatively performing the multicast routing operation can include traversing the data structures to perform a heuristic binary search for one or more of a multicast routing cache entry a multicast source a multicast group a RP and a RPF neighbor.

For example as described above if the multicast routing operation performed is processing an RP down event the particular affected RP may be searched using the RP Tree and all affected groups may be identified by traversing the RP Group Tree of the affected RP. All affected S G entries may then be identified by traversing the Group S G Trees of the affected groups. That is the affected mcache S G entries are identified by traversing the data structures along an RP dimension and a group dimension. For an RP up event the affected mcache S G entries are also identified along the RP dimension and the group dimension. Similarly for RPF up or down events the affected mcache S G entries are identified along a RPF neighbor dimension and a source dimension. In contrast to conventional multicast routing techniques the above procedures do not result in any unaffected S G entries being traversed or checked for an effect e.g. a changed parameter from the event. This leads to faster and more efficient performance of the multicast routing operations.

The multicast protocols supported may include the PIM protocol which is a collection of multicast routing protocols including protocols such as PIM Sparse Mode PIM dense Mode Bi directional PIM and others. For example in one embodiment network device may execute the PIM protocol to facilitate multicast routing. The protocols used in a network connected to network device may include wired and or wireless protocols. While embodiments have been described using the PIM protocol other multicast protocols are also included within the scope of embodiments of the present invention.

A multicast router is adapted to or configured to receive packets including unicast and multicast packets and forward the packets in such a way that it facilitates delivery of the packets to their intended one or multiple destinations. For a multicast packet network device may be adapted to or configured to replicate the packet depending upon the number of recipients of the packet and forward the replicates to facilitate delivery of the packets to members of the multicast group corresponding to the packet s multicast destination address.

In the embodiment depicted in network device comprises a plurality of ports for receiving and forwarding data packets and multiple cards that are configured to perform processing to facilitate forwarding of the data packets. The multiple cards may include one or more line cards and one or more management cards . A card sometimes also referred to as a blade or module can be inserted into one of a plurality of slots on the chassis of network device . This modular design allows for flexible configurations with different combinations of cards in the various slots of the device according to differing network topologies and switching requirements. The components of network device depicted in are meant for illustrative purposes only and are not intended to limit the scope of the invention in any manner. Alternative embodiments may have more or less components than those shown in .

In the embodiment depicted in network device comprises a plurality of ports for receiving and forwarding data including unicast and multicast packets using ports . Ports represent the I O plane for network device . A port within ports may be classified as an input port or an output port depending upon whether network device receives or transmits a data packet using the port. A port over which a data packet is received by network device is referred to as an input port. A port used for communicating or forwarding a data packet from network device is referred to as an output port. A particular port may function both as an input port and an output port. A port may be connected by a link or interface to a neighboring network device or network. Ports may be capable of receiving and or transmitting different types of data traffic including multicast data traffic at different speeds including 1 Gigabit sec 10 Gigabits sec 40 Gigabits sec or more. In some embodiments multiple ports of network device may be logically grouped into one or more trunks.

Upon receiving a data packet via an input port network device is configured to determine an output port for the packet for transmitting the data packet from the network device to another neighboring network device or network. Within network device the packet is forwarded from the input network device to the determined output port and transmitted from network device using the output port. In one embodiment forwarding of packets from an input port to an output port is performed by one or more line cards .

Line cards represent the data forwarding plane of network device . Each line card may comprise one or more packet processors that are programmed to perform forwarding of data packets from an input port to an output port. A packet processor on a line card may also be referred to as a line processor. Each packet processor may have associated memories to facilitate the packet forwarding process. In one embodiment as depicted in each packet processor may have an associated content addressable memory CAM and a RAM for storing forwarding parameters RAM may accordingly also be referred to as a parameter RAM or PRAM . In one embodiment for a packet received via an input port the packet is provided to a packet processor of a line card coupled to the input port. The packet processor receiving the packet is configured to determine an output port of network device to which the packet is to be forwarded based upon information extracted from the packet. The extracted information may include for example the header of the received packet. In one embodiment a packet processor is configured to perform a lookup in its associated CAM using the extracted information. A matching CAM entry then provides a pointer to a location in the associated PRAM that stores information identifying how the packet is to be forwarded within network device . Packet processor then facilitates forwarding of the packet from the input port to the determined output port.

Since processing performed by a packet processor needs to be performed at a high packet rate in a deterministic manner packet processor is generally a dedicated hardware device configured to perform the processing. In one embodiment packet processor is a programmable logic device such as a field programmable gate array FPGA . Packet processor may also be an ASIC.

Management card is configured to perform management and control functions for network device and thus represents the management plane for network device . In one embodiment management card is communicatively coupled to line cards and includes software and hardware for controlling various operations performed by the line cards. In one embodiment a single management card may be used for all the line cards in network device . In alternative embodiments more than one management cards may be used with each management card controlling one or more line cards.

A management card may comprise a processor also referred to as a management processor that is configured to perform functions performed by management card and associated memory . As depicted in multicast information may be stored in memory . The multicast information may be stored and used in a manner as described above. Data structures and APIs may be stored in memory . Data structures may include one or more of the data structures described above such as the data structures depicted in or the data structures provided in Appendix A. APIs may include one or more of the APIs described above or the APIs provided in Appendix B. Memory is also configured or adapted to store various programs code instructions and data constructs that are used for processing performed by processor of management card . For example programs code instructions which when executed by processor cause the data structures and APIs to be accessed and cause the multicast information to be stored and used in an efficient manner in performing the above described multicast routing related operations may be stored in memory . In one embodiment processor is a general purpose microprocessor such as a PowerPC Intel AMD or ARM microprocessor operating under the control of software stored in associated memory .

This may be used to help the mcache library preserve state between traversal calls to support incremental traversals.

The mcache library may cast this value internally into pointers to structures that hold information about the current state of the traversal.

This API may be called repeatedly until the API returns NULL. All the entries in the mcache will be returned one by one sorted in ascending order of group addresses sorted in ascending order of source addresses within each group.

This API may be called repeatedly until the API returns NULL. All distinct groups will be returned one by one sorted in ascending order of group addresses. The entry with lowest source address will be returned for each group.

This API may be called repeatedly until the API returns NULL. All the groups for the same RP will be returned one by one sorted in ascending order of group addresses. The entry with lowest source address will be returned for each group.

This API may be called repeatedly until the API returns NULL. All the entries for the same group will be returned one by one sorted in ascending order of source addresses.

This API may be called repeatedly until the API returns NULL. All the entries for the same source will be returned one by one sorted in ascending order of group addresses.

This API may be called repeatedly until the API returns NULL. All the entries for the same RPF neighbor will be returned one by one sorted in ascending order of group and source addresses.

Various embodiments described above can be realized using any combination of dedicated components and or programmable processors and or other programmable devices. The various embodiments may be implemented only in hardware or only in software or using combinations thereof. The various processes described herein can be implemented on the same processor or different processors in any combination with each processor having one or more cores. Accordingly where components or modules are described as being adapted to or configured to perform a certain operation such configuration can be accomplished e.g. by designing electronic circuits to perform the operation by programming programmable electronic circuits such as microprocessors to perform the operation by providing software or code instructions that are executable by the component or module e.g. one or more processors to perform the operation or any combination thereof. Processes can communicate using a variety of techniques including but not limited to conventional techniques for interprocess communication and different pairs of process may use different techniques or the same pair of processes may use different techniques at different times. Further while the embodiments described above may make reference to specific hardware and software components those skilled in the are will appreciate that different combinations of hardware and or software components may also be used and that particular operations described as being implemented in hardware might also be implemented in software or vise versa.

The various embodiments are not restricted to operation within certain specific data processing environments but are free to operate within a plurality of data processing environments. Additionally although embodiments have been described using a particular series of transactions this is not intended to be limiting.

Thus although specific invention embodiments have been described these are not intended to be limiting. Various modifications and equivalents are within the scope of the following claims.

