---

title: Method and system for scheduling requests in a portable computing device
abstract: A method and system for managing requests among resources within a portable computing device include a scheduler receiving data from a client for scheduling a plurality of requests. Each request identifies at least one resource and a requested deadline. Next, data from the client is stored by the scheduler in a database. The scheduler then determines times and a sequence for processing the requests based on requested deadlines in the requests and based on current states of resources within the portable computing device. The scheduler then communicates the requests to the resources at the determined times and according to the determined sequence. The scheduler, at its discretion, may schedule a request after its requested deadline in response to receiving a new request command from a client. The scheduler may allow a sleep set corresponding to a sleep processor state to power off a processor.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08793697&OS=08793697&RS=08793697
owner: QUALCOMM Incorporated
number: 08793697
owner_city: San Diego
owner_country: US
publication_date: 20120313
---
This application claims priority under 35 U.S.C. 119 e to U.S. Provisional Patent Application Ser. No. 61 602 287 filed on Feb. 23 2012 entitled METHOD AND SYSTEM FOR SCHEDULING REQUESTS IN A PORTABLE COMPUTING DEVICE. The entire contents of which are hereby incorporated by reference.

Portable computing devices PCDs are becoming increasingly popular. These devices may include cellular telephones portable personal digital assistants PDAs portable game consoles portable navigation units palmtop computers and other portable electronic devices. Each of these devices may have a primary function. For example a cellular telephone generally has the primary function of receiving and transmitting telephone calls.

In addition to the primary function of these devices many include peripheral functions. For example a cellular telephone may include the primary function of making cellular telephone calls as described above and the peripheral functions of a still camera a video camera global positioning system GPS navigation web browsing sending and receiving e mails sending and receiving text messages and push to talk capabilities etc. As the functionality of PCDs increases the computing or processing power required to support such functionality also increases. Processing power may be increased by increasing the number of processors in the PCD. As the computing power and number of processors increases there exists a greater need to effectively manage the processors.

Functions such as those described above may be embodied in various corresponding hardware and software elements that may be referred to as resources. A processor may request various resources at various times under control of software such as an application program. In a multi processor PCD a first processor may control resources that are different from the resources controlled by a second processor. In the conventional art managing resources efficiently across processors may be very complex and difficult for conserving power consumed by these resources.

A method and system for managing requests among resources within a portable computing device are disclosed. The method and system include a scheduler receiving data from a client for scheduling a plurality of requests. Each request identifies at least one resource and a requested deadline. Next data from the client is stored by the scheduler in a database. The scheduler then determines times and a sequence for processing the requests based on requested deadlines in the requests and based on current states of resources within the portable computing device. The scheduler then communicates the requests to the resources at the determined times and according to the determined sequence.

The scheduler at its discretion may schedule a request after its requested deadline in response to receiving a new request command from a client. The scheduler may allow a sleep set corresponding to a sleep processor state to power off a processor such that the processor may continue working on one or more scheduled requests when the processor exits the sleep processor state. If the scheduler receives an unexpected request during a scheduled sleep state then it may determine if one or more scheduled requests are needed to respond to the unexpected request. If one or more scheduled requests are not needed to respond to the unexpected request during the scheduled sleep state then the scheduler may cancel one or more scheduled requests and then reschedule one or more of the cancelled scheduled requests to occur in a next scheduled active state.

The word exemplary is used herein to mean serving as an example instance or illustration. Any aspect described herein as exemplary is not necessarily to be construed as preferred or advantageous over other aspects.

In this description the term application may also include files having executable content such as object code scripts byte code markup language files and patches. In addition an application referred to herein may also include files that are not executable in nature such as documents that may need to be opened or other data files that need to be accessed.

The term content may also include files having executable content such as object code scripts byte code markup language files and patches. In addition content referred to herein may also include files that are not executable in nature such as documents that may need to be opened or other data files that need to be accessed.

As used in this description the terms component database module system and the like are intended to refer to a computer related entity either hardware firmware a combination of hardware and software software or software in execution. For example a component may be but is not limited to being a process running on a processor a processor an object an executable a thread of execution a program and or a computer. By way of illustration both an application running on a computing device and the computing device may be a component. One or more components may reside within a process and or thread of execution and a component may be localized on one computer and or distributed between two or more computers. In addition these components may execute from various computer readable media having various data structures stored thereon. The components may communicate by way of local and or remote processes such as in accordance with a signal having one or more data packets e.g. data from one component interacting with another component in a local system distributed system and or across a network such as the Internet with other systems by way of the signal .

In this description the terms communication device wireless device wireless telephone wireless communication device and wireless handset are used interchangeably. With the advent of third generation 3G and fourth generation 4G wireless technology greater bandwidth availability has enabled more portable computing devices with a greater variety of wireless capabilities.

In this description the term portable computing device PCD is used to describe any device operating on a limited capacity power supply such as a battery. Although battery operated PCDs have been in use for decades technological advances in rechargeable batteries coupled with the advent of third generation 3G and fourth generation 4G wireless technology have enabled numerous PCDs with multiple capabilities. Therefore a PCD may be a cellular telephone a satellite telephone a pager a personal digital assistant PDA a smartphone a navigation device a smartbook or reader a media player a combination of the aforementioned devices and a laptop computer with a wireless connection among others.

The CPU A may comprise a zeroth core a first core etc. through an Nth core as understood by one of ordinary skill in the art. In alternative embodiments instead of CPU A and a graphics processor B one or more digital signal processors DSPs may also be employed as understood by one of ordinary skill in the art. Further in alternative embodiments two or more multi core processors may be included.

As illustrated in a display controller and a touchscreen controller are coupled to the multi core CPU A. A touchscreen display external to the on chip system is coupled to the display controller and the touchscreen controller . Also included in PCD is a video coder decoder codec e.g. a phase alternating line PAL encoder a sequential couleur avec memoire SECAM encoder a national television system s committee NTSC encoder or any other type of video encoder coupled to the multi core central processing unit CPU A. A video amplifier is coupled to the video encoder and the touchscreen display . A video port is coupled to the video amplifier . As depicted in a universal serial bus USB controller is coupled to the CPU A. Also a USB port is coupled to the USB controller . A subscriber identity module SIM card may also be coupled to the CPU A. Further as shown in a digital camera may be coupled to the CPU A. In an exemplary aspect the digital camera is a charge coupled device CCD camera or a complementary metal oxide semiconductor CMOS camera.

As further illustrated in a stereo audio CODEC may be coupled to the analog signal processor . Moreover an audio amplifier may be coupled to the stereo audio CODEC . In an exemplary aspect a first stereo speaker and a second stereo speaker are coupled to the audio amplifier . shows that a microphone amplifier may be also coupled to the stereo audio CODEC . Additionally a microphone may be coupled to the microphone amplifier . In a particular aspect a frequency modulation FM radio tuner may be coupled to the stereo audio CODEC . Also an FM antenna is coupled to the FM radio tuner . Further stereo headphones may be coupled to the stereo audio CODEC .

Some of the above described elements of the PCD may comprise hardware while others may comprise software and still others may comprise a combination of hardware and software. The term resource is used herein to refer to any such element whether hardware software or a combination thereof that is controllable by a processor. A resource may be defined in one aspect as an encapsulation of the functionality of such an element. Except where it may otherwise be indicated the term processor or master processor is used herein to refer to a processor such as the CPU graphics processor B the analog signal processor or to any other processor controller or similar element that operates under the control of software firmware or similar control logic.

As described in further detail below an example of a resource is a software element that executes on a processor. A thread of execution on a processor such as for example a thread relating to an executing application program may access a resource by causing a request to be issued on the resource. As described below resource requests are processed through a software based system referred to in this disclosure as a framework. 

The term client is used broadly in this disclosure to refer to an element that effects the function of requesting a resource. Thus as the terms are used herein a thread may create or make use of a client for the purpose of issuing resource requests. It should be noted that in some instances a resource may create or use a client such that a resource may cause a resource request to be issued against another resource. As described in further detail below such other resource may be referred to herein as a dependent resource due to a dependency relationship between the requesting resource and requested resource. Resources and clients may be represented by data structures in memory.

Since resources are controlled by specific processors in a multi processor PCD not every processor in PCD has access to every resource in PCD . illustrates an example of an instance in which it may be desirable for a first processor in PCD to issue a resource request against a resource power manager controlled by a second processor in PCD . Note that the first processor may also control a plurality of resources A A. Likewise the second processor may control a plurality of additional resources B B.

In an instance in which the first processor is executing a thread relating to for example a video player application program the thread may call for adjustment of one or more operating parameters of the first processor that enhance the performance of the first processor . Although thread and the resource power manager are conceptually illustrated as residing in their respective processors and for purposes of clarity one of ordinary skill in the art understands that such elements are executed or otherwise operated upon by the processor in the processor s memory space in accordance with well understood computing principles. Such operating parameters may include for example clock speed and bus speed.

For example various processors may use the same bus clock but only one of the processors may have direct hardware level control of the bus clock. Increasing clock speed may result in better performance by for example a video player application program since the playback of video is generally a more processing power intensive task than some other tasks. As processing power is commonly expressed in millions of instructions per second MIPS the thread may issue a call for a certain number of MIPS. The resource power manager may include an algorithm that in response to a request for a specified number of MIPS causes changes in signals that may represent clock speed bus speed or other parameters that promote the first processor operating at the requested MIPS level.

It may be possible for a thread to access the resource power manager through an application program interface API specific to a bus or protocol through which the first processor may communicate with the second processor . However the framework described below may provide a more uniform way to handle resource requests than a resource specific and bus specific API. As described below via the framework resource requests are issued and serviced in a uniform manner without regard to whether the request is against a resource controlled by the same processor from which the resource request is issued or against a resource controlled by a different processor. A resource controlled by the same processor from which the resource request is issued may be referred to as a native resource. A resource controlled by a processor other than that from which the resource request is issued may be referred to herein as a remote resource or distributed resource. 

In addition issuing a request against a remote resource incurs processing overhead in the form of a time delay or latency. That is a certain amount of time is required for the message or messages relating to the resource request to be sent between processors. In some instances a single resource request may result in multiple inter processor messages.

The CPU A controls or has access to the above referenced resources because the resources are within the memory space of the CPU A and no other restrictions such as security restrictions exist that would inhibit CPU A from accessing those resources. For example CPU A may be capable of controlling or accessing hardware registers of those resources. It should be noted that PCD may include other CPUs see e.g. that control or have access to resources other than the above referenced resources.

A framework manager which may comprise a library of computer instructions manages nodes that encapsulate functionality of the resources. That is the nodes may be accessed to indirectly access the resources. For convenience a node encapsulating the functionality of a resource may be referred to herein as including comprising having etc. the resource. Each node may include one or more resources. The nodes may be defined in software code firmware or a similar medium and instantiated as data structures in for example memory during operation of the PCD .

The nodes may be instantiated during a start up power up initialization boot up etc. sequence or at any other suitable time during operation of the PCD . It should be noted that a reference herein to instantiating issuing a request on or otherwise interacting with a resource should be understood as meaning interacting with a node that includes that resource. For the remainder of this disclosure a generic or non specific node will be designated with reference numeral as described below with reference to .

Nodes may include for example a first node having a single resource that generally corresponds with the first hardware element or central processing unit . With the software architecture described in this disclosure each resource of a node may be provided with a unique name comprising one or more alphanumeric characters. In the exemplary embodiment illustrated in the resource of the first node has been assigned the resource name of core cpu. This exemplary resource name generally corresponds to conventional file naming structures known to one of ordinary skill in the art. However as recognized by one of ordinary skill the art other types of resource names containing any other combination of alpha numeric characters and or symbols are well within the scope of this disclosure.

Nodes may further include for example a second node having a plurality of resources. In this exemplary embodiment the second node has a first resource comprising a single hardware element corresponding to the bus arbiter or scheduler . The second resource of the second node comprises a software element generally corresponding to the first software element of the bus program A A. The third resource of the second node comprises another software element generally corresponding to the second software element of the bus program B B. One of ordinary skill in the art recognizes that any combination and any number of resources and resource types for a given node are well within the scope of this disclosure.

Other relationships displayed in include dependencies illustrated with dashed lines . Dependencies are relationships between respective resources of another node . A dependency relationship usually indicates that a first resource A is reliant upon a second resource B that may provide the first resource A with information. This information may be a result of an operation performed by a second resource B or it may simply comprise status information that is needed by the first resource A or any combination thereof. The first resource A and second resource B may be part of the same node or they may be part of different nodes . It should be noted that client requests may originate not only from threads of execution such as in the example of the above described keypress action but also from other nodes . To obtain information from a dependent node a node may issue a client request to its dependent node . Thus the dashed lines that indicate dependencies may also indicate the direction of potential client requests .

In the first node is dependent upon the second node as indicated by the dependency arrow B which originates with the first node and extends to the second at . also illustrates that the first node is also dependent upon the third node as illustrated by the dependency arrow A. also illustrates that the second node is dependent upon the fourth node as illustrated by the dependency arrow C. One of ordinary skill in the art recognizes that the dependencies illustrated with the dashed arrows of are only exemplary in nature and that other combinations of dependencies between respective nodes are within the scope of this disclosure.

The framework manager is responsible for maintaining the relationships described above that include but are not limited to the client requests and the dependencies illustrated in . Some such relationships such as dependencies exist at a PCD start up time i.e. power up initialization boot up etc. by virtue of the way the resources and their nodes have been defined in the software code in PCD that the framework manager accesses at such a start up time to begin the node instantiation process. Other such relationships such as client requests arise after nodes have been instantiated such as during execution of an application program thread in which an application program invokes a resource. Whether client requests originate from executing application program threads or similar elements other than nodes e.g. client request A or originate from a node client requests are directed through the framework manager . The framework manager directs the transfer of information among the nodes . Conceptually the framework manager serves as a matrix through which multiple threads may essentially concurrently communicate with the nodes . Though different threads may involve different data the same framework manager software code may service multiple threads.

As described below in further detail the framework manager may instantiate a node as soon as the node s dependent nodes are instantiated i.e. when the dependencies for any given node have been resolved. The framework manager attempts to instantiate all nodes that have been defined in the software architecture of PCD . A dependency is completed or resolved when a resource that supports a dependency is in existence or is in a ready state for handling information that relates to the dependency .

For example the first node comprising the single resource core cpu may not be instantiated by the framework manager if the third node comprising the single resource clk cpu has not been instantiated because of the dependency relationship A that exists between the first node and the third node . Once the third node has been instantiated by the framework manager then the framework manager may instantiate the second node because of the dependency relationship A.

If the framework manager is unable to instantiate a particular node because one or more of its dependencies are incomplete or unresolved the framework manager will continue running or executing steps corresponding to those nodes that were instantiated successfully. The framework manger will usually skip over a call for a particular node that may not exist due to incomplete dependencies in which dependent resources have not been created and return messages to that call which reflect that incomplete status.

In a multi core environment such as illustrated in the framework manager may create or instantiate nodes on separate cores such as the 0th first and Nth cores and of . Nodes may generally be created in a multi core environment on separate cores and in parallel as long as the nodes are not dependent on one another and if all of a particular node s corresponding dependencies as described below are complete. In a multi processor environment the nodes may be created or instantiated on various processors such as the CPU A graphics processor B etc. of . That is some nodes may exist in the memory space of one processor while other nodes may exist in the memory space of another processor. It should be noted however that nodes on one processor may not be accessible to nodes on the other processor via only framework manager .

A remoting framework manager that is similar to the above described main framework manager may exist in parallel with the framework manager . The remoting framework manager cooperates with or works with the framework manager to coordinate inter processor information transfers between nodes on different processors. That is the remoting framework manager helps framework manager maintain the relationships described above such as dependencies and client requests in instances in which the nodes that are involved exist on different processors.

Thus nodes on one processor may not rendered accessible to nodes on another other processor via the combined effect of framework managers and . Moreover the combination of framework managers and may perform all of the functions ascribed in this disclosure to framework manager whether the nodes that are involved exist on the same processor different processors. In such a multi processor embodiment individual copies of the software that framework managers and comprise may reside in the domain of each of the processors. Thus each processor has access to the same framework manager software.

The acyclic property of the graph is important to prevent deadlocks since as described below each node is locked in a transaction processing sense when it is accessed. If two nodes were to depend on each other in an instance in which a first thread were to access and lock one of these two nodes at the same time that a second thread were to access and lock the other of these two nodes both threads would be hung.

However in the relatively rare instances in which a software developer or other such person involved in defining the software architecture deems it desirable to define in the software architecture two resources that depend on each other the two or more resources may be included in the same node as each other. Two resources in the same node will share the same lock state. It is at least in part for this reason that a software developer or other such person may choose to define a plural resource node such as node in the architecture.

Although this disclosure may for purposes of clarity and convenience reference a node rather than a resource of the node it should be understood that client requests may be directed to specified resources rather than nodes. In other words a node which as described above may be a data structure encapsulating of the functionality of one or more resources may be transparent from the perspective of a client or other issuer of a client request such as another node . From the perspective of a client a request is issued against a resource rather than a node. Likewise from the perspective of a client a state query event or other element of the architecture is associated with a resource rather than a node.

A resource graph such as the exemplary graph is useful for understanding the instantiation of nodes in accordance with dependencies described below with regard to . Leaf nodes such as the nodes and are instantiated before non leaf nodes because leaf nodes have no dependencies. In general a node must be instantiated before a node that depends on it may be instantiated. Furthermore it can be seen that servicing a resource request corresponds to traversing a directed acyclic graph in which the vertices correspond to the nodes the edges correspond to client requests and adjacent nodes or vertices represent resource dependencies.

In a multi processor PCD a first processor may have access to or be capable of controlling a first set of nodes in a first resource graph while a second processor may have access to or be capable of controlling a second set of nodes in a second resource graph where the first and second resource graphs do not share any resources i.e. they are mutually exclusive of resources. That is in such an environment each processor has its own resource graph that defines relationships among resources and other elements that are not accessible to other processors. The distributed resource management of the present disclosure relates to maintaining the relationships described above such as dependencies and client requests in instances in which two or more processors each have access to resources in their own resource graphs and do not have access to resources in other processors resource graphs.

The above referenced limitation upon access to resources may in some embodiments be limited by hardware configuration. That is a processor may have no means by which it can affect a hardware device such as a register because the hardware device is controlled by or in the memory space of another processor. Alternatively or in addition the limitation upon access to resources may be imposed in software for reasons such as minimizing exposure of a processor to security risks e.g. a virus that may be infecting another processor .

For example the first node has a dependency arrow B to indicate that the first node is dependent upon the three resources of the second node . Similarly the third resource bus ahb sysB comprising the second software element B and generally designated with the reference letter C in has a dependency arrow C that indicates this third resource C is dependent upon the single clk sys ahb resource of the fourth node .

The node or resource graphs B of represent relationships which exist in memory under the control of a processor and which are managed by the framework manager . The node or resource graph B may be automatically generated by the framework manager as a useful tool for identifying relationships between respective elements managed by the framework manager and for troubleshooting by a software team.

Block is the first routine of the method or process for managing resources of a PCD . In block a routine may be executed or run by the framework manager for receiving node structure data. The node structure data may comprise a dependency array that outlines the dependencies a particular node may have with other nodes . Further details about node structure data and this routine or submethod will be described in more detail below in connection with .

Next in block the framework manager may review the dependency data that is part of the node structure data received in block . In decision block the framework manager may determine if the node structure data defines a leaf node . A leaf node generally means that the node to be created based on the node structure data does not have any dependencies such as the nodes and in . If the inquiry to decision block is positive meaning that the node structure data for creating the current node does not have any dependencies then the framework manager continues to routine block .

If the inquiry to decision block is negative then the No branch is followed to decision block in which the framework manager determines if all of the hard dependencies within the node structure data exist. A hard dependency may comprise one in which a resource cannot exist without it. Meanwhile a soft dependency may comprise one in which a resource may use the dependent resource as an optional step. A soft dependency means that a node or resource of the node which has a soft dependency may be created or instantiated within the node architecture even when the soft dependency does not exist.

An example of a soft dependency may comprise an optimization feature that is not critical to the operation for a resource oriented node containing multiple resources. The framework manager may create or instantiate a node or a resource for all hard dependencies that are present even when a soft is dependency is not present for those nodes or resources which have soft dependencies that are not created. A call back feature may be used to reference the soft dependency so that when the soft dependency becomes available to the framework manager the framework manager will inform each callback referencing the soft dependency that the soft dependencies are now available.

If the inquiry to decision block is negative then the No branch is followed to block in which the node structure data is stored by the framework manager in temporary storage such as memory and the framework manager creates a call back feature associated with this un instantiated node.

If the inquiry to decision block is positive then the Yes branch is followed to routine in which a node is created or instantiated based on the node structure data received in routine block . Further details of routine block will be described below in connection with . Next in block the framework manager publishes the newly created node using its unique resource name s so that other nodes may send information to or receive information from the newly created node .

Referring now to which is a continuation flow chart of in block the framework manager notifies other nodes which are dependent on the newly created node that the newly created node has been instantiated and is ready to receive or transmit information. According to one exemplary aspect notifications are triggered immediately when a dependent node like node B of is created i.e. the notifications are performed recursively. So if node B of is constructed node A is immediately notified. This notification may allow node A to be constructed since node B was node A s final dependency . Construction of node B may causes other nodes to be notified and so on. Node B does not get completed until the final resource dependent on node B is completed.

A second slightly more complex implementation is to put all of the notifications onto a separate notification queue and then run through the queue beginning at a single point in time i.e. the notifications are performed iteratively. So when node B of is constructed the notification to node A is pushed onto a list. Then that list is executed and node A is notified. This causes the notification to other additional nodes besides node A not illustrated in to be put on the same list and that notification is then sent after the notification to node A is sent. The notifications to other nodes besides the notification to node A does not occur until after all the work associated with node B and node A has been completed.

Logically these two implementations are equivalent but they have different memory consumption properties when implemented. The recursive realization is simple but can consume an arbitrary amount of stack space with the stack consumption being a function of the depth of the dependency graph. The iterative implementation is slightly more complex and requires a bit more static memory the notification list but stack usage is constant irrespective of the depth of a dependency graph such as illustrated in .

Also notification of node creation in block is not limited to other nodes. It may also used internally for alias construction. Any arbitrary element in the system A may use the same mechanism to request for notification when a node becomes available not just other nodes. Both nodes and non nodes may use the same notification mechanism.

In decision block the framework manager determines if other nodes or soft dependencies are now released for creation or instantiation based on the creation of the current node . Decision block generally determines whether resources may be created because certain dependency relationships have been fulfilled by the current node which has recently undergone creation or instantiation.

If the inquiry to decision block is positive then the Yes branch is followed back to routine block in which the released node may now be created or instantiated because of the fulfillment of a dependency by the node that was just created.

If the inquiry to decision block is negative then the No branch is followed to block in which the frame work manager may manage communications between elements of the software architecture as illustrated in . Next in block the framework manager may continue to log or record actions taken by resources by using the resource names associated with a particular resource. Block may be executed by the framework manager after any action taken by the framework manager or any of the elements managed by the framework manager such as the resources nodes clients events and query functions . Block shows another aspect of the node architecture in which the framework manager may maintain a running log of activity that lists actions performed by each element according to their unique identifier or name provided by the authors who created a particular element such as a resource of a node .

Compared to the prior art this logging of activity in block that lists unique names assigned to each resource of a system is unique and may provide significant advantages such as used in debugging and error troubleshooting. Another unique aspect of the node architecture A is that separate teams may work on different hardware and or software elements independently of one another in which each team will be able to use resource names that are unique and easy to track without the need for creating tables to translate less meaningful and usually confusing resource names assigned by other teams and or the original equipment manufacturer OEM .

Next in decision block the framework manager determines if a log of activity recorded by the framework manager has been requested. If the inquiry to decision block is negative then the No branch is followed to the end of the process in which the process returns back to routine . If the inquiry to decision block is positive then the Yes branch is followed to block in which the framework manager sends the activity log comprising meaningful resource names and respective actions performed by the resource names to an output device such as a printer or a display screen and or both. The process then returns to routine block described above.

Block is the first step in the sub method or routine of . In block the framework manager may receive a unique name for a software or hardware element such as the CPU and the clock of . As discussed previously a node must reference at least one resource. Each resource has a unique name in the system A. Each element within the system A may be identified with a unique name. Each element has a unique name from a character perspective. In other words generally there are no two elements within the system A which have the same name. According to exemplary aspects of the system resources of nodes may generally have unique names across the system but it is not required that client or event names be unique though they may be unique as desired.

For convenience a conventional tree file naming structure or file naming metaphor that employs forward slash characters for creating unique names may be employed such as but not limited to core cpu for CPU and clk cpu for clock . However as recognized by one of ordinary skill the art other types of resource names containing any other combination of alphanumeric characters and or symbols are well within the scope of this disclosure.

Next in block the framework manager may receive data for one or more driver functions associated with one or more resources of the node being created. A driver function generally comprises the action to be completed by one or more resources for a particular node . For example in the driver function for the resource core cpu of node may request the amount of bus bandwidth and the CPU clock frequency it requires in order to provide the requested amount of processing that has been requested. These requests would be made via clients of the resources in nodes and node . The driver function for clk cpu in node would usually be responsible for actually setting the physical clock frequency in accordance with the request it received from the core cpu resource of node .

In block the framework manager may receive node attribute data. The node attribute data generally comprises data that defines the node policies such as security can the node be accessed via user space applications remotability can the node be accessed from other processors in the system and accessibility can the resource support multiple concurrent clients . The framework manager may also define attributes that allow a resource to override default framework behavior such as request evaluation or logging policy.

Subsequently in block the framework manager may receive customized user data for the particular node being created. The user data may comprise a void star field as understood by one of ordinary skill in the art with respect to the C programming language. User data is also known to one of ordinary skill in the art as a trust me field. Exemplary customized user data may include but is not limited to tables such as frequency tables register maps etc. The user data received in block is not referenced by the system B but allows for customization of a resource if the customization is not recognized or fully supported by the framework manager . This user data structure is a base class in the C programming language intended to be extended for particular or specific uses.

One of ordinary skill the art recognizes that other kinds of data structures for extending specific uses of a particular class are within the scope of this disclosure. For example in the programming language of C C plus plus an equivalent structure may comprise the key word public which would become an extension mechanism for a resource within a node .

Next in block the framework manager may receive dependency array data. The dependency array data may comprise the unique and specific names of one or more resources on which the node being created is dependent. For example if the first node of was being created then in this block the dependency array data may comprise the resource names of the three resources of the second node and the single resource name of the third node on which the first node is dependent.

Subsequently in block the framework manager may receive resource array data. The resource array data may comprise parameters for the current node being created such as parameters relevant to the first node of if this first node was being created. The resource array data may comprise one or more of the following data the names of other resources unit maximum value resource attributes plug in data and any customized resource data similar to the customize user data of block . The plug in data generally identifies functions retrieved from a software library and usually lists the client types that may be supported by the particular node or plurality of nodes being created. The plug in data also allows for customization of client creation and destruction. After block the process returns to block of .

In the attribute data block customized user data block and the dependency array data block have been illustrated with dashed lines to indicate that these particular steps are optional and not required for any given node . Meanwhile the unique name block a driver function block and resource array data block have been illustrated with solid lines to indicate that these steps of routine are generally important for creating a node .

In block the framework manager may create or instantiate the one or more resources corresponding to the node structure data of block . Next in block the framework manager may activate the driver functions received in routine block of routine block . According to one exemplary aspect the driver functions may be activated using the maximum values received in the resource array data block of routine block . According to another preferred exemplary aspect each driver function may be activated with an optional initial value that is passed along with the node structure data from routine . If initial data is not provided the driver function is initialized at 0 the minimum value. The driver function is also usually activated in manner such that it is known that it is being initialized. This enables the resource to perform any operations that are specific to initialization but do not need to be performed during normal or routine operation. The process then returns to step of .

Next in block customized user data may be received by the framework manager if there are any particular customizations for this client being created. Block has been illustrated with dashed lines to indicate that the step is optional. The customized user data of block is similar to the customized user data discussed above in connection with the creation of resources for nodes .

In block the framework manager receives the client type category assigned to the particular client being created. The client type category as of this writing may comprise one of four types a required b impulse c vector and d isochronous. The client type category list may be expanded depending upon the resources being managed by the system and upon the application programs relying upon the resources of the nodes .

The required category generally corresponds with the processing of a scalar value that is passed from the required client to a particular resource . For example a required request may comprise a certain number of millions of instructions per second MIPs . Meanwhile the impulse category generally corresponds with the processing of a request to complete some activity within a certain period of time without any designation of a start time or stop time.

An isochronous category generally corresponds with a request for an action that is typically reoccurring and has a well defined start time and a well defined end time. A vector category generally corresponds with an array of data that usually is part of multiple actions that are required in series or in parallel.

Subsequently in block the framework manager receives data that indicates whether the client has been designated as synchronous or asynchronous. A synchronous client is one that typically requires the framework manager to lock a resource of a node until the resource returns data and an indication that the resource has finished completing the requested task from the synchronous client .

On the other hand an asynchronous client may be handled by one or more threads in parallel which are accessed by the framework manager . The framework may create a callback to a thread and may return a value when the callback has been executed by a respective thread. One of ordinary skill the art recognizes that the asynchronous client does not lock up a resource like a synchronous client does when the task of the synchronous client is being executed.

After block in decision block the framework manager determines if the resource identified by the client are available. If the inquiry to decision block is negative then the No branch is followed to block in which a null value or message is returned to a user indicating that the client cannot be created at this time.

If the inquiry to decision block is positive then the Yes branch is followed to decision block in which the framework manager determines if each resource identified by the client supports the client type provided in block . If the inquiry to decision block is negative then the No branch is followed back to block in which a null value or message is returned indicating that the client cannot be created at this time.

If the inquiry to decision block is positive then the Yes branch is followed to block in which the framework manager creates or instantiates the client in memory. Next in block if any customized user data is received in block such as optional arguments then these optional arguments may be mapped with their respective resources to a particular node . Next in block the newly created client is coupled to its corresponding one or more resources in an idle state or on requested state as described above. The process then returns to block of .

Block is the first step in the method for creating a client request against the resource . This method will describe how the following three types of client requests are handled by the framework manager a required b impulse and c vector. As the names of the requests mentioned above suggest client requests generally correspond with client types that were created and described above.

In block the framework manager may receive the data associated with a particular client request such as one of the three mentioned above a required b impulse and c vector. The data associated with a required request generally comprises a scalar value that is passed from the required client to a particular resource . For example a required request may comprise a certain number of millions of instructions per second MIPs . An impulse request comprises a request to complete some activity within a certain period of time without any designation of a start time or stop time.

Data for a vector request generally comprises an array of multiple actions that are required to be completed in series or in parallel. A vector request may comprise an arbitrary length of values. A vector request usually has a size value and an array of values. Each resource of a node may be extended to have a pointer field in order to support a vector request. In the C programming language the pointer field is supported by the union function as understood by one of ordinary skill in the art.

Next in block the framework manager issues the request through the client that was created by the method described above in connection with . Subsequently in block the framework manager double buffers the request data being passed through the client if the request is a required type or a vector type. If the request is an impulse type then block is skipped by the framework manager .

For required requests in this block values from a prior request are maintained in memory so that the framework manager may determine if there is any difference between the previous requested values in the current set of requested values. For vector requests prior requests are usually not maintained in memory although a resource of a node may maintain it as desired for a particular implementation. Therefore block is optional for vector types of requests.

In block the framework manager calculates the delta or difference between the previous set of requested values in the current set of requested values. In decision block the framework manager determines if the current set of requested values is identical to the previous set of requested values. In other words the framework manager determines if a difference exists between the current set of requested values and the previous set of requested values. If there is no difference between the current set and previous set of requested values then the Yes branch is followed which skips blocks through block to block in which the process ends.

If the inquiry to decision block is negative meaning that the set of requested values are different relative to the set of pre previous requested values then the No branch is followed to decision block .

In decision block the framework manager determines if the current request is an asynchronous request. If the inquiry to decision block is negative then the No branch is followed to block in which the resource corresponding to the client request is locked by the framework manager . If the inquiry to decision block is positive meaning that the current request is asynchronous request type then the Yes branch is followed to block in which the request may be pushed onto another thread and may be executed by another core if a multi core system like that of is currently managed by the framework manager . Block has been illustrated with dashed lines to indicate that this step may be optional if the PCD is a single core central processing system.

Subsequently in block the resources corresponding to the request is locked by the framework manager . Next in block the resource executes the update function which generally corresponds to the plug in data of the resource array data received in block of . The update function generally comprises a function responsible for the new resource state in light of a new client request.

The update function compares its previous state with the requested state in the client request. If the requested state is greater than the previous state then the update function will perform the client request. However if the requested state is equal to or less than the current state and which the resource is operating at then the client request will not be performed in order to increase the efficiency since the old state achieves or satisfies the requested state. An update function takes a new request from the client and aggregates it with all the other active requests to determine the new state for the resource.

As an example multiple clients may be requesting a bus clock frequency. The update function for the bus clock would usually take the maximum of all the client requests and use that as the new desired state for the bus clock. It is not the case that all resources will use the same update function although there are some update functions that will be used by multiple resources. Some common update functions are to take the maximum of client requests to take the minimum of client requests and to sum the client request. Or resources may define their own custom update function if their resource needs to aggregate requests in some unique way.

Next in block the framework manager passes the data to the resource corresponding to the client so that the resource may execute the driver function which is specific to the resource of a node . A driver function applies the resource state as computed by the update function. This may entail updating hardware settings issuing requests to dependent resources calling legacy functions or some combination of the above.

In the previous example the update function computed the requested bus clock frequency. The driver function may receive that requested frequency and it may update the clock frequency control HW to run at that frequency. Note that sometimes it is not possible for the driver function to meet the exact requested state that update function has computed. In this case the driver function may choose the frequency that best meets the request. For example the bus clock HW may only be able to run at 128 MHz and 160 MHz but the requested state might be 150 MHz. In this case the driver function should run at 160 MHz as that exceeds the requested state.

Next in block the framework receives state control from the resource which has executed the driver function in block . Subsequently in block if defined against the resource events may be triggered so that data is passed back to the client which corresponds to the event . Events may be processed in another thread. This may minimize the amount of time spent with the resources locked and allows for parallel operation in a multi core system as illustrated in .

One or more events may be defined against a resource in a manner similar to how a request may be defined against a resource as described in this method . In other words the event creation process may largely parallel the client creation process. One thing that is different with the events is that it is possible to define events that only get triggered when certain thresholds are crossed.

This defining of events that only get triggered based on thresholds allows for notification of when a resource is getting oversubscribed it has more concurrent users than it can support which is indicative of a system overloading condition or when a resource goes low off which may allow other things to be shut off restore functionality that was disabled when the system became oversubcscribed etc. Because the event registration may be done with thresholds it reduces the amount of work the system has to do on event notification to only happen when there is something really necessary. It is also possible to register for an event on every state change.

Next in optional block if the request being processed is a vector request then this optional block is usually performed. Optional block generally comprises a check or determination to assess whether the vector pointer is still positioned on the same data that the user passed into the vector. If the inquiry to this optional block is positive meaning that the pointer is still pointing to the same data which was passed by the user into the vector then the pointer is cleared out so that references to old data is not maintained. This optional block is generally performed to account for the double buffering block described above when a vector request is being processed compared to an impulse request and a required request.

Subsequently in block the framework unlocks the requested resource so that other client requests may be handled by the current but now released requested resource of a particular node . The process then returns to the first block for receiving the next client request.

The above described methods and data structures are essentially as applicable to a multi processor PCD as they are to a single processor PCD . However the remoting framework may provide additional features that may enhance operation in a multi processor embodiment.

For example the remoting framework may advantageously render the details of inter processor communication transparent to an application programmer or similar person. Thus an application program for example may define a client that issues a request on a target resource without having to include in the client definition any identification of the processor domain that controls that resource. Rather the remoting framework ensures that the request will reach the target resource regardless of which processor controls the client and which processor controls the target resource.

In addition the remoting framework manages the inter processor communication so that for example an application program need not include any instructions relating to the protocol or other aspects of the communication paths e.g. buses between processors. Furthermore as different inter processor communication paths may use different protocols the remoting framework allows the resource definition to specify a protocol along with other aspects of the resource. These and other features relating to distributed resource management are described below with regard to .

A distributed resource is used as a means to access the corresponding native resource. In this example the term resource may be used interchangeably with the term node as it should be understood that a resource may be included in a node.

A broken line illustrates a division between resources controlled by the first processor to the left of the line and resources controlled by the second processor to the right of the line . The first resource is one of two or more resources that are controlled by the first processor. One such resource may be a protocol resource on which the first resource depends. Likewise the second resource is one of two or more resources that are controlled by the second processor. One such resource may be a protocol resource on which the second resource depends.

The first and second resources and may also depend on additional resources in the same manner as described above with regard to resources or nodes in general but such additional resources are not shown in for purposes of clarity. Note that the resources controlled by the first processor are defined by a first resource graph i.e. a directed acyclic graph and the resources controlled by the second processor are defined by a second such resource graph that does not share any resources with the first resource graph.

The first and second resources and under control of their respective processors are capable of communicating information via a communication path . The communication path represents the combination of the physical medium between the first and second processors and the one or more layers of transport protocols used to communicate via that medium. Accordingly any communications between the first resource and the second resource must conform to the protocols. Protocol resources and define a protocol or may point to a protocol definition in a library not shown . The remoting framework and main framework operate in conjunction with one another to manage the resources and communications between them. As described below a client under control of the first processor may issue one or more resource requests on the first resource . The first resource uses the functionality of the corresponding second resource to service the resource request.

In different application states it may be necessary or desirable for a processor to request different configurations or states of resources. For example a bus resource may control the speed of a bus clock. In one application state a processor may request a bus clock that allows the processor to operate at a rate of for example 100 million instructions per second MIPS while in another application state the processor may request a bus clock that allows it to operate at a rate of for example 150 MIPS. In the case of a processor preparing to enter an application state that is a sleep state the processor may request a bus clock of zero MIPS.

Similarly in one application state defined by a processor executing a first application program the processor may request MIPS while in another application state defined by the processor executing a second application program the processor may request MIPS. Likewise in one application state defined by a processor concurrently executing a certain number of application programs the processor may request MIPS while in a second application state defined by the processor concurrently executing a different number of application programs the processor may request MIPS. It should be understood that the above referenced bus clock is intended only as an example of a resource that can be configured by a processor issuing a resource request and also that the numbers 100 and 150 are intended as arbitrary examples of processing speeds.

Resource configurations or states may be grouped into resource state sets. A resource state set defines the configurations or states of one or more resources that are used together by a processor in a certain processor application state. For example a certain resource state set may include configuration or state information for a bus clock resource to provide a processor with a certain number of MIPS of processing speed and configuration or state information for a decoder i.e. another example of a resource to provide a decoding function to the processor.

The system may switch among resource state sets desired by a processor in a manner that minimizes resource latency. The term resource latency refers to the delay or latency that occurs between a time at which a master processor begins preparing controller and resource power manager to transition to another resource state set and the time that the resources of that set become configured to the specified states and ready for use by the processor. As described below resource state sets can be broadly categorized into active resource state sets in which a processor is provided with resources configured to aid the processor in executing application programs and otherwise providing processing power and a sleep resource state in which a processor is provided only with resources that aid the processor in maintaining a sleep state i.e. a state in which the processor is not executing application programs or otherwise providing processing power. Although a processor in a sleep state may maintain low level functions the processor does not execute software that would be understood by one of ordinary skill in the art to be an application program. It should be understood that the next active state feature described below may be applied to transitions between any resource state sets regardless of whether they may be active sets or sleep sets.

In the exemplary embodiment shown in the first master processor A may be coupled to the resource power manager and the controller . The controller may be coupled to the clock code A of the first master processor A. The controller may comprise one or more low level drivers . The one or more low level drivers may be responsible for communicating with one or more shared resources A C. Shared resources A C may comprise any type of device that supports tasks or functions of a master processor . Shared resources A C may include devices such as clocks of other processors as well as single function elements like graphical processors decoders and the like.

The shared resources A C may be coupled to one or more local resources D H. The one or more local resources D H may be similar to the shared resources A C in that they may comprise any type of device that supports or aids tasks or functions of a master processor . Local resources D H may include devices such as clocks of other processors as well as single function elements like graphical processors decoders and the like. The local resources D H may comprise leaf nodes. Leaf nodes are understood by one of ordinary skill in the art as local resources D H that usually do not refer or include other dependent resources .

The controller may be responsible for managing requests that are issued from the one or more master processors . For example the controller may manage a request that originates from the first master processor A. The first master processor A may issue this request in response to an operator manipulating the touchscreen . The touchscreen may issue signals to the touchscreen driver controller . The touchscreen driver controller may in turn issue signals to the clock code A of the first master processor A.

The controller may also be responsible for managing the sleep states for a particular processor . Prior to entering a sleep state a processor will provide information for managing sleep states. Information for managing sleep states includes the entry into and exiting from a sleep state. This information for managing sleep states will be referred to below as triggers and resource states. A resource state set may include resource information for configuring one or more resources in a manner that supports a sleep state of a processor.

Triggers may define events that cause a processor to either enter into a sleep state or to leave a sleep state. Triggers will generally reference resource states that are contained within or that are accessible by the controller . Resource states define a desired state of resources needed by particular processor . In an exemplary embodiment each processor may provide at least two resource state sets to a controller an active set of resource states and a sleep set of resource states.

However in other embodiments a processor may provide resource state sets in addition to a single active set and a single sleep set or resource state sets that are different from a single active set and a single sleep set. Such other resource state sets may correspond to one or more of the processor application states described above. That is for any application state the processor may provide a corresponding resource state set.

In the exemplary embodiment the active set of resource states may define states of resources for when the processor is actively performing processing functions and requiring action functions from its resources . The sleep set of resource states may define states of resources when the processor is in a sleep or idle state. Further details about triggers and resource states will be described below in connection with .

Each resource set generally comprises information relating to states of resources desired by a particular master processor . Each resource set assigned to a particular master processor may comprise an active resource set and a sleep resource set . The active resource set may define or describe states of resources when a particular master processor is active or functioning normally. The sleep resource set may define or describe states of resources when a particular master processor is in a sleep or dormant state as understood by one of ordinary skill in the art. Each resource set may also comprise additional sets such as set 1 and set 2 assigned to the first master processor in the exemplary embodiment illustrated in .

As an example the active resource set for the first master processor A A as illustrated in has assigned the following values for each of its resources for the first shared resource SR A the value is one the value for the second shared resource SR B is one the value for the Nth shared resource SR N C is one while the four values for the first local resource LR D are one zero one and one.

As noted previously states of resources are not limited to single values and may include a plurality of values. Further states of resources may include any of a number of different types of parameters. For example a state may designate hundreds of megahertz for the amount of clock speed of a particular clock that may function as a resource .

As another example the sleep resource set A for the first master processor A A as illustrated in has assigned the following values for each of its resources for the first shared resource SR A this resource has been assigned value of zero the second shared resource SR B has an assigned value of zero while the Nth shared resource SR N C has an assigned value of zero. The first local resource LR D may have assigned values of zero one zero and zero.

Each trigger set assigned to a particular master processor may comprise at least three fields an interrupt field a from set and a go to set . Each of these three fields of a trigger set may also include a corresponding set of three columns a trigger start column a clear column and a timer column .

The interrupt field describes the action or activity that may be generated and or detected by the resource power manager . The interrupt field may be generally characterized as the trigger event that may allow a controller to select a specific resource set which is desired by a particular processor based on the trigger event detected by the RPM . The selection of a resource set by the controller may avoid the time consuming software handshake described above in the background section.

Reviewing the first trigger set trigger set of for the first master processor A A the fields of the set are discussed in order by columns. Starting with the first column of the trigger set A the trigger start column has an action listed as decode interrupt in its first row corresponding to the interrupt field .

As noted previously the interrupt field may define parameters that cause the controller to activate the states of a resource set in response to the detection of the trigger start field . In the exemplary embodiment illustrated in the interrupt field A has been defined or described as a decode interrupt which means that when the resource power manager detects a decode interrupt such as when a PCD is decoding video then this event may alert the controller to review the from set field in the first column A under the trigger start column.

The from set field may comprise a value that denotes what the current resource set should be for the particular master processor being reviewed by the controller . This field may list a resource set by its identifier such as the active set the sleep set or a set number like set 1 or set 2 The field may also comprise a wild card like an asterisk.

A wildcard designation in the from set field may cause the controller to retrieve the last known active resource set that was being used by a particular master processor . In the exemplary embodiment illustrated in the from set row A and trigger start column A have a value of an asterisk or wildcard.

The go to set like the from set may comprise a listing of a resource set by its identifier such as the active set the sleep set or a set number like set 1 or set 2 . The field may also comprise a wild card like an asterisk that means the last resource set being utilized by a processor . In the exemplary embodiment illustrated in the go to set field A and the trigger start field column A has a value of set 1 which is the resource set 1 listed in column A of the first resource set A.

For the example illustrated in when a decode interrupt event is detected by the RPM it alerts the controller . The controller reviews the first trigger set for the first master processor . Since the trigger start column A lists a matching value a decode interrupt the controller reviews the from set field A and determines that the value is a wildcard value or asterisk. The controller then reviews the go to field A which has a value of set 1 that designates a particular resource set A. Based on this information reviewed by the controller the controller will switch the current resource set A for the first master processor A from its current set to the resource set set 1. Resource Set 1 is listed in column A of the resource set A assigned to the first master processor A.

Further when the RPM or the controller detects a not decode event such as illustrated in the clear column A of the first trigger set then the controller will then review the from set field A and determine that this value comprises set 1. The controller will then review the go to set field which has a value of a wildcard or an asterisk in this example. This means that the controller will switch the resource set A of the first master processor A from the set 1 resource set to the last active resource set used by the processor A.

The timer field of the trigger set may denote an amount of time that a particular resource set may be used by the controller . So for the exemplary embodiment illustrating for the timer field A of the first trigger set this field has a value of three milliseconds. This means that when the decode interrupt event is matched with the trigger start field A of the first trigger set then the controller utilizes the resource set specified in the go to set field A for only a period of three milliseconds. In other exemplary embodiments situations may occur or exist in which there is no information in the timer field or the value is defined to correspond with a value that indicates that there is no timer trigger for this transition and that the transition only applies to the no decode field. In a situation in which the timer field is defined such as illustrated in FIG. timer fields A and A then whichever event occurs first between the timer field and the Clear field will usually initiate the transition.

In the exemplary embodiment in when a shut down event is detected the controller transitions the current active resource set to a sleep set . The sleep set is listed in a master resource set of table in .

When the controller receives a message from the RPM that a bring up event has occurred such as a power on event initiated by an operator of the PCD then the controller would transition the processor from its sleep set to the last active resource set based on the wildcard or asterisk value listed in the go to set field of the trigger set .

As described above the system is not limited to active and sleep sets . The system may be used for switching between resource sets for events other than entering or exiting sleep states as illustrated in .

In block a processor may request the RPM to generate a shutdown signal to the controller . In block the RPM may send the shutdown signal to the controller .

The controller may receive the shutdown signal in block and activate the trigger sets which may be assigned to a shutdown event as illustrated in . In the exemplary embodiment illustrated in the shutdown signal is matched against the interrupt field of the trigger set . The trigger set directs the controller to access a sleep set as indicated in the go to set field . In block the controller may immediately send an acknowledgment signal to the RPM while the controller continues to activate resource sets that are referenced by the trigger sets which match the shutdown signal event.

In block for each matching trigger set such as the matching trigger set listing the shutdown event in the corresponding interrupt field illustrated in the controller may switch the current resource set to a sleep set such as the sleep set A of the first resource set A for the master processor A of .

Next in block the controller may issue sleep request states to low level drivers such as illustrated in . The low level drivers may pass the requested states to the corresponding resources .

In block each resource may issue a shutdown signal acknowledgment to the controller and the RPM . The method may then end.

Next in block the RPM may send a wake up signal to the controller . In block the controller may receive the wake up signal from the RPM and activate one or more trigger sets that matched the wake up signal. For example the controller may match the wake up signal with the bring up event listed in the interrupt field in the active column of the trigger set of . In the exemplary embodiment of the go to field in the active column directs the controller to the last resource set which was used by the current processor .

So in block the controller would change the current resource set for a processor based on this matching trigger set . One of ordinary skill in the art recognizes that the controller will cycle through all of its trigger sets that it maintains as illustrated in .

Next in block the controller may send a wake up acknowledgment to the RPM identifying which master processors have been awakened from the sleep state. Next in block each processor with a matching wake up trigger set is released from a sleep state and restored to its active state with power supplied by the RPM . The method then ends.

Blocks and are the same as blocks and respectively of and are therefore not described here. Note that when the processor begins shutting down it is in the awake application state corresponding to the awake set stored in the A buffer . The processor then enters the sleep application state corresponding to the sleep set that is stored in the B buffer in the same way as described above with regard to . The processor awakes from the sleep application state in the next awake application state corresponding to the next awake set that is stored in the C buffer . By pre storing the next awake set updates in the C buffer and applying them as soon as possible the controller is able to immediately begin configuring the resources specified by that next awake set upon a wake up event thereby helping to minimize resource latency.

Resources A N are described above in connection with and . Resources may comprise hardware and or software. The resources may receive requests managed by the scheduler .

The scheduler is part of the framework as described above in connection with . The scheduler is coupled to the framework manager . The scheduler provides a queuing feature which specifies at which time a request issued by a client See which illustrate details for clients and requests described above in connection with the framework manager destined for a resource will be applied based on the timing deadline specified in the request. Timing for the scheduler is tracked by the timer .

After a scheduled request is made by client then the calling application or entity may then continue on with its own processing. The scheduler will then determine the best way to execute the request based on the deadline provided by the client .

The scheduler looks at each scheduled request and determines how long the scheduled request will take to execute. The scheduler also performs back off calculations. The scheduler works with the timer and or with the sleep cycle of a processor to understand what scheduled requests can be applied and when they should occur to meet the requested deadline as specified by a client or calling entity.

The timer may comprise a clock that includes a 32 bit timer running at T tick time as understood by one of ordinary skill in the art. Other clocks may be employed without departing from this disclosure. For example 64 bit timer systems may be used as understood by one of ordinary skill in the art. For 32 bit timers the scheduler may determine if a time for a request is in the past or future by looking at the difference between the current time tracked by the timer and the requested time specified in the scheduled request .

To detect a late request versus one in the far future with a 32 bit timer system a desired time must be less than half the resolution of the time if the difference is less than UINT MAX 0X8000 0000 then it is considered by the scheduler to be in the future. If requested time is greater than or equal to UINT MAX the requested time is considered by the scheduler to occur in the past and the scheduler usually processes such a request immediately.

The timings described above determined by the scheduler are stored by the scheduler in the scheduler database . The scheduler database maintains information which may relate to various ways that requests may be performed and the respective timing needed for the requests when they occur in specific sequences. In other words the scheduler may track various options and their respective times that it may take for the scheduler to service requests according to different scenarios and store these values in the scheduler database . Further details about the scheduler database will be described below in connection with .

The sleep low power resource LPR determines what resources need to be triggered when the system wakes up from a sleep state by reviewing the database . The sleep LPR may be characterized as an application programming interface API . The sleep LPR will issue those requests that it determines to be present in the next awake set of a resource set managed by the controller and RPM .

The CPU busy monitor CBM resource is established by the scheduler . The CBM resource tracks the states of all CPUs that service requests managed by the scheduler . Any nonzero requests to the CBM resource indicate that the client or a user thread See issuing the request expects a CPU to remain busy during execution of the request . The CBM resource is a binary resource meaning that a CPU is considered busy by the CBM resource as long as any client is busy and the CPU is not characterized as free or not busy until the last client completes a nonzero or busy request. Further details about the operation and relationship between the scheduler and CBM resource will be describe below in connection with .

The scheduler database may include but is not limited to the following information thread configuration data scheduler latency minimum scheduler delta request latency fork look ahead delta fork latency join latency sleep wake transition latency time queue latency low power resource LPR enter latency LPR exit latency LPR now delta scheduled linked lists request states request times start times late probability callback notifications latest notifies states and request lateness .

The thread configuration data may track various information from threads that originate clients and requests . The scheduler latency may measure the time it takes to handle the scheduler code in ticks measured by the timer . The minimum scheduler delta may measure the time that a requests will be rescheduled because it is too far into the future to be handled by a current timeout.

The request latency may measure a default scheduled request latency used if a scheduled requests is issued on a resource . Request latency may comprise a time expected from issuing a fully asynchronous request for completion. The fork look ahead delta may measure the time to add to a sleep wake time determining what requests may be eligible to be forked. This delta may be used to compensate for the work that a sleep set may have to perform when waking up from a sleep state.

The fork latency is a default scheduled fork latency that may be used if a scheduled request is issued on a resource but the resource does not support a fork request latency query. Fork latency comprises the time from issuing a forked request to when it returns. A request for fork latency is only valid for a resource which is forkable.

Join latency may comprise the time when a fork operation completes until the resource is joined and the forked request is retired. The join latency is also a default scheduled fork latency that may be used if a scheduled request is issued on a resource but the resource does not support a join latency query.

The sleep wake transition latency measures a time that it takes for a sleep set to exit a sleep state the time needed for the timer to fire and any time needed when multiple requests are joined. The time queue latency may measure the time it takes for the timer to call a timer function.

The low power resource LPR enter latency may comprise a default time for overhead when using the LPR enter function. The LPR exit latency may comprise a default time for overhead when using the LPR exit function. The LPR now delta may comprise a default time for the LPR enter function to reschedule the timer .

The remaining data of the scheduler database may be characterized as request definitions as illustrated in . The scheduled linked list may track clients that are waiting to run. The request state may track the states of various requests such as waiting processing forked joined etc. the request time may comprise the time in which a requests must be completed. The start time may track the time needed to start a requests issued from a client .

The late probability parameter provides flexibility to the scheduler when it is time for the scheduler to schedule requests . If the late probability parameter is set equal to 0 then this means that a request may not tolerate any lateness with respect to its scheduling by the scheduler . If it is set equal to one then the scheduler may allow a request to be late if such lateness will help the scheduler process its current set of requests . The scheduler may also track values between 0 and 1 as an accepted rate of lateness. Thus a value between 0 and 1 can be set such that the client will accept half of the requests being late. Another value between 0 and 1 may be set such that all requests may be accepted late. And so on.

The callback notification tracks when a call back to a client issuing a request has been completed. The latest notify state tracks whether callback notifications have been issued on time have been issued late or the request was unscheduled. And request lateness may track times when requests were not late times when a requests were late and or the sum of time between when a late request finished and when it was requested. The scheduler database is not limited to these parameters. Other parameters may be tracked by the scheduler database as understood by one of ordinary skill the art.

Referring now to this figure illustrates an exemplary timing diagram that demonstrates relationships between clients client requests the scheduler and the timer . The Y axis of corresponds with time. See prior discussion of for the baseline discussion of clients and requests .

A client established by a user thread wants to make a scheduled request to a resource not illustrated to occur at a specific requested deadline of time X. The user thread issues the request after the client has been created within the framework .

The request may comprise any type of request such as but not limited to a vector request a scalar request or any other standard synchronous request as described previously. The scheduler places the information from the request into the database . The information from the request may include but is not limited to the information described above in in connection with the database .

Specifically the scheduler determines at what time each request should be executed so that all requests are finished or executed by their respective deadlines. The scheduler may also calculate back off for each request . The scheduler maintains this information in the database . As described previously the database maintains information which may relate to various ways that requests may be performed and the respective timing needed for the requests when they occur in specific sequences. In other words the scheduler may track various options and their respective times that it may take for the scheduler to service requests according to different scenarios and store these values in the scheduler database .

After time at client block the user thread or requesting entity may continue with its own processing of other data or functions. The scheduler determines the time at which the one or more resources must be awakened to process one or more requests and then the scheduler issues a timing command to the timer at stage A.

At stage the timer expires or hits the time specified by the scheduler and issues a signal to the scheduler . At stage the scheduler queries the database and determines the one or more requests that it needs to process. While the scheduler is executing the requests listed in the database for the particular stage the scheduler is measuring and tracking if requests are being processed on time or if they are running late. The scheduler at this stage also determines the respective durations of the requests for their completion and records this information in the database .

The scheduler at the stage may also determine to delay the processing or execution of certain requests and the scheduler may then issue additional timing commands to the timer setup desired timer event so that these other requests as listed in the database may occur at a later time.

The scheduler will try to honor and execute all requests in the database even if the scheduler needs to run certain requests later than their requested deadlines as specified by the client in accordance with the user thread .

When the scheduler determines that a resource completes a request A it generates a client notification or call back A. In this client notification A the scheduler will inform the client if the request A was performed on time by the requested deadline or if it was performed later than the requested deadline.

Each request may also comprise additional information such as a probability of lateness as described above in connection with . This probability of lateness parameter may provide flexibility to the scheduler when it is time for the scheduler to schedule requests .

If the probability of lateness parameter is set equal to 0 then this means that a request may not tolerate any lateness with respect to its scheduling by the scheduler . Meanwhile if the probability of lateness parameter is set equal to 0x8000 0000 out of 0xffff ffff fifty percent this means that the request may tolerate lateness out of every 100 times one percent that the request is processed by the scheduler . The probability of lateness parameter allows the scheduler to schedule requests with some flexibility.

For example suppose a request may take at least 10 milliseconds ms to be completed by a resource . In some situations the request may only take 1 ms to be completed by a resource ninety nine percent of the time. If the probability of lateness parameter for this request is set equal to zero then the scheduler must schedule this request at least 10 seconds out from its requested deadline so that the request is completed on time.

However if the probability of lateness parameter is set equal to 0x2900000 meaning one percent then the scheduler at its discretion may schedule this request one second out from its requested deadline such that the request may be completed on time if its duration only lasts one msecond. But the request could finish much later than its requested deadline if its duration exceeded 1 ms such as the worst case 10 ms scenario described above.

The database may also maintain a worst case value for each request . The worst case value typically comprises the most conservative estimation of a request start time see database parameter discussed above in connection with so that a request is completed by a specified deadline. As noted previously each request has a deadline specified by the client via a user thread . As a default when the probability of lateness parameter is not specified or provided with a value in the request then the scheduler will always use the worst case value for the request .

Referring now to this figure illustrates an exemplary timing diagram that demonstrates relationships between clients client requests the scheduler the timer and the controller that tracks sleep sets. The Y axis of corresponds with time. See prior discussion of for the description of clients and requests . See also that illustrates further details of the controller that tracks sleep sets as previously described above.

A client via a user thread wants to make a request to a resource not illustrated . The client wants the request to occur at a specified time. The request may comprise any type of request such as but not limited to a vector request a scalar request or any other standard synchronous request.

The scheduler puts the information from the request into the database . As noted above the scheduler figures out what time each request should be executed so that all submitted requests are finished or executed by their respective deadlines. The scheduler at this stage calculates back off After time at request block the user thread or requesting entity may continue with its own processing of other data or functions. The scheduler determines the time at which the one or more resources not illustrated must be awakened to process request and then the scheduler issues a timing command to the timer at stage A.

A sleep event is scheduled to occur at stage . At stage the controller or scheduler reviewing the resource states of the controller stores in the database the duration of the sleep event or sleep cycle at stage and its anticipated wake up time at stage . At stage the scheduler may communicate the next awake set of a resource set See to the resource power manager RPM so that the RPM will know which awake set should be used once the sleep cycle at stage has finished.

The scheduler may include one or more requests that should now be made part of the next awake set of a resource set so that these requests are executed when the system comes out of the sleep state at stage . After the RPM communication at stage in which the next awake sets are communicated to the RPM at stage the scheduler issues the command handle request . The command handle request causes the RPM to include any subsequent requests at this stage to be part of the next awake set of a resource set managed by the controller .

As noted previously the database at a high level tracks clients resources and requests . The database may also track the requested deadline which is part of a request .

The database may also maintain the calculations made by the scheduler that include the time each request needs to be completed the time each request needs to be started the end time for each request the duration to run a request the duration needed to perform a notification or callback to the originating client or user thread the duration of all handling and the duration of any timers before a request is executed.

If there are multiple requests then the database tracks calculations made by the scheduler on any overlap in timing which may occur between multiple requests and any shifting in timing because of this overlap. Specifically the high level data mentioned above may be tracked by the information as listed in described above.

A resource may provide information to the database on how long certain requests may take also referred to as latency for execution by that resource or another resource . The scheduler may query a resource for at least three types of latency data as described above in connection with request latency fork latency and join latency . If the resource does not provide a value for any latency the resource s default values will be used.

If a resource would rather handle a request directly and not have the scheduler involved it can return a latency value of zero in response to the request latency command from the scheduler . Upon receiving this zero value for latency scheduler may then drop the handling of the request so that the resource handles the request directly.

A resource may provide additional information beyond what is requested in a latency request from the scheduler . For example a resource may inform the scheduler that the resource supports at least two modes of operation with respect to the actual request issued by the client via the user thread to be serviced by the resource . The resource may provide the scheduler with data on latency for each of the two modes of operation.

At stage just prior to the sleep state at stage the scheduler via the controller handling the sleep state will call the sleep scheduler low power resource LPR . The sleep LPR determines what resources need to be triggered when the system wakes up from the sleep state at stage by reviewing the database . The sleep LPR may be characterized as an application programming interface API .

The sleep LPR will issue those requests that it determines to be present in the next awake set of a resource set managed by the controller . At this stage tremendous energy savings may be realized since the sleep LPR is preplanning requests that need to be processed when the system that includes a processor exits from the sleep state at stage .

Specifically just prior to the sleep state at stage the sleep LPR will identify which resources have scheduled requests that could be advanced and processed upon exiting the sleep state at stage and that may be placed in the next awake set of a resource set .

The requests will be issued as forkable requests and the sleep LPR will then notify the controller and the scheduler of the new deadline for the scheduler to perform the join functions. The sleep state at stage can then proceed normally. When the system or processor wakes up at stage the RPM will signal the join and the LPR s exit function will signal the scheduler to join the requests . When the sleep state is exited at stage the scheduler may complete any final work needed prior to use of a resource and then invoke a scheduled client s completion callback or client notification A.

The first request is created in block A as a first client A while the second request is created in block B as a second client B. The user thread indicates to the scheduler that these two requests A B from clients A B will be scheduled requests and that the first request A from the first client A is going to be completed earlier in time relative to the second request B from the second client B.

The user thread then issues the requests A B as represented by blocks A B. While the first request A from the first client A is to occur prior to the second request B from the second client B the user thread may issue the requests in any order. Therefore as illustrated in the second request B from the second client is issued at stage before the first request A from the first client at stage .

The scheduler records the data from the issued requests B B in the database at this stage and establishes the time the request will be processed by working with the timer . Specifically the scheduler informs the timer of certain triggers as described above in connection with . Triggers indicate at what time each request A B should be started.

When the timer expires at stage A which may correspond with the triggers that were communicated to the timer by the scheduler noted above the scheduler at block A may process the first request A at block A. Then at block A the scheduler may issue the client notification that the first request A was completed by the scheduler .

In block B the scheduler may then process the second request B. Then in block B the scheduler may issue the client notification that the second request B was completed by the scheduler . As noted previously the first request A was processed prior to the second request B since the first request A had a finish or end time that was earlier then the finish or end time for the second request B.

According to one exemplary embodiment the scheduler manages requests on a first come first served basis. This means that if two or more requests have the same desired end time or completion time then the scheduler may process and complete the request which first reaches the scheduler for scheduling. The later to arrive request at the scheduler will be completed in time after the request which reaches the scheduler first even though both requests may have the same completion deadline.

So for the exemplary embodiment illustrated in if both the first and second requests A B had the same desired completion times then the second request B would have been completed prior to the first request A because the second request B reached the scheduler first before the first request A did. It follows then that blocks A and A and blocks B and B would have been reversed to reflect the completion of the second request B prior to the completion of the first request A.

However the scheduler is not limited to this exemplary embodiment of the first come first serve protocol. In other exemplary embodiments the scheduler may be provided with functionality that allows it to schedule requests according to how quickly they may be completed and or how their completion may impact the power savings for the portable computing device .

In some exemplary embodiments there may be no overlap in the timing between requests . For example if the first request A was required to be completed at a hypothetical time deadline of 100 with a back off of 10 ms and if the second request B was required to be completed at time with a back off of 10 ms then there is no overlap in the scheduling of these two requests A B with these timings. Each request in such an exemplary scenario would be provided with its own timer event such that two timer events would be established to process these two non overlapping requests A B.

Usually multiple requests may be collapsed and processed by the scheduler when there is some overlap in time between multiple requests . Overlap between requests may be defined in terms of timing as well as system overhead needed to process the requests .

For example the scheduler may determine if it is more efficient to process two requests concurrently or in sequence rather than creating two separate timer events if the requested deadlines for the two requests are significantly far apart from one another. In other words the scheduler may override requested completion deadlines if it determines that scheduling two requests concurrently or in sequence would be more efficient than creating two separate timer events with timer .

This efficiency determination by the scheduler of when requests should be processed prior to a requested deadline may take into consideration power consumption of the PCD . The scheduler may schedule requests earlier than their requested deadlines if it determines that the PCD currently has sufficient power to process the requests compared to a requested deadline in which the PCD may have less power to process one or more requests . The scheduler may track power consumption of requests by determining if a particular request relates to an ON state or if a particular request relates to an OFF state.

There two situations with respect to unscheduled requests a requests which have not been processed by the scheduler and b requests which are currently being processed by the scheduler . illustrate requests which have been created but have not been processed by the scheduler . Meanwhile illustrates requests that have been created and which are being processed by the scheduler and then a client later issues an unscheduled request command while the request is being processed.

Referring back to a client A is created at stage A as represented by block A. Before a request is issued by the client A an unscheduled request command is issued at stage . In this scenario the scheduler usually may simply remove the undesired requests from its list of scheduled requests which are stored in the database . At stage a new immediate request may be issued without it being scheduled as illustrated or another scheduled request can be started repeating A and A on the client .

Then at stage the client A issues the request . At stage A the database with its start times for the one or more requests will then be updated. Then at stage B an unscheduled request command has been issued by the client A. At this point the database may be updated by the scheduler in which the request may be removed from the database . The request may be removed from the database because the request has not been processed by the scheduler i.e. the request has not been sent to its requested resource . At this stage another immediate or scheduled request can be started.

According to this exemplary embodiment when the user thread or client issues the unscheduled request command at stage the scheduler issues a lock on the resource as illustrated by block which blocks out the unscheduled request command as indicated by block . In this way the scheduler completes the scheduled request in block irrespective of the unscheduled request command that was issued at stage .

So in this exemplary embodiment of in response to the unscheduled request command at stage the user thread or client may receive a client notification as well as a return value for the unscheduled request command that the request has been processed completed.

In a scenario where the request has been placed in the next awake set of a resource set to be completed by the sleep LPR and the user thread has issued an unscheduled request command and the client notification is not issued until the system wakes up the scheduler may inherit the priority of the request which allows the scheduler to act immediately on the request .

This becomes a synchronization point that allows the user thread and the scheduler to finish a request immediately rather than waiting for its requested deadline. The scheduler will issue a notification back to the user thread indicating that the request has been completed and therefore the unscheduled request command is also complete.

Between stages 1 and 2 the single application is running and the CPU is active and has a voltage like 5 milliVolts. Then the CPU enters into a sleep state between stages 2 and 3 on the X axis timeline in which there is zero voltage. Similarly between stages 3 and 4 on the X axis timeline the CPU is active and again has a voltage such as 5 milliVolts.

At stage 1.1 a user thread not illustrated may issue a request to a CPU busy monitor CBM resource as described above in . The CBM resource is part of and or established by the scheduler . Any nonzero requests to the CBM resource indicates that the client or user thread issuing the request expects the CPU to remain busy during execution of the request . The CBM resource is a binary resource meaning that the CPU is considered busy by the CBM resource as long as any client is busy and the CPU is not characterized as free or not busy until the last client completes a nonzero or busy request.

At stage 1.2 a suppressible resource request may be issued by the scheduler to a resource . A suppressible request is a resource request issued from a suppressible client and these requests have the property that they do not need to be honored when the CPU goes idle or enters the sleep state . In other words suppressible requests may be shut off when a CPU enters into a sleep state .

Suppressible requests are in direct contrast to required requests which are required to be honored at all times by a CPU . By issuing a suppressible request at stage 2 there is no requirement to explicitly cancel the request prior to entering the sleep state .

In the particular case of resources that are serviced by the RPM a suppressible request is not added to a sleep set of a resource set and is only added to an active set of a resource set . Meanwhile a required request is added to both active and sleep sets of a resource set See as understood by one of ordinary skill in the art.

At stage 2.1 once the request is completed by the resource the resource issues a notification back to the CBM resource of the scheduler indicating that the resource expects to go idle soon. The resource at stage 2.1 begins conducting clean up work in association with the completion of the request .

At stage 2.2 just before the sleep state the client or user thread may schedule a new request using a discretionary function feature and indicate to the scheduler that it needs a new request to be processed at stage 3 on the X axis. The discretionary function or feature may comprise data that is part of the new request .

The discretionary function informs the scheduler that the client has no expectation that the prior request or current request being completed will be persisted until the new request takes effect. In other words with this additional discretionary function or feature that is part of a new request the scheduler will recognize that the current client request being processed does not need to be completed or continued with his processing when the new scheduled request takes effect. With this discretionary function the scheduler has the freedom but not a requirement to complete a current request at the scheduler discretion.

At stage 2 the scheduler may review the current state of the CPU being tracked by the CBM resource . If the scheduler notices that the CPU is not in a busy state as tracked by the CBM resource the scheduler may allow the sleep state via controller and its resource sets of to turn the CPU off for the idle or sleep state between stages 2 and 3 without having the need to cancel the new request that was issued with the discretionary function. In this way the RPM via the sleep set of the resource sets see does not need to issue any cancel requests before the actual sleep state is entered.

In summary the scheduler may look into the busy state tracked by the CBM resource to determine if a sleep state is expected soon for the CPU . With the discretionary function of a new request the scheduler can decide whether to end cancel a current request and schedule the new request if it anticipates that sleep will occur soon.

Alternatively the scheduler may decide not to cancel end the current request and not to honor the new request if it recognizes the current request and new request are similar or identical. This will allow the RPM via the sleep set of the controller to turn off a CPU and then turn on the CPU so that the current request is processed when the CPU is turned on.

This discretionary function feature of a new request allows the scheduler to decide whether or not to cancel a current request or to issue a new request . This discretionary function and the ability of the scheduler to anticipate sleep states by reviewing the busy state tracked by the CBM resource increases efficiency and conserves power for the system . Power may be conserved since the scheduler does not have to perform as much work compared to a system that does not allow the scheduler to have discretion with respect to the requests it receives from clients .

Work is reduced by the scheduler using the CBM resource and the discretionary functions in new requests because the scheduler does not need to adjust any parameters to the sleep sets and active sets of resource sets managed by the RPM and the controller See . Adjustments are not needed to any parameters of the sleep sets and active sets because the scheduler is able to determine that successive active states of the CPU are similar to one another and do not require any changes to the active or sleep sets.

Without adjusting active or sleep sets of resource sets managed by the RPM and controller efficiencies on the order of 15 have been realized with the inventive system i.e. time savings on the order of 7 ms or more out of 45 ms have been realized compared to systems which do not allow the scheduler to have discretion with respect to honoring canceling current and new requests .

As noted previously the scheduler is able to increase the efficiency of the system with the CBM resource . The CBM resource is a binary indicator of whether or not something is happening in the system that allows the scheduler to infer that a sleep state will not occur relatively soon.

When the CBM resource indicates that most or all system elements are not busy usually meaning that extensive amounts of work have not been scheduled for resources and that those resources are about to be completely free of requests then the scheduler may anticipate that a sleep state will occur relatively soon. By looking at the states tracked by the CBM resource the scheduler may make decisions on whether or not resources should be powered off depending upon whether the scheduler anticipates whether a sleep state will occur.

For example if the scheduler notices that all resources are currently registering a non busy state then the scheduler may anticipate that a sleep state will occur and the scheduler may allow the sleep state via the RPM and controller with their resource sets to power off one or more resources so that the scheduler does not need to perform these tasks and hence saves work.

In another example if the scheduler notices that a first resource A is registering a non busy state i.e. that the first resource A is inactive while a second resource B is registering a busy state and if the scheduler determines that the first resource A is not scheduled for any upcoming requests then the scheduler may power down the non busy first resource A in order to conserve power. This action of powering down the non busy first resource A may conserve overall power since a sleep state is not anticipated because the second resource B is registering a busy state. Power may be wasted if the non busy first resource A were allowed to remain in an On but in inactive state.

The scheduler manages one or more requests for one or more resources at the beginning of stage of a busy or active state A of a CPU . Prior to entering the sleep state at stage the scheduler after reviewing the database recognizes that at the next scheduled busy or active state C scheduled for stage the current state of the resource which is currently active will likely not change.

Therefore prior to stage the scheduler avoids additional work and conserves energy by not going through a process of shutting down or shutting off the resource and updating respective sleep and active sets of resource sets . So at stage the scheduler allows all resources to enter into a sleep state and allows the sleep set of each resource to power down a respective resource . Upon entering stage the scheduler is expecting the system to wake up at stage .

In other words the scheduler is not expecting the wake up B at stage . However at stage the scheduler does receive an unexpected interrupt causing a wake up and unexpected active state B at stage . The scheduler may determine that active state B is unexpected by comparing timing data that is present in the database . Specifically if the scheduler notices that the current active state B has occurred much earlier in time than the scheduled wake up time at stage then the scheduler will recognize that this second active state B is unexpected or unanticipated by the system.

At this stage the scheduler determines the resource scheduled to be active when the system exits the previously scheduled sleep state which started at stage is not needed for the current interrupt or wakeup event B. Accordingly the scheduler may issue a cancel request to the resource so that the resource is turned off during this unanticipated second active state B in which the resource is not being used.

Next at the end of the second unexpected active state B at stage the scheduler may communicate with the sleep LPR in order to schedule a new request to turn on the resource such that the request becomes part of the next active awake set at the third active state C which was previously anticipated scheduled by the scheduler . In this way the scheduler has the intelligence to defer or reschedule requests that may not be needed for interrupt states such as the unexpected second active state B of .

For example assume the resource in is an RF modem. The scheduler in its discretion has decided that it will allow the sleep set of a resource to turn off the RF modem so that the RF modem will immediately become active for the next anticipated active state C scheduled for stage .

However the scheduler learns at stage that the interrupt causing the unexpected wake up at this stage is associated with an application program that does not need the RF modem. The scheduler may then issue a cancel request for the RF modem so that it does not perform its scheduled work corresponding to its previous request during this unanticipated active state B at stage . The scheduler using the sleep LPR will then schedule a new request associated with the RF modem for the next awake set NAS in the expected and previously scheduled active state C at stage .

In view of the disclosure above one of ordinary skill in the art is able to write computer code or identify appropriate hardware and or other logic or circuitry to implement the distributed resource management system and method without difficulty based on the flowcharts and associated description in this specification for example. Therefore disclosure of a particular set of program code instructions or detailed hardware devices is not considered necessary for an adequate understanding of how to make and use the distributed resource management system and method. The inventive functionality of the claimed computer implemented processes is explained in more detail in the above description and in conjunction with the drawing figures which may illustrate various process flows. Further the processors etc. in combination with the memory and the instructions stored therein may serve as a means for performing one or more of the method steps described herein.

In one or more exemplary aspects the functions described may be implemented in hardware software firmware or any combination thereof. If implemented in software the functions may be stored on or transmitted as one or more instructions or code on a computer readable medium. Computer readable media include both computer storage media and communication media including any medium that facilitates transfer of a computer program from one place to another. A storage medium may be any available medium that may be accessed by a computer. By way of example and not limitation such computer readable media may comprise RAM ROM EEPROM CD ROM or other optical disk storage magnetic disk storage or other optical or magnetic storage devices or any other medium that may be used to carry or store desired program code in the form of instructions or data structures and that may be accessed by a computer. The term disk or disc as used herein includes but is not limited to compact disc CD laser disc optical disc digital versatile disc DVD floppy disk and Blu ray disc. Combinations of the above should also be included within the scope of computer readable media.

Although selected aspects have been illustrated and described in detail it will be understood that various substitutions and alterations may be made therein without departing from the spirit and scope of the present disclosure as defined by the following claims.

