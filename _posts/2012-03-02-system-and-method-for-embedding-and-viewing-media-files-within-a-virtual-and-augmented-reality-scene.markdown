---

title: System and method for embedding and viewing media files within a virtual and augmented reality scene
abstract: A preferred method for viewing embedded media in a virtual and augmented reality (VAR) scene can include at a viewer device, defining a real orientation of the viewer device relative to a projection matrix; and orienting a VAR scene on the viewer device in response to the real orientation in block, in which the VAR scene includes one or both of visual data and orientation data. The preferred method can further include selecting a media file in the VAR scene, wherein the media file is selected at a media location correlated at least to the real orientation of the viewer device; and activating the media file in the VAR scene at the media location. The preferred method and variations thereof functions to allow a viewer to interact with media that is embedded, tagged, linked, and/or associated with a VAR scene viewable on the viewer device.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09118970&OS=09118970&RS=09118970
owner: Aria Glassworks, Inc.
number: 09118970
owner_city: San Francisco
owner_country: US
publication_date: 20120302
---
The present application claims priority to U.S. Provisional Patent Application Ser. No. 61 448 327 filed on 2 Mar. 2011 and entitled Method for Linking Media in Virtual and Augmented Reality Scenes the entirety of which is incorporated herein by this reference and U.S. Provisional Patent Application Ser. No. 61 451 071 filed on 9 Mar. 2011 and entitled Method for Linking Media in Virtual and Augmented Reality Scenes the entirety of which is incorporated herein by this reference

This invention relates generally to the virtual and augmented reality field and more specifically to a new and useful system and method for distributing virtual and augmented reality scenes through a social network.

The capability to view augmented reality on mobile devices has been increasing in recent years. The data viewable with these augmented reality devices have largely been controlled by large entities and as there use has largely been adjusted specifically for the purposes of that entity. The public has not had technology readily available to them to simply create such virtual and augmented reality scenes and to embed and or interact with the virtual and augmented reality scenes through other media. Thus there is a need in the virtual and augmented reality field for new and useful systems and methods for embedding and viewing media within virtual and augmented reality scenes. This invention provides such a new and useful system and or method the details of which are described below in its preferred embodiments with reference to the following drawings.

The following description of the preferred embodiments of the invention is not intended to limit the invention to these preferred embodiments but rather to enable any person skilled in the art to make and use this invention.

As shown in a system of a preferred embodiment can include a device for embedding and or viewing media files in virtual and augmented reality VAR scenes. As used herein the user device and the viewer device are defined in terms of the function being performed by the respective user viewer and each type of device is interchangeable with the other as described herein depending upon the use the device is being put to by the user viewer. The preferred user device can be used by a user to capture process create and or generate a viewable scene such as for example a VAR scene and to generate place position embed and or actuate one or more media files inside the VAR scene. The preferred viewer device can be used by a viewer to receive process orient render generate and or view a viewable scene such as for example a VAR scene and to render view activate and or interact with one or more media files inside the VAR scene.

Preferably the user device and the viewer device are substantially similar. One or both of the user device and the viewer device can include one or more cameras front rear an accelerometer a gyroscope a MEMS gyroscope a magnetometer a pedometer a proximity sensor an infrared sensor an ultrasound sensor a global position satellite transceiver WiFi transceiver mobile telephone components and or any suitable combination thereof for calculating a projection matrix and or the associated Euler angles. In the user device and or the viewer device orientation and or position information can be gathered in any suitable fashion including device Application Programming Interfaces API or through any suitable API exposing device information e.g. using Flash or HTML5 to expose device information including orientation location.

As shown in in one the device of the preferred embodiment the VAR scene can include a spherical image . Preferably the portion of the spherical image i.e. the VAR scene that is displayable by the device corresponds to an overlap between a viewing frustum of the device i.e. a viewing cone projected from the device and the imaginary sphere that includes the spherical image . The scene is preferably a portion of the spherical image which can include a substantially rectangular display of a concave convex or hyperbolic rectangular portion of the sphere of the spherical image . Preferably the nodal point is disposed at approximately the origin of the spherical image such that a viewer has the illusion of being located at the center of a larger sphere or bubble having the VAR scene displayed on its interior. Alternatively the nodal point can be disposed at any other suitable vantage point within the spherical image displayable by the device . In another alternative the displayable scene can include a substantially planar and or ribbon like geometry from which the nodal point is distanced in a constant or variable fashion. Preferably the display of the scene can be performed within a 3D or 2D graphics platform such as OpenGL WebGL or Direct 3D. Alternatively the display of the scene can be performed within a browser environment using one or more of Flash HTML5 CSS3 or any other suitable markup language. In another variation of the device of the preferred embodiment the geometry of the displayable scene can be altered and or varied in response to an automated input and or in response to a user input.

As shown in the user and or viewer mobile device of the preferred embodiment can include a display an orientation module including a real orientation module and a user orientation module a location module a camera oriented in substantially the same direction as the display and a processor connected to each of the display orientation module location module and camera . The device of the preferred embodiment preferably functions to capture and or present a VAR scene to a user from the point of view of a nodal point or center thereof such that it appears to the user that he or she is viewing the world represented by the VAR scene through a frame of a window. The device of the preferred embodiment can include any suitable type of mobile computing apparatus such as a smart phone a personal computer a laptop computer a tablet computer a television monitor paired with a separate handheld orientation location apparatus or any suitable combination thereof.

As shown in the orientation module of the device of the preferred embodiment includes at least a real orientation portion and a user orientation portion. The real orientation portion of the orientation module preferably functions to provide a frame of reference for the device as it relates to a world around it wherein the world around can include real three dimensional space a virtual reality space an augmented reality space or any suitable combination thereof. As noted below the projection matrix can preferably include a mathematical representation of an arbitrary orientation of a three dimensional object i.e. the device having three degrees of freedom relative to a second frame of reference. As noted in the examples below the projection matrix can include a mathematical representation of the device orientations in terms of its Euler angles pitch roll yaw in any suitable coordinate system.

In one variation of the device of the preferred embodiment the second frame of reference can include a three dimensional external frame of reference i.e. real space in which the gravitational force defines baseline directionality for the relevant coordinate system against which the absolute orientation of the device can be measured. In such an example implementation the device will have certain orientations corresponding to real world orientations such as up and down and further such that the device can be rolled pitched and or yawed within the external frame of reference. Preferably the orientation module can include a MEMS gyroscope configured to calculate and or determine a projection matrix indicative of the orientation of the device . In one example configuration the MEMS gyroscope can be integral with the orientation module . Alternatively the MEMS gyroscope can be integrated into any other suitable portion of the device or maintained as a discrete module of its own.

As shown in the user orientation portion of the orientation module preferably functions to provide a frame of reference for the device relative to a point or object in space including a point or object in real space. Preferably the user orientation can include a measurement of a distance and or rotational value s of the device relative to a nodal point. In another variation of the device of the preferred embodiment the nodal point can include a user s head such that the user orientation includes a measurement of the relative distance and or rotational value s of the device relative to a user s field of view. Alternatively the nodal point can include a portion of the user s head such as for example a point between the user s eyes. In another alternative the nodal point can include any other suitable point in space including for example any arbitrary point such as an inanimate object a group of users a landmark a location a waypoint a predetermined coordinate and the like. Preferably as shown in the user orientation portion of the orientation module can function to create a viewing relationship between a viewer optionally located at the nodal point and the device such that a change in user orientation can cause a consummate change in viewable content consistent with the user s VAR interaction i.e. such that the user s view through the frame will be adjusted consistent with the user s orientation relative to the frame.

As shown in one variation of the device of the preferred embodiment includes a location module connected to the processor and the orientation module . The location module of the preferred embodiment functions to determine a location of the device . As noted above location can refer to a geographic location which can be indoors outdoors above ground below ground in the air or on board an aircraft or other vehicle. Preferably as shown in the device of the preferred embodiment can be connectable either through wired or wireless means to one or more of a satellite positioning system a local area network or wide area network such as a WiFi network and or a cellular communication network . A suitable satellite position system can include for example the Global Positioning System GPS constellation of satellites Galileo GLONASS or any other suitable territorial or national satellite positioning system. In one alternative embodiment the location module of the preferred embodiment can include a GPS transceiver although any other type of transceiver for satellite based location services can be employed in lieu of or in addition to a GPS transceiver.

The processor of the device of the preferred embodiment functions to manage the presentation of the VAR scene to the viewer . In particular the processor preferably functions to display a scene to the viewer on the display in response to the real orientation and the user orientation. The processor of the preferred embodiment can be configured to process compute calculate determine and or create a VAR scene that can be displayed on the device to a viewer wherein the VAR scene is oriented to mimic the effect of the viewer viewing the VAR scene as if through the frame of the device . Preferably orienting the scene can include preparing a VAR scene for display such that the viewable scene matches what the user would view in a real three dimensional view that is such that the displayable scene provides a simulation of real viewable space to the viewer as if the device were a transparent frame. As noted above the scene is preferably a VAR scene therefore it can include one or more virtual and or augmented reality elements composing in addition to and or in lieu of one or more real elements buildings roads landmarks and the like either real or fictitious . Alternatively the scene can include processed or unprocessed images videos multimedia files of one or more displayable scene aspects including both actual and fictitious elements as noted above. Preferably the device can be configured in any suitable combination of hardware firmware and or software for both embedding and activating media links within the displayable VAR scenes according to any of the preferred methods and variations thereof.

As shown in a method according to a first preferred embodiment can include at a user device defining a real orientation of the user device relative to a projection matrix in block S orienting a VAR scene on the user device in response to the real orientation in block S. Preferably the VAR scene includes at least visual data and orientation data. The first preferred method can further include selecting a media location in the VAR scene in the VAR scene in block S and embedding a media file in the VAR scene at the media location in block S. Preferably the media location in correlated at least to the real orientation of the user device. The first preferred method functions to allow a user to embed tag link and or associate a secondary media file with a VAR scene with which the user is interacting. Preferably the first preferred method can be performed on a user device of the type described above with reference to .

As shown in the first preferred method can include block S which recites defining a real orientation of the user device relative to a projection matrix. Block S preferably functions to determine a physical orientation of the user device relative to an external frame of reference with which a secondary or additional media element is to be associated. Preferably the physical orientation of the user device can include the real orientation of the user device as calibrated relative to an external three dimensional frame of reference. A suitable external frame of reference can include a natural frame of reference in which a vertical axis is defined substantially collinearly with a gravitational pull on the user device. Alternatively the user device can be configured to set and or establish any suitable frame of reference and or rotation or permutation thereof in determining the real orientation of the user device. Alternatively block S can further include determining a location of the user device wherein the location can include a geographic location which can be indoors outdoors above ground below ground in the air or on board an aircraft or other vehicle. In other variations of the first preferred method the real orientation and or the location can include a fictional frame of reference and or location such as within a VAR scene related to a film or novel.

One variation of the first preferred method can further include defining a user orientation of the user device relative to a nodal point. As noted above the user orientation can include a measurement of a distance and or rotational value s of the device relative to a nodal point. Preferably the nodal point can include a user s head such that the user orientation includes a measurement of the relative distance and or rotational value s of the device relative to a user s field of view. Alternatively the nodal point can include a portion of the user s head such as for example a point between the user s eyes. In another alternative the nodal point can include any other suitable point in space including for example any arbitrary point such as an inanimate object a group of users a landmark a location a waypoint a predetermined coordinate and the like. Preferably determining the user orientation functions to create a viewing relationship between a user and or viewer optionally located at the nodal point and the device such that a change in user orientation can cause a consummate change in viewable content consistent with the user s VAR interaction i.e. such that the user s view through the frame will be adjusted consistent with the user s orientation relative to the frame.

As shown in the first preferred method can further include block S which recites orienting a VAR scene on the user device in response to the real orientation. Block S preferably functions to render configure calculate compute and or display the VAR scene to the user in response to at least the real orientation of the user device. Preferably the VAR scene includes both visual data and orientation data such that the portion of the scene that is displayable to the user is a function of the real orientation of the user device. Preferably the portion of the VAR scene that is displayable by the user device corresponds to an overlap between a viewing frustum of the device i.e. a viewing cone projected from the device and the imaginary sphere that includes the spherical image as shown in . As noted above the VAR scene is preferably a portion of the spherical image which can include a substantially rectangular display of a concave convex or hyperbolic rectangular portion of the sphere of the spherical image. Preferably the nodal point can be disposed at approximately the origin of the spherical image such that a user viewer has the illusion of being located at the center of a larger sphere or bubble having the VAR scene displayed on its interior. Alternatively the nodal point can be disposed at any other suitable vantage point within the spherical image displayable by the device. In another alternative the displayable scene can include a substantially planar and or ribbon like geometry from which the nodal point is distanced in a constant or variable fashion. Preferably block S can be performed within a 3D or 2D graphics platform such as OpenGL WebGL or Direct 3D. Alternatively the orientation and or display of the VAR scene can be performed within a browser environment using one or more of Flash HTML5 CSS3 or any other suitable markup language. In another variation of the first preferred method the geometry of the displayable scene can be altered and or varied in response to an automated input and or in response to a user input.

As shown in the preferred method can further include block S which recites selecting a media location in the VAR scene and wherein the media location is correlated at least to the real orientation of the user device. Block S preferably functions to establish in the VAR scene a particular location correlated to a particular orientation of the user viewer device at which the media file can be disposed for embedding and subsequent viewing and or interaction by a viewer. Preferably block S can be performed at the user device in conjunction with a user manipulating the user device to change the portion of the VAR scene that is displayed on the user device as described above. Upon orienting the VAR scene at the desired location the user device can respond to one or more user inputs to locate and or register the media location as correlated to any combination of the real orientation the user orientation and or the user device location. Suitable user inputs can include for example a touch keystroke input gesture voice command and or any suitable combination thereof. In one variation of the first preferred method the media location can include a range in display space correlated to a pixel area of the display which in turn correlates to a real orientation range of the projection matrix of the user device which can be matrix analogs to ranges of associated pitch roll and yaw values for the user device.

In another variation of the first preferred method the media location can be further correlated to a depth value within the VAR scene. Preferably depth is communicated through either varying scale of the link data or by varying the Z depth of the link e.g. when rendered on top of and after the rest of the VAR scene . An alternative approach enabling out of order drawing includes rendering the VAR scene close i.e. inside a photosphere and scaling according to the perspective transform along the plane formed by the X axis and Y axis. Any suitable technique for generating a depth value can be used including a user input such as a touch keystroke input gesture voice command or other manipulation of the user device. Alternatively the depth value can be determined solely in response to one or more of the user device real orientation user orientation and or the location of the VAR scene.

In another variation of the preferred method block S can further include assigning a location to the VAR scene which can include for example receiving an input from an authoring tool in assigning a location of the VAR scene. Authoring can include indicating location during authoring the VAR scene or while viewing a previously created VAR scene. For example while creating a VAR scene on a mobile device the user can initiate the assignment of a location by orientating the device in the desired location and then recording an audio clip. The audio clip will now be assigned that location within the VAR scene. Similarly links text multimedia and other forms of media can be authored in any suitable manner. In another variation of the preferred method a location module of the type described above can automatically generate the location. For example if an additional image is to be added to the VAR scene and the image was taken from substantially similar geographic location include then image processing can be employed to align the image with a corresponding view from the VAR scene. Similarly if the media has location information enabling relative positioning of the VAR scene and the media then the location can be determined through the relative positioning. For example if a second VAR scene is directly north of a first VAR scene then the second VAR scene can be assigned a location in the northern portion of the first VAR scene. In another example implementation selecting the media location of the second VAR scene can further include determining a location of the second VAR scene and locating the second VAR scene at its location within the VAR scene. For example if the VAR scene is a Golden Gate Bridge VAR scene and the second VAR scene is a VAR scene at the North tower of the Golden Gate Bridge then the media location of the second VAR scene can be automatically or manually located at the North tower in the VAR scene. Accordingly viewer selection of the North tower can link the viewer directly to the second VAR scene.

As shown in the preferred method can further include block S which recites embedding a media file in the VAR scene at the media location. Preferably the media file can include any one of an image file a video file an audio file a second VAR scene a second rendering of the VAR scene a hyperlink or any other suitable type of ancillary media the content of which was not originally captured in the VAR scene. Alternatively a hyperlink can be configured to direct the viewer device to the media file. In another alternative the media file can include an embedded media fragment that optionally can include a fully qualified file format. For example a viewer can click on or otherwise select the hyperlink which directs the viewer device to the media file which can include any one of an image file a video file an audio file a second VAR scene a second rendering of the VAR scene and or a second hyperlink to still more media file s. The media file can be embedded into the VAR scene using any suitable process algorithm software suite programming language or application programming interface API including for example any suitable combination of OpenGL WebGL Direct 3D Flash HTML5 and or CSS3. Additionally or alternatively block S can include embedding the media file in the VAR scene in more than one type of format such that the media file can be accessed when the viewer device is operating in different locations or under different conditions e.g. with or without access to WiFi or high speed cellular network communications . Additionally or alternatively the media can be rendered directly into the VAR scene such as an image positioned in a VAR scene. Flash or HTML5 can be used to render the media links. For example uniform resource identifier URI links can be represented in a VAR scene with Flash HTML or any other markup language and Flash or HTML5 transforms or other suitable methods can be used to render the media links.

As shown in the preferred method can further include block S which recites associating a media activation parameter with the media file. Block S preferably functions to correlate generate calculate monitor maintain and or associate a media activation parameter with the media file and or the associated media location. Preferably the media activation parameter can include any suitable user interface through which the user viewer can activate the media file. For example the media activation parameter can include a receivable user input such as a touch keystroke input gesture voice command or other manipulation of the user device. In the example embodiment shown in the media activation parameter can include an activation area the location of which defined by the orientation of the device relative to the VAR scene. Preferably the activation area is displayable within the VAR scene such that through manipulation of the orientation of the VAR scene the activation area can be substantially overlapping with the media file and thus activate the media file. As shown in reorientation of the device such that activation area surrounds or otherwise at least partially overlaps the media file can result in activation of the media file in this case an audio file that recites Here are the Rocky Mountains. 

Preferably the activation area is disposed substantially in and around the center of the display i.e. the activation area is a fixed range of pixels displayable in or around the center of the display. The size of the activation area can be related to the size of the display of the device on which the VAR scene is being rendered whether a user device or a viewer device. Alternatively the size of the activation area can be related to the number of and or spatial density of media files. For example if a VAR scene has a large number of selectable media files that are densely located then the activation area can be relatively small to permit precise selection of the desired media file. Alternatively if the VAR scene has relatively few or sparsely located media files then the activation area can be relatively large thus permitting a viewer greater latitude in selecting the desired media file. Accordingly as the VAR scene is traversed by the user viewer the portions of the VAR scene disposed within the activation area will change accordingly. As the media file which is embedded at the media location selected in block S is displayed within the activation area the media file will be activated. Activation of the media file can include any suitable action depending upon the type of media file that is being activated such as playing the media file if it is an audio or video file rendering an image file following a hyperlink and or rendering and displaying a second VAR scene. The activation area can be used alone or in combination with other means or mechanisms for activating the media file including at least touch keypad and voice enabled interaction with the VAR scene.

As shown in a second preferred method can include at a viewer device defining a real orientation of the viewer device relative to a projection matrix in block and orienting a VAR scene on the viewer device in response to the real orientation in block S. Preferably the VAR scene includes one or both of visual data and orientation data. The second preferred method can further include selecting a media file in the VAR scene wherein the media file is selected at a media location correlated at least to the real orientation of the viewer device in block S and activating the media file in the VAR scene at the media location in block S. The second preferred method functions to allow a viewer to interact with media that is embedded tagged linked and or associated with a VAR scene viewable on the viewer device. Preferably the second preferred method can be performed on a viewer device of the type described above with reference to .

As shown in the second preferred method can include block S which recites defining a real orientation of the viewer device relative to a projection matrix. Block S preferably functions to determine a physical orientation of the viewer device relative to an external frame of reference with which a secondary or additional media element is to be associated. Preferably the physical orientation of the viewer device can include the real orientation of the viewer device as calibrated relative to an external three dimensional frame of reference. As noted above a suitable external frame of reference can include a natural frame of reference in which a vertical axis is defined substantially collinearly with a gravitational pull on the viewer device. Alternatively the viewer device can be configured to set and or establish any suitable frame of reference and or rotation or permutation thereof in determining the real orientation of the viewer device. Alternatively block S can further include determining a location of the viewer device wherein the location can include a geographic location which can be indoors outdoors above ground below ground in the air or on board an aircraft or other vehicle. In other variations of the second preferred method the real orientation and or the location can include a fictional frame of reference and or location such as within a VAR scene related to a film or novel.

One variation of the second preferred method can further include defining a user orientation of the viewer device relative to a nodal point. As noted above the user orientation can include a measurement of a distance and or rotational value s of the device relative to a nodal point. Preferably the nodal point can include a viewer s head such that the user orientation includes a measurement of the relative distance and or rotational value s of the device relative to a viewer s field of view. Alternatively the nodal point can include a portion of the viewer s head such as for example a point between the viewer s eyes. In another alternative the nodal point can include any other suitable point in space including for example any arbitrary point such as an inanimate object a group of users a landmark a location a waypoint a predetermined coordinate and the like. Preferably determining the user orientation functions to create a viewing relationship between a viewer optionally located at the nodal point and the device such that a change in user orientation can cause a consummate change in viewable content consistent with the user s VAR interaction i.e. such that the viewer s view through the frame will be adjusted consistent with the viewer s orientation relative to the frame.

As shown in the second preferred method can further include block S which recites orienting a VAR scene on the viewer device in response to the real orientation. Block S preferably functions to render configure calculate compute and or display the VAR scene to the viewer in response to at least the real orientation of the viewer device. As noted above the VAR scene preferably includes both visual data and orientation data such that the portion of the scene that is displayable to the viewer is a function of the real orientation of the viewer device. Preferably the portion of the VAR scene that is displayable by the viewer device corresponds to an overlap between a viewing frustum of the device i.e. a viewing cone projected from the device and the imaginary sphere that includes the spherical image as shown in FIG. . As noted above the VAR scene is preferably a portion of the spherical image which can include a substantially rectangular display of a concave convex or hyperbolic rectangular portion of the sphere of the spherical image. Preferably the nodal point can be disposed at approximately the origin of the spherical image such that a user viewer has the illusion of being located at the center of a larger sphere or bubble having the VAR scene displayed on its interior. Alternatively the nodal point can be disposed at any other suitable vantage point within the spherical image displayable by the device. In another alternative the displayable scene can include a substantially planar and or ribbon like geometry from which the nodal point is distanced in a constant or variable fashion. Preferably block S can be performed within a 3D or 2D graphics platform such as OpenGL WebGL or Direct 3D. Alternatively the orientation and or display of the VAR scene can be performed within a browser environment using one or more of Flash HTML5 CSS3 or any other suitable markup language. In another variation of the second preferred method the geometry of the displayable scene can be altered and or varied in response to an automated input and or in response to a viewer input.

As shown in the second preferred method can further include block S which recites selecting a media file in the VAR scene wherein the media file is selected at a media location correlated at least to the real orientation of the user device. Block S preferably functions to allow a viewer to interact with choose and or select a media file in the VAR scene at particular location correlated to a particular orientation of the viewer device. Preferably block S can be performed at the viewer device in conjunction with a viewer manipulating the viewer device to change the portion of the VAR scene that is displayed on the viewer device as described above. Upon orienting the VAR scene at the desired location the viewer device can respond to one or more viewer inputs to choose and or select the media location as correlated to any combination of the real orientation the user orientation and or the viewer device location. Suitable viewer inputs can include for example a touch keystroke input gesture voice command and or any suitable combination thereof. In one variation of the second preferred method the media location can include a range in display space correlated to a pixel area of the display which in turn correlates to a real orientation range of the projection matrix of the viewer device which can be matrix analogs to ranges of associated pitch roll and yaw values for the viewer device. Accordingly in response to the viewer orienting the viewer device in a particular orientation the appropriate media file can be selected.

In another variation of the second preferred method the media location can be further correlated to a depth value within the VAR scene. As noted above preferably depth is communicated through either varying scale of the link data or by varying the Z depth of the link e.g. when rendered on top of and after the rest of the VAR scene . A previously noted alternative approach enabling out of order drawing includes rendering the VAR scene close i.e. inside a photosphere and scaling according to the perspective transform along the plane formed by the X axis and Y axis. Any suitable technique for selecting a depth value can be used including a user input such as a touch keystroke input gesture voice command or other manipulation of the viewer device. Alternatively the depth value can be determined solely in response to one or more of the viewer device real orientation user orientation and or the location of the VAR scene.

As shown in the second preferred method can further include block S which recites activating the media file in the VAR scene at the media location. Preferably the media file can include any one of an image file a video file an audio file a second VAR scene a second rendering of the VAR scene a hyperlink or any other suitable type of ancillary media the content of which was not originally captured in the VAR scene. Alternatively a hyperlink can be configured to direct the viewer device to the media file. Alternatively a hyperlink can be configured to direct the viewer device to the media file. In another alternative the media file can include an embedded media fragment that optionally can include a fully qualified file format. For example a viewer can click on or otherwise select the hyperlink which directs the viewer device to the media file which can include any one of an image file a video file an audio file a second VAR scene a second rendering of the VAR scene and or a second hyperlink to still more media file s. As noted above the media file can be embedded into the VAR scene using any suitable process algorithm software suite programming language or application programming interface API including for example any suitable combination of OpenGL WebGL Direct 3D Flash HTML5 and or CSS3. Preferably activating the media file can include at least causing the media file to transmit communicate its desired content such as for example playing an audio or video file displaying an image retrieving a webpage rendering and displaying a second VAR scene and or following a hyperlink. Additionally or alternatively block S can include activating the media file in the VAR scene in more than one type of format such that the media file can be accessed when the viewer device is operating in different locations or under different conditions e.g. with or without access to WiFi or high speed cellular network communications .

In another variation of the second preferred method the media location of the second VAR scene can include a representation of the second VAR scene at its real location within the VAR scene. As noted above if a second VAR scene is directly north of a first VAR scene then the second VAR scene can be assigned a location in the northern portion of the first VAR scene. In another example implementation described above selecting the media location of the second VAR scene can further include determining a location of the second VAR scene and locating the second VAR scene at its location within the VAR scene. For example if the VAR scene is a Golden Gate Bridge VAR scene and the second VAR scene is a VAR scene at the North tower of the Golden Gate Bridge then the media location of the second VAR scene can be automatically or manually located at the North tower in the VAR scene. Accordingly viewer selection of the North tower can link the viewer directly to the second VAR scene.

As shown in and activation of example types of media files can result in the display and or transmission of different types of media within the displayed VAR scene. for example illustrates another variation of the second preferred method in which activation of the media file can include activating the media file in response to an orientation of the viewer device such that an activation area in the VAR scene overlaps the media file. As noted above suitable media activation parameters can include any suitable user interface through which the user viewer can activate the media file. For example the media activation parameter can include a receivable user input such as a touch keystroke input gesture voice command or other manipulation of the user device. In the example embodiment shown in the media activation parameter can include an activation area displayable within the VAR scene such that through manipulation of the orientation of the VAR scene the activation area can be substantially overlapping with the media file and thus activate the media file. As shown in reorientation of the device such that activation area surrounds or otherwise at least partially overlaps the media file can result in activation of the media file in this case an audio file that recites Here are the Rocky Mountains. Similarly activation of the example media file shown in can result in the display of a nighttime image of the portion of the Rocky Mountains with which the media file is associated by location and or user orientation. by contrast illustrates an example webpage media file in which activation of the hyperlink causes retrieval and display of a webpage relating to the portion of the Rocky Mountains with which the media file hyperlink is associated by location and or user orientation.

For audio either a recording or audio channel from a video binaural audio can be incorporated into the playback to simulate the audio originating from the location within the VAR scene. Thus as the viewer changes the orientation of the viewer device to view different portions of the VAR scene the audio can continue but will use internal time and level differences or other binaural techniques to create the perception of the sound originating from a stationary point. For example when looking north an audio file can be played sounding as if it is originating from a northerly source. When the viewer turns south to view other portions of the VAR scene the sound can be transmitted as if it is still coming from a northerly direction behind the viewer. Similarly the volume of playback can change based on orientation such that the volume is greater when the audio link is in the center of the display and quieter when not viewed. The audio can alternatively be prepared such that a surround sound experience is created in which case the mixing of the audio alters depending on the orientation of the user viewer. For video images or other visual media taken from substantially the same location image processing can be used to align the image with the VAR scene imagery as described above.

In other variations of the preferred methods the media file is preferably graphically represented via either text or graphics. For example a sound media file can have a sound graphic positioned at the location of the sound file in the VAR scene. The media file can alternatively be generally hidden and selectively viewable in the VAR scene in response to the real orientation of the viewer device. The visibility of the media file can additionally be toggled in response to a real orientation or viewing mode of the viewer device. In other variations of the preferred methods a predetermined real orientation user orientation and or location of the device can activate interactive controls or a hover state of the media file. For example when a second VAR scene link is substantially in the center of the display a text description image preview or other information relating to the second VAR scene can be displayed. Additionally or alternatively activation or selection of the media file can cause the display of one or more media controls such as play or volume to permit further interaction with the media file.

In another variation of the preferred methods activating a second VAR scene causes the display of transition between the two VAR scenes. Preferably the transition is a video of images showing the transition between the locations associated with the two VAR scenes. As an alternative the displayed image of the current VAR scene can transition to an image of the second VAR scene in the same orientation. As another alternative a graphical animation of moving between the location of the first VAR scene and the second VAR scene can be displayed on a map. The second VAR scene preferably has a reciprocating VAR scene link to allow a transition back to the original VAR scene. Preferably a network of links between VAR scenes can be provided allowing exploration of spatial scenes while the user viewer device remains in a single location. In other variations of the preferred methods the links for the respective VAR scenes can be user generated or automatically created based on which VAR scenes are popular close by or interrelated in any other suitable way.

Additional variations of the preferred methods can additionally include automatically embedding a VAR scene which functions to automatically populate a VAR scene with suitable links. A database or map of VAR scenes is preferably used to identify nearby associated and or popular VAR scenes to which to link. The link to an identified VAR scene is preferably added and the location is preferably based on the relative geographic location of the two VAR scenes as described above. This automatic embedding of links preferably creates a navigable space from and between discrete VAR scenes. Additionally other content can be automatically included. For example based on the location of the VAR scene links to other media files can be embedded. For example if a VAR scene is near a restaurant a link to a media file containing a review of that restaurant can be embedded and preferably positioned in the direction of the restaurant. Links from other nearby VAR scenes that were manually added can additionally be automatically embedded into a VAR scene. For example if a nearby VAR scene has a link to media file containing an informative webpage a VAR scene can embed that same informative webpage as its own media file. Outside content from other sources such as a social network with geo tagged content can additionally be used to auto populate a VAR scene with links to one or more media files.

The user and viewer devices and methods of the preferred embodiment can be embodied and or implemented at least in part as a machine configured to receive a computer readable medium storing computer readable instructions. The instructions are preferably executed by computer executable components preferably integrated with the user viewer device and one or more portions of the processor orientation module and or location module . Other systems and methods of the preferred embodiment can be embodied and or implemented at least in part as a machine configured to receive a computer readable medium storing computer readable instructions. The instructions are preferably executed by computer executable components preferably integrated by computer executable components preferably integrated with a user viewer device of the type described above. The computer readable medium can be stored on any suitable computer readable media such as RAMs ROMs flash memory EEPROMs optical devices CD or DVD hard drives floppy drives or any suitable device. The computer executable component is preferably a processor but any suitable dedicated hardware device can alternatively or additionally execute the instructions.

As a person skilled in the art will recognize from the previous detailed description and from the figures and claims modifications and changes can be made to the preferred embodiments of the invention without departing from the scope of this invention defined in the following claims.

