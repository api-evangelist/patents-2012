---

title: Memory-access-resource management
abstract: The present application is directed to a memory-access-multiplexing memory controller that can multiplex memory accesses from multiple hardware threads, cores, and processors according to externally specified policies or parameters, including policies or parameters set by management layers within a virtualized computer system. A memory-access-multiplexing memory controller provides, at the physical-hardware level, a basis for ensuring rational and policy-driven sharing of the memory-access resource among multiple hardware threads, cores, and/or processors.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09251103&OS=09251103&RS=09251103
owner: VMware, Inc.
number: 09251103
owner_city: Palo Alto
owner_country: US
publication_date: 20120808
---
The present patent application is directed to multi core processor architecture and virtual machine based computing and in particular to hardware level memory access control that is exposed to and managed by external entities including management layers within virtualized computing systems.

The development and evolution of modern computing has in many ways been facilitated by the power of logical abstraction. Early computers were manually programmed by slow and tedious input of machine instructions into the computers memories. Over time assembly language programs and assemblers were developed in order to provide a level of abstraction namely assembly language programs above the machine instruction hardware interface level to allow programmers to more rapidly and accurately develop programs. Assembly language based operations are more easily encoded by human programmers than machine instruction based operations and assemblers provided additional features including assembly directives routine calls and a logical framework for program development. The development of operating systems provided yet another type of abstraction that provided programmers with logical easy to understand system call interfaces to computer hardware functionality. As operating systems developed additional internal levels of abstraction were created within operating systems including virtual memory implemented by operating system paging of memory pages between electronic memory and mass storage devices which provided easy to use linear memory address spaces much larger than could be provided by the hardware memory of computer systems. Additional levels of abstractions were created in the programming language domain with compilers developed for a wide variety of compiled languages that greatly advanced the ease of programming and the number and capabilities of programming tools with respect those provided by assemblers and assembly languages. Higher level scripting languages and special purpose interpreted languages provided even higher levels of abstraction and greater ease of application development in particular areas. Similarly block based and sector based interfaces to mass storage devices have been abstracted through many levels of abstraction to modern database management systems which provide for high available and fault tolerant storage of structured data that can be analyzed interpreted and manipulated through powerful high level query languages.

In many ways a modern computer system can be thought of as many different levels of abstractions along many different often interdependent dimensions. More recently powerful new levels of abstraction have been developed with respect to virtual machines which provide virtual execution environments for application programs and operating systems. Virtual machine technology essentially abstracts the hardware resources and interfaces of a computer system on behalf of multiple virtual machines each comprising one or more application programs and an operating system. Even more recently the emergence of cloud computing services can provide abstract interfaces to enormous collections of geographically dispersed data centers allowing computational service providers to develop and deploy complex Internet based services that execute on tens or hundreds of physical servers through abstract cloud computing interfaces. Despite the many layers of abstraction present in virtualized computing systems physical hardware operation may nonetheless impact and constrain operations at higher levels of abstraction. Slow memory access due to conflicts between processors in multi processor systems for example may degrade performance and constrain operations of virtual machines in virtualized computer systems. Researchers and developers of computer hardware and virtualization technologies continue to develop methods systems and technologies that ameliorate physical hardware constraints and conflicts that propagate upward through virtualization layers to degrade performance of virtual machines.

The present application is directed to a memory access multiplexing memory controller that can multiplex memory accesses from multiple hardware threads cores and processors according to externally specified policies or parameters including policies or parameters set by management layers within a virtualized computer system. A memory access multiplexing memory controller provides at the physical hardware level a basis for ensuring rational and policy driven sharing of the memory access resource among multiple hardware threads cores and or processors.

The detailed discussion includes two subsections 1 An Overview of Virtualized Computer Systems and Multi Core Processors and 2 Memory Access Resource Management Systems and Methods. The initial overview section discusses virtual machines virtual data centers processor architecture hardware threads and multi core processors reading of which can be omitted by those familiar with these subjects.

As discussed above modern computing can be considered to be a collection of many different levels of abstraction above the computing hardware level that includes physical computer systems data storage systems and devices and communications networks. The term abstraction is not in any way intended to mean or suggest an abstract idea or concept. Computational abstractions are tangible physical interfaces that are implemented ultimately using physical computer hardware data storage devices and communications systems. Instead the term abstraction refers in the current discussion to a logical level of functionality encapsulated within one or more concrete tangible physically implemented computer systems with defined interfaces through which electronically encoded data is exchanged process execution launched and electronic services are provided. Interfaces may include graphical and textual data displayed on physical display devices as well as computer programs and routines that control physical computer processors to carry out various tasks and operations and that are invoked through electronically implemented application programming interfaces APIs and other electronically implemented interfaces. There is a tendency among those unfamiliar with modern technology and science to misinterpret the terms abstract and abstraction when used to describe certain aspects of modern computing. For example one frequently encounters allegations that because a computational system is described in terms of abstractions functional layers and interfaces that it is somehow different from a physical machine or device. Such allegations are unfounded. One only needs to disconnect a computer system or group of computer systems from their respective power supplies to appreciate the physical machine nature of complex computer technologies. One also frequently encounters statements made by those unfamiliar with modern technology and science that characterize a computational technology as being only software and thus not a machine or device. Software is essentially a sequence of encoded symbols such as a printout of a computer program or digitally encoded computer instructions sequentially stored in a file on an optical disk within an electromechanical mass storage device in an electronic memory or in a solid state disk. Software alone can do nothing. It is only when encoded computer instructions are loaded into an electronic memory within a computer system and executed on a physical processor that so called software implemented functionality is provided. The digitally encoded computer instructions are an essential control component of processor controlled machines and devices no less essential than a cam shaft control system in an internal combustion engine.

Of course there are many different types of computer system architectures that differ from one another in the number of different memories including different types of hierarchical cache memories the number of processors and the connectivity of the processors with other system components the number of internal communications busses and serial links and in many other ways. However computer systems generally execute stored programs by fetching instructions from memory and executing the instructions in one or more processors. Computer systems include general purpose computer systems such as personal computers PCs various types of servers and workstations and higher end mainframe computers but may also include a plethora of various types of special purpose computing devices including data storage systems communications routers network nodes tablet computers and mobile telephones.

Until recently computational services were generally provided by computer systems and data centers purchased configured managed and maintained by service provider organizations. For example an e commerce retailer generally purchased configured managed and maintained a data center including numerous web servers back end computer systems and data storage systems for serving web pages to remote customers receiving orders through the web page interface processing the orders tracking completed orders and other myriad different tasks associated with an e commerce enterprise.

Cloud computing facilities are intended to provide computational bandwidth and data storage services much as utility companies provide electrical power and water to consumers. Cloud computing provides enormous advantages to small organizations without the resources to purchase manage and maintain in house data centers. Such organizations can dynamically add and delete virtual computer systems from their virtual data centers within public clouds in order to track computational bandwidth and data storage needs rather than purchasing sufficient computer systems within a physical data center to handle peak computational bandwidth and data storage demands. Moreover small organizations can completely avoid the overhead of maintaining and managing physical computer systems including hiring and periodically retraining information technology specialists and continuously paying for operating system and database management system upgrades. Furthermore cloud computing interfaces allow for easy and straightforward configuration of virtual computing facilities flexibility in the types of applications and operating systems that can be configured and other functionalities that are useful even for owners and administrators of private cloud computing facilities used by a single organization.

While the execution environments provided by operating systems have proved to be an enormously successful level of abstraction within computer systems the operating system provided level of abstraction is nonetheless associated with difficulties and challenges for developers and users of application programs and other higher level computational entities. One difficulty arises from the fact that there are many different operating systems that run within various different types of computer hardware. In many cases popular application programs and computational systems are developed to run on only a subset of the available operating systems and can therefore be executed within only a subset of the various different types of computer systems on which the operating systems are designed to run. Often even when an application program or other computational system is ported to additional operating systems the application program or other computational system can nonetheless run more efficiently on the operating systems for which the application program or other computational system was originally targeted. Another difficulty arises from the increasingly distributed nature of computer systems. Although distributed operating systems are the subject of considerable research and development efforts many of the popular operating systems are designed primarily for execution on a single computer system. In many cases it is difficult to move application programs in real time between the different computer systems of a distributed computer system for high availability fault tolerance and load balancing purposes. The problems are even greater in heterogeneous distributed computer systems which include different types of hardware and devices running different types of operating systems. Operating systems continue to evolve as a result of which certain older application programs and other computational entities may be incompatible with more recent versions of operating systems for which they are targeted creating compatibility issues that are particularly difficult to manage in large distributed systems.

For all of these reasons a higher level of abstraction referred to as the virtual machine has been developed and evolved to further abstract computer hardware in order to address many difficulties and challenges associated with traditional computing systems including the compatibility issues discussed above. illustrates one type of virtual machine and virtual machine execution environment. uses the same illustration conventions as used in . In particular the computer system in includes the same hardware layer as the hardware layer shown in . However rather than providing an operating system layer directly above the hardware layer as in the virtualized computing environment illustrated in features a virtualization layer that interfaces through a virtualization layer hardware layer interface equivalent to interface in to the hardware. The virtualization layer provides a hardware like interface to a number of virtual machines such as virtual machine executing above the virtualization layer in a virtual machine layer . Each virtual machine includes one or more application programs or other higher level computational entities packaged together with an operating system such as application and operating system packaged together within virtual machine . Each virtual machine is thus equivalent to the operating system layer and application program layer in the general purpose computer system shown in . Each operating system within a virtual machine interfaces to the virtualization layer interface rather than to the actual hardware interface . The virtualization layer partitions hardware resources into abstract virtual hardware layers to which each operating system within a virtual machine interfaces. The operating systems within the virtual machines in general are unaware of the virtualization layer and operate as if they were directly accessing a true hardware interface. The virtualization layer ensures that each of the virtual machines currently executing within the virtual environment receive a fair allocation of underlying hardware resources and that all virtual machines receive sufficient resources to progress in execution. The virtualization layer interface may differ for different operating systems. For example the virtualization layer is generally able to provide virtual hardware interfaces for a variety of different types of computer hardware. This allows as one example a virtual machine that includes an operating system designed for a particular computer architecture to run on hardware of a different architecture. The number of virtual machines need not be equal to the number of physical processors or even a multiple of the number of processors. The virtualization layer includes a virtual machine monitor module that virtualizes physical processors in the hardware layer to create virtual processors on which each of the virtual machines executes. For execution efficiency the virtualization layer attempts to allow virtual machines to directly execute non privileged instructions and to directly access non privileged registers and memory. However when the operating system within a virtual machine accesses virtual privileged instructions virtual privileged registers and virtual privileged memory through the virtualization layer interface the accesses result in execution of virtualization layer code to simulate or emulate the privileged resources. The virtualization layer additionally includes a kernel module that manages memory communications and data storage machine resources on behalf of executing virtual machines. The kernel for example maintains shadow page tables on each virtual machine so that hardware level virtual memory facilities can be used to process memory accesses. The kernel additionally includes routines that implement virtual communications and data storage devices as well as device drivers that directly control the operation of underlying hardware communications and data storage devices. Similarly the kernel virtualizes various other types of I O devices including keyboards optical disk drives and other such devices. The virtualization layer essentially schedules execution of virtual machines much like an operating system schedules execution of application programs so that the virtual machines each execute within a complete and fully functional virtual hardware layer.

The advent of virtual machines and virtual environments has alleviated many of the difficulties and challenges associated with traditional general purpose computing. Machine and operating system dependencies can be significantly reduced or entirely eliminated by packaging applications and operating systems together as virtual machines and virtual appliances that execute within virtual environments provided by virtualization layers running on many different types of computer hardware. A next level of abstraction referred to as virtual data centers or virtual infrastructure provide a data center interface to virtual data centers computationally constructed within physical data centers. illustrates virtual data centers provided as an abstraction of underlying physical data center hardware components. In a physical data center is shown below a virtual interface plane . The physical data center consists of a virtual data center management server and any of various different computers such as PCs on which a virtual data center management interface may be displayed to system administrators and other users. The physical data center additionally includes generally large numbers of server computers such as server computer that are coupled together by local area networks such as local area network that directly interconnects server computer and and a mass storage array . The physical data center shown in includes three local area networks and that each directly interconnects a bank of eight servers and a mass storage array. The individual server computers such as server computer each includes a virtualization layer and runs multiple virtual machines. Different physical data centers may include many different types of computers networks data storage systems and devices connected according to many different types of connection topologies. The virtual data center abstraction layer a logical abstraction layer shown by a plane in abstracts the physical data center to a virtual data center comprising one or more resource pools such as resource pools one or more virtual data stores such as virtual data stores and one or more virtual networks. In certain implementations the resource pools abstract banks of physical servers directly interconnected by a local area network.

The virtual data center management interface allows provisioning and launching of virtual machines with respect to resource pools virtual data stores and virtual networks so that virtual data center administrators need not be concerned with the identities of physical data center components used to execute particular virtual machines. Furthermore the virtual data center management server includes functionality to migrate running virtual machines from one physical server to another in order to optimally or near optimally manage resource allocation provide fault tolerance and high availability by migrating virtual machines to most effectively utilize underlying physical hardware resources to replace virtual machines disabled by physical hardware problems and failures and to ensure that multiple virtual machines supporting a high availability virtual appliance are executing on multiple physical computer systems so that the services provided by the virtual appliance are continuously accessible even when one of the multiple virtual appliances becomes compute bound data access bound suspends execution or fails. Thus the virtual data center layer of abstraction provides a virtual data center abstraction of physical data centers to simplify provisioning launching and maintenance of virtual machines and virtual appliances as well as to provide high level distributed functionalities that involve pooling the resources of individual physical servers and migrating virtual machines among physical servers to achieve load balancing fault tolerance and high availability.

The distributed services include a distributed resource scheduler that assigns virtual machines to execute within particular physical servers and that migrates virtual machines in order to most effectively make use of computational bandwidths data storage capacities and network capacities of the physical data center. The distributed services further include a high availability service that replicates and migrates virtual machines in order to ensure that virtual machines continue to execute despite problems and failures experienced by physical hardware components. The distributed services also include a live virtual machine migration service that temporarily halts execution of a virtual machine encapsulates the virtual machine in an open virtualization format OVF package transmits the OVF package to a different physical server and restarts the virtual machine on the different physical server from a virtual machine state recorded when execution of the virtual machine was halted. The distributed services also include a distributed backup service that provides centralized virtual machine backup and restore.

The core services provided by the VDC management server include host configuration virtual machine configuration virtual machine provisioning generation of virtual data center alarms and events ongoing event logging and statistics collection a task scheduler and a resource management module. Each physical server also includes a host agent virtual machine through which the virtualization layer can be accessed via a virtual infrastructure application programming interface API . This interface allows a remote administrator or user to manage an individual server through the infrastructure API. The virtual data center agents access virtualization layer server information through the host agents. The virtual data center agents are primarily responsible for offloading certain of the virtual data center management server functions specific to a particular physical server to that physical server. The virtual data center agents relay and enforce resource allocations made by the VDC management server relay virtual machine provisioning and configuration change commands to host agents monitor and collect performance statistics alarms and events communicated to the virtual data center agents by the local host agents through the interface API and to carry out other similar virtual data management tasks.

The virtual data center abstraction provides a convenient and efficient level of abstraction for exposing the computational resources of a cloud computing facility to cloud computing infrastructure users. A cloud director management server exposes virtual resources of a cloud computing facility to cloud computing infrastructure users. In addition the cloud director introduces a multi tenancy layer of abstraction which partitions VDCs into tenant associated VDCs that can each be allocated to a particular individual tenant or tenant organization both referred to as a tenant. A given tenant can be provided one or more tenant associated VDCs by a cloud director managing the multi tenancy layer of abstraction within a cloud computing facility. The cloud services interface in exposes a virtual data center management interface that abstracts the physical data center.

Thus as discussed above virtualized computer systems may include a server or host with virtualization layers that support execution of virtual machines within the server or host clusters of servers and other hardware managed by cluster management server or virtual machine as an aggregated platform for virtual machine execution virtual data centers managed by a virtual data center management server and multi tenant virtual data centers managed by VCC servers. The current application is directed to any level of virtual computing abstraction that aggregates physical computational resources including multiple hardware threads cores and or processors into resource pools from which resources are allocated to virtual machines as well as to traditional operating systems used in multi processor computer systems.

Next physical computer hardware including multi core processors is discussed. illustrates an instruction set architecture ISA provided by a modern processor as the low level execution environment for binary code and assembler code. The ISA commonly includes a set of general purpose registers a set of floating point registers a set of single instruction multiple data SIMD registers a status flags register an instruction pointer special status control and instruction pointer and operand registers for floating point instruction execution segment registers for segment based addressing a linear virtual memory address space and the definitions and specifications of the various types of instructions that can be executed by the processor . The length in bits of the various registers is generally implementation dependent often related to the fundamental data unit that is manipulated by the processor when executing instructions such as a 16 bit 32 bit or 64 bit word and or 64 bit or 128 bit floating point words. When a computational entity is instantiated within a computer system the values stored in each of the registers and in the virtual memory address space together comprise the machine state or architecture state for the computational entity. While the ISA represents a level of abstraction above the actual hardware features and hardware resources of a processor the abstraction is generally not too far removed from the physical hardware. As one example a processor may maintain a somewhat larger register file that includes a greater number of registers than the set of general purpose registers provided by the ISA to each computational entity. ISA registers are mapped by processor logic often in cooperation with an operating system and or virtual machine monitor to registers within the register file and the contents of the registers within the register file may in turn be stored to memory and retrieved from memory as needed in order to provide temporal multiplexing of computational entity execution.

In many modern operating systems the operating system provides an execution environment for concurrent execution of a large number of processes each corresponding to an executing application program on one or a relatively small number of hardware processors by temporal multiplexing of process execution. illustrates a general technique for temporal multiplexing used by many operating systems. The operating system maintains a linked list of process context data structures such as data structure in memory. Each process context data structure stores state information for the process such as state information in data structure along with additional state for concurrently executing threads such as thread states in data structure . The operating system generally provides blocks of time or blocks of execution cycles to the concurrently executing processes according to a process execution scheduling strategy such as round robin scheduling or various types of more complex scheduling strategies many employing pre emption of currently executing processes. Dormant processes are made executable by a context switch as indicated in during which a portion of the architectural state of a currently executing process is stored into an associated process context data structure for the process as represented by arrow in and the stored portion of the architectural state of a dormant process is loaded into processor registers as indicated by arrows in . In general a process is allowed to execute for some predetermined length of time or until the process is stalled or blocked waiting for the availability of data or the occurrence of an event. When either the allotted amount of time or number of processor cycles have been used or when the process is stalled a portion of the architectural state of the process and any concurrent threads executing within the context of the process are stored in the associated process context data structure freeing up the hardware resources mapped to the process in order to allow execution of a different process. In the operating system context threads are essentially lightweight processes with minimal thread specific state. In many cases each thread may have a thread specific set of registers but all the threads within a particular process context generally share the virtual memory address space for the process. Thus in general the threads represent different execution instantiations of a particular application corresponding to the process within which the threads execute. One example of a multi threaded application is a server application in which a new execution thread is launched to handle each incoming request. In general an operating system may provide for simultaneous execution of as many threads as there are logical processors in the computing system controlled by the operating system. Until recently the smallest granularity hardware resource for execution of an execution thread was an actual hardware processor. As discussed further below in certain more recent and currently available processors the smallest granularity hardware resource supporting execution of a process or thread is a logical processor that corresponds to a hardware thread within an SMT processor or SMT processor core.

SMT processors a relatively recent development in hardware architecture provide for simultaneous execution of multiple hardware execution threads. SMT processors or SMT processor cores provide for simultaneous hardware execution threads by duplicating a certain portion of the hardware resources including certain of the ISA registers within a processor or processor core by partitioning other of the hardware resources between hardware execution threads and by allowing hardware execution threads to compete for and share other of the hardware resources. Modern processors are highly pipelined and SMT processors or SMT processor cores can often achieve much higher overall computational throughput because the various processor resources that would otherwise be idled during execution of the instructions corresponding to one hardware thread can be used by other simultaneously executing hardware threads. Operating system threads discussed earlier with reference to and hardware threads are conceptually similar but differ dramatically in implementation and operational characteristics. As discussed above with reference to operating system provided threads are products of temporal multiplexing by the operating system of hardware resources and the temporal multiplexing involves operating system executed context switches. By contrast hardware threads actually simultaneously execute within a processor or processor core without hardware thread context switches. Complex pipelined architecture of modern processors allows many different instructions to be executed in parallel and an SMT processor or SMT processor core allows instructions corresponding to two or more different hardware threads to be simultaneously executed.

With the introduction of SMT processors and SMT processor cores the level of complexity has additionally increased. Monitoring computational throughput provided to each virtual machine in these complex environments is non trivial and the performance monitoring registers and other hardware facilities provided by modern processors are generally inadequate for determining the computational throughputs for VMs mapped to hardware threads. Determination of computational throughputs for VMs managed by VMM is useful in scheduling VM execution and optimizing execution schedules as well as in accounting operations used to charge clients of large computer systems such as cloud computing facilities based on the processor cycles used by the clients or on some type of measured computational throughput often related to the rate of instruction execution provided to the clients. As further discussed below in the case that clients are billed based on clock time during which their applications run within a cloud computing facility and when their applications experience performance imbalances that result in frequent stalling on exhausted resources with respect to one or VMs of another client simultaneously executing on hardware threads within an SMT processor or SMT processor core shared by multiple clients accounting only by clock time or even by instruction throughput may result in less than fair billing practices. A more fair accounting procedure would be to bill clients based on productive execution of instructions. However current hardware performance monitoring facilities are generally inadequate to detect many types of performance imbalance.

Each core in a multi core processor is essentially a discrete separate processor that is fabricated along with all the other cores in a multi core processor within a single integrated circuit. As discussed below each core includes multiple instruction execution pipelines and internal L1 caches. In some cases each core also contains an L2 cache while in other cases pairs of cores may share an L2 cache. As discussed further below SMT processor cores provide for simultaneous execution of multiple hardware threads. Thus a multi SMT core processor containing four SMT processors that each supports simultaneous execution of two hardware threads can be viewed as containing eight logical processors each logical processor corresponding to a single hardware thread.

The memory caches such as the L3 cache and the multi core processor shown in is generally SRAM memory which is much faster but also more complex and expensive than DRAM memory. The caches are hierarchically organized within a processor. The processor attempts to fetch instructions and data during execution from the smallest highest speed L1 cache. When the instruction or data value cannot be found in the L1 cache the processor attempts to find the instruction or data in the L2 cache. When the instruction or data is resident in the L2 cache the instruction or data is provided from the L2 cache to the L1 cache. When the L1 cache is full instruction or data within the L1 cache is evicted or overwritten by the instruction or data moved from the L2 cache to the L1 cache. When the data or instruction is not resident within the L2 cache the processor attempts to access the data or instruction in the L3 cache and when the data or instruction is not present in the L3 cache the data or instruction is fetched from DRAM system memory. Ultimately data and instruction are generally transferred from a mass storage device to the DRAM memory. As with the L1 cache when intermediate caches are full eviction of an already resident instruction or data generally occurs in order to copy data from a downstream cache into an upstream cache. It should be noted that in certain types of processor architectures cache lines may be replicated down through the cache hierarchy while in other processor architectures a given cache line generally resides in a single cache within the hierarchy.

The processor core illustrated in includes an L2 cache connected to an L3 cache in shared by other processor cores as well as to an L1 instruction cache and an L1 data cache . The processor core also includes a first level instruction translation lookaside buffer TLB a first level data TLB and a second level universal TLB . These TLBs store virtual memory translations for the virtual memory addresses of instructions and data stored in the various levels of caches including the L1 instruction cache the L1 data cache and L2 cache. When a TLB entry exists for a particular virtual memory address accessing the contents of the physical memory address corresponding to the virtual memory address is far more computationally efficient than computing the physical memory address using the previously described page directory and page tables.

The processor core includes a front end in order functional block and a back end out of order execution engine . The front end block reads instructions from the memory hierarchy and decodes the instructions into simpler microinstructions which are stored in the instruction decoder queue IDQ . The microinstructions are read from the IDQ by the execution engine and executed in various parallel execution pipelines within the execution engine. The front end functional block include an instruction fetch unit IFU that fetches 16 bytes of aligned instruction bytes on each clock cycle from the L1 instruction cache and delivers the 16 bytes of aligned instruction bytes to the instruction length decoder ILD . The IFU may fetch instructions corresponding to a particular branch of code following a branch instruction before the branch instruction is actually executed and therefore before it is known with certainty that the particular branch of code will be selected for execution by the branch instruction. Selection of code branches from which to select instructions prior to execution of a controlling branch instruction is made by a branch prediction unit . The ILD processes the 16 bytes of aligned instruction bytes provided by the instruction fetch unit on each clock cycle in order to determine lengths of the instructions included in the 16 bytes of instructions and may undertake partial decoding of the individual instructions providing up to six partially processed instructions per clock cycle to the instruction queue IQ . The instruction decoding unit IDU reads instructions from the IQ and decodes the instructions into microinstructions which the IDU writes to the IDQ . For certain complex instructions the IDU fetches multiple corresponding microinstructions from the MS ROM .

The back end out of order execution engine includes a register alias table and allocator that allocates execution engine resources to microinstructions and uses register renaming to allow instructions that use a common register to be executed in parallel. The register alias table and allocator component then places the microinstructions following register renaming and resource allocation into the unified reservation station URS for dispatching to the initial execution functional units and of six parallel execution pipelines. Microinstructions remain in the URS until all source operands have been obtained for the microinstructions. The parallel execution pipelines include three pipelines for execution of logic and arithmetic instructions with initial functional units a pipeline for loading operands from memory with initial functional unit and two pipeline initial functional units for storing addresses and data to memory. A memory order buffer MOB facilitates speculative and out of order loads and stores and ensures that writes to memory take place in an order corresponding to the original instruction order of a program. A reorder buffer ROB tracks all microinstructions that are currently being executed in the chains of functional units and when the microinstructions corresponding to a program instruction have been successfully executed notifies the retirement register file to commit the instruction execution to the architectural state of the process by ensuring that ISA registers are appropriate updated and writes to memory are committed.

A processor core is of course an exceedingly complex device containing a forest of signal paths and millions of individual transistors and other circuit components. The myriad components and operational details are far beyond the scope of the current discussion. Instead the current discussion is intended to provide a context for the performance imbalance monitoring registers included within a processor in order to facilitate performance monitoring with respect to hardware threads.

Those functional components that are not altered the functional components that are replicated with each hardware thread exclusively accessing its own copy and the functional components that are strictly partitioned are not generally associated with thread specific performance monitoring problems. However the functional components that are either flexibly partitioned or shared may be difficult to monitor in order to provide useful hardware thread specific performance monitoring data. The performance imbalance monitoring registers which the current application discloses are therefore most usefully applied to flexibly partitioned and shared functional units of an SMT processor core. The following discussion provides a high level description of performance imbalance monitoring registers that are added to an SMT processor or SMT processor core to supplement according to the current application the existing performance monitoring registers of a processor in order to facilitate thread specific monitoring.

Returning to it can be seen that all of the cores in a multi core processor end up sharing a single memory controller through which the cores access system memory . Memory accesses by cores result in writing of data from local caches to local memory or writing data from system memory to the local caches. Like the memory controller the L3 cache is also shared among all four cores. When a virtual machine is executing on each core or hardware thread within each core different virtual machines may be currently accessing local memory through the single memory controller and associated data transmission paths between local memory the L3 cache and other components of the multi core processor. Because of the inherent multiplexing nature of the memory controller memory bus and other components of the multi core processor a single virtual machine executing on a hardware thread or core within the multi core processor may through frequent and or large volume memory accesses end up monopolizing a large portion of the total memory subsystem bandwidth available to the multi core processor to the detriment of other virtual machines executing in other hardware threads and or cores within the multi core processors. Currently only very coarse grained control of memory accesses by virtual machines within multi core processors is available to various types of virtualized computer system managers in order to attempt to provide a reasonable level of access to each of multiple concurrently and simultaneously executing virtual machines within a multi core processor. For example a management server may use performance monitoring data provided by modern processors to detect long cache and or TLB latencies as well as high CPU utilization due to frequent memory stalls and infer from the data that these cores may be receiving less than a fair share of the total memory access bandwidth available to the cores of a multi core processor. Alternatively application level throughput can be measured at system levels with degradation in application throughput serving as one indication of performance lowering memory bandwidth contention. However performance monitoring based control has significant lag times and may be based on inferences drawn from incomplete data as a result of which it is currently not possible to accurately and reliably provide specified allotments of memory access bandwidth to particular virtual machines simultaneously executing on a multi core processor. As discussed above a portion of various aggregated resources including data storage memory networking and processor resources used by virtual machines may be controlled through the resource pool administrative interface but viewing the memory controller and associated components as a memory access resource it is not currently possible to administer the memory access resource in the same fashion as data storage capacity memory capacity networking bandwidth and processing bandwidth.

The current application is directed to methods and systems for providing fine grained immediate and direct control over multiplexing of memory access resources among virtual machines executing within a multi core processor. These methods and systems can be extended in a straightforward manner to many other types of physical resources and subsystems within processors and other components of computer systems for which simultaneously and concurrently executing virtual machines contend.

By providing a memory controller within a multi core processor that provides for multi channel operation the memory access resource represented by the memory controller L3 caches and other components of the multi core processor can be shared among virtual machines within a virtualized computer system as another type of resource component of a resource pool. shows resource pool related allocations for a virtual machine within a virtualized computer system similar to the resource pool allocations encoded in the data storage shown in . The resource pool allocations include resource allocations related to data storage memory capacity CPU bandwidth networking bandwidth and memory access . In the implementation shown in the memory access resource pool component of the resource pool provides for allocation of shares and specification of a memory access resource limit as shown in . In this implementation a memory access resource reservation is considered unnecessary since it is unlikely that a virtual machine would fail to be launched based on a memory access resource reservation. However in those cases in which memory access resource reservation is desired reservation can be added and enforced by a virtualized computer system manager.

In general the share allotment for a channel specifies the frequency in time at which memory access requests are processed by the channel while the tokens and token increments for channels present a hard limit on the portion of the memory access resource that can be used by a particular channel. Thus the shares and token increment provide a physical hardware basis for resource pool share based and limit based administration.

As discussed above with reference to a memory controller timestamps incoming memory access requests and then processes the requests in timestamp order. Selection of memory access requests having the same timestamp value may be carried out randomly or according to any of various different arbitrary selection methods and criteria. The number of shares allotted to a channel influence the frequency with which memory access requests are processed for the channel via setting of the F value for the channel in step in . The token increment assigned to a channel determines the maximum rate of memory request processing carried out by the memory controller on behalf of the channel as seen in step in . Thus the multi channel memory controller provides a physical hardware basis for treating the memory access resource as an additional type of computing resource that can be managed within a resource pool by a management layer or virtualized computer system manager.

The above described timestamp based multiplexing memory controller can be alternatively implemented. For example. a simpler implementation is to associate within the memory controller a memory access request queue with each channel and service the memory access request queues in round robin fashion with the number of requests serviced or amount of memory accessed during a single servicing of a memory access request queue proportional to the fraction of total shares allocated to the channel corresponding to the memory access request queue. Additional implementations are possible including periodic allocations of shares to channels and random servicing of memory requests with corresponding deductions in shares until no further memory requests are serviceable at which point a next allocation of shares is made.

Multiplexing of memory bandwidth provides for controlled and specifiable fair sharing of the memory bandwidth resource among multiple computational entities competing for the memory bandwidth resource. Moreover a multiplexing controller can be used to provide a specified quality of service to each concurrently and or simultaneously executing computational entity within a multi core processor or other hardware device that supports simultaneous execution of multiple threads or processes that access memory through one or more shared memory controllers and other hardware components related to memory access.

Although the present invention has been described in terms of particular embodiments it is not intended that the invention be limited to these embodiments. Modifications within the spirit of the invention will be apparent to those skilled in the art. For example any of a variety of different implementation and design parameters including control structures modular organization data structures type of logic circuitry type of firmware and other such parameters may be varied in order to generate a variety of different implementations of a multi channel memory controller and a management layer or virtualized computer system manager that employs a multi channel controller to allocate memory access resources as part of a resource pool to virtual machines. While several separate buffers are shown in within the memory controller certain implementations may use only a single input queue within which memory access requests are time stamped and from which memory access requests are selected for processing. A multiple buffer implementation is shown in for convenience in explaining the memory access resource multiplexing carried out by the memory controller in addition to representing one type of implementation. Multi channel memory controllers can be included in any of various different types of multi core processors as well as in other types of computer systems in which a single memory controller is shared by multiple computational entities that execute simultaneously or concurrently. Generation of a multi channel capacity can be extended in similar fashion to other types of physical hardware devices in which simultaneously and or concurrently executing virtual machines or other computational entities contend.

It is appreciated that the previous description of the disclosed embodiments is provided to enable any person skilled in the art to make or use the present disclosure. Various modifications to these embodiments will be readily apparent to those skilled in the art and the generic principles defined herein may be applied to other embodiments without departing from the spirit or scope of the disclosure. Thus the present disclosure is not intended to be limited to the embodiments shown herein but is to be accorded the widest scope consistent with the principles and novel features disclosed herein.

