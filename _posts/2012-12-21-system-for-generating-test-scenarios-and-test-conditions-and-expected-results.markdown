---

title: System for generating test scenarios and test conditions and expected results
abstract: A requirements testing system facilitates the review and analysis of requirement statements for software applications. The requirements testing system automatically generates test artifacts from the requirement statements, including test scenarios, test conditions, test hints, and expected results. These test artifacts characterize the requirements statements to provide valuable analysis information that aids understanding what the intentions of the requirement statements are. The automation of the generation of these test artifacts produces numerous benefits, including fewer errors, objectivity, and no dependency on the skills and experience of a creator.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09262404&OS=09262404&RS=09262404
owner: ACCENTURE GLOBAL SERVICES LIMITED
number: 09262404
owner_city: Dublin
owner_country: IE
publication_date: 20121221
---
This application claims priority to Indian provisional patent application No. 122 CHE 2012 filed Jan. 12 2012 and to the corresponding Indian non provisional patent application given the same serial number by the Indian Patent Office as the provisional patent application No. 122 CHE 2012 and that was received at the Indian Patent Office on Dec. 13 2012. Both applications are entirely incorporated by reference.

This disclosure relates to test scenarios and test conditions and expected results TCERs . More specifically this disclosure relates to systems and methods generally referred to as systems for generating test scenarios and TCERs from natural language e.g. English language specification documents.

The present systems and methods described herein may be embodied in a number of different forms. Not all of the depicted components may be required however and some implementations may include additional different or fewer components from those expressly described in this disclosure. Variations in the arrangement and type of the components may be made without departing from the spirit or scope of the claims as set forth herein.

It is noted that this description discusses document analysis. The analysis operates on units of information in the document. The units of information as examples may include full sentences sentence fragments phrases or any other sequence of one or more words. In some examples given below the units of information are statements such as requirements statements in a requirements specification. The statements may or may not be full sentences. Thus while some examples may refer specifically to sentence analysis or statement analysis the techniques described below are not limited to sentences or statements but may be applied to other units of information present in a document. For instance the document analysis methods described in this description may be applied to both requirements statements and requirements sentences in a same or similar way.

The network may follow any of a wide variety of network topologies and technologies. As examples the network may include Local Area Networks LANs Wide Area Networks WANs Internet connections Ethernet networks or Fiber Distributed Data Interconnect FDDI packet switched networks that may communicate Transmission Control Protocol Internet Protocol TCP IP packets or any data following any other communication protocol. The network provides a transport mechanism or interconnection of multiple transport mechanisms supporting data exchange between the system and any source of documents to analyze including the requirements document .

A processor in the system may analyze the requirement statements to determine the test artifacts. A requirement statement may for example be implemented as a single sentence or other sequence of one or more words. The requirement statement may for example be in unconstrained natural language structured formats or model based formats. An example of a requirement statement in a structured format may be a requirement statement limited to subject action and object. Such a restriction may exclude requirement statements with multiple objects or requirement statements with nouns which are neither subjects nor objects. Other examples are possible.

In some instances the requirement statements may include data that is not intended for processing. Such data may be marked e.g. the data not intended for processing may be enclosed in brackets . The requirement statements may first be processed by the preprocessor as described in more detail below. Among other things the preprocessor may remove data enclosed in brackets as well as the brackets themselves. The processor may generate e.g. on the display an analysis report . The analysis report may specify the artifacts test artifacts or any other analysis details that the system determines.

An artifact may be a tangible by product produced during the development of software e.g. a use case or a class diagram . Artifacts of a software project may be or resemble deliverables of the software project though the software itself i.e. the released end product may not be an artifact. A test artifact may be a tangible by product produced during software testing. Test artifacts may relate to a characteristic of a requirement statement. Examples of test artifacts may include an indication of one or more of the following requirement testability requirement intent test scenario test conditions expected results requirement category requirement data and requirement ambiguity. Testability artifacts intent artifacts category artifacts and data artifacts may be examples of test artifacts as well as or alternatively an ambiguous phrase identified in a requirement statement. For example the test artifacts may include Testability specifying for example whether the requirement statement is testable Intent specifying for example the intent or purpose of the requirement statement Category specifying for example what type of requirement the requirement statement establishes Data specifying for example the data that the requirement statement operates on and Ambiguity specifying whether all or parts of a requirement statement are ambiguous with regard to its testability The test scenarios the test conditions and the expected results. The system may determine additional fewer or different artifacts including grammatical correctness of the requirement statement in whole or in part.

In some requirements testing systems and methods a requirement statement is obtained and stored in a memory. The requirement statement is submitted to a grammatical parser executed by a processor to obtain parser outputs characterizing the requirement statement. A test artifact ruleset is applied with the processor to the parser outputs to determine a test artifact applicable to the requirement statement.

These and other requirements testing systems and methods allow for developers to check for testability and various features of statements and documents. Another benefit of the requirements testing system is that it facilitates creation of test artifacts from requirement statements. The test artifacts reduce testing cycle time effort and expense and improve test quality. As a result the resulting software application is more reliable less expensive and is more timely delivered. This allows developers to implement complex statements and documents in less time and with fewer mistakes or ambiguities increasing efficiency and effectiveness of the requirements statements. Requirements testing systems also result in various other advantages and effects.

In some instances it may be beneficial to identify and review one or more test artifacts such as one or more Test Scenarios Test Conditions and Expected Results and or the Test Hints each of which is explained in more detail below. However there are significant technical challenges involved with generating these test artifacts in a consistent accurate and reliable way. The technical implementation of the requirement testing system addresses these technical challenges. The resulting requirement testing system is less error prone may in some instances provide a guarantee on completeness may be very efficient and objective and not time consuming or subjective and or may not require any extra effort to build traceability and audit compliance.

The requirement testing system may include a communication interface processor memory and or display 

The memory may also or alternatively include pre processor logic an entity extractor scenario generation module and or test condition logic . The entity extractor may be configured to extract proper nouns from a requirements sentence as discussed later. The scenario generation module may be configured to generate one or more test scenarios for the requirements sentence as discussed later. The test condition logic may be configured to generate one or more positive or negative test conditions test hints and or expected results for the requirements sentence as discussed later. While shown as separate modules or logic one or more of the entity extractor scenario generation module test condition logic may be combined incorporated or part of one module or logic such as one analysis logic component. Other implementations are possible.

A Test Scenario also referred to as a scenario description or a test scenario description may be a short description of the test being conducted to verify a requirement. A Test Condition may be the particular condition which is being tested. Test Hints may be the ordered sequence of steps a tester would have to execute to perform the test. The corresponding output from a correctly implemented system may be an Expected Result . The combination of a Test Condition and the Expected Result is referred to as a TCER.

The requirements testing system may also or alternatively determine the categorization of a requirements sentence and or identify ambiguous phrases in the requirements sentence . In some embodiments no restriction on the structure of the requirements sentence is imposed and the requirement testing system may work on each single requirements sentence and generate one or more test conditions and associated test hints and expected results. The requirement testing system may be configured or operable to analyze a single sentence or requirement statement or as many as desired and need not maintain state across multiple statements or sentences within statements.

The requirement sentences are parsed using Link Grammar parser abbreviated as LG . LG provides information on the syntax of a grammatically correct English sentence by connecting pairs of words through labeled links shows the parsed output of a sentence . The links between pairs of words are labeled and each label provides specific information about the connecting pair. For example the Subject of a sentence is indicated by a link having a label S . LG also provides the Noun Phrases Verb Phrases etc. through a constituent tree output . The links are created through a Grammar maintained in a dictionary file. The interpretation of the parsed output through the label of the links the structure of the sentence and the constituent tree helps us decipher the following information the sentence structure antecedent and consequent quantity singular and plural Parts of Speech Noun Verb Verb in past tense etc. and grammar Subject Object etc. . We interpret and exploit this information to extract entities and generate the Test Cases

The pre processor logic includes preprocessing rulesets e.g. the preprocessing rulesets and . The preprocessing rulesets cause the pre processor logic to perform analysis modification or other actions on requirement statements. Table 1 and Table 2 give examples of the pre processing rulesets.

The logic may begin with the requirements testing system obtaining a sentence or statement from a document . The statement may be a requirements sentence from a requirements document or may be another statement sentence or collection of words which may be gathered from a document. The requirements sentence may be a statement randomly selected from a document may be selected in a sequential order may be selected according to one or more algorithms or may be selected in various other ways. Other examples are possible.

The logic applies pre processing e.g. prior to applying the entity extractor . The pre processing may be applied by the pre processor according to any one of the methods described throughout this disclosure.

The logic applies the entity extractor to analyze the requirements sentence . While the entity extractor is shown as being applied to each requirements sentence individually before each requirements sentence is analyzed in some systems the logic may apply the entity extractor to an entire requirements document prior to any parsing or analysis of the requirements document or requirements sentence within the document. For example the entity extractor may be used before any parsing or analysis of the requirements document to ensure that link grammar parser logic correctly identifies all nouns in each requirements sentence when the requirements sentence is parsed. Other variations are possible.

The role of the entity extractor may be to pick up some or all Noun Phrases in a requirements sentence . The entity extractor may determine which nouns should be identified in a given requirements sentence in various ways. For example the logic may cause the requirements testing system to identify noun phrases in a requirements sentence with or using link grammar parser logic as shown in . The logic may also cause the requirements testing system to run the link grammar parser in a Batch mode on the entire requirements document . The entity extractor may identify or otherwise pick up Noun Phrases from the links generated by the parser. The Noun Phrases may include acronyms such as ACBG proper nouns such as Peru common nouns such as Order Processing System and or phrases included in parentheses. Additionally or alternatively if the requirements document contains a Glossary section such as an entity glossary the corresponding terms in the entity glossary may also be considered and used by the entity extractor in gathering nouns . In some instances the automatically identified nouns or entities gathered by the entity extractor may be presented to the user for verification . In these cases the user may have the option to either accept or modify the selected entities and or may add new ones. The entity extractor may in some instances be used to extract nouns from requirements sentences that even fail to link. Entity extractor examples may be found in Appendix A.

An example of the logic in hardware or software which the entity extractor may implement to identify noun phrases is 

One reason for the inclusion of an entity extractor is to ensure that all entities are treated as Nouns by link grammar or link grammar parser logic . For example some requirements sentence may use verbs as nouns. As an example of a requirements sentence that uses a verb as a noun consider this requirements sentence 

In the requirements sentence 1 the word Order which is normally a verb is used as a noun in the requirements sentence. In such cases without applying the entity extractor link grammar parser logic may interpret Order to be a verb and may produce a wrong set of linkages treating Verbs as Verbs which may lead to a wrong set of TCERs.

In the example of the requirements sentence 1 the entity extractor may automatically highlight Order Processing System ACBG reports and Peru because they are nouns. The entity extractor may also suggest entities from requirements sentences where the link grammar parser logic fails to completely link the sentence. When such words are forcibly treated as a noun the linkage by the link grammar parser logic may succeed. As noted in some instances the requirements testing system may show selected entities identified by the entity extractor to the user for verification and the user may modify the selected entities.

The entity extractor may help increase the accuracy of the requirements testing system . In other systems the entity extractor may not be needed or included in the requirements testing system and the user may generate test artifacts without running the entity extractor .

In some systems once the selected entities are accepted by the user the logic may re run the link grammar parser logic making sure the entities are treated as nouns. The link grammar parser logic may also obtain syntactic and lexical information about the requirements sentence when re running the link grammar parser logic .

Once the requirements testing system has applied the entity extractor as in and each requirements sentence has been again parsed by the link grammar parser logic as in and to obtain syntactic and lexical information the logic may cause the requirements testing system to determine if the requirements sentence is testable . Upon parsing if the requirements sentence is unable to be linked by the parser the requirements testing system may report the requirements sentence as failed to analyze and the requirements testing system may move onto the next requirements sentence . The requirements testing system may quantify input requirements sentence that are successfully linked by the parser as either testable or non testable . The requirements testing system may analyze the link grammar parser logic output of a sentence to identify a particular set of contiguous links which imply testability. In some instances the logic may apply testability rules T.x to determine if the requirements sentence is testable . The logic may check the testability of the requirements sentence as explained below or in other manners.

An example of links which the requirements testing system may identify testability is shown above in Table 3. These examples may or may not include the presence of a modal verb denoted by links I and Ix examples may include should or will which may indicate what a system ought to do or quantify an action by a system. For example the requirement statement may be determined to be testable based on the presence of contiguous links e.g. S I O rule ID T.1 . The linkage S I O denotes that a subject link S should connect to a modal verb link I which in turn should connect to an object link O . In some configurations the requirement statement may be determined to be testable based on the presence of a combination of 8 links as shown in Table 3 i.e. links S I Ix P O OF Pv TP as specified in LG nomenclature in Table 3 . Other examples are possible.

A requirements sentence that the requirements testing system determines is testable may be analyzed further to generate test artifacts as discussed later. The requirements testing system may report a non testable sentence or statement may be reported and or may proceed to the next requirement sentence.

Once the logic determines that a requirements sentence is testable the logic may break down a compound requirements sentence into simple sentences or statements .

A compound sentence may be one which has conjunctions joining multiple nouns verbs or prepositions. The requirements testing system may break down these compound sentences into simple ones where each simple sentence has no conjunction.

As an example of sentence simplification consider the following sentence. This example and its simplification are shown in 

In the sentence 3 the compound sentence has two nouns administrator user identified in this example by italics that are joined by a conjunction and. The corresponding simple sentences for this compound sentence are 

The requirements testing system may record the conjunction and joining the two nouns as well. The simplification of the example is represented as Rule 1.4 in the above table and corresponds to Link Grammar Link SJ.

The link grammar parser logic may identify a noun conjunction with SJl and SJr links the labels SJl and SJr indicate Subject Join Left and Subject Join Right respectively . In this example the first simple sentence SENTENCE 1 is obtained by removing all words that are reachable only through the SJr link and the second simple sentence SENTENCE 2 is obtained by removing all words that are only reachable through SJl . In some instances by breaking the sentence into two the number agreement between the noun and the verb may be broken i.e. Noun Administrator and Verb are . In these instances the plural verbs may be converted by the requirements testing system into singular through the method of Stemming. The plural verb in the compound sentence and the corresponding singular verb in the simple sentences are underlined in sentences 3 and 4 respectively. In some instances one or more simple sentences generated from a compound sentence may have a large portion of text that is common.

Compound sentences or statements of other structures may also or alternatively be broken by the requirements testing system into simple sentences such as where the parsed output indicates multiple subjects links with label S . The link grammar parser logic may identify various cases of multiple subjects sentence openers coordinating conjunctions dependent clauses and causal sentences. These structures correspond to the rules provided in the table above. A causal sentence has an if then else type of structure and corresponds to rule ID S.1 in the table above. As an example of a causal sentence consider this sentence 

The requirements testing system may save the antecedent SENTENCE 1 and the consequent SENTENCE 2 as properties if and then respectively of the two simple sentences. These properties may be used in the generation of TCERs as discussed later. The link generated or otherwise denoted by the link grammar parser logic that corresponds to causal sentences is MVs Cs Rule ID S.1 .

Upon breaking up a sentence in certain cases the simplification may lead to a clause. As an example consider this sentence 

The properties binding the simplified sentences and clauses may be recorded to be used while generating TCERs.

The requirements testing system may represent the compound sentences as a tree in memory. The order of breaking up a sentence may be useful or important when a single compound sentence has multiple attributes of compoundness . In some instances the break up of compound statements into simplified sentences or statements may follow a specific order. For example one order of breaking up compound sentences may be a the coordinating conjunctions may be broken first then b the conjunctions may be broken next and c the causal attributes sentence openers and dependent clauses may be broken last. Other orders are possible.

When a compound sentence is broken up the compound sentence may be represented as a tree and each leaf of the tree may either be a simple sentence or a clause. shows an example of a tree generated by the requirements testing system illustrating how a compound sentence Shas been broken up into six simple sentences and . Logic in the requirements testing system may create the tree as follows or or 1 

Once the compound sentence has been broken down into simple sentences the logic may analyze the simple sentences in parallel or in various orders through one or more of three analysis processes including a first analysis process performed at and a second analysis process performed in block and or a third analysis process performed in block .

For example after the compound sentence has been broken down into simple sentences the method may proceed to block where the requirements testing system may create test scenarios or scenario descriptions from the simple sentences. Creation or generation of test scenarios may be performed for example by scenario generation module of the memory in the requirements testing system .

A test scenario also referred to as a scenario description or a test scenario description may be a short description of the test being conducted to verify a requirement. The scenario generation module may generate a single test scenario for each requirements sentence such that there may be a one to one mapping between a requirements sentence and a test scenario. Where the requirements testing system has simplified a given requirement sentence or other statement into multiple simple sentence the scenario creation module may generate a test scenario from or for each simple sentence. In some systems no test scenario is and or needs to be generated for clauses. The scenario generation module may generate the test scenarios for the simple sentences by analyzing the links generated from the output of the link grammar parser logic for the given simple sentence.

Table 14 below provides an example set of test scenario generation rule that the scenario generation module may apply or implement to generate a test scenario of a simple sentence.

The scenario generation module may apply the rules of Table 14 to the simple sentence of a requirements sentence to generate test scenarios for each of the simple sentences. After test scenarios have been generated such as using the rules in Table 14 for each of the simplified sentences the scenario generation module may create or obtain a test scenario of the original requirements sentence before simplification by combining the individual test scenarios of the simple sentences.

The requirements testing system may arrange a given requirements sentence that has been simplified in the form of a tree. The tree can have a single node corresponding to a requirement sentence being simple or have a depth of D.

An example of logic that the scenario generation module may implement to generate the test scenario such as a test scenario for an original requirements sentence from test scenarios generated for multiple simple statements of the original requirements sentence is 

The generation of the Scenario Description from the simple sentences as created from the rules of Table 14 is shown as Scenario . The corresponding output may be saved in a temporary object and is shown as SD . The operator joins the scenarios descriptions only if the descriptions are distinct.

The test scenarios generated by the scenario generation module using the rules in Table 14 and or logic an algorithm or code may be further illustrated with one or more examples. For example the sentence 1 above is a simple sentence Order Processing System should generate ACBG reports only for locations in Peru. which does not get broken into further sentences clauses. Thus the input to the scenario generation module would be the entire sentence as in sentence 1 . The scenario generation module would then generate a test scenario for this sentence 1 according or due to Rule S.2 of Table 14 which would be 

As another example the compound sentence in sentence 5 The user can proceed to the next screen if the entered password is correct. is simplified into two sentences corresponding to the if and then part respectively. As shown in line 14 of the logic used by the scenario generation module the test scenario is created only for the then part of the compound sentence. The corresponding test scenario for this sentence 5 generated according or due to Rule S.1 of Table 14 is thus 

As another example the compound sentence in sentence 7 Upon successful save the module must update the last save time. has an Opener and is simplified into a Clause and a Simple Sentence. As shown in line 14 of the logic used by the scenario generation module the test scenario is created only for the simple sentence and not for the Clause. Thus the test scenario for sentence 7 generated according or due to Rule S.2 of Table 14 is thus 

The requirements testing system may simplify the sentence 13 into two simple sentences and can be represented as a simplified compound sentence illustrated in .

The scenario generation module may determine or otherwise generate test scenarios for each simple sentence using the rules in Table 14 such as Rule S.2. The scenario generation module may then merge test scenarios for each simple sentence such as described in line 21 of the logic used by the scenario generation module .

Various other examples of generating test scenarios using the scenario generation module are possible.

After creating the test description from the simple sentences the logic may break the simple sentences of the requirements sentence into individual test intents .

Test intents may be the most atomic unit of a sentence that conveys enough information for a test to be made. As an example a Parts Of Speech tagger has its atomic unit as a word. A test intent may be the smallest contiguous set of words i.e. a phrase that carries sufficient meaning. A single requirements sentence or simple sentence may generate multiple test intents. These test intents when collated into a test condition and test sequence may correspond to the high level execution steps which a tester must perform.

The analysis logic may generate test intents for a sentence that has been simplified from or using the parsed output of link grammar parser logic . A test intent may be a set of words along a path of a link starting from the Subject of the sentence and bounded by noun phrases. The Subject of the sentence may in some instances tend to be the start of the sentence. An example of a generation of a test intent is shown below and in 

Referring to the analysis logic may determine that the first test intent begins from the subject the PRTS system and concludes at the occurrence of the noun phrase the reports . The second test intent begins from this point and proceeds till the next noun phrase is encountered the user . There are two paths that can be traversed from the verb selected . The second path from the noun phrase the reports till the touch screen may form the third test intent . The breaking of the sentence bounded by Noun Phrases may create a simple or the simplest form of English sentences sentences that have a Subject an Action and an Object or SAO . The structure of these SAO patterns may be identified in terms of the linkages created by the link grammar parser logic . The requirements testing system may insert static text at precise points in the phrase to bring meaning from a testing perspective. An example set of rules and the corresponding templates are shown Table 5a above.

Referring to statement 16 the analysis logic or another component of the requirements testing system may generate Test Intent 1 due to Rule I.1 see e.g. Table 5a . The S I O structure of Rule I.1 is shown in . This intent may be referred to as Primary since it includes the Subject of the sentence link grammar parser logic may indicate an S indicating that the subject is the PRTS system . Test Intent 2 and Test Intent 3 may be generated through Rule I.1.2 of Table 5a. The requirements testing system may incorporate the tense and the number for the test intents through were . The requirements testing system may identify these through the labels of the links Mv and Op respectively . The requirements testing system may refer to test intents that do not include the subject as Secondary .

Breaking a sentence into the individual components may remove structural ambiguity. As an example consider this sentence 

The sentence 17 is ambiguous because it can imply the selection is via the ACC module or the print is via the ACC module . The test intents for this example are

The analysis logic or another component of the requirements testing system may generate test intents for all simple sentences. In some systems the requirements testing system may not generate a specific test intent for clauses. In some of these systems the requirements testing system may treat a clause as a test intent. The requirements testing system may represent the generation of test intents in the form of notations below 2 3 

After a test scenario has been created by the requirements testing system and test intents have been created the method may proceed to block . In block test condition logic may group and arrange test intents into a test condition test hints and expected result. For example the test condition logic may be used in block to create a positive test condition corresponding test hints and an expected result from the test intents from . The test condition logic may generate a single positive test condition for each requirements sentence in .

An example of logic that the test condition logic may implement to create the positive test condition corresponding test sequence and expected result collectively referred to as positive TCERs and Test Sequence is 

The requirements testing system may for a given requirements sentence that has been simplified arrange the given requirements sentence in the form of a tree after being broken down which may have a single node corresponding to a requirement sentence being simple or have a depth of D. The requirements testing system may number every node of the tree and may create one or more TCER objects at each node. The requirements testing system may denote the TCER object as Obj. The TCER object may have three attributes the Test Condition the Test Hints and the Expected Results. These may be denoted as TC TS and ER respectively. The operator may in some instances combine the operands only if they are distinct i.e. the operation a b ab only if a b.

When executing the logic for generating the positive TCERs and test sequences the requirements testing system may operate as follows. Initially if the given requirements sentence is simple and thus the tree representation has a single node Line 4 of the positive TCERs and Test Hints logic shows the population of the Test Condition. The process of the creation of Test Condition is shown later. The Test Hints may be created using the Create Test Hints function see line 5 . The Create Test Hints function is explained later. The Expected Results may be created using the Create Expected Results function. see line 6 . This function is explained later in this disclosure.

The requirements testing system when executing the above logic may generate Pre Conditions by identifying data and associated conditions in sentences. Data may include numerals numbers written in text like four Boolean conditions like ON TRUE and date time. By conjoining data with the conditions like 

For requirements sentences which are compound in nature the simplification may result in a tree with at least 3 nodes and at least a depth of 2 . The requirements testing system may begin the analysis then at the level above the leaves i.e. if depth of the tree is D the analysis starts at level D 1 as shown in line 10 . When the sentence is broken due to dependency structure if then the requirements testing system may put test intents corresponding to the if part of the sentence into the Test Conditions see line 16 . The order in which a compound sentence is broken may ensure that the first child of the node is the if part of the sentence and the second child is the then part. The requirements testing system may then identify the Test Hints as the test intents of the then part line 17 . The requirements testing system may then identify the expected result from the complete simplified sentence. line 18 .

Certain sentences or statements may semantically imply an opposite of if then even though they carry the same syntactic structure. An example of such a sentence is 

Here the sentence 22 implies if the user is a Guest verify for not editable . This semantic understanding may be achieved by negating the action of the test intents lines 24 25 . The Negation may work by introducing a not before the Verb of the sentence.

For a compound sentence that has been broken due to the conjunction and the requirements testing system may identify the TCER as a combination of the test conditions and expected results of both the simple sentences. This may be similar to checking for both simple sentence 1 and simple sentence 2 together. The requirements testing system may merge the test intents shown by the operator . Some test intents may be an exact duplicate because of the way a compound sentence may be broken up. The duplication may arise because the simple sentences may be lexically common and the test intents are created from the common text. In such a case the merge operation performed by the requirements testing system when executing the logic may drop one of the duplicate test intent without any loss of information. The requirements testing system may identify the Test Condition in such a case as the merging of the Test Conditions line 31 . The requirements testing system may identify the Test Hints as the merge of the Test Hints of the simplified sentences line 33 . Similarly the expected result may be the merge of the Expected Results of the simplified sentences. line 34 .

A compound sentence with conjunction or may create individual TCERs each corresponding to the simple sentence created. This may be similar to having a tester check individually for each of the options specified through or . The requirements testing system may represent the operation as in lines 39 41 of the positive TCERs and Test Hints logic.

Having created Objects of TCERs using test intents the requirements testing system may merge the objects according to the way the sentence has been broken based on conjunctions and or . The requirements testing system when executing the positive TCERs and Test Hints logic may proceed to move from the depth above the leaves to the root line 51 . At this point all nodes may have one or more TCER objects created. The requirements testing system may again merge the nodes according to the logic of and or or . Because of the order in which a compound sentence is broken if then type of sentences may in some systems always occur at the last level and not appear at the current level of analysis. Finally the requirements testing system may display the TCER objects created at the root.

The Test Conditions generated at this point and with this logic may correspond to positive test conditions i.e. cases which verify the positive action of on the Subject.

The Create Test Condition logic is explained here. The test condition is obtained by the concatenation of the first test intent of the simplified sentence and subsequent intents provided certain conditions hold between the first and subsequent intents. The rules to create a test condition are shown in the table below. The Test Condition is obtained by merging the first intent with all secondary intents that satisfy the rules mentioned in the table below 

The Create Test Hints function is explained here. This function simply takes every Test Intent generated and adds the keyword Verify . These test intents with the appended keyword is then set as the Test Hints.

The Create Expected Results is explained here. The Expected Result is taken as the entire simplified sentence and the modal verb if present is dropped. In this case the verb following the modal verb is changed to present tense.

Once the positive test conditions test hints and expected results have been determined by requirements testing system in block the method may move to . The test condition logic may be executed to generate negative test conditions and associated test hints and expected results .

The test condition logic may make or generate negative test conditions by first looking at the Test Condition field of the TCER object created in block . If the Test Condition of the TCER object is populated due to a sentence of type dependency if then unless etc. or the presence modifiers such as only except then the test condition logic may create the negative test conditions and may generate Test Hints and Expected Results which correspond to this negative test condition.

In generating negative test conditions the requirements testing system may leverage the test condition logic to identify test data from a test data ruleset as described Boundary Value Analysis and the negation of sentences.

The identification of test data by the requirements testing system may include picking up the absolute number the units associated with the data and the data condition. This may be achieved by modifying the dictionary of the link grammar parser logic to annotate all data with an appropriate tag. Data may include numbers like 1 and textual numerals like one . Rules may also be developed to pick up the condition associated with the data i.e. 

The test condition logic may be executed by the requirements testing system to determine if negative TCERs can be generated using Boundary Value Analysis after having picked up the data condition and the units. The test condition logic may look to generate TCERs at the given data value i.e. data TCERs below the data value i.e. data . For example if the data in the requirement sentence is 10 .

The negation of the action specified in a sentence by the requirements testing system may also be useful or needed for the generation of TCERs. The requirements testing system may achieve the negation in a three step process. The first option is to negate any roles present in the sentence. Currently admin and administrator are considered roles. If such words occur then they are negated into non admin and non administrator respectively. If no roles are present then the presence of permissions is checked in the sentence. Permission includes words like having using and with . If any of these prepositions occur the negation is achieved by inserting a not before the preposition. If no permissions can be found in the sentence then negation is achieved by inverting the verb associated with the subject. If there are two contiguous verbs between the subject and the object the requirements testing system may insert a not between them. The requirements testing system may identify the verb by passing the given sentence or sentence fragment into the link grammar parser logic and checking the constituent tree output see and for examples of constituent tree output . For example the below sentence has two verbs 

In cases where there is a single verb and the verb is is are were etc. the requirements testing system may replace the verb with is not are not were not etc. This list of verbs may be limited and not endless and may be maintained as a semantic list. For other cases of single verbs the requirements testing system may convert the verb to singular using stemming a heuristic based approach which looks at the last few characters of words and replaces them to convert the word into singular . The singular verb may then be pre pended with does not . See for example 

The requirements testing system may generate the negative TCERs for TCERs identified as negative using Boundary Value Analysis and other non data TCERs with Test Conditions populated from causal sentences. An example of logic that the test condition logic may implement to create the negative test condition corresponding test sequence and expected result collectively referred to as negative TCERs and Test Sequence is 

The test condition logic may start the generation of negative TCERs by handling a requirements sentence that was initially compound and has been simplified at this stage. The TCER object may contain the Test Conditions from the Create Test Condition module and is shown in line 7 of the negative TCERs and Test Hints logic. The Test Hints is similarly generated from the Create Test Hints function. For a compound sentence of type dependency if then etc. the test condition logic may designate or determine the Test Conditions as in the case of the Positive TCER. Similarly for sentences of type until then etc. the test condition logic may not negate the Test Conditions unlike the case in the Positive TCER . The requirements testing system may identify the test hints as the entire test intents lines 14 .

The test condition logic may reverse the functioning of conjunctions or and and from that in the case of Positive TCERs. Here for the case of or the test condition logic may join test intents lines 19 20 . For the case of and or the test condition logic may keep the test intents distinct lines 25 34 . Once the TCER Objects are created at the level of Leaf 1 the test condition logic may collate along the tree representation of the sentence. The collation may be similar to the case of the Positive TCER with the difference being that the collation for or and and are reversed. For example for a conjunction and the test condition logic may keep the TCER objects distinct and may merge the objects for the case of or . This is shown in lines 113 149.

The operation when data is present is shown from lines 48 101. The logic for the various data conditions may be implemented which will give the negative test coverage. In cases where the test intent does not contain data the test condition logic may perform the negation of the Test Condition and the associated Test Hints and Expected Result lines 105 109 . A set of examples depicting the various aspects of this logic are provided in the Appendix A.

After the negative test conditions and associated test hints and expected result are generated the requirements testing system may publish or otherwise display the TCERs generated for the given requirements sentence in a report . For example the requirements testing system may publish the TCERs generated in an excel sheet. The requirements testing system may in some instances largely be based on the syntax of the sentence. Semantics may be needed at specific places such as a the User defined Entities although optional b the semantics of sub ordinate conjunctions if then unless etc. c the semantics of conjunctions and or and d the semantics of conditions associated with data 

Returning to where the method illustrated in may break down compound requirements sentence into simple sentences or statements along with performing the processes the method may also or alternatively proceed to and or the requirements testing system may perform the function in in parallel or at any time before during or after any of the processes . The requirements testing system may identify ambiguous phrases in the requirements sentence .

The identification of ambiguous phrases by the requirements testing system may be driven by a semantic list of phrases and a set of Links. When the words occurring with this given set of Links is not present in the list of phrases the requirements testing system may mark a word as ambiguous. Ambiguous phrases may often occur as adjectives or adverbs though not all adjectives or adverbs may be ambiguous. In some systems a set of words have been developed whose occurrence does not imply ambiguity. The ruleset for ambiguity is shown in the table below.

The requirements testing system may also or alternatively display the results of the identification of ambiguous phrases may also or alternatively in a created report in block . In some instances the requirements testing system may display results of the identification of ambiguous phrases with the results of the test scenarios test conditions test sequences and expected results. In other instances the results may be displayed separate. Other examples or variations are possible.

Returning to block along with performing the processes the method may also or alternatively proceed to in parallel or at any time before during or after any of the processes . The requirements testing system may classify a requirement sentence into one or more categories .

Categorization of a given requirements sentence into one or more pre defined categories by the requirements testing system may be driven through a set of Links and a glossary of terms. During certain categorizations the requirements testing system may look at the links and the type of nouns associated with them e.g. Person noun versus a system noun . Other categories may be decided based on the presence of terms in the glossary.

The requirements testing system or may also as described below leverage the categorization processing described in the document commenting analysis and reporting applications DARAs including U.S. Pat. Publication Nos. 2011 0022902 2010 0005386 and 2009 0138793 which are incorporated by reference in this document in their entireties.

The system may employ the entity glossary and the category keyword glossary from the DARAs or may employ customized glossaries including additional different or fewer glossary entries. In particular the entity glossary may be implemented as the agent glossary in the DARAs. An example NFR dictionary including a logging and security section is given above in Table 12.

For the security category the requirements testing system or may compare the requirement statement to the indicator phrases in the DARAs NFR glossary marked as security. For error handling the system may compare the requirement statement to the indicator phrases in the DARAs NFR glossary marked as logging disaster recovery DisasterRecoveryRequirements Recovery Time or any other phrases that indicate error handling.

As noted above the non functional requirement NFR statement specifies how a system should behave. What the behavior should be is captured in the functional requirement. The requirements testing system may compare the requirements sentence to the indicator phrases in the DARAs NFR glossary except those marked for security or error handling as noted above .

The requirements testing system may categorize a requirement statement as involving an inter module test as follows 

Then the requirements testing system may confirm that both the nouns are not actors and not persons. An example inter module test statement is shown in for the requirement statement The system should send the report to the xyz module. 

The requirements testing system may classify verbs as input output. For example the requirements testing system may regard send and click as outputs and receive as an input. The requirements testing system may then determine whether a person noun phrase occurs to the left of the verb or to the right of the verb. If the person noun phrase is to the left the requirements testing system may categorize the requirement sentence as an Input domain else as an Output domain. An example Input domain sentence is present in . An example output domain sentence is also present in .

The requirements testing system may determine that a requirement sentence is of the category Condition Dependency when the link grammar parser logic locates condition C structures in the requirement sentence. An example Condition Dependency statement is shown in .

The requirements testing system may determine that a requirement sentence is of the category Usability Conformance when the link grammar parser logic locates any of the keywords in the usability glossary or in the conformance glossary respectively in the requirement sentence. An example Usability Conformance statement is shown in .

The requirements testing system may also or alternatively display the results of the classification of the sentence into the categories in a created report in block . In some instances these results may be displayed by the requirements testing system with the results of the test scenarios test conditions test sequences and expected results. In other instances the results may be displayed separate. Other examples or variations are possible.

As noted the requirements testing system may create a report . shows an example of a user display screen generated and displayed by the requirements testing system the screen having a window showing a display of data and information provided by the requirements testing system . The window may include an Analyze tab . The Analyze tab may provide summary information about analysis performed by the requirements testing system . The requirements testing system may also or alternatively provide an Analyze Interactively tab which may allow the requirements testing system to analyze requirement sentences or other statements interactively. Business analysts or other users may use this feature to write a requirement sentence or other sentence execute the requirements testing system and review the results in window itself thereby enabling users to improve the requirements or statement definition process. shows an example of a report that may be generated by the requirements testing system such as in of the method in .

In some systems the requirements testing system may operate in two basic modes Analyze the entire document or analyze interactively. In the interactive mode the requirements testing system may enable receive a given sentence and output without generating any report. If the given requirement document has already been analyzed the Open Report will directly open the analyzed excel report. This Open Report may also be present as a separate button in the Ribbon. Many other examples of displays and reports generated by a requirements testing system are possible.

In some instances the requirements testing system may create or generate separate reports for each process or branch of the method. In other instances the requirements testing system may create one report for all of the processes or branches. In one example after the analysis of all the requirements sentence in a requirements document the requirements testing system may generate an excel report which may include the test description of a requirements sentence the Test Conditions Test Sequence and Expected Results for the requirements sentence and or the Category and Ambiguous phrases if any. The results of a single requirements sentence may also be seen in some systems by running the tool in interactive mode where an excel sheet is not created but the result is shown in the requirements document itself.

The report generated by the requirements testing system in may give a summary of analysis that includes the total requirements analyzed number of scenarios total number of TCERs and total number of test steps generated. The logic may also or alternatively categorize requirement sentences or statements in the report generated in into different categories for example as defined by IEEE standards . The category information may be used to identify gaps in requirements gathering and to fine tune the estimates. For example if security is an important consideration for the system and the number of security requirements is either zero or very few then the requirements gathering process may be incomplete. Similarly if there are many non functional requirements NFRs then the test estimates may need to budget for this. The report generated in may also or alternatively give the details of the analysis which includes sentences that could not be analyzed the ambiguous words in requirement sentences the category a requirement belongs to and or the requirement sentences that are Non Testable . The report may be automatically generated and or fields in the report may be filled automatically or manually. Other examples or reports and displays are possible.

The requirements testing system may be implemented as a plug in or add in into Microsoft word and Excel.

The requirements testing system has been tested on approximately 1600 functional requirements from various projects on different domains with the results achieved shown below in Table 16.

The requirements testing system may be configured to operate in an insert label mode. In this mode the requirements testing system may identify that a requirements document does not have labels for a requirements sentence. The requirements testing system may identify requirements for which labels are desired and then the requirements testing system may insert label for the sentences. In some systems the prompting of the insertion of the label may take place after the sentence has been identified but before the label has been applied such as where a user may identify or otherwise highlight the sentence and select or trigger the requirements testing system to insert a label for it.

The requirements testing system may be configured or operable to handle short forms like i.e. and e.g. . The requirements testing system may ignore images which may improve processing speeds. The requirements testing system may be configured or operable to account for and handle track changes in requirements document such as track changes in a Microsoft Word document. For example if the requirement document has many revisions and user has not accepted changes the requirements testing system may identify that and prompt a user or require a user to accept all the changes before analyzing the document. The requirements testing system may be configured or operable to handle table of contents and table of figures. The requirements testing system may skip the table of contents and table of figures for labeling functionality and may not analyze sentences in those sections.

The requirements testing system may be used by many different users businesses or entities such as Business Analysts and Test Engineers. Business Analysts may run a requirements testing system to ensure that the requirements captured are testable and cover all the required test categories. Test Engineers may use a requirements testing system to generate test design artifacts and estimate the testing effort based on the summary report generated by the requirements testing system . The requirements testing system may ensure completeness and reduce dependency on skilled and experienced resources during the test design phase. The requirements testing system may improve productivity and provide more fine grained information that helps in better testing effort estimation. Reports generated by the requirements testing system may help in traceability and audit. The requirements testing system may include various other advantages.

The methods systems and logic described above may be implemented in many different ways in many different combinations of hardware software or both hardware and software. For example the logic executed by the system may be circuitry in a controller a microprocessor or an application specific integrated circuit ASIC or may be implemented with discrete logic or a combination of other types of circuitry. The logic may be encoded or stored in a machine readable or computer readable medium such as a compact disc read only memory CDROM magnetic or optical disk flash memory random access memory RAM or read only memory ROM erasable programmable read only memory EPROM or other machine readable medium as for example instructions for execution by a processor controller or other processing device. Similarly the memory in the system may be volatile memory such as Dynamic Random Access Memory DRAM or Static Radom Access Memory SRAM or non volatile memory such as NAND Flash or other types of non volatile memory or may be combinations of different types of volatile and non volatile memory. When instructions implement the logic the instructions may be part of a single program separate programs implemented in an application programming interface API in libraries such as Dynamic Link Libraries DLLs or distributed across multiple memories and processors. The system may test input sentences other than requirement sentences.

While various embodiments have been described it will be apparent to those of ordinary skill in the art that many more embodiments and implementations are possible. For example a method for testing a requirement sentence may be provided. The method may include obtaining a requirement sentence and storing the requirement sentence in a memory. The method may further include submitting the requirement sentence to a grammatical parser executed by a processor to obtain parser outputs characterizing the requirement sentence. The method may further include applying a test artifact ruleset with the processor to the parser outputs to determine a test artifact applicable to the requirement sentence.

In some cases applying the test artifact ruleset includes applying a testability ruleset with the processor to the parser outputs to determine a test artifact that indicates whether the requirement sentence is testable. Additionally or alternatively it may be that applying the test artifact ruleset includes applying an ambiguity ruleset with the processor to the parser outputs to determine a test artifact that indicates whether the requirement sentence is ambiguous with respect to testability. Additionally or alternatively applying the test artifact ruleset may include applying an intent ruleset with the processor to the parser outputs to determine a test artifact that indicates an intent characteristic of the requirement sentence. Additionally or alternatively applying the test artifact ruleset may include applying a category ruleset with the processor to the parser outputs to determine a test artifact that indicates a category characteristic of the requirement sentence. Additionally or alternatively applying the test artifact ruleset may include applying a data ruleset with the processor to the parser outputs to determine a test artifact that indicates a data characteristic of the requirement sentence. Additionally or alternatively applying the test scenario ruleset can determine the test scenarios. Additionally or alternatively applying the test condition and expected results logic can generate the test conditions the expected results and the test hints. Additionally or alternatively the method may further include executing a pre processor on the requirement sentence prior to submitting the requirement sentence to the grammatical parser.

According to another aspect a computer program product including computer readable instructions may be provided. The instructions when loaded and executed on a computer system may cause the computer system to perform operations according to the steps aspect and or embodiments discussed above.

According to yet another aspect a requirement sentence analysis system may be provided. The system may include a processor and a memory in communication with the processor. The memory may include a requirement sentence and grammatical parser logic. The memory may further include analysis logic operable to when executed by the processor obtain the requirement sentence and store the requirement sentence in the memory. When executed the analysis logic may be further operable to submit the requirement sentence to the grammatical parser logic and obtain parser outputs characterizing the requirement sentence. The analysis logic may be further operable to apply a test artifact ruleset to the parser outputs to determine a test artifact applicable to the requirement sentence.

In some cases the test artifact ruleset may include a testability ruleset configured to determine as the test artifact whether the requirement sentence is testable. Additionally or alternatively the test artifact ruleset may include an ambiguity ruleset configured to determine as the test artifact whether the requirement sentence is ambiguous with regard to testability. Additionally or alternatively the test artifact ruleset may include an intent ruleset configured to determine as the test artifact an intent characteristic of the requirement sentence. Additionally or alternatively the test artifact ruleset may include a category ruleset configured to determine as the test artifact a category characteristic of the requirement sentence. Additionally or alternatively the test artifact ruleset may include a data ruleset with the processor to the parser outputs to determine a test artifact that indicates a data characteristic of the requirement sentence. Additionally or alternatively applying the test scenario ruleset can determine the test scenarios. Additionally or alternatively applying the test condition and expected results logic can generate the test conditions the expected results and the test hints. Also the analysis logic may be further operable to execute a pre processor on the requirement sentence prior to submitting the requirement sentence to the grammatical parser.

It should be understood that various modifications to the disclosed examples and embodiments may be made. In particular elements of one example may be combined and used in other examples to form new examples. Accordingly the implementations are not to be restricted except in light of the attached claims and their equivalents.

Req 4 When the employee roll off date from the project is less than 20 days the Reporting module may send email notification to the Project Manager with roll off information.

Req 6 Build the ABC solution identified during analyze phase as a contingency option on Number Ranges.

Various examples depicting the automatic generation of the Test Artifacts are provided. The examples range from a simple sentence to compound ones including data. The Generated Test Intents are shown as well.

