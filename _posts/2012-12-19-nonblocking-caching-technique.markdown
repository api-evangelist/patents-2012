---

title: Non-blocking caching technique
abstract: The described implementations relate to processing of electronic data. One implementation is manifested as a system that can include a cache module and at least one processing device configured to execute the cache module. The cache module can be configured to store data items in slots of a cache structure, receive a request for an individual data item that maps to an individual slot of the cache structure, and, when the individual slot of the cache structure is not available, return without further processing the request. For example, the request can be received from a calling application or thread that can proceed without blocking irrespective of whether the request is fulfilled by the cache module.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09223780&OS=09223780&RS=09223780
owner: Microsoft Technology Licensing, LLC
number: 09223780
owner_city: Redmond
owner_country: US
publication_date: 20121219
---
In technical fields such as computer science caching techniques are often used to provide relatively low latency access to data. For example hardware caches on modern computer processors store fixed amounts of data that can be accessed via data writes or reads very quickly. When the cache is not available e.g. data is not in the cache the cache or portion thereof is locked etc. relatively slower operations can be used instead. For example instead of using reading or writing to the cache operations such as reading or writing to main memory e.g. RAM or storage e.g. a hard drive can be performed instead.

The concept of caching can also be implemented in software. For example data structures such as hash tables linked lists or trees can be used to store computed data values so that future requests for these data values do not involve recomputing the data values. Instead of recomputing the data values lookup operations into a software cache can be used to retrieve the data from the software cache.

Various techniques have been used to ensure data consistency in hardware and software caches but these techniques tend to have certain drawbacks. For example some techniques use relatively heavyweight locking mechanisms to restrict access to a cache or portions thereof e.g. to ensure cache consistency when two threads attempt to access the same cache data. Other techniques may dedicate portions of a cache to a particular thread or process over an extended execution timeframe but these techniques often result in redundant caching of data.

The above listed example is intended to provide a quick reference to aid the reader and is not intended to define the scope of the concepts described herein.

This document relates to processing of electronic data. One implementation is manifested as a technique that can include receiving a lookup request to obtain a data item from a cache structure having a plurality of slots. The data item can map to an individual slot of the cache structure. The technique can also include performing an access check to determine whether the individual slot of the cache structure is available and in an instance when the individual slot is not available returning to the lookup request without checking whether the data item is stored in the individual slot of the cache structure.

Another implementation is manifested as one or more computer readable storage media comprising instructions which when executed by one or more processing devices cause the one or more processing devices to perform acts. The acts can include receiving an insertion request to insert a data item into a cache structure having a plurality of slots. The data item can map to an individual slot of the cache structure. The acts can also include performing an access check to determine whether the individual slot of the cache structure is available and in an instance when the slot is not available returning to the insertion request without inserting the data item into the cache structure.

Another implementation is manifested as a system that can include a cache module and at least one processing device configured to execute the cache module. The cache module can be configured to store data items in slots of a cache structure receive a request for an individual data item that maps to an individual slot of the cache structure and when the individual slot of the cache structure is not available return without further processing the request.

This discussion relates to electronic data processing and more particularly to caching of data. For the purpose of this document the term cache includes data storage techniques managed in software as well as in hardware. A cache can be used to store results of various operations e.g. computations queries etc. so that the results can be accessed with relatively low latency. The disclosed techniques can be implemented for caching of data used for local processing and can also be used in many different network scenarios such as mobile scenarios client server scenarios cloud storage etc.

For example some implementations may use a cache for local storage of data on a mobile device. The data may be accessible from a server via network communications but obtaining the data over the network may involve additional financial costs and or substantially higher latency compared to accessing a locally stored copy of the data. As another example a server may provide an application or service that requires a relatively substantial amount of computation. The server can store computational results in a cache so that subsequent requests for those results do not require recomputation. Some implementations may use multiple data storage techniques together e.g. a relatively high latency compressed hash table can be used to store computational results and some of the computational results can be stored in an uncompressed form in a lower latency cache.

Cache module can be configured to store images that are downloaded by the mobile device over the network e.g. responsive to insertion requests received from application . Once the user has downloaded the images initially the images may be stored locally on the device by the cache module. Now the user may reenter the same search request for college friends into the application at a later time perhaps the next day. The application can submit a corresponding retrieval request to the cache module and retrieve the images without performing another download from a relatively high latency cloud server. Thus as shown in instance the user is presented with the images more quickly than in instance after entering their search request. For example the user may not see the hourglass icon or the hourglass icon may be displayed for a shorter period of time than in instance .

Slot column identifies each location where data is stored in the cache structure . In this case each slot corresponds to a single row of a two dimensional array and the contents of cache structure for each slot are stored in KEY column VALUE column and IN USE column as discussed more below. In each slot is represented by a binary number that identifies the address of the slot e.g. the corresponding row of the two dimensional array. Note that slot column does not necessarily appear in the actual cache structure but rather can represent an address offset into the cache structure. In some implementations alternative data structures can be used e.g. separate arrays for each of KEY VALUE and IN USE columns respectively. In such implementations each row of the individual distinct arrays can correspond to a particular cache slot e.g. slot 00 refers to the first row of each array slot 01 refers to the second row of each array etc.

Generally speaking KEY column can include various identifiers for corresponding data items that are stored in the cache structure . The data items themselves can be stored in VALUE column . IN USE column can be used to restrict access e.g. provide exclusive access to individual slots of the cache structure on a slot by slot basis. For example the IN USE column can be provided in implementations where context switching can occur e.g. from one thread to another. Until writing reading operations to a given cache slot have completed that slot of the caching data structure may be unavailable for use by other threads. As discussed in more detail below some implementations may initialize the entire IN USE column to 0 and set IN USE column to 1 for a given row when data is being written to or read from that cache slot.

Thus some implementations may use a single bit in association with a given cache slot to represent whether the given cache slot is in use. Other implementations may use other levels of granularity for the IN USE column e.g. a word instead of a bit. Regardless of whether bits bytes words or other units of data are used for the IN USE column operations that modify the IN USE column can be atomic operations such as compare and swap operations that can be implemented using single instructions as discussed more below. In some architectures word level operations can be faster than bit level operations because word level operations may not involve any bit masking.

In the example given above the file name JenSmith.jpg is used as a key and this key maps to cache slot 10. Note that some implementations may use other types of keys e.g. a database primary key can be used such as an integer data type that identifies perhaps uniquely the file with the name JenSmith.jpg. Example techniques for mapping keys to individual cache slots are discussed in more detail below.

As indicated above the key for a particular data item can be determinative of where the data item is stored in cache structure . For example a mapping function can be applied to the key and output a value that identifies a particular slot of cache structure . In the example of the mapping function may return the slot 10 and thus the key JenSmith.jpg and the corresponding value file contents are stored in slot 10. In some implementations the slot number returned by the mapping function can be considered a logical offset into the cache structure. More generally the cache structure can be provided as a direct addressed hash table having a fixed number of slots N where N is 2 d and d is a number of bits. Thus in the example of N is 4 and d is 2 because there are four slots represented by two bits each.

As one example of a mapping function a hash function can be used to derive slot addresses from keys. For example the key JenSmith.jpg may hash directly to 10 if a hashing function that outputs two bit hash values is used. If the hashing function outputs longer bit strings a mapping function that uses a subset of the bit strings can be applied. For example the mapping function may use a certain number of the least significant bits from the result of the hash two bits in the examples of .

As mentioned above the output of the mapping function can be used as an offset into the caching data structure. To generalize the offset can be the lowest d bits of a hash of the key K using a hash function H. Some implementations can use the formula offset H k N 1 where is a bitwise AND operator and N is 2 d. In this implementation the operator masks the higher order bits of the return value of the hash function so that they are each set to 0.

Also note that some implementations may store a hash of the key in the cache structure instead of the key itself. For example if the key JenSmith.jpg hashes to 0010 0010 can be written to KEY column and used instead of the file name. In this example 0010 can be determined using the same hash function that determines the offset.

At block an insertion request is received. For example cache module may receive a request to cache a data item. In the example of scenario the request can be received from another module on mobile device such as application that is being used by the user to search for and download photos. In some implementations the insertion request is provided by a function call with one or more arguments. For example the arguments can include a key that identifies the data being inserted the data item itself etc.

At decision block an access check is performed to determine whether the data item will be inserted into the cache structure. For example cache module can perform a mapping function as discussed herein to identify a particular slot of the cache structure. The cache module can then check the IN USE column for identified slot. For example some implementations can perform a compare and swap operation on the IN USE column that compares the current value of the IN USE column to 0 and if 0 sets the value to 1.As mentioned operations used to modify the IN USE column can be atomic to ensure consistency in multi threaded implementations. For example in some implementations compare and swap operations are performed using a single machine instruction e.g. from an x instruction set or another instruction set. Example x instructions include CMPXCHGB and CMXCHGB word level instructions as well as lock bts or lock btc bit level instructions. If the access check did not succeed e.g. the compare operation indicated that IN USE had a value of 1 the method can return without further processing. For example the method can return to a calling application or thread that provided the insertion request. In some implementations insertion method does not provide an indication that the insertion has not been performed e.g. application may proceed as if the data had actually been inserted into cache structure . Note that this does not affect the correctness of the processing by the calling application. Rather as discussed more below when the data value is not added to the cache structure the data value can simply be obtained elsewhere e.g. by recomputation or other higher latency source when needed at a later time.

If the access check succeeds at block e.g. the compare and swap operation set the value of IN USE for the identified cache slot to 1 the method goes to block . At block the key is written to the KEY column of the cache structure at the identified cache slot. Next at block the data value is written to the VALUE column of the cache structure at the identified cache slot. As discussed more below subsequent requests to retrieve the data value from the cache structure can use the key to determine whether the data value is present in the cache.

Once the data value has been written to the cache insertion method can move to block where access control to the cache slot can be released. For example another compare and swap operation can be performed to set the IN USE bit back to 0 although some implementations may use non atomic operations as well and or omit the compare and simply write a value of 0. Next insertion method can go to block and return. Note that in some implementations blocks and can return to the calling thread routine identically. As previously mentioned there is not necessarily a return value that indicates whether the insertion request succeeded or failed. Rather as mentioned above proper program behavior does not depend on the success of the insertion operation.

At block a lookup request is received. For example cache module may receive from application a request to look up a particular data item such as the file JenSmith.jpg. In some implementations the request is provided by a function call with one or more arguments. For example the arguments can include an identifier of the data being written e.g. the key for the file JenSmith.jpg. As mentioned above the key can be an identifier of the file such as the file name a database key or other suitable identifier to distinguish the file from other data items in the cache structure.

At decision block an access check is performed to determine whether the appropriate cache slot for the data item is available. As discussed above cache module can perform a mapping function to identify the particular slot where JenSmith.jpg would be stored if present in the caching data structure. The access check can be performed for the slot using a compare and swap instruction or other atomic operation to ensure cache consistency. The compare and swap instruction can use a single machine instruction that checks whether the IN USE bit is set to 0 and if so sets the bit to 1.If the access check fails retrieval method can proceed to block where a sentinel value is returned from the method. The sentinel value can be a predetermined value e.g. NULL indicating that the requested data item is not available. Other techniques can be used besides returning a sentinel value e.g. setting a global flag modifying an argument passed by reference returning a Boolean FALSE value etc. Calling applications threads such as application may implement appropriate processing to handle this situation e.g. recomputing the requested data obtaining the data from a higher latency source e.g. download the picture again etc.

If the access check succeeds retrieval method can continue to decision block . At decision block a check is made to determine whether the data item is present in the cache structure. For example the cache slot that the key maps to can be checked to see whether the KEY column is equal to the key received as an argument. Continuing with the example above block can include checking whether the KEY column is equal to JenSmith.jpg. 

If there is agreement between the KEY column and the key argument the method can proceed to block where the item is obtained from the cache structure. For example the file contents of JenSmith.jpg can be retrieved from the cache structure. At block the contents can be returned e.g. to the calling application thread.

If instead there is no agreement between the key field in the cache structure and the received key argument this indicates that the data in the cache structure is not the data item requested by the caller or that the particular cache slot is empty. In this case the method proceeds to block and a sentinel value is returned.

In the following pseudocode examples the cache structure is referred to in pseudocode as an object C STRUCT having members corresponding to the columns of the cache structure. Specifically C STRUCT has a member key corresponding to KEY column a member value corresponding to VALUE column and can also have a member in use corresponding to IN USE column .

Insert routine includes a line with a function return type Void a function name Insert and two function arguments a key k of data type K and a value v of data type V. As mentioned above the data type of the key K can be an integer file name e.g. string data type or other data type suitable for distinguishing individual data items from one another. The value data type V depends on the type of data being stored in the cache structure e.g. in the examples discussed above V can be an image file type such as the contents of a .jpg file.

When Insert routine is called by application e.g. corresponding to block of insertion method the aforementioned arguments are received from the application. Next line performs a MAP function as mentioned above to determine an offset into cache structure . The offset can be stored in an integer variable slot that represents the particular cache slot where the data can be inserted.

Next at line the routine AcquireBit is called to determine whether the IN USE column can be acquired for the cache slot determined at line . For example AcquireBit can check C STRUCT slot .in use and if the bit is 0 set the bit to 1. The AcquireBit routine can be implemented as a routine that returns a Boolean value indicating whether the bit is acquired. Thus line be used to implement block of insertion method e.g. performing an access check. As previously mentioned AcquireBit can be implemented using a single instruction compare and swap to set IN USE to a value of 1.

If the IN USE column cannot be acquired e.g. the value of IN USE at the cache slot is already set to 1 the method returns without further processing. Thus the return logic in line can be used to implement block of insertion method .

If the IN USE column can be acquired e.g. the value of IN USE for the determined cache slot is set to 0 then processing proceeds as follows. As mentioned above the compare and swap instruction can be used so that the IN USE column is set to 1.Next processing proceeds to line where C STRUCT is populated with the key value K at the determined slot. Thus line can implement block of insertion method setting the key in the cache structure. Next processing proceeds to line where C STRUCT is populated with the value V at the determined slot. Thus line can implement block of insertion method setting the value in the cache structure.

Next at line the IN USE field is released e.g. set to 0 by the routine ReleaseBit. As discussed above with respect to the routine AcquireBit ReleaseBit can also be implemented using a single instruction that compares and swaps e.g. if IN USE is set to 1 it is set to 0 by a compare and swap operation or other operation that sets IN USE to 0. At this point other operations e.g. other threads can read from write to this slot of cache structure . Note also that ReleaseBit can be implemented as a void routine that does not return a value.

Lookup routine includes a line with a function return type V a function name Lookup and a function argument of type K. As discussed above with respect to Insert routine V is the data type of the data values that are inserted into the cache structure and K is the data type of the keys used to distinguish between different data values.

When Lookup routine is called by application e.g. corresponding to block of retrieval method the aforementioned arguments are received from the application. Next line performs a map function to populate an integer variable named slot as discussed previously e.g. with respect to line of Insert routine . The integer slot can be used to refer to a particular cache slot where the key k is located in cache structure if currently stored therein. Next a variable ret of type v is set to NULL as a sentinel value at line .

At line an AcquireBit operation can be performed as mentioned above in a first if statement that evaluates to either 1 or 0.A value of 1 indicates the IN USE bit at slot was 0 before AcquireBit was called and has been set to 1 and thus indicates that the determined cache slot is available for use by the caller. In other words the Boolean return value of AcquireBit indicates the compare and swap operation succeeded. A return value of 0 indicates that the IN USE bit at slot was 1 and the determined cache slot is not currently available e.g. the compare and swap operation did not succeed. Thus line can correspond to block of method e.g. performing an access check.

If the AcquireBit operation at line does not succeed lines are skipped and the NULL value is returned to the caller at line . The NULL value indicates that the requested value has not been retrieved from the cache structure. Thus when the variable ret has a value of NULL line can implement block of method e.g. returning a sentinel value.

If the AcquireBit operation at line does succeed line executes. Line includes a second if statement that compares the key in the cache structure C STRUCT slot .key to the input argument k. If there is a match the second if statement evaluates to true and otherwise evaluates to false. Thus line can implement block of method e.g. determining whether the key is in the cache structure.

If the second if statement evaluates to false a ReleaseBit operation is performed at line to set IN USE to 0. Since the variable ret has not been modified line will return a value of NULL again corresponding to block of method .

If the second if statement evaluates to true line obtains the value from the cache structure at the appropriate slot e.g. C STRUCT slot .value is set to the value of the variable ret. Setting the value of the variable ret at line can correspond to block of method e.g. obtaining the item from the cache structure.

Next line releases the IN USE bit at slot so that other operations e.g. other threads can read from or write to that slot of the cache structure . Next line returns the variable ret in response to the function call. Since the variable ret is populated with the requested data item at line line can be used to implement block of method in instances when line executes.

As previously mentioned some implementations may not use a concurrency control mechanism e.g. single threaded implementations may not necessarily benefit from concurrency control. In such implementations certain blocks of methods and and corresponding language of routines and can be omitted. In the case of insertion decision block and block are not performed and thus each insertion request can proceed to insert the data value into the cache structure . Likewise bolded underlined code in Insert routine is not performed e.g. lines and . For retrieval method decision block is not performed for similar reasons. Likewise bolded underlined code in Lookup routine e.g. lines and can be omitted in implementations that do not use concurrency control.

One reason that single threaded implementations may benefit from removing the aforementioned instructions is that these instructions may incur a slight penalty due to the compare and swap operations AcquireBit and ReleaseBit or equivalent word level operations . This is partly due to the direct overhead of executing the instructions themselves and also due to the fact that some processors may prohibit instruction reordering around compare and swap operations to ensure data consistency.

As previously mentioned simultaneous access to an individual cache slot by two threads can result in inconsistencies. Thus atomic read and write operations can be used to access the cache. Otherwise the cache can be corrupted by multiple threads trying to access the same cache slot concurrently.

For example if a first thread is reading from a cache slot while a second thread is writing to the cache slot the first thread may or may not read the data written by the second thread depending on the order in which individual instructions of the threads take place. As another example if the two threads are attempting to insert two different data items into the cache structure with keys that both map to the same cache slot it is possible for that cache slot to enter a state where the key written by the first thread is in the cache slot with the value written by the second thread. Thus the next thread to read the cache slot can retrieve a data value from the cache slot that does not match the key written at the cache slot.

As introduced above the disclosed implementations protect slots from concurrent access using the IN USE column which provides a bit array for each slot of the cache structure. Recall that a single compare and swap machine instruction can be used to determine whether a slot is in use and to lock the slot if the slot is not in use . This is a relatively lightweight approach in the sense that the bits themselves take up relatively little space. Furthermore the disclosed implementations can protect portions of the cache structure at a relatively fine level of granularity e.g. individual cache slots can be separately protected.

Some approaches to concurrency control may use blocking behavior where threads wait for locks before continuing processing. Note that the disclosed implementations can allow threads to implement non blocking behavior while still providing protected access to individual cache slots. Considering the examples set forth above note that insert requests can be ignored e.g. block of insertion method returns without inserting the data item when the access check fails. This in turn means that the calling thread can continue processing as if the data item had been inserted. As mentioned above this does not affect correctness of the operation of the calling thread or other threads . Rather subsequent attempts to retrieve the data item from the cache structure can simply proceed as if the data item were actually inserted and had been subsequently overwritten. In other words subsequent attempts to retrieve the data item may fail but this only means that the data items have to be recomputed or otherwise obtained from a higher latency source not that an incorrect value is used.

Also note that some implementations allow for a false negative. When a given slot is in use calls to read from that slot can return false irrespective of whether the key is actually in that cache slot. This is apparent from the path from decision block to block of retrieval method where the failure of the access check means the sentinel value of null is returned before the cache slot is even checked for the key. The worst case scenario here is that the calling thread simply recomputes the value or obtains the value from a higher latency source when the value was actually available in the cache structure but protected from access by the IN USE bit.

Thus in some disclosed implementations the calling thread does not need to block when the thread cannot obtain access control over a cache slot. This can be the case irrespective of whether the calling thread is reading or writing from the cache structure. If the thread is attempting to write to a cache slot that is currently being used by another thread the thread simply continues as if the data were actually written instead of blocking and waiting for the other thread to release a lock. Similarly if the thread is trying to read from an occupied cache slot the thread continues processing and simply obtains the value from a source other than the cache structure e.g. recompute higher latency source etc. and continues without blocking. Furthermore calling threads are freed from the overhead of managing locks for access control.

In some contexts the use of slot by slot contention detection as discussed herein can reduce the likelihood of actual contention for any individual slot. For example an implementation where a single lock is used for the entire cache is much more likely to have contention between any two threads because all cache accesses require the lock. Performing contention detection at finer levels of granularity can reduce the likelihood of contention e.g. if a cache is divided in half and each half has a lock it is less likely that two threads will concurrently try to access the same half of the cache. This is relevant to the present implementations because contention is generally reduced when cache slots are protected on an individual basis. Also note that increasing the cache size e.g. doubling also tends to reduce the likelihood for contention since there are more individually protected slots available for data.

As mentioned above some implementations allow for a false negative state where a read attempt fails because of access control even though the key provided in the read attempt is present in the cache. The expected false negative rate for a lookup can be computed using thread in cache cache time cache time noncache time False negative rate of threads 1 thread in cache number of slots in cache

Here P thread in cache represents the probability of a thread getting access control on any given read or write. Cache time represents the time for lookup or insert operations and noncache time represents the rest of the time the program is executing. Also note that some write requests are ignored and thus some recently computed data is not present in the cache. Both false negatives and ignored write requests can cause the hit rate for the cache to be somewhat lower than in a pure locking implementation where threads block and wait to complete their insert read operations. This lower hit rate can be a characteristic of implementations that exhibit cache recency. Cache recency describes a situation where data items that have been written to the cache structure more recently tend to be looked up more than older data items.

In the examples discussed previously data items were identified in the cache structure by identifiers such as the key itself or a hash of the key. Further implementations such as shown in may use a fingerprint to identify individual data items in the data structure. For example the fingerprint for a given data item can be part of the hash of the key.

Considering note that cache structure is shown as including a FINGERPRINT column instead of KEY column . The fingerprints for each data item can be obtained from the high order bits of a hash of the key. As discussed previously the low order bits can determine the cache slot where the data item is stored. shows the fingerprint for MarthaJones.jpg as 110 and thus the high order bits for the hash of the key e.g. file name database key etc. are 110. Since MarthaJones.jpg is stored in cache slot 00 it follows that 00 are the low order bits of the hash. Thus the complete hash for the key MarthaJones.jpg could be 11000 if the entire hash is used for the fingerprint and cache slot. Continuing with the examples in the complete hash for JenSmith.jpg could be 00110 and the complete hash for Martha and Jen.jpg could be 11111. Note however that this is one example of how bits of the hash can be used for the fingerprint and cache slot. In other implementations lower order bits can be used for the fingerprint and high order bits for the cache slot. Other variations are possible provided distinct e.g. mutually exclusive nonoverlapping bits are used for the cache slot and the cache fingerprint.

The fingerprint can be used in a manner similar to the key key hash in the examples discussed above for both insertion and retrieval operations. Considering insertion operations first assume a request is received to insert a file named DaveLong.jpg into the cache. Further assume that the key DaveLong.jpg hashes to 01111. Since the low order bits are 11 the cache module can check cache slot 11 to determine if this slot is available. If the access check succeeds the fingerprint 011 can be set in the cache structure at block as shown in .

Note also that Insert routine can be modified to determine the fingerprint and write the fingerprint to the fingerprint column at line . One example implementation could be to pass by reference an integer slot and another integer fingerprint to the map routine while passing the key by value e.g. void MAP K k int slot int fingerprint . The map routine can modify the integer variable slot to equal the low order bits of the hash of the key and can modify the integer variable fingerprint to equal the high order bits of the hash of the key. In such implementations a single computation of the hash can be used to populate both variables. The integer variable fingerprint can then be used at line e.g. C STRUCT slot .fingerprint fingerprint.

Now consider a retrieval operation for DaveLong.jpg using retrieval method . Blocks and of retrieval method can proceed as discussed above and if access control fails the method can return at block . If access control succeeds the fingerprint instead of the full key can be checked at decision block . If the fingerprint does not match the method returns at block . If the fingerprint does match DaveLong.jpg is retrieved from the cache and returned to the caller.

Lookup routine can likewise be modified to accommodate fingerprint implementations as follows. Map routine can be implemented as discussed above to modify an integer variable slot and another integer variable fingerprint. At line the second comparison operation can be rewritten as C STRUCT slot .fingerprint fingerprint.

Note that some implementations using fingerprints may result in false positives. Because the key itself is not checked directly there is some chance that two different data items mapping to the same cache slot can also have the same fingerprint. The likelihood of this occurring is related to the size of the fingerprint e.g. larger fingerprints tend to reduce the likelihood of two data items having both the same key and fingerprint. For a random hash function the probability of a false positive for a fingerprint of d bits can be represented as 1 2 d .

Considering the example above assume a retrieval request for Rex and Maggie.jpg is received. Further assume that the low order bits of the hash are 11 and the high order bits of the hash are 011.In this context Rex and Maggie.jpg is indistinguishable from DaveLong.jpg and DaveLong.jpg will be returned in response to the request.

Note also that some implementations may use hash functions that return more total bits than the number of bits used collectively by the fingerprint and slot address. For example an 8 bit hash function could be used where the full hash for Rex and Maggie.jpg is 01111 and the full hash for DaveLong.jpg is 01111 and the underlined bits are not part of hash or slot addressing scheme. Although the full hash values differ for the two files the false positive still occurs in this case because the distinguishing bits are not used for either slot addressing or the fingerprint. Also note that the map routine can be implemented by bit masking to obtain both the slot address and fingerprint for this example a logical AND operation with 11100000 followed by a 5 bit shift to the right can be used to obtain the fingerprint and a logical AND operation with 00000011 can be used to obtain the slot address.

Alternatively a modulus function can be used to determine the slot e.g. the hash mod the size of the hash table. Generally mod operations involve a division which can be slower than bitwise operations. Thus it can be useful to explicitly code the slot mask rather than relying on compiler optimization of a mod operation. Alternatively some implementations may rely on compiler optimization by providing the size of the cache structure to the compiler and the compiler can replace the mod operation with bitwise operations when the cache structure size is a power of 2.

In some implementations the number of bits dedicated to the slot address can be a function of the size of the cache. On the other hand for any given cache size arbitrary fingerprint sizes can be used depending on how important it is to minimize or reduce the number of false positives. Thus implementations that choose a relatively lightweight small fingerprint tend to accept more frequent false positives.

Some implementations may provide various facilities for creating and or configuring a cache in accordance with the disclosed implementations. illustrates a graphical user interface GUI that allows a user to select a type of data structure to use for caching data. For example GUI can be presented to a developer that wishes to create a cache for a particular application e.g. a development environment for creating the aforementioned imaging application or for an individual configuring a remote caching service. Here the developer is given options of configuring the application including non blocking cache option a chained hash table option a linked list option or a tree option . In the developer has selected non blocking cache option .

The developer is also given the option to configure whether to use the actual key values e.g. file names database keys etc. or hashes of key values by selecting either the use actual key option or the use hash of key option . Here the developer has chosen to use hashes of key values in the cache structure so the developer is also given the choice to use a fingerprint and set a fingerprint size via use fingerprint option and fingerprint size option . Here the developer has opted to configure the cache to use an 8 bit fingerprint.

Note that GUIs and as presented above are exemplary in nature and various other forms of interfaces both graphical and non graphical can be used to configure cache processing in accordance with the disclosed implementations. As another example some implementations can provide application programming interfaces APIs that define functions for creating cache structures and configuring cache functionality as set forth herein. For example a function to create a cache structure can include parameters to set the size of the cache size of the fingerprint if any hash function to use e.g. a particular technique and or size of the hash etc.

Caching strategies can be implementation dependent e.g. certain strategies may be more or less appropriate depending on the specific environment in which a cache is implemented. Compare a first system where a cache miss results in a local in memory recomputation that takes on average twice as long as reading from the cache. Now consider a second system where a cache miss results in a disk access followed by a network send operation a remote computation server latency and a network receive operation where these operations collectively take on average 100 times longer than reading from the cache. The first system may be more well suited for the disclosed implementations because of the relatively low penalty for the cache miss. This is because the false negative lookup scenario mentioned previously has relatively less impact in the first system than in the second system. In the second system it may be worthwhile to allow threads to block and wait to read from the cache rather than perform the disc network operations. In the first system it may be more efficient for the thread to continue processing and go ahead with the recomputation.

Another characteristic of systems that may benefit from the disclosed implementations include systems where false negative rates are expected to be relatively low. Consider an example with a data space of 2000 distinct values where 90 of the reads writes in the system involve a subset of only 700 of these values. In other words the distribution of data has a relatively high concentration of 700 values that get looked up frequently and a relatively long tail of 1300 values that get looked up infrequently. A cache with 1 k cache slots e.g. 1028 is large enough to encompass all of the high frequency values. Thus the percentage of false negatives is likely to be relatively low because of the distribution of the data and the size of the cache. Note that hash functions can effectively randomize the storage locations of data items in the cache which helps reduce the likelihood that two threads will concurrently ask for the same cache slot. That said particular data items with very high frequency of access are more likely to result in contention for an individual cache slot.

Cache structure sizing is another relevant design consideration. As suggested above it can be helpful to size the cache structure such that most or all data items that exhibit high access frequency can fit into the cache structure. Furthermore note that hardware considerations can also come into play when considering cache sizing. Generally it can be helpful to size the cache structure so that the entire cache structure keys fingerprints data access bit can fit into a lower latency layer of a given memory hierarchy. Thus for example it may be useful to size the cache structure to fit into an L2 cache on a given processor instead of providing a larger cache structure that would need to be partially stored in an L3 cache or in RAM. This can reduce the number of L3 or RAM access in the system. This concept is extensible to other layers of a memory hierarchy depending on the specific implementation e.g. sizing a cache structure to fit into RAM to avoid needing to perform disk accesses etc. Also note that there can be some benefit to sizing the individual cache slots so that a single cache slot can be read as a single memory page because it can be relatively inefficient to use multiple paging disk I O operations to populate a single cache slot.

To the extent that additional low latency memory is available it can be worthwhile to increase the number of slots of the cache structure e.g. as long as increasing the size will not cause the cache to spill over into a slower level of the memory hierarchy. This is because doubling the number of cache slots allows the key space to double as well which in turn reduces the likelihood of a false negative. To the extent that memory is available this may be a worthwhile trade off relative to other implementations such as blocking or thread local caching.

Also note that the disclosed implementations may provide relatively consistent time cache lookup operations or even performance guarantees. This is true because of the nonblocking behavior e.g. when a cache read is performed the operation returns to the calling thread even if access control cannot be obtained. In contrast blocking implementations often tend to exhibit inconsistent cache lookup times because thread blocking times can vary greatly from one cache operation to another. The disclosed implementations can also be distinguished from techniques that create variable sized hash tables by chaining data items together e.g. when multiple items are added that map to a single location in the cache. This in turn can result in variable cache seek times whereas the disclosed implementations may provide relatively consistent cache seek times e.g. by storing only a single data item in each cache slot. Thus the disclosed implementations may be suitable for situations where bounded times for caching operations are desirable e.g. real time processing.

One issue to keep in mind in designing a cache using the disclosed implementations is that variable sized data values can result in cache fragmentation. To the extent there are reasonable bounds on the data size or the size of the cache is not constrained by available memory or hardware cache using the caching techniques disclosed herein will result in unreasonable amounts of cache fragmentation. For example in scenario if each of the images is of a relatively similar size e.g. between 30 and 40 Kb fragmentation is not that wasteful because each cache slot can be sized to accommodate 40 Kb worth of data for a worst case of 10 Kb of wasted memory per cache slot. However if images range from 10 Kb to 1 000 Kb it may not be reasonable to adopt this approach e.g. the worst case here is 990 Kb of wasted memory in a single cache slot e.g. 99 . In implementations with variable data sizes it can be useful to implement a level of indirection by caching a memory offset and length that identifies where the requested data is stored in the cache structure instead of directly storing the data in the cache structure. This allows for uniform sizing of the items in the cache structure and alleviates the fragmentation issue.

Another design consideration relates to what size data units are used for the IN USE column of the cache structure . As mentioned word level operations can be faster in some architectures than bit level operations. On the other hand using relatively fewer bits for the IN USE column can result in a lower memory footprint for the cache structure. When the data items are relatively large the IN USE column is likely to represent a relatively small portion of the overall size of the cache structure and thus using words for access control may be appropriate. Conversely if the data items are relatively small the IN USE column is a larger portion of the overall size of the cache structure and using individual bits for access control may be more appropriate. This decision can also be informed by the extent to which using relatively smaller data units for access control influences whether the cache structure will fit into lower latency levels of a memory hierarchy e.g. an L2 cache as opposed to main memory.

System can also include one or more server s . Server can be a computing device that also includes a processor memory storage cache module cache structure and chained hash table . Note the suffix is used to indicate an occurrence of a given component on server . Certain processing discussed above with respect to can also be performed by cache module resident on server .

Client device s and server can communicate over one or more networks such as but not limited to the Internet. Cache module can be implemented as software hardware and or firmware. Processor s can execute computer readable instructions to provide any of the functionality discussed herein e.g. methods and or and associated processing. Data and or computer readable instructions can be stored on memory and or storage . The memory and storage can include any one or more of volatile or non volatile memory hard drives and or optical storage devices e.g. CDs DVDs etc. among others.

Client device s and server can also be configured to receive and or generate computer readable instructions from an external storage . Examples of external storage can include optical storage devices e.g. CDs DVDs etc. hard drives and flash storage devices e.g. memory sticks or memory cards among others. In some cases the cache module discussed herein can be installed on the client devices or server during assembly or at least prior to delivery to the user. In other scenarios cache module can be installed by the user after delivery such as a download available over network and or from external storage . Cache module can be manifest as a freestanding or service an application module and or part of the computing device s operating system.

The cache module discussed herein can achieve the functionality described above relative to . Specifically cache module can be configured to perform methods and or and the associated processing described herein. From one perspective the cache module can be viewed as ignoring certain requests provided by calling applications or threads. For example the cache module can ignore an insertion request by not fulfilling the insertion request e.g. when the appropriate slot is not available. Similarly the cache module can ignore a lookup request by not fulfilling the lookup request when the appropriate slot is not available. As a consequence the calling application or thread can proceed without blocking irrespective of whether the cache module fulfilled a particular request.

It is worth noting that in some instances the client devices or servers can comprise multiple computing devices or machines such as in a distributed environment. In such a configuration methods and or can be implemented using distributed processing across the multiple computing devices or machines.

The terms computer client device server and computing device as used herein can mean any type of device that has some amount of processing capability and or storage capability. Generally a mobile device refers to a computing device embodied in a form factor suitable for a user to carry on their person. A computing device can obtain computer readable instructions that are stored on storage and or memory devices. The storage and or memory devices can be internal and or external to the computing device. The storage and or memory devices can include any one or more of volatile or non volatile memory hard drives flash storage devices and or optical storage devices e.g. CDs DVDs etc. among others. Also note that the term system can refer to a single device or multiple devices. Such devices can include various input output mechanisms such as keyboards displays touchscreens e.g. typing and or gesture inputs voice activated interfaces etc.

As used herein the term computer readable media can include signals. In contrast the term computer readable storage media excludes pure signals. Computer readable storage media includes computer readable storage devices. Examples of computer readable storage devices include volatile storage media such as RAM and non volatile storage media such as hard drives optical discs and flash memory among others.

In addition to the illustrated configurations of computing devices consistent with the disclosed implementations can employ a system on a chip SOC type design. In such a case functionality provided by the computer can be integrated on a single SOC or multiple coupled SOCs. One or more processors can be configured to coordinate with shared resources such as memory storage etc. and or one or more dedicated resources such as hardware blocks configured to perform certain specific functionality. Thus the term processor as used herein can also refer to controllers microcontrollers processor cores or other types of processing devices suitable for implementation both in conventional computing architectures as well as SOC designs.

Consider an implementation where application on client devices uses a very large set of data e.g. 100 gigabytes of data items. For the purposes of the following discussion application can be a natural language processing application that uses probabilities of certain n gram strings appearing in a given document. Each time application requests a probability for a particular n gram a corresponding service thread is initiated on server to honor the request.

Chained hash table can include keys for all of the n gram strings and corresponding probabilities and a subset of keys or fingerprints and corresponding probabilities can be stored in cache structure by cache module . Each service thread can first attempt to retrieve a requested n gram probability pair from the cache module via a lookup request. If this fails the service thread can obtain the requested n gram probability pair from the chained hash table and then execute an insertion request to insert the requested n gram probability pair into the cache structure.

To motivate this example chained hash table may be a variable sized hash table that uses substantial chaining of probabilities in an individual cache location. This can result in uncertain seek times because any given request is mapped to a particular cache location and then may have to iterate over multiple probabilities stored at that cache location. Furthermore the chained hash table may also be stored in a compressed form that uses some processing to obtain usable data.

Now assume application on client device wishes to obtain a probability of the n gram the quick brown fox in a particular natural language context e.g. for translation purposes.

Furthermore assume cache structure is initialized all IN USE bits are 0 and empty no key value pairs . In this first instance application can send a request to server . A corresponding service thread is initiated that first makes a lookup call to cache module . Since the cache structure is empty the service thread will receive a NULL or other sentinel value indicating the probability is not available in cache structure . Next the service thread can retrieve the probability from chained hash table . Once the probability is obtained the service thread can call an insert routine to insert the probability into cache structure for future use.

Over time cache structure can eventually become populated with various n grams and probabilities as the values are looked up and inserted by service threads that are responding to received requests from various client devices. In some implementations server may use a different thread for each individual client request so that at any given time there may be multiple concurrent requests from a particular client device or from different client devices. Cache module may implement the aforementioned access control mechanisms to ensure consistency of the data in cache structure .

As discussed previously cache module can ignore insert requests that are received for a slot that is currently in use. For example assume a first service thread has requested to insert an n gram probability pair for the purple elephant and this phrase maps to the same cache slot as the quick brown fox. Now while cache module is writing the purple elephant and corresponding probability to cache structure another insertion request is received from a second service thread to insert the quick brown fox and corresponding probability. Since the insert operation for the purple elephant has not finished IN USE is still cache module can simply return the second service thread if the quick brown fox has been inserted. Thus the second service thread does not need to block and wait for the insertion request to proceed. Indeed the second service thread can proceed immediately with further processing after sending the insertion request and does not need to wait for a return value from the insertion request indicating that the insertion request for the quick brown fox has failed. This is also true for the first service thread e.g. the first service thread can proceed without waiting for a return value indicating the insertion request for the purple elephant was successful.

As also discussed previously some implementations allow for a false negative. Consider a situation where the quick brown fox and corresponding probability are present in cache structure . Assume a first service thread attempts to lookup the quick brown fox and immediately thereafter a second service thread also attempts to lookup the quick brown fox. If the first lookup operation has not completed e.g. IN USE for the cache slot is set to 1 when the second service thread attempts the lookup the second service thread receives a sentinel value indicating the probability cannot be retrieved. Thus the second service thread can proceed to access the chained hash table even though the requested probability was actually available in cache structure .

Note that further implementations may use various multi tiered caching techniques in accordance with the disclosed implementations. For example some implementations may provide cache structure and a chained hash table together on an individual client device. Other implementations may use a relatively small cache structure on an individual client device in conjunction with a larger cache structure on server . In further implementations a given cache structure and associated cache module functionality can be distributed across multiple devices e.g. in a peer to peer network.

The order in which the example methods are described is not intended to be construed as a limitation and any number of the described blocks or steps can be combined in any order to implement the methods or alternate methods. Furthermore the methods can be implemented in any suitable hardware software firmware or combination thereof such that a computing device can implement the methods. In one case the methods are stored on one or more computer readable storage media as a set of instructions such that execution by one or more computing devices causes the one or more computing devices to perform the method.

Although techniques methods devices systems etc. discussed herein are described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described. Rather the specific features and acts are disclosed as exemplary forms of implementing the claimed methods devices systems etc.

