---

title: Use of predefined block pointers to reduce duplicate storage of certain data in a storage subsystem of a storage server
abstract: A method and system for eliminating the redundant allocation and deallocation of special data on disk, wherein the redundant allocation and deallocation of special data on disk is eliminated by providing an innovate technique for specially allocating special data of a storage system. Specially allocated data is data that is pre-allocated on disk and stored in memory of the storage system. “Special data” may include any pre-decided data, one or more portions of data that exceed a pre-defined sharing threshold, and/or one or more portions of data that have been identified by a user as special. For example, in some embodiments, a zero-filled data block is specially allocated by a storage system. As another example, in some embodiments, a data block whose contents correspond to a particular type document header is specially allocated.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08892527&OS=08892527&RS=08892527
owner: NetApp, Inc.
number: 08892527
owner_city: Sunnyvale
owner_country: US
publication_date: 20120914
---
This application is a divisional of U.S. patent application Ser. No. 12 394 002 filed Feb. 26 2009 entitled USE OF PREDEFINED BLOCK POINTERS TO REDUCE DUPLICATE STORAGE OF CERTAIN DATA IN A STORAGE SUBSYSTEM OF A STORAGE SERVER and naming inventors Sandeep Yadav and Subramanian Periyagaram which application is hereby incorporated by reference in its entirety.

At least one embodiment of the present invention pertains to storage systems and more particularly to a method and system for specially allocating data within a storage system when the data matches data previously designated as special data.

In a large file system it is common to find duplicate occurrences of individual blocks of data. Duplication of data blocks may occur when for example two or more files or other data containers share common data or where a given set of data occurs at multiple places within a given file. Duplication of data blocks results in inefficient use of storage space by storing the identical data in a plurality of different locations served by a storage system.

A technique commonly referred to as deduplication that has been used to address this problem involves detecting duplicate data blocks by computing a hash value fingerprint of each new data block that is stored on disk and then comparing the new fingerprint to fingerprints of previously stored blocks. When the fingerprint is identical to that of a previously stored block the deduplication process determines that there is a high degree of probability that the new block is identical to the previously stored block. The deduplication process then compares the contents of the data blocks with identical fingerprints to verify that they are in fact identical. In such a case the block pointer to the recently stored duplicate data block is replaced with a pointer to the previously stored data block and the duplicate data block is deallocated thereby reducing storage resource consumption.

Deduplication processes assume that all data blocks have a similar probability of being shared. However this assumption does not hold true in certain applications. For example this assumption does not often hold true in virtualization environments where a single physical storage server is partitioned into multiple virtual machines. Typically when a user creates an instance of a virtual machine the user is given the option to specify the size of a virtual disk that is associated with the virtual machine. Upon creation the virtual disk image file is initialized with all zeros. When the host system includes a deduplication process such as the technique described above the zero filled blocks of the virtual disk image file may be fingerprinted and identified as duplicate blocks. The duplicate blocks are then deallocated and replaced with a block pointer to a single instance of the block on disk. As a result the virtual disk image file consumes less space on the host disk.

However there are disadvantages associated with a single instance of a block on disk being shared by a number of deallocated blocks. One disadvantage is that hot spots may occur on the host disk as a result of the file system frequently accessing the single instance of the data. This may occur with high frequency due to the fact that the majority of the free space on the virtual disk references the single zero filled block. To reduce hot spots some deduplication processes include a provision for predefining a maximum number of shared block references e.g. 255 . When such a provision is implemented the first 255 duplicate blocks reference a first instance the shared block the second 255 duplicate blocks reference a second instance and so on.

Another disadvantage of deduplication is disk fragmentation. Disk fragmentation may occur as a consequence of the duplicate blocks being first allocated and then later deallocated by the deduplication process. Moreover the redundant allocation and deallocation of duplicate blocks further results in unnecessary processing time and bookkeeping overhead.

The technology introduced herein eliminates the redundant allocation and deallocation of special data on disk by providing a technique for specially allocating data within a storage system. As used herein special data is one or more pieces of data that have been designated as special within a host file system such as one or more pieces of data that exceed a pre defined sharing threshold. For example in some embodiments a zero filled data block is specially allocated by a storage system. As another example in some embodiments a data block whose contents correspond to a particular type of document header is specially allocated. It is noted that the technology introduced herein can be applied to specially allocate any type of data. As such references to particular data such as zero filled data should not be taken as restrictive. It is further noted that the term disk is used herein to refer to any computer readable storage medium including volatile nonvolatile removable and non removable media or any combination of such media devices that are capable of storing information such as computer readable instructions data structures program modules or other data. It is also noted that the term disk may refer to physical or virtualized computer readable storage media.

As described herein a specially allocated data block is a block of data that is pre allocated on disk or another non volatile mass storage device of a storage system. It is noted that the term pre allocated is used herein to indicate that a specially allocated data block is stored on disk prior to a request to write the special data to disk prior to operation of the storage system and or set via a configuration parameter prior to or during operation of the storage system. That is the storage system may be pre configured to include one or more blocks of special data. In some embodiments a single instance of the special data is pre allocated on disk. When a request to write data matching the special data is received the storage system does not write the received data to disk. Instead the received data is assigned a special pointer that identifies the location on disk at which the special data was pre allocated. In some embodiments the storage manager maintains a data structure or other mapping of specially allocated data blocks and their corresponding special pointers. When a request to read specially allocated data is received the storage system recognizes the special pointer as corresponding to specially allocated data and instead of issuing a request to read the data from disk the storage system reads the data from memory e.g. RAM .

By accessing special data in memory e.g. RAM of the storage system requests for such data can be responded to substantially faster because the special data may be read without accessing the disk. Moreover by not issuing write requests to store special data on disk the technology introduced herein avoids disk fragmentation caused by freeing duplicate data blocks. Also by not issuing write requests to store special data on disk the technology introduced herein substantially reduces the processing time and overhead associated with deduplicating duplicate data blocks. In addition by not issuing requests to access special data exceeding a pre defined sharing threshold on disk the technology introduced herein eliminates hot spots associated with reading a single instance of a deduplicated data block.

The technology introduced herein can be implemented in accordance with a variety of storage architectures including but not limited to a network attached storage NAS configuration a storage area network SAN configuration a multi protocol storage system or a disk assembly directly attached to a client or host computer referred to as a direct attached storage DAS for example. The storage system may include one or more storage devices and information stored on the storage devices may include structured semi structured and unstructured data. The storage system includes a storage operating system that implements a storage manager such as a file system which provides a structuring of data and metadata that enables reading writing of data on the storage devices of the storage system. It is noted that the term file system as used herein does not imply that the data must be in the form of files per se.

Each file maintained by the storage system is represented by a tree structure of data and metadata the root of which is an inode. Each file has an inode within an inode file or container in embodiments in which the storage system supports flexible volumes and each file is represented by one or more indirect blocks. The inode of a file is a metadata container that includes various items of information about that file including the file size ownership last modified time date and the location of each indirect block of the file. Each indirect block includes a number of entries. Each entry in an indirect block contains a volume block number VBN or physical volume block number PVBN virtual volume block number VVBN pair in embodiments in which the storage system supports flexible volumes and each entry can be located using a file block number FBN given in a data access request. The FBNs are index values which represent sequentially all of the blocks that make up the data represented by an indirect block. An FBN represents the logical position of the block within a file. Each VBN is a pointer to the physical location at which the corresponding FBN is stored on disk. In embodiments in which the storage system supports flexible volumes a VVBN identifies an FBN location within the file and the file system uses the indirect blocks of the container file to translate the FBN into a PVBN location within a physical volume.

When a write request e.g. block access or file access is received by the storage system the data is saved temporarily as a number of fixed size blocks in a buffer cache e.g. RAM . At some later point the data blocks are written to disk or other non volatile mass storage device for example during an event called a consistency point. In some embodiments prior to the data blocks being written to disk the technology introduced herein examines the contents of each queued data block to determine whether the contents of the data block correspond to special data. When the contents of a data block correspond to data that has been previously identified as special data e.g. a zero filled data a special VBN pointer or VVBN PVBN pair is assigned to the corresponding indirect block of the file to signify that the data is specially allocated. That is in response to a write request the storage system determines whether the file or block includes specially allocated data and if so the storage system assigns a corresponding special pointer value to the indirect blocks of the file to identify the locations of the data that have been pre allocated on disk. For example if a data block corresponds to a special zero filled block the corresponding VBN pointer in the level 1 indirect block may be assigned a special VBN e.g. VBN 0 thereby signifying that the zero filled data block is specially allocated on disk. As introduced herein data blocks containing special data are removed from the buffer cache so that they are not written to disk. By not issuing write requests to store special data on disk the technology introduced herein avoids disk fragmentation caused by freeing duplicate data blocks. Also by not issuing write requests to store special data on disk the technology introduced herein substantially reduces the processing time and overhead associated with deduplicating duplicate data blocks.

In some embodiments described herein one or more of VBN pointers are defined each signifying that a data block referenced by the pointer contains special data that is specially allocated on disk. For example the storage system may define a special VBN pointer labeled VBN ZERO which signifies that any data block referenced by the pointer is zero filled and that the zero filled data is pre allocated on disk. As another example the storage system may define a special VBN pointer or PVBN VVBN pair labeled VBN HEADER which signifies any block whose contents correspond to a particular document header type and that the contents of the particular document header are pre allocated on disk. When a read request is received by the storage system the storage system determines whether the corresponding block pointer to the file or block matches a special pointer e.g. VBN ZERO and if so the storage system reads the specially allocated data block from memory rather than retrieving the data from disk . By accessing special data in memory e.g. RAM of the storage system requests for such data can be responded to substantially faster because the special data may be read without accessing the disk. In addition by not issuing requests to access special data exceeding a pre defined sharing threshold on disk the technology introduced herein eliminates hot spots associated with reading a single instance of a deduplicated data block.

Before considering the technology introduced herein in greater detail it is useful to consider an environment in which the technology can be implemented. is a data flow diagram that illustrates various components or services that are part of a storage network. A storage server is connected to a non volatile storage subsystem which includes multiple mass storage devices and to a number of clients through a network such as the Internet or a local area network LAN . The storage server may be a file server used in a NAS mode a block based server such as used in a storage area network SAN or a server that can operate in both NAS and SAN modes. The storage server provides storage services relating to the organization of information on storage devices e.g. disks of the storage subsystem .

The clients may be for example a personal computer PC workstation server etc. A client may request the services of the storage server and the system may return the results of the services requested by the client by exchanging packets of information over the network . The client may issue a request using a file based access protocol such as the Common Internet File System CIFS protocol or Network File System NFS protocol over TCP IP when accessing information in the form of files and directories. Alternatively the client may issue a request using a block based access protocol such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP when accessing information in the form of blocks.

The storage subsystem is managed by the storage server . The storage server receives and responds to various transaction requests e.g. read write etc. from the clients directed to data stored or to be stored in the storage subsystem . The mass storage devices in the storage subsystem may be for example magnetic disks optical disks such as CD ROM or DVD based storage magneto optical MO storage or any other type of non volatile storage devices suitable for storing large quantities of data. Such data storage on the storage subsystem may be implemented as one or more storage volumes that comprise a collection of physical storage devices e.g. disks cooperating to define an overall logical arrangement of volume block number VBN space on the volumes. Each logical volume is generally although not necessarily associated with a single file system. The storage devices within a volume are may be organized as one or more groups and each group can be organized as a Redundant Array of Inexpensive Disks RAID in which case the storage server accesses the storage subsystem using one or more well known RAID protocols. However other implementations and or protocols may be used to organize the storage devices of storage subsystem .

In some embodiments the technology introduced herein is implemented in the storage server or in other devices. For example the technology can be adapted for use in other types of storage systems that provide clients with access to stored data or processing systems other than storage servers. While various embodiments are described in terms of the environment described above those skilled in the art will appreciate that the technology may be implemented in a variety of other environments including a single monolithic computer system as well as various other combinations of computer systems or similar devices connected in various ways. For example in some embodiments the storage server has a distributed architecture even though it is not illustrated as such in .

The processors are the central processing units CPUs of the storage server and thus control its overall operation. In some embodiments the processors accomplish this by executing software stored in memory . A processor may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such devices.

Memory includes the main memory of the storage server . Memory represents any form of random access memory RAM read only memory ROM flash memory or the like or a combination of such devices. Memory stores among other things the storage operating system . The storage operating system implements a storage manager such as a file system manager to logically organize the information as a hierarchical structure of directories files and special types of files called virtual disks on the disks. A portion of the memory is organized as a buffer cache for temporarily storing data associated with requests issued by clients that are during the course of a consistency point flushed written to disk or another non volatile storage device. The buffer cache includes a plurality of storage locations or buffers organized as a buffer tree structure. A buffer tree structure is an internal representation of loaded blocks of data for e.g. a file or virtual disk vdisk in the buffer cache and maintained by the storage operating system . In some embodiments a portion of the memory is organized as a specially allocated data SAD data structure for storing single instances of data that are to be or have been specially allocated by the storage server .

The non volatile RAM NVRAM is used to store changes to the file system between consistency points. Such changes may be stored in a non volatile log NVLOG that is used in the event of a failure to recover data that would otherwise be lost. In the event of a failure the NVLOG is used to reconstruct the current state of stored data just prior to the failure. In some embodiments the NVLOG includes a separate entry for each write request received from a client since the last consistency point. In some embodiments the NVLOG includes a log header followed by a number of entries each entry representing a separate write request from a client . Each request may include an entry header followed by a data field containing the data associated with the request if any e.g. the data to be written to the storage subsystem . The log header may include an entry count a CP consistency point count and other metadata. The entry count indicates the number of entries currently in the NVLOG . The CP count identifies the last consistency point to be completed. After each consistency point is completed the NVLOG is cleared and started anew. The size of the NVRAM is variable. However it is typically sized sufficiently to log a certain time based chunk of requests from clients for example several seconds worth .

Also connected to the processors through the interconnect system are one or more internal mass storage devices a storage adapter and a network adapter . Internal mass storage devices may be or include any computer readable storage medium for storing data such as one or more disks. As used herein the term disk refers to any computer readable storage medium including volatile e.g. RAM nonvolatile e.g. ROM Flash etc. removable and non removable media or any combination of such media devices that are capable of storing information such as computer readable instructions data structures program modules or other data. It is further noted that the term disk may refer to physical or virtualized computer readable storage media. The storage adapter allows the storage server to access the storage subsystem and may be for example a Fibre Channel adapter or a SCSI adapter. The network adapter provides the storage server with the ability to communicate with remote devices such as the clients over a network and may be for example an Ethernet adapter a Fibre Channel adapter or the like.

In some embodiments storage manager implements a write in place file system algorithm while in other embodiments the storage manager implements a write anywhere file system. In a write in place file system the locations of the data structures such as inodes and data blocks on disk are typically fixed and changes to such data structures are made in place. In a write anywhere file system when a block of data is modified the data block is stored written to a new location on disk to optimize write performance sometimes referred to as copy on write . A particular example of a write anywhere file system is the Write Anywhere File Layout WAFL file system available from NetApp Inc. of Sunnyvale Calif. The WAFL file system is implemented within a microkernel as part of the overall protocol stack of a storage server and associated storage devices such as disks. This microkernel is supplied as part of Network Appliance s Data ONTAP software. It is noted that the technology introduced herein does not depend on the file system algorithm implemented by the storage manager .

Logically under the storage manager layer the storage operating system also includes a multi protocol layer and an associated media access layer to allow the storage server to communicate over the network e.g. with clients . The multi protocol layer implements various higher level network protocols such as Network File System NFS Common Internet File System CIFS Direct Access File System DAFS Hypertext Transfer Protocol HTTP and or Transmission Control Protocol Internet Protocol TCP IP . The media access layer includes one or more drivers which implement one or more lower level protocols to communicate over the network such as Ethernet Fibre Channel or Internet small computer system interface iSCSI .

Also logically under the storage manager layer the storage operating system includes a storage access layer and an associated storage driver layer to allow the storage server to communicate with the storage subsystem . The storage access layer implements a higher level disk storage protocol such as RAID while the storage driver layer implements a lower level storage device access protocol such as Fibre Channel Protocol FCP or small computer system interface SCSI . Also shown in is a path of data flow through the storage operating system associated with a request.

In some embodiments the storage operating system includes a special allocation layer logically above the storage manager . The special allocation layer is an application layer that examines the contents of data blocks included in the NVLOG to determine whether the contents correspond to specially allocated data. For example specially allocated data may include zero filled data blocks data blocks whose contents correspond to a particular document header type data blocks exceeding a pre defined sharing threshold or other designated data. In yet another embodiment the special allocation layer is included in the storage manager . Note however that the special allocation layer does not have to be implemented by the storage server . For example in some embodiments the special allocation layer is implemented in a separate system to which the NVLOG buffer cache or data blocks are provided as input.

In operation a write request issued by a client is forwarded over the network and onto the storage server . A network driver of layer processes the write request and if appropriate passes the request on to the multi protocol layer for additional processing e.g. translation to an internal protocol prior to forwarding to the storage manager . The write request is then temporarily stored queued by the storage manager in the NVLOG of the NVRAM and temporarily stored in the buffer cache . In some embodiments the special allocation layer examines the contents of queued write requests data blocks to determine whether the contents correspond to specially allocated data. When the contents of a data block corresponds to data that has been previously identified as special data a special VBN or VVBN PVBN pair is assigned to the corresponding level 1 block pointer in the buffer tree that contains the block and the data block is removed from the buffer cache so that it is not flushed to disk. For example a special VBN labeled VBN ZERO may be assigned to a level 1 block pointer to signify that the data for the corresponding level 0 block is zero filled and has been pre allocated on disk. By not issuing write requests to store special data on disk the technology introduced herein avoids disk fragmentation caused by freeing duplicate data blocks. Also by not issuing write requests to store special data on disk the technology introduced herein substantially reduces the processing time and overhead associated with deduplicating duplicate data blocks.

Subsequently if a read request is received the storage manager indexes into the inode of the file using a file block number FBN given in the request to access an appropriate entry and retrieve a volume block number VBN . If a retrieved VBN corresponds to a special VBN e.g. VBN ZERO the storage manager reads the specially allocated data from memory e.g. from the SAD data structure . Otherwise the storage manager generates operations to read the requested data from disk unless the data is present in the buffer cache . If the data is in the buffer cache the storage manager reads the data from buffer cache . Otherwise if the data is not in the buffer cache the storage manager passes a message structure including the VBN to the storage access layer to map the VBN to a disk identifier and disk block number disk DBN which are then sent to an appropriate driver e.g. SCSI of the storage driver layer . The storage driver accesses the DBN from the specified disk and loads the requested data blocks into buffer cache for processing by the storage manager . By accessing special data in memory of the storage server requests for such data can be responded to substantially faster because the special data may be read without accessing the disk and or the storage subsystem . In addition by not issuing requests to access special data exceeding a pre defined sharing threshold on disk the technology introduced herein eliminates hot spots associated with reading a single instance of a deduplicated data block.

In some embodiments the special allocation component processes the NVLOG prior to the data blocks being written to disk . This may occur for example during an event called a consistency point in which the storage server stores new or modified data to its mass storage devices based on the write requests temporality stored in the buffer cache . However it is noted that in some embodiments a consistency point may begin before the special allocation component finishes examining all of the data block of NVLOG . In such cases the data blocks which have not yet been examined are examined during the consistency point and before the consistency point completes. That is each unexamined data block is examined before being flushed to disk.

In some embodiments the special allocation component is embodied as one or more software modules within the special allocation layer of the storage operating system . In other embodiments however the functionality provided by the special allocation component is implemented at least in part by one or more dedicated hardware circuits. The special allocation component may be stored or distributed on for example computer readable media including magnetically or optically readable computer discs hard wired or preprogrammed chips e.g. EEPROM semiconductor chips nanotechnology memory or other computer readable storage medium. Indeed computer implemented instructions data structures screen displays and other data under aspects of the technology described herein may be distributed over the Internet or over other networks including wireless networks on a propagated signal on a propagation medium e.g. an electromagnetic wave s etc. over a period of time or they may be provided on any analog or digital network packet switched circuit switched or other scheme .

Returning to in some embodiments the storage manager cooperates with virtualization layers e.g. vdisk layer and translation layer to virtualize the storage space provided by storage devices . That is the storage manager together with the vdisk layer and the translation layer aggregates the storage devices of storage subsystem into a pool of blocks that can be dynamically allocated to form a virtual disk vdisk . A vdisk is a special file type in a volume that has associated export controls and operation restrictions that support emulation of a disk. The vdisk includes a special file inode that functions as a container for storing metadata associated with the emulated disk. It should be noted that the storage manager the vdisk layer and translation layer can be implemented in software hardware firmware or a combination thereof.

The vdisk layer is layered on the storage manager to enable access by administrative interfaces such as user interface UI in response to a user e.g. a system administrator issuing commands to the storage server . The vdisk layer implements a set of vdisk LUN commands issued through the UI by a user. These vdisk commands are converted to file system operations that interact with the storage manager and the translation layer to implement the vdisks.

The translation layer in turn initiates emulation of a disk or LUN by providing a mapping procedure that translates LUNs into the special vdisk file types. The translation layer is logically between the storage access layer and the storage manager to provide translation between the block LUN space and the file system space where LUNs are represented as blocks. In some embodiments the translation layer provides a set of application programming interfaces APIs that are based on the SCSI protocol and that enable a consistent interface to both the iSCSI and FCP drivers. In operation the translation layer may transpose a SCSI request into a message representing an operation directed to the storage manager . A message generated by the translation layer may include for example a type of operation e.g. read write along with a pathname and a filename of the vdisk object represented in the file system. The translation layer passes the message to the storage manager where the operation is performed.

In some embodiments the storage manager implements so called flexible volumes hereinafter referred to as virtual volumes VVOLs where the file system layout flexibly allocates an underlying physical volume into one or more VVOLs. illustrates an aggregate in one embodiment. As used herein the underlying physical volume for a plurality of VVOLs is an aggregate of one or more groups of disks of a storage system.

As illustrated in each VVOL can include named logical unit numbers LUNs directories qtrees and files . A qtree is a special type of directory that acts as a soft partition i.e. the storage used by the qtrees is not limited by space boundaries. The aggregate is layered on top of the translation layer which is represented by at least one RAID plex wherein each plex includes at least one RAID group . Each RAID group further includes a plurality of disks e.g. one or more data D disks and at least one P parity disk.

Whereas the aggregate is analogous to a physical volume of a conventional storage system a VVOL is analogous to a file within that physical volume. That is the aggregate may include one or more files wherein each file contains a and wherein the sum of the storage space consumed by flexible volumes associated with the aggregate is physically less than or equal to the size of the overall physical volume. The aggregate utilizes a physical volume block number PVBN space that defines the storage space of blocks provided by the disks of the physical volume while each VVOL embedded within a file utilizes a logical or virtual volume block number VVBN space in order to organize those blocks as files. The PVBNs reference locations on disks of the aggregate whereas the VVBNs reference locations within files of the VVOL . Each VVBN space is an independent set of numbers that corresponds to locations within the file that may be translated to disk block numbers DBNs on disks . Since a VVOL is also a logical volume it has its own block allocation structures e.g. active space and summary maps in its VVBN space.

Each VVOL may be a separate file system that is mingled onto a common set of storage in the aggregate by an associated storage operating system . In some embodiments the translation layer of the storage operating system builds a RAID topology structure for the aggregate that guides each file system when performing write allocations. The translation layer also presents a PVBN to disk block number DBN mapping to the storage manager.

A container file may be associated with each VVOL . As used herein a container file is a file in the aggregate that contains all blocks of the VVOL . In some embodiments the aggregate includes one container file per VVOL . is a block diagram of a container file for a VVOL . The container file has an inode of the flexible volume type that is assigned an inode number equal to a virtual volume id VVID . The container file is typically one large sparse virtual disk and since it contains all blocks owned by its VVOL. It is noted that a block with VVBN of x in the VVOL can be found at the file block number FBN of x in the container file . For example VVBN in the VVOL can be found at FBN in its container file . Since each VVOL in the aggregate has its own distinct VVBN space another container file may have FBN that is different from FBN in the container file . The inode references indirect blocks which in turn reference both physical data blocks and virtual data blocks at level 0.

In some embodiments the process is invoked in response to an entry associated with a write request being added to the NVLOG . While in other embodiments the process is invoked after a pre defined number of entries are added to the NVLOG . In yet other embodiments the process is invoked as a precursor to or as part of a consistency point.

Initially at step the special allocation component selects a data block from the NVLOG . For example the selected data block may be one of the 20 971 520 4 kb zero filled data blocks of the virtual disk image file.

Next at step the special allocation component determines whether the contents of the selected data block correspond to specially allocated data . For example the allocation component may compare the contents of the selected block to the contents of a specially allocated zero filled block . As another example the special allocation component may compute a hash of the selected block and compare the hash to a hash of the specially allocated zero filled block . If the contents of the selected data block do not correspond to specially allocated data the process continues at step as described below. Otherwise if the contents of the selected data block correspond to specially allocated data e.g. for zero filled special data if the hash is zero the process proceeds to step .

At step the special allocation component assigns a special pointer e.g. VBN ZERO to the corresponding level 1 indirect block of the file to signify that the contents of the data block corresponds to specially allocated data e.g. is zero filled and has been pre allocated on disk. Then the process proceeds to step .

At step the special allocation component removes the selected block from the buffer cache so that the block is not written to disk . Then the process proceeds to step .

At step the special allocation component determines whether all of the blocks in the NVLOG have been selected for processing. If any block remains the process continues at step where the special allocation component selects a block as described above. Otherwise if all of the blocks have been selected the process ends.

Those skilled in the art will appreciate that the steps shown in and in each of the following flow diagrams may be altered in a variety of ways. For example the order of certain steps may be rearranged certain substeps may be performed in parallel certain shown steps may be omitted or other steps may be included etc.

In some embodiments the process is invoked in response to the stprage server receiving a read request from a client . Initially the process begins at step when the storage manager receives a read request such as a file request a block request and so on. Next at step the storage manager processes the request by for example converting the request to a set of file system operations. Then the process proceeds to step . At step the storage manager identifies the data blocks to load. This may be accomplished for example by identifying the inode corresponding to the request. Then the process proceeds to step .

At step for each identified block a determination is made as to whether the block is specially allocated. This determination may be made for example by examining the corresponding level 1 block pointer referencing the data block to determine whether it is a predetermined special pointer. For each specially allocated block the process proceeds to step . Otherwise the process continues at step as described below.

At step for each block that is specially allocated the storage manager reads the block from SAD buffer . Then the process continues at step as described below.

At step for each block that is not specially allocated the storage manager determines whether the block is stored in the buffer cache . For each block that is stored in the buffer cache the process proceeds to step . Otherwise the process continues at step as described below.

At step for each block that is stored within the buffer cache the storage manager reads the block from the buffer cache . Then the process continues at step as described below.

At step for each block that is not specially allocated or stored within the buffer cache the storage manager retrieves the block from disk . Then the process proceeds to step .

At step the storage manager determines whether there are more blocks to load. If there are more data blocks to load the process proceeds to step . At step the storage manager selects the next block. Then the process proceeds to step as described above. Otherwise if there are no more blocks to load at step the process proceeds to step . At step the storage server returns the requested data blocks to the client . Then the process ends.

Thus a system and method for specially allocating data has been described. Note that references in this specification to an embodiment one embodiment some embodiments or the like mean that the particular feature structure or characteristic being described is included in at least one embodiment of the present invention. Occurrences of such phrases in this specification do not necessarily all refer to the same embodiment. Although the technology introduced herein has been described with reference to specific exemplary embodiments it will be recognized that the invention is not limited to the embodiments described but can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense.

