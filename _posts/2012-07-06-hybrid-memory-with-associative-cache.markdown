---

title: Hybrid memory with associative cache
abstract: A hybrid memory system includes a primary memory comprising a host memory space arranged as memory sectors corresponding to host logical block addresses (host LBAs). A secondary memory is implemented as a cache for the primary host memory. A hybrid controller is configured map the clusters of host LBAs to clusters of solid state drive (SSD) LBAs. The SSD LBAs correspond to a memory space of the cache. Mapping of the host LBA clusters to the SSD LBA clusters is fully associative such that any host LBA cluster can be mapped to any SSD LBA cluster.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09390020&OS=09390020&RS=09390020
owner: SEAGATE TECHNOLOGY LLC
number: 09390020
owner_city: Cupertino
owner_country: US
publication_date: 20120706
---
A hybrid memory system includes a nonvolatile primary memory comprising a host memory space arranged as memory sectors corresponding to host logical block addresses host LBAs . A secondary memory is implemented as a cache for the primary host memory. A hybrid controller is configured to map the clusters of host LBAs to clusters of solid state drive SSD LBAs. The SSD LBAs correspond to a memory space of the cache. Mapping of the host LBA clusters to the SSD LBA clusters is fully associative such that any host LBA cluster can be mapped to any SSD LBA cluster.

A method of implementing a hybrid memory system includes mapping clusters of host logical block addresses LBAs corresponding to a host LBA space to solid state drive SSD LBA clusters. The host LBA space corresponds to a memory space of a nonvolatile primary memory and the SSD LBA clusters corresponding to memory space of a secondary memory. The secondary memory may be arranged to operate as a fully associative cache for the primary memory wherein any host LBA cluster can be mapped to any SSD LBA cluster.

A hybrid memory system includes a hard magnetic disk drive HDD which comprises a memory space arranged as memory sectors corresponding to host logical block addresses LBAs . A flash memory is configured to serve as a cache for the HDD. A hybrid controller is configured to map the clusters of host LBAs to clusters of solid state drive SSD LBAs the SSD LBAs corresponding to a memory space of the flash memory. The mapping of the host LBA clusters to the SSD LBA clusters being fully associative wherein any host LBA cluster can be mapped to any SSD LBA cluster.

These and other features and aspects of the various embodiments disclosed herein can be understood in view of the following detailed discussion and the accompanying drawings.

Some memory devices use at least two types of memory in a hybrid or tiered memory system where at least one type of memory is used as a primary memory and at least one other type of memory is used as a secondary memory that operates as a cache. The primary memory may have greater storage capacity but slower access times than the secondary memory for example. In this arrangement the secondary memory can serve as a read cache and or a write cache for the primary memory. One example of such a tiered memory device is a hybrid drive in which the primary memory may comprise nonvolatile memory such as magnetic disk magnetic tape and or optical disk and the secondary memory may comprise solid state flash memory and or the secondary memory may be a nonvolatile or volatile memory with or without battery backup. Note that the terms primary memory and secondary memory are used herein to denote differences in memory e.g. usage capacity performance memory class or type etc. and not necessarily order or preference. Furthermore although examples provided herein refer to the primary memory as magnetic disk and to secondary memory as flash memory the disclosed approaches are applicable to any types of primary and secondary memory.

The host sends memory access requests to the hybrid drive to read or write data. The memory access requests may specify a host LBA range used for the operation of the memory access request. For example a memory access request from the host may request that a host LBA range be written to the hybrid drive and or a memory access request may request that a host LBA range be read from the hybrid drive . The memory access requests are managed by the hybrid controller to cause data to be written to and or read from the hybrid drive with optimal efficiency. The second cache in this example may optionally be read only in that only data marked for read operations by the host are placed in the second cache . In such a configuration data marked for writing are sent directly to the main storage either directly or via the first cache .

According to some embodiments the controller of the hybrid memory device also denoted hybrid drive may be implemented as a hierarchy of abstraction layers. Pairs of the abstraction layers are communicatively coupled through application programming interfaces APIs . The organization of the hybrid controller into abstraction layers to some extent allows each layer to work relatively independently and or can reduce potential conflicts that arise from processing multiple threads of execution. For purposes of discussion some examples provided below are based on the use of a magnetic disk as the main memory dynamic random access memory as the first or primary cache and solid state flash memory as the second or secondary cache. It will be apparent to those skilled in the art that the various memory components are not restricted to these types of memory and may be implemented using a wide variety of memory types.

In some configurations the cache may be configured as a secondary cache being faster and smaller than the main storage . The cache is a primary cache being faster and smaller than the secondary cache . Generally the terms primary and secondary or first and second refer generally to hierarchy of time and or priority relative to commands received via the host interface . For example current read write requests from the host may be processed first via the primary cache e.g. identified by the data s logical block address . This enables host commands to complete quickly should the requested data be stored in the primary cache . If there is a miss in the primary cache the requested data may be searched for in the secondary cache . If not found in either requested data may be processed via the main storage .

Some of the data stored in the primary cache may either be copied or moved to the secondary cache as new requests come in. The copying movement from primary cache to secondary cache may also occur in response to other events e.g. a background scan. Both copying and moving involve placing a copy of data associated with an LBA range in the secondary cache and moving may further involve freeing up some the LBA range in the primary cache for other uses e.g. storing newly cached data.

The host processor communicates with the hybrid memory device also referred to herein as hybrid drive through a host interface . As previously discussed the main memory includes a memory space that corresponds to a number of memory sectors each sector addressable using a unique a logical block address LBA . The sectors of the main memory are directly accessible by the host using the LBAs and thus the corresponding LBAs of the main memory are referred to herein as host LBAs.

The host sends memory access requests to the hybrid drive for example the host may request that data be written to and or read from the hybrid memory device. The host interface is configured to transfer memory access requests from the host to the hybrid memory device and to transfer data between the host and the hybrid memory device.

The hybrid controller illustrated in includes number of layers wherein each layer communicates to its nearest neighboring layer s e.g. through a set of requests. For example each layer may only communicate to its nearest neighboring layer s without communicating to other layers. As an example the layer may only communicate directly to layer and layer without communicating directly with the layer or to the host interface . As an operation such as a memory access request from the host is being carried out each layer is configured to pass control to the next lower layer as the operation is implemented.

The example illustrated in includes four layers which are described in terms applicable to the use of flash memory as a cache. It will be appreciated that these terms are not restrictive and if other types of memory were used as the secondary memory if desired different terminology could be used to reflect the type of secondary memory. Nevertheless the basic functions of the layers can be similar regardless of the type of memory used for primary and or secondary memory and or the terminology used to describe the layers.

The layers illustrated in include the flash cache interface FCI layer the flash cache control and transfer management FCTM layer the solid state drive SSD layer and the programmable state machine PSM layer . Requests may be passed as indicated by arrows from a higher layer to the next lower layer starting with the FCI layer and proceeding to the PSM layer which interacts directly with the flash memory . The layered architecture of the hybrid controller described herein allows for handling host memory access requests which can be serviced from either the magnetic memory or one of the caches The layered structure used in conjunction with the flash cache can be configured to achieve specified rates and response times for servicing memory access requests.

The FCI layer decides whether a host read request should be serviced from the primary memory or from one of the caches . The FCI layer implements processes to determine which data should be promoted to the flash secondary cache and or the primary cache based on various criteria to achieve optimal workload for the hybrid drive . The flash content and transfer management FCTM layer maintains a mapping e.g. a fully associative mapping as discussed below of the host LBAs to a memory space corresponding to the flash memory space arranged as LBAs which are referred to as solid state drive SSD LBAs. The SSD layer interacts with programmable state machine PSM layer and performs tasks such as optimal scheduling of promotion requests among dies of the flash referred to as die scheduling wear leveling garbage collection and so forth. The SSD layer maps the SSD LBAs of the FCTM layer to physical flash locations die block and page locations . The PSM layer programs hardware controllers to generate the required signals to read from and write to the flash for example.

In some cases one or more of the layers of the hybrid controller may be implemented by circuitry and or by one or more processors e.g. such as reduced instruction set computer RISC processors available from ARM. In some cases each layer may be implemented by a separate processor. The processes discussed herein are implementable in hardware interconnected electronic components that carry out logic operations and or by a processor implementing software instructions and or by any combination of hardware and software.

Embodiments described herein involve processes carried out by the FCTM layer to implement a fully associative cache in the flash memory. In connection with implementation of the fully associative cache the FCTM layer receives memory access requests from the FCI layer that include host LBA ranges. In some cases the FCTM layer clusters partitions the host LBAs into clusters. The host LBAs or host LBA clusters are mapped to SSD LBAs or clusters of SSD LBAs. The FCTM issues requests to the SSD layer that include SSD LBAs or clusters of SSD LBAs. In the SSD layer the clusters of SSD LBAs are mapped to physical flash addresses i.e. die block and page addresses.

As introduced above the FCI layer receives various memory access requests from the host and in response to the host requests the FCI layer issues various memory access requests to the FCTM layer. For example in some embodiments the requests from the FCI layer to the FCTM layer may include promote requests requests to store cluster aligned host LBAs in the flash memory read requests requests to read host LBAs from flash memory invalidate requests requests to mark cluster aligned host LBAs as invalid in flash memory and lookup requests requests to determine if host LBAs are present in flash memory . The FCTM layer is configured to support fully associative mapping of host LBA clusters to SSD LBA clusters for these requests. Each request from the FCI layer identifies the type of request e.g. read promotion invalidate etc. and also includes a host LBA range. The FCTM maps the LBA address range specified in the FCI memory access requests to clusters in the SSD LBA space.

The mapping of the host LBA clusters to the SSD clusters by the FCTM layer is fully associative meaning that any host LBA cluster can be mapped to any of the SSD LBA clusters so long as there is room in the cache. diagrammatically depicts mapping of the host LBA space to the SSD LBA space . In the FCTM layer the host LBA space is clustered into clusters of host LBAs and the SSD LBA space is clustered into clusters of SSD LBAs. In the host LBA space each cluster of host LBAs is uniquely identified by a number between 0 and N 1 and each cluster includes n contiguous sectors. In the SSD LBA space each SSD cluster is uniquely identified by a number between 0 and K 1 K is typically less than N and each cluster includes n sectors. The number of sectors per cluster n may be fixed and can depend on the size of a host sector the geometry of the flash memory the error correction code ECC used to store data in the flash memory and or other factors. In the example illustrated in n 32 however in other implementations n may be greater than or less than 32. Furthermore in general n need not be a power of two.

In some implementations and as shown in the host sectors are aligned with the cluster boundaries. In other words a host LBA is not allowed to span more than one host LBA cluster.

The mapping from host LBA space to SSD LBA space is accomplished by a hash function . As previously discussed the hash function can support fully associative caching with regard to clusters. In other words the hash function allows any host cluster to be mapped to any SSD cluster as indicated by arrows . However the mapping may be constrained such that any host LBA can exist in only one SSD cluster at any given time. The offset within a cluster where an LBA is located within a cluster is fixed and is can be determined by the host LBA modulo the number of host LBAs per cluster i.e. the remainder resulting from dividing the host LBA by n. Allowing a host LBA cluster to be mapped into any SSD cluster and ensuring that promotes and invalidates implemented by the FCTM layer are aligned to cluster boundaries avoids cache fragmentation.

The hash function is used to convert the tag upper L bits of the host LBA into a hash table index in the hash table . The entry in the hash table indicated by the hash table index the tag converted by the hash function points to one or more clusters in the SSD LBA space. For example for a host LBA of L M bits the lower M bits can be used as a sector offset to identify the sector within an SSD cluster. The remaining L bits are used for the tag. The hash function operates on the tag to generate the index into the hash table . For example the hash function may discard the upper L H bits of the tag and use the lower H bits as the hash table index. Discarding a portion of the tag means that in some cases a number of different host LBAs will map to the same entry in the hash table and a collision will occur. An entry in the hash table is associated with more than one cluster identification ID only if a collision occurs. In this scenario 2host LBAs mapped to a cluster will all have the same tag. If the hash function discards the upper bits leaving only H lower bits for the hash table index the theoretical maximum number of possible collisions i.e. the number of clusters that map into the same SSD LBA space is 2. The L H bits of the tag identify the cluster ID. The collisions are resolved using a linked list . The linked list contains the cluster IDs that are hashed to the same entry in the hash table i.e. have the same hash index . To access a particular cluster the linked list is scanned for an entry with the correct cluster ID. For example when the FCI layer requests a look up involving a particular host LBA cluster the FCTM layer applies the hash function and if there is a collision two clusters that map to the same space then the FCTM layer traverses through the linked list to locate the requested cluster.

The above description assumes that the number of host sectors per cluster is a power of two. However non power of two sector sizes may also be used. A representative set of host sector sizes that are supportable by the fully associative cache structure described herein include but is not limited to the following sector sizes 512 520 524 528 4096 4192 and 4224 bytes. For example based on sector to cluster mapping calculations there may be 30 5XX byte sectors per cluster assuming a cluster is 16 KB of the flash such as an 8 KB flash page size with dual plane support .

Non powers of two can be handled by modifying the mapping described above as follows The tag is determined as tag host LBA sectors per cluster where indicates an integer division via truncation and the host sector offset within the cluster is determined by host LBA modulo the sectors per cluster i.e. the remainder after dividing the host LBA by the sectors per cluster.

The division and modulo operations can be implemented by executing a multiply instruction e.g. a 64 bit multiply instruction on the FCTM processor assuming the FCTM processor supports 64 bit multiple instructions. To facilitate the multiply the value p 0xFFFFFFFF sectors per cluster is pre computed is a constant value. The tag is now determined by tag host LBA p 32 where indicates a 64 bit multiply operation and where 32 means that the result of host LBA p is right shifted 32 times. Using this process there is a possibility that the tag is off by one. To correct for this occurrence the tag is incremented by one if the following condition is satisfied Host LBA tag sectors per cluster sector per cluster. The remainder can be similarly determined.

The FCTM maintains a cluster use list and a cluster free list in metadata of the FCTM. SSD clusters in the use list are those that are currently being used and correspond to valid data stored in the flash. SSD clusters in the free list are those clusters that are available to be written to. The flash can become saturated meaning that there are no SSD clusters in the free list all SSD clusters are in the use list . If the cache is saturated there is no space available in the flash to implement requests from the FCI layer to promote write data to the flash unless an eviction occurs. During execution of a promotion request if the cache is saturated the FCTM layer performs an eviction before allocating SSD clusters for the promotion. For example the FCTM layer may perform an eviction by evicting SSD clusters identified as the least valuable to make room for the new clusters to be written. In other words if the promotion request involves writing J clusters to the flash then the FCTM module evicts the J least valuable clusters from the flash.

The FCTM layer maintains a most valuable least valuable MVLV list to identify the least valuable clusters. The MVLV list may comprise a linked list of all the SSD clusters in ranked order according to a value level determined by the FCTM module. The value level of a cluster may be based on how recently the SSD cluster was used and or may be based on how frequently the SSD cluster was used. The MVLV list may be updated in conjunction with the execution of some memory access requests. For example when a read request is executed by the FCTM the SSD clusters involved in the request are moved to the head of the MVLV list making them the most valuable. Similarly when a promotion request is executed by the FCTM the SSD clusters involved in the promotion request are moved to the head of the list making them the most valuable.

An approach for implementing a promotion request by the FCTM layer is conceptually illustrated in the flow diagram of . The promotion request from the FCI layer includes a cluster aligned host LBA range to be promoted to the cache. A cluster aligned host LBA range is an LBA range that starts at the beginning of a cluster and ends at the end of a cluster. In other words the cluster aligned LBA range does not include partial clusters.

In response to the promotion request the FCTM layer maps the cluster aligned host LBA range of the request to SSD clusters. The FCTM determines if the cluster aligned host LBA range corresponds to any SSD clusters. If so these SSD clusters are already present in the flash and are referred to herein as overlapped clusters. The FCTM layer creates a list of the overlapped SSD clusters. Because these overlapped SSD clusters are already stored in the flash re writing the overlapped clusters to the flash is unnecessary. To avoid re writing clusters the FCTM layer creates a bitmap of the overlapped SSD clusters and skips writing these clusters to the flash when executing the promotion request. Identifying overlapped clusters to avoid re writing the overlapped clusters to the flash reduces memory wear. For example if all cluster aligned host LBAs in the promotion request are already stored in the flash the host LBA range of the promotion request is entirely present in overlapped clusters no data transfer to the flash occurs the FCTM does not issue a write request to the SSD layer . If some host LBAs of the promotion request are already present in the flash then the FCTM uses the bitmap to exclude the overlapped SSD clusters already present in the cache from being re written to the flash.

The FCTM layer determines if there are sufficient SSD clusters available to implement the promotion request by checking the use list maintained in the FCTM metadata. If not the FCTM evicts the required number of SSD clusters. The clusters at the tail of the MVLV list are chosen for eviction. The FCTM allocates SSD clusters to be used for the promotion and executes the promotion by writing to the flash. In this example the promoted SSD clusters are now most valuable because they are the most recently used SSD clusters. The promoted SSD clusters are moved to the head of the MVLV list. The FCTM maintains a cluster use list and a cluster free list in metadata of the FCTM. SSD clusters in the use list are those that are present in the flash. SSD clusters in the free list are those available to be written to. In conjunction with execution of a promotion request the FCTM metadata is updated to indicate that the promoted SSD clusters are in use.

It is to be understood that this detailed description is illustrative only and various additions and or modifications may be made to these embodiments especially in matters of structure and arrangements of parts and or processes. Accordingly the scope of the present disclosure should not be limited by the particular embodiments described above but should be defined by the claims set forth below and equivalents thereof.

