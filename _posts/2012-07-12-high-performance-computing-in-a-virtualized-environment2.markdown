---

title: High performance computing in a virtualized environment
abstract: Double-bypass virtualization techniques are provided to enable use of high performance computing (HPC) application in a virtualized environment. In one example, a HPC user space application running on a virtual machine obtains direct access to virtual network interface card (vNIC) on a host-computing device associated to that virtual machine. The HPC user space application is configured to transmit and/or receive one or more data frames via the vNIC while bypassing the operating system of the virtual machine and the virtual machine hypervisor of the host-computing device.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09612877&OS=09612877&RS=09612877
owner: Cisco Technology, Inc.
number: 09612877
owner_city: San Jose
owner_country: US
publication_date: 20120712
---
Traditional HPC environments run with an operating system OS kernel that controls one or more central processing units CPUs . Large HPC applications may span multiple OS kernels that in turn span multiple CPUs. Typically only one HPC application is run at a time on a given CPU and HPC schedulers control access to entire clusters of HPC computational resources e.g. servers and regulate which HPC jobs run on which servers and CPUs. These schedulers operate in a batch mode where HPC applications are slotted into various queues for execution. However typically only the applications in the highest priority job queue are executed. If there are multiple applications in the highest priority job queue the scheduler will switch between applications with a large batch time slice that may take for example on order of several seconds minutes or hours.

Double bypass virtualization techniques are presented herein to enable deployment of high performance computing HPC applications in a virtualized environment. In one example a HPC user space application running in a virtual machine obtains direct access to a virtual network interface card vNIC of the host computing device associated to that virtual machine. The HPC user space application is configured to transmit and or receive one or more data frames via the vNIC while bypassing the operating system of the virtual machine and the virtual machine hypervisor of the host computing device.

High Performance Computing HPC is often characterized by computing systems used by scientists and engineers for modeling simulating and analyzing complex physical or algorithmic phenomena. HPC environments are evolving away from the monolithic mainframe or super computer single tenant environments to environments wherein multiple HPC tenants each have separate HPC computing systems to service their specific needs. As used herein a HPC tenant is a set of users who develop test tune and run i.e. share a common set of HPC applications. In an organization university departments individual research projects etc. may define tenants. In an organization such as a service bureau a tenant may be defined as an individual customer.

With the emergence of cloud computing there is a trend towards re centralization within multi tenant organizations to for example simplify network and systems management and maintenance. However running multiple HPC tenants in a single environment i.e. the cloud poses a number of challenges for traditional mechanisms. For example in a single environment the tenants need to agree on a common operating system kernel and a common set of system libraries. The tenants also need to share the same file systems and segregation is enforced by access control. Additionally the tenants generally need to use the same applications particularly those of the system utility variety and choose unique user and group identifiers. The HPC and kernel schedulers need to arbitrate competition for resources among tenants. For these and other reasons the cloud computing model has traditionally been viewed as impractical for HPC tenants. One conventional mechanism for multi tenant HPC is to assign each tenant exclusive use of nodes within the shared system where the tenant is able to operate their HPC environment separate from the other tenants. However this exclusive assignment approach is sub optimal as it effectively segments the shared system and creates special cases for service provisioning. Moreover the exclusive assignment of nodes makes resource sharing impractical and short term urgent tasks cannot backfill into any unused nodes that have been exclusively assigned to a particular tenant.

Applying server virtualization techniques to a multi tenant HPC environment is a solution to the shortcomings mentioned above however traditional virtualization is associated with an overhead and loss of performance that is considered unacceptable by most HPC users. As such virtualization has not been widely deployed in HPC environments. Proposed herein are double bypass virtualization techniques that significantly reduce the overhead and loss of performance of traditional virtualization techniques so as to make multi tenant virtualized HPC environments practical. With these virtualization techniques tenants may be assigned their own set of virtual machines running their preferred environment operating system kernel system libraries file systems applications and user and group identifiers and are not forced into the single environment that may be sub optimal for a given HPC application especially if they have legacy HPC applications that require the use of a legacy environment. Moreover running HPC applications in a virtualized environment provides additional benefits such as elastic computation fault resistance and load balancing. Elastic computation is the ability to expand or contract the resources used by long running HPC applications based on external resource constraints and variable computational demands throughout the application lifetime. Fault resilience is useful for example when a hardware problem is identified by one of the virtual machines running as part of the HPC application. In such cases the virtual machine running on a server that suffers a hardware problem may be migrated to another computation node. Load balancing improves performance since parts of the HPC computation may be migrated to underutilized computation nodes and away from computational nodes whose use would impede other aspects of the HPC calculation. In general applying virtualization constructs that have been developed to collapse multiple one server computations over a reduced number of servers to HPC computations i.e. to massive parallel computations that in nature are not contained nor containable in a single server and that intrinsically require parallelism to be performed because no single processor server can process them leads to use of the virtualization constructs in a very different way from how they have been conceived e.g. a typical case would be a single virtual server per physical server .

Computer operating systems generally segregate virtual memory into kernel space and user space . Kernel space is strictly reserved for running the kernel kernel extensions and most device drivers. In contrast user space is the memory area where user space processes are executed. A process is an executing i.e. running instance of an application program. User space processes are instances of all applications other than the kernel i.e. utilities and application programs . When an application is to be run it is copied from storage into user space memory so that it can be accessed at high speed by a processor e.g. central processing unit CPU . In the example of virtual machine executes a HPC user space application under an operating system OS .

The hypervisor sometimes referred to as a virtual machine manager is a program that allows multiple operating systems to share a single physical host computing device. Each operating system appears to have exclusive use of the host s processor memory and other resources. However the hypervisor actually controls the host processor and resources allocates what is needed by each operating system and ensures that all of the operating systems cannot disrupt one another. In the example of hypervisor allows operating system of virtual machine to share the resources of server with the operating systems of any other virtual machines not shown in hosted by server .

In a conventional arrangement in which a user space application is executed by a virtual machine several steps need to be completed in order for the user space application to send receive traffic via a computer network. With regard to transmission the user space application first communicates with the network software stack e.g. Transmission Control Protocol TCP User Datagram Protocol UDP stack of the virtual machine s operating system i.e. the operating system of the virtual machine executing the user space application . Next the network software stack of the operating system communicates with the network software stack of the hypervisor. The network software stack of the hypervisor then communicates with the network interface hardware. This process is reversed when traffic is received from the computer network. More specifically the network interface hardware communicates with the network software stack of the hypervisor that then communicates with the network software stack of the virtual machine s operating system. Finally the network software stack of the virtual machine s operating system communicates with the user space application.

In other words in conventional arrangements traffic between the user space application and the network are processed by both the operating system of the virtual machine as well as the hypervisor i.e. network software stacks of the operating system and hypervisor . This virtualization process inherently includes overhead e.g. latencies that are generally unacceptable for HPC. The arrangement of is configured to substantially reduce such virtualization overhead between HPC user space application and the network. More specifically server and the elements thereof is configured to execute double bypass virtualization such that traffic between the HPC user space application and the network skips or bypasses both the network software stack of operating system of virtual machine and the network software stack of hypervisor . In other words traffic between the HPC user space application and another network connected computing device is not processed by either the operating system or the hypervisor . The double bypass of operating system and hypervisor is represented by bi directional arrow .

Bypassing the hypervisor and the operating system is performed with different methods. The bypass of the hypervisor may involve specific hardware support from the Ethernet NIC. In general an Ethernet NIC is identified by a MAC address and performs transmission and reception of Ethernet frames through a hardware send queue SQ receive queue RQ and completion queue CQ . In one example hypervisor bypass may be supported when an Ethernet NIC is able to present multiple instances of its functionality on the Peripheral Component Interconnect PCI bus not shown of a server thereby presenting virtual NICs vNICs on the PCI bus. Each vNIC is identified by its own PCI bus address Bus Device Function 3 tuple on the PCI bus and by its own MAC address. From a protocol perspective a vNIC may be identified also by a vNIC Tag e.g. VNTag or ETag as specified by the Institute of Electrical and Electronics Engineers IEEE Std. 802.1BR in addition to its MAC address. Each vNIC controls its own set of send queues SQ receive queues RQ and completion queues CQ in registered memory pages. These vNIC queues are properly multiplexed and demultiplexed by the physical NIC to access the physical media. For example certain network interface hardware such as the Cisco Virtual Interface Card VIC is capable of presenting up to 256 virtual NICs to a PCI bus. Once the hypervisor detects that an Ethernet NIC supports virtual NICs the hypervisor may associate one or more specific vNICs to a specific virtual machine by assigning exclusive access of the vNICs PCI bus addresses to that virtual machine. In this way frame processing is performed in hardware by the Ethernet NIC rather than in software by the hypervisor.

Bypassing the operating system by an HPC user application is instead performed in software by obtaining direct access to the vNIC through one or more bootstrapping procedures. Example bootstrapping procedures that may be executed by the HPC user space application to obtain the direct access to a vNIC e.g. vNIC of the Ethernet NIC are described below with reference to . By obtaining direct access to the vNIC the HPC user space application obtains direct access to one set of the send queues SQ receive queues RQ and completion queues CQ controlled by vNIC .

In the example of when HPC user space application transmits traffic to a destination via vNIC of Ethernet NIC and the computer network the HPC user application does not use traditional sockets based application programming interface commands APIs e.g. send or write with a file descriptor . Rather HPC user space application creates one or more raw Layer 2 L2 data frames in a selected memory location that is registered with that vNIC of the Ethernet NIC i.e. registered with the network interface hardware . In other words the Ethernet NIC is able to directly access this selected registered memory location. The registration of the memory location occurs during the bootstrapping procedure s . The creation of raw L2 data frames in registered memory may be performed by a function library such as the MPI middleware rather than directly by the user code. The L2 frame created by the HPC user space application may include in certain examples a source media access control address MAC a destination MAC address an Ethertype and payload data. The source MAC address may be an address associated with the vNIC associated with HPC user space application and the destination MAC address is the address associated with the destination computing device connected to the network presumably with the vNIC associated with the HPC user space application communicating with HPC user space application . The L2 frame created by the HPC user space application may include also additional protocol headers between the basic L2 header and the payload data such as for example a VLAN Tag an IP header an UDP header etc. In another example the source destination MAC may be placed in the frame by the hardware components.

By creating raw L2 data frames in registered memory the HPC user space application is able to trigger an event such that the actual frame sending mechanics are offloaded directly to the Ethernet NIC . More specifically placed in the registered memory location is the L2 frame i.e. the frame that is to be transmitted to the destination computing device and metadata associated with the L2 frame e.g. metadata indicating that this is a send command metadata indicating the length of the L2 frame to send etc. . The event is an access to this registered memory location and a pointer to this event is also placed in the send queue of the involved vNIC of Ethernet NIC . Since vNIC of Ethernet NIC can directly read the registered memory location the Ethernet NIC can see the entire event determine that this is a send command obtain the entire L2 frame etc. As such the Ethernet NIC can then perform the operations for transmission of the L2 frame to the destination.

When the Ethernet NIC finishes transmission of the frame or sequence of frames constituting the transmitted traffic the Ethernet NIC will generate an event indicating that the transmission of the L2 frame was completed and will place this event in the completion queue of the involved vNIC . The HPC user space application or the MPI middleware may poll the completion queue and after locating this event determine that the L2 frame was successfully transmitted to the destination. As noted execution of the above process over the vNIC bypasses the operating system and the hypervisor to offload many of the sending operations to the Ethernet NIC thereby reducing latency and increasing throughput.

On the receiving side the Ethernet NIC may use the specific vNIC MAC address or other information such as a VNTag or ETag or an IP address or an UDP port etc. to steer incoming frames directly to HPC user space application . When a vNIC is associated with only one HPC user application the vNIC MAC address is sufficient to uniquely identify that HPC user application. This is the case in the example of where vNIC is associated with virtual machine and only HPC user space application is running. More specifically HPC application has a pre posted buffer to receive incoming data frames. This buffer corresponds only to the HPC application and is registered with the involved vNIC of Ethernet NIC .

When a receive data frame directed to vNIC arrives the Ethernet NIC uses the vNIC MAC address or other information such as a VNTag or ETag or an IP address or an UDP port etc. to determine if the frame is destined to HPC user application and places the frame directly into the memory of the buffer that corresponds to the vNIC associated with HPC user space application . The involved vNIC then creates an event indicating that a receive data frame directed to HPC user space application has arrived. The involved vNIC then adds this event to the completion queue and the event serves as an indication that an incoming frame is now residing in the buffer of HPC user space application . The HPC user space application polls periodically continually etc. the completion queue to determine when such an event has been added to the completion queue. Once the HPC user space application identifies an event indicating that a receive data frame is now residing in its corresponding buffer the HPC user space application retrieves the received data frame from the buffer. This processing may be performed also by the MPI middleware rather than by the user code. In this way similar to the transmission of traffic received traffic bypasses both the operating system and hypervisor by offloading most of the receiving mechanics to the Ethernet NIC thereby driving down the latency.

The transmission receipt of data frames described above significantly reduces the latency associated with conventional virtualization techniques by avoiding the entire TCP UDP stack kernel traps etc. This improved performance provides the illusion of bare metal network speeds desired in HPC applications even when running a HPC user space application in a virtual machine. More specifically in a virtualized environment the hypervisor typically acts a proxy for all network traffic. NICs are managed by the hypervisor and the virtual machines do not see the direct hardware but rather only see NIC proxy software agents that operate like the hardware. In other words when a frame is received on a NIC in a conventional arrangement the hypervisor evaluates the information in the frame to determine to which virtual machine it is directed. In such arrangements the hypervisor then passes the frame via software to the NIC proxy software agent in the target virtual machine. The NIC proxy software agent then passes the frame to the virtual machine operating system for forwarding to the user space application. This process is required in conventional arrangements because NICs are usually shared between multiple virtual machines. As such the hypervisor operates as a network traffic switch that directs incoming traffic to the correct virtual machine.

Memory may comprise read only memory ROM random access memory RAM magnetic disk storage media devices optical storage media devices flash memory devices electrical optical or other physical tangible memory storage devices. The processor is for example a microprocessor or microcontroller that executes instructions for the logic and . Thus in general the memory may comprise one or more tangible non transitory computer readable storage media e.g. a memory device encoded with software comprising computer executable instructions and when the software is executed by the processor it is operable to perform the operations described herein in connection with virtual machine logic hypervisor logic and HPC user space application logic .

Virtual machine logic is executable to provide virtual machines at server . These virtual machines may in turn execute HPC user space application logic and to provide corresponding HPC applications not shown in . For ease of reference the virtual machines of the example of will be referred to as virtual machines . Also for ease of reference the HPC user space applications of the example of will be referred to as HPC user space applications and each executed by one of the virtual machines respectively.

In the example of the HPC user space applications and are configured to communicate with one or more destinations e.g. other HPC user space applications residing on other computing devices via network interface card and a network not shown in using double bypass virtualization techniques described above with reference to . In the example of in order to execute the double bypass virtualization techniques the various virtual machines and are each independently and exclusively associated with a vNIC and respectively. HPC user space applications and executed by the virtual machines and then each obtain direct access to a set of the various queues of their associated vNIC and respectively. More specifically HPC user space application obtains access to send queue receive queue and completion queue in registered memory using one or more bootstrapping procedures. HPC user space application obtains access to send queue receive queue and completion queue in registered memory using one or more bootstrapping procedures. Finally HPC user space application obtains access to send queue receive queue and completion queue in registered memory using one or more bootstrapping procedures.

In the example of each HPC user space application and is associated with a selected memory region that is registered with the appropriate vNIC of network interface card . These selected memory regions are represented in by registered memory associated with HPC user space application registered memory associated with HPC user space application and registered memory associated with HPC user space application . Each HPC user space application and is also associated with a buffer and respectively within the respective registered memory location.

As described elsewhere herein hypervisor bypass is enabled by exclusively associating one or more vNICs with a virtual machine. In other words only that specific virtual machine can access the specific PCI bus address of that vNIC. Operating system bypass is enabled by providing a HPC application with direct access to a vNIC associated with the virtual machine executing the application. Direct access means that the HPC application can access one set of the send queues receive queues and completion queues of that vNIC. Each vNIC may have multiple sets of these queues.

More specifically in another example each virtual machine may be associated with one vNIC but is configured to run multiple HPC applications. In such examples each HPC application can obtain direct access to a different set of send queues receive queues and completion queues of the vNIC associated with its virtual machine. Additionally in these examples the MAC address alone is not sufficient to identify the HPC application and additional protocol information such as a VNTag or ETag or an IP address or an UDP port etc. is needed.

In another example each virtual machine may be associated with more than one vNIC and be configured to run multiple HPC applications. In this example each HPC application can obtain direct access to a different set of send queues receive queues and completion queues of one of the vNICs associated with its virtual machine. Again the MAC address alone is not sufficient enough to identify the HPC application and additional protocol information such as a VNTag or ETag or an IP address or an UDP port etc. is needed.

In a further example the server may include multiple physical NICs. In such examples a virtual machine may be associated with a vNIC on each physical NIC.

