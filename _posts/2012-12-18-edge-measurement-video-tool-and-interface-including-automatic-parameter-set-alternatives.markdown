---

title: Edge measurement video tool and interface including automatic parameter set alternatives
abstract: A user interface for setting parameters for an edge location video tool is provided. In one implementation, the user interface includes a multi-dimensional parameter space representation with edge zones that allows a user to adjust a single parameter combination indicator in a zone in order to adjust multiple edge detection parameters for detecting a corresponding edge. The edge zones indicate the edge features that are detectable when the parameter combination indicator is placed within the edge zones. In another implementation, representations of multiple edge features that are detectable by different possible combinations of the edge detection parameters are automatically provided in one or more windows. When a user selects one of the edge feature representation, the corresponding combination of edge detection parameters is set as the parameters for the edge location video tool.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09177222&OS=09177222&RS=09177222
owner: Mitutoyo Corporation
number: 09177222
owner_city: Kanagawa-ken
owner_country: JP
publication_date: 20121218
---
This application is a continuation in part of U.S. patent application Ser. No. 13 669 333 entitled EDGE MEASUREMENT VIDEO TOOL PARAMETER SETTING USER INTERFACE filed on Nov. 5 2012 the disclosure of which is hereby incorporated by reference in its entirety.

Precision machine vision inspection systems or vision systems for short can be utilized to obtain precise dimensional measurements of inspected objects and to inspect various other object characteristics. Such systems may include a computer a camera and optical system and a precision stage that is movable in multiple directions so as to allow the camera to scan the features of a workpiece that is being inspected. One exemplary prior art system that is commercially available is the QUICK VISION series of PC based vision systems and QVPAK software available from Mitutoyo America Corporation MAC located in Aurora Ill. The features and operation of the QUICK VISION series of vision systems and the QVPAK software are generally described for example in the QVPAK 3D CNC Vision Measuring Machine User s Guide published January 2003 and the QVPAK 3D CNC Vision Measuring Machine Operation Guide published September 1996 each of which is hereby incorporated by reference in their entirety. This product as exemplified by the QV 302 Pro model for example is able to use a microscope type optical system to provide images of a workpiece at various magnifications and move the stage as necessary to traverse the workpiece surface beyond the limits of any single video image. A single video image typically encompasses only a portion of the workpiece being observed or inspected given the desired magnification measurement resolution and physical size limitations of such systems.

Machine vision inspection systems generally utilize automated video inspection. U.S. Pat. No. 6 542 180 the 180 patent teaches various aspects of such automated video inspection and is incorporated herein by reference in its entirety. As taught in the 180 patent automated video inspection metrology instruments generally have a programming capability that allows an automatic inspection event sequence to be defined by the user for each particular workpiece configuration. This can be implemented by text based programming for example or through a recording mode which progressively learns the inspection event sequence by storing a sequence of machine control instructions corresponding to a sequence of inspection operations performed by a user with the aid of a graphical user interface or through a combination of both methods. Such a recording mode is often referred to as learn mode or training mode or record mode. Once the inspection event sequence is defined in learn mode such a sequence can then be used to automatically acquire and additionally analyze or inspect images of a workpiece during run mode. 

The machine control instructions including the specific inspection event sequence i.e. how to acquire each image and how to analyze inspect each acquired image are generally stored as a part program or workpiece program that is specific to the particular workpiece configuration. For example a part program defines how to acquire each image such as how to position the camera relative to the workpiece at what lighting level at what magnification level etc. Further the part program defines how to analyze inspect an acquired image for example by using one or more video tools such as edge boundary detection video tools.

Video tools or tools for short and other graphical user interface features may be used manually to accomplish manual inspection and or machine control operations in manual mode . Their set up parameters and operation can also be recorded during learn mode in order to create automatic inspection programs or part programs . Video tools may include for example edge boundary detection tools autofocus tools shape or pattern matching tools dimension measuring tools and the like.

Various methods are known for locating edge features in workpiece images. For example various algorithms are known which apply brightness gradient operators to images which include an edge feature to determine its location e.g. a Canny Edge detector or a differential edge detector. Such edge detection algorithms may be included in the machine vision inspection systems which also use carefully configured illumination and or special image processing techniques to enhance brightness gradients or otherwise improve edge location accuracy and repeatability.

Some machine vision systems e.g. those utilizing the QVPAK software described above provide edge location video tools which have adjustable parameters for an edge detection algorithm. In certain implementations the parameters may initially be determined for an edge on a representative workpiece during a learn mode operation and then utilized during a run mode operation to find the corresponding edge of a similar workpiece. When desirable edge detection parameters are difficult or impossible to determine automatically during the learn mode the user may choose to adjust the parameters manually. However certain edge detection parameters e.g. thresholds such as TH THR and THS outlined herein are considered to be difficult to understand for the majority of users e.g. relatively unskilled users and how their adjustment affects a particular edge detection operation is considered difficult to visualize particularly for a combination of parameters. The adjustments of the parameters may be further complicated by the variety of edge conditions and workpiece materials in part to part variations encountered when programming and using general purpose machine vision inspection system. An improved method and system that allows relatively unskilled users to adjust the parameters of edge location video tools so that they can be used to reliably inspect a variety of types of edges would be desirable.

A method for defining edge detection parameters in a machine vision inspection system user interface is provided. In one embodiment the method begins by providing an edge location video tool having a plurality of edge detection parameters. A region of interest ROI is defined for the edge location video tool including at least a first respective edge feature in the region of interest. A plurality of candidate parameter combinations of the plurality of edge detection parameters are automatically determined wherein respective candidate parameter combinations are usable to detect respective edge features in the region of interest. The user interface is operated to display respective candidate parameter combination representations corresponding to the respective candidate parameter combinations usable to detect respective edge features in the region of interest. The user interface is configured to allow a user to perform a parameter combination selection action relative to the displayed respective candidate parameter combination representations. The user selection action results in a selection of a combination of edge detection parameters that govern operation of the edge location video tool in the defined region of interest.

The user interface may comprise a multi dimensional parameter space representation of possible combinations of the plurality of edge detection parameters. The step of determining a plurality of candidate parameter combinations may comprise determining a first set of respective candidate parameter combinations usable to detect the first respective edge feature. The step of operating the user interface may comprise displaying a representation of a first set of respective candidate parameter combinations as a first zone in the multi dimensional parameter space representation. The user interface may be configured to allow a user to perform a parameter combination selection action relative to the first zone comprising moving a parameter combination indicator to a location within the first zone and operating the parameter combination indicator to select the parameter combination corresponding to that location.

The region of interest ROI may include at least a second respective edge feature in the region of interest. The step of determining a plurality of candidate parameter combinations may comprise determining a second set of respective candidate parameter combinations usable to detect the second respective edge feature. The step of operating the user interface may comprise displaying a representation of the second set of respective candidate parameter combinations as a second zone in the multi dimensional parameter space representation.

The user interface may comprise at least one edge feature representation window that includes an image of the field of view of the machine vision inspection system and a representation of the edge features detectable by the combination of parameters indicated by a current configuration of the parameter combination indicator superimposed on the image of the field of view. The representation of the edge features detectable by the combination of parameters indicated by the current configuration of the parameter combination indicator may comprise a plurality of detectable edge points corresponding to a plurality of scan lines across the ROI superimposed on the image of the field of view. The at least one edge feature representation window and the multi dimensional parameter space representation may be synchronized such that an adjustment or selection in the at least one edge feature representation window results in a corresponding indication in the multi dimensional parameter space representation.

The region of interest ROI may include at least a second respective edge feature in the region of interest. The user interface may comprise a representation of the first and second respective edge features. The step of automatically determining a plurality of candidate parameter combinations may comprise determining at least a first respective candidate parameter combination usable to detect the first respective edge feature and determining at least a second respective candidate parameter combination usable to detect the second respective edge feature. The step of operating the user interface may comprise displaying the representation of the first and second respective edge features.

The user interface may be configured to allow a user to perform a parameter combination selection action relative to the displayed representations of the first and second respective edge features comprising moving a cursor to a location proximate to the representation of a desired one of the first and second respective edge features and operating the cursor to select the parameter combination corresponding to that edge. The representation of the first and second respective edge features may comprise a first indicator superimposed on an image of the first respective edge feature indicating that a corresponding first respective candidate parameter combination has been determined and a second indicator superimposed on an image of the second respective edge feature indicating that a corresponding second respective candidate parameter combination has been determined. The first and second indicators may comprise detected edge points along the first and second edge features respectively.

The representation of the first respective edge feature may comprise a first edge window including the first respective edge feature indicating that a corresponding first respective candidate parameter combination has been determined and the representation of the second respective edge feature may comprise a second edge window including the second respective edge feature indicating that a corresponding second respective candidate parameter combination has been determined. The user interface may be configured to allow a user to perform a parameter combination selection action relative to the displayed representations of the first and second respective edge features comprising moving a cursor to a location in a desired one of the first and second edge windows and operating the cursor to select the parameter combination corresponding to that edge.

The user interface may further comprise a multi dimensional parameter space representation of possible combinations of the plurality of edge detection parameters. The user interface may be configured to allow a user to perform a parameter combination selection action relative to the displayed representations of the first and second respective edge features. The selection action may comprise moving a cursor to a location proximate to the representation of a desired one of the first and second respective edge features and operating the cursor to select either the first or second respective edge feature. In response to the selection the user interface may be further configured to display a representation of a first or second zone. More specifically a representation of a first set of respective candidate parameter combinations may be displayed as a first zone in the multi dimensional parameter space representation if the first respective edge feature is selected wherein the first set of respective parameter combinations comprises the first respective candidate parameter combination usable to detect the first respective edge feature in addition to other candidate parameter combinations usable to detect the first respective edge feature. Alternatively a representation of a second set of respective candidate parameter combinations may be displayed as a second zone in the multi dimensional parameter space representation if the second respective edge feature is selected wherein the second set of respective parameter combinations comprises the second respective candidate parameter combination usable to detect the second respective edge feature in addition to other candidate parameter combinations usable to detect the second respective edge feature.

In one embodiment a method for defining edge location parameters in a machine vision inspection system user interface is provided. A plurality of edge detection parameters for a region of interest ROI of an edge location video tool is defined. A multi dimensional parameter space representation is displayed which indicates possible combinations of the plurality of edge detection parameters. In one implementation the multi dimensional parameter space representation is a two dimensional grid with each dimension indicating possible values corresponding to one of the edge detection parameters. A parameter combination indicator e.g. including a parameter combination marker that can be selected and dragged in a user interface is located within the multidimensional parameter space representation which indicates a combination of the edge detection parameters based on its location. One or more edge feature representation windows are displayed which represent edge features located in the ROI of the edge location video tool. In one embodiment edge features detectable by the combination of edge detection parameters indicated by a current configuration of the parameter combination indicator are automatically updated in the one or more edge feature representation windows. It should be appreciated that the term window used herein includes previously known types of user interface windows and also refers more generally to unconventional elements of a user interface that may include one or more user interface characteristics such as they may include display elements made more compact than an entire display area and or may be hidden at some times e.g. as resized and or relocated and or hidden by a user they may focus on a particular class of information and or menus or selections related to a particular class of information and so on. Thus particular forms of windows illustrated herein are exemplary only and not limiting. For example in some embodiments a window may not have a well defined limiting boundary or the like it may have hyper link like behavior it may appear on a separate and or isolated display element and so on.

The edge feature representation windows may include representations of a scanline intensity and or scanline intensity gradient of the region of interest of the edge location video tool. Another edge feature representation window may include an image of the field of view of the machine vision inspection system. A representation of one or more of the edge features detectable by the combination of parameters indicated by a current configuration of the parameter combination indicator may be superimposed on the representation of the scanline intensity and or scanline intensity gradient and or the image of the field of view.

The edge feature representation windows and the multi dimensional parameter space representation may be synchronized such that a parameter adjustment or selection in one of the edge feature representation windows results in a corresponding adjustment or selection of the parameter indicator e.g. its position in the multi dimensional parameter space representation. The adjustment or selection in the edge feature representation window may comprise an adjustment or selection of a threshold level and the corresponding indication in the multi dimensional parameter space representation may comprise a movement of the parameter combination indicator to a location which corresponds to the selected threshold level.

Various embodiments of the invention are described below. The following description provides specific details for a thorough understanding and an enabling description of these embodiments. One skilled in the art will understand however that the invention may be practiced without many of these details. In addition some well known structures or functions may not be shown or described in detail so as to avoid unnecessarily obscuring the relevant description of the various embodiments. The terminology used in the description presented below is intended to be interpreted in its broadest reasonable manner even though it is being used in conjunction with a detailed description of certain specific embodiments of the invention.

Those skilled in the art will appreciate that the controlling computer system may generally consist of any computing system or device. Suitable computing systems or devices may include personal computers server computers minicomputers mainframe computers distributed computing environments that include any of the foregoing and the like. Such computing systems or devices may include one or more processors that execute software to perform the functions described herein. Processors include programmable general purpose or special purpose microprocessors programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such devices. Software may be stored in memory such as random access memory RAM read only memory ROM flash memory or the like or a combination of such components. Software may also be stored in one or more storage devices such as magnetic or optical based disks flash memory devices or any other type of non volatile storage medium for storing data. Software may include one or more program modules which include routines programs objects components data structures and so on that perform particular tasks or implement particular abstract data types. In distributed computing environments the functionality of the program modules may be combined or distributed across multiple computing systems or devices and accessed via service calls either in a wired or wireless configuration.

The vision measuring machine includes a moveable workpiece stage and an optical imaging system which may include a zoom lens or interchangeable lenses. The zoom lens or interchangeable lenses generally provide various magnifications for the images provided by the optical imaging system . The machine vision inspection system is generally comparable to the QUICK VISION series of vision systems and the QVPAK software discussed above and similar state of the art commercially available precision machine vision inspection systems. The machine vision inspection system is also described in commonly assigned U.S. Pat. Nos. 7 454 053 7 324 682 8 111 905 and 8 111 938 which are each incorporated herein by reference in their entireties.

The optical assembly portion is controllably movable along a Z axis that is generally orthogonal to the X and Y axes by using a controllable motor that drives an actuator to move the optical assembly portion along the Z axis to change the focus of the image of the workpiece . The controllable motor is connected to the input output interface via a signal line .

A workpiece or a tray or fixture holding a plurality of workpieces which is to be imaged using the machine vision inspection system is placed on the workpiece stage . The workpiece stage may be controlled to move relative to the optical assembly portion such that the interchangeable objective lens moves between locations on a workpiece and or among a plurality of workpieces . One or more of a stage light a coaxial light and a surface light e.g. a ring light may emit source light and or respectively to illuminate the workpiece or workpieces . The light source may emit light along a path including a mirror . The source light is reflected or transmitted as workpiece light and the workpiece light used for imaging passes through the interchangeable objective lens and the turret lens assembly and is gathered by the camera system . The image of the workpiece s captured by the camera system is output on a signal line to the control system portion . The light sources and may be connected to the control system portion through signal lines or busses and respectively. To alter the image magnification the control system portion may rotate the turret lens assembly along axis to select a turret lens through a signal line or bus .

As shown in in various exemplary embodiments the control system portion includes a controller the input output interface a memory a workpiece program generator and executor and a power supply portion . Each of these components as well as the additional components described below may be interconnected by one or more data control buses and or application programming interfaces or by direct connections between the various elements.

The input output interface includes an imaging control interface a motion control interface a lighting control interface and a lens control interface . The motion control interface may include a position control element and a speed acceleration control element although such elements may be merged and or indistinguishable. The lighting control interface includes lighting control elements and which control for example the selection power on off switch and strobe pulse timing if applicable for the various corresponding light sources of the machine vision inspection system .

The memory may include an image file memory portion an edge detection memory portion a workpiece program memory portion that may include one or more part programs or the like and a video tool portion . The video tool portion includes video tool portion and other video tool portions e.g. which determine the GUI image processing operation etc. for each of the corresponding video tools and a region of interest ROI generator that supports automatic semi automatic and or manual operations that define various ROIs that are operable in various video tools included in the video tool portion .

In the context of this disclosure and as known by one of ordinary skill in the art the term video tool generally refers to a relatively complex set of automatic or programmed operations that a machine vision user can implement through a relatively simple user interface e.g. a graphical user interface editable parameter windows menus and the like without creating the step by step sequence of operations included in the video tool or resorting to a generalized text based programming language or the like. For example a video tool may include a complex pre programmed set of image processing operations and computations which are applied and customized in a particular instance by adjusting a few variables or parameters that govern the operations and computations. In addition to the underlying operations and computations the video tool comprises the user interface that allows the user to adjust those parameters for a particular instance of the video tool. For example many machine vision video tools allow a user to configure a graphical region of interest ROI indicator through simple handle dragging operations using a mouse in order to define the location parameters of a subset of an image that is to be analyzed by the image procession operations of a particular instance of a video tool. It should be noted that the visible user interface features are sometimes referred to as the video tool with the underlying operations being included implicitly.

In common with many video tools the edge location and parameter setting subject matter of this disclosure includes both user interface features and underlying image processing operations and the like and the related features may be characterized as features of an edge location tool and corresponding parameter setting portion included in the video tool portion . The edge location tool may utilize an algorithm for determining edge locations. The algorithm may be governed by edge detection parameters which may be determined and programmed automatically in some cases during learn mode and or manually adjusted by a user e.g. thresholds such as TH THR and THS described in greater detail below. 

In one implementation in order that a user may manually set edge detection video tool parameters as outlined above the parameter setting portion provides a multi dimensional parameter space representation e.g. a 2 dimensional grid with TH on one axis and THS on the other axis . A parameter marker or indicator e.g. cursor is provided that can be moved within the parameter space representation by a user to adjust or select a desired parameter combination e.g. of TH and THS . Parameter combination edge zones are provided within the parameter space representation which indicate locations where the parameter indicator may be positioned to detect certain edge features. One or more edge feature representation windows e.g. showing a scanline intensity and or a scanline intensity gradient and or a field of view of the machine vision system are provided which illustrate changes to the parameters and or the edge features that are detectable according to the current configuration as will be described in more detail below with respect to and . The system may also or alternatively automatically scan the search space of the edge detection parameters and generate one or more images showing meaningful variations of the edge detection results that occur in response to the changing edge detection parameter values. A user interface display may be provided showing the different meaningful variations that a user can select from e.g. by clicking on a window or an edge feature as will be described in more detail below with respect to .

The signal lines or busses and of the stage light the coaxial lights and and the surface light respectively are all connected to the input output interface . The signal line from the camera system and the signal line from the controllable motor are connected to the input output interface . In addition to carrying image data the signal line may carry a signal from the controller that initiates image acquisition.

One or more display devices e.g. the display of and one or more input devices e.g. the joystick keyboard and mouse of can also be connected to the input output interface . The display devices and input devices can be used to display a user interface which may include various graphical user interface GUI features that are usable to perform inspection operations and or to create and or modify part programs to view the images captured by the camera system and or to directly control the vision system components portion . The display devices may display user interface features associated with the edge location video tool and parameter setting portion described in greater detail below.

In various exemplary embodiments when a user utilizes the machine vision inspection system to create a part program for the workpiece the user generates part program instructions by operating the machine vision inspection system in a learn mode to provide a desired image acquisition training sequence. For example a training sequence may comprise positioning a particular workpiece feature of a representative workpiece in the field of view FOV setting light levels focusing or autofocusing acquiring an image and providing an inspection training sequence applied to the image e.g. using an instance of one of the video tools on that workpiece feature . The learn mode operates such that the sequence s are captured or recorded and converted to corresponding part program instructions. These instructions when the part program is executed will cause the machine vision inspection system to reproduce the trained image acquisition and inspection operations to automatically inspect that particular workpiece feature that is the corresponding feature in the corresponding location on a run mode workpiece or workpieces which matches the representative workpiece used when creating the part program.

In the embodiment shown in the edge detection parameter window includes scan line intensity and scan line gradient windows and respectively as well as a multi dimensional parameter space representation of possible combinations of a plurality of edge detection parameters also referred to as edge characteristic parameters described further below. The scan line intensity window and scan line gradient window illustrate graphs of a scanline intensity profile IP and a scanline intensity gradient profile GP at pixel locations along the scanline direction for a representative scanline e.g. a central or average scanline or the like across the region of interest and each provides edge feature representations ER of edge features located in the region of interest of the edge location video tool . As used herein an edge feature representation is some form of representation that a user may understand as indicating an edge. In the scan line intensity window edge features are understood to be represented by significant changes in intensity over a relatively limited distance along the scan line. Typical rising edge and a falling edge feature representations ER one of several cases are indicated on the scanline intensity profile IP for example. In the scan line intensity gradient window edge features are understood to be represented by significant gradient changes over a relatively limited distance along the scan line and or gradient peaks or valleys . Typical positive and negative gradient edge feature representations ER one of several cases are indicated on the scanline intensity gradient profile GP for example. Of course in the field of view window edge features are understood to be represented by their image as indicated by the edge feature representations ER for example. Accordingly any or all of the windows or the like may be referred to as an edge representation window. The scanline intensity gradient profile GP may be understood to indicate the slope of the scanline intensity profile IP in this embodiment thus they will generally have corresponding edge feature representations at corresponding locations along their respective profiles.

In the embodiment shown in the multi dimensional parameter space representation includes a two dimensional graph showing potential combinations of edge detection parameters TH and THS with a current combination of the parameters indicated by the location of a parameter combination indicator PCI. The edge features detectable by the current combination of the edge detection parameters TH and THS are displayed and automatically updated in the windows and as described for one exemplary embodiment below.

The edge detection parameters TH and THS are edge detection parameters for an edge detection algorithm of the edge location video tool . In one embodiment these and other settings may be determined during a learn mode of the video tool and then utilized during a run mode for determining edges. When desirable settings are not able to be determined during the learn mode or when the edge points found by the video tool are determined to not be satisfactory the user may choose to adjust these settings manually. Some settings for video tools may be intuitive and readily adjustable. However other settings e.g. for the edge detection parameters TH and THS are sometimes considered to be relatively complicated and difficult to adjust particularly in combination.

The parameters may provide various functions in governing the algorithm. For example in some cases the parameters may provide a failsafe type function. That is a parameter that requires a minimum level of brightness change across an edge may prevent an edge detection video tool from returning an edge location in the case of unexpectedly low exposure e.g. due to a lighting failure or other anomalous condition. The parameter TH referred to herein defines a threshold that is related to a minimum level of brightness change required across an edge. In another case a parameter that requires a minimum level of brightness rate of change across an edge e.g. a gradient value which may characterize a width or sharpness of an edge may further characterize a particular instance of an edge and may prevent an edge detection video tool from returning an edge location in the case of an unexpected change in the form of an edge or its illumination e.g. an ambient illumination change or direction change or the focus of its image a blurry image broadens and softens an edge relative to the learn mode edge formation or illumination that was used for the initial training programming of the video tool. The parameter THS referred to herein defines a threshold that is related to a minimum brightness gradient required across an edge. It will be appreciated that each of the parameters outlined above and particularly their combination may be set to correspond to and or characterize a prototype instance of an edge during learn mode to increase the edge detection reliability and or specificity the detection of the expected edge using the expected imaging conditions . The parameters may be set discriminate for a particular edge or may cause the failure of the video tool when the expected conditions are not fulfilled or nearly fulfilled . In some embodiments a video tool may be set such that all the parameters are static resulting in video tool failure unless the expected conditions are strictly reproduced. In some embodiments a parameter THR referred to in the incorporated references may define a relationship between THS and TH and or a threshold value for that relationship such that video tool may set to adjust some of the parameters e.g. THS dynamically based on the actual brightness of an image provided that the brightness falls in a range deemed to provide a reasonable image for inspection resulting in a video tool that fails less often due to expected lighting variations and or part finish variations or the like.

In some cases a number of edges may be crowded together on a workpiece such that a target edge cannot be reliably isolated by the location and size of a video tool region of interest. In such cases the parameters outlined above and particularly their combination may be set at levels that are satisfied by a target edge including expected workpiece to workpiece variations and not satisfied by other nearby edges on the workpiece such that the video tool discriminates the target edge from the other edges during inspection and measurement operations. It should be appreciated that the inventive features disclosed herein are of particular value for setting a combination of parameters that are useful in this latter case as well as more generally providing improved ease of use and understanding for users.

The intensity window shows an intensity profile IP along a scanline of the edge detection video tool with an adjustable TH line . Similarly the gradient window shows a gradient profile GP along the same scanline of the edge detection video tool with an adjustable THS line . The windows and are configured to include operations wherein a user is able to select and adjust the parameter value of the TH line and the THS line graphically e.g. by dragging the lines without having to edit the TH and THS text boxes and respectively. This type of display and functionality may be particularly useful for experienced users for whom the adjustment may be easier and faster than utilizing the prior text box and methods. The location of the parameter combination indicator PCI and the TH and THS text boxes may be updated in real time in response to such a line adjustment. A disadvantage of only utilizing the adjustable lines and is that only one edge detection parameter may be adjusted at a time and less experienced users may not necessarily know how to interpret the raw intensity profile IP and the gradient profile GP illustrated in the edge feature representation windows and .

As illustrated in the windows and in order to increase user understanding of the edge discrimination effect of the TH and THS parameter values in one embodiment the windows and GUI are configured such that detectable edges DE may be indicated in those windows that is the corresponding detectable edge representations along the intensity profile IP and the gradient profile GP may be indicted . In the case shown in in the gradient window segments of the profile GP that fall below the threshold value for the gradient parameter THS are shaded out. The corresponding segments are shaded out in the intensity window as well. In addition the portions of the intensity profile that fall below the threshold value for the intensity parameter TH are shaded out in the intensity window . Thus three edge representations that satisfy the current combination of parameters are highlighted or indicated as detectable edges in the windows and by the unshaded areas marked as DE DE and DE in . Such visual indications assist users with understanding how changes in the edge detection parameters TH and THS separately and in combination affect the determination of edges and provide a real time indication of how the algorithm is working. In one embodiment not shown detectable edge indicators may also be superimposed on the corresponding edges in the field of view window as well. As shown in because the detectable edge number to select box is set to the default value of 1 meaning the edge to be located is the first detectable edge in the ROI along the scan line direction the edge selected ES is set for the video tool in the window to the edge corresponding to DE. The detectable edge DE may be marked as the selected edge in the window as well in some embodiments.

In contrast to the individual adjustments of the TH and THS lines and in the windows and the multi dimensional parameter space representation allows a user to adjust both of the thresholds TH and THS at the same time. In the graph the edge detection parameter TH is represented along the x axis while the edge detection parameter THS is represented along the y axis. The indicator PCI may be selected and dragged by the user to any location on the graph and the current location will define the current TH and THS values. Experiments have shown that by using various features of this user interface even relatively unskilled users can rapidly explore and optimize parameter combinations that reliably isolate particular edges using the various features outlined above or just as importantly help them understand that an edge cannot be reliably isolated without special conditions e.g. by selecting a particular detectable edge in the region of interest. 

As an illustrative example for the operation of the user interface in as shown in the indicator PCI in the graph has been moved to a new location e.g. by dragging in the user interface . In the embodiment of the windows and are synchronized with the multi dimensional parameter space representation such that an adjustment in the position of the indicator PCI results in a corresponding adjustment in the level of the TH line and the THS line and vice versa. Thus in accordance with the new location of the indicator PCI the TH line in the window and the THS line in the window have also been correspondingly adjusted. In any case as a result of the particular new levels defined for the parameters TH and THS an additional edge indicated by the reference number DE in the windows and is now indicated as a detectable edge. It will be understood by inspection of the figures that the detectable edge DE is a weaker edge than the edges DE DE and DE and cannot be isolated in the region of interest based solely on the parameters TH and THS. However it is readily observable that it is the second detectable rising edge along the scan line and so the user has set the detectable edge number to select box to a value of 2. Accordingly the parameters TH and THS isolate the relatively strong detectable edges DE DE while rejecting weaker edges and or noise and the detectable edge number selector further refines the desired edge to be located the second detectable edge along the scan line DE using the current set of parameters. In one embodiment at least one of the windows and and or the window and the detectable edge number e.g. as shown in the detectable edge number to select box are synchronized such that a user may select an indicated detectable edge in one of the windows and a corresponding parameter selection is automatically made for the number of the detectable edge along the scan line in the region of interest.

It will be appreciated that one advantage of the multi dimensional parameter space representation is that it allows the user to adjust multiple parameters e.g. edge detection parameters TH and THS at the same time to rapidly explore the detection margins and other detection reliability tradeoffs e.g. detection of an incorrect edge vs. the likelihood of tool failure associated with various combinations of settings. The user need not understand the functions of the various parameters because by adjusting the location of indicator PCI and observing the real time feedback of a corresponding detectable edge indication the user intuitively feels the sensitivity of the edge detection results to the location indicator PCI and can intuitively set it in the best spot to produce the desired edge detection. Just as importantly the user may rapidly scan all combinations of parameters by simply sweeping the indicator PCI and learn that no particular combination isolates a target edge and determine that additional parameters e.g. the detectable edge number to select box may need to be set or the lighting may need to be changed or the region of interest adjusted or the like. In contrast it is impractical or impossible to make this same determination with the same efficiency and certainty using prior art methods and interfaces for setting edge detection parameters.

While the techniques described above with respect to the multi dimensional parameter space representation allow a user to adjust multiple edge detection parameters at the same time it may still be difficult to precisely predict where the indicator PCI will need to be located for the detection of certain edges. As will be described in more detail below candidate parameter combination edge zones may be provided to assist with the determination of desirable positions for the indicator PCI.

As shown in within the multi dimensional parameter space representation a candidate parameter combination edge zone EZ represents a region in which the indicator PCI can be placed for detecting the first detectable edge DE. Similarly a candidate parameter combination edge zone EZ represents an area in which the indicator PCI can be placed for detecting the second detectable edge DE and a candidate parameter combination edge zone EZ represents an area in which the indicator PCI can be placed for detecting the third detectable edge DE. A fourth candidate parameter combination edge zone EZ for detecting the fourth detectable edge DE may be considered to completely overlap with the first candidate parameter combination zone EZ. It will be appreciated that the candidate parameter combination edge zones EZ EZ EZ and EZ may all include overlapping areas in which multiple edges may be detected and may in some implementations be illustrated with colors or other visible features.

As shown in to illustrate the use and operation of this feature the indicator PCI has been placed in a location that is within the candidate parameter combination edge zones EZ EZ and EZ but not within the candidate parameter combination edge zone EZ. As a result as illustrated in the windows and and as described above only the edges DE DE and DE are potentially detectable by the indicated candidate parameter combination. As shown in the indicator PCI has been moved to a new location where it is within the candidate parameter combination edge zone EZ as well as being within the candidate parameter combination edge zones EZ EZ and EZ. As a result each of the edges DE DE DE and DE are potentially detectable by the indicated parameter combination as are indicated in the windows and and as described above.

It will be appreciated that the display of the candidate parameter combination edge zones EZ EZ EZ and EZ in this manner allows a user to know where to position the indicator PCI so as to be able to discriminate a desired edge from other edges if possible and also what possibilities exist within the multi dimensional parameter space representation for detecting edges. In one embodiment the candidate parameter combination edge zones EZ EZ EZ and EZ are synchronized with one or more of the windows and wherein a user may select an edge feature to highlight or display a corresponding edge zone EZ. For example in one of the windows or a user may select e.g. click on a detectable edge feature DE or an edge representation ER in response to which the corresponding candidate parameter combination edge zone EZ is highlighted and or displayed individually without the other edge zones EZ.

As shown in at a block an intensity profile and an edge strength profile e.g. a gradient profile are determined along a scan line s in the region of interest. At a block respective groups of pixels along a scan line s corresponding to respective edges in the region of interest are determined e.g. by identifying a group of adjacent pixels that exhibit a significant intensity increase decrease for a rising falling edge . At a block for each respective group the range of parameter combinations e.g. TH THS combinations exhibited by the pixels in that group are determined.

At a block the determined range of parameter combinations for a respective group or edge are mapped or represented as candidate parameter combinations represented as a range or zone of selectable candidate parameter combinations in a user interface e.g. as a candidate parameter combination edge zone such that a user may select a parameter combination in the represented range or zone in order to detect the corresponding respective edge.

As previously indicated in the field of view window edge features are understood to be represented by their image as indicated by the edge feature representations ER for example. The multi dimensional parameter space representation includes a two dimensional graph showing potential combinations of edge detection parameters TH and THS with a current combination of the parameters indicated by the location of a parameter combination indicator PCI. Parameter combination edge zones EZ EZ EZ and EZ are indicated within the multi dimensional parameter space representation . In the embodiment shown in the scan line intensity window and scan line gradient window illustrate graphs of a scanline intensity profile IP and a scanline intensity gradient profile GP at pixel locations along the scanline direction for a representative scanline e.g. a central or average scanline or the like across the region of interest. The edge features detectable by the current combination of the edge detection parameters TH and THS the detectable edges DE are displayed and automatically updated in the windows and as previously described. In the field of view window detected edge points DEP that satisfy the current combination of parameters are indicated as described in greater detail below.

In the case shown in in comparison to a user has repositioned the parameter combination indicator PCI to be within the parameter combination edge zones EZ EZ and EZ and such that the edge detection parameter TH has a value 98.7. The three edge representations that satisfy the current combination of parameters are highlighted or indicated as detectable edges in the windows and by the unshaded areas marked as DE DE and DE.

An important feature added in user interface display in comparison to the user interface display is that in the field of view window detected edge points DEP that satisfy the current combination of parameters are indicated. This provides more information than the representations in the user interface display . For example it may be seen in the scan line intensity window that the parameter TH is set such that the detectable edge DE of the representative scan line that is illustrated in the window barely exceeds the parameter TH. However importantly the detected edge points DEP in the field of view window indicate that along only a few of the scan lines does the detectable edge DE exceeds the parameter TH. Thus the detected edge points DEP in the field of view window also indicate that along some of the scan lines the first rising edge that exceeds the parameter TH correspond to the detectable edge DE. Such visual indications assist users with understanding how changes in the edge detection parameters TH and THS separately and in combination affect the determination of edges and provide a real time indication of how the algorithm is working. In the case shown in the detected edge points DEP clearly and immediately illustrate that the current parameters do not distinguish reliably between the edges DE and DE.

In contrast as shown in the indicator PCI in the graph has been moved to a new location e.g. by dragging in the user interface to only be within the parameter combination edge zone EZ. In the embodiment of the windows and and are all synchronized with the multi dimensional parameter space representation such that an adjustment in the position of the indicator PCI results in a corresponding adjustment in the level of the TH line and the THS line as well as the detected edge points DEP. As a result of the particular new levels defined for the parameters TH and THS only the edge indicated by the reference number DE in the windows and is now indicated as a detectable edge. Perhaps more importantly it may be seen in the window that the detected edge points DEP are also now all located at the corresponding detectable edge DE conveying the information that the current set of parameters distinguish the edge DE from all other edges along all scan lines not just along the representative scan line in the windows and . In fact a user may perhaps drag the PCI and set the parameters more reliably by observing the corresponding detected edge points DEP than by observing the windows and in some embodiments. Thus in some embodiments where detected edge points DEP are illustrated in the field of view window the windows and may be optional or omitted and or hidden unless selected for display by the user.

The field of view windows A D primarily differ from one another in accordance with the locations of detected edge points DEP. As shown in the window A the detected edge points DEP are located on a first detectable edge DE. In contrast in the window B the detected edge points DEP are located on a second detectable edge DE . Similarly in the window C the detected edge points DEP are located on a third detectable edge DE and in the window D the detected edge points DEP are located on a fourth detectable edge DE. A parameter indicator box is provided which indicates parameter values TH and THS a parameter combination corresponding to the current selected field of view window. In one embodiment the candidate parameter combination indicated in the indicator box which may result in the particular displayed detected edge points DEP may be an estimated best combination for detecting the corresponding edge e.g. in one embodiment a median or average parameter combination from a parameter combination range exhibited by the edge points of the corresponding edge . As described above with respect to the example of different parameter combinations of the edge detection parameters TH and THS may result in different detected edge points DEP. As will be described in more detail below each of the field of view windows A D represents a different candidate parameter combination of the edge detection parameters TH and THS and or other edge parameters such as an edge number parameter in some embodiments which results in the different locations of the detected edge points DEP.

In one embodiment in order to produce the field of view windows A D the system automatically scans the search space of the edge detection parameters and generates images e.g. field of view windows showing meaningful variations of the edge detection results that occur in response to candidate edge detection parameter combinations. As will be described in more detail below with respect to this process may be performed utilizing various techniques e.g. an exhaustive parameter space search a feature guided search etc. . In an alternative embodiment a single field of view window may be provided wherein different detectable edges are indicated in the single field of view e.g. by color coding of detected edge points at each edge or superimposed numbering or the like . A user is then able to simply choose the desirable candidate parameter combination e.g. by clicking on a field of view window or a selected edge feature in order to set the corresponding edge detection parameters for the video tool. It will be appreciated that by displaying the results of various possible candidate edge detection combinations a user is also effectively shown what the system is capable of detecting in a given region of interest which allows a user to make informed choices regarding the edges to be detected.

It will be appreciated that the embodiment illustrated by simplifies the edge detection threshold setting process and may reduce user frustration. In other words in previous systems where a user was required to manually set individual edge detection parameters the process could often be confusing and time consuming. Even with the improved methods described above with respect to in particular in an implementation where the parameter combination edge zones are not indicated the adjustments to the edge detection parameters may still be somewhat confusing in that the mechanism of how the thresholds affect the edge location is not intuitive and is often understood by only the most advanced users.

In one embodiment after a user chooses the desired candidate parameter combination e.g. the user clicks on one of the field of view windows A D the user may then further fine tune the parameter settings e.g. within a wider range of candidate parameter combinations exhibited by the corresponding edge points utilizing the user interface and techniques described above with respect to . In certain implementations allowing a user to fine tune the settings may be beneficial to certain edge detection techniques in particular those that do not utilize feature detection e.g. line arc etc. in the edge detection algorithm. For the fine tuning in certain implementations the user may drag the edge in the image or the edge selector as illustrated in directly from which the algorithm will automatically relearn the edge detection parameters. For certain edges that are not detectable when they are shadowed by a previous edge along a scan line direction a user may be provided with directions to move or alter a GUI element to exclude the shadowing edge or a suggestion to utilize a different tool such as an edge selection tool.

At a block for each combination of parameter values the edge detection algorithm is run in the specified tool region of interest wherein it is assumed that there are N different combinations that produce valid results. In one implementation combinations that produce very small point buffers e.g. relatively few points may be discarded. A threshold may be utilized for the minimum number of points that a point buffer is expected to deliver. At a block the resulting 2D point buffer is stored as a cluster i.e. a set of points and the relevant algorithm parameters are stored as metadata of the cluster. In one implementation the blocks may be repeated for all N combinations of parameter values.

At a block given N clusters point buffers produced by the previous steps hierarchical agglomerative clustering is performed to reduce the number of clusters to a number K. In one implementation the hierarchical agglomerative clustering algorithm may operate in the following manner. The process begins with N clusters each consisting of one 2D point buffer returned by the edge detection algorithm. Then the two most similar clusters A and B are found and are merged into one cluster if they are similar enough this process is repeated up to N 1 times if needed. If no two clusters are similar enough the clustering process ends. If a tie occurs i.e. two pairs of clusters have equal similarity the first pair that is found is merged. In one implementation the similarity of two clusters may be measured using a distance measure as will be described in more detail below. In one implementation the similar enough requirement is determined according to whether the distance between the two clusters is smaller than a predefined distance threshold.

Various distance measures may be utilized for agglomerative clustering. In one implementation a Hausdorff distance i.e. a maximum distance of a set to the nearest point in the other set is utilized e.g. see http cgm.cs.mcgill.ca godfried teaching cg projects 98 normand main.html. In another implementation a modified Hausdorff like distance DH may be more suitable to reduce sensitivity to outliers in point buffers wherein D A B fraction of points in cluster A that are not close enough to the nearest point in cluster B not close enough meaning the Euclidean distance between points is larger than a predefined threshold D B A fraction of points in cluster B that are not close enough to the nearest point in cluster A and DH maxD A B D B A . Other cluster distance measures may also be utilized e.g. average linkage or minimum variance such as Ward s method etc. 

Returning to at a block for each of the K clusters the representative values of the edge detection parameters are computed. In one implementation since each cluster A of the K final clusters is an aggregate of one or more of the original clusters i.e. point buffers the representative values for each parameter can be computed e.g. as a median or average of the values of that parameter for all the original clusters belonging to cluster A. At a block K edge detection variation images are generated using the computed K sets of parameters.

It will be appreciated that in certain implementations the exhaustive search technique described above with respect to may take a relatively longer time to perform than certain other techniques e.g. due to the edge detection needing to be done N times once for each setting . However the overall timing may still be relatively short i.e. the basic edge detection processes can be performed relatively quickly for which the entire process may in some implementations take less than a few seconds. Such timing may be acceptable for certain applications e.g. when the process is being utilized as part of a learn mode procedure where timing considerations are not as critical etc. In certain implementations a matrix of inter point distances for all points detected in N combinations may be precomputed in order to avoid repeated calculations during the clustering process which allows the process to be performed more quickly.

As shown in at a block all of the edge points are found and are grouped into connected edges. In various implementations this process may utilize techniques such as Canny edge detection followed by edge linking segmentation followed by edge tracing any combination of segmentation and edge detection 1D projection to identify all possible lines and arcs etc. At a block the TH and THS ranges are calculated for each edge point. As described above in general for certain standard edge detection processes each edge point may only be found when the TH and THS are within certain ranges. It will be appreciated that in certain implementations some edge points may have null ranges of TH and THS e.g. due to an early out process of the particular edge detection algorithm that is utilized etc. 

At a block for each connected edge the optimal thresholds are found. In certain implementations the optimal thresholds may be determined according to criteria such as maximum number of inliers minimum number of outliers robustness to image noise etc. In one embodiment if the edge points along a connected edge have significantly different ranges of thresholds a technique may be utilized to break the edge down into segments of similar edges e.g. by divisive clustering etc . At a block edge detection is run using each optimal threshold settings to produce actual edges for the variation maps. In one embodiment when the number of variation maps is large a number of the most significant maps e.g. 10 may be presented and a user may then select the map that is closest to the desired values and then be allowed to further manually adjust the thresholds.

From the foregoing it will be appreciated that specific embodiments of the invention have been described herein for purposes of illustration but that various modifications may be made without deviating from the scope of the invention. For example a multidimensional parameter space representation may include adding a third dimension to a two dimensional grid to form a volume e.g. represented isometrically and or rotatably or the like and locating a parameter combination indicator in the volume. Or a two dimensional grid may be augmented with a nearby linear parameter space representation for a third parameter or the like. As another example it will be appreciated that image processing operations e.g. filtering operations may be applied to a region of interest and or edges therein to improve or reveal a property of an edge or edges which makes edge detection or discrimination possible or more reliable. For example texture edges color edges or the like may represent in a filtered image or pseudo image or the like and the systems and methods disclosed herein may be applied to such images to supplement or replace the various operations outlined herein. Accordingly the invention is not limited except as by the appended claims.

