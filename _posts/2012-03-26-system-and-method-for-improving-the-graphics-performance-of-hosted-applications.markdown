---

title: System and method for improving the graphics performance of hosted applications
abstract: A system and method for efficiently processing a video stream using limited hardware and/or software resources. For example, one embodiment of a computer-implemented method for efficiently processing a video stream with a processor pipeline having a plurality of pipeline stages, comprises: identifying a bottleneck stage within the processor pipeline the bottleneck stage processing frames of the video stream; receiving a feedback signal from the bottleneck stage at one or more upstream stages, the feedback signal providing an indication of the speed at which the bottleneck stage is processing the frames of the video stream; and responsively adjusting the speed at which the one or more upstream stages are processing frames of the video stream to approximate the speed at which the bottleneck stage is processing the frames of the video stream.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09446305&OS=09446305&RS=09446305
owner: Sony Interactive Entertainment America LLC
number: 09446305
owner_city: San Mateo
owner_country: US
publication_date: 20120326
---
This application is a continuation in part of U.S. patent application Ser. No. 12 538 077 filed Aug. 7 2009 now U.S. Pat. No. 9 138 644 entitled SYSTEM AND METHOD FOR ACCELERATED MACHINE SWITCHING which claims priority to U.S. Provisional Application Ser. No. 61 210 888 filed Mar. 23 2009 and is a continuation in part CIP application of Ser. No. 10 315 460 filed Dec. 10 2002 now U.S. Pat. No. 7 849 491 entitled APPARATUS AND METHOD FOR WIRELESS VIDEO GAMING which is assigned to the assignee of the present CIP application.

The present disclosure relates generally to the field of data processing systems and particularly to a system and method for improving the graphics performance of hosted applications.

For low latency applications such as video games it is critical that graphics operations proceed as efficiently as possible. However attempts to speed the graphics rendering process may result in undesirable visual artifacts such as tearing in which information from two or more different frames is shown on a display device in a single screen draw. The embodiments of the invention described below provide a variety of techniques for improving the efficiency of graphics rendering while at the same time reducing these undesirable visual artifacts.

In the following description specific details are set forth such as device types system configurations communication methods etc. in order to provide a thorough understanding of the present disclosure. However persons having ordinary skill in the relevant arts will appreciate that these specific details may not be needed to practice the embodiments described.

The assignee of the present application has developed an online video gaming and application hosting system. Certain embodiments of this system are described for example in U.S. patent application Ser. No. 12 538 077 filed Aug. 7 2009 entitled SYSTEM AND METHOD FOR ACCELERATED MACHINE SWITCHING hereinafter 077 application which claims priority to U.S. Provisional Application Ser. No. 61 210 888 filed Mar. 23 2009 and is a continuation in part CIP application of Ser. No. 10 315 460 filed Dec. 10 2002 entitled APPARATUS AND METHOD FOR WIRELESS VIDEO GAMING which is assigned to the assignee of the present CIP application. These applications are sometimes referred to as the co pending applications and are incorporated herein by reference. A brief description of certain pertinent aspects of the online video game and application hosting system described in the co pending applications will now be provided following by a detailed description of a virtualization and encryption system and method for hosting applications.

As shown in the network connection between the Hosting Service and Home and Office Client may be implemented through a wide range of network technologies of varying degrees of reliability such as wired or optical fiber technologies that are typically more reliable and wireless technologies that may be subject to unpredictable interference or range limitations e.g. Wi Fi and are typically less reliable. Any of these client devices may have their own user input devices e.g. keyboards buttons touch screens track pads or inertial sensing wands video capture cameras and or motion tracking cameras etc. or they may use external input devices e.g. keyboards mice game controllers inertial sensing wand video capture cameras and or motion tracking cameras etc. connected with wires or wirelessly. As described in greater detail below the hosting service includes servers of various levels of performance including those with high powered CPU GPU processing capabilities. During playing of a game or use of an application on the hosting service a home or office client device receives keyboard and or controller input from the user and then it transmits the controller input through the Internet to the hosting service that executes the gaming program code in response and generates successive frames of video output a sequence of video images for the game or application software e.g. if the user presses a button which would direct a character on the screen to move to the right the game program would then create a sequence of video images showing the character moving to the right . This sequence of video images is then compressed using a low latency video compressor and the hosting service then transmits the low latency video stream through the Internet . The home or office client device then decodes the compressed video stream and renders the decompressed video images on a monitor or TV. Consequently the computing and graphical hardware requirements of the client device are significantly reduced. The client only needs to have the processing power to forward the keyboard controller input to the Internet and decode and decompress a compressed video stream received from the Internet which virtually any personal computer is capable of doing today in software on its CPU e.g. a Intel Corporation Core Duo CPU running at approximately 2 GHz is capable of decompressing 720p HDTV encoded using compressors such as H.264 and Windows Media VC9 . And in the case of any client devices dedicated chips can also perform video decompression for such standards in real time at far lower cost and with far less power consumption than a general purpose CPU such as would be required for a modern PC. Notably to perform the function of forwarding controller input and decompressing video home client devices do not require any specialized graphics processing units GP Us optical drive or hard drives.

As games and applications software become more complex and more photo realistic they will require higher performance CPUs GPUs more RAM and larger and faster disk drives and the computing power at the hosting service may be continually upgraded but the end user will not be required to update the home or office client platform since its processing requirements will remain constant for a display resolution and frame rate with a given video decompression algorithm. Thus the hardware limitations and compatibility issues seen today do not exist in the illustrated system.

Further because the game and application software executes only in servers in the hosting service there never is a copy of the game or application software either in the form of optical media or as downloaded software in the user s home or office office as used herein unless otherwise qualified shall include any non residential setting including schoolrooms for example . This significantly mitigates the likelihood of a game or application software being illegally copied pirated as well as mitigating the likelihood of a valuable database that might be use by a game or applications software being pirated exploited or otherwise compromised. Indeed if specialized servers are required e.g. requiring very expensive large or noisy equipment to play the game or application software that are not practical for home or office use then even if a pirated copy of the game or application software were obtained it would not be operable in the home or office.

Inbound internet traffic from user clients is directed to inbound routing . Typically inbound internet traffic will enter the server center via a high speed fiber optic connection to the Internet but any network connection means of adequate bandwidth reliability and low latency will suffice. Inbound routing is a system of network the network can be implemented as an Ethernet network a fiber channel network or through any other transport means switches and routing servers supporting the switches which takes the arriving packets and routes each packet to the appropriate application game app game server . In one embodiment a packet which is delivered to a particular app game server represents a subset of the data received from the client and or may be translated changed by other components e.g. networking components such as gateways and routers within the data center. In some cases packets will be routed to more than one server at a time for example if a game or application is running on multiple servers at once in parallel. RAID arrays are connected to the inbound routing network such that the app game servers can read and write to the RAID arrays . Further a RAID array which may be implemented as multiple RAID arrays is also connected to the inbound routing and data from RAID array can be read from app game servers . The inbound routing may be implemented in a wide range of prior art network architectures including a tree structure of switches with the inbound internet traffic at its root in a mesh structure interconnecting all of the various devices or as an interconnected series of subnets with concentrated traffic amongst intercommunicating device segregated from concentrated traffic amongst other devices. One type of network configuration is a SAN which although typically used for storage devices it can also be used for general high speed data transfer among devices. Also the app game servers may each have multiple network connections to the inbound routing . For example a server may have a network connection to a subnet attached to RAID Arrays and another network connection to a subnet attached to other devices.

The app game servers may all be configured the same some differently or all differently as previously described. In one embodiment each user when using the hosting service is typically using at least one app game server . For the sake of simplicity of explanation we shall assume a given user is using app game server but multiple servers could be used by one user and multiple users could share a single app game server . The user s control input sent from client as previously described is received as inbound Internet traffic and is routed through inbound routing to app game server . App game server uses the user s control input as control input to the game or application running on the server and computes the next frame of video and the audio associated with it. App game server then outputs the uncompressed video audio to shared video compression . App game server may output the uncompressed video via any means including one or more Gigabit Ethernet connections but in one embodiment the video is output via a DVI connection and the audio and other compression and communication channel state information is output via a Universal Serial Bus USB connection.

The shared video compression compresses the uncompressed video and audio from the app game servers . The compression maybe implemented entirely in hardware or in hardware running software. There may a dedicated compressor for each app game server or if the compressors are fast enough a given compressor can be used to compress the video audio from more than one app game server . For example at 60 fps a video frame time is 16.67 ms. If a compressor is able to compress a frame in 1 ms then that compressor could be used to compress the video audio from as many as 16 app game servers by taking input from one server after another with the compressor saving the state of each video audio compression process and switching context as it cycles amongst the video audio streams from the servers. This results in substantial cost savings in compression hardware. Since different servers will be completing frames at different times in one embodiment the compressor resources are in a shared pool with shared storage means e.g. RAM Flash for storing the state of each compression process and when a server frame is complete and ready to be compressed a control means determines which compression resource is available at that time provides the compression resource with the state of the server s compression process and the frame of uncompressed video audio to compress.

Note that part of the state for each server s compression process includes information about the compression itself such as the previous frame s decompressed frame buffer data which may be used as a reference for P tiles the resolution of the video output the quality of the compression the tiling structure the allocation of bits per tiles the compression quality the audio format e.g. stereo surround sound Dolby AC 3 . But the compression process state also includes communication channel state information regarding the peak data rate and whether a previous frame is currently being output and as result the current frame should be ignored and potentially whether there are channel characteristics which should be considered in the compression such as excessive packet loss which affect decisions for the compression e.g. in terms of the frequency of I tiles etc . As the peak data rate or other channel characteristics change over time as determined by an app game server supporting each user monitoring data sent from the client the app game server sends the relevant information to the shared hardware compression . These and other features of the hosting service are described in detail the co pending applications.

The shared hardware compression also packetizes the compressed video audio using means such as those previously described and if appropriate applying FEC codes duplicating certain data or taking other steps to as to adequately ensure the ability of the video audio data stream to be received by the client and decompressed with as high a quality and reliability as feasible.

Some applications such as those described below require the video audio output of a given app game server to be available at multiple resolutions or in other multiple formats simultaneously. If the app game server so notifies the shared hardware compression resource then the uncompressed video audio of that app game server will be simultaneously compressed in different formats different resolutions and or in different packet error correction structures. In some cases some compression resources can be shared amongst multiple compression processes compressing the same video audio e.g. in many compression algorithms there is a step whereby the image is scaled to multiple sizes before applying compression. If different size images are required to be output then this step can be used to serve several compression processes at once . In other cases separate compression resources will be required for each format. In any case the compressed video audio of all of the various resolutions and formats required for a given app game server be it one or many will be output at once to outbound routing . In one embodiment the output of the compressed video audio is in UDP format so it is a unidirectional stream of packets.

The outbound routing network comprises a series of routing servers and switches which direct each compressed video audio stream to the intended user s or other destinations through outbound Internet traffic interface which typically would connect to a fiber interface to the Internet and or back to the delay buffer implemented as a RAID array in one embodiment and or back to the inbound routing and or out through a private network not shown for video distribution. Note that as described below the outbound routing may output a given video audio stream to multiple destinations at once. In one embodiment this is implemented using Internet Protocol IP multicast in which a given UDP stream intended to be streamed to multiple destinations at once is broadcasted and the broadcast is repeated by the routing servers and switches in the outbound routing . The multiple destinations of the broadcast may be to multiple users clients via the Internet to multiple app game servers via inbound routing and or to one or more delay buffers . Thus the output of a given server is compressed into one or multiple formats and each compressed stream is directed to one or multiple destinations.

Further in another embodiment if multiple app game servers are used simultaneously by one user e.g. in a parallel processing configuration to create the 3D output of a complex scene and each server is producing part of the resulting image the video output of multiple servers can be combined by the shared hardware compression into a combined frame and from that point forward it is handled as described above as if it came from a single app game server .

Note that in one embodiment a copy in at least the resolution or higher of video viewed by the user of all video generated by app game servers is recorded in delay buffer for at least some number of minutes 15 minutes in one embodiment . This allows each user to rewind the video from each session in order to review previous work or exploits in the case of a game . Thus in one embodiment each compressed video audio output stream being routed to a user client is also being multicasted to a delay buffer . When the video audio is stored on a delay buffer a directory on the delay buffer provides a cross reference between the network address of the app game server that is the source of the delayed video audio and the location on the delay buffer where the delayed video audio can be found.

For low latency applications such as video games it is critical that graphics operations proceed as efficiently as possible. However attempts to speed the graphics rendering process may result in undesirable visual artifacts such as tearing in which information from two or more different frames is shown on a display device in a single screen draw. The embodiments of the invention described below provide a variety of techniques for improving the efficiency of graphics rendering while at the same time reducing these undesirable visual artifacts.

As illustrated in in one embodiment each application game server is equipped with a central processing unit CPU for executing video game program code stored in memory and a graphics processing unit GPU for executing graphics commands to render the video game output . The architectures of the CPU and GPU are well known and as such a detailed description of these units and the instructions commands executed by these units will not be provided herein. Briefly the GPU is capable of processing a library of graphics commands as specified by one or more graphics application programming interfaces APIs such as Open GL or Direct D. The program code for executing these graphics APIs is represented in as graphics engine . As the CPU processes the video game program code it hands off graphics commands specified by the API to the GPU which executes the commands and generates the video output . It should be noted however that the underling principles of the invention are not limited to any particular graphics standard.

In one embodiment both the CPU and GPU are pipelined processors meaning that a set of data processing stages are connected in series within the CPU and GPU so that the output of one stage is the input of the next one. By way of example the CPU pipeline typically includes an instruction fetch stage an instruction decode stage an execution stage and a retirement stage each of which may have multiple sub stages. A GPU pipeline may have many more stages including by way of example and not limitation transformation vertex lighting viewing transformation primitive generation project transformation clipping viewport transformation rasterization texturing fragment shading and display. These pipeline stages are well understood by one of ordinary skill in the art and will not be described in detail herein. The elements of a pipeline are often executed in parallel or in time sliced fashion and some amount of queuing storage is often required between stages of the pipeline.

Each of the above stages and the queuing required between the stages adds a certain amount of latency to the execution of graphics commands. The embodiments of the invention below provide techniques for minimizing this latency. Reducing latency is important because it expands the markets in which a device can be used. Moreover the manufacturer of a device may not have control over significant sources of latency. For example a user may attach a high latency television to a video game console or a multimedia device may be used remotely e.g. online video games a medical device controlled over the internet or military devices engaging targets on the front line while the operator remains safely behind the lines .

As illustrated in one embodiment of the invention includes a back buffer and a front buffer for storing video game image frames generated by the graphics engine as the user plays a video game. Each frame is comprised of a set of pixel data representing one screen image of the video game. In operation each frame is created in the back buffer as graphics commands are executed using graphics data. When a frame has been completed in the back buffer it is transferred to the front buffer from where it is scanned out line by line to create the uncompressed video output . The scan out process may occur at a predetermined standard frequency e.g. such as 60 Hz or 120 Hz as implemented on standard CRT or LCD monitors . The uncompressed video output may then be compressed using the various advanced low latency video compression techniques described in the co pending applications. Of course the frame buffer doesn t need to be scanned out of the video card e.g. via a digital video interface DVI as implied above. It may be transferred directly to the compression hardware for example over the application server s internal bus e.g. a PCI Express bus . The frame buffer may be copied in memory either by one of the CPUs or GPUs. The compression hardware may be by way of example and not limitation the CPU the GPU hardware installed in the server and or hardware on the GPU card.

The obvious first step to minimizing latency is to minimize the queues or even get rid of them entirely. One common way to do this is to synchronize the pipeline stages as per . Every stage operates simultaneously on different sets of data. When all stages are ready they all pass their data to the next stage in the pipeline. Queuing becomes trivial and will no longer be shown in the figures. Latency of a synchronized pipeline is the number of stages times the time for the slowest stage to complete.

This slowest stage in the pipeline is the bottleneck P in all figures. This stage is often a fixed feature of the device over which a designer has no control. shows the dataflow downstream from the bottleneck stage. Notice there is no need for queuing or synchronization. Latency is the sum of the time it takes to complete each stage. Latency cannot be lower than this.

This inspires a method for minimizing the latency of pipeline stages upstream from the bottleneck as per . If the first pipeline stage knows exactly how long every pipeline stage will take and when the bottleneck stage will request new data it can predict when to begin producing new data that will be ready just in time for the bottleneck stage. As such in one embodiment the first pipeline stage may throttle down its clock to slow down data processing based on when the new data will be needed by the bottleneck stage. This technique may be referred to as a phase locked pipeline. The total latency is the sum of the times for each pipeline stage.

Another embodiment is illustrated in in which the bottleneck stage is artificially moved to the first pipeline stage by slowing the first pipeline stage down to be slightly slower than the actual bottleneck stage. The box labeled in P starts after box in P. Box in P should also be slightly lower than the top of box in P. This is common practice in video games where the bottleneck stage is the physical connection between the computer and the monitor. One drawback in is there must be some latency inducing queuing not shown between stages P and P. Another drawback is that the latency experienced by the user may drift over time decreasing steadily and then suddenly increasing only to begin decreasing again. It may also result in dropped frames. Developers often minimize dropped frames by driving the first stage at a rate as close to the bottleneck rate as possible. However this rate is often not known exactly. If the first stage is driven even slightly faster than the bottleneck rate the queues in the system will fill and stall the upstream stages. Ironically attempting to minimize latency using this method risks maximizing it.

In one embodiment of the invention shown in the first stage is limited to be the same rate as the bottleneck stage. The tops of the numbered boxes in P should be the distance apart as the tops of the boxes in P. The rates at which P is producing frames exactly matches the rate at which P is consuming them. Feedback is necessarily provided from the bottleneck stage to the first stage to ensure the rates match exactly. Every stage provides feedback including but not limited to the time required to operate on the data and time spent queued. The phase locking component maintains statistical information on each stage and can accurately predict with a predetermined confidence level that the data will be ready when the bottleneck stage requires it with a minimum amount of queuing. Note that a universal clock is not necessary in this embodiment. The phase locking component only requires relative times. As such the pipeline stages may use different clocks. In fact the clocks may be in separate physical devices that could potentially be thousands of miles apart. In summary in this embodiment of the invention a bottleneck phase is identified based on timing constraints. Feedback is then provided to upstream stages from the bottleneck phase to allow the upstream stages to match the bottleneck stage rate precisely. The phase of the upstream stages is adjusted to minimize time wasted in queues.

The preceding figures illustrated lightweight applications. These are inefficient because the hardware sits around idle most of the time. One embodiment of the invention which forms a less expensive design is one which dedicates the minimum hardware resources to each stage but still guarantees that each stage is faster than the bottleneck stage as illustrated in . In this case the phase locking method gains very little over a fully synchronized pipeline as per . Another example is computer games that render more polygons with higher resolution textures more anti aliasing special effects until the frame rate starts to drop.

This embodiment leads directly to another embodiment of the invention in which advanced graphics is implemented using minimal hardware but with low latency. In this embodiment the video stream is subdivided into two logical parts which may be processed independently a a resource light latency critical part and b a resource heavy latency tolerant part. These two parts can be combined in a hybrid system as illustrated in . One specific example of many possible would be a computer game known as a first person shooter in which a user navigates around from the perspective of a game character in a 3 dimensional world. With this type of game rendering the background and non player characters is resource heavy and latency tolerant denoted in with a b for background while rendering the image of the player s character is made resource light and latency intolerant i.e. because anything less than very low latency performance will result in an undesirable user experience denoted in with an a for avatar. When the user pulls the trigger he expects to see his weapon to fire immediately. In the specific embodiment illustrated the game is implemented on a personal computer with a central processing unit CPU as stage P and a graphics processing unit GPU as stage P. The monitor represented as P is the bottleneck stage. Monitor in the case means any device that consumes the uncompressed video stream. Which could be the compressing hardware.

In this embodiment the CPU completes its work on the background image represented by before completing its work on avatar image represented by . Nonetheless to reduce latency associated with the avatar the GPU processes ahead of rendering the avatar on a previously rendered background to render the motion of the avatar as efficiently as possible outputs that frame and then immediately begins rendering the background of the next frame represented by . The GPU may sit idle for a short time waiting for data from the CPU to complete the next frame. In this embodiment the CPU sits idle waiting for the phase lock to signal that it s time to make a list of drawing commands for the user s avatar and pass it on to the GPU. The CPU then immediately begins to draw the background of a new frame but it can t be the next frame because the GPU will start drawing the next frame. There s no way the CPU will have the next frame ready in time. Therefore the CPU must start drawing the background for the frame after the next. This situation is similar to the operation of a synchronized pipeline as illustrated in .

This one frame phase difference between the avatar and the background is in most cases acceptable to the user. However in cases where highest possible quality is desired the following additional techniques may be employed. The high latency path predicts the inputs to generate the data. In the first person shooter example the location of the camera is predicted ahead of time. When the output of the high and low latency paths are combined the output of the high latency path e.g. the background is modified to more closely match what would have been generated using the actual inputs instead of the predicted inputs. In the first person shooter example the background would be translated scaled and or rotated in order to match the actual camera position. Note this implies the high latency path would have to render an area somewhat larger than what is actually viewed by the player as illustrated in which shows an actual camera location a predicted camera location an actual background and a rendered background . Thus if a user is playing a game in which a character is running at a tree every frame the tree gets a little closer meaning bigger. The user shoots a gun which hits the tree. In the hybrid scenario the tree is lagging behind the shot by one frame. So things might look wrong for a frame i.e. the shot will look like it missed . To compensate the described embodiments of the invention enlarge the tree to approximate what it would look like in the frame in which the shot was fired.

As another example when a user is playing a first person shooter video game and pushes the fire button the user wants to immediately see flames coming out of the gun. Thus in one embodiment the program draws the firing gun on top of a previously rendered background and the game times it so that the frame is done just in time to be picked up by the next stage in the pipeline which is the dvi output vsync or the encoder input or some other bottleneck . Then the game draws its best guess at what the background should be for the next frame. If the guess is poor then one embodiment modifies the background to more closely match what it would have been if the it had been rendered from the correct camera position. Thus the technique shown in is a simple affine warp. More sophisticated techniques employed in other embodiments use the z buffer to do a better job.

In one embodiment the various functional modules illustrated herein and the associated steps may be performed by specific hardware components that contain hardwired logic for performing the steps such as an application specific integrated circuit ASIC or by any combination of programmed computer components and custom hardware components.

In one embodiment the modules may be implemented on a programmable digital signal processor DSP such as a Texas Instruments TMS320x architecture e.g. a TMS320C6000 TMS320C5000 . . . etc . Various different DSPs may be used while still complying with these underlying principles.

Embodiments may include various steps as set forth above. The steps may be embodied in machine executable instructions which cause a general purpose or special purpose processor to perform certain steps. Various elements which are not relevant to these underlying principles such as computer memory hard drive input devices have been left out of some or all of the figures to avoid obscuring the pertinent aspects.

Elements of the disclosed subject matter may also be provided as a machine readable medium for storing the machine executable instructions. The machine readable medium may include but is not limited to flash memory optical disks CD ROMs DVD ROMs RAMs EPROMs EEPROMs magnetic or optical cards propagation media or other type of machine readable media suitable for storing electronic instructions. For example the present invention may be downloaded as a computer program which may be transferred from a remote computer e.g. a server to a requesting computer e.g. a client by way of data signals embodied in a carrier wave or other propagation medium via a communication link e.g. a modem or network connection .

It should also be understood that elements of the disclosed subject matter may also be provided as a computer program product which may include a machine readable medium having stored thereon instructions which may be used to program a computer e.g. a processor or other electronic device to perform a sequence of operations. Alternatively the operations may be performed by a combination of hardware and software. The machine readable medium may include but is not limited to floppy diskettes optical disks CD ROMs and magneto optical disks ROMs RAMs EPROMs EEPROMs magnet or optical cards propagation media or other type of media machine readable medium suitable for storing electronic instructions. For example elements of the disclosed subject matter may be downloaded as a computer program product wherein the program may be transferred from a remote computer or electronic device to a requesting process by way of data signals embodied in a carrier wave or other propagation medium via a communication link e.g. a modem or network connection .

Additionally although the disclosed subject matter has been described in conjunction with specific embodiments numerous modifications and alterations are well within the scope of the present disclosure. Accordingly the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.

