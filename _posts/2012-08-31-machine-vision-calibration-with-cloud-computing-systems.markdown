---

title: Machine vision calibration with cloud computing systems
abstract: A cloud computing system is configured to (i) receive image and environmental data from a computing device, (ii) apply a plurality of image processing algorithms to the received image a plurality of times to generate a corresponding plurality of image processing results, where each application of an image processing algorithm to the received image is executed with a different corresponding parameter set, and (iii) based on the image processing results, select an image processing algorithm and corresponding parameter set for the computing device to use for image processing operations. The cloud computing device may also correlate the results of its analysis with the environmental data received from the computing device, and store the correlation in a machine vision knowledge base for future reference. In some embodiments, the computing device is a component of a robot.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08965104&OS=08965104&RS=08965104
owner: Google Inc.
number: 08965104
owner_city: Mountain View
owner_country: US
publication_date: 20120831
---
The present application claims priority to provisional application 61 597 495 filed on Feb. 10 2012. The entire contents of the 61 597 495 application are incorporated herein by reference.

Cloud computing refers to the provision of computing resources via a computer network. In a traditional model of computing both data and software are fully contained on a user s computer. In a cloud computing arrangement however the user s computer may contain relatively little software or data perhaps just a minimal operating system and web browser for example and may serve as a display terminal for processes occurring on a network of computers. One common shorthand term used to describe a cloud computing system or service or even an aggregation of cloud services is the cloud. 

Cloud computing is sometimes referred to as client server computing. However there are distinctions between cloud computing and general client server computing. For example client server computing may include a distributed application structure that partitions tasks or workloads between providers of a resource or service e.g. servers and service requesters e.g. clients . Client server computing generally involves a one to one relationship between the server and the client whereas cloud computing includes generic services that can be accessed by generic clients such that a one to one relationship or connection may not be required. Thus cloud computing generally includes client server computing along with additional services and functionality.

In many situations cloud computing may free users from certain hardware and software installation and maintenance tasks through the use of simplified hardware on the user s computer. Also because the user s computer can access a vast network of computing resources e.g. processors disk drives etc. the user is not limited just to the computing and storage power of his or her local computer. Additionally the sharing of computing resources across many users may reduce computing costs to individuals. For example multiple computers connected to the cloud may be able to share the same pool of computing power applications and files. Users can store and access personal files such as music pictures videos and bookmarks or play games or use productivity applications on a remote server rather than physically carrying around a storage medium such as a DVD or thumb drive.

In one example a user may open a web browser and connect to a host of web servers that run user interface software configured to collect commands from the user and interpret the commands into commands on the servers. The servers may handle the computing and can either store or retrieve information from database servers or file servers and display an updated page to the user. Through cloud computing data across multiple servers can be synchronized around the world allowing for collaborative work on one file or project from multiple users around the world for example.

Various embodiments of systems and methods for using cloud computing systems for machine vision calibration are disclosed herein.

Any of the methods described herein may be implemented in the form of instructions stored on a non transitory computer readable media. When executed by a computing device the instructions may cause the computing device to perform functions of the disclosed method. Further examples may also include articles of manufacture including tangible computer readable media that have computer readable instructions encoded thereon and the instructions may comprise instructions to perform functions of the methods described herein.

The computer readable media may include non transitory computer readable media such as computer readable media that stores data for short periods of time like register memory processor cache and Random Access Memory RAM . The computer readable media may also include non transitory media such as secondary or persistent long term storage like read only memory ROM optical or magnetic disks or compact disc read only memory CD ROM for example. The computer readable media may also be any other volatile or non volatile storage system. In some embodiments the computer readable media may be considered a computer readable storage media for example or a tangible storage media.

In addition some aspects of the disclosed methods may be performed by circuitry configured to perform logical functions in any of the processes or methods described herein. In still further examples many types of devices may be used or configured to perform logical functions in any of the processes or methods described herein. In yet further examples many types of devices and or components or sub components of the devices may be used or configured as means for performing functions of any of the methods described herein or any portions of the methods described herein .

In some embodiments of the disclosed systems and methods a cloud computing system may be configured to receive an image from an image sensor associated with a computing device. In some embodiments the computing device may be a robot with a machine vision system and the image sensor may be a digital camera or other type of image sensor associated with the robot s machine vision system. The cloud computing system may receive multiple images from the robot in some embodiments. In operation the robot sends the image or images to the cloud computing system so that the cloud computing system can analyze the image or images and calibrate the robot s machine vision system based on the analysis as described herein.

In some embodiments the cloud computing system may be configured to apply multiple different image processing algorithms to the received image multiple times. In operation each image processing algorithm of the multiple image processing algorithms may have a number of different corresponding configurable parameters and each application of a particular image processing algorithm to the image may be executed with a different parameter set that includes one or more image processing parameters for use with that particular image processing algorithm. Applying the multiple different image processing algorithms to the image multiple times with different parameter sets each time enables the cloud computing system to generate a set of image processing results where each individual image processing result in the set of image processing results is the result of one particular application of one particular image processing algorithm configured with one particular parameter set to the image received from the computing device.

The cloud computing system may also be configured to determine a quality score for each image processing result in the set of image processing results in some embodiments. In operation a quality score for a particular image processing result may be determined by comparing attributes of that particular image processing result with a set of quality metrics associated with the image processing algorithm that was used to generate that particular image processing result. After determining a score for the image processing results the cloud computing system may be configured rank the quality scores from highest to lowest and then select an image processing algorithm and a corresponding parameter set that was used to generate an image processing result having a corresponding quality score that exceeds a minimum quality threshold. In some embodiments the cloud computing system may select the image processing algorithm and the corresponding parameter set that was used to generate the image processing resulting having the highest corresponding quality score.

After selecting the image processing algorithm and the corresponding parameter set the cloud computing may then instruct the computing device that originally sent the image to use the selected image processing algorithm and the selected parameter set for image processing operations. In embodiments where the computing device may already have a copy of the selected image processing algorithm and the selected parameter set stored in local memory the cloud computing system may just instruct the computing device to execute the stored copy of the selected image processing algorithm with the stored parameter set. However in some embodiments the cloud computing system may send one or both of the selected image processing algorithm and the parameter set to the computing device in addition to the instructions to use the selected image processing algorithm and the selected parameter set for image processing operations.

In some embodiments the computing device may send both an image and environmental data to the cloud computing system. The environmental data may include information about the environmental conditions in the area where in the computing device is operating including conditions in which the image was captured such as for example the amount of light in the area the source of the light in the area e.g. sunlight fluorescent incandescent etc. the direction of the source of light relative to the sensor that captured the image the location where the image was captured e.g. GPS location inside outside etc. the time of day when the image was captured the weather conditions when the image was captured information about the background or objects in the background of the captured image and or other similar information.

In operation the cloud computing system may correlate the environmental data with the selected image processing algorithm and parameter set. Correlating environmental data with selected image processing algorithms and parameter sets may enable the cloud computing system to build and maintain a machine vision knowledge base comprising a collection of which image processing algorithms and parameter sets work well in different environmental conditions. In some embodiments the cloud computing system may be configured to use the information in the machine vision knowledge base for calibrating machine vision systems for computing devices such as robots. For example in some embodiments the cloud computing system may be able to select an image processing algorithm and parameter set based on information stored in the machine vision knowledge base without having to perform the multiple algorithm analysis described above.

The foregoing summary is illustrative only and is not intended to be in any way limiting. In addition to the illustrative aspects embodiments and features described above further aspects embodiments and features will become apparent by reference to the figures and the following detailed description.

The following detailed description includes references to the accompanying figures. In the figures similar symbols typically identify similar components unless context dictates otherwise. The example embodiments described herein are not meant to be limiting. Other embodiments may be utilized and other changes may be made without departing from the scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure as generally described herein and illustrated in the figures can be arranged substituted combined separated and designed in a wide variety of different configurations all of which are contemplated herein.

In one example multiple computing devices connected to the cloud may access and use a common pool of computing power services applications storage and files. Thus cloud computing enables a shared pool of configurable computing resources e.g. networks servers storage applications and services that can be provisioned and released with minimal management effort or interaction by the cloud service provider.

As an example in contrast to a predominately client based or server based application a cloud based application may store copies of data and or executable program code in the cloud computing system while allowing client devices to download at least some of this data and program code as needed for execution at the client devices. In some examples downloaded data and program code can be tailored to the capabilities of specific client devices e.g. a personal computer tablet computer mobile phone smartphone and or robot accessing the cloud based application. Additionally dividing application execution and storage between client devices and the cloud computing system allows more processing to be performed by the cloud computing system thereby taking advantage of the cloud computing system s processing power and capability for example.

Cloud based computing can also refer to distributed computing architectures where data and program code for cloud based applications are shared between one or more client devices and or cloud computing devices on a near real time basis. Portions of this data and program code may be dynamically delivered as needed or otherwise to various clients accessing the cloud based application. Details of the cloud based computing architecture may be largely transparent to users of client devices. Thus a PC user or a robot client device accessing a cloud based application may not be aware that the PC or robot downloads program logic and or data from the cloud computing system or that the PC or robot offloads processing or storage functions to the cloud computing system for example.

In the cloud computing system includes one or more cloud services one or more cloud platforms cloud infrastructure components and cloud knowledge bases . The cloud computing system may include more of fewer components and each of the cloud services the cloud platforms the cloud infrastructure components and the cloud knowledge bases may comprise multiple computing and storage elements as well. Thus one or more of the described functions of the cloud computing system may be divided into additional functional or physical components or combined into fewer functional or physical components. In some further examples additional functional and or physical components may be added to the examples shown in . Delivery of cloud computing based services may involve multiple cloud components communicating with each other over application programming interfaces such as web services and multi tier architectures for example.

The example cloud computing system shown in is a networked computing architecture. The cloud services may represent queues for handling requests from client devices. The cloud platforms may include client interface frontends for the cloud computing system . The cloud platforms may be coupled to the cloud services to perform functions for interacting with client devices. The cloud platforms may include applications for accessing the cloud computing system via user interfaces such as a web browser. The cloud platforms may also include robot interfaces configured to exchange data with robot clients. The cloud infrastructure may include service billing and other operational and infrastructure components of the cloud computing system . The cloud knowledge bases are configured to store data for use by the cloud computing system and thus the cloud knowledge bases may be accessed by any of the cloud services the cloud platforms and or the cloud infrastructure components .

Many different types of client devices may be configured to communicate with components of the cloud computing system for the purpose of accessing data and executing applications provided by the cloud computing system . For example a computer a mobile device a host and a robot client are shown as examples of the types of client devices that may be configured to communicate with the cloud computing system . Of course more or fewer client devices may communicate with the cloud computing system . In addition other types of client devices may also be configured to communicate with the cloud computing system as well.

The computer shown in may be any type of computing device e.g. PC laptop computer tablet computer etc. and the mobile device may be any type of mobile computing device e.g. laptop smartphone mobile telephone cellular telephone tablet computer etc. configured to transmit and or receive data to from the cloud computing system . Similarly the host may be any type of computing device with a transmitter receiver including a laptop computer a mobile telephone a smartphone a tablet computer etc. which is configured to transmit receive data to from the cloud computing system .

The robot client may include any type of computing device that is configured to communicate with the cloud computing system and has an actuation capability e.g. electromechanical capabilities for moving about its environment and or interacting with objects in its environment. In some embodiments the robot client may include various combinations of computing devices sensors and electromechanical actuation elements. In some examples the robot client may collect data via one or more sensors and upload the data to the cloud computing system via one or more communications interfaces. The cloud computing system may be configured to analyze data received from the robot client and return processed data and or instructions to the robot client . In some embodiments a robot client may be configured to send and receive data to a remote host via the cloud computing system . In other examples the robot client may be configured to send receive data to from another client device via the cloud computing system . For example the robot client may be configured to send receive information to from the computer the mobile device and or even other robots either directly indirectly via the cloud computing system or indirectly via other network systems.

Any of the client devices used with the cloud computing system may include additional components. For example the robot client may include one or more sensors such as a digital camera or other type of image sensor. Other sensors may further include a gyroscope accelerometer Global Positioning System GPS receivers infrared sensors sonar optical sensors biosensors Radio Frequency identification RFID systems Near Field Communication NFC chip sensors wireless sensors and or compasses among others for example.

Additionally any of the client devices may also include a user interface UI configured to allow a user to interact with the client device. For example the robot client may include various buttons and or a touchscreen interface configured to receive commands from a human or provide output information to a human. As another example the robot client may also include a microphone configured to receive voice commands from a human. Furthermore the robot client may also include one or more interfaces that allow various types of user interface devices to be connected to the robot client . For example the mobile device the computer and or the host may be configured to run a user interface for sending and receiving information to from the robot client or otherwise configuring and controlling the robot client .

In communication links between client devices and the cloud may include wired connections such as a serial or parallel bus Ethernet optical connections or other type of wired connection. Communication links may also be wireless links such as Bluetooth IEEE 802.11 IEEE 802.11 may refer to IEEE 802.11 2007 IEEE 802.11n 2009 or any other IEEE 802.11 revision CDMA 3G GSM WiMAX or other wireless based data communication links.

In other examples the client devices may be configured to communicate with the cloud computing system via wireless access points. Access points may take various forms. For example an access point may take the form of a wireless access point WAP or wireless router. As another example if a client device connects using a cellular air interface protocol such as CDMA GSM 3G or 4G an access point may be a base station in a cellular network that provides Internet connectivity via the cellular network.

As such the client devices may include a wired or wireless network interface through which the client devices can connect to the cloud computing system directly or via access points. As an example the client devices may be configured to use one or more protocols such as 802.11 802.16 WiMAX LTE GSM GPRS CDMA EV DO and or HSPDA among others. Furthermore the client devices may be configured to use multiple wired and or wireless protocols such as 3G or 4G data connectivity using a cellular communication protocol e.g. CDMA GSM or WiMAX as well as for WiFi connectivity using 802.11 . Other types of communications interfaces and protocols could be used as well.

In some embodiments one or more of the robot components may be custom designed for a specific robot or for a particular model or type of robot. In other embodiments one or more of the robot components may be generic to many different robots and or types of robots. The robot may have one or communications interfaces not shown for communicating with the cloud computing system as shown in . The communications interfaces may include wired or wireless interfaces. Examples of wired interfaces include for example a parallel interface a Universal Serial Bus USB interface an Ethernet interface an optical interface or any other type of wired communications interface now known or later developed. Examples of wireless interface include for example a Bluetooth interface an IEEE 802.11 and its variants interface a cellular such as GSM CDMA UMTS EV DO WiMAX or LTE interface a Zigbee interface or any other type of wireless communications interface now known or later developed.

In one embodiment the storage may be used for storing data from various sensors of the robot . The storage may also be used for storing program instructions for execution by the processor . The processor may include one or more processors or other data processing sub systems. The processor may be coupled to the storage and may be configured to control the robot based on the program instructions stored at least partially in the storage . The processor may also be configured to receive and interpret data from the various sensors on the robot . Examples of sensors that may be configured for use on the robot include for example smoke sensors light sensors radio sensors infrared sensors microphones gyroscopes accelerometers cameras radars capacitive sensors touch sensors or any other type of sensor now known or later developed.

The robot may also have electromechanical actuation devices configured to enable the robot to move about its environment or interact with objects in its environment. For example the robot may have one or more electromechanical devices such as motors wheels movable arms electromagnets hands grasping claws tool attachments etc. that enable the robot to move about its environment interact with objects located in its environment and or perform tasks with objects in its environment.

In some embodiments the various sensors and electromechanical devices on the robot may be modular in nature. For modular sensors and electromechanical devices different modules may be added to or removed from the robot depending on particular requirements. For example in a low power situation the robot may remove or perhaps power down one or more modules to reduce power usage. In some situations the robot may add one or more additional modular electromechanical devices as required. For example a robot may remove a modular grasping claw electromechanical device from its arm and replace the grasping claw electromechanical device with a tool interface mechanism configured to accept various tool implements such as a screwdriver bit driver knife wrench or other tool etc. to enable the robot to perform a specific task. Similarly a robot may remove a small wheeled drive mechanism configured for indoor use and replace it with a large wheeled drive mechanism configured for outdoor use. From a sensor standpoint the robot may remove a camera interface configured for daytime use and replace it with a night vision enabled interface configured for use in dark or unlit areas. Other types of modular electromechanical devices and or modular sensors could be used as well. Robots with modular sensors and electromechanical devices may be advantageous in situations where the robot may need to adapt to different situations and environments and or use different tools sensors and or other attachments to accomplish various tasks.

In some embodiments the processor storage and sensors of the robot may optionally be components of a removable device indicated by the dashed line shown in the . For example the robot may have a number of mechanical actuation devices e.g. a movable base one or more grasping claws robot hands etc. and the robot may be configured to receive a removable device to function as the brains or control unit of the robot . In some embodiments the removable device may correspond to a mobile telephone a smartphone a tablet computer a laptop computer etc. For example the device may be a smartphone configured to plug in to the robot. When plugged in to the robot the smartphone may form an interactive display configured to receive commands from a human for example. The smartphone may also provide a robot with sensors e.g. a camera compass GPS receiver accelerometer etc. one or more wireless communications interfaces and processing capabilities for example.

In some embodiments the robot may be able to leverage the sensor and communications capabilities of neighboring client devices to supplement or augment its own sensor and communications capabilities. For example the robot may access a separate smartphone via a Bluetooth connection and use the smartphone s cellular data network interface to access the cloud computing system shown in . Similarly the robot may be configured to connect to other sensors to obtain information. For example the robot may be configured to connect to security system cameras or other sensors positioned near the entrance of a warehouse to monitor the shipments into and out of the warehouse for example.

In most embodiments the robot will have a machine vision system. A robot machine vision system typically includes i at least one image sensor configured to obtain image data and ii at least one processor configured to process images obtained via the at least one image sensor for use by the robot. In operation a robot may use its machine vision system alone or in combination with other sensor systems to navigate through an area to identify objects in its environment to track objects in its environment to interact with objects in its environment e.g. grasp and manipulate an object etc.

In the embodiment shown in the robot s machine vision system may be entirely contained within the removable device . For example the robot may rely on i an image sensor e.g. a digital camera in the removable device to capture images and ii processor to process images captured by the image sensor . In other embodiments the robot may have a built in i.e. onboard integrated etc. machine vision system that includes i a built in image sensor or sensors to capture images and ii a built in processor or processors to process images captured by the built in image sensor s .

In some embodiments a smartphone e.g. removable device of can be attached to the robot . The smartphone may include one or more components of the robot s machine vision system. In one example the smartphone may include on one or more image sensors e.g. a digital camera that are configured to capture images for the robot to use in connection with its machine vision system. In such embodiments one or more onboard processors of the robot may be configured to process the images collected by the image sensors of the smartphone. In another example the robot may have onboard image sensors configured to capture images for the robot to use in connection with its machine vision system and one or more processors of the smartphone may be configured to process images captured by the one or more onboard image sensors of the robot .

Any of the robots illustrated in may be configured to operate according to a robot operating system e.g. an operating system designed for specific functions of the robot. A robot operating system may provide libraries and tools e.g. hardware abstraction device drivers visualizers message passing package management etc. to enable robots to execute robot applications. Additionally a robot operating system may include publish and subscribe functionality and may also include functionality to control components of the robot such as head tracking base movement e.g. velocity control navigation framework etc.

As described above in some embodiments a robot may have a machine vision system that the robot uses alone or in combination with other sensor systems to navigate through an area to identify objects in its environment to track objects in an environment to interact with objects in the environment e.g. grasp and manipulate an object etc. A particular robot s machine vision system may include i at least one image sensor configured to obtain image data and ii at least one processor configured to process image data obtained via the at least one image sensor. In operation the robot s machine vision system can be configured to execute different image processing algorithms depending on the task the robot may be performing the particular sensor that the robot may be using with its machine vision system the particular environment that the robot may be navigating through and or other factors.

For example when navigating within a room it may be desirable for the robot s machine vision system to execute a particular image processing algorithm designed to identify walls doors windows large objects and people to help the robot navigate through the room without running into obstacles. But when the robot needs to perform a particular task such as retrieving a red pen from a cup of pens for a user for example it may be desirable for the robot s machine vision system to execute a different image processing algorithm designed to discern finer details between smaller objects so that the robot can distinguish between individual pens in the cup rather than simply treating the cup of pens as a single object or simply treating the desk and all of its contents including the cup of pens as a single object for example.

Additionally each individual image processing algorithm that can be executed by the robot s machine vision system can also be calibrated and or fine tuned by adjusting values for various image processing parameters. Each individual image processing algorithm may have different corresponding image processing parameters and each parameter may have a wide range of configurable values.

For example the previously described image processing algorithm for use in traveling within the office building may have a number of different configurable parameters for fine tuning the image processing algorithm to operate in different environments. As the robot navigates through the office building the robot may travel through internal hallways lit by fluorescent lighting along the ceiling and the robot may also travel within individual offices and conference rooms lit via combinations of overhead fluorescent lighting wall mounted or tabletop based incandescent lighting and or natural sunlight from exterior windows. Thus as the robot travels within the offices conference rooms and hallways the robot may need to periodically recalibrate and or fine tune its machine vision system to adjust to environmental changes such as changes in the type of light source fluorescent lighting vs. incandescent lighting vs. sunlight the direction of the light source overhead versus from outward facing windows the intensity of the light source e.g. in the morning the east side of the building may have direct sunlight while the west side may have indirect sunlight etc. etc.

In some situations it may be desirable to change the parameters used by a particular image processing algorithm based on changes in the lighting conditions for example. Changing the parameters for the image processing algorithm may improve the image processing results produced by the machine vision system s application of the image processing algorithm to the image data captured by the robot s image sensor s . And with better image processing results the robot may able to perform its task better. For example if changing the parameters for a particular image processing algorithm helps the robot to identify walls doors windows objects and people better then the robot may be less likely to run in to things as it navigates through a particular area.

In other situations it may be desirable for the machine vision system to change to a different image processing algorithm altogether with a whole new set of corresponding image processing parameters . For example if the office building lost power and the hallway became very dark then the robot may need to switch to a totally different image processing algorithm designed to process images in low light environments. Like the earlier image processing algorithm the example low light image processing algorithm may also have different corresponding image processing parameters and each parameter for use with the low light algorithm may also have a wide range of configurable values.

Although the examples above focus on changes in lighting conditions there may be other reasons to use different image processing algorithms configured with different parameters in various situations. For example if a robot needs to locate and retrieve a particular object based in part on its color e.g. a red pen a blue pen a yellow highlighter etc. then it may be desirable for the robot to use an image processing algorithm configured to discern fine differences between small objects with a set of corresponding image processing parameters designed to detect the desired color. Similarly if the robot needs to locate and retrieve a particular object based in part an image on the surface of the object e.g. a coffee cup with a particular design or logo then it may be desirable for the robot to use a different image processing algorithm configured with a set of corresponding image processing parameters to help the robot s machine vision system identify the particular design or logo on the cup. Additionally if the robot needs to read a bar code QR code or other type of code on the surface of an object then it may be desirable for the robot to use yet a different image processing algorithm configured with a set of corresponding image processing parameters to help the robot s machine vision system pick out the details of the code from other information on the surface object.

As described above machine vision systems in the presently disclosed embodiments may be configurable to execute many different image processing algorithms in various circumstances and each of the different image processing algorithms may be highly configurable with many different parameter settings having many different values. Having the capability to execute many different highly configurable image processing algorithms may enable a robot to perform a wide variety of tasks in a wide variety of different environmental conditions. However it may be impractical to store all the potential image processing algorithms available to the machine vision system in the robot s local memory. Similarly it may difficult for a robot to determine or choose a particular image processing algorithm and a corresponding set of parameter settings for use by its machine vision system in different operating environments and circumstances.

In some examples a cloud computing system may be used to determine or select an image processing algorithm along with a corresponding set of image processing parameters for use by a robot s machine vision system. In operation the cloud computing system can be configured to analyze image data received from a robot and perhaps additional information received from the robot as described later e.g. environmental data task data and or object data and then determine or select one or both of i an image processing algorithm and ii a corresponding set of one or more image processing parameters for use with the selected image processing algorithm. Then the cloud computing system may send an indication of the selected algorithm and corresponding parameter set or perhaps even the actual algorithm code configured with the appropriate parameter settings to the robot for the robot s machine vision system to execute locally. Because the robot s machine vision system can be configured to execute the selected algorithm with the corresponding parameter set locally the robot is able to perform image processing tasks with little or no network latency that may be present for alternative systems and methods where actual image processing is performed by a cloud computing system or other type of computing system located remotely from the robot.

Because the cloud computing system has substantially more computing power than an individual robot the cloud computing system can analyze many different algorithms configured with many different combinations of parameter settings to determine an algorithm and parameter set appropriate for a particular robot in a particular operating environment or situation. Additionally in some examples because the cloud computing system can be configured to send an indication of the algorithm and a parameter set for the robot s machine vision system to use rather than sending the image processing results back to the robot the systems and methods described herein make efficient use of bandwidth between the cloud computing system and the robot.

In example the robot sends image data to the cloud processing engine of the cloud computing system. In operation the cloud processing engine is configured to use the image data to determine a particular image processing algorithm and corresponding parameter set for the robot to use with the robot s machine vision system. After determining a particular image processing algorithm and corresponding parameter set for execution by the robot s machine vision system the cloud processing engine alone or in combination with other cloud computing system components may send the determined image processing algorithm and parameter set to the robot via a response .

In some embodiments the image data may comprise an image captured by an image sensor associated with the robot such as a digital camera infrared sensor a 3D scanner or any type of image sensor now known or later developed. In some embodiments the image data may additionally or alternatively comprise an image that has a reduced resolution or color space compared to the image captured by the image sensor associated with the robot . Additionally the image data may also contain information about the type of sensor that the robot used to obtain the image associated with the image data .

Also in some embodiments the robot may also send environmental data along with the image data . The environmental data may be associated with the image captured by the robot s image sensor alone or in combination with one or more other of the robot s sensors e.g. GPS sensors light sensors accelerometers etc. . The environmental data may include an amount of light a type of light source a direction of the light source a geographic location e.g. Global Positioning System or similar coordinates a weather condition sunny cloudy rain etc. a type of background color design pattern reflectivity etc. a speed that the robot is traveling or any other type of data associated the robot s environment or surroundings that may have at least some impact on the operation and function of the robot s machine vision system.

In some embodiments the robot may additionally send information about the task it is trying to accomplish and or the objects that it is attempting to interact with. In some circumstances the cloud processing engine may take the task that the robot is trying to accomplish and or the object or objects that the robot is trying to interact with into consideration when determining or selecting a particular image processing algorithm and corresponding parameter set for the robot to use with its machine vision system.

As mentioned briefly above the cloud processing engine is configured to i analyze the image data along with perhaps additional data e.g. environmental data task data and or object data received from the robot ii select an image processing algorithm and a set of one or more parameters for use with the selected image processing algorithm for the robot to use with its machine vision system and iii send a response to the robot with an indication of the selected image processing algorithm and corresponding parameter set based on the cloud processing engine s analysis of the image data and perhaps the additional data as well .

In example the cloud processing engine receives the image data and perhaps additional data e.g. environmental data task data and or object data from the robot . The image data includes at least one image. To determine an appropriate image processing algorithm and corresponding parameter set for the robot the cloud processing engine may perform at least one of a few different functions. In some embodiments the cloud processing engine may apply a plurality of different image processing algorithms to the image included in the image data . Additionally or alternatively the cloud processing engine may analyze environmental data and or task data and or object data received from the robot compare the received environmental data with environmental data stored in the machine vision knowledge base . In some embodiments the cloud processing engine may perform some combination of the algorithm analysis and environmental data lookup functions. Each of the functions is described below in turn.

For the algorithm analysis function the cloud processing engine applies a plurality of different image processing algorithms to the image included in the image data . Each algorithm and can be applied to the image multiple times and each application of the image processing algorithm to the image is executed with a set of parameters. For example image processing algorithm 1 shown as block is applied to the image x number times as shown in block image processing algorithm 2 shown as block is applied to the image y number of times as shown in block and algorithm n shown as block is applied to the image z number of times as shown in block .

In operation the cloud computing system may store many different image processing algorithms for use with many different types of machine vision systems that may be deployed by many different types of robots. However the robot s machine vision system may not be able to execute every image processing algorithm stored by the cloud computing system. Instead the image processing algorithms that can be executed by the robot s machine vision system may be based on the type of image sensors associated with the robot and or the processing capability of the robot or at least the processing capability available to the robot s machine vision system . Nevertheless a particular robot s machine vision system may be able to execute many different image processing algorithms. Additionally new image processing algorithms may be added to the cloud computing system over time as new image processing algorithms are developed and older image processing algorithms may be removed from the cloud computing system as older algorithms are discarded in favor of newer algorithms.

In some embodiments the total number of image processing algorithms n in the plurality of algorithms to be applied to the image received from the robot may include all of the image processing algorithms that can be executed by the robot s machine vision system. In operation the robot s machine vision system may be able to execute hundreds or more different image processing algorithms. But because the cloud processing engine has substantial computing resources compared to the robot the cloud processing engine is able execute many different image processing algorithms very quickly perhaps even in parallel or at least substantially in parallel or at least likely more quickly than an individual robot might be able to perform a similar computation.

Alternatively the cloud processing system may select some subset of all of the image processing algorithms that can be executed by the robot s machine vision system to include in the plurality of n algorithms to be applied to the image. In these embodiments the subset of image processing algorithms included in the plurality of n algorithms to be applied to the image may be based on other information included in the additional data received from the robot . For example the cloud computing system may use environmental data task data and or object data as previously described received from the robot to select or determine which algorithms to be applied to the image. In some embodiments the cloud processing engine may access the machine vision knowledge base to determine which image processing algorithms to apply to the image. The machine vision knowledge base is described in more detail later.

As described above to determine an appropriate image processing algorithm and corresponding parameter set for the robot the cloud processing engine shown in example applies n different image processing algorithms to the image included in the image data received from the robot . Example also shows each algorithm of the plurality of algorithms being applied to the image multiple times. In operation each application of the image processing algorithm to the image is executed with a different set of image processing parameters which may include some similar or same parameters and each application of a particular algorithm configured with a corresponding parameter set yields a different image processing result.

As shown in blocks and algorithm 1 is configured with image processing parameter set 1 1 and applied to the image to generate image processing result 1 1 image processing algorithm 1 is configured with image processing parameter set 1 2 and applied to the image to generate image processing result 1 2 and so on through the application of algorithm 1 configured with parameter set 1 x to the image to generate image processing result 1 x. Similarly algorithm 2 is configured with image processing parameter set 2 1 and applied to the image to generate image processing result 2 1 image processing algorithm 2 is configured with image processing parameter set 2 2 and applied to the image to generate image processing result 2 2 and so on through the application of algorithm 2 configured with image processing parameter set 2 y to the image to generate image processing result 2 y. Finally as shown in blocks and algorithm n i.e. the last of the n total algorithms in the plurality of algorithms applied to image is configured with image processing parameter set n 1 and applied to the image to generate image processing result n 1 image processing algorithm n is configured with image processing parameter set n 2 and applied to the image to generate image processing result n 2 and so on through the application of algorithm n configured with parameter set n z to the image to generate image processing result n z. In some embodiments the application of the multiple algorithms configured with the multiple different parameter sets can be performed in parallel or at least substantially in parallel to generate a plurality of different image processing results.

Thus as shown in example i algorithm 1 is applied to the image with x different parameter sets to generate x different image processing results ii algorithm 2 is applied to the image with y different parameter sets to generate y different image processing results and iii algorithm n is applied to the image with z different parameter sets to generate z different image processing results . In operation x y and z may be very large numbers. For example if algorithm 1 has three parameters and each parameter has 15 unique settings then at least in some embodiments x may equal 3 and thus the cloud processing engine may apply algorithm 1 to the image up to 14 348 907 times depending on the specific algorithm parameters and parameter settings . But because the cloud processing engine has substantial computing resources compared to the robot the cloud processing engine is able execute an algorithm even multiple different algorithms with multiple different parameter sets very quickly perhaps even in parallel or at least substantially in parallel or at least likely more quickly than an individual robot might be able to perform a similar computation or set of computations .

For image processing algorithms with many different parameters having many different settings the total number of parameter sets and resulting image processing results could be quite large perhaps even hundreds of millions or more. Therefore in some embodiments the cloud processing system may select some subset of the total number of parameter sets to use with an algorithm. For example for algorithm 1 shown in block the subset of x parameter sets may be based on the additional data received from the robot . In operation the cloud computing system may use the additional data e.g. environmental data task data and or object data as previously described to determine a subset of parameter sets to use when applying a particular algorithm to an image. In some embodiments the cloud processing engine may access the machine vision knowledge base to determine which parameter sets to use when applying a particular algorithm to an image. The machine vision knowledge base is described in more detail later.

In example after applying the plurality of algorithms configured with the plurality of parameter sets to the image to produce the plurality of image processing results the cloud processing engine then determines a quality score for each image processing result. For example the cloud processing engine determines a quality score for each of the different image processing results result 1 1 through result 1 x generated via the application of algorithm 1 configured with the different parameter sets parameter set 1 1 through parameter set 1 x thereby generating x different quality scores score 1 1 through score 1 x . The cloud processing engine calculates quality scores for each of the other image processing results as well.

In some embodiments the quality score generated for an individual image processing result may be based on a comparison of one or more attributes of the individual image processing result with a set of one or more quality metrics associated with the algorithm that was used to generate the individual image processing result. In other embodiments the quality score generated for an individual image processing result may be based on objective quality measures that may be relevant to multiple or perhaps all of the image processing algorithms. In still further embodiments the quality score generated for an individual image processing result may be based on a combination of algorithm specific metrics and objective metrics.

After generating the plurality of quality scores and the cloud processing engine determines which quality score is the highest. After determining the highest quality score the cloud processing engine selects the image processing algorithm and the parameter set that was used to generate the image processing result having the highest score and then the cloud processing engine sends an indication of the selected algorithm and parameter set to the robot via response . In situations where multiple image processing results have the highest score the cloud processing engine may be configured to select one of the multiple highest scoring results based in part on environmental data task data and or object data as previously described received from the robot . In some embodiments rather than selecting the image processing result having the highest quality score the cloud processing engine may instead select an image processing result having a quality score that meets or exceeds a minimum quality score threshold.

In example quality score 2 2 is shaded to indicate that it has the highest quality score of the different sets of quality scores and that were determined for the corresponding sets of image processing results and . Because image processing result 2 2 had the highest quality score the cloud processing system in example is shown selecting algorithm 2 and parameter set 2 2 as the algorithm and parameter set for the robot to use with its machine vision system. In operation the cloud processing engine alone or in combination with other cloud computing system components sends the robot a response based on the selected algorithm and parameter set.

In some embodiments the robot may have copies of the program code for one or more commonly used image processing algorithms. In such embodiments the response sent from the cloud processing engine to the robot may include an indication of the selected algorithm and an indication of the parameter settings. When the robot receives the indication in the response the robot s machine vision system may execute the indicated algorithm configured with the indicated parameter settings. In other embodiments the robot may not have a copy of the program code for the selected image processing algorithm. In such embodiments the response sent from the cloud processing engine to the robot may also include program code for the selected image processing algorithm. In such embodiments after the robot receives the program code for the selected algorithm along with the selected parameter settings the robot s machine vision system can execute the selected algorithm with the selected parameter settings.

In some embodiments the cloud processing engine may also send the robot one or more quality metrics associated with the selected image processing algorithm. Additionally or alternatively the robot may also store one or more objective metrics. In operation the robot may periodically measure the performance of its machine vision system based in part on one or more of the received quality metrics associated with the selected algorithm and or the objective quality metrics. If the performance of the robot s machine vision system starts to decline the robot may obtain and send new image data and perhaps additional data to the cloud processing engine and the cloud computing engine may select either i a new parameter set to use with the current algorithm being executed by the robot s machine vision system which would be the algorithm previously selected by the cloud processing engine or ii a new algorithm and corresponding parameter set.

There are multiple ways that the robot can determine that its machine vision system is starting to degrade. For example any set of one or more of the following conditions could be sufficient to trigger the process of obtaining an sending new image data perhaps along with additional data to the cloud processing system for selecting a new parameter set or a new algorithm and parameter set i the quality score of an image processing result falls below a threshold quality score ii the quality score of a later image processing results differs more than a predetermined amount from the quality score of an earlier image processing result iii the rate of change in quality scores of image processing results measured over a period of time is greater than a threshold rate of change or iv any combination of the foregoing. Other methods of measuring algorithm performance degradation could be used as well.

The selection of the new parameter set or algorithm and parameter set may be performed by any of the methods described herein. For example the cloud computing system may conduct the multiple algorithm analysis described above to determine a new parameter set or new algorithm and parameter set. Similarly the cloud computing system could query the machine vision knowledge base to determine a new parameter set or new algorithm and parameter set. Likewise the cloud computing system could query the machine vision knowledge base to select a set of candidate algorithms and or parameter sets for use in a multiple algorithm analysis described herein.

In some examples an advantage to having the robot monitor the performance of its machine vision system locally is that the robot can be configured to determine when it needs to update the algorithm being executed by its machine vision system. The robot can continue using the current algorithm with its current parameter set until it receives an updated parameter set or an indication of a new algorithm with a new parameter set from the cloud computing system. Thus the transition between a prior machine vision system configuration and an updated machine vision system configuration is substantially seamless. Also because the robot can be configured to send image data to the cloud computing system periodically the bandwidth required for the machine vision system calibration methods described above can be lower than a comparable machine vision system calibration method where the robot streams image data to a cloud computing system for analysis and processing.

As mentioned above some embodiments may include a machine vision knowledge base . In addition to sending the response to the robot the cloud computing engine may also send i at least some of the image data ii at least some of the additional data e.g. environmental data task data and or object data if applicable and iii at least some of the information associated with the response to the machine vision knowledge base .

For example in some embodiments after the cloud processing engine has selected a particular algorithm and corresponding parameter set based on the above described multiple algorithm analysis process the cloud processing engine may send the selected algorithm and parameter set or at least an indication of the selected algorithm and parameter set to the machine vision knowledge base along with the environmental data task data and or object data as previously described that may have been received from the robot along with the image data .

Example shows the machine vision knowledge base storing example dataset which includes correlations of different environmental data with different combinations of algorithms and parameter sets based on results from the cloud processing engine s analysis of many different images received from many different robots. In operation the machine vision knowledge base can be configured to store the best algorithms and parameter sets e.g. most often used algorithms and parameter sets for use by robots in various environments. For example different algorithms and parameter sets could be correlated or associated with certain lighting conditions e.g. particular type of light source particular direction of light source particular intensity of light source etc. Similarly different algorithms and parameter sets could be correlated or associated with certain locations e.g. certain GPS coordinates certain rooms of a house or a building etc.

In some embodiments the cloud processing engine can be configured to determine or infer certain environmental conditions rather than rely on environmental data received from a robot. Determining or inferring environmental conditions might be advantageous in situations where a robot lacks sensors to record certain environmental conditions. For example for outdoor images and or for indoor images affected by sunlight from windows the cloud processing engine could infer the direction and intensity of sunlight based on a robot s location and current weather data at that location and the inferred direction and intensity of sunlight and the local weather conditions could be correlated with certain algorithms and parameter sets in the machine vision knowledge base .

Example data shows the machine vision knowledge base storing w different environments where each environment has a corresponding algorithm and parameter set. Different environments as defined by environmental data may be associated with the same corresponding algorithm and parameter set. In operation the number w of different environments may be very large. In some embodiments the machine vision knowledge base may even be configured to store many hundreds of millions of different environments and their corresponding algorithms and parameter sets. In some embodiments the machine vision knowledge base may also include datasets including correlations with task and or object data with various algorithms and parameter sets as well. Although the example data is shown as a table other types of data sets and data structures could be used as well.

After the machine vision knowledge base has amassed a large collection of different environmental data task data and or object data associated with various algorithms and parameter sets based on analyses of image data received from many different robots the cloud processing engine in some embodiments may be configured to query the machine vision knowledge base based on environmental data task data and or object data received from a particular robot to select an algorithm and parameter set for use by the particular robot. In some embodiments the cloud processing engine may not need to perform the multiple algorithm analysis procedure described above if it is able to select an appropriate algorithm and parameter set based on a machine vision knowledge base query. Thus later robots that need to configure their machine vision systems for operation in a particular environment can benefit from the algorithms and corresponding parameter sets that the cloud computing system has previously determined for earlier robots that have operated in the same or similar environment as the later robot.

For example when the cloud processing engine receives environmental data object data and or task data from a robot such as robot the cloud processing engine may search or query the machine vision knowledge base to identify an algorithm and parameter set that is correlated or otherwise associated with the same or similar environmental data object data and or task data received from the robot . If the machine vision knowledge base contains an algorithm with a corresponding parameter set that is correlated or associated with the same or similar environmental data object data and or task data received from the robot then the cloud processing engine may simply return that particular algorithm and parameter set to the robot in the response without having to perform the multiple algorithm analysis procedure described above.

Alternatively when the cloud processing engine receives environmental data object data and or task data associated with a particular image from the robot the cloud processing engine may search or query the machine vision knowledge base to identify one or more candidate algorithms and parameter sets that are correlated with the same or similar environmental data object data and or task data received from the robot in the image data . The candidate algorithms and parameter sets for the different environmental conditions or tasks objects etc. selected from the machine vision knowledge base may be based on a history of the algorithms and parameter sets determined by the multiple algorithm analysis process described above. After identifying the candidate algorithms and parameter sets the cloud processing engine may then perform the multiple algorithm analysis process described above to select one of the algorithms and parameter sets to return to the robot in the response . Thus in some embodiments the cloud computing system can use the machine vision knowledge base to reduce the total number of algorithms and parameter sets to evaluate in the multiple algorithm analysis process described above.

Additionally in some embodiments the cloud computing system may be configured to periodically re evaluate algorithms and parameter sets that have previously been correlated or associated with particular environmental data task data and or object data stored in the machine vision knowledge base . In some instances the cloud computing system may revise the algorithms and parameter sets associated with particular environmental data task data and or object data based on more detailed algorithm analyses.

For example the cloud processing engine may select an algorithm and parameter set for the robot based on a first pass analysis of a limited number of algorithms and parameter sets. Analyzing a limited number of algorithms and parameter sets in a first pass manner may enable the cloud processing engine to quickly identify and select an acceptable but perhaps not optimal algorithm and parameter set for the robot to use so that the cloud processing engine can send the robot a quick response. However the cloud processing engine may be able to determine a better algorithm and parameter set after analyzing the image with a greater number of algorithms and parameter sets. Thus during timeframes where the cloud processing engine may be receiving fewer machine vision system calibration requests from robots the cloud processing engine can re visit the earlier results of the first pass analysis to determine whether better performing algorithms and parameter sets can be found for the image. And if the cloud processing engine finds a better performing algorithm and or parameter set for that particular image the cloud processing engine may update the data in the machine vision knowledge base to associate the better performing algorithm and or parameter set with the environmental data object data and or task data that was received along with the image on which the cloud processing engine previously performed the quick first pass analysis.

Similarly in some embodiments the cloud processing engine may also be configured to re run analyses on previously received images when new image processing algorithms become available. If the new image processing algorithm performs better in certain environments then the machine vision knowledge base can be updated accordingly. In this manner the cloud processing engine improves the data in the machine vision knowledge base over time so that better performing algorithms and corresponding parameter sets can be returned to robots in response to machine vision calibration requests.

Method begins at block . At block a cloud computing system receives image data from a robot. The image data may be similar to the image data described previously herein. The cloud computing system may also receive additional data e.g. environmental data task data and or object data as described herein.

At block the cloud computing system applies a plurality of different image processing algorithms to the image a plurality of times to generate a plurality of different image processing results where each application of each algorithm to the image is performed with a different set of one or more image processing parameters. The application of multiple algorithms to the image may be similar to the procedure described herein with respect to .

At block the cloud computing system determines a plurality of quality scores where each image processing result has a corresponding quality score. Determining a quality score may be similar to the procedures for determining a quality score described herein with respect to . Then at block the cloud computing system determines which image processing result of the plurality of image processing results from block has the highest quality score. Determining which image processing result has the highest quality score may be similar to the procedures for determining a highest quality score described herein with respect to . In other examples at block the cloud computing system may determine which image processing result of the plurality of image processing results from block is above a threshold quality score and if multiple image processing results are above the threshold quality score the cloud computing system may select any algorithm and parameter set corresponding to any result above the threshold quality score.

At block the cloud computing system selects the image processing algorithm and the corresponding set of one or more image processing parameters that were used to generate the image processing result that had the highest quality score from block . Then at block the cloud computing system instructs the robot that sent the image data to use the selected image processing algorithm and corresponding parameter set for its machine vision system. Example method then ends at block .

Method begins at block . At block the cloud computing system receives from a robot i image data and ii additional data. The image data includes at least one image but could perhaps include multiple images . The additional data includes at least one of environmental data task data and or object. The image data at least one image environmental data task data and or object data of block may be similar to the image data at least one image environmental data task data and or object data described elsewhere herein.

At block the cloud computing system queries a machine vision knowledge base. The machine vision knowledge base includes a plurality of different image processing algorithms and corresponding image processing parameter settings where individual algorithms and parameter sets are associated with different environmental data task data and or object data. The machine vision knowledge base of block may be similar to the machine vision knowledge base shown and described herein with respect to .

At block the cloud computing system selects an image processing algorithm and corresponding parameter set from the machine vision knowledge base of block based at least in part on at least one of i the environmental data received from the robot ii the task data received from the robot and or iii the object data received from the robot. Thus the robot s machine vision system can be calibrated for its current environment task etc. based at least in part on results from the machine vision knowledge base query. After selecting the image processing algorithm and corresponding parameter set in block the cloud computing system instructs the robot to use the selected image processing algorithm and corresponding parameter set at block . Example method then ends at block .

In alternative embodiments not shown block may include selecting a plurality of image processing algorithms and corresponding parameter sets from the machine vision knowledge base based at least in part on at least one of i the environmental data received from the robot ii the task data received from the robot and or iii the object data received from the robot. The cloud processing system may then use the selected plurality of image processing algorithms and corresponding parameter sets in a multiple algorithm analysis procedure similar to the procedure described in blocks shown and described herein with respect to . Then after selecting a particular image processing algorithm and corresponding parameter set in in a fashion similar to that described in blocks of the cloud computing system can instruct the robot to use the particular image processing algorithm and corresponding parameter set at block . Example method then ends at block .

Example method begins at block . At block the robot obtains an image from one or more image sensors. In some embodiments the image sensor may be integrated with the robot. In other embodiments the image sensor may be integrated with a detachable device e.g. a smartphone or similar mobile computing device with a sensor as describe herein with respect to B and C. The image sensor of block may be similar to any of the image sensors described elsewhere herein.

At block the robot may optionally determine environmental data task data and or object data associated with its surroundings a task it is attempting to perform and or an object it is attempting to interact with. The environmental data task data and or object data of block may be similar to any of the environmental data task data and or object data described elsewhere herein.

At block the robot sends the image to a cloud computing system. In embodiments where the robot has determined environmental data task data and or object data associated with the image in block the robot may also send the determined environmental data task data and or object data associated to the cloud computing system at block .

After the cloud computing system receives the image along with the optional environmental data task data and or object data the cloud computing system selects an image processing algorithm and corresponding parameter set based on an analysis of the image received from the robot and perhaps also based on the environmental data task data and or object data received from the robot according to any of the procedures shown and described herein with respect to .

At block the robot receives from the cloud computing system an indication of an image processing algorithm and a set of one or more image processing parameters for use with the indicated image processing algorithm. In some embodiments the robot may additionally receive program code associated with the indicated image processing algorithm as described herein.

Then at block the robot uses the image processing algorithm and corresponding parameter settings indicated at block with its machine vision system. Example method then ends at block .

While various aspects and embodiments have been disclosed herein other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration only and are not intended to be limiting with the true scope and spirit being indicated by the following claims.

