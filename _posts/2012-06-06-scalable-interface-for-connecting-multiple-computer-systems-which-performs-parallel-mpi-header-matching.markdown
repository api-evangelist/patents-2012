---

title: Scalable interface for connecting multiple computer systems which performs parallel MPI header matching
abstract: An interface device for a compute node in a computer cluster which performs Message Passing Interface (MPI) header matching using parallel matching units. The interface device comprises a memory that stores posted receive queues and unexpected queues. The posted receive queues store receive requests from a process executing on the compute node. The unexpected queues store headers of send requests (e.g., from other compute nodes) that do not have a matching receive request in the posted receive queues. The interface device also comprises a plurality of hardware pipelined matcher units. The matcher units perform header matching to determine if a header in the send request matches any headers in any of the plurality of posted receive queues. Matcher units perform the header matching in parallel. In other words, the plural matching units are configured to search the memory concurrently to perform header matching.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08537828&OS=08537828&RS=08537828
owner: Oracle America, Inc.
number: 08537828
owner_city: Redwood City
owner_country: US
publication_date: 20120606
---
This application is a continuation of U.S. patent application Ser. No. 12 402 804 titled Scalable Interface for Connecting Multiple Computer Systems Which Performs Parallel MPI Header Matching filed Mar. 12 2009 now U.S. Pat. No. 8 249 072 whose inventors are Rabin A. Sugumar Lars Paul Huse and Bj rn Dag Johnsen which is hereby incorporated by reference in its entirety as though fully and completely set forth herein.

The present disclosure relates generally to clusters and more particularly to a scalable interface for interconnecting a plurality of computing platforms into a cluster.

A cluster generally refers to a group of computers that have been linked or interconnected to operate closely together such that in many respects they form a single computer. Large clusters can comprise thousands of individual computer systems that have been linked together. The components e.g. individual computers or compute nodes of a cluster are often connected to each other through local area networks. Clusters generally provide greatly improved performance and or availability over that provided by a single computer. A cluster is also typically more cost effective than a single computer of comparable speed or availability.

A cluster generally comprises a plurality of computer systems e.g. servers and the interconnect between these computer systems. An important aspect to building large cluster systems is the interconnect. The interconnect may comprise a fabric e.g. the network that connects all of the servers together as well as host adaptors that interface each of the computer systems servers to the fabric. One commonly used cluster interconnect is Ethernet. More recently clusters have started using Infiniband as the interconnect. InfiniBand is a switched fabric communications link primarily used in high performance computing and provides quality of service and failover features as well as scalability. An Infiniband interconnect generally provides lower latency higher bandwidth and improved reliability.

Many organizations such as research centers and national laboratories require large clusters with thousands of nodes to satisfy their compute requirements. It is very important to reduce the overhead of communication in such large clusters to allow applications to scale efficiently.

Clusters may use a protocol referred to as Message Passing Interface MPI for data communication. Current prior art implementations of MPI perform header matching searching in software. For example the message arrives at the receive process and the network adapter places it in a queue. A software layer then walks through the receive buffers in the queue to determine matches. In general performing a search via software on the host CPU impacts the message rate due at least in part to cache misses. As the queues become large the message rate becomes quite small. Another problem is that because hardware is first storing the incoming message in a support queue and then moving it from there to a receive buffer multiple copies of the payload may be required which further impacts performance.

Embodiments of the invention relate to a scalable interface useable in creating and or configuring clusters. Embodiments of the invention also provide improved cluster performance. The cluster may comprise a plurality of compute nodes e.g. servers connected by a fabric. Each of the compute nodes may comprise a host CPU and a host memory. The fabric may comprise a fabric switch which interconnects the compute nodes e.g. using Infiniband. The fabric may also couple the compute nodes to storage devices or to external networks such as the Internet. The compute nodes may communicate with each other using a Message Passing Interface MPI protocol

Each compute node may comprise a compute node interface device for interfacing between the compute node and the fabric. The compute node interface device is thus configured to interface between the compute node and other compute nodes in the cluster. The compute node interface device may comprise a memory that stores a plurality of posted receive queues and a plurality of unexpected queues. Each of the posted receive queues is configured to store receive requests from a process executing on the compute node wherein the receive requests include headers. The unexpected queues are each configured to store headers of send requests e.g. from other compute nodes that do not have a matching receive request in one of the posted receive queues. The memory may be implemented as a shared SRAM static random access memory structure.

The compute node interface device may also comprise a plurality of matcher units coupled to the memory. The matcher units are implemented in hardware and may be pipelined for improved performance. The plurality of matcher units e.g. a plural subset of the matcher units are configured to analyze one or more send requests received by the compute node interface device and perform header matching to determine if a header in the send request s matches any headers in any of the plurality of posted receive queues. The plural subset or all of the plurality of matcher units perform the header matching in parallel. In other words the plural matching units are configured to search the memory concurrently to perform header matching. In one embodiment only one matcher is assigned to search one request but multiple searches will be in progress concurrently for different send requests. In another embodiment a plurality of matchers are assigned to search each of one or more of the requests or all of the requests . In other words a plurality of matchers set may be assigned to search a single send request and different sets of matchers may operate concurrently i.e. may concurrently search the memory to perform header matching .

If a header in the send request matches a header in one of the plurality of posted receive queues the compute node interface device is configured to store the send request in a receive buffer of the memory e.g. as indicated by the matching receive request. If the send request header does not match any entries in the posted receive queues the send request header is stored in one of the unexpected queues. When a new receive request is later stored in the posted receive queue one or more and in one embodiment a plural subset typically only one matcher is assigned to search one request but multiple searches will be in progress concurrently of the plurality of matcher units is configured to analyze the new receive request and perform header matching in parallel to determine if a header in the new receive request matches any send request headers in any of the plurality of unexpected queues.

In one embodiment the memory is dynamically partitioned among the plurality of posted receive queues and unexpected queues to provide separate queues for different processes applications and or MPI ranks For example the compute node interface device may be configured to dynamically adjust memory allocation of the posted receive queues for different processes executing on the compute node. Further the compute node interface device may be configured to dynamically adjust relative memory allocation of the posted receive queues and the unexpected queues.

In addition receive requests may support wildcard values for one or more fields in the header of the receive request wherein a field with a wildcard value is not searched during the header matching. The compute node interface device may comprise a second memory which may be part of the first memory coupled to the plurality of matcher units that stores one or more header masks. For a first receive request comprising a header having a first field containing a wildcard value one or more of the matcher units is configured to use the header mask during header matching to mask out and hence ignore the first field containing the wildcard value.

The matcher units may be configured to perform header matching to implement a rendezvous data transfer protocol without host CPU intervention. In some embodiments the compute node interface device is configured to utilize resources of the compute node as needed e.g. dynamically. For example the compute node interface device may be configured to implement at least a subset of the unexpected queues in the host memory of the compute node.

In one embodiment the compute node interface device also comprises an unexpected message buffer coupled to the memory. The unexpected message buffer is configured to store a payload of a send request that does not have a matching header in any receive requests stored in the plurality of posted receive queues. Thus when a receive request is later stored in one of the posted receive queues and matched to this send request the payload can be accessed without requiring a memory to memory transfer.

The compute node interface device may also be configured to perform header matching for intra compute node transfers in response to a command received by host software executing on the compute node. Further the compute node interface device may be configured to synchronize header matching with other compute node interface devices comprised on the compute node.

While the invention is susceptible to various modifications and alternative forms specific embodiments thereof are shown by way of example in the drawings and are herein described in detail. It should be understood however that the drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.

Compute node refers to a computer system having a processor or CPU and memory. A Compute node may have a single processor which may be single core or multi core or may have a plurality of processors. One example of a Compute node is a blade server.

On the left side of exemplary the cluster comprises one or more racks each comprising 4 blade server chassis which each comprise a plurality of blade servers compute nodes . The blade servers on the left connect to the fabric switch through Infiniband. In one embodiment the blade servers connect to the fabric switch over Infiniband in all cases. As shown on the right side of the fabric switch may couple to additional racks having additional compute nodes .

Each of the compute nodes may comprise a compute node interface device referred to herein as Scalable Interface SIF which provides an interface for the respective compute nodes . The compute node interface device or SIF couples to a respective compute node and provides an IB interface for the compute node to the fabric e.g. to the fabric switch .

Each of the SIF blocks A D may couple to Infiniband switches referred to in as First Level IB switches . The First Level IB switches in turn may couple to Second Level IB switches as shown. The First Level IB switches may couple to the Second Level IB switches through cables as shown. There may be additional levels of IB switches.

As shown the SIF block includes a host bus interface for coupling to a computer system e.g. in blade server . The SIF block also includes a network fabric interface such as Infiniband interface for coupling to Infiniband e.g. for coupling to the network fabric .

The exemplary SIF block comprises a TU 1 Transaction Unit Requestor block and TU 2 Responder block . The TU 1 Requestor block generates processes requests that are provided to other computer systems. The TU 2 Responder block is responsible for responding to incoming packets e.g. incoming send requests from other compute nodes . The TU 1 Requestor block and the TU 2 Responder block may each couple to the host bus interface and the IB interface .

TU 1 is the Requester TU. All requests sent by SIF go through one of the TUs. SIF supports the Infiniband IB reliable communication protocol whereby in one implementation every request is properly acknowledged by the receiver before the request is completed at the sender. SIF supports multiple command registers for applications to deposit commands each command register typically dedicated to a host process running on a core. As the deposited commands are kicked off with doorbells the commands merge into a stream of commands that then feeds into the TU scheduler which schedules them onto available threads command slots on the 64 thread engine. A doorbell is a write issued by software running on a CPU to an address mapped to a device such as a network adapter which causes the device to perform some action such as sending a packet over the network fabric . Thus a doorbell is analogous to a house doorbell in the sense that it tells the device that some new action is needed.

TU 2 is the Responder TU. All incoming requests are directed to the Responder TU TU 2 which processes the packet ultimately delivering the packet to off chip or on chip memory or to other logic for processing collectives. The Responder TU TU 2 accesses a header matching block comprising a large header store that can hold many thousands of headers a buffer to hold a send request payload when a matching receive is not immediately found and many matcher units to perform multiple searches in parallel. When a successful match is found the incoming payload is directly copied into the receive buffer in memory.

MPI allows out of order association of receive buffers to incoming packets which may be based on a tag source or other criteria. For example assume a situation where two processes are communicating over the fabric. According to the process model the sending process sends a message. The receiving process makes available one or more receive buffers referred to as the posted receive queue or PRQ . The incoming message may be associated with any of the receive buffer and not necessarily the first one. The receiving process then examines the message and takes an appropriate action.

The MPI maintains characteristics for each receive buffer such as the sender the tag a value agreed upon by the sender and receiver type of message etc. Based on this characteristic the MPI searches the available receive buffers that have been posted by the receiving process posted receive queue and decides which of those receive buffers is the best candidate for the incoming message. Thus as described above the MPI header matching effectively walks down the list of available headers and finds the header that matches the incoming message.

The MPI header matching system in the SIF may also comprise a plurality of matcher units which may be implemented as matcher pipelines coupled to the Header Store . The matcher units also called matchers may comprise logic circuitry which implements one or more state machines. The matcher units may be implemented in various ways such as an ASIC Application Specific Integrated Circuit an FPGA or discrete logic or combinations thereof among others. The matcher units are implemented in hardware and may be pipelined for improved performance. The matcher units thus perform header matching in hardware as described herein. The matcher units may be comprised in the TU 2 block .

The MPI header matching system may further comprise Hardware Thread and DMA Control block . In one embodiment the SIF also comprises an Unexpected Message Buffer coupled to the Header Store . Unexpected send request messages can be stored in the Unexpected Message Buffer temporarily until a posted receive queue match is determined for these unexpected messages. More specifically the Unexpected Message Buffer is configured to store a payload of a send request that does not have a matching header in any receive requests stored in the plurality of posted receive queues . Thus when a receive request is later stored in one of the posted receive queues and matched to this send request the payload can be accessed without requiring a memory to memory transfer. The MPI header matching system further comprises a plurality of header queue descriptors e.g. header queue descriptors. The header queue descriptors may be used to access and control posted receive queues and unexpected queues as described further below.

The compute node interface device SIF may thus comprise a memory that stores a plurality of posted receive queues and a plurality of unexpected queues . Each of the posted receive queues is configured to store receive requests from a process executing on the compute node wherein the receive requests include headers. The unexpected queues are each configured to store headers of send requests e.g. from other compute nodes that do not have a matching receive request in one of the posted receive queues . The Header Store memory may be implemented as a shared SRAM static random access memory structure.

In one embodiment the Header Store is dynamically partitioned among the plurality of posted receive queues and unexpected queues to provide separate queues for different processes applications and or MPI ranks For example the SIF may be configured to dynamically adjust memory allocation of the posted receive queues for different processes executing on the compute node. Further the SIF may be configured to dynamically adjust relative memory allocation of the posted receive queues and the unexpected queues .

In the plurality of matcher units e.g. a plural subset of the matcher units are configured to analyze the send request received by the SIF in and perform header matching to determine if a header in the send request matches any headers in any of a subset or all of the plurality of posted receive queues . For example one or more receive requests may optionally have been received from a process executing on the first compute node and stored in one or more posted receive queues in the Header Store . In one embodiment each send request is typically searched against only one posted receive queue .

For example if the SIF includes 64 matcher units a plural subset e.g. 8 16 32 etc. or all all 64 may be used to perform the header matching. The plural subset or all of the plurality of matcher units perform the header matching in parallel. In other words the plural matching units are configured to search the memory concurrently to perform header matching. Thus for each incoming send request packet or message the matcher units examine or walk down the appropriate posted receive queue s to determine if there is a match. The plural matching units are configured to search multiple queues concurrently and or the plural matching units are configured to concurrently search the same queue. The matcher units are typically used to search different requests. For example the first incoming packet may be assigned to Matcher the second incoming packet may be assigned to Matcher etc. In one embodiment multiple matchers work in parallel on the same incoming packet to reduce delay.

In if a header in the send request matches a header in one of the posted receive queues the SIF is configured to store the send request in a receive buffer of the memory in . For example in the SIF may store the send request e.g. the payload of the send request in a receive buffer location indicated by the matching receive request. In if the send request header does not match any entries in the posted receive queues the send request header is stored in one of the unexpected queues in and the payload of the send request may be stored in the Unexpected Message Buffer at .

In one embodiment receive requests may support wildcard values for one or more fields in the header of the receive request wherein a field with a wildcard value is not searched during the header matching. The SIF may comprise a second memory which may be part of the Header Store memory coupled to the plurality of matcher units that stores one or more header masks. For a first receive request comprising a header having a first field containing a wildcard value one or more of the matcher units is configured to use the header mask during header matching to mask out and hence ignore the first field containing the wildcard value.

As shown in when a new receive request is later issued by host CPU in the posted receive queue at in one or more of the plurality of matcher units is configured to analyze the new receive request and perform header matching in parallel to determine if a header in the new receive request matches any send request headers in any of the plurality of unexpected queues . In one embodiment each posted receive is typically searched against only one unexpected queue . The plurality of queues is provided to allow multiple applications or processes to use the header matching facility. As noted above performance of the header matching in parallel may refer to one or more of the matcher units examining the one or more or plurality of unexpected queues wherein a plurality of matcher units may be operating for different requests concurrently. Alternatively a plurality of matcher units may be operating concurrently for a single request. If the header in the new receive request matches a send request header in one of the unexpected queues as determined in then in also in the send request e.g. the payload of the send request is stored in a receive buffer indicated by the matching receive request. If the posted receive does not match any entry in the unexpected queue the posted receive is stored in the posted receive queue in . In one embodiment storage in the posted receive queue PRQ happens only when there is no match in the unexpected queue UEQ .

Therefore in summary when an incoming message is received the MPI header matching searches the available receive buffers in the posted receive queue and in some cases does not determine any receive buffer matches for the incoming message. In these cases the incoming message is held in an unexpected queue until a matching receive buffer is posted. Thus as incoming messages arrive if the incoming messages do not match any posted receive buffer the incoming messages are placed in the unexpected queue . Then as the receive process posts new receive buffers those messages are searched against the unexpected queue first to determine if the message they are targeting has already arrived. Thus in one embodiment the MPI header matching utilizes searching of posted received messages and queues at the receiver for unexpected messages.

The matcher units may be configured to perform header matching to implement a rendezvous data transfer protocol e.g. without host CPU intervention. In some embodiments the SIF is configured to utilize resources of the SIF s compute node as needed e.g. dynamically. For example the SIF may be configured to implement at least a subset of the unexpected queues in the host memory of the SIF s compute node .

The SIF may also be configured to perform header matching for intra compute node transfers in response to a command received by host software executing on the compute node. Further the SIF on a compute node may be configured to synchronize header matching with other SIFs comprised on the same compute node. In another embodiment the SIF on a compute node may be configured to synchronize header matching with other SIFs comprised on other compute nodes in the cluster.

Therefore to summarize the above the system described herein performs MPI header matching in hardware for greater efficiency and reduced latency. Performance of MPI header matching in hardware is a considerable improvement over current prior art implementations.

As discussed in the Background Section above current prior art implementations of MPI perform header matching searching in software. For example the message arrives at the receive process and the network adapter places it in a queue. A software layer then walks through the receive buffers in the queue to determine matches. In general performing a search via software on the host CPU impacts the message rate due at least in part to cache misses. As the queues become large the message rate becomes quite small. Another problem is that because hardware is first storing the incoming message in a support queue and then moving it from there to a receive buffer multiple copies of the payload may be required which further impacts performance.

Another issue with current implementations is rendezvous interrupt overhead. The model described above which involves a sending process sending a message and the receiving process determining where the message should be placed works fairly well for reasonable sized messages. However when the message becomes very large e.g. in the Megabyte range the above method is very inefficient. For example with large messages it becomes undesirable to store the message temporarily while determining the appropriate receive buffer due to the size of the message. A rendezvous protocol is desired to avoid the cost of multiple copies of the message. A rendezvous protocol is a handshake method where the sending process sends a rendezvous request to a receiving process. The receiving process searches a portion of the receive queue and eventually finds a match. Once the receiving process has found a matching receive buffer the receiving process sends a pointer to the receive buffer an address to the sending process. Now the sending process has the address of the receive buffer and the sending process now has the capability to perform RDMA remote direct memory access to that receive buffer. RDMA allows data to be moved directly from the memory of one computer to the memory of another computer without involving either computer s operating system. This permits high throughput low latency networking If the sending process is not capable of RDMA it can still perform a send operation as described above now that the sending hardware knows that the matching receive has been posted.

If the header matching is not performed in hardware the software would be required to determine what receive buffer should be used. Whenever a rendezvous request arrives hardware would be required to interrupt the software to perform the matching immediately and provide the address back to the sender. These interrupts add a large amount of overhead.

Thus in one embodiment of the invention hardware based header matching is performed to eliminate the overhead described above. In one embodiment the MPI header matching system comprises on chip header storage and multithreaded matchers as shown in . This provides complete offload of matching from the host CPU on the compute node . Thus in one embodiment the sender sends a rendezvous request the header matching is performed in hardware and the hardware then generates a response to the sender containing the address of the receive buffer i.e. telling the sender the location where the data should be written. The sender can then perform an RDMA write to this address. Performance of header matching in hardware significantly and in one embodiment completely offloads the host CPU from having to perform this operation in software. In addition since the headers are held in the on chip structure i.e. the SIF the header store there are no cache misses.

The following provides greater detail on exemplary embodiments of MPI header matching. Note that the following description is exemplary only and other embodiments are also contemplated.

The Message Passing Interface MPI is a standardized language independent communication protocol or API Application Programming Interface used to program parallel computers. In MPI the processes involved in the execution of a parallel program are identified by a sequence of non negative integers referred to as the ranks If there are p processes executing a program they will have ranks 0 1 . . . p 1.

MPI supports both point to point and collective communication. MPI point to point communications performs message passing using send MPI Send and receive MPI Recv commands. Thus MPI Send and MPI Recv are the two basic point to point operations in MPI. The send command sends a message to a designated process and the receive command receives a message from a process.

int MPI Recv void buf int count MPI Datatype datatype int source int tag MPI Comm comm MPI Status status 

In sending a send request to the receive process the following fields and information may be packaged by the MPI library in software .

4. If the communication connection or resources are shared by all local processes an application ID assigned by an MPI scheduler may identify the receiving and sending MPI process 

5. Communication type reflecting the MPI send flavors e.g. collective immediate synchronous persistent ready buffered in line local or remote and

Similarly application receive requests specify header fields they target and where to place received payload data and associated request completion status. The MPI infrastructure operates to get the first send to the MPI rank whose header matches a receive target and deliver the corresponding payload to the application as data for the receive. Some fairness between senders to the same MPI process is assumed and messages are non overtaking i.e. if several send requests match a receive request the first match from the process is selected. The receive is allowed to use wild cards on source and tag any source any tag . The MPI message envelope refers to the tuple of source destination tag and communicator. The MPI 1.x standard requires tag range to be from 0 to no less than 32767 actual range defined by the attribute MPI TAG UB i.e. 16 bit or more.

In practice the above MPI specification results in two queues for each MPI process residing on the receiver node 

1. The posted receive queue PRQ holds receive requests that are posted by the application and not yet matched to any send request and

2. The unexpected send queue UEQ contains send requests that have been received from the fabric or local host and not yet matched to any receive request.

The PRQ and UEQ queues and can reach up to 100 s of entries per MPI rank. As discussed above with respect to when the application posts a receive the matcher units first sequentially in order matches against the entries in the UEQ until a match is found if any or if no match is found the request is atomically appended to the PRQ . When a packet is received from the fabric e.g. a local send request is posted the matcher units first sequentially in order matches against the entries in the PRQ until a match is found or if no match is found in the PRQ the request is atomically appended to the UEQ . The atomic append is used to maintain the nonovertaking requirement from the MPI standard.

As described herein the header matching is performed in hardware by a plurality of matcher units which operate to concurrently or simultaneously analyze the respective queues for matches. This provides performance benefits over a software implementation where both the UEQ and the PRQ are maintained in software and software executes code to match send requests to posted receives.

In a software implementation when queues are large matching overhead is significant and could become a performance limiter. In addition given the delay of interrupting the host CPU and performing the matching it is not practical to hold incoming data on the SIF until the match is determined. Accordingly the SIF may copy data to a temporary buffer in system memory and when software identifies the match data is copied from the temporary buffer to the receive buffer. Also in an embodiment where an embedded processor is present on the SIF the embedded processor may not have sufficient compute power to handle the matching in software. Each SIF can have 16 or 32 associated cores MPI ranks which is too great of a processing burden for one embedded processor to support.

Thus in one embodiment of the present application as described herein the PRQ and the UEQ are maintained in hardware in SIF SRAMs and incoming packets and new receives posted by host software are matched in hardware. The sections below describe the hardware structures that are implemented on the SIF the programming interfaces host software uses to setup queues and control the matching and the copy operations.

SIF has a set of NUM RESP HW THDS NUM RESP HW THDS 32 in one embodiment hardware threads HWTs for processing incoming IB packets. These threads have overall responsibility for parsing incoming packets performing IB specified validity checks determining disposition of packets to host memory to embedded processor to header matching initiating DMA reads and writes to host memory and returning acks or naks to the requesting node. A payload buffer is associated with the threads to hold incoming packets while they are being processed.

SIF may provide an unexpected message buffer UMB to hold unexpected messages. In one embodiment the size of the UMB is 512 KB. The buffer is broken into smaller fragments 64B or 128B . Multiple fragments are combined in a linked list to form larger messages. This structure is illustrated in . Fragments are allocated from a free list as messages are entered into the UMB and fragments are freed later as matches are identified and messages are copied into host memory .

SIF has a large header store e.g. size 1 MB for holding posted receive headers and unexpected message headers illustrated in . Headers may be maintained in linked lists one for posted receives and one for unexpected sends. There is typically one pair of linked lists per MPI rank e.g. at 32B per header and 32 MPI ranks per SIF a 1 MB header store has space for up to 1K header queue entries per MPI rank. Software may divide available header store space flexibly among MPI ranks and between posted receive queues and unexpected queues . For example when one rank handles most communication software may allocate a high fraction of available headers to that rank and reduce allocation for other ranks Software may also provide more than one queue for an MPI rank partitioning posted receives among the queues based on sender MPI rank communicator or tag. Such partitioning shortens search queue lengths however this partitioning may not be performed when wildcards are used depending on what field is wildcarded and how partitioning is done. There are NUM HDR QUEUES NUM HDR QUEUES 128 for instance pairs of header queues per SIF. Each header queue is described by a descriptor. As shown in the payload pointer of entries in the unexpected queue may point to data in the unexpected message buffer . The unexpected message buffer may be maintained as linked lists of buffer fragments to allow messages of various sizes to be co resident and allow flexible allocation and deallocation of buffer space.

Header structures are described below. The header match word used on compares maybe 64 bits where the most significant two bits may encode wild cards source tag and other MPI matching values are then located in this 64 bit word.

Type Comm App Software defined encoding of application communication group and type collective or not for instance variable width 

Message location address Address of location containing message. This address may be a pointer to a fragment in the UMB a virtual or physical address in local memory or a connection identifier to use for the rendezvous ack.

The two most significant bits of the match word are reserved on unexpected headers. They correspond to the wildcard encoding bits of the posted receive header.

An all zero match word indicates an invalid header. Software should avoid all zero scenarios when creating match words.

WC Wildcard encoding. 00 no wildcard 01 wildcard tag 10 wildcard rank 11 wildcard tag rank. When tag is wildcarded Mask of the queue descriptor is used to mask out the tag field when matching. When rank is wildcarded Mask of the queue descriptor is used to mask out the rank field when matching.

Match Compl ID Completion ID deposited in completion entry when there is a successful header match. This field is provided by software when posting the receive for later identification. Others field definitions are the same as before. The buffer is preferably located in local host memory . An all zero match word indicates an invalid header. Software should avoid all zero scenarios when creating match words.

Each matcher unit is a finite state machine that may implement a pipelined read and compare loop. SIF will provide multiple matcher units that can be scheduled to search different queues in a flexible manner. Each matcher can sustain a rate of 1 match per cycle in the absence of memory port conflicts with other matchers . A matcher searching a queue will not overtake another matcher that started earlier and is searching the same queue.

The headers are stored in the header store which may comprise a banked SRAM structure with enough bandwidth to sustain matching at least when access patterns are well behaved.

A host bus request describing the posted receive to be added is written to a doorbell in hardware. One doorbell is provided for each supported header queue up to NUM HDR QUEUES per SIF. Privileged software maps doorbells to a user process allowing the user process to access the doorbell directly.

Structure of posted receive request packet to hardware mirrors the structure of the posted receive header shown in the previous section.

CR If set to 1 completion is performed when receive is successfully added to PRQ . Else an explicit completion is not needed.

Req Compl ID Software provided ID for request included in posted completion. For use by software to associate request with completion.

Software may wish to remove posted receives either when it hits a situation where a receive posted earlier is no longer applicable or when the message to the posted receive is delivered in a manner that does not use SIF e.g. an intra node message .

A host bus request describing the posted receive to be removed is written to a doorbell in hardware. One doorbell is provided for each supported header queue up to NUM HDR QUEUES per SIF. Privileged software maps doorbells to a user process allowing the user process to access the doorbell directly.

The request packet to remove a posted receive to hardware is similar to the request packet to add a posted receive. Explicit completions may be performed when removing posted receives.

Posted receive queues and unexpected queues are accessed and controlled through a descriptor. There are NUM HDR QUEUES descriptors one for each of the PRQ UEQ pairs supported on SIF. Access to descriptors is typically restricted to privileged software.

Compl Q Completion Queue associated with descriptor when a successful header match occurs an entry is posted in the completion queue 

MMU Ctxt MMU context to use when a match is found and a message is to be written to a receive buffer in host memory 

An incoming packet from the IB fabric to be header matched against posted receives includes a bit in a special IB header field same field that contains the receive queue pointer indicating that it should be matched. The packet does not contain a pointer to the header queue to which the packet should be matched against. Instead the packet s target receive queue descriptor contains a pointer to the header queue.

Registers are provided that software can write to force a dump of selected queue contents to specified locations in memory. This facility is used to move headers from SIF to memory when software wishes to take over header matching for instance when a queue overflows.

Incoming IB packets are accepted by a free HWT validated against the current state of the connection or QP in IB terminology and checked for a valid protection domain at the receive queue. When header matching is enabled in hardware when the incoming packet indicates that it should be header matched and when the receive queue points to valid header matching queues the payload matching field is sent to a free one or more matchers when all matchers are busy the HWT waits for one to free up . The matcher s e.g. one matcher or a plurality of matchers operating concurrently or in parallel walks through the PRQ and returns a match result. If a valid match is found the matching header is removed from the PRQ and the receive buffer pointer from the header is returned to the HWT. The HWT first checks if the receive buffer is large enough to hold the incoming message and then proceeds to copy the incoming message into the receive buffer. Once the copy is done a completion entry is written into the completion queue associated with the header queue with status set to SUCCESSFUL MATCH COPY COMPLETE. When the receive buffer is not large enough a NAK may be returned to sender with a Remote Operational Error code or the payload may be written to the buffer up to the size of the buffer .

When no match is found in the PRQ for that rank the message is an unexpected message. A header is added to the UEQ after a synchronization sequence to prevent a race between the insertion of the message into the UMB and the addition of new posted receives from the host to the PRQ . The packet payload is deposited in the UMB.

New posted receives from the host are allocated to the next available matcher or plurality of matchers and are checked against the UEQ . When a matcher is not available the posted receives wait for one to become available flow control is in hardware. When a matcher becomes available the matcher picks up the next posted receives and walks through the UEQ for the rank and returns a match result. If a match is found the matching entry is removed from the UEQ and a HWT is picked for service. The HWT initiates a DMA to move the payload into the UMB corresponding to the matching UEQ entry into the buffer in system memory provided by the matching receive and writes a completion entry into the completion queue associated with the header queue with status set to SUCCESSFUL MATCH COPY COMPLETE. When no match is found the header is added to the PRQ after a synchronization sequence to prevent a race condition between the addition of the posted receive into the PRQ and the addition of incoming packets to the UEQ .

When the UMB is full the payload of an incoming packet is deposited in a temporary buffer in system memory and the header is added to the UEQ the UEQ header carries the location of the payload as described herein e.g. .

Overflow of the UMB does not stop hardware header matching. When a match is identified on a UEQ entry whose payload is in host memory SIF will either 

1. Have the HWT initiate DMA reads and writes to copy the message from host memory to the target receive buffer and then write a completion entry with status set to SUCCESSFUL MATCH COPY COMPLETE or

2. Write a completion queue entry with pointer to the message in memory and the ID of the matching posted receive and with status set to SUCCSSFUL MATCH NO COPY. Software will perform the copy.

When a header queue PRQ or UEQ overflows further packet processing is stopped for incoming packets that target the overflowing queue pair and a host CPU is interrupted with the interrupt group of the overflowing queue s descriptor. Incoming packets targeting this queue are RNR Resource Not Ready NAKed Negative Acknowledged back to the sender until software resets the status bit in the descriptor or disables header matching for the associated receive queues. A negative acknowledge is similar to a no thank you . When a responder receives a message from a requester and the targeted resource say a receive buffer is temporarily not available the Infiniband protocol requires that the responder send a RNR NAK to the requester indicating that the resource is currently not available. The requester may retry later.

Host software treats header queue overflow as a slow case. Several handling options are possible at increasing levels of complexity and performance.

When a node needs to send a large message for instance greater than IB MTU size a rendezvous protocol may be used on the SIF where the sender and receiver first perform a handshake to exchange receive buffer information followed by the sender performing an RDMA to transfer data and a message to signal completion of transfer. Typically when header matching is in software the sequence requires host software involvement to perform the handshake. However since the SIF performs the header matching in hardware the handshake can be accomplished without host involvement eliminating latency and overhead of host interrupts. A rendezvous sequence on SIF comprises the following steps 

As indicated above the SIF holds large messages in the sender until the corresponding receive buffer is posted at which point the sender RDMAs the content buffer to the receive buffer. This rendezvous approach reduces memory bandwidth on the receiver by avoiding the write and read from a temporary buffer at the receive node. However it might result in a slight increase in latency since the receiver has to communicate with the sender across the fabric when the receive buffer is posted.

Keeping send resources active while waiting for the rendezvous to complete provides the best rendezvous performance. However it may block other requests to the same connection from completing during the wait. If this is an issue software may allocate dedicated connections for rendezvous. Another approach is to send information on the address of the send buffer to the receiving node along with the original rendezvous request and release send resources. The receiving node which may comprise client software or embedded logic will perform an RDMA read of the send buffer when the rendezvous match occurs.

As an illustration of these different approaches we describe four rendezvous scenarios classified based on whether the match occurs immediately and based on the size of the rendezvous request. The four scenarios are 

In one embodiment when rendezvous size is very large even when receive is posted at the time of rendezvous arrival the rendezvous is completed in software to limit continuous HWT use by a few requesters. If the receive is not posted when the rendezvous request arrives the sender preferably does not wait for the receive to be posted it is a non deterministic wait time and also prevents other requests using the same connection from making forward progress. To avoid the sender waiting the transaction may be completed from sender hardware in one of two ways either execute an RDMA read from the responder node or transition over to a software rendezvous on larger rendezvous requests .

The four scenarios are described in greater detail below using one embodiment where rendezvous requests and responses are sent over Infiniband using the ATOMIC fetchadd request and response packets.

Sending node sends a rendezvous request to the receiving node as a special ATOMIC fetchadd request. The requesting HWT on the sender then waits for a response from the receiver. The receiving node handles the rendezvous request as an incoming message and searches against the PRQ . It will find a match in the PRQ by definition in this scenario . When the match is found the receiving node provides the address and size of the receive buffer to the requesting node along with match completion information in the ATOMIC acknowledge response. When the HWT at requesting node receives the response and the request size is less than a threshold the HWT performs an RDMA write to the buffer up to the provided size. It then sends a message to the receiving node with the completion information. The receiving node processes the completion similar to a normal successful header match completion.

When the request size is greater than a threshold the HWT posts a completion with information on the target buffer. Software completes the rendezvous by issuing one or more RDMA write requests.

Similar to the prior case the sending node sends a rendezvous request to the receiving node as a special ATOMIC fetchadd request. The requesting HWT on the sender then waits for a response from the receiver. The receiving node handles the rendezvous request as an incoming message and searches against the PRQ and does not find a match by definition in this scenario . The rendezvous request is added to the UEQ . When size is below set threshold an ATOMIC acknowledge response is sent to the sender indicating receipt of the rendezvous request and a pending completion. The sender then completes the rendezvous request locally but in a pending state. Hardware at the sender side no longer participates in the rendezvous and the connection may be used by other requests. However since this is a pending completion the application may not reuse the buffer yet i.e. the MPI or other framework does not signal an application level completion.

Later when a matching receive is posted at the receiver a match against the rendezvous is detected and the rendezvous request is removed from the UEQ . The original receiver sends an RDMA read request to the original sender node. The original sender node then reads data out of the original send buffer and returns it and the data is written into the posted receive buffer at the original receiver node. At the completion of the RDMA read the original receiver node sends a completion message to the original sender node. Software on the sender upon receiving the completion knows that the original Rendezvous request is complete and can reclaim the send buffer.

Similar to the prior case the sending node sends a rendezvous request to the receiving node as a special ATOMIC fetchadd request. The requesting HWT on the sender then waits for a response from the receiver. The receiving node handles the rendezvous request as an incoming message and searches against the PRQ and does not find a match by definition in this scenario . The rendezvous request is added to the UEQ . When size is above set threshold an ATOMIC acknowledge response is sent to the sender indicating that a match was not found. At this point the sender completes the rendezvous request indicating a pending completion. Hardware at the sender side no longer participates in the rendezvous and the connection may be used by other requests. However since this is a pending completion the application may not reuse the buffer yet i.e. the MPI or other framework does not signal an application level completion.

Later when a matching receive is posted at the responder a match against the rendezvous is detected and the rendezvous request is removed from the UEQ . The responder node sends a message to sender with the buffer address. This message may be initiated by hardware or software on the responder. To initiate through software hardware first posts a completion for the posted receive that software interprets to send the message. At the sender the HWT that initiated the rendezvous request is no longer active and the response is similar to a normal message that is processed by software. Software on the sender then issues the RDMA write and once the write is complete it sends a completion to the responder and also completes the rendezvous locally at which point the rendezvous is complete. Software may also choose to perform the data transfer by having the original receiver node issue RDMA reads to obtain the data.

When the unexpected message queue becomes too large or when unexpected messages are rare software may use this option to conserve header store . Here the PRQ is still searched by hardware and on a match the incoming packet is directly moved into the receive buffer in memory. However the unexpected queue is maintained in host memory and searched by software.

Software searches the unexpected queue in host memory . If a match is found the matching entry from the unexpected queue is associated with the receive when the message is on the SIF on chip software initiates a DMA to the new receive buffer and when the message is in host memory software or hardware performs the copy.

When posted receive does not find a match in the unexpected queue synchronization may be performed to prevent an incoming packet from going into the unexpected queue after the unexpected queue has been searched but before the receive is added to the PRQ .

Step 1 After unexpected queue is searched send a signal to hardware that a new posted receive is to be added. The signal will likely be in the form of a posted write to a SIF register.

Step 2 When hardware receives the signal it stops adding incoming packets to the unexpected queue for the rank it can continue to process incoming packets that match and sends a signal to software that it has stopped adding to the unexpected queue . This signal may be an update to a SIF internal register or memory location that software will poll.

Step 3 When software gets the signal from hardware it performs a check of new entries that may have been added to the unexpected queue during the signaling window and on no match in new entries adds posted receive to hardware through a doorbell write and enables further unexpected queue additions.

Send the posted receive optimistically to hardware without a synchronization sequence assuming that in the common case there is no new entry added to the unexpected queue during the race window that will match the new posted receive. Include a tag identifying the last entry in the unexpected queue checked prior to sending the posted receive to hardware. Hardware will check the tag against the last unexpected it has processed for the rank. When tags are the same the posted receive can be added right away. When tags are different either maintain a set of recent unexpecteds in a small content addressable memory CAM in hardware for a quick check or indicate a failure to software and have software retry the posting perhaps with full synchronization the second time . The term unexpected refers to entries in the unexpected queue or incoming messages that don t find any match in the posted receive queue .

An application may need to send intra node messages i.e. messages from one MPI rank to another within the same SMP node. Intra node transfers may use the hardware header matching infrastructure by sending the message through a loopback path back to the node. The message will then enter the header matching infrastructure and be handled in a manner similar to messages from remote nodes. Performing the transfer through such a loopback might be simplest to implement since the any source wild card allowed on receives implies that intra node and inter node sends might target the same set of receive buffers and implementations need to ensure that the same buffer does not get associated with two sends. In certain situations e.g. communication dominated by intra node transfers implementations might want to use SMP communication mechanisms to send intra node messages instead of loop back. Such implementations may use a software based header matching scheme and or the posted receive removal mechanism described above to identify and remove target buffers for intra node sends.

When there are multiple SIFs on a node each SIF performs its own header matching independent of the others. If a single MPI rank receives messages from more than one SIF it becomes complex for software to maintain PRQs in all the SIFs and keep them synchronized.

For the general application scenario when multiple SIFs are on a platform it is recommended that each MPI rank be allowed to receive from one and only one SIF . When a MPI rank is created it is assigned a SIF and from thereon all communications targeting the MPI rank arrives through that SIF .

This might reduce the interconnect bandwidth that one MPI rank can exploit. On applications where such a reduction in interconnect bandwidth per MPI rank is undesirable on a case by case basis specialized schemes in software may be implemented where an MPI rank is allowed to use multiple SIFs while maintaining a consistent view of PRQs among the SIFs e.g. when an application does not use any source wildcards paths may be partitioned among SIFs based on source rank . In another embodiment a flag is associated with each receive header that is posted to multiple SIFs . The flag is set to zero when the receive is posted and is atomically fetched and set to a non zero value when there is a match on the header. If the fetch and set returns zero the match is valid. If the fetch and set returns non zero another SIF has already taken the posted receive the entry is deleted and search continues.

Although the embodiments above have been described in considerable detail numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications.

