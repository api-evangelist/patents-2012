---

title: Offloading health-checking policy
abstract: Methods and systems for offloading health-checking policy in a distributed management environment are provided. A failure policy is received at a node of a cloud from a cloud health monitor. The node transmits a notification to a health monitor of the node that the node has failed when the failure policy is satisfied. The node reports at least one fault based on the satisfied failure policy to the cloud health monitor.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09026864&OS=09026864&RS=09026864
owner: Red Hat, Inc.
number: 09026864
owner_city: Raleigh
owner_country: US
publication_date: 20120229
---
Embodiments of the present invention relate to distributed management environments including cloud computing systems and more specifically to a method and apparatus for offloading health checking policy in a distributed management environment.

Cloud computing is the provision of dynamically scalable and often virtualized resources as a service over the Internet on a utility basis. Users need not have any knowledge of expertise in or control over the technology infrastructure in the cloud that supports them. Cloud computing services often provide common business applications online that are accessed from a web browser while the software and data are stored on servers.

Cloud computing customers do not generally own the physical infrastructure serving as host to the software platform in question. They typically consume resources as a service and pay only for resources that they use. The majority of cloud computing infrastructures typically include services delivered through data centers and built on servers with different levels of virtualization technologies. The services are accessible from various locations that provide access to networking infrastructure. Clouds often appear as single points of access for all consumers computing needs.

Cloud computing is quickly becoming the platform of choice for businesses that want to reduce operating expenses and be able to scale resources rapidly. Eased automation flexibility mobility resiliency and redundancy are several other advantages of moving resources to the cloud. On premise private clouds permit businesses to take advantage of cloud technologies while remaining on a private network. Public clouds permit businesses to make use of resources provided by third party vendors. Hybrid clouds permit the best of both public and private cloud computing models. Many organizations are being introduced to cloud computing by building an on premise Infrastructure as a Service IaaS cloud which delivers computing storage and networking resources to users. Some organizations utilize cloud computing technology in an evolutionary way that leverages and extends their existing infrastructure and maintains portability across different technology stacks and providers.

One or more physical host machines or virtual machines VMs may be employed in a cloud hereinafter referred to as nodes . For VMs each VM may function as a self contained platform running its own operating system OS and software applications processes . Typically a virtual machine monitor VMM manages allocation and virtualization of computer resources and performs context switching as may be necessary to cycle between various VMs. Virtualization systems provide a potential means to access computing resources in a confidential and anonymous way.

High availability when applied to computer systems in general and cloud computing systems in particular refers to the application of well known techniques to improve availability A as defined by the equation A MTBF MTTR MTBF where MTTR refers to mean time to recovery and MTBF refers to mean time between failures. MTBF is the predicted elapsed time between inherent failures of a system during operation. MTTR is the average time that a device may take to recover from any failure. Reducing MTTR may include the automation of manual operations of activities such as but not limited to fault detection fault isolation fault recovery and administrative repair.

For software increasing MTBF may include but is not limited to technical source code reviews high quality automated validation minimizing complexity and employing software engineers having a mixture of levels of experience. For hardware increasing MTBF may include but is not limited to using higher quality components preemptively replacing hardware components prior to predicted wear out and employing a sufficient burn in period to remove infant mortalities from a product delivery stream.

In current cloud computing systems a management component of the cloud computing system typically polls for data concerning the health of managed components from one centralized location. These managed components may be nodes which may include one or more virtual machines in a network infrastructure. The centralized management component may periodically poll a node for state information such as how much memory is consumed how much disk space is consumed the system load or other details over the network. The management component then applies a policy to detect if a node is faulty e.g. the memory consumed is greater then 98 based on data returned by the node.

Periodically polling nodes for state information and having a node transmit back to the management component state results may consume significant network resources and slow the time required to detect a failure. The slower detection time results in a higher MTTR and results in lower availability.

Methods and systems for offloading health checking policy in a distributed management environment are described herein. In one embodiment a node e.g. a physical host machine a virtual machine or VM running on a host machine or a collection of virtual machines i.e. a deployable running on a host machine of a cloud receives a failure policy from a cloud health monitor e.g. of a cloud controller . The failure policy may be received at startup initialization of the node or at a later date and time. The failure policy may be based on a state of the node. The failure policy is a descriptor of system state that must be in bounds. The failure policy is one or more system statistics as a component for equation testing including for example memory or CPU usage and system load. The failure policy may be at least one configuration containing one or more policy equations applicable to all nodes in the cloud or to a particular node.

The node then transmits a notification to a health monitor of the node that the node has failed when the failure policy is satisfied. The node reports at least one fault based on the satisfied failure policy to the cloud health monitor. The node may mark itself as failed and indicate it is faulty through its fault notification mechanisms. As a result the fault is reported to the cloud health monitor without the latter polling for each individual metric.

Policy decision execution is offloaded into the network node while the activity of deciding the policy still occurs in the central management system. This mitigates network consumption problems and reduces MTTR which improves availability by permitting very fast metric gathering using the CPU speed of a locally managed node without involving the network to transmit the data using network speed which is 100 000 s times slower . As a result availability improves and significant network resource utilization is reduced.

In the following description numerous details are set forth. It will be apparent however to one skilled in the art that the present invention may be practiced without these specific details. In some instances well known structures and devices are shown in block diagram form rather than in detail in order to avoid obscuring the present invention.

As illustrated a break out box of the cloud shows the actual cloud resources including hardware that may be employed by embodiments of the invention as computing resources of the cloud . In one embodiment one or more organized workstations or host machines may be utilized to execute a plurality of virtual machines VMs i.e. the nodes that may be used as cloud computing resources. In embodiments of the invention each host machine is capable of running one or more virtual machines VMs . Each of the VM s runs a guest operating system OS that may be different from one another. The guest OS may include Microsoft Windows Linux Solaris etc. The host machine may include a hypervisor that emulates the underlying hardware platform for the VMs . The hypervisor may also be known as a virtual machine monitor VMM a kernel based hypervisor or a host operating system. In one embodiment each of the VM may be accessed by one or more of clients over a network not shown . The network may be a private network e.g. a local area network LAN wide area network WAN intranet etc. or a public network e.g. the Internet .

In another embodiment one or more organized workstations or physical machines i.e. the nodes may be utilized directly as cloud computing resources.

In one embodiment a cloud health monitor of the cloud controller is configured to oversee offload and distribute a failure policy to health monitoring components of nodes i.e. the VMs the deployables not shown in see and the physical machines of a cloud e.g. the corresponding VM health monitors the collection of VMs health monitors of described below and the node health monitors respectively .

A node health monitor e.g. is configured to transmit a notification from the node e.g. the VM that the node has failed when the failure policy is satisfied. The node health monitor e.g. that has receive a notification of a failure from a node e.g. the VM based on the satisfied failure policy reports the failure to the cloud health monitor of the cloud controller of the cloud .

Each cloud is managed by a cloud controller . In one embodiment the cloud controller is part of an enterprise virtualization solution. The cloud may include one or more physical machines . In one embodiment individual applications not shown may be instantiated started and executed on one or more of the individual physical machines . The one or more physical machines may each include an active node health monitor e.g. which is configured to apply a failure policy and to receive health status of the physical machine e.g. internally.

The cloud may include one or more host machines each including a hypervisor configured to virtualize the resources of the host machine for the execution of one or more VMs . In one embodiment individual applications not shown may be instantiated started and executed on one or more of the individual VMs . The VMs may be grouped into corresponding assemblies . An assembly e.g. one of may include a virtual machine e.g. one of plus an active node health monitor e.g. one of which is configured to apply a failure policy and to receive health status of the virtual machine e.g. one of internally. One or more of the assemblies may be grouped into a deployable . In one embodiment a deployable may be defined as a collection of assemblies e.g. the individual VMs their corresponding hypervisor plus an active node health monitor e.g. a collection of virtual machines VM health monitor within the hypervisor configured to apply a failure policy and to receive health status of the collection of VMs internally. Although depicts one deployable mapped to one hypervisor in another embodiment multiple deployables may run on one hypervisor .

As used herein a failure policy is a descriptor of system state that must be in bounds. When operating out of bounds the policy will indicate a failure. A failure policy may include an individual policy or multiple policies for example 

These individual policies may be combined to form a failure policy. The failure policy may be stored in the cloud controller s data storage not shown and may be transmitted to a monitoring component on initialization.

Finally the cloud controller may include a cloud health monitor configured to apply a failure policy and to receive health status of one or more nodes internally. In response to a transmission of a failure policy to one or more nodes at system startup when one or more failure criteria corresponding to the failure policy are reached by internal monitoring components of a node the node may asynchronously report a failure status to a corresponding health monitor component respectively. The node may then mark itself as failed.

In an embodiment a high availability cloud service is operable to deliver maximum application service availability for a node . This is achieved by the detection and recovery of failures in any of the following components monitored applications not shown physical machines assemblies and deployables . Recovery from a detected failure may require terminations of components of a physical machine an assembly or a deployable . The restarting of components is controlled by the cloud health monitor the collection of VMs health monitor or the individual node monitors .

At block a node of a cloud receives a failure policy from a cloud health monitor e.g. . In the node may receive the failure policy at startup initialization of the node or at a later date and time from for example the cloud health monitor . The failure policy may be based on a state of the node including for example memory or CPU usage and system load. In one embodiment the failure policy may be a configuration containing one or more policy equations applicable to all nodes in the cloud or to a particular node . In another embodiment the failure policy may not be an equation but an out of band operation of an on system command to check the health of the system. More generally a failure policy may include any type of system statistic as a component for equation testing even those that may not yet exist or are exported from the system.

One example of a set of failure policy equations sent to managed node on system start up may be as follows 

At block the node e.g. the VM transmits a notification to a health monitor of the node that the node has failed when the failure policy is satisfied. At block the node e.g. the VM reports at least one fault based on the satisfied failure policy to the health monitors e.g. one or more of the health monitors . The node e.g. the VM may mark itself as failed and indicate it is faulty through its fault notification mechanisms. Fault notification mechanisms may include but not limited to a bus based QPID architecture a simple TCP connection a REST interface or any other multi system transport mechanism. As a result the fault is reported to the management component without the latter polling for each individual metric.

The exemplary computer system includes a processing device a main memory e.g. read only memory ROM flash memory dynamic random access memory DRAM such as synchronous DRAM SDRAM or Rambus DRAM RDRAM etc. a static memory e.g. flash memory static random access memory SRAM etc. and a data storage device which communicate with each other via a bus .

Processing device represents one or more general purpose processing devices such as a microprocessor central processing unit or the like. More particularly the processing device may be complex instruction set computing CISC microprocessor reduced instruction set computer RISC microprocessor very long instruction word VLIW microprocessor or processor implementing other instruction sets or processors implementing a combination of instruction sets. Processing device may also be one or more special purpose processing devices such as an application specific integrated circuit ASIC a field programmable gate array FPGA a digital signal processor DSP network processor or the like. Processing device is configured to execute device queue manager logic for performing the operations and steps discussed herein.

Computer system may further include a network interface device . Computer system also may include a video display unit e.g. a liquid crystal display LCD or a cathode ray tube CRT an alphanumeric input device e.g. a keyboard a cursor control device e.g. a mouse and a signal generation device e.g. a speaker .

Data storage device may include a machine readable storage medium or more specifically a computer readable storage medium having one or more sets of instructions e.g. processing logic embodying any one or more of the methodologies of functions described herein. Processing logic may also reside completely or at least partially within main memory and or within processing device during execution thereof by computer system main memory and processing device also constituting machine readable storage media. Processing logic may further be transmitted or received over a network via network interface device .

Machine readable storage medium may also be used to store the device queue manager logic persistently. While machine readable storage medium is shown in an exemplary embodiment to be a single medium the term machine readable storage medium should be taken to include a single medium or multiple media e.g. a centralized or distributed database and or associated caches and servers that store the one or more sets of instructions. The term machine readable storage medium shall also be taken to include any medium that is capable of storing or encoding a set of instruction for execution by the machine and that causes the machine to perform any one or more of the methodologies of the present invention. The term machine readable storage medium shall accordingly be taken to include but not be limited to solid state memories and optical and magnetic media.

The components and other features described herein can be implemented as discrete hardware components or integrated in the functionality of hardware components such as ASICs FPGAs DSPs or similar devices. In addition these components can be implemented as firmware or functional circuitry within hardware devices. Further these components can be implemented in any combination of hardware devices and software components.

Some portions of the detailed descriptions are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here and generally conceived to be a self consistent sequence of steps leading to a desired result. The steps are those requiring physical manipulations of physical quantities. Usually though not necessarily these quantities take the form of electrical or magnetic signals capable of being stored transferred combined compared and otherwise manipulated. It has proven convenient at times principally for reasons of common usage to refer to these signals as bits values elements symbols characters terms numbers or the like.

It should be borne in mind however that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the above discussion it is appreciated that throughout the description discussions utilizing terms such as enabling transmitting requesting identifying querying retrieving forwarding determining passing processing disabling or the like refer to the action and processes of a computer system or similar electronic computing device that manipulates and transforms data represented as physical electronic quantities within the computer system s registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage transmission or display devices.

Embodiments of the present invention also relate to an apparatus for performing the operations herein. This apparatus may be specially constructed for the required purposes or it may comprise a general purpose computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a computer readable storage medium such as but not limited to any type of disk including floppy disks optical disks CD ROMs and magnetic optical disks read only memories ROMs random access memories RAMs EPROMs EEPROMs magnetic or optical cards flash memory devices including universal serial bus USB storage devices e.g. USB key devices or any type of media suitable for storing electronic instructions each of which may be coupled to a computer system bus.

The algorithms and displays presented herein are not inherently related to any particular computer or other apparatus. Various general purpose systems may be used with programs in accordance with the teachings herein or it may prove convenient to construct more specialized apparatus to perform the required method steps. The required structure for a variety of these systems will be apparent from the description above. In addition the present invention is not described with reference to any particular programming language. It will be appreciated that a variety of programming languages may be used to implement the teachings of the invention as described herein.

It is to be understood that the above description is intended to be illustrative and not restrictive. Many other embodiments will be apparent to those of skill in the art upon reading and understanding the above description. Although the present invention has been described with reference to specific exemplary embodiments it will be recognized that the invention is not limited to the embodiments described but can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense. The scope of the invention should therefore be determined with reference to the appended claims along with the full scope of equivalents to which such claims are entitled.

