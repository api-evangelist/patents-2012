---

title: GPU memory buffer pre-fetch and pre-back signaling to avoid page-fault
abstract: This disclosure proposes techniques for demand paging for an IO device (e.g., a GPU) that utilize pre-fetch and pre-back notification event signaling to reduce latency associated with demand paging. Page faults are limited by performing the demand paging operations prior to the IO device actually requesting unbacked memory.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09134954&OS=09134954&RS=09134954
owner: QUALCOMM Incorporated
number: 09134954
owner_city: San Diego
owner_country: US
publication_date: 20120910
---
This disclosure relates to techniques for graphics processing and more specifically to techniques for pre fetch and pre back signaling from a graphics processing unit GPU to avoid page faults in a virtual memory system.

Visual content for display such as content for graphical user interfaces and video games may be generated by a graphics processing unit GPU . A GPU may convert two dimensional or three dimensional 3D objects into a two dimensional 2D pixel representation that may be displayed. In addition GPUs are being increasingly used to perform certain types of computations that are more efficiently handled by the highly parallel nature of GPU cores. Such applications are sometimes called general purpose GPU GPGPU applications. Converting information about 3D objects into a bit map that can be displayed as well as large GPGPU applications requires considerable memory and processing power. Often inefficiencies in GPU processing may occur during memory access as there is a lack of techniques for stopping and resuming the highly parallel jobs executing on a GPU. As such complex and expensive memory controllers as well as central processing unit CPU oversight is used to improve memory access efficiency.

In general this disclosure describes techniques for pre fetch and pre back signaling from a graphics processing unit GPU to avoid page faults in a virtual memory system.

In one example of the disclosure a method for demand paging in an input output device includes tracking by the input output device a usage of a first portion of mapped pages in a virtual memory system by an application executing on the input output device wherein the first portion of mapped pages represent a portion of a number of pages that may be needed by the application and wherein the first portion of mapped pages are backed into physical memory. The input output device may be further configured to determine if the usage crosses a threshold and in the case that the threshold is determined to be crossed signal a processor to back a second portion of pages in physical memory wherein the second portion of pages represents a different portion of the number of pages that may be needed by the application.

In one example of the above technique the threshold is a watermark representing a percentage of usage of the first portion of mapped pages and at least the first portion of mapped pages are stored in a buffer. In this case the watermark is a location within the buffer. The input output device determines if the usage crosses the threshold by determining if a current location accessed in the buffer by the input output device is past the watermark.

In another example of the disclosure a method for demand paging in an input output device includes tracking by the input output device a usage of a first portion of mapped pages in a virtual memory system by an application executing on the input output device wherein the first portion of mapped pages represent a portion of a number of pages that may be needed by the application and wherein page table entries for the first portion of mapped pages are stored in a memory management unit. The input output device may be further configured to determine if the usage crosses a threshold and in the case that the threshold is determined to be crossed signal a processor to fetch page table entries for a second portion of mapped pages wherein the second portion of pages represents a different portion of the number of pages that may be needed by the application.

In one example of the above technique the threshold is a watermark representing a percentage of usage of the first portion of mapped pages. More specifically the watermark may be location within a last page of the first portion of mapped pages. In this case the input output device determines if the usage crosses the threshold by determining if a current location accessed in the last page by the input output device is past the watermark.

The techniques of this disclosure are also described in terms of an apparatus and a computer readable storage medium storing instructions for causing a processor to perform the techniques. The details of one or more examples are set forth in the accompanying drawings and the description below. Other features objects and advantages will be apparent from the description and drawings and from the claims.

This disclosure relates to techniques for graphics processing and more specifically to techniques for pre fetch and pre back signaling from a graphics processing unit to avoid page faults in a virtual memory system.

Modern operating systems OS that run on central processing units CPU typically use a virtual memory scheme for allocating memory to multiple programs operating on the CPU. Virtual memory is a memory management technique that virtualizes a computer system s physical memory e.g. RAM disk storage etc. so that an application need only refer to one set of memory i.e. the virtual memory . Virtual memory consists of contiguous address spaces that are mapped to locations in physical memory. In this way the fragmentation of physical memory is hidden from the applications which instead may interact with contiguous blocks of virtual memory. The contiguous bocks in virtual memory are typically arranged into pages. Each page is some fixed length of contiguous blocks of virtual memory addresses. Mapping from the virtual memory to the physical memory is often handled by a memory management unit MMU . Virtual memory space that is currently mapped to locations in physical memory is considered to be backed to physical memory.

The mapping of locations in virtual memory space to physical memory is stored with a translation lookaside buffer TLB . The TLB is used by the MMU to quickly translate virtual addresses to physical addresses. The TLB may be implemented as a content addressable memory CAM that uses a virtual memory address as an input and outputs a physical memory address. The MMU may then quickly retrieve the requested data using the output physical memory address

Some applications may use a large amount of memory during execution. However the amount of memory that may be used by the application may not be needed at the same time. Instead of mapping all the pages into virtual memory that might be needed by a program pages may only be mapped for the memory that is actually currently being requested by the program. Additional pages of virtual memory are mapped if and when the program requests data that has not previously been mapped. This is called demand paging or demand memory backing. If a program requests data that has not been mapped a page fault is triggered. In response to a page fault an MMU may then map the physical memory locations being requested. Responding to page faults generally slows down the response time of a virtual memory system.

Reductions in the response time of a virtual memory system may also be caused by a TLB miss. A TLB miss occurs when data is requested from a backed page but the actual mapping for the virtual memory addresses in the page are not currently stored in the TLB. In many circumstances the TLB may not store all page mappings even if the page had been previously backed. When a TLB miss occurs the MMU accesses the desired mappings and stores them in the TLB.

Modern graphics processing units GPUs have page faulting capabilities similar to CPUs allowing memory allocations to not be present at GPU access time. However the latency to handle a page fault in a GPU relative to the computational power of GPU makes demand fill page faulting undesirable. The latency may be noticeable to a user thus creating an unsatisfactory user experience. This can be particularly problematic for GPU systems that cannot effectively reschedule work to cover the latency involved in paging from memory.

Historically input output IO devices such as a graphical processing unit GPU have required any memory buffers accessed by such devices to be backed in entirety into physical memory and mapped into an IO memory management unit IOMMU virtual memory space prior to launching a job against those buffers. In this context a buffer is a region of a physical memory storage used to temporarily hold data. When used with a virtual memory system buffers are implemented virtually in software as pointers to locations in physical memory. In this way the virtual software buffer is backed in physical memory.

As buffer sizes grow it has become desirable to move to a demand paging model as occurs in most modern central processing units CPU OS s. In this scenario a page fault is triggered within the IOMMU when the IO device attempts to access memory that is not currently backed and mapped into physical memory representing a portion of the buffer. When the page fault occurs the IO device halts processing of the faulting job and either switches to another job or halts until the fault handling is complete. When the fault occurs the host CPU is typically signaled via an interrupt from the IO subsystem e.g. the IOMMU indicating a fault. The OS determines in this case that the fault is a demand page fault and moves some amount of the buffer in question from backing store into physical memory and maps it into the IOMMU. Then the OS signals to the IO subsystem that the backing has occurred allowing the faulting IO job to continue.

Demand paging is a valuable technique to fill memory at utilization time. However the performance cost of a demand page fault can be extremely high particularly if the IO device e.g. a GPU is incapable of scheduling other work during the fault handling. The fault handling is a long path including IO subsystem to CPU interrupt handling and then to disk access for backing store retrieval. It is thus highly desirable to avoid page faults where possible.

In view of these drawbacks this disclosure proposes techniques for demand paging for an IO device e.g. a GPU that utilize pre fetch and pre back notification event signaling to reduce latency associated with demand paging. According to one example of this disclosure page faults are limited by performing the demand paging operations prior to the IO device actually requesting unbacked memory. If the IO device is able to look ahead during current processing while still working in mapped memory the IO device can anticipate a future page fault and can send a pre back signal to the host CPU OS driver to request a backing of memory that will be accessed in the future. If the signaling occurs early enough to hide the latency of the page backing the memory will be backed prior to the IO device accessing that memory and thus avoiding the page fault.

In one example of the disclosure a method for demand paging in an input output device includes tracking by the input output device a usage of a first portion of mapped pages in a virtual memory system by an application executing on the input output device wherein the first portion of mapped pages represent a portion of a number of pages that may be needed by the application and wherein the first portion of mapped pages are backed into physical memory. The input output device may be further configured to determine if the usage crosses a threshold and in the case that the threshold is determined to be crossed signal a processor to back a second portion of pages in physical memory wherein the second portion of pages represents a different portion of the number of pages that may be needed by the application.

In another example of the disclosure a method for demand paging in an input output device includes tracking by the input output device a usage of a first portion of mapped pages in a virtual memory system by an application executing on the input output device wherein the first portion of mapped pages represent a portion of a number of pages that may be needed by the application and wherein page table entries for the first portion of mapped pages are stored in a memory management unit. The input output device may be further configured to determine if the usage crosses a threshold and in the case that the threshold is determined to be crossed signal a processor to fetch page table entries for a second portion of mapped pages wherein the second portion of pages represents a different portion of the number of pages that may be needed by the application.

As illustrated in the example of computing device may include a user input interface a central processing unit CPU one or more memory controllers a system memory a graphics processing unit GPU a graphics memory a display interface a display and buses and . Note that in some examples graphics memory may be on chip with GPU . In some cases all hardware elements show in may be on chip for example in a system on a chip SoC design. User input interface CPU memory controllers GPU and display interface may communicate with each other using bus . Memory controllers and system memory may also communicate with each other using bus . Buses may be any of a variety of bus structures such as a third generation bus e.g. a HyperTransport bus or an InfiniBand bus a second generation bus e.g. an Advanced Graphics Port bus a Peripheral Component Interconnect PCI Express bus or an Advanced eXentisible Interface AXI bus or another type of bus or device interconnect. It should be noted that the specific configuration of buses and communication interfaces between the different components shown in is merely exemplary and other configurations of computing devices and or other graphics processing systems with the same or different components may be used to implement the techniques of this disclosure.

CPU may comprise a general purpose or a special purpose processor that controls operation of computing device . A user may provide input to computing device to cause CPU to execute one or more software applications. The software applications that execute on CPU may include for example an operating system a word processor application an email application a spread sheet application a media player application a video game application a graphical user interface application or another program. Additionally CPU may execute a GPU driver for controlling the operation of GPU . The user may provide input to computing device via one or more input devices not shown such as a keyboard a mouse a microphone a touch pad a touch screen or another input device that is coupled to computing device via user input interface .

The software applications that execute on CPU may include one or more graphics rendering instructions that instruct CPU to cause the rendering of graphics data to display . In some examples the software instructions may conform to a graphics application programming interface API such as e.g. an Open Graphics Library OpenGL API an Open Graphics Library Embedded Systems OpenGL ES API an Open Computing Language OpenCL API a Direct3D API an X3D API a RenderMan API a WebGL API or any other public or proprietary standard graphics API. In order to process the graphics rendering instructions CPU may issue one or more graphics rendering commands to GPU e.g. through GPU driver to cause GPU to perform some or all of the rendering of the graphics data. In some examples the graphics data to be rendered may include a list of graphics primitives e.g. points lines triangles quadrilaterals triangle strips etc.

Memory controllers facilitate the transfer of data going into and out of system memory . For example memory controllers may receive memory read and write commands and service such commands with respect to system memory in order to provide memory services for the components in computing device . Memory controllers are communicatively coupled to system memory via memory bus . Although memory controllers are illustrated in as being a processing module that is separate from both CPU and system memory in other examples some or all of the functionality of memory controller may be implemented on one or both of CPU and system memory .

Memory controllers may also include one or more memory management units MMUSs including an IOMMU for controlling IO device access e.g. a GPU to system memory . The memory management units may implement a virtual memory system. The virtual memory space may be divided into a plurality of virtual pages. These virtual pages may be contiguous but the physical pages in system memory to which these virtual pages correspond may not be contiguous in system memory . Pages may be considered as the minimum units that an MMU may be able to manage.

Physical page may be stored across multiple memory units of system memory . For example physical page may encompass both memory unit A and memory unit N. For example memory unit A may store a portion of physical page indicated as portion A and memory unit N may store a portion of physical page indicated as portion B. As illustrated memory unit A stores section 0 and section 2 of physical page and memory unit N stores section 1 and section 3 of physical page .

Memory unit A may store section 0 and section 2 and memory unit N may store section 1 and section 3 because of IOMMU storing data in an interleaving manner. When data is stored in an interleaving manner one portion of data is stored in a first memory unit and then a second portion of data is stored in a second memory unit before further data is stored in the first memory unit. This example only includes two memory units but any number of memory units may be used. For instance referring back to GPU driver may transmit instructions that cause GPU to store pixel values or any other computed value and may transmit the virtual addresses for where the pixel value are to be stored. GPU in turn may request IOMMU to store the pixel values in accordance with the virtual addresses. IOMMU in turn may map the virtual addresses to physical addresses and store the pixel values in pages of system memory in an interleaving manner based on the physical addresses.

IOMMU may be configured to store the pixel values in an interleaving manner. As one example IOMMU may be pre programmed to store the pixel values in the interleaving manner. As another example IOMMU may receive instructions that instruct IOMMU to store the pixel values in the interleaving manner.

System memory may store program modules and or instructions that are accessible for execution by CPU and or data for use by the programs executing on CPU . For example system memory may store a window manager application that is used by CPU to present a graphical user interface GUI on display . In addition system memory may store user applications and application surface data associated with the applications. System memory may additionally store information for use by and or generated by other components of computing device . For example system memory may act as a device memory for GPU and may store data to be operated on by GPU as well as data resulting from operations performed by GPU . For example system memory may store other graphics data such as any combination of texture buffers depth buffers stencil buffers vertex buffers frame buffers or the like. System memory may include one or more volatile or non volatile memories or storage devices such as for example random access memory RAM static RAM SRAM dynamic RAM DRAM read only memory ROM erasable programmable ROM EPROM electrically erasable programmable ROM EEPROM Flash memory a magnetic data media or an optical storage media.

GPU may be configured to perform graphics operations to render one or more graphics primitives to display . Thus when one of the software applications executing on CPU requires graphics processing CPU may provide graphics commands and graphics data to GPU for rendering to display . The graphics data may include e.g. drawing commands state information primitive information texture information etc. GPU may in some instances be built with a highly parallel structure that provides more efficient processing of complex graphic related operations than CPU . For example GPU may include a plurality of processing elements that are configured to operate on multiple vertices or pixels in a parallel manner. The highly parallel nature of GPU may in some instances allow GPU to draw graphics images e.g. GUIs and two dimensional 2D and or three dimensional 3D graphics scenes onto display more quickly than drawing the scenes directly to display using CPU .

GPU may in some instances be integrated into a motherboard of computing device . In other instances GPU may be present on a graphics card that is installed in a port in the motherboard of computing device or may be otherwise incorporated within a peripheral device configured to interoperate with computing device . GPU may include one or more processors such as one or more microprocessors application specific integrated circuits ASICs field programmable gate arrays FPGAs digital signal processors DSPs or other equivalent integrated or discrete logic circuitry.

GPU may be directly coupled to graphics memory . Thus GPU may read data from and write data to graphics memory without using bus . In other words GPU may process data locally using a local storage instead of using other slower system memory. This allows GPU to operate in a more efficient manner by eliminating the need of GPU to read and write data via system bus which may experience heavy bus traffic. In some instances however GPU may not include a separate memory but instead utilize system memory via bus . Graphics memory may include one or more volatile or non volatile memories or storage devices such as e.g. random access memory RAM static RAM SRAM dynamic RAM DRAM erasable programmable ROM EPROM electrically erasable programmable ROM EEPROM Flash memory a magnetic data media or an optical storage media.

CPU and or GPU may store rendered image data in a frame buffer . Typically frame buffer would be allocated within system memory but may in some circumstances be an independent memory. Display interface may retrieve the data from frame buffer and configure display to display the image represented by the rendered image data. In some examples display interface may include a digital to analog converter DAC that is configured to convert the digital values retrieved from the frame buffer into an analog signal consumable by display . In other examples display interface may pass the digital values directly to display for processing. Display may include a monitor a television a projection device a liquid crystal display LCD a plasma display panel a light emitting diode LED array such as an organic LED OLED display a cathode ray tube CRT display electronic paper a surface conduction electron emitted display SED a laser television display a nanocrystal display or another type of display unit. Display may be integrated within computing device . For instance display may be a screen of a mobile telephone. Alternatively display may be a stand alone device coupled to computer device via a wired or wireless communications link. For instance display may be a computer monitor or flat panel display connected to a personal computer via a cable or wireless link.

As shown in graphics 3D processing pipeline may include a command engine a geometry processing stage a rasterization stage and a pixel processing pipeline . Each of the components in graphics 3D processing pipeline may be implemented as fixed function components programmable components e.g. as part of a shader program executing on a programmable shader unit or as a combination of fixed function and programmable components. Memory available to CPU and GPU may include system memory that may include frame buffer . Frame buffer may store rendered image data.

Software application may be any application that utilizes the functionality of GPU . For example software application may be a GUI application an operating system a portable mapping application a computer aided design program for engineering or artistic applications a video game application or another type of software application that uses 2D or 3D graphics. Software application may also be an application that uses the GPU to perform more general calculations such as in a GPGPU application.

Software application may include one or more drawing instructions that instruct GPU to render a graphical user interface GUI and or a graphics scene. For example the drawing instructions may include instructions that define a set of one or more graphics primitives to be rendered by GPU . In some examples the drawing instructions may collectively define all or part of a plurality of windowing surfaces used in a GUI. In additional examples the drawing instructions may collectively define all or part of a graphics scene that includes one or more graphics objects within a model space or world space defined by the application.

Software application may invoke GPU driver via graphics API to issue one or more commands to GPU for rendering one or more graphics primitives into displayable graphics images. For example software application may invoke GPU driver via graphics API to provide primitive definitions to GPU . In some instances the primitive definitions may be provided to GPU in the form of a list of drawing primitives e.g. triangles rectangles triangle fans triangle strips etc. The primitive definitions may include vertex specifications that specify one or more vertices associated with the primitives to be rendered. The vertex specifications may include positional coordinates for each vertex and in some instances other attributes associated with the vertex such as e.g. color coordinates normal vectors and texture coordinates. The primitive definitions may also include primitive type information e.g. triangle rectangle triangle fan triangle strip etc. scaling information rotation information and the like. Based on the instructions issued by software application to GPU driver GPU driver may formulate one or more commands that specify one or more operations for GPU to perform in order to render the primitive. When GPU receives a command from CPU graphics 3D processing pipeline decodes the command and configures one or more processing elements within graphics 3D processing pipeline to perform the operation specified in the command. After performing the specified operations graphics 3D processing pipeline outputs the rendered data to frame buffer associated with a display device. Graphics 3D processing pipeline may be configured to execute in one of a plurality of different rendering modes including a binning rendering mode and a direct rendering mode.

GPU driver may be further configured to compile one or more shader programs and to download the compiled shader programs onto one or more programmable shader units contained within GPU . The shader programs may be written in a high level shading language such as e.g. an OpenGL Shading Language GLSL a High Level Shading Language HLSL a C for Graphics Cg shading language etc. The compiled shader programs may include one or more instructions that control the operation of a programmable shader unit within GPU . For example the shader programs may include vertex shader programs and or pixel shader programs. A vertex shader program may control the execution of a programmable vertex shader unit or a unified shader unit and include instructions that specify one or more per vertex operations. A pixel shader program may include pixel shader programs that control the execution of a programmable pixel shader unit or a unified shader unit and include instructions that specify one or more per pixel operations. In accordance with some examples of this disclosure a pixel shader program may also include instructions that selectively cause texture values to be retrieved for source pixels based on corresponding destination alpha values for the source pixels.

Graphics 3D processing pipeline may be configured to receive one or more graphics processing commands from CPU via GPU driver and to execute the graphics processing commands to generate displayable graphics images. As discussed above graphics 3D processing pipeline includes a plurality of stages that operate together to execute graphics processing commands. It should be noted however that such stages need not necessarily be implemented in separate hardware blocks. For example portions of geometry processing stage and pixel processing pipeline may be implemented as part of a unified shader unit. Again graphics 3D processing pipeline may be configured to execute in one of a plurality of different rendering modes including a binning rendering mode and a direct rendering mode.

Command engine may receive graphics processing commands and configure the remaining processing stages within graphics 3D processing pipeline to perform various operations for carrying out the graphics processing commands. The graphics processing commands may include for example drawing commands and graphics state commands. The drawing commands may include vertex specification commands that specify positional coordinates for one or more vertices and in some instances other attribute values associated with each of the vertices such as e.g. color coordinates normal vectors texture coordinates and fog coordinates. The graphics state commands may include primitive type commands transformation commands lighting commands etc. The primitive type commands may specify the type of primitive to be rendered and or how the vertices are combined to form a primitive. The transformation commands may specify the types of transformations to perform on the vertices. The lighting commands may specify the type direction and or placement of different lights within a graphics scene. Command engine may cause geometry processing stage to perform geometry processing with respect to vertices and or primitives associated with one or more received commands.

Geometry processing stage may perform per vertex operations and or primitive setup operations on one or more vertices in order to generate primitive data for rasterization stage . Each vertex may be associated with a set of attributes such as e.g. positional coordinates color values a normal vector and texture coordinates. Geometry processing stage modifies one or more of these attributes according to various per vertex operations. For example geometry processing stage may perform one or more transformations on vertex positional coordinates to produce modified vertex positional coordinates. Geometry processing stage may for example apply one or more of a modeling transformation a viewing transformation a projection transformation a ModelView transformation a ModelViewProjection transformation a viewport transformation and a depth range scaling transformation to the vertex positional coordinates to generate the modified vertex positional coordinates. In some instances the vertex positional coordinates may be model space coordinates and the modified vertex positional coordinates may be screen space coordinates. The screen space coordinates may be obtained after the application of the modeling viewing projection and viewport transformations. In some instances geometry processing stage may also perform per vertex lighting operations on the vertices to generate modified color coordinates for the vertices. Geometry processing stage may also perform other operations including e.g. normal transformations normal normalization operations view volume clipping homogenous division and or backface culling operations.

Geometry processing stage may produce primitive data that includes a set of one or more modified vertices that define a primitive to be rasterized as well as data that specifies how the vertices combine to form a primitive. Each of the modified vertices may include for example modified vertex positional coordinates and processed vertex attribute values associated with the vertex. The primitive data may collectively correspond to a primitive to be rasterized by further stages of graphics 3D processing pipeline . Conceptually each vertex may correspond to a corner of a primitive where two edges of the primitive meet. Geometry processing stage may provide the primitive data to rasterization stage for further processing.

In some examples all or part of geometry processing stage may be implemented by one or more shader programs executing on one or more shader units. For example geometry processing stage may be implemented in such examples by a vertex shader a geometry shader or any combination thereof. In other examples geometry processing stage may be implemented as a fixed function hardware processing pipeline or as a combination of fixed function hardware and one or more shader programs executing on one or more shader units.

Rasterization stage is configured to receive from geometry processing stage primitive data that represents a primitive to be rasterized and to rasterize the primitive to generate a plurality of source pixels that correspond to the rasterized primitive. In some examples rasterization stage may determine which screen pixel locations are covered by the primitive to be rasterized and generate a source pixel for each screen pixel location determined to be covered by the primitive. Rasterization stage may determine which screen pixel locations are covered by a primitive by using techniques known to those of skill in the art such as e.g. an edge walking technique evaluating edge equations etc. Rasterization stage may provide the resulting source pixels to pixel processing pipeline for further processing.

The source pixels generated by rasterization stage may correspond to a screen pixel location e.g. a destination pixel and be associated with one or more color attributes. All of the source pixels generated for a specific rasterized primitive may be said to be associated with the rasterized primitive. The pixels that are determined by rasterization stage to be covered by a primitive may conceptually include pixels that represent the vertices of the primitive pixels that represent the edges of the primitive and pixels that represent the interior of the primitive.

Pixel processing pipeline is configured to receive a source pixel associated with a rasterized primitive and to perform one or more per pixel operations on the source pixel. Per pixel operations that may be performed by pixel processing pipeline include e.g. alpha test texture mapping color computation pixel shading per pixel lighting fog processing blending a pixel ownership text a source alpha test a stencil test a depth test a scissors test and or stippling operations. In addition pixel processing pipeline may execute one or more pixel shader programs to perform one or more per pixel operations. The resulting data produced by pixel processing pipeline may be referred to herein as destination pixel data and stored in frame buffer . The destination pixel data may be associated with a destination pixel in frame buffer that has the same display location as the source pixel that was processed. The destination pixel data may include data such as e.g. color values destination alpha values depth values etc.

Frame buffer stores destination pixels for GPU . Each destination pixel may be associated with a unique screen pixel location. In some examples frame buffer may store color components and a destination alpha value for each destination pixel. For example frame buffer may store Red Green Blue Alpha RGBA components for each pixel where the RGB components correspond to color values and the A component corresponds to a destination alpha value. Pixel values may also be represented by a luma component Y and one or more chroma components e.g. U and V . Although frame buffer and system memory are illustrated as being separate memory units in other examples frame buffer may be part of system memory .

General purpose shader may be any application executable on GPU to perform calculations. Typically such calculations are of the type that takes advantage of the highly parallel structure of GPU processing cores including arithmetic logic units ALUs . An example general purpose shader may conform to the OpenCL API. OpenCL is an API that allows an application to have access across multiple processors in a heterogeneous system e.g. a system including a CPU GPU DSP etc. . Typically in an OpenCL conforming application GPU would be used to perform non graphical computing. Examples of non graphical computing applications may include physics based simulations fast Fourier transforms audio signal processing digital image processing video processing image post filtering computational camera climate research weather forecasting neural networks cryptography and massively parallel data crunching among many others.

This disclosure proposes techniques for demand paging for an IO device e.g. a GPU . In particular the techniques of this disclosure add pre fetch and pre back notification event signaling to reduce latency associated with demand paging. The signaling is used to avoid the bulk of the page faults by informing the OS CPU of upcoming accesses to unmapped memory.

The techniques of the disclosure include the use of pre back and or pre fetch signaling to anticipate and possibly prevent both page faults and TLB misses in a demand paging virtual memory system for an IO device. A pre back signal may be used to inform an OS or a CPU that unmapped pages i.e. pages of virtual memory that are not currently backed in physical memory are about to be accessed by an IO device. In response to such a signal the CPU and or OS may back the anticipated pages into physical memory in attempt to have such pages backed before they are accessed by the IO device. In some use cases such a pre back signal may avoid the majority of page faults and their resultant latency drawbacks.

With reference to GPU may send a host CPU pre back signal to CPU to request that additional pages be backed into physical memory. GPU may send pre back signal in response to a determination that usage of currently backed pages has exceeded some predetermined threshold of buffer usage. In essence by tracking usage of currently backed pages GPU is anticipating that currently unbacked pages will be needed in the near future. Additional details of usage tracking and thresholds will be discussed with reference to below.

In response to pre back signal CPU may back additional pages in page table into physical memory CPU IOMMU page table backing . That is additional pages of virtual memory in page table are mapped to physical memory locations e.g. system memory of . Page table stores the mapping between virtual memory addresses and physical memory. When backing is completed CPU may signal a backing complete signal to GPU to inform a GPU. Backing complete signal may inform GPU that suspension of any operations is not necessary since a page fault was avoided due to the pre backing operation. In other cases when pre backing was unable to be performed in time to prevent a page fault backing complete signal may be used to inform GPU that any suspended operations may be restarted.

On a related topic even buffers that contain pages that are backed in physical memory can suffer performance impacts in a virtualized memory system provided by an IOMMU. Specifically the IOMMU typically contain small caches TLBs to hold portions of the translation page table. This is to avoid fetching translation page table entries PTE from memory e.g. DRAM for every translation. Heavy missing on the TLB cache i.e. a TLB miss can lead to significant performance loss because the data operations of the IO device get stalled behind PTE fetches to memory. As such this disclosure proposes a pre fetch signal that is used to inform an IOMMU that page table entries PTE that are not currently stored in the TLB will be accessed soon. In response to the pre fetch signal an IOMMU may access PTEs for the anticipated pages in attempt to have PTEs for such pages stored in the TLB before they are accessed by the IO device. In some use cases such a pre fetch signal may avoid the majority of TLB misses and their resultant latency drawbacks.

With reference to GPU may send a TLB PTE pre fetch signal to IOMMU to request that PTEs be loaded into TLB . GPU may send pre fetch signal in response to a determination that usage of a mapped page with a TLB PTE has exceeded some predetermined threshold. In essence by tracking usage of page with TLB PTEs GPU is anticipating that PTEs for other pages will be needed in the near future in TLB of IOMMU . In response to pre fetch signal IOMMU may fetch and store PTEs relating to additional mapped pages from memory e.g. system memory or from DRAM . Additional details of usage tracking and thresholds for page PTEs will be discussed with reference to below.

There are a multitude of possible methods and techniques in an IO device e.g. GPU to generate the pre back or pre fetch signaling. is a conceptual diagram showing one example of pre back signal triggering according to one example of the disclosure. In the example of GPU tracks the usage of memory accesses to buffer . The usage may be both reads and writes to buffer . Buffer may store one or more mapped pages of virtual memory. In many cases only a portion of the total number of pages in a buffer may be backed in physical memory. For example a first portion of mapped pages in buffer e.g. first mapped pages may be backed into physical memory while a second portion of mapped pages e.g. second mapped pages may not yet be backed in physical memory. Buffer start indicates the first memory address in the contiguous address entries of the mapped pages stored in buffer .

GPU is configured to track the current location of buffer access current buffer location against some threshold buffer watermark . Buffer watermark indicates a specific virtual memory address for a page stored in buffer . In one example the host CPU pre back signal may be triggered when current buffer location is past buffer watermark . That is pre back signal is triggered once GPU accesses virtual memory locations with a higher address than buffer watermark . In response to pre back signal CPU would then back second mapped pages into physical memory.

The above technique for pre back signaling may be particularly applicable for applications where buffer is accessed in a highly linear fashion. Examples of buffers that are accessed in a highly linear fashion include command stream buffers vertex buffers instruction stream buffers texture buffers meta data flow buffers compute buffers and intermediate stage flow buffers. Command buffers contain the command stream between the driver producer and GPU consumer . The command stream may be a stream of jobs or sub jobs register writes for example . Vertex buffers contain the geometry information the GPU uses to draw with such as position color texture coordinates and other attribute data. Instruction stream buffers contain the instruction or program which the GPU shader compute units run such as vertex pixel or compute shaders. Texture Buffers contain texture image data. Intermediate stage flow buffers handle data flow for a job. Often times the GPU will have limited internal memory to handle the data flow for a job in which case the GPU will stream or dump the data flow to an intermediate stage buffer or other dedicated graphics or system memory and a subsequent GPU stage will consume back from that memory. Also would be worth mentioning two other buffer types. Meta data flow buffers contain inter state data created by the GPU explicitly. An example of such explicitly created inter state data would be a deferred renderer that consumes vertex data and outputs a visibility stream for a subsequent stage to use. Compute buffers are used to store general purpose data computed by a GPU. Modern GPUs are designed to support generic computational tasks that are not graphics specific. In this case buffers such as a compute buffer can represent arbitrary data structures array of lists or list of arrays for example . It should be noted that the buffer watermark triggering technique may be applied to multiple bound buffer streams bound to an IO device.

GPU is configured to track the usage of memory addresses in pages against some threshold e.g. page watermark . Page watermark indicates a specific virtual memory address for a page stored in pages . In one example GPU may be configured to track the current location of virtual memory for a command stream stream location . The host TLB PTE pre fetch signal may be triggered when stream location is past page watermark . That is pre fetch signal is triggered once GPU accesses virtual memory locations with a higher address than page watermark . In response to pre fetch signal IOMMU would then fetch PTEs related to subsequent pages in page . The instruction IF STREAM PAGESIZE 1 WATERMARK causes a pre fetch signal every time the stream approached a 4 KB boundary i.e. PAGESIZE of the next page. This instruction would do accomplish that. For example every time the stream got close defined by WATERMARK to the next page boundary it would send a pre fetch signal. In one example pre fetch signaling may be triggered in response to a GPU accessing virtual memory location close to a page boundary. In examples where multiple pages are contained in pages page watermark may be positioned before the page boundary for a last page having PTEs in TLB of IOMMU .

The above technique for pre fetch signaling may also be particularly applicable for applications where pages are accessed in a highly linear or sequential fashion. Like pre back signaling examples of pages that are accessed in a highly linear fashion may include pages in command stream buffers vertex buffers instruction stream buffers texture buffers and intermediate stage flow buffers.

CPU may be configured to provide pre fetch and pre back triggers to GPU either implicitly or explicitly before GPU launches memory transactions to memory buffers including both read and write transactions. The pre fetch and pre back triggers may be signaled to the GPU either through an application executing at CPU or through a device driver e.g. GPU driver of .

When a new buffer is bound for GPU e.g. a vertex texture or command buffer it is likely that such a buffer will be accessed by the subsequent job. In this case the binding of the buffer can be paired with a signaling trigger for GPU to utilize the pre fetch and pre back signals. There are other scenarios where both the GPU driver and or an application executing on CPU may determine a general access pattern for a subsequent job e.g. a highly linear buffer access pattern or a spatially deterministic access pattern . In these cases a pre fetch pre back execution command can be put into the command stream for execution on GPU prior to a memory access job.

An device such as a GPU may contain stream processors that run highly parallelized jobs. Instruction programs that execute on the stream processors can be extended to include pre fetch and pre back trigger instructions. For example as shown in graphics 3D processing pipeline including a shader subsystem and or other pipeline blocks executing on GPU may be configured to track the thresholds provided by CPU . This would allow any known access patterns that could be determined at program development or compile time to be expressed as pre back and pre fetch triggers.

The techniques of this disclosure may also be extended to the unmapping of virtual memory pages no longer in use. As GPU completes the use of mapped pages the same type of signaling can be used to instruct CPU to free e.g. unmap the virtual memory pages that are no longer needed.

The buffer and page watermarking techniques described above may also be used for other situations where buffer and or page access is not highly linear. In particular many GPU applications are spatially deterministic. That is when an application is launched on the GPU it is known in what spatial order pixels will be drawn on the screen. Examples of GPU applications that are often spatially deterministic include rasterization. While rasterization in a GPU is not always spatially deterministic there are many cases that are. For example large block transfers BLTs and tiled renderer resolve to DRAM occurs in a spatial pattern known a prioi to launching such a job. A BLT is an instruction that copies data from one memory to another. BLTs of pixel data are often executed when a GPU is instructed to draw a particular area of a scene.

In these cases an extended method of the simple linear watermarks described with reference to and could be used to trigger a pre back signal and or pre fetch signal. For example the raster pattern in x y space may occur in a known pattern across a given screen space region. As the raster walks GPU may be configured to trigger a pre back and or pre fetch signals for regions in the raster scan that will be accessed in the future. In particular a watermark may be placed at a location s in the raster scan pattern near locations where subsequent entries in the raster scan pattern are not currently packed in physical memory. Likewise a watermark may be placed at a location s in the raster scan pattern near locations where subsequent entries in the raster scan pattern do not have PTEs currently stored in a TLB.

Other techniques may also be used to track and or estimate the usage of memory and to generate the pre back and pre fetch signaling described in this disclosure. One such example includes utilizing a second rasterizer that runs ahead of the normal rasterizer e.g. rasterization stage of . The second rasterizer could be a coarse grained rasterizer. That is the second rasterizer may perform rasterization at a lower level of resolution and less precision as the goal of this rasterizer is not to produce pixels for display but to determine what future memory usage warrants a pre fetch or pre back signal. As one example the second rasterizer may operate a number of pixels e.g. 100 pixels in front of the normal rasterizer. However any number of run ahead pixels may be used that allows for useful pre back and pre fetch signaling.

In general any run ahead techniques using sparse element execution could be used to track memory usage and trigger pre back and pre fetch signaling. Sparse element execution generally means that only a portion of jobs e.g. pixels or work items of a total number of jobs are executed at a time. As one example for a drawcall made up of 1000 s of vertices the run ahead engine could fully execute a vertex ahead of the current vertex. Again any number of run ahead vertices may be used that allows for pre fetch and or pre back signaling to be useful for the particular application. The run ahead engine may be a parallel processing pipeline that is identical or nearly identical to the pipeline producing compute jobs or pixels for display. In another example the same engine used for producing compute jobs or pixels for display may be paused to execute a run ahead job to determine pre back or pre fetch signaling. After the run ahead job is completed the main job may be resumed. As with the run ahead rasterizer example above the goal of the run ahead engine is not to produce accurate compute job results or pixels for display but rather to determine what future memory usage warrants a pre fetch or pre back signal.

Note that for OpenCL applications the run ahead engine may operate on a work item that is 1000 work items in front of the current work item. For typical OpenCL applications a work item is effectively equivalent to a pixel. The GPU does not treat a work item as an x y position per se like a pixel but rather a work item belongs to an x y z grid of work items called a work group. In effect a work group is somewhat equivalent to a triangle in 3D graphics processing.

Another technique for tracking future memory usage to determine when to send a pre fetch or pre back signal may involve using the front end of a GPU pipeline to look ahead to future commands. Typically GPU pipelines will include some sort of command processor at the beginning of the pipeline to process a command stream that includes jobs for later stages of the pipeline. An example command processor may be command engine of graphics 3D processing pipeline of . However this technique is not limited to 3D graphics applications but to any type of application OpenCL application video encoding and decoding image processing etc. that may use a command processor. The command processor may be configured to evaluate a command stream to determine when to send a pre back or pre fetch signal.

GPU command streams for 3D graphics processing for example generally contain commands that set some register or issue some rendering action. Usually there exists an adequate number of registers that hold memory addresses or a range thereof from which data will be fetched or to which data will be written. Rendering commands often hold memory addresses of buffers that the GPU will access. When processing command streams the command processor e.g. the command engine of the GPU may be configured to scan the command stream for memory addresses create a list of mapped pages that will be accessed soon and use such a list to trigger a pre back pre fetch signal. More specifically the command engine would be configured to determine the need for future unmapped pages before later stages in the pipeline e.g. geometry processing stage rasterization stage pixel processing pipeline need to access such unmapped pages.

GPU may be further configured to determine if the usage crosses a threshold . The threshold may be implicitly determined by the GPU or may be optionally received from CPU . In some examples the threshold is a watermark representing a percentage of usage of the first portion of mapped pages. In this example at least the first portion of mapped pages is stored in a buffer and the watermark is a location within the buffer. In this case GPU may be further configured to determine if a current location accessed in the buffer is past the watermark. The buffer may be one of a command stream buffer a vertex buffer a texture buffer an instruction stream buffer a rasterization buffer and an intermediate stage flow buffer. In another example the buffer is the rasterization buffer and the application executing on the GPU is one of a block transfer BLT and a tiled renderer resolve.

In the case that the threshold is determined to be crossed GPU may be further configured to signal CPU to back a second portion of pages in physical memory i.e. translate virtual memory address to physical memory addresses . The second portion of pages represents a different portion of the number of pages that may be needed by the application. In response to the signal CPU may be configured to back the second portion of pages to physical memory . Optionally GPU may be further configured to receive a signal from CPU indicating that the backing is complete .

GPU may be further configured to determine if the usage crosses a threshold . In this context crossing a threshold may include exceeding or falling below a certain threshold value. The threshold may be implicitly determined by the GPU or may be optionally received from CPU . In some examples the threshold is a watermark representing a percentage of usage of the first portion of mapped pages. In one example the watermark is a location within a last page of the first portion of mapped pages. In this case GPU is further configured to determine if a current location accessed in the last page is past the watermark.

In the case that the threshold is determined to be crossed GPU may be configured to signal IOMMU to fetch page table entries for a second portion of mapped pages . The second portion of pages represents a different portion of the number of pages that may be needed by the application. In response to the signal IOMMU may be configured to fetch page table entries for the second portion of pages .

In one or more examples the functions described above may be implemented in hardware software firmware or any combination thereof. If implemented in software the functions may be stored as one or more instructions or code on an article of manufacture comprising a non transitory computer readable medium. Computer readable media may include computer data storage media. Data storage media may be any available media that can be accessed by one or more computers or one or more processors to retrieve instructions code and or data structures for implementation of the techniques described in this disclosure. By way of example and not limitation such computer readable media can comprise RAM ROM EEPROM CD ROM or other optical disk storage magnetic disk storage or other magnetic storage devices flash memory or any other medium that can be used to carry or store desired program code in the form of instructions or data structures and that can be accessed by a computer. Disk and disc as used herein includes compact disc CD laser disc optical disc digital versatile disc DVD floppy disk and Blu ray disc where disks usually reproduce data magnetically while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer readable media.

The code may be executed by one or more processors such as one or more DSPs general purpose microprocessors ASICs FPGAs or other equivalent integrated or discrete logic circuitry. In addition in some aspects the functionality described herein may be provided within dedicated hardware and or software modules. Also the techniques could be fully implemented in one or more circuits or logic elements.

The techniques of this disclosure may be implemented in a wide variety of devices or apparatuses including a wireless handset an integrated circuit IC or a set of ICs e.g. a chip set . Various components modules or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques but do not necessarily require realization by different hardware units. Rather as described above various units may be combined in a codec hardware unit or provided by a collection of interoperative hardware units including one or more processors as described above in conjunction with suitable software and or firmware.

Various examples have been described. These and other examples are within the scope of the following claims.

