---

title: Provisioning a cluster of distributed computing platform based on placement strategy
abstract: Embodiments perform automated provisioning of a cluster for a distributed computing platform. Target host computing devices are selected from a plurality of host computing devices based on configuration information, such as a desired cluster size, a data set, code for processing the data set and, optionally, a placement strategy. One or more virtual machines (VMs) are instantiated on each target host computing device. Each VM is configured to access a virtual disk that is preconfigured with code for executing functionality of the distributed computing platform and serves as a node of the cluster. The data set is stored in a distributed file system accessible by at least a subset of the VMs. The code for processing the data set is provided to at least a subset of the VMs, and execution of the code is initiated to obtain processing results.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09268590&OS=09268590&RS=09268590
owner: VMware, Inc.
number: 09268590
owner_city: Palo Alto
owner_country: US
publication_date: 20120229
---
Distributed computing platforms such as Hadoop include software that allocates computing tasks across a group or cluster of distributed software components executed by a plurality of computing devices enabling large data sets to be processed more quickly than is generally feasible with a single software instance or a single device. Such platforms typically utilize a distributed file system that can support input output I O intensive distributed software components running on a large quantity e.g. thousands of computing devices to access a large quantity e.g. petabytes of data. For example the Hadoop Distributed File System HDFS is typically used in conjunction with Hadoop a data set to be analyzed by Hadoop may be stored as a large file e.g. petabytes on HDFS which enables various computing devices running Hadoop software to simultaneously process different portions of the file.

Typically distributed computing platforms such as Hadoop are configured and provisioned in a native environment where each node of the cluster corresponds to a physical computing device. In such native environments administrators typically need to manually configure the settings for the distributed computing platform by generating or editing configuration or metadata files that for example specify the names and network addresses of the nodes in the cluster as well as whether any such nodes perform specific functions for the distributed computing platform e.g. such as the JobTracker or NameNode nodes in Hadoop . More recently service providers that offer cloud based Infrastructure as a Service IaaS offerings have begun to provide customers with Hadoop frameworks as a Platform as a Service PaaS . For example the Amazon Elastic MapReduce web service which runs on top of the Amazon Elastic Compute Cloud Amazon EC2 IaaS service provides customers with a user interface to i provide data for processing and code specifying how the data should be processed e.g. Mapper and Reducer code in Hadoop and ii specify a number of nodes in a Hadoop cluster used to process the data. Such information is then utilized by the Amazon Elastic MapReduce web service to start a Hadoop cluster running on Amazon EC2 to process the data.

Such PaaS based Hadoop frameworks however are limited for example in their configuration flexibility reliability and robustness scalability quality of service QoS and security. For example such frameworks may not address single point of failure SPoF issues in the underlying distributed computing platform such as the SPoF represented by the NameNode in Hadoop. As another example such frameworks are not known to provide user selectable templates such that a preconfigured application environment with a known operating system and support software e.g. a runtime environment can be quickly selected and provisioned.

Embodiments described herein perform automated provisioning of a cluster of nodes for a distributed computing platform. Target host computing devices are selected from a plurality of host computing devices based on configuration information such as a desired cluster size a data set and code for processing the data set. One or more virtual machines VMs are instantiated on each target host computing device. Each VM is configured to access a virtual disk that is preconfigured with code for executing functionality of the distributed computing platform and serves as a node of the cluster. The data set is stored in a distributed file system accessible by at least a subset of the VMs. The code for processing the data set is provided to at least a subset of the VMs and execution of the code is initiated to obtain processing results.

In some embodiments the configuration information may also include a placement strategy including operational efficiency operational robustness or a combination of operational efficiency and operational robustness. In such embodiments the target host computing devices are selected based on the placement strategy and a location e.g. a physical location and or a network location of each of the target host computing devices.

This summary introduces a selection of concepts that are described in more detail below. This summary is not intended to identify essential features nor to limit in any way the scope of the claimed subject matter.

Embodiments described herein provide a distributed computing platform e.g. Hadoop etc. service that for example runs within an IaaS environment managed by a service provider or within an enterprise s own internal data center environment. Certain of such embodiments provide a user interface for users to provide cluster size data sets data processing code also referred to herein as jobs and other preferences and configuration information to the distributed computing platform service in order to process or otherwise analyze the provided data sets within the environment. The distributed computing platform service is then able to provision and deploy a properly configured distributed computing cluster in which nodes of the cluster are implemented as virtual machines VMs running on a number of host computing devices e.g. hardware servers etc. in the environment e.g. in the IaaS or data center . In exemplary embodiments the distributed computing platform service includes a cluster management application that receives the foregoing user specified inputs e.g. cluster size data sets jobs etc. and interacts with a virtualization management application to select appropriate VM templates that include distributed software components that conform to the conventions of the distributed computing platform and then select appropriate host computing devices within the environment to launch VMs based on such templates. Accordingly such embodiments enable a reduction in manual provisioning and configuration effort and also reduce the risk of error and satisfy the operator s requirements such as scalability response latency and speed of provisioning. In addition as further discussed below certain embodiments may apply virtualization technologies such as linked cloning thin provisioning multi level resource isolation resource pooling and fault tolerance enabling efficient creation of the cluster and robust efficient and secure operation of the cluster once established.

Exemplary embodiments may operate using management application programming interfaces APIs to control and query cloud resources hypervisors and or virtual machines VMs . Virtualization technologies such as thin provisioning and linked cloning may be employed to reduce input output I O traffic associated with provisioning similarly reducing the time used to complete provisioning operations.

To facilitate robust and efficient execution in a cloud environment embodiments described provide network topology information enabling proper operation of the distributed computing platform s data replication functions despite the fact that some infrastructure information may be concealed from the operator of the cluster. Further distributed computing nodes e.g. executed by VMs may be placed on hosts according to a user selectable placement strategy to achieve a desired balance between robustness and efficiency.

Multi level resource isolation may be applied to prevent inter cluster access to network traffic and or storage resources even when the clusters involved include nodes executing in the same physical network and or on the same hosts addressing security issues in a multi tenant environment. In some embodiments resource isolation techniques such as resource pooling and or classification enable performance isolation between clusters potentially providing QoS guarantees. Further provisioning a cluster of virtualized nodes enables fault tolerance features of the virtualization platform to be employed enhancing the reliability of the distributed computing platform by addressing single points of failure.

Host also includes a network communication interface which enables host to communicate with a remote device e.g. a client device and or any other host via a communication medium such as a wired or wireless packet network. For example host may transmit and or receive data via network communication interface . Host may further include a storage interface that enables host to communicate with one or more network data storage systems that may for example store virtual disks that are accessed by node VMs. In one embodiment storage interface is a host bus adapter HBA that couples host to a storage area network SAN e.g. a Fibre Channel network and or a network interface card NIC that couples host to a network attached storage NAS system e.g. storage interface may be the same as network communication interface in certain embodiments etc. . Although host is described above with reference to its operation as a computing device that supports one or more VM nodes it should be recognized that similar computing devices may be configured e.g. programmed to operate as other systems described herein.

One or more VMs of host such as VM may serve as a VM node of a cluster generated and managed by a distributed computing platform service as described herein. In the embodiment depicted in VM node may include a guest operating system OS e.g. Microsoft Windows Linux Solaris NetWare FreeBSD etc. which may support one or more applications including runtime environments such as Java Virtual Machines JVMs that support the execution of distributed software component code e.g. Java code etc. for the distributed computing platform. For example if the distributed computing platform is Hadoop VM node may support a runtime environment with a JVM that serves as a master node that executes distributed software component code e.g. Java code etc. implementing the JobTracker function TaskTracker function and NameNode function of Hadoop that manages HDFS. Alternatively VM node may serve as a worker node that executes code implementing the TaskTracker and DataNode functions that support execution of additional user specified code e.g. Mapper and Reducer code etc. that processes specific data files stored in HDFS accordingly. In particular the NameNode function of a master VM node implements HDFS by communicating with DataNode components of worker VM nodes to store and access data files in a distributed fashion across the worker VMs. In one Hadoop embodiment a primary virtual disk accessed by VM node is represented by emulated local storage and implemented as a file stored in local storage of hardware platform . One example of a format for a virtual disk file is the .vmdk file format developed by VMware although it should be recognized that any virtual disk file format may be utilized consistent with the teachings herein. Such a primary virtual disk which may be referred to as a boot disk includes guest OS runtime environment and the distributed software component code of the distributed computing platform. In such an embodiment DataNode components of worker VM nodes may store and access HDFS files within the primary virtual disk i.e. emulated local storage itself e.g. where HDFS operates on top of the file system of guest OS and for example stores HDFS data files within a folder of the file system of guest OS .

Alternatively worker VM nodes may be configured to have access to a second emulated local storage device in virtual hardware platform that corresponds to a different partition or portion of local storage of hardware platform . In such an alternative Hadoop environment an administrator of the distributed computing platform service may configure the DataNode component of worker VM nodes to store and access HDFS data files using the second emulated local storage device rather than the primary virtual disk. Such an approach allows cluster management application to attach HDFS virtual disks to and detach HDFS virtual disks from VMs dynamically. Accordingly virtual disks may be reassigned to VMs by detaching from one VM attaching to another VM and updating metadata managed by the NameNode. Further in some embodiments the primary virtual disk boot disk includes guest OS and a secondary virtual disk includes code for executing the distributed computing platform. Both primary and secondary virtual disks may be based on VM templates in local storage and or VM templates in networked storage described in more detail below.

In yet another alternative Hadoop embodiment the primary virtual disk utilized used by VM node i.e. which stores guest OS runtime environment the distributed software component code of the distributed computing platform etc. may be represented by networked storage in virtual hardware platform and may be stored as a file in shared storage e.g. networked storage described with reference to such as a SAN networked to the host running the VM node. Such a virtual disk file is accessed by the VM node for example through another distributed file system used by hypervisor to store virtual disks used by VMs running on the host e.g. the VMFS file system from VMware Inc. etc. . In such a Hadoop environment an administrator of the distributed computing platform service may configure the DataNode functionality of worker VM nodes to store and access HDFS data files using emulated local storage which as previously discussed is mapped or otherwise corresponds to portions of local storage of hardware platform rather than networked storage . In certain embodiments the administrator of the distributed computing platform service may further configure any data processing output e.g. results produced by worker VM nodes to be stored in networked storage rather than local storage thereby reducing risk that reconfiguration or failure of the host will cause loss of data processing results. It should be recognized that the above Hadoop examples are merely a few examples of how a distributed computing platform may be structured across a cluster of VM nodes and that alternative structures within a VM node may be implemented consistent with the teachings herein. For example rather than executing distributed software component code in a runtime environment such as a JVM on top of a guest OS alternative embodiments may execute distributed software component code directly on top of guest OS .

In some embodiments the location of virtual disks accessed by VMs is determined based on the function of individual VMs . For example a VM executing DataNode functionality may be associated with a virtual disk in emulated local storage and a VM executing TaskTracker functionality may be associated with a virtual disk in networked storage . As another example VMs executing JobTracker and or NameNode functionality may be associated with a virtual disk in networked storage such that another VM may be attached to the virtual disk and executed as a replacement in the event of a failure of the VM originally executing the JobTracker and or NameNode functionality.

It should be recognized that the various terms layers and categorizations used to describe the virtualization components in may be referred to differently without departing from their functionality or the spirit or scope of the disclosure. For example virtual hardware platforms may also be considered to be separate from VMMs and VMMs may be considered to be separate from hypervisor . Furthermore in certain embodiments hypervisor may manage e.g. monitor initiate and or terminate execution of VMs according to policies associated with hypervisor such as a policy specifying that VMs are to be automatically restarted upon unexpected termination and or upon initialization of hypervisor . Similarly hypervisor may manage execution of VMs based on requests received from a virtualization management application running on a computing device other than host . For example hypervisor may receive an instruction from a virtualization management application via network communication interface to instantiate VM based on a virtual disk as further described below that is stored on a SAN accessible by storage interface e.g. HBA . One example of hypervisor that may be used in an embodiment herein is the VMware ESXi hypervisor provided as part of the VMware vSphere solution commercially available from VMware Inc. VMware . Similarly one example of a virtualization management application that may be used in an embodiment is the VMware vCenter Server commercially available from VMware.

VM nodes in hosts communicate with each other via a network . For example in a Hadoop embodiment the NameNode functionality of a master VM node may communicate with the DataNode functionality via network to store delete and or copy a data file using HDFS. As depicted in the embodiment cluster also includes a management device that is also networked with hosts via network . Management device executes a virtualization management application e.g. VMware vCenter Server etc. and a cluster management application . As previously discussed virtualization management application monitors and controls hypervisors executed by hosts for example to instruct such hypervisors to initiate and or to terminate execution of VMs such as VM nodes . As further detailed herein in embodiments cluster management application communicates with virtualization management application in order to configure and manage e.g. initiate terminate etc. VM nodes in hosts for use by the distributed computing platform service i.e. it should be recognized that in certain embodiments administrators of the IaaS or other data center environment in which hosts reside may utilize virtualization management application to manage VMs running on hosts that are used for purposes other than the distributed computing platform service described herein . It should be recognized that in alternative embodiments virtualization management application and cluster management application may be integrated into a single application or be implemented on different devices. Similarly it should be recognized that management device or the functions therein may be implemented as a one or more VMs running in a host in the IaaS or data center environment or may be a separate computing device.

As depicted in the embodiment of users of the distributed computing platform service may utilize a user interface on a remote client device to communicate with cluster management application in management device . For example client device may communicate with management device using a wide area network WAN the Internet and or any other network not shown . In one embodiment the user interface is a web page of a web application component of cluster management application that is rendered in a web browser running on a user s laptop. The user interface may enable a user to provide a cluster size data sets data processing code e.g. in a Hadoop environment Mapper and Reducer related code and other preferences and configuration information to cluster management application in order to launch cluster to perform a data processing job i.e. in accordance with the data processing code on the provided data sets. It should be recognized in alternative embodiments cluster management application may further provide an application programming interface API in addition supporting the user interface to enable users to programmatically launch or otherwise access clusters to process data sets. It should further be recognized that cluster management application may provide an interface for an administrator. For example in one embodiment an administrator may communicate with cluster management application through a client side application such as vSphere Client from VMware or through a command line interface CLI .

As previously described in the context of certain embodiments of a distributed computing platform service utilize VM nodes that access a primary virtual disk that is stored locally in local storage of hardware platform . In such embodiments portions of data sets e.g. HDFS files utilized by the distributed computing platform e.g. Hadoop may also be stored locally within the primary virtual disk or alternatively within other partitions of local storage that have been configured to be accessible by the relevant VM node. As further discussed below in one embodiment an administrator of the distributed computing platform service may prepare hosts that may be candidates to support VM nodes for the distributed computing platform service by storing one or more base virtual disk templates e.g. .vmdk file etc. in the local storages of such hosts . Such a virtual disk template may include guest OS runtime environment and distributed software components of the distributed computing platform. As such when cluster management application via virtualization management application requests a certain host to instantiate a VM node for a cluster such host can rapidly generate a primary virtual disk for VM node as a linked clone as further described herein by using the locally stored virtual disk template as a base template for the primary virtual disk .

As further depicted in the embodiment of hosts are further networked to a networked storage such as a storage area network SAN . In certain embodiments an administrator may configure VM nodes to consolidate and or output any data processing or analysis results in shared storage system . Certain embodiments may utilize networked storage to store primary virtual disks for VM nodes as opposed to storing such primary virtual disks locally as previously discussed. In one such embodiment VM nodes access their primary virtual disks e.g. .vmdk files etc. stored in networked storage through a distributed file system supported by hypervisors such as the VMFS file system from VMware. In certain of such embodiments as previously discussed data sets processed by VM nodes may still be stored and accessed using local storage through a different distributed file system supported by the VM nodes such as the HDFS file system from Hadoop.

Upon receipt of configuration information in step cluster management application determines a placement of VM nodes consistent with the specified cluster size in the received configuration information by selecting one or more target hosts from host group and determining the quantity of VM nodes to execute at each target host based on the specified cluster size e.g. as provided by a user . For example cluster management application may select a quantity of hosts equal to the cluster size as the target hosts such that each target host will execute one VM node.

In certain embodiments cluster management application determines the placement of VM nodes within host group based on a placement strategy which may be predetermined e.g. as a default setting and or received from a user e.g. in step . For example placing all VM nodes within hosts that reside on a single rack in a data center may provide significant efficiency at the cost of robustness as a hardware failure associated with this rack may disable the entire cluster. Conversely placing all VM nodes at different hosts or at hosts in different racks in the data center may provide significant robustness at the cost of efficiency as such a configuration may increase inter host and or inter rack communication traffic. Accordingly while in some scenarios the administrator may be satisfied with one of the configurations described above embodiments of cluster management application may further enable the administrator to customize a placement strategy for VM nodes within host group that provides a combination of efficiency and robustness.

In step cluster management application communicates with virtualization management application to identify a base virtual disk template as previously discussed that may serve as a base template for a primary virtual disk for each VM node to be instantiated among hosts in host group as determined in step . Cluster management application may identify such a base virtual disk template based upon configuration information received in step or may otherwise choose such a base virtual disk template based on other criteria e.g. pre determined etc. . In one embodiment such an identified base virtual disk template may be stored locally in each of the hosts local storage as previously discussed. For example the administrator of the distributed computing platform service may have pre provisioned each host in host group with one or more such base virtual disk templates. In an alternative embodiment the identified base virtual disk template may reside in networked storage that is accessible by each of the hosts supporting a VM node as determined in step . In yet another alternative embodiment cluster management application may as part of step copy such a base virtual disk template from networked storage or local storage of one host containing such base virtual disk template into the local storage of hosts that have been selected to support a VM node in step . Furthermore in certain embodiments the identified base virtual disk template may be stored as thinly provisioned template in which available e.g. allocated but unused portions of a the virtual disk are omitted from base virtual disk template. Storing a base virtual disk template in a thinly provisioned format may for example reduce the I O usage and time spent by embodiments that copy the base virtual disk template from networked storage to each hosts local storage as discussed above.

In step cluster management application further communicates with virtualization management application to generate a primary virtual disk for each VM node that is based on the identified base virtual disk template. In one embodiment virtualization management application may instruct each host to generate a primary virtual disk for each VM node to be instantiated on such host as a linked clone of the base virtual disk template which may be implemented as a delta disk e.g. set of differences between the linked clone and the base virtual disk template. It should be recognized that generating primary virtual disks as linked clones can significantly speed up the time needed for step as well as use significantly less storage since linked clones are significantly smaller in size relative to the total size of the base virtual disk template. It should be recognized that alternative embodiments may utilizes full copies of the base virtual disk template for each VM node in a host rather than using linked clones.

In step cluster management application communicates with virtualization management application to instruct each target host identified in step to instantiate an appropriate number of VM nodes. In step cluster management application may then communicate with the VM nodes to configure them to properly interact as a cluster of the distributed computing platform. For example in a Hadoop embodiment cluster management application may determine the hostnames and or network addresses for each VM node and provide such mappings to each of the VM nodes to ensure that each VM node can communicate with other VM nodes in the cluster. In some embodiments the configuration information received by cluster management application includes VM attributes such as the number and or speed of virtual processors the amount of memory the amount of local storage and the number and or type of network communication interfaces . In such embodiments cluster management application configures the VM nodes using the specified VM attributes. Similarly cluster management application may provide or otherwise update Hadoop configuration files e.g. script files such as hadoop env.sh and or configuration files such as core site.xml hdfs site.xml and mapred site.xml etc. in the VM nodes to properly identify master and worker VM nodes in the cluster e.g. select VM nodes to serve as NameNodes JobTrackers etc. HDFS paths a quantity of data block replicas for HDFS files as further discussed below etc. As further discussed below embodiments of cluster management application may further generate and provide a rack awareness script to each VM node to identify the physical rack of the host running the VM node.

Once the VM nodes are functioning as a cluster of the distribute computing platform in step cluster management application may provide the data set to the cluster to be processed. Different portions of the received data set are stored in different local storages . Further each portion of data may be stored redundantly in local storages corresponding to hosts at two or more locations e.g. racks as indicated by the generated rack awareness script. Accordingly robustness of the data set may be increased as the data set is protected against single points of failure associated with a single location in cluster . In addition or alternatively redundant copies or replicas of a portion of data may be stored in the same location improving input output efficiency. For example storing replicas at the same location may decrease computing resource e.g. network bandwidth utilization associated with reading the data by reducing the amount of data transferred over network hardware e.g. switches and or routers between locations. Replica placement e.g. same location or different locations may be based on the placement strategy received from the user in step . For example given a placement strategy of operational robustness replicas may be placed at different locations whereas given a placement strategy of operational efficiency replicas may be placed at the same location. Further for a combination of operational robustness and operational efficiency replicas may be placed both at the same location and at different locations. For example for a given original portion e.g. data block of the data set a first replica may be placed at the same location as that of the original portion and a second replica may be placed at a different location from that of the original portion.

In some embodiments as part of step the data set received from the user in step is persistently stored within a distributed file system such as HDFS. In exemplary embodiments cluster management application enables the user to allocate the distributed file system among local storages of hosts . Persistently storing the data set enables the user to maintain the life cycle of cluster by for example ensuring that the data set received from the user is stored in the distributed file system until the user instructs cluster management application to de allocate or tear down the distributed file system and or cluster . Alternatively the user may specify e.g. as a configuration option received in step that the life cycle of the distributed file system should be maintained automatically. In such a scenario cluster management application may allocate the distributed file system among local storages store the data set within the distributed file system and de allocate the distributed file system from local storages when processing of the data set is completed successfully. Regardless of whether the distributed file system is maintained by the user or by cluster management application exemplary embodiments enable the data set to be received from the user and directly stored in the distributed file system rather than first storing the data set outside the cluster and then copying the data set into the distributed file system. Accordingly the delay and computing resource e.g. network bandwidth and or storage input output utilization associated with this step of copying the data set may be avoided. Such resource savings may be significant especially when the data set is large in size e.g. multiple terabytes or petabytes . Further by protecting against single points of failure exemplary embodiments enable overall processing of the data set to continue when a malfunction e.g. hardware failure or power outage occurs at a single location. For example tasks operating against data at the affected location may be restarted in other locations at which redundant copies of the data are stored.

In step cluster management application provides data processing code e.g. also received from the user in step etc. that instructs the distributed computing platform to analyze such data sets e.g. Mapper and Reducer code in Hadoop . For example in a Hadoop embodiment cluster management application may provide a data set or reference to a previously stored data set to the NameNode functionality of a master VM node which in turn directs the DataNode functionality of worker VMs in the cluster to locally store portions of the data set in replicated fashion in accordance with the conventions of HDFS. Additionally cluster management application may submit Mapper and Reducer code to the JobTracker functionality of the master VM node which subsequently determines which worker VM nodes in the cluster will perform execute the Mapper and Reducer code to collectively perform a distributed computing job on the data set to create results . In some embodiments step includes allocating worker VM nodes and or allocating Mapper and or Reducer code to hosts associated with local storage that stores e.g. within the distributed file system the portion of the data set against which the allocated VM nodes Mapper code and or Reducer code will operate.

In certain embodiments cryptographic keys may be used to secure communications between master and worker VM nodes. For example in step cluster management application may generate one or more public private cryptographic key pairs using a utility such as ssh keygen and provide such public keys to worker VM nodes to ensure the master VM node or nodes can communicate with worker VM nodes securely. Accordingly the master VM node may be automatically authenticated by a worker VM node by encrypting such communication with the private key so that the communication may be decrypted using the public key. In step cluster management application is able to communicate with the master VM node and initiate processing of the data set.

Certain distributed file systems such as HDFS that may be used in embodiments to store data sets for processing by VM nodes are designed to reliably store large data sets as files on commodity hardware e.g. local storage of hosts by providing fault detection and automatic data recovery. For example HDFS stores a large data set file by dividing into many data blocks of the same size except for the last block which may have a smaller size and then replicating such data blocks at a certain quantity among the VM nodes in the cluster to provide fault tolerance.

Strategically placing replicated data blocks with VM nodes residing on certain hosts in certain physical racks can provide a balance between writing cost e.g. inter host and or inter rack communication data reliability and availability and aggregating the reading bandwidth. In one embodiment when in the process of storing a data set file e.g. received from step HDFS generates a data block from a data set file a first copy of the data node is placed in the local storage of the first VM node e.g. NameNode etc. a second replica is placed at in the local storage of a second VM node running on a host on a different rack and a third replica is placed on a different VM node running on a host on the same rack as the first VM node. In such an embodiment if the number of replicas is greater than three additional replicas are placed on the local storage random VM nodes in the cluster with few restrictions.

Accordingly awareness of the rack in which each VM node and or host resides may affect the performance e.g. reliability and or efficiency of a distributed file system. In a non cloud non virtualized environment the user of a Hadoop cluster is generally also the administrator and can therefore provide a script to mapping each Hadoop node to a rack in order to provide rack information. It should be recognized that in a cloud environment as described herein network topology such as rack information may not be directly available to the user since the user does not have access to the data center in which the Hadoop cluster operates. Accordingly as previously discussed e.g. step of embodiments may automatically generate a rack awareness script for use by VM nodes to identify the physical location e.g. physical rack or facility and or network location e.g. local area network sub network router or switch at which the VM nodes operate.

In particular embodiments may leverage neighbor device discovery protocols such as IEEE 802.1AB Link Layer Discovery Protocol LLDP and Cisco Discovery Protocol CDP that are used by network devices of VM nodes to advertise information about themselves to other VM nodes on the network. Hypervisors may include a network component e.g. a virtual switch supporting one or more of these protocols. Based on information provided by one or more hypervisors cluster management application can generate a rack awareness script that retrieves location e.g. rack information for the VM nodes by receiving physical switch information e.g. a device identifier from the virtual switches to which these VM nodes connect. Accordingly the rack awareness script may include a mapping of VM node to host and host to location such that data or a task placed at a VM node may be traced to a corresponding location.

By exposing rack information Hadoop embodiments enable for example the NameNode functionality of a master VM node to place a second data block in a physical rack other than the physical rack in which the master VM node or any other VM node with the responsibility of storing the data set file resides. Further to facilitate placing a third data block in the same rack as the master VM node but on a different host the location of a data block may be specified by a combination of a data center a physical rack within the datacenter a host within the rack and a VM node within the host. Providing location information including the VM node enables the master VM node to distinguish between a VM node placed at the same host as the master VM node and a VM node placed at a different host but within the same rack as the master VM node such that redundant data blocks may be distributed across physical equipment e.g. hosts and racks reducing the risk of data loss in the event of an equipment failure.

Certain embodiments also utilize network virtualization techniques such as cross host fencing and virtual eXtensible local area network VXLAN to create isolated networks for different clusters provisioned by the distributed computing platform service that may for example have been requested by different customers through different remote client devices . Network isolation for different clusters dedicate to each such cluster a virtual network to carry traffic between VM nodes inside that cluster and between the cluster and outside reliable storage. The virtual network associated with one cluster is inaccessible to other clusters even though the clusters may share the same physical network. Accordingly such embodiments protect a customer s data from being read or tampered with by another customer.

It should be recognized that the utilization of VM nodes in embodiments provides a certain level of storage isolation with respect to storage e.g. local storage networked storage etc. corresponding to a VM node. For example embodiments of hypervisor ensure that the virtual disk files utilized by VM nodes whether in local storage or networked storage are isolated from other VM nodes or other VMs that may be running on other the hosts or other hosts in the cluster or data center. As such to the extent that certain embodiments of a distributed computing platform such as Hadoop require only certain nodes e.g. NameNode DataNode of a cluster to have access certain data use of VM nodes as described herein provides such isolation.

Additionally in certain embodiments an administrator may have the capability to configure cluster management application and virtualization management application to guarantee certain levels of quality of service QoS and or service level agreements SLA for provisioned clusters. For example the administrator may have the ability to utilize certain virtualized resource isolation techniques such as resource pooling and or classification e.g. NetIOC and or SIOC both provided by VMware vSphere to reserve amounts or shares of computing resources processor network storage etc. for provisioned clusters such that the performance of a cluster provisioned by the distributed computing platform service is unaffected by resource utilization of other cluster or other VMs running in the data center. Such embodiments allow computing resources to be allocated to customers based on the SLA and or QoS level purchased by each customer. For example one customer may be willing to pay for an assured level of resource allocation whereas another customer may accept a best effort allocation of resources in exchange for a lower cost.

In certain embodiments a supported distributed computing platform of the distributed computing platform service may have points of failures within its architecture with respect to certain functions. For example in Hadoop if the NameNode and JobTracker node of a cluster fail then an administrator typically needs to manually intervene to restore operation of the cluster including starting the re processing of a large data set from the beginning.

In certain embodiments cluster management application and virtualization management application provide the administrator of the distributed computing platform service an ability to apply fault tolerance FT techniques to VM nodes of a cluster that may be points of failure within the architecture of the distributed computing platform. For example in a Hadoop environment the administrator may provide a backup VM node on a different host that runs in a synchronized fashion with a master VM node of a provisioned cluster running NameNode and JobTracker functionalities. In one such embodiment the master VM node is configured to transmit its instruction stream e.g. non deterministic events etc. to the backup VM node such that the backup VM node runs in lock step with the master VM node. In the event of a failure of the master VM node the backup VM node can assume the responsibilities of the master VM node without significant loss of time or data. executing as the name node and or as the job tracker node when configuring these nodes.

vHadoop Daemon includes a frontend module to receive user interface requests from customers e.g. cluster operators at client devices such as client devices shown in . Frontend module forwards the requests to a vHadoop core backend module . In addition to frontend module and backend module vHadoop Daemon includes an API handler to support automated service e.g. receipt of commands from a third party process rather than a human operative via the user interface . API handler provides a representational state transfer REST application programming interface API such that remote devices may interact directly with vHadoop core backend module without manual input by a user.

vHadoop Daemon also includes a Resource Management module a Job Management module and a Cluster Management module . Resource Management module includes logic for provisioning nodes to create a vHadoop cluster and for provisioning storage e.g. within a datastore to store the results of the customer s computing job. Job Management module includes logic for delivering customer s computing job to a master VM node starting and stopping job execution and tracking the status of the job during execution. Cluster Management module includes logic for configuring the vHadoop cluster and providing runtime light weight services to Hadoop nodes .

Embodiments described herein provide largely automated provisioning and configuration of nodes in a distributed computing cluster. In addition such embodiments enable automated scaling of computing resources to accommodate submitted computing jobs. For example VM nodes are stateless before a new job is loaded and after a job is completed and are therefore easily created and recycled. If a computing job requiring a large amount of resources is submitted new VM nodes can be automatically provisioned and added to a current cluster. A rebalance command may be executed to distribute some data to newly added nodes such that a new and large scale cluster is created. The procedure may be similar for recycling nodes when computing resource demand decreases and the cluster size is scaled down. For example N 1 nodes where N is the number of data replicas in the cluster may be recycled and the rebalance command may be executed. This process may be performed iteratively avoiding data loss.

The methods described may be performed by computing devices such as hosts and or management device in cluster system shown in . The computing devices communicate with each other through an exchange of messages and or stored data. A computing device may transmit a message as a broadcast message e.g. to an entire network and or data bus a multicast message e.g. addressed to a plurality of other computing devices and or as a plurality of unicast messages each of which is addressed to an individual computing device. Further in some embodiments messages are transmitted using a network protocol that does not guarantee delivery such as User Datagram Protocol UDP . Accordingly when transmitting a message a computing device may transmit multiple copies of the message enabling the computing device to reduce the risk of non delivery.

The operations described herein may be performed by a computer or computing device. A computer or computing device may include one or more processors or processing units system memory and some form of tangible computer readable storage media. Exemplary computer readable storage media include flash memory drives hard disk drives solid state disks digital versatile discs DVDs compact discs CDs floppy disks and tape cassettes. Computer readable storage media store information such as computer readable instructions data structures program modules or other data. Computer readable storage media typically embody computer executable instructions data structures program modules or other data. For example one or more of the operations described herein may be encoded as computer executable instructions and stored in one or more computer readable storage media.

Although described in connection with an exemplary computing system environment embodiments of the disclosure are operative with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use with aspects of the disclosure include but are not limited to mobile computing devices personal computers server computers hand held or laptop devices multiprocessor systems gaming consoles microprocessor based systems set top boxes programmable consumer electronics mobile telephones network PCs minicomputers mainframe computers distributed computing environments that include any of the above systems or devices and the like.

Embodiments of the disclosure may be described in the general context of computer executable instructions such as program modules executed by one or more computers or other devices. The computer executable instructions may be organized into one or more computer executable components or modules. Generally program modules include but are not limited to routines programs objects components and data structures that perform particular tasks or implement particular abstract data types. Aspects of the disclosure may be implemented with any number and organization of such components or modules. For example aspects of the disclosure are not limited to the specific computer executable instructions or the specific components or modules illustrated in the figures and described herein. Other embodiments of the disclosure may include different computer executable instructions or components having more or less functionality than illustrated and described herein.

Aspects of the disclosure transform a general purpose computer into a special purpose computing device when programmed to execute the instructions described herein. The operations illustrated and described herein may be implemented as software instructions encoded on a computer readable storage medium in hardware programmed or designed to perform the operations or both. For example aspects of the disclosure may be implemented as a system on a chip.

The order of execution or performance of the operations in embodiments of the disclosure illustrated and described herein is not essential unless otherwise specified. That is the operations may be performed in any order unless otherwise specified and embodiments of the disclosure may include additional or fewer operations than those disclosed herein. For example it is contemplated that executing or performing a particular operation before contemporaneously with or after another operation is within the scope of aspects of the disclosure.

When introducing elements of aspects of the disclosure or the embodiments thereof the articles a an the and said are intended to mean that there are one or more of the elements. The terms comprising including and having are intended to be inclusive and mean that there may be additional elements other than the listed elements.

Having described aspects of the disclosure in detail it will be apparent that modifications and variations are possible without departing from the scope of aspects of the disclosure as defined in the appended claims. As various changes could be made in the above constructions products and methods without departing from the scope of aspects of the disclosure it is intended that all matter contained in the above description and shown in the accompanying drawings shall be interpreted as illustrative and not in a limiting sense.

