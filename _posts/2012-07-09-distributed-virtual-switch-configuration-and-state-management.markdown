---

title: Distributed virtual switch configuration and state management
abstract: Techniques are disclosed for pushing configuration changes of a distributed virtual switch from a management server to a plurality of host servers underlying the distributed virtual switch. The approach includes sending, in parallel, by the management server, a message to each of the plurality of host servers. The message specifies a final configuration state for one or more virtual ports emulated via virtualization layers of the host servers. The approach further includes determining, by each of the plurality of host servers, port state configuration changes to make to the virtual ports to achieve the final configuration state, and reconfiguring, by each of the plurality of host servers, their respective virtual ports, to match the final configuration state.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09231892&OS=09231892&RS=09231892
owner: VMware, Inc.
number: 09231892
owner_city: Palo Alto
owner_country: US
publication_date: 20120709
---
Computer virtualization is a technique that involves encapsulating a computing machine platform into a virtual machine VM that is executed under the control of virtualization software running on a single hardware computing platform also referred to herein as host server or host . A group of hardware computing platforms may be organized as a cluster to provide resources for VMs. In a data center it is common to see hundreds even thousands of VMs running on multiple clusters of host servers.

An administrator may use virtualization management software to manage virtual machine configurations and computing resource allocations. Because of the large number of VMs managed within some data centers and sometimes across multiple data centers some of the administrator s tasks are automated. For example software techniques such as adaptive resource scheduling and adaptive power management have been developed to assist the administrator in balancing workloads across host servers and powering host servers on and off as needed.

One feature of the virtualized computing environment controlled by the virtualization management software is virtual networking. VMs can be created with software based virtual network adapters that are logically connected to physical network adapters installed in the host computer. The virtual network adapters are connected to the physical network adapters through software based virtual switches. One way to manage virtual network connections in data centers with a large number of VMs running on multiple host servers i.e. host servers is via a distributed virtual switch DVSwitch described in US 2009 0292858 incorporated in its entirety herein by reference. DVSwitches permit users to manage virtual switches on multiple host servers as if the ports of those virtual switches belonged to a single switch. Further DVSwitches persist port runtime states across host servers.

DVSwitches have components residing in both host servers hosting VMs and a management server running virtual management software. Communication between the host servers and the management server can become a bottleneck as the scale of the DVSwitch increases either in terms of the number of virtual ports in the DVSwitch or in terms of the number of host servers that joined the DVSwitch. In particular switch configuration changes in the DVSwitch management plane may need to be propagated to a large number of host servers which impacts the latency of operations. The volume of port runtime data being transmitted between the large number of host servers and the management server and processed by the management server may also negatively affect the performance of the management server and or the network. That is increases in the number of host servers and virtual ports result in a proportionately large amount of port configuration and runtime state data that must be sent over the network from the management server to the host servers and vice versa.

The disclosure provides a technique for pushing in parallel configuration changes of a distributed virtual switch from a management server to a plurality of host servers underlying the distributed virtual switch. The technique includes sending by the management server a message to each of the plurality of host servers. The message specifies a final configuration state for one or more virtual ports emulated via virtualization layers of the host servers. The technique further includes determining by each of the plurality of host servers port state configuration changes to make to the virtual ports to achieve the final configuration state and reconfiguring by each of the plurality of host servers their respective virtual ports to match the final configuration state.

Further embodiments of the present invention include a computer readable storage medium storing instructions that when executed by a computer system cause the computer system to perform one or more the of the techniques set forth above and a computer system programmed to carry out one or more of the techniques set forth above.

Embodiments presented herein provide techniques for managing configuration and runtime state changes of a distributed virtual switch DVSwitch . In one embodiment a management server asynchronously pushes a final configuration state to a plurality of host servers each of which then reconfigures its own virtual port states to match the final configuration state where appropriate. Doing so reduces resulting network traffic because only the final configuration state is sent. In addition this approach relieves the management server from having to cache port state data for virtual ports of the plurality of host servers because the management server instead sends the final configuration state and each host server determines based on the final configuration state what action s to take for virtual ports supported by the host server. Embodiments presented herein further provide techniques for host servers to push port runtime state data to the management server for persisting in a database of the management server. That is the virtual switch on a given host notifies the management server about port state changes and port state runtime data.

In one embodiment each host server includes a clobber API configured to accomplish state transition in a manner that is idempotent from the perspective of the management server. That is the management server need only send a desired final state configuration by invoking the clobber API and each host server can accomplish state transitions independent of the management server. Doing so relieves the management server from tracking the current state of each host server or having to perform change delta computations before invoking state transitions on the host servers. Upon a successful state transition the clobber API may make a callback notifying the management server of the success.

In a further embodiment a management application asynchronously invokes clobber APIs on host servers. Specifically the management application sends API invocations in batches to a network transport component. Because the API invocations are sent in batches and executed in parallel the time required to transition multiple hosts is not linearly proportional to the number of invocations. Each API invocation may further lead to a success fail or time out error callback from the network transport component the management application may wait for such callbacks in parallel and add an entry to a re push queue for each timed out and failed API invocation. Periodically the management application may re invoke each API that the re push queue indicates as having previously timed out or failed.

In yet another embodiment the host servers may push certain port runtime state data to the management server by sending port data when certain predefined events occur. That is the host server may send port state change data for some port states only when a predefined port state change event occurs. Sending port data upon predefined events ensures that runtime data is communicated only after those events thereby reducing the number of times and the amount of data being sent and processed. For other port states that change frequently the host server may send state data only when the port state stops changing. Only sending port data when the port state stops changing also reduces the number of times and the amount of data being sent and processed.

Reference is now be made in detail to several embodiments examples of which are illustrated in the accompanying figures. It is noted that wherever practicable similar or like reference numbers may be used in the figures and may indicate similar or like functionality. The figures depict embodiments for purposes of illustration only. One skilled in the art will readily recognize from the following description that alternative embodiments of the structures and method illustrated herein may be employed without departing from the principles described herein.

DVSwitch simplifies provisioning and administrating virtual networking across many hosts and clusters though a centralized interface. That is a user need not manage multiple individual and segregated virtual switches on corresponding host servers because switch configuration e.g. VLAN configurations and port runtime states can be managed across host servers via DVSwitch . Additionally DVSwitch may provide simplified end to end physical and virtual network management through third party switch extensions enhanced provisioning and traffic management capabilities through private VLAN support and bidirectional VM rate limiting enhanced security and monitoring for VM migrations prioritized controls between different traffic types and or load based dynamic adjustment across a team of physical adapters on the distributed virtual switch.

Network transport component permits asynchronous and parallel invocation of APIs on DVSwitch server and on the host servers . After a DVSwitch management application not shown running on DVSwitch sever sends data to network transport component control is returned immediately to the management application. Network transport component may make a callback to the application e.g. notifying the application that data was successfully delivered . Because API invocation via the networking transport is asynchronous applications may send batch invocation requests to the network transport component. For example a management application for DVSwitch may send a switch configuration change as a batch request to network transport component which then invokes APIs on each of a plurality of host servers to accomplish the configuration change. Similarly host servers may asynchronously push port state runtime data to DVSwitch server via network transport component .

As shown each host server includes a plurality of virtual machines VMs and a virtual switch . Host servers are configured to deliver virtualization based distributed services to information technology environments. Each host server provides a virtualization layer that abstracts processor memory storage and or networking resources into multiple VMs that run side by side on the same host server . In one embodiment virtualization software can be installed directly on the server hardware and inserts a virtualization layer between the hardware and the operating system. The virtualization software partitions a host server into multiple secure and portable VMs that run on the same host server. Thus each VM represents a complete system with processors memory networking storage and or BIOS.

Like the DVSwitch each virtual switch of host server is a software abstraction of a physical switch and includes a plurality of virtual ports. Each virtual port may support a virtual network interface card vNIC instantiated in a virtualization layer supporting VMs . Each VM may be logically connected to a virtual port on a virtual switch which is in turn logically connected to a physical NIC pNIC not shown included in host server . Each pNIC is connected to one or more physical networks and communicates with other pNICs and the outside world via a router or a switch.

A user may use the DVSwitch abstraction to manage a plurality of virtual switches on one or more host servers . DVSwitch includes DVPorts which are software abstractions capturing the configuration and runtime states of the virtual ports. In one aspect each DVPort stores data structures representing the configuration and runtime state of a corresponding virtual port of a DVSwitch . As noted DVSwitch itself spans multiple virtual switches on host servers thereby permitting a user to manage every virtual port of those virtual switches as though they were part of a larger virtual switch namely DVSwitch .

As suggested by a single host may interact with a plurality of DVSwitches each associated with a corresponding network. In the present example hosts each interact with DVSwitches which in turn are connected to networks and respectively. Host includes pNIC connecting DVSwitch to network pNIC connecting DVSwitch to network and pNIC connecting DVSwitch to network . Host includes corresponding components although many other configurations are possible as would be recognized by those skilled in the art.

The DVSwitch as a software abstraction resides on a variety of hardware in a distributed manner hence the term distributed virtual switch. For example DVSwitch components A B reside in hosts as well as DVSwitch server . DVSwitch components A B are illustrated in with a dotted line box indicating portions of DVSwitch A B that make up the DVSwitch. In addition to these components logic implementing DVSwitch functionality is located in virtualization software and DVSwitch manager as described in more detail below.

As shown in a virtual port is maintained for each VNIC respectively. Each VNIC emulator interacts with NIC drivers in VMs to send and receive data to and from VMs . For example each VNIC emulator may maintain the state for one or more VNICs for each VM . Alternatively multiple instances of VNIC emulators only one shown for each host may be instantiated within a virtualization software layer. In either case a single VM may have one or more VNICs which may be implemented by one or more VNIC emulators. For the purpose of illustration shows only one VNIC for each VM and only one VM for each host. It should be recognized that discussion herein of VNICs is actually a discussion of a VNIC state implemented and maintained by each VNIC emulator . As mentioned previously virtual devices such as VNICS are software abstractions that are convenient to discuss as though part of VMs but are actually implemented by virtualization software using emulators . The state of each VM however includes the state of its virtual devices which is controlled and maintained by the underlying virtualization software . When a VM is suspended or shut down and migrated its state which includes network settings such as the MAC addresses of any VNICS are migrated along with the VM.

Virtual switches that are connected to the same physical network may be managed using one DVSwitch. Physical network may be e.g. a local area network. In DVSwitch includes distributed virtual ports DVPorts . As described above with respect to each DVPort is a software abstraction that encapsulates the personality both configuration and runtime state of a corresponding virtual port. For example DVPort may contain one or more data structures representing the configuration and runtime states of a virtual port of a virtual switch on host server . Each DVPort may be created with a configuration predefined by a network administrator. Virtual ports are created and start with a blank configuration state but once associated with a DVPort assume the configuration and runtime state of the associated DVPort. When a VM is migrated or powered off and on the connection between a DVPort and a virtual NIC is not affected because the DVPort persists and migrates with the VM to which it is connected. For example when a VM powers off or is suspended the DVSwitch management application may release the underlying virtual port and resources used for the associated DVPort but DVSwitch does not release the DVPort to be used by another virtual NIC. Before releasing the virtual port the system may synchronize all registered port state back to DVPort. When the VM powers on again or resumes the DVPort may request a new virtual port on host and synchronize all the registered port state back to it.

The term connection is used herein to describe an association between a virtual NIC with a DVPort. In one embodiment this association is maintained locally by virtualization software in a table or other data structure within database as described in more detail below. When a connection to a DVPort is created another virtual NIC cannot be connected to that same DVPort without explicitly disconnecting the already connected virtual NIC. Once the VM is powered on the DVPort may be referred to as being in a linked up state meaning the virtual NIC and a virtual port are ready to send and receive frames.

DVSwitch and DVports are created from the physical resources available to physical NICs in the managed domain of hosts . Once created database in DVSwitch server stores the state of DVSwitch and DVports . Database may be provided on DVSwitch manager which is connected to hosts via physical network . For states that are global to a given DVSwitch DVSwitch manager pushes read only copies to each of the hosts in the managed domain. States that are specific to a given DVport however are needed by the host where the DVport s corresponding virtual port is located. Thus the DVSwitch manager may push the DVport state only to the necessary host.

In addition to being stored in database some DVSwitch states may be cached on each host in the managed domain via local storage . For example DVSwitch server push relevant updates to each host s local storage in the managed domain. To push DVSwitch updates to host servers DVSwitch manager may send an asynchronous batch request to a network transport component . In response network transport component may invoke in parallel clobber APIs on each of hosts indicated in the batch request. Clobber APIs are configured to accomplish state transitions in a manner that is idempotent. DVSwitch manager may simply send via network transport component the desired final state configuration and each clobber API independently determines how to achieve that final state configuration for the respective host server. Even if those hosts do not have inconsistent DVSwitch configuration state data the idempotency of the clobber API ensures that further clobber API invocations are harmless. Such an approach relieves the DVSwitch manager from having to cache the prior states of each host server and performing change delta computations before invoking state transitions on host servers. Further clobber APIs may return from invocation upon a successful state transition. Network transport component may be configured to wait for such invocation return and make a callback to notify DVSwitch manager of the completion of the invocation. DVSwitch manager may add an entry to a re push queue if the clobber API encounters an error or timeout. Similarly for a request to remove an object the DVSwitch manager may add an entry to a rip queue if the removal is unsuccessful. For example where removal of a virtual port object from a host server fails the DVSwitch manager may add an entry to the rip queue indicating the failure.

The phrase local storage should be interpreted broadly herein to reflect a data storage device or system that is readily accessible by the host servers. In one embodiment hosts assume that local storage is up to date and that any updates they make to the local storage will be pushed back to database in a timely manner. In such a case hosts may push runtime port state changes to database via one of two techniques. For runtime port states which change infrequently hosts may monitor the port state and update database when a port state change is detected. Such an approach eliminates the need for DVSwitch manager to periodically poll local storage to determine whether the port state has changed. In contrast for port runtime states which change frequently such as port statistics counter states hosts may monitor the port state and update database only when the port state stops changing. That is data pertaining to these port runtime states which would often be stale in database even if database were updated periodically is not sent to database until the port state stops changing. While the port states continue changing a user wishing to retrieve the current port runtime states may request DVSwitch manager to directly poll local storage to retrieve data for the port runtime state.

As shown method begins at step where DVSwitch manager determines or receives a network configuration state for multiple ports on DVSwitch . For example a user may assign a VLAN or a security profile affecting multiple DVports of DVSwitch . DVSwitch server stores data of such assignments in Database and DVSwitch manager may push out such assignments to one or more of hosts whose virtualization layers support virtual ports associated with the DVports . This ensures DVSwitch configuration data in database remains consistent with configuration data in local storage of each host .

At step DVSwitch manager identifies hosts having virtual switches with virtual ports corresponding to DVports on DVSwitch . For states that are global to DVSwitch DVSwitch manager may push read only copies to each of hosts in the managed domain. For states that are specific to a given DVport however the DVSwitch manager may push updates only to the necessary host or hosts. In such cases the DVSwitch manager may identify those hosts having virtual switches supporting DVports to which the state change applies.

At step DVSwitch manager makes a send batch request to network transport component specifying a target configuration state for ports on virtual switches for the identified hosts. To eliminate waiting times network transport component may be configured to receive asynchronous input output I O requests from DVSwitch manager and network transport component may further be configured to perform I O operations to hosts in parallel. In particular network transport component may invoke clobber APIs on each of host servers in parallel and make a callback to DVSwitch manager upon completion of the invocation of clobber APIs . For such parallel invocations the time taken is not linearly proportional to the number of invocations so virtual ports on a plurality of host servers can be updated faster than via non parallel invocations.

In one embodiment clobber API may perform state transition in a manner that is idempotent. In such a case DVSwitch manager may only send a desired final configuration state to hosts by invoking clobber API . Even if those hosts do not have inconsistent DVSwitch configuration state data the idempotence of clobber API ensures that further clobber API invocations are harmless. Because DVSwitch manager may only send a desired final state DVSwitch manager need not determine the change delta for virtual ports on each of hosts necessary to change the current configuration states of those virtual ports to the final configuration state. Further DVSwitch server need not store e.g. in database current configuration states for virtual ports on hosts for use in such change delta comparisons.

In one embodiment the making of change delta comparisons is offloaded to each of hosts . That is rather than having DVSwitch manager poll hosts and make change delta comparisons for virtual ports on each of hosts each of hosts makes change delta comparisons for itself based on each of hosts own configuration states and the final configuration state received from DVSwitch manager . For example to move DVports from one VLAN to another VLAN DVSwitch manager may simply push the final DVport assignments to one or more of hosts . Hosts may then determine what change of port assignments are necessary to update each DVport and associated virtual port of the respective host s virtual switch.

At step DVSwitch manager monitors for timed out and failed API invocations and DVSwitch manager resends configuration transition requests to hosts corresponding to those invocations that timed out or failed. DVSwitch manager may add an entry to re push queue for every clobber API call that times out or fails. To ensure consistency of DVSwitch configuration state data across DVSwitch server and hosts DVSwitch manager may monitor re push queue and resend configuration transition requests to hosts on which clobber API invocations timed out or failed because those hosts may have inconsistent DVSwitch configuration state data. Even if those hosts do not have inconsistent DVSwitch configuration state data the idempotency of clobber API ensures that further clobber API invocations are harmless. DVSwitch manager may also report contents of the re push queue to a user as out of sync information.

Method begins at step where network transport component receives a batch send request from DVSwitch manager . The send request may be an asynchronous I O request invoked via an API. That is network transport component may expose an API for sending data via asynchronous I Os and DVSwitch manager may invoke this API to for example push port state configurations to hosts . Further network transport component may return control to DVSwitch manager immediately following the send request.

At step network transport component invokes the clobber API on the host servers indicated in the batch request. As discussed above because the clobber API is asynchronous network transport component may invoke clobber APIs essentially in parallel.

At step network transport component waits for a callback from the relevant host servers indicating the state transition was completed successfully. For example clobber APIs may each make a success callback to network transport component indicating that state transition was successful. Further clobber APIs may return an error message in cases where a state transition is unsuccessful. Network transport component may wait for such a success return or fail with an error message so that network transport component can then report the results of the batch request to DVSwitch manager .

At step network transport component determines whether any clobber API calls made at step have timed out or failed. A time out or failed of a clobber API call indicates that the DVSwitch configuration state data of a corresponding host server was not updated. In such cases a re push of the DVSwitch configuration state may be necessary to ensure data consistency between DVSwitch server and the host server on which clobber API timed out or failed. At step DVSwitch manager which waits for notification from the network transport component of the completion of invocation adds an entry to a re push queue for each timed out failed API call identified by network transport component at step to indicate that a re push of the unsuccessfully pushed DVSwitch configuration state data is necessary.

In one embodiment DVSwitch server maintains a re push queue for each host server spanned by the DVSwitch i.e. one queue per physical host server and the DVSwitch manager adds an entry to the appropriate re push queue when a corresponding clobber API invocation times out or fails. For example network transport component may add a reference to a configuration state which was not successfully pushed to a host to a corresponding re push queue for that host. In another embodiment DVSwitch manager may periodically resend configuration transition requests based on the entries of re push queue . That is DVSwitch manager may periodically re push DVSwitch configuration states to host servers when clobber API calls time out or fail.

Method begins at step where DVSwitch manager identifies a network virtualization construct to remove from one or more host servers. For example where a VM is connected to a virtual port associated with a DVport of DVSwitch the virtualization layer of host may instantiate and execute both the VM and the virtual port. When the VM is relocated to a second host server a virtual port may be created on the second host server and associated with DVport . After the VM is relocated and associated with the DVport the virtual port on the original host server is neither connected to the VM nor associated with a DVport. In such a case DVSwitch manager may identify the virtual port on the original host server as a network virtualization construct for removal i.e. the object representing the virtual port on the original host server can be deleted .

At step DVSwitch manager invokes an API on the host server to remove the network virtualization construct. In one embodiment DVSwitch manager may invoke a remove API exposed by network transport component for asynchronous I O and network transport component may then invoke an API on the host server to actually remove the network virtualization construct. The API on the host server may return indicating the network virtualization construct was successfully removed from the host server. Network transport component may wait for such a return and indicate unsuccessful removal if a success return is not received e.g. if the invocation times out or fails to DVSwitch manager . Further DVSwitch manager may add an entry to a rip queue for each unsuccessful invocation.

At step DVSwitch manager monitors rip queue for timed out or failed API invocations. In one embodiment DVSwitch manager may periodically examine rip queue . For example DVSwitch manager may examine the rip queue when the DVSwitch manager examines re push queue . For each entry in rip queue DVSwitch manager may further re invoke an API call on the corresponding host server to remove the network virtualization construct. DVSwitch manager may also report contents of rip queue to a user.

Method begins at step where a host server monitors a predefined runtime port state. In one embodiment the predefined port state is a state change that happens relatively infrequently. For example port VLAN and mirroring assignments tend to remain stable in large data center environments with many host servers and corresponding VMs . As a result DVSwitch manager need not poll host servers regarding port VLAN and mirroring assignments because such polling is often inefficient. The approach of method avoids such inefficient polling by having host server update DVSwitch manager only when a state change event occurs.

At step host server identifies a state change event for the predefined port state. In one embodiment the state change event may be predefined. For example the host server may determine that a state change made by a user or by a hypervisor corresponds to a predefined event upon which updating of DVSwitch manager must be performed.

At step host server updates DVSwitch manager with the state change. Such an update may be asynchronous relative to the operation of the DV switch manager . In one embodiment host server pushes state data to DVSwitch manager via network transport component . In such a case host servers may utilize network transport component for asynchronous I O similar to the asynchronous I O discussed above with respect to . In another embodiment host server may simply notify DVSwitch manager of the port state change event and DVSwitch manager may then in its discretion fetch data associated with the port state from host .

In a further embodiment DVSwitch manager store updates in database or memory not shown of DVSwitch server . As discussed above database stores the runtime state of DVports so that DVSwitch maintains connections between DVports and virtual NICs of VMs even when those VMs are migrated or powered off and on again.

Method begins at step where host server monitors a predefined port state. In one embodiment the predefined port state is a port state which is identified either manually by a user or automatically by host servers or DVSwitch server as changing relatively frequently. For example counters which measure network performance such as packet in and packet out statistics change relatively frequently. As a result DVSwitch manager need not periodically poll host servers statistics counters because the cached results of such polling would often be stale anyway. The approach illustrated by method avoids such polling altogether by having host server update DVSwitch manager only when the predefined port state stops changing.

At step host server identifies that the predefined port state has stopped changing. For example host server may determine that a virtual port to which a DVport is associated has a state of linked down. This state refers to a release of virtual port resources by a virtual NIC. This may occur for example when a VM associated with the virtual NIC is powered off. After linking down all I O and other activity on the virtual port is quiesced and as a result statistics counters for example stop changing.

At step host server updates DVSwitch manager with data for the port state. Such an update may be asynchronous. In one embodiment host server pushes state data to DVSwitch manager via network transport component . In such a case network transport component may support asynchronous I O by the host server similar to the asynchronous I O discussed above with respect to . In another embodiment host server may simply notify DVSwitch manager that the predefined port state has stopped changing and DVSwitch manager may then in its discretion fetch data associated with the port state from host .

In one embodiment DVSwitch manager may cause received state data updates to be stored in database or memory not shown of DVSwitch server . State data stored in database or memory may then be displayed to a user for various purposes including debugging of the network.

In a further embodiment the user may further fetch real time port runtime state data via a pass through API exposed by DVSwitch manager . The pass through API may be configured to invoke an API exposed by host servers for retrieving real time runtime state data. In such a case the user may have access to real time runtime state data even though state data is otherwise updated to DVSwitch manager only when the state stops changing. For example the user may invoke the pass through API to fetch real time statistics counter data for a DVport even when the virtual port associated with that DVport is linked up to a virtual NIC and thus the statistics counter is frequently changing . The method ends thereafter.

Embodiments disclosed herein provide a technique for pushing configuration changes to the configuration of a distributed virtual switch to a plurality of host servers. The distributed virtual switch provides a software abstraction of a physical switch with a plurality of distributed virtual ports that correspond to virtual port of virtual switches on the plurality of host servers. The approach includes invoking by an application of the management server application programming interfaces APIs exposed by each of the plurality of host servers. Each API invocation requests a state transition to a final configuration state. The approach further includes determining by each API port state configuration changes required for virtual ports emulated via a virtualization layer of the respective host server to achieve the final configuration state. Each of the host servers then performs the required changes for virtual ports on the respective host server.

In a further embodiment host servers push port runtime state data for distributed virtual ports of a distributed virtual switch to a management server by monitoring a predefined port state identifying a state change event for the port state and updating the management server with data for the port state. In such a case the predefined port state may be a port state which is determined to change infrequently. Conversely for a predefined port state which frequently changes the host servers may push port runtime state data for distributed virtual ports of a distributed virtual switch from a host server to a management server by monitoring a predefined port state identifying that the port state has stopped changing and updating the management server with data for the port state.

Advantageously embodiments described herein distribute data processing across host servers and reduce network congestion. To ensure consistency of port state configuration data across a distributed virtual switch a management server may invoke a clobber API which accomplishes state change in an idempotent manner from the perspective of the management server. That is rather than determining and sending the change delta needed for each port to attain the new configuration management server simply sends a desired final state to the host servers via clobber API and the host servers independently determine and make any necessary port state changes. Further to update the management server on runtime state data of ports of the distributed virtual switch each host may either only send state data when the runtime state changes or only send state data when the runtime state stops changing. This approach eliminates the need for management server to periodically poll each host and to store cached copies of each hosts runtime state data.

The various embodiments described herein may employ various computer implemented operations involving data stored in computer systems. For example these operations may require physical manipulation of physical quantities usually though not necessarily these quantities may take the form of electrical or magnetic signals where they or representations of them are capable of being stored transferred combined compared or otherwise manipulated. Further such manipulations are often referred to in terms such as producing identifying determining or comparing. Any operations described herein that form part of one or more embodiments of the invention may be useful machine operations. In addition one or more embodiments of the invention also relate to a device or an apparatus for performing these operations. The apparatus may be specially constructed for specific required purposes or it may be a general purpose computer selectively activated or configured by a computer program stored in the computer. In particular various general purpose machines may be used with computer programs written in accordance with the teachings herein or it may be more convenient to construct a more specialized apparatus to perform the required operations.

The various embodiments described herein may be practiced with other computer system configurations including hand held devices microprocessor systems microprocessor based or programmable consumer electronics minicomputers mainframe computers and the like.

One or more embodiments of the present invention may be implemented as one or more computer programs or as one or more computer program modules embodied in one or more computer readable media. The term computer readable medium refers to any data storage device that can store data which can thereafter be input to a computer system. Computer readable media may be based on any existing or subsequently developed technology for embodying computer programs in a manner that enables them to be read by a computer. Examples of a computer readable medium include a hard drive network attached storage NAS read only memory random access memory e.g. a flash memory device a CD Compact Discs CD ROM a CD R or a CD RW a DVD Digital Versatile Disc a magnetic tape and other optical and non optical data storage devices. The computer readable medium can also be distributed over a network coupled computer system so that the computer readable code is stored and executed in a distributed fashion.

Although one or more embodiments of the present invention have been described in some detail for clarity of understanding it will be apparent that certain changes and modifications may be made within the scope of the claims. Accordingly the described embodiments are to be considered as illustrative and not restrictive and the scope of the claims is not to be limited to details given herein but may be modified within the scope and equivalents of the claims. In the claims elements and or steps do not imply any particular order of operation unless explicitly stated in the claims.

In addition while described virtualization methods have generally assumed that VMs present interfaces consistent with a particular hardware system persons of ordinary skill in the art will recognize that the methods described may be used in conjunction with virtualizations that do not correspond directly to any particular hardware system. Virtualization systems in accordance with the various embodiments implemented as hosted embodiments non hosted embodiments or as embodiments that tend to blur distinctions between the two are all envisioned. Furthermore various virtualization operations may be wholly or partially implemented in hardware. For example a hardware implementation may employ a look up table for modification of storage access requests to secure non disk data.

Many variations modifications additions and improvements are possible regardless the degree of virtualization. The virtualization software can therefore include components of a host console or guest operating system that performs virtualization functions. Plural instances may be provided for components operations or structures described herein as a single instance. Finally boundaries between various components operations and data stores are somewhat arbitrary and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of the invention s . In general structures and functionality presented as separate components in exemplary configurations may be implemented as a combined structure or component. Similarly structures and functionality presented as a single component may be implemented as separate components. These and other variations modifications additions and improvements may fall within the scope of the appended claims s .

