---

title: Global memory as non-volatile random access memory for guest operating systems
abstract: Global memory of a storage system may be used to provide NVRAM capabilities to guest operating systems accessing the storage system. The non-volatility of NVRAM (i.e. that retains its information when power is turned off) provides that an NVRAM device provided by global memory may be used as a journaling device to track storage operations and facilitate recovery and/or failover processing in a storage system without needing to add additional hardware and/or other installed devices. Use of the global memory according to the system described herein to provide an NVRAM device, that may function as a journaling device, provides for the speeding up of transactions, thereby improving metadata intensive operations performance and reducing recovery time and/or failover time of a storage system without adding additional hardware support.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08990520&OS=08990520&RS=08990520
owner: EMC Corporation
number: 08990520
owner_city: Hopkinton
owner_country: US
publication_date: 20120330
---
This application is related to the field of virtualized computing environments and more particularly the use of global memory as non volatile random access memory NVRAM for a guest operating system Guest OS .

Host processor systems may store and retrieve data using a storage device containing a plurality of host interface units I O modules disk drives and disk interface units disk adapters . Such storage devices are provided for example by EMC Corporation of Hopkinton Mass. and disclosed in U.S. Pat. No. 5 206 939 to Yanai et al. U.S. Pat. No. 5 778 394 to Galtzur et al. U.S. Pat. No. 5 845 147 to Vishlitzky et al. and U.S. Pat. No. 5 857 208 to Ofek which are incorporated herein by reference. The host systems access the storage device through a plurality of channels provided therewith. Host systems provide data and access control information through the channels to the storage device and the storage device provides data to the host systems also through the channels. The host systems do not address the disk drives of the storage device directly but rather access what appears to the host systems as a plurality of logical disk units. The logical disk units may or may not correspond to the actual disk drives. Allowing multiple host systems to access the single storage device unit allows the host systems to share data stored therein.

A hypervisor is a software implementation that may provide a software virtualization environment in which other software may run with the appearance of having full access to the underlying system hardware but in which such access is actually under the complete control of the hypervisor. The software running in such a hypervisor managed environment may execute within a virtual machine VM and multiple VMs may be managed simultaneously by a hypervisor. Hypervisors may generally be classed as type 1 or type 2 depending on whether the hypervisor is running in a supervisor mode on bare metal type 1 or is itself hosted by an operating system OS type 2 . A bare metal environment describes a computer system in which a VM is installed directly on hardware rather than within a host OS. ESX and ESXi produced by VMware Inc. of Palo Alto Calif. are examples of bare metal hypervisors that may run directly on server hardware without requiring an additional underlying operating system. For discussions of the use of known hypervisors or virtual machine monitors in virtualized computing environments see for example U.S. Pat. No. 7 665 088 to Bugnion et al. entitled Context Switching to and from a Host OS in a Virtualized Computer System U.S. Pat. No. 7 743 389 to Mahalingam et al. entitled Selecting Between Pass Through and Emulation in a Virtual Machine Environment and U.S. Pat. No. 7 945 436 to Ang et al. entitled Pass Through and Emulation in a Virtual Machine Environment which are all assigned to VMware Inc. and which are all incorporated herein by reference. Although the term hypervisor is principally used herein this term should be understood herein to refer to any appropriate software layer having the described features and functions discussed herein.

Techniques are known in storage systems to provide failover capability and recovery operations that involve use of journaling devices that are used to track and log storage operations. Reference is made for example to U.S. Pat. No. 7 558 926 to Oliveira et al. entitled Continuous Data Backup Using Distributed Journaling and U.S. Pat. No. 7 599 951 to Oliveira et al. entitled Continuous Data Backup which are both assigned to EMC Corporation of Hopkinton Mass. and are both incorporated herein by reference and which disclose various techniques for providing continuous storage backup of storage data using journaling devices. In some circumstances it is noted that the journaling devices may act as performance bottlenecks and that recovering from failures using such journaling devices may in some situations take significant amounts of time. Additionally the use of additional hardware and or other installed devices to support journaling processing to support failover and recovery capability may result in additional costs.

Accordingly it would be desirable to provide a system and techniques for enabling efficient use of resources in connection with journaling failover and recovery capabilities particularly in connection with storage systems.

According to the system described herein a method for using global memory of a distributed system to provide non volatile memory random access memory NVRAM capabilities includes identifying the global memory of the distributed system. Access by a guest operating system is provided to the global memory. The global memory accessed by the guest operating system is used as NVRAM. Operations of the guest operating system are performed using the NVRAM provided by the global memory. The distributed system may be a storage system and the operations may be journaling operations that include recovery or failover processing. The providing of access by the guest operating system to the global memory may include loading the guest operating system using a hypervisor and controlling access of the guest operating system to the storage system according to the hypervisor. The global memory acting as NVRAM may be distributed across a plurality of storage devices and processing resources accessing the global memory acting as NVRAM may be distributed across the plurality of storage devices.

According further to the system described herein a non transitory computer readable medium stores software for using global memory of a distributed system to provide non volatile memory random access memory NVRAM capabilities. The software includes executable code that identifies the global memory of the distributed system. Executable code is provided that provides access by a guest operating system to the global memory. Executable code is provided that uses the global memory accessed by the guest operating system as NVRAM. Executable code is provided that performs operations of the guest operating system using the NVRAM provided by the global memory. The distributed system may be a storage system and the operations may be journaling operations that include recovery or failover processing. The executable code that provides access by the guest operating system to the global memory may include executable code that loads the guest operating system using a hypervisor and controlling access of the guest operating system to the storage system according to the hypervisor. The global memory acting as NVRAM may be distributed across a plurality of storage devices and processing resources accessing the global memory acting as NVRAM may be distributed across the plurality of storage devices.

According further to the system described herein a storage system having global memory used to provide non volatile memory random access memory NVRAM capabilities includes at least one processor providing processing resources for the distributed system and a computer readable medium storing software executable by the at least one processor. The software includes executable code that identifies the global memory of the distributed system. Executable code is provided that provides access by a guest operating system to the global memory. Executable code is provided that uses the global memory accessed by the guest operating system as NVRAM. Executable code is provided that performs operations of the guest operating system using the NVRAM provided by the global memory. The operations may be journaling operations that include recovery or failover processing. The executable code that provides access by the guest operating system to the global memory may include executable code that loads the guest operating system using a hypervisor and controlling access of the guest operating system to the storage system according to the hypervisor. The global memory acting as NVRAM may be distributed across a plurality of storage devices and processing resources accessing the global memory acting as NVRAM may be distributed across the plurality of storage devices.

In an embodiment of the system described herein data from the storage device may be copied to the remote storage device via a link . For example the transfer of data may be part of a data mirroring or replication process that causes the data on the remote storage device to be identical to the data on the storage device . Although only the one link is shown it is possible to have additional links between the storage devices and to have links between one or both of the storage devices and other storage devices not shown . The link may in various embodiments be a direct link and or a network link such as a network connection provided over the Internet and or over an area network. The storage device may include a first plurality of adapter units RAs . The RAs may be coupled to the link and be similar to the I O Module IOM but are used to transfer data between the storage devices .

The storage device may include one or more disks each containing a different portion of data stored on each of the storage device . shows the storage device including a plurality of disks . The storage device and or remote storage device may be provided as a stand alone device coupled to the host as shown in or alternatively the storage device and or remote storage device may be part of a storage area network SAN that includes a plurality of other storage devices as well as routers network connections etc. The storage devices may be coupled to a SAN fabric and or be part of a SAN fabric. The system described herein may be implemented using software hardware and or a combination of software and hardware where software may be stored in a computer readable medium and executed by one or more processors.

Each of the disks may be coupled to a corresponding disk adapter unit DA that provides data to a corresponding one of the disks and receives data from a corresponding one of the disks . An internal data path exists between the DAs the IOM and the RAs of the storage device . Note that in other embodiments it is possible for more than one disk to be serviced by a DA and that it is possible for more than one DA to service a disk. The storage device may also include a global memory that may be used to facilitate data transferred between the DAs the IOM and the RAs . The memory may contain tasks that are to be performed by one or more of the DAs the IOM and the RAs and a cache for data fetched from one or more of the disks 

The storage space in the storage device that corresponds to the disks may be subdivided into a plurality of volumes or logical devices. The logical devices may or may not correspond to the physical storage space of the disks . Thus for example the disk may contain a plurality of logical devices or alternatively a single logical device could span both of the disks . Similarly the storage space for the remote storage device that may comprise disks like that of the disks may be subdivided into a plurality of volumes or logical devices where each of the logical devices may or may not correspond to one or more of the disks.

In some embodiments one or more of the directors may have multiple processor systems thereon and thus may be able to perform functions for multiple directors. In some embodiments at least one of the directors having multiple processor systems thereon may simultaneously perform the functions of at least two different types of directors e.g. an IOM and a DA . Furthermore in some embodiments at least one of the directors having multiple processor systems thereon may simultaneously perform the functions of at least one type of director and perform other processing with the other processing system. In addition all or at least part of the global memory may be provided on one or more of the directors and shared with other ones of the directors . In an embodiment the features discussed in connection with the storage device may be provided as one or more director boards having CPUs memory e.g. DRAM etc. and interfaces with I O modules and in which multiple director boards may be networked together via a communications network such as for example an internal Ethernet communications network a serial rapid I O SRIO fabric and or Infiniband fabric v3 .

An instance is a single binary image of the OS that performs a specific set of operations. In an embodiment there may be up to eight instances configured on a director board at any given time. A thread is a separately schedulable set of code or process of an instance. Threads may be co operative and or preemptive and may be scheduled by the OS. An instance may run on more than one core that is an instance may provide a symmetric multiprocessing SMP environment to threads running within the instance.

A thread may be provided that runs as a hypervisor within the storage system OS environment. As previously discussed a hypervisor is a software implementation providing a software virtualization environment in which other software may run with the appearance of having full access to the underlying system hardware but in which such access is actually under the complete control of the hypervisor. The hypervisor running as the OS thread may be called a container hypervisor. The container hypervisor may manage a virtual hardware environment for a guest operating system Guest OS and in an embodiment the container hypervisor may run multiple OS threads e.g. 1 to N threads within a single instance. The Guest OS is an operating system that may be loaded by a thread of the container hypervisor and runs in the virtual environment provided by the container hypervisor. The Guest OS may also access real hardware devices attached to a director board using a virtual device provided by the container hypervisor or via a peripheral component interconnect PCI pass through device driver. There may be multiple container hypervisors running within a single instance at the same time. There may also be multiple container hypervisors running within different instances on the same director board at the same time.

In a hypervisor layer is shown as including hypervisor A and hypervisor B that may be examples of container hypervisors in accordance with the system described herein. Each of the container hypervisors may run as threads embedded within the storage system OS operating environment the storage system OS . The container hypervisor is shown running as a thread tand may be running independently of the container hypervisor . The container hypervisor is shown running two threads tand t. These threads may run independently of each other as well as the thread tof the container hypervisor . The independent operation of the threads tand tof the container hypervisor is shown schematically with a dashed line. In each case the threads t tand tof the container hypervisors may run as threads of one or more instances of the storage system OS . For example in an embodiment the container hypervisors may be threads running as part of an Enginuity instance or a Linux instance. The container hypervisors may be scheduled like any other thread and may be preempted and interrupted as well as started and stopped. Advantageously since the container hypervisors runs as threads within the storage system OS environment physical resource sharing of the underlying hardware is already provided for according to the storage system OS scheduling.

A Guest OS may be loaded using the thread tof the container hypervisor A and for example may run an application in the virtual environment provided thereby. As shown a Guest OS may be loaded using independent threads t tof the container hypervisor . As further discussed elsewhere herein threads t tand tmay all be run independently of each other. The ability to run a container hypervisor as a storage system OS thread provides that the storage system may run with no performance penalty until the container hypervisor thread is enabled. Even when the hypervisor thread is enabled and running an application in a Guest OS the performance impact may be controlled. Additionally developments in physical hardware may be accommodated through a software development process that is decoupled from modifications to the hypervisor code. Accordingly releases of new storage device code hypervisor code and Guest OS and applications code may all be realized in an independent manner.

In various embodiments the container hypervisors may each provide for one or more of the following features boot a Guest OS run the Guest OS as a storage system OS thread e.g. Symm K be scheduled preemptable etc. reset the Guest OS without restarting the instance allow the Guest OS to access storage devices e.g. Symmetrix using a Cut through Device CTD as further discussed elsewhere herein and allow the Guest OS to access the I O Modules IOMs using a PCI pass through device.

According to the system described herein when the container hypervisor starts the Guest OS the Guest OS may run in the context of the container hypervisor. The container hypervisor may access all of the Guest s memory while the Guest may only access the memory given to it by the container hypervisor. In order to avoid time consuming calls that cause an exit from a VM e.g. vmexit as a result of certain Guest OS activities virtual PCI devices may be used in connection with the container hypervisor. A virtual PCI device looks and behaves like normal PCI hardware to the Guest OS. Guest OS access to memory mapped I O MMIO space does not necessarily cause a vmexit depending on the virtual PCI device code of the container hypervisor. To allow I O with the storage system e.g. Symmetrix a Cut through Device CTD may be used that may be a virtual PCI device used in connection with the container hypervisor.

According to another embodiment by using a thread of a container hypervisor in the storage system OS environment e.g. Enginuity running Symm K it is possible for a Guest OS to operate in several modes. The container hypervisor thread may inherit the same number of CPU cores as that of the OS instance and may run as a single thread on those cores when active. However since the container hypervisor is running as a thread rather than being scheduled as an OS instance as described elsewhere herein other OS threads may also continue to run on other cores in the same SMP environment. The use of the OS scheduling algorithms e.g. Symm K for scheduling the threads of the container hypervisors thus provide the ability to schedule fractions of CPU time on multiple cores for the Guest OSs. Furthermore it is possible for the container hypervisor to allocate fewer virtual cores than physical cores available to the instance and allow the Guest OS to operate SMP on those cores while still allowing other OS threads to operate with full CPU core resources and to adjust the CPU allocation between Guest OSs and other threads. In an embodiment in a VMAX system from EMC Corporation of Hopkinton Mass. the granularity of the CPU time scheduling according to the system described herein may be on the order of 500 microseconds or less.

The scheduling of fractional CPU time on the physical CPU cores is shown schematically as fractions and of each of the CPU cores . Each of the threads t t and tof the container hypervisors may operate in an SMP regime on multiple ones of the cores while allowing others of the threads to also operate with full CPU core resources. The system described herein provides for flexible control of physical CPU allocation between Guest OSs without causing one or more of the Guest OSs to become inactive due to resource overlaps. In this way the Guest OSs may run based on the threads of the container hypervisors using varying amounts of CPU time per CPU core in an SMP regime. The system described herein may further provide for the use of global memories of the hardware layer that may be accessed via the virtual CPUs mapped by the container hypervisors to the physical CPU cores to provide NVRAM capabilities as further discussed elsewhere herein for example to provide one or more journaling devices.

After the step processing proceeds to a step where the first and second container hypervisors may share resources according to fractional resource sharing scheduled by the scheduler Symm K of the storage system OS and in connection with separate resource requirements of the first and second Guest OSs and or an application of the first and second Guest OSs . It is noted that in various embodiments the fractional resource scheduling depicted in illustration may be implemented according to systems like that shown in . After the step processing is complete. One or more of the above noted processing steps may be implemented via executable code stored on a non transitory computer readable medium and executable by at least one processor according to an embodiment of the system described herein.

According to the system described herein it has been found that by using global memory features such as features of the global memory of one or more storage devices e.g. EMC Symmetrix devices see e.g. the global memory of the storage system may be advantageously used to provide NVRAM capabilities. The non volatility of NVRAM i.e. that retains its information when power is turned off provides that an NVRAM device provided using the global memory may be used as a journaling device to track storage operations and facilitate recovery and or failover processing in a storage system without needing to add additional hardware and or other installed devices. Use of the global memory according to the system described herein to provide an NVRAM device functioning as a journaling device provides for the speeding up of transactions thereby improving metadata intensive operations performance and reducing recovery time and or failover time of a storage system. Additionally by adding NVRAM capabilities using the global memory of the storage device s Guest OSs accessing the storage system are provided with an NVRAM device with journaling device capability without adding additional hardware support. Furthermore according to the system described herein NVRAM provided using the global memory may be accessed by multipath processing enabling the channeling of NVRAM to different nodes without adding additional hardware support.

In an embodiment the Guest OSs and may be loaded using one or more of the container hypervisors via one or more of the threads t0 t1 t2 in a manner like that discussed in detail elsewhere herein. It is noted that in other embodiments other mechanisms may be used to load Guest OSs other than the use of container hypervisors and the system described herein involving the use of global memory to provide NVRAM capabilities may similarly be used in connection with such other embodiments. The global memory of the hardware layer may be distributed across multiple storage devices such as the memory of the storage device and similarly the storage device in . The distributed nature of the global memory is shown schematically by the illustration of memories forming the global memory . By using the global memory of the storage system to provide NVRAM capabilities to the Guest OSs accessing the storage system the system described herein provides Guest OSs with NVRAM capabilities without adding additional hardware support.

In the illustrated example the container hypervisors map virtual CPU cores to the physical CPU cores of the hardware layer . Through the use of the container hypervisors running as storage system OS threads t tand t the system described herein provides for the ability to schedule processing CPU time on multiple cores for one or more of the Guest OSs according to the scheduling algorithms of the storage system OS components e.g. Symm K . As further discussed elsewhere herein the scheduling of processing time on the multiple cores may be on a fractional basis.

The scheduling of processing time on the physical CPU cores is shown schematically as fractional portions and of each of the CPU cores . For example each of the threads t t and tof the container hypervisors may operate in an SMP regime on multiple ones of the cores while allowing others of the threads to also operate with full CPU core resources. The system described herein provides for flexible control of physical CPU allocation between Guest OSs without causing one or more of the Guest OSs to become inactive due to resource overlaps. In this way the Guest OSs may run based on the threads of the container hypervisors using varying amounts of CPU time per CPU core in an SMP regime.

The storage system may provide for the use of a global memory of the hardware layer that may be accessed via the virtual CPUs mapped by the container hypervisors to the physical CPU cores to provide NVRAM capabilities that enables use of journaling to track and log storage operations and provide for failover and recovery processing in the storage system . The global memory may be distributed across the multiple storage devices of the storage system which is shown schematically by the global memory portions . The Guest OSs that are loaded onto the storage system may be provided with NVRAM capabilities through the use of the global memory and that for example enable the journaling functions. As illustrated the CPU cores of the hardware layer may control the writing and reading of journaling records and data to and from the global memory as NVRAM in connection with journaling functions being provided independently to each of the Guest OSs that are loaded onto and accessing the storage system . The NVRAM provided by the global memory may be access by multipathing techniques as discussed elsewhere herein in which more than one path may be provided between any one or more CPUs and the global memory providing NVRAM capabilities.

In an embodiment according to the system described herein the resources accessed in the step in connection with the requirements of the Guest OS may include global memory provided by one or more distributed storage device memories of the storage system in which the global memory acts to provide NVRAM capabilities as discussed in detail elsewhere herein. Further processing resources may be used to provide journaling features in connection with the use of the global memory as NVRAM to thereby provide a journaling device that is used by the Guest OS in connection with the tracking and logging of storage operations that may be used to provide failover and or recovery processing without adding additional hardware support for the Guest OS operations. Accordingly after the step processing proceeds to a step where one or more journaling operations and or other appropriate operations are performed using the NVRAM capabilities provided by the use of the global memory according to the system described herein. In various embodiments journaling operations may include allocating storage space for journal entries concerning storage operations and processes writing and or time stamping of journal entries controlling mapping operations with respect to the mapping of journal data to storage space and or reading of journal entries in connection with recovery and failover processes among other possible journaling operations. After the step processing is complete. One or more of the above noted processing steps may be implemented via executable code stored on a non transitory computer readable medium and executable by at least one processor according to an embodiment of the system described herein.

Various embodiments discussed herein may be combined with each other in appropriate combinations in connection with the system described herein. Additionally in some instances the order of steps in the flowcharts flow diagrams and or described flow processing may be modified where appropriate. Further various aspects of the system described herein may be implemented using software hardware a combination of software and hardware and or other computer implemented modules or devices having the described features and performing the described functions. Software implementations of the system described herein may include executable code that is stored in a computer readable medium and executed by one or more processors. The computer readable medium may include non volatile and or volatile memory and examples may include a computer hard drive ROM RAM flash memory portable computer storage media such as a CD ROM a DVD ROM a flash drive and or other drive with for example a universal serial bus USB interface and or any other appropriate tangible or non transitory computer readable medium or computer memory on which executable code may be stored and executed by a processor. The system described herein may be used in connection with any appropriate operating system.

Other embodiments of the invention will be apparent to those skilled in the art from a consideration of the specification or practice of the invention disclosed herein. It is intended that the specification and examples be considered as exemplary only with the true scope and spirit of the invention being indicated by the following claims.

