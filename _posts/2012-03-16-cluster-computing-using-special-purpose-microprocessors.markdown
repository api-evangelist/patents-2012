---

title: Cluster computing using special purpose microprocessors
abstract: In some embodiments, a computer cluster system comprises a plurality of nodes and a software package comprising a user interface and a kernel for interpreting program code instructions. In certain embodiments, a cluster node module is configured to communicate with the kernel and other cluster node modules. The cluster node module can accept instructions from the user interface and can interpret at least some of the instructions such that several cluster node modules in communication with one another and with a kernel can act as a computer cluster.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08676877&OS=08676877&RS=08676877
owner: Advanced Cluster Systems, Inc.
number: 08676877
owner_city: Aliso Viejo
owner_country: US
publication_date: 20120316
---
This application is a continuation of U.S. patent application Ser. No. 12 040 519 filed Feb. 29 2008 now U.S. Pat. No. 8 140 612 which is a continuation in part of U.S. patent application Ser. No. 11 744 461 filed May 4 2007 now U.S. Pat. No. 8 082 289 which claims the benefit under 35 U.S.C. 119 e of U.S. Provisional Patent Application No. 60 813 738 filed Jun. 13 2006 and U.S. Provisional Patent Application No. 60 850 908 filed Oct. 11 2006. The entire contents of each of the above referenced applications are incorporated by reference herein and made a part of this specification.

The present disclosure relates to the field of cluster computing generally and to systems and methods for adding cluster computing functionality to a computer program in particular.

Computer clusters include a group of two or more computers microprocessors and or processor cores nodes that intercommunicate so that the nodes can accomplish a task as though they were a single computer. Many computer application programs are not currently designed to benefit from advantages that computer clusters can offer even though they may be running on a group of nodes that could act as a cluster. Some computer programs can run on only a single node because for example they are coded to perform tasks serially or because they are designed to recognize or send instructions to only a single node.

Some application programs include an interpreter that executes instructions provided to the program by a user a script or another source. Such an interpreter is sometimes called a kernel because for example the interpreter can manage at least some hardware resources of a computer system and or can manage communications between those resources and software for example the provided instructions which can include a high level programming language . Some software programs include a kernel that is designed to communicate with a single node. An example of a software package that includes a kernel that is designed to communicate with a single node is Mathematica from Wolfram Research Inc. Mathematica . Mathematics software packages from other vendors and other types of software can also include such a kernel.

A product known as gridMathematica also from Wolfram Research Inc. gives Mathematica the capability to perform a form of grid computing known as distributed computing. Grid computers include a plurality of nodes that generally do not communicate with one another as peers. Distributed computing can be optimized for workloads that consist of many independent jobs or packets of work which do not need to share data between the jobs during the computational process. Grid computers include at least one node known as a master node that manages a plurality of slave nodes or computational nodes. In gridMathematica each of a plurality of kernels runs on a single node. One kernel is designated the master kernel which handles all input output and scheduling of the other kernels the computational kernels or slave kernels . Computational kernels receive commands and data only from the node running the master kernel. Each computational kernel performs its work independently of the other computational kernels and intermediate results of one job do not affect other jobs in progress on other nodes.

Embodiments described herein have several features no single one of which is solely responsible for their desirable attributes. Without limiting the scope of the invention as expressed by the claims some of the advantageous features will now be discussed briefly.

Some embodiments described herein provide techniques for conveniently adding cluster computing functionality to a computer application. In one embodiment a user of a software package may be able to achieve higher performance and or higher availability from the software package by enabling the software to benefit from a plurality of nodes in a cluster. One embodiment allows a user to create applications using a high level language such as Mathematica that are able to run on a computer cluster having supercomputer like performance. One embodiment provides access to such high performance computing through a Mathematica Front End a command line interface one or more high level commands or a programming language such as C or FORTRAN.

One embodiment adapts a software module designed to run on a single node such as for example the Mathematica kernel to support cluster computing even when the software module is not designed to provide such support. One embodiment provides parallelization for an application program even if no access to the program s source code is available. One embodiment adds and supports Message Passing Interface MPI calls directly from within a user interface such as for example the Mathematica programming environment. In one embodiment MPI calls are added to or made available from an interactive programming environment such as the Mathematica Front End.

One embodiment provides a computer cluster including a first processor a second processor and a third processor. The cluster includes at least one computer readable medium in communication at least one of the first processor the second processor or the third processor. A first kernel resides in the at least one computer readable medium and is configured to translate commands into code for execution on the first processor. A first cluster node module resides in the at least one computer readable medium. The first cluster node module is configured to send commands to the first kernel and receives commands from a user interface. A second kernel resides in the at least one computer readable medium. The second kernel is configured to translate commands into code for execution on the second processor. A second cluster node module resides in the at least one computer readable medium. The second cluster node module is configured to send commands to the second kernel and communicates with the first cluster node module. A third kernel resides in the at least one computer readable medium. The third kernel is configured to translate commands into code for execution on the third processor. A third cluster node module resides in the at least one computer readable medium. The third cluster node module is configured to send commands to the third kernel and configured to communicate with the first cluster node module and the second cluster node module. The first cluster node module comprises a data structure in which messages originating from the second and third cluster node modules are stored.

Another embodiment provides a computer cluster that includes a plurality of nodes and a software package including a user interface and a single node kernel for interpreting program code instructions. A cluster node module is configured to communicate with the single node kernel and other cluster node modules. The cluster node module accepts instructions from the user interface and interprets at least some of the instructions such that several cluster node modules in communication with one another act as a cluster. The cluster node module appears as a single node kernel to the user interface. In one embodiment the single node kernel includes a Mathematical kernel. In some embodiments the user interface can include at least one of a Mathematica front end or a command line. In some embodiments the cluster node module includes a toolkit including library calls that implement at least a portion of MPI calls. In some embodiments the cluster node module includes a toolkit including high level cluster computing commands. In one embodiment the cluster system can include a plurality of Macintosh computers Macs Windows based personal computers PCs and or Unix Linux based workstations.

A further embodiment provides a computer cluster including a plurality of nodes. Each node is configured to access a computer readable medium comprising program code for a user interface and program code for a single node kernel module configured to interpret user instructions. The cluster includes a plurality of cluster node modules. Each cluster node module is configured to communicate with a single node kernel and with one or more other cluster node modules to accept instructions from the user interface and to interpret at least some of the user instructions such that the plurality of cluster node modules communicate with one another in order to act as a cluster. A communications network connects the nodes. One of the plurality of cluster node modules returns a result to the user interface.

Another embodiment provides a method of evaluating a command on a computer cluster. A command from at least one of a user interface or a script is communicated to one or more cluster node modules within the computer cluster. Each of the one or more cluster node modules communicates a message based on the command to a respective kernel module associated with the cluster node module. Each of the one or more cluster node modules receives a result from the respective kernel module associated with the cluster node module. At least one of the one or more cluster node modules responds to messages from other cluster node modules.

Another embodiment provides a computing system for executing Mathematica code on multiple nodes. The computing system includes a first node module in communication with a first Mathematica kernel executing on a first node a second node module in communication with a second Mathematica kernel executing on a second node and a third node module in communication with a third Mathematica kernel executing on a third node. The first node module the second node module and the third node module are configured to communicate with one another using a peer to peer architecture. In some embodiments each of the first node module the second node module and third node module includes a data structure for maintaining messages originating from other node modules and a data structure for maintaining data specifying a location to which an message is expected to be received and an identifier for a node from which the message is expected to be sent.

For purposes of illustration some embodiments are described herein in the context of cluster computing with Mathematica software. The present disclosure is not limited to a single software program the systems and methods can be used with other application software such as for example Maple MATLAB MathCAD Apple Shake Apple Compressor IDL other applications employing an interpreter or a kernel Microsoft Excel Adobe After Effects Adobe Premiere Adobe Photoshop Apple Final Cut Pro and Apple iMovie . Some figures and or descriptions however relate to embodiments of computer clusters running Mathematica. The system can include a variety of uses including but not limited to students educators scientists engineers mathematicians researchers and technicians. It is also recognized that in other embodiments the systems and methods can be implemented as a single module and or implemented in conjunction with a variety of other modules. Moreover the specific implementations described herein are set forth in order to illustrate and not to limit the disclosure.

The cluster computing system described herein generally includes one or more computer systems connected to one another via a communications network or networks. The communications network can include one or more of a local area network LAN a wide area network WAN an intranet the Internet etc. In one embodiment a computer system comprises one or more processors such as for example a microprocessor that can include one or more processing cores nodes . The term node refers to a processing unit or subunit that is capable of single threaded execution of code. The processors can be connected to one or more memory devices such as for example random access memory RAM and or one or more optional storage devices such as for example a hard disk. Communications among the processors and such other devices may occur for example via one or more local buses of a computer system or via a LAN a WAN a storage area network SAN and or any other communications network capable of carrying signals among computer system components. In one embodiment one or more software modules such as kernels run on nodes within the interconnected computer systems. In one embodiment the kernels are designed to run on only a single node. In one embodiment cluster node modules communicate with the kernels and with each other in order to implement cluster computing functionality.

In the embodiment shown in each of the kernel modules is in communication with a single cluster node module respectively. For example the kernel module is in communication with the cluster node module the kernel module is in communication with the cluster node module and so forth. In one embodiment one instance of a cluster node module is loaded into a computer system s memory for every instance of a kernel module running on the system. As shown in each of the cluster node modules is in communication with each of the other cluster node modules . For example one cluster node module is in communication with all of the other cluster node modules . A cluster node module may communicate with another cluster node module via a local bus not shown when for example both cluster node modules execute on processors within the same computer system . A cluster node module may also communicate with another cluster node module over a communications network when for example the cluster node modules execute on processors within different computer systems .

As shown in an optional user interface module such as for example a Mathematica front end and or a command line interface can connect to a cluster node module . The user interface module can run on the same computer system and or the same microprocessor on which the cluster node module runs. The cluster node modules provide MPI calls and or advanced cluster functions that implement cluster computing capability for the single threaded kernel modules. The cluster node modules are configured to look and behave like a kernel module from the perspective of the user interface module . Similarly the cluster node modules are configured to look and behave like a user interface module from the perspective of a kernel module . The first cluster node module is in communication with one or more other cluster node modules and so forth each of which provides a set of MPI calls and or advanced cluster commands. In one embodiment MPI may be used to send messages between nodes in a computer cluster.

Communications can occur between any two or more cluster node modules for example between a cluster node module and another cluster node module and not just between adjacent kernels. Each of the cluster node modules is in communication with respective kernel modules . Thus the cluster node module communicates with the kernel module . MPI calls and advanced cluster commands are used to parallelize program code received from an optional user interface module and distribute tasks among the kernel modules . The cluster node modules provide communications among kernel modules while the tasks are executing. Results of evaluations performed by kernel modules are communicated back to the first cluster node module via the cluster node modules which communicates them to the user interface module .

Intercommunication among kernel modules during thread execution which is made possible by cluster node modules provides advantages for addressing various types of mathematic and scientific problems for example. Intercommunication provided by cluster computing permits exchange of information between nodes during the course of a parallel computation. Embodiments of the present disclosure provide such intercommunication for software programs such as Mathematica while grid computing solutions can implement communication between only one master node and many slave nodes. Grid computing does not provide for communication between slave nodes during thread execution.

For purposes of providing an overview of some embodiments certain aspects advantages benefits and novel features of the invention are described herein. It is to be understood that not necessarily all such advantages or benefits can be achieved in accordance with any particular embodiment of the invention. Thus for example those skilled in the art will recognize that the invention can be embodied or carried out in a manner that achieves one advantage or group of advantages as taught herein without necessarily achieving other advantages or benefits as can be taught or suggested herein.

As shown in one embodiment of a cluster system includes computer systems in communication with one another via a communications network . A first computer system can include one or more processors a memory device and an optional storage device . Similarly a second computer system can include one or more processors a memory device and an optional storage device . Likewise a third computer system can include one or more processors a memory device and an optional storage device . Each of the computer systems includes a network interface not shown for connecting to a communications network which can include one or more of a LAN a WAN an intranet a wireless network and or the Internet.

In one embodiment a first computer system communicates with other computer systems via a network as part of a computer cluster . In one embodiment the computer system is a personal computer a workstation a server or a blade including one or more processors a memory device an optional storage device as well as a network interface module not shown for communications with the network .

In one embodiment the computer system includes one or more processors . The processors can be one or more general purpose single core or multi core microprocessors such as for example a Pentium processor a Pentium II processor a Pentium Pro processor a Pentium III processor Pentium 4 processor a Core Duo processor a Core 2 Duo processor a Xeon processor an Itanium processor a Pentium M processor an x86 processor an Athlon processor an 8051 processor a MIPS processor a PowerPC processor an ALPHA processor etc. In addition one or more of the processors can be a special purpose microprocessor such as a digital signal processor. The total number of processing cores for example processing units capable of single threaded execution within all processors in the computer system corresponds to the number of nodes available in the computer system . For example if the processors were each Core 2 Duo processors having two processing cores computer system would have four nodes in all. Each node can run one or more instances of a program module such as a single threaded kernel module.

The computer system can also include a network interface module not shown that facilitates communication between the computer system and other computer systems via the communications network .

The network interface module can use a variety of network protocols. In one embodiment the network interface module includes TCP IP. However it is to be appreciated that other types of network communication protocols such as for example Point to Point Protocol PPP Server Message Block SMB Serial Line Internet Protocol SLIP tunneling PPP AppleTalk etc. may also be used.

The computer system can include memory . Memory can include for example processor cache memory such as processor core specific or cache memory shared by multiple processor cores dynamic random access memory DRAM static random access memory SRAM or any other type of memory device capable of storing computer data instructions or program code. The computer system can also include optional storage . Storage can include for example one or more hard disk drives floppy disks flash memory magnetic storage media CD ROMs DVDs optical storage media or any other type of storage device capable of storing computer data instructions and program code.

The computer system may be used in connection with various operating systems such as Microsoft Windows 3.X Windows 95 Windows 98 Windows NT Windows 2000 Windows XP Windows CE Palm Pilot OS OS 2 Apple MacOS MacOS X MacOS X Server Disk Operating System DOS UNIX Linux VxWorks or IBM OS 2 Sun OS Solaris OS IRIX OS operating systems etc.

In one embodiment the computer system is a personal computer a laptop computer a Blackberry device a portable computing device a server a computer workstation a local area network of individual computers an interactive kiosk a personal digital assistant an interactive wireless communications device a handheld computer an embedded computing device or the like.

As can be appreciated by one of ordinary skill in the art the computer system may include various sub routines procedures definitional statements and macros. Each of the foregoing modules are typically separately compiled and linked into a single executable program. However it is to be appreciated by one of ordinary skill in the art that the processes that are performed by selected ones of the modules may be arbitrarily redistributed to one of the other modules combined together in a single module made available in a shareable dynamic link library or partitioned in any other logical way.

In one embodiment a second computer system communicates with other computer systems via a network as part of a computer cluster . In one embodiment the computer system is a personal computer a workstation a server or a blade including one or more processors a memory device an optional storage device as well as a network interface module not shown for communications with the network .

In one embodiment the computer system includes one or more processors . The processors can be one or more general purpose single core or multi core microprocessors such as a Pentium processor a Pentium II processor a Pentium Pro processor a Pentium III processor Pentium 4 processor a Core Duo processor a Core 2 Duo processor a Xeon processor an Itanium processor a Pentium M processor an x86 processor an Athlon processor an 8051 processor a MIPS processor a PowerPC processor an ALPHA processor etc. In addition the processors can be any special purpose microprocessors such as a digital signal processor. The total number of processing cores for example processing units capable of single threaded execution within all processors in the computer system corresponds to the number of nodes available in the computer system . For example if the processors were each Core 2 Duo processors having two processing cores computer system would have four nodes in all. Each node can run one or more instances of a program module such as a single threaded kernel module.

The computer system can also include a network interface module not shown that facilitates communication between the computer system and other computer systems via the communications network .

The network interface module can use a variety of network protocols. In one embodiment the network interface module includes TCP IP. However it is to be appreciated that other types of network communication protocols such as for example Point to Point Protocol PPP Server Message Block SMB Serial Line Internet Protocol SLIP tunneling PPP AppleTalk etc. may also be used.

The computer system can include memory . Memory can include for example processor cache memory such as processor core specific or cache memory shared by multiple processor cores dynamic random access memory DRAM static random access memory SRAM or any other type of memory device capable of storing computer data instructions or program code. The computer system can also include optional storage . Storage can include for example one or more hard disk drives floppy disks flash memory magnetic storage media CD ROMs DVDs optical storage media or any other type of storage device capable of storing computer data instructions and program code.

The computer system may be used in connection with various operating systems such as Microsoft Windows 3.X Windows 95 Windows 98 Windows NT Windows 2000 Windows XP Windows CE Palm Pilot OS OS 2 Apple MacOS MacOS X MacOS X Server Disk Operating System DOS UNIX Linux VxWorks or IBM OS 2 Sun OS Solaris OS IRIX OS operating systems etc.

In one embodiment the computer system is a personal computer a laptop computer a Blackberry device a portable computing device a server a computer workstation a local area network of individual computers an interactive kiosk a personal digital assistant an interactive wireless communications device a handheld computer an embedded computing device or the like.

As can be appreciated by one of ordinary skill in the art the computer system may include various sub routines procedures definitional statements and macros. Each of the foregoing modules are typically separately compiled and linked into a single executable program. However it is to be appreciated by one of ordinary skill in the art that the processes that are performed by selected ones of the modules may be arbitrarily redistributed to one of the other modules combined together in a single module made available in a shareable dynamic link library or partitioned in any other logical way.

In one embodiment a third computer system communicates with other computer systems via a network as part of a computer cluster . In one embodiment the computer system is a personal computer a workstation a server or a blade including one or more processors a memory device an optional storage device as well as a network interface module not shown for communications with the network .

In one embodiment the computer system includes a processor . The processor can be a general purpose single core or multi core microprocessors such as a Pentium processor a Pentium II processor a Pentium Pro processor a Pentium III processor Pentium 4 processor a Core Duo processor a Core 2 Duo processor a Xeon processor an Itanium processor a Pentium M processor an x86 processor an Athlon processor an 8051 processor a MIPS processor a PowerPC processor or an ALPHA processor. In addition the processor can be any special purpose microprocessor such as a digital signal processor. The total number of processing cores for example processing units capable of single threaded execution within processor in the computer system corresponds to the number of nodes available in the computer system . For example if the processor was a Core 2 Duo processor having two processing cores the computer system would have two nodes. Each node can run one or more instances of a program module such as a single threaded kernel module.

The computer system can also include a network interface module not shown that facilitates communication between the computer system and other computer systems via the communications network .

The network interface module can use a variety of network protocols. In one embodiment the network interface module includes TCP IP. However it is to be appreciated that other types of network communication protocols such as for example Point to Point Protocol PPP Server Message Block SMB Serial Line Internet Protocol SLIP tunneling PPP AppleTalk etc. may also be used.

The computer system can include memory . Memory can include for example processor cache memory such as processor core specific or cache memory shared by multiple processor cores dynamic random access memory DRAM static random access memory SRAM or any other type of memory device capable of storing computer data instructions or program code. The computer system can also include optional storage . Storage can include for example one or more hard disk drives floppy disks flash memory magnetic storage media CD ROMs DVDs optical storage media or any other type of storage device capable of storing computer data instructions and program code.

The computer system may be used in connection with various operating systems such as Microsoft Windows 3.X Windows 95 Windows 98 Windows NT Windows 2000 Windows XP Windows CE Palm Pilot OS OS 2 Apple MacOS MacOS X MacOS X Server Disk Operating System DOS UNIX Linux VxWorks or IBM OS 2 Sun OS Solaris OS IRIX OS operating systems etc.

In one embodiment the computer system is a personal computer a laptop computer a Blackberry device a portable computing device a server a computer workstation a local area network of individual computers an interactive kiosk a personal digital assistant an interactive wireless communications device a handheld computer an embedded computing device or the like.

As can be appreciated by one of ordinary skill in the art the computer system may include various sub routines procedures definitional statements and macros. Each of the foregoing modules are typically separately compiled and linked into a single executable program. However it is to be appreciated by one of ordinary skill in the art that the processes that are performed by selected ones of the modules may be arbitrarily redistributed to one of the other modules combined together in a single module made available in a shareable dynamic link library or partitioned in any other logical way.

In one embodiment computer systems are in communication with one another via a communications network .

The communications network may include one or more of any type of electronically connected group of computers including for instance the following networks a virtual private network a public Internet a private Internet a secure Internet a private network a public network a value added network a wired network a wireless network an intranet etc. In addition the connectivity to the network can be for example a modem Ethernet IEEE 802.3 Gigabit Ethernet 10 Gigabit Ethernet Token Ring IEEE 802.5 Fiber Distributed Datalink Interface FDDI Frame Relay InfiniBand Myrinet Asynchronous Transfer Mode ATM or another interface. The communications network may connect to the computer systems for example by use of a modem or by use of a network interface card that resides in each of the systems.

In addition the same or different communications networks may be used to facilitate communication between the first computer system and the second computer system between the first computer system and the third computer system and between the second computer system and the third computer system .

As shown in one embodiment of a cluster system includes a user interface module that is able to access a plurality of kernel modules by communicating with a first cluster node module . User interface module can be stored in a memory while running for example and or can be stored in a storage device . The first cluster node module is in communication with each of the other cluster node modules . The kernel modules can reside in the memory of one or more computer systems on which they run. For example the memory of the first computer system can store instances of kernel modules the memory of the second computer system can store instances of kernel modules and the memory of the third computer system can store an instance of kernel module . The kernel modules which include single threaded program code are each associated with one of the processors . A cluster configuration module stored on one or more of the computer systems or on a remote computer system for example can establish communication with the cluster node modules . In one embodiment communication between the cluster configuration module and the cluster node modules initializes the cluster node modules to provide cluster computing support for the computer cluster .

In one embodiment the cluster node modules provide a way for many kernel modules such as for example Mathematica kernels running on a computer cluster to communicate with one another. A cluster node module can include at least a portion of an application programming interface API known as the Message Passing Interface MPI which is used in several supercomputer and cluster installations. A network of connections for example the arrows shown in between the cluster node modules can be implemented using a communications network such as for example TCP IP over Ethernet but the connections could also occur over any other type of network or local computer bus.

A cluster node module can use an application specific toolkit or interface such as for example Mathematica s MathLink Add Ons or packets to interact with an application. Normally used to connect a Mathematica kernel to a user interface known as the Mathematica Front End or other Mathematica kernels MathLink is a bidirectional protocol to sends packets containing messages commands or data between any of these entities. MathLink does not allow direct cluster computing like simultaneous communication between Mathematica kernels during execution of a command or thread. MathLink is also not designed to perform multiple simultaneous network connections. In some embodiments a cluster node module can use an application specific toolkit such as for example MathLink for connections between entities on the same computer.

When speaking about procedures or actions on a cluster or other parallel computer not all actions happen in sequential order nor are they required to. For example a parallel code as opposed to a single processor code of the classic Turing machine model has multiple copies of the parallel code running across the cluster typically one for each processor or processing element or core . Such parallel code is written in such a way that different instances of the same code can communicate collaborate and coordinate work with each other. Multiple instances of these codes can run at the same time in parallel.

If the count of the code instances is an integer N each instance of code execution can be labeled 0 through N 1. For example a computer cluster can include N connected computers each containing a processor. The first has cluster node module 0 connected with kernel module 0 running on processor 0. The next is cluster node module 1 and kernel module 1 on processor 1 and so forth for each of the N connected computers. Some steps of their procedure are collaborative and some steps are independent. Even though these entities are not necessarily in lock step they do follow a pattern of initialization main loop behavior for example cluster node module operation and shut down.

In contrast a parallel computing toolkit PCT that is provided as part of the gridMathematica software package does not provide a means for instances of the same code running on different nodes to communicate collaborate or coordinate work among the instances. The PCT provides commands that connect Mathematica kernels in a master slave relationship rather than a peer to peer relationship as enabled by some embodiments disclosed herein. A computer cluster having peer to peer node architecture performs computations that can be more efficient easier to design and or more reliable than similar computations performed on grid computers having master slave node architecture. Moreover the nature of some computations may not allow a programmer to harness multi node processing power on systems that employ master slave node architecture.

In one embodiment the cluster node module includes an MPI module . The MPI module can include program code for one or more of at least five kinds of MPI instructions or calls. Selected constants instructions and or calls that can be implemented by the MPI module are as follows 

Node identifiers are used to send messages to nodes or receive messages from them. In MPI this is accomplished by assigning each node a unique integer IdProc starting with 0. This data with a knowledge of the total count NProc makes it possible to programmatically divide any measurable entity.

In one embodiment the MPI module can include basic MPI calls such as for example relatively low level routines that map MPI calls that are commonly used in other languages such as C and Fortran so that such calls can be available directly from the Mathematica user interface . In some embodiments basic MPI calls include calls that send data equations formulas and or other expressions.

Simply sending expressions from one node to another is possible with these most basic MPI calls. One node can call to send an expression while the other calls a corresponding routine to receive the sent expression. Because it is possible that the receiver has not yet called mpiRecv even if the message has left the sending node completion of mpiSend is not a confirmation that it has been received.

Asynchronous calls make it possible for the kernel to do work while communications are proceeding simultaneously. It is also possible that another node may not be able to send or receive data yet allowing one kernel to continue working while waiting.

The mpiISend command can be called from within a kernel module for example a Mathematica kernel . It creates a packet containing the Mathematica expression to be sent as payload and where the expression should be sent. The packet itself is destined only for its local cluster node module. Once received by its local cluster node module this packet is decoded and its payload is forwarded on to the cluster node module specified in the packet.

The mpiIRecv command can also be called from within a kernel module . It creates a packet specifying where it expects to receive an expression and from which processor this expression is expected. Once received by its local cluster node module this packet is decoded and its contents are stored in a message receiving queue MRQ .

The mpiTest command can be called from within a kernel module . It creates a packet specifying which message to test for completion then waits for a reply expression to evaluate. Once received by the kernel module s associated cluster node module this packet is decoded and its message specifier is used to search for any matching expressions listed as completed in its received message queue RMQ . If such completed expressions are found it is sent to its local kernel module as part of the reply in mpiTest . The kernel module receives this reply expression and evaluates it which updates the kernel module s variables as needed.

Other MPI calls are built on the fundamental calls mpiISend mpiIRecv and mpiTest. For example mpiBcast a broadcast creates instructions to send information from the broadcast processor to all the others while the other processors perform a Recv. Similarly high level calls of the toolkit can be built on top of the collection of MPI calls.

In one embodiment the MPI module can include program code for implementing collective MPI calls for example calls that provide basic multi node data movement across nodes . Collective MPI calls can include broadcasts gathers transpose and other vector and matrix operations for example. Collective calls can also provide commonly used mechanisms to send expressions between groups of nodes.

In one embodiment the MPI module includes program code for implementing parallel sums and other reduction operations on data stored across many nodes. MPI module can also include program code for implementing simple parallel input output calls for example calls that allow cluster system to load and store objects that are located on a plurality of nodes .

These additional collective calls perform operations that reduce the data in parallel. The operation argument can be one of the constants below.

In one embodiment the MPI module includes program code for implementing communicator world calls for example calls that would allow subsets of nodes to operate as if they were a sub cluster . Communicators organize groups of nodes into user defined subsets. The communicator values returned by mpiCommSplit can be used in other MPI calls instead of mpiCommWorld.

In one embodiment the cluster node module includes an advanced functions module . The advanced functions module can include program code that provides a toolkit of functions inconvenient or impractical to do with MPI instructions and calls implemented by the MPI module . The advanced functions module can rely at least partially on calls and instructions implemented by the MPI module in the implementation of advanced functions. In one embodiment the advanced functions module includes a custom set of directives or functions. In an alternative embodiment the advanced functions module intercepts normal Mathematica language and converts it to one or more functions optimized for cluster execution. Such an embodiment can be easier for users familiar with Mathematica functions to use but can also complicate a program debugging process. Some functions implemented by the advanced functions module can simplify operations difficult or complex to set up using parallel computing. Several examples of such functions that can be implemented by the advanced functions module are shown below.

Built on the MPI calls the calls that are described below provide commonly used communication patterns or parallel versions of Mathematica features. Unless otherwise specified these are executed in the communicator mpiCommWorld whose default is mpiCommWorld but can be changed to a valid communicator at run time.

In one embodiment the advanced functions module includes functions providing for basic parallelization such as for example routines that would perform the same operations on many data elements or inputs stored on many nodes. These functions can be compared to parallelized for loops and the like. The following calls address simple parallelization of common tasks. In the call descriptions expr refers to an expression and loopspec refers to a set of rules that determine how the expression is evaluated. In some embodiments the advanced functions module supports at least three forms of loopspec including var count where the call iterates the variable var from 1 to the integer count var start stop where the call iterates the variable var every integer from start to stop and var start stop increment where the call iterates the variable var from start adding increment for each iteration until var exceeds stop allowing var to be a non integer.

In one embodiment the advanced functions module includes functions providing for guard cell operations such as for example routines that perform nearest neighbor communications to maintain edges of local arrays in any number of dimensions optimized for 1 2 and or 3 D . Typically the space of a problem is divided into partitions. Often however neighboring edges of each partition can interact so a guard cell is inserted on both edges as a substitute for the neighboring data. Thus the space a processor sees is two elements wider than the actual space for which the processor is responsible. EdgeCell helps maintain these guard cells.

The advanced functions module can also include functions providing for linear algebra operations such as for example parallelized versions of basic linear algebra on structures partitioned on many nodes. Such linear algebra operations can reorganize data as needed to perform matrix and vector multiplication or other operations such as determinants trace and the like. Matrices are partitioned and stored in processors across the cluster. These calls manipulate these matrices in common ways.

In one embodiment the advanced functions module includes element management operations. For example a large bin of elements or particles cut up in space across the nodes may need to migrate from node to node based on rules or criteria such as their spatial coordinate . Such operations would migrate the data from one node to another. Besides the divide and conquer approach a list of elements can also be partitioned in arbitrary ways. This is useful if elements need to be organized or sorted onto multiple processors. For example particles of a system may drift out of the space of one processor into another so their data would need to be redistributed periodically.

In one embodiment the advanced functions module includes program code for implementing large scale parallel fast Fourier transforms FFTs . For example such functions can perform FFTs in one two and or three dimensions on large amounts of data that are not stored on one node and that are instead stored on many nodes. Fourier transforms of very large arrays can be difficult to manage not the least of which is the memory requirements. Parallelizing the Fourier transform makes it possible to make use of all the memory available on the entire cluster making it possible to manipulate problem sizes that no one processor could possibly do alone.

In one embodiment the advanced functions module includes parallel disk input and output calls. For example data may need to be read in and out of the cluster in such a way that the data is distributed across the cluster evenly. The calls in the following table enable the saving data from one or more processors to storage and the retrieval data from storage.

Some function calls can take an inconsistent amount of processing time to complete. For example in Mathematica the call f 20 could in general take much longer to evaluate than f 19 . Moreover if one or more processors within the cluster are of different speeds for example if some operate at a core frequency of 2.6 GHz while other operate at less than one 1 GHz one processor may finish a task sooner than another processor.

In some embodiments the advanced functions module includes a call that can improve the operation of the computer cluster in such situations. In some embodiments the root processor assigns a small subset of the possible calls for a function to each processor on the cluster . Whichever processor returns its results first is assigned a second small subset of the possible calls. The root processor will continue to assign small subsets of the possible calls as results are received until an evaluation is complete. The order in which the processors finish can vary every time an expression is evaluated but the root processor will continue assigning additional work to processors as they become available.

In one illustrative example there are 4 processors and f 1 to f 100 to evaluate. One could implement this by assigning f 1 f 2 f 3 f 4 to each of processors 0 the root can assign to oneself through 3. If the f 2 result came back first then processor 1 would be assigned f 5 . If the f 4 result is returned next f 6 would be assigned to processor 3. The assignments continue until all results are calculated. The results are organized for output back to the user.

In alternative embodiments the subsets of possible calls can be assigned in any order rather than sequentially or in batches for example f 1 f 5 f 9 assigned to processor etc. . Also the subsets could be organized by delegation. For example one processor node may not necessarily be in direct control of the other processors. Instead a large subset could be assigned to a processor which would in turn assign subsets of its work to other processors. The result would create a hierarchy of assignments like a vast army.

In one embodiment the cluster node module includes a received message queue . The received message queue includes a data structure for storing messages received from other cluster node modules. Related data pertaining to the messages received such as whether an expression has been completed may also be stored in the received message queue . The received message queue may include a queue and or another type of data structure such as for example a stack a linked list an array a tree etc.

In one embodiment the cluster node module includes a message receiving queue . The message receiving queue includes a data structure for storing information about the location to which an expression is expected to be sent and the processor from which the expression is expected. The message receiving queue may include a queue and or another type of data structure such as for example a stack a linked list an array a tree etc.

Cluster configuration module includes program code for initializing a plurality of cluster node modules to add cluster computing support to computer systems . U.S. Pat. No. 7 136 924 issued to Dauger the 924 patent the entirety of which is hereby incorporated by reference and made a part of this specification discloses a method and system for parallel operation and control of computer clusters. One method generally includes obtaining one or more personal computers having an operating system with discoverable network services. In some embodiments the method includes obtaining one or more processors or processor cores on which a kernel module can run. As described in the 924 patent a cluster node control and interface CNCI group of software applications is copied to each node. When the CNCI applications are running on a node the cluster configuration module can permit a cluster node module in combination with a kernel module to use the node s processing resources to perform a parallel computation task as part of a computer cluster. The cluster configuration module allows extensive automation of the cluster creation process in connection with the present disclosure.

In some embodiments computer cluster includes a user interface module such as for example a Mathematica Front End or a command line interface that includes program code for a kernel module to provide graphical output accept graphical input and provide other methods of user communication that a graphical user interface or a command line interface provides. To support a user interface module the behavior of a cluster node module is altered in some embodiments. Rather than sending output to and accepting input from the user directly the user interface module activates the cluster node module to which it is connected and specifies parameters to form a connection such as a MathLink connection between the cluster node module and the user interface module . The user interface module s activation of the cluster node module can initiate the execution of instructions to activate the remaining cluster node modules on the cluster and to complete the sequence to start all kernel modules on the cluster. Packets from the user interface module normally intended for a kernel module are accepted by the cluster node module as a user command. Output from the kernel module associated with the cluster node module can be forwarded back to the user interface module for display to a user. Any of the cluster node modules can be configured to communicate with a user interface module .

A kernel module typically includes program code for interpreting high level code commands and or instructions supplied by a user or a script into low level code such as for example machine language or assembly language. In one embodiment each cluster node module is connected to all other cluster node modules while each kernel module is allocated and connected only to one cluster node module . In one embodiment there is one cluster node module kernel module pair per processor. For example in an embodiment of a computer cluster including single processor computer systems each cluster node module kernel module pair could reside on a single processor computer. If a computer contains multiple processors or processing cores it may contain multiple cluster node module kernel module pairs but the pairs can still communicate over the cluster node module s network connections.

In one embodiment the computer cluster includes a cluster initialization process a method of cluster node module operation and a cluster shut down process.

In one embodiment a cluster configuration module initializes one or more cluster node modules in order to provide cluster computing support to one or more kernel modules as shown in .

At cluster node modules are launched on the computer cluster . In one embodiment the cluster node module running on a first processor for example where the user is located accesses the other processors on the computer cluster via the cluster configuration module to launch cluster node modules onto the entire cluster. In an alternative embodiment the cluster configuration module searches for processors connected to one another via communications network and launches cluster node modules on each of the processors .

The cluster node modules establish communication with one another at . In one embodiment each of the cluster node modules establish direct connections using the MPI Init command with other cluster node modules launched on the computer cluster by the cluster configuration module .

At each cluster node module attempts to connect to a kernel module . In one embodiment each instance of the cluster node modules locates launches and connects with a local kernel module via MathLink connections and or similar connection tools for example built into the kernel module .

At the cluster node modules that are unconnected to a kernel module are shut down. In one embodiment each cluster node module determines whether the local kernel module cannot be found or connected to. In one embodiment each cluster node module reports the failure to connect to a kernel module to the other cluster node modules on computer cluster and quits.

Processor identification numbers are assigned to the remaining cluster node modules at . In one embodiment each remaining cluster node module calculates the total number of active processors N and determines identification numbers describing the remaining subset of active cluster node modules and kernel modules . This new set of cluster node module kernel module pairs may be numbered 0 through N 1 for example.

Message passing support is initialized on the kernel modules at . In one embodiment each cluster node module supplies initialization code for example Mathematica initialization code to the local kernel module to support message passing.

Finally at the cluster node modules enter a loop to accept user entry. In one embodiment a main loop for example a cluster operation loop begins execution after the cluster node module on the first processor returns to user control while each of the other cluster node modules waits for messages from all other cluster node modules connected to the network .

The initialization process creates a structure enabling a way for the kernel modules to send messages to one another. In some embodiments any kernel module can send data to and receive data from any other kernel module within the cluster when initialization is complete. The cluster node module creates an illusion that a kernel module is communicating directly with the other kernel modules. The initialization process can create a relationship among kernel modules on a computer cluster such as the one shown by way of example in .

In one embodiment a cluster node module implements cluster computing support for a kernel module during a main loop as shown in .

At cluster node modules wait for user commands or messages from other cluster node modules. In one embodiment the cluster node module connected to the user interface module waits for a user command while the other cluster node modules continue checking for messages.

Once a command or message is received the method proceeds to . At the cluster node module determines whether the message received is a quit command. If a quit command is received the cluster node module exits the loop and proceeds to a cluster node module shut down process at . If the message received is not a quit command the process continues to .

At received commands are communicated to all cluster node modules on the computer cluster . In one embodiment when a user enters a command in the user interface module the cluster node module connected to the user interface module submits the user command to all other cluster node modules in the computer cluster . The user commands can be simple for example 1 1 but can also be entire subroutines and sequences of code such as for example Mathematica code including calls to MPI from within the user interface module for example the Mathematica Front End to perform message passing between kernel modules for example Mathematica kernels . These include the fundamental MPI calls which are implemented using specially identified messages between a cluster node module and its local kernel module .

The message or user command is communicated to the kernel modules at . In one embodiment the cluster node module connected to the user interface module submits the user command to the kernel module to which it is connected. Each of the other cluster node modules after receiving the message submits the command to the respective kernel module to which it is connected.

At a cluster node module receives a result from a kernel module . In one embodiment once the kernel module completes its evaluation it returns the kernel module s output to the cluster node module to which it is connected. Depending on the nature of the result from the kernel module the cluster node module can report the result to a local computer system or pass the result as a message to another cluster node module . For example the cluster node module running on the first processor reports the output on its local computer system . For example on the first processor cluster node module only directly reports the output of kernel module

Messages from other cluster node modules are responded to at . In one embodiment each cluster node module for example the cluster node module checks for and responds to messages from other cluster node modules and from the kernel module repeatedly until those are exhausted. In one embodiment output messages from the kernel module are forwarded to output on the local computer system. Messages from other cluster node modules are forwarded to a received message queue RMQ . Data from each entry in the message receiving queue MRQ is matched with entries in the RMQ see for example description of the mpiIRecv call above . If found data from the MRQ are combined into those in the RMQ and marked as completed see for example description of the mpiTest call above . This process provides the peer to peer behavior of the cluster node modules . Via this mechanism code running within multiple simultaneously running kernel modules for example Mathematica kernels can interact on a pair wise or collective basis performing calculations processing or other work on a scale larger and or faster than one kernel could have done alone. In this manner user entered instructions and data specifying what work will be done via user commands can be executed more quickly and or reliably. Once responding to messages has completed the process returns to .

In some embodiments a computer system includes software such as an operating system that divides memory and or other system resources into a user space a kernel space an application space for example a portion of the user space allocated to an application program and or an operating system space for example a portion of the user space allocated to an operating system . In some embodiments some or all of the cluster node modules are implemented in the application space of a computer system. In further embodiments at least some of the cluster node modules are implemented in the operating system space of a computer system. For example some cluster node modules in a computer cluster may operate in the application space while others operate in the operating system space.

In some embodiments some or all of the functionality of the cluster node modules is incorporated into or integrated with the operating system. The operating system can add cluster computing functionality to application programs for example by implementing at least some of the methods modules data structures commands functions and processes discussed herein. Other suitable variations of the techniques described herein can be employed as would be recognized by one skilled in the art.

In some embodiments the operating system or components of the operating system can identify and launch the front end and the kernels . The operating system or its components can connect the front end and kernels to one another in the same manner as a cluster node module would or by a variation of one of the techniques described previously. The operating system can also be responsible for maintaining the communications network that connects the modules to one another. In some embodiments the operating system implements at least some MPI style calls such as for example collective MPI style calls. In some embodiments the operating system includes an application programming interface API library of cluster subroutine calls that is exposed to application programs. Applications programs can use the API library to assist with launching and operating the computer cluster.

In one embodiment a computer cluster includes a procedure to shut down the system. If the operation process or main loop on the cluster node module connected to the user interface module detects a Quit or Exit command or otherwise receives a message from the user indicating a shut down the sequence to shut down the cluster node modules and the kernel modules is activated. In one embodiment the cluster node module connected to the user interface module sends a quit message to all other cluster node modules . Each cluster node module forwards the quit command to its local kernel module . Once its Mathematica kernel has quit each cluster node module proceeds to tear down its communication network with other cluster node modules for example see description of the MPI Finalize command above . At the conclusion of the process each cluster node module exits execution.

For purposes of illustration sample scenarios are discussed in which the computer cluster system is used in operation. In these sample scenarios examples of Mathematica code are given and descriptions of how the code would be executed by a cluster system are provided.

Fundamental data available to each node includes the node s identification number and total processor count.

The first element should be unique for each processor while the second is generally the same for all. Processor 0 can see what other values are using a collective see below communications call such as mpiGather .

The mpiSend and mpiRecv commands make possible basic message passing but one needs to define which processor to target. The following defines a new variable targetProc so that each pair of processors will point to each other.

In this example the even processors target its right processor while the odd ones point its left. For example if the processors were lined up in a row and numbered in order every even numbered processor would pair with the processor following it in the line and every odd numbered processor would pair with the processor preceding it. Then a message can be sent 

The If statement causes the processors to evaluate different code the odd processor sends 22 digits of Pi while the even receives that message. Note that these MPI calls return nothing. The received message is in the variable a 

The variable a on the odd processors would have no definition. Moreover if NProc is 8 processor 3 sent Pi to processor 2 processor 5 sent Pi to processor 4 and so on. These messages were not sent through processor 0 but they communicated on their own.

The mpiISend and mpiIRecv commands have a letter I to indicate asynchronous behavior making it possible to do other work while messages are being sent and received or if the other processor is busy. So the above example could be done asynchronously 

The variable e has important data identifying the message and mpiTest e can return True before the expressions are to be accessed. At this point many other evaluations can be performed. Then one can check using mpiTest when the data is needed 

The mpiWait e command could have also have been used which does not return until mpiTest e returns True. The power of using these peer to peer calls is that it becomes possible to construct any message passing pattern for any problem.

In some cases such explicit control is not required and a commonly used communication pattern is sufficient. Suppose processor 0 has an expression in b that all processors are meant to have. A broadcast MPI call would do 

The second argument specifies which processor is the root of this broadcast all others have their b overwritten. To collect values from all processors use mpiGatherD 

The variable c of processor 0 is written with a list of all the b of all the processors in mpiCommWorld. The temporal opposite is mpiScatter 

The mpiScatter command cuts up the variable a into even pieces when possible and scatters them to the processors. This is the result if NProc 2 but if NProc 4 b would only have 2.

MPI provides reduction operations to perform simple computations mixed with messaging. Consider the following 

The mpiSum constant indicates that variable a of every processor will be summed. In this case NProc is 2 so those elements that were not identical result in odd sums while those that were the same are even.

Most of these calls have default values if not all are specified. For example each of the following calls will have the equivalent effect as the above mpiGather call 

High level calls can include convenient parallel versions of commonly used application program calls for example Mathematica calls . For example ParallelTable is like Table except that the evaluations are automatically performed in a distributed manner 

The third argument specifies that the answers are collated back to processor 0. This is a useful simple way to parallelize many calls to a complex function. One could define a complicated function and evaluate it over a large range of inputs 

A Fourier transform of a large array can be solved faster in parallel or made possible on a cluster because it can all be held in memory. A two dimensional Fourier transform of the above example follows 

Many problems require interactions between partitions but only on the edge elements. Maintaining these edges can be performed using EdgeCell .

In particle based problems items can drift through space sometimes outside the partition of a particular processor. This can be solved with ElementManage 1 

The second argument of ElementManage describes how to test elements of a list. The fcn identifier returns which processor is the home of that element. Passing an integer assumes that each element is itself a list whose first element is a number ranging from 0 to the passed argument.

While the examples above involve Mathematica software and specific embodiments of MPI calls and cluster commands it is recognized that these embodiments are used only to illustrate features of various embodiments of the systems and methods.

Although cluster computing techniques modules calls and functions are disclosed with reference to certain embodiments the disclosure is not intended to be limited thereby. Rather a skilled artisan will recognize from the disclosure herein a wide number of alternatives for the exact selection of cluster calls functions and management systems. For example single node kernels can be managed using a variety of management tools and or can be managed manually by a user as described herein. As another example a cluster node module can contain additional calls and procedures including calls and procedures unrelated to cluster computing that are not disclosed herein.

Other embodiments will be apparent to those of ordinary skill in the art from the disclosure herein. Moreover the described embodiments have been presented by way of example only and are not intended to limit the scope of the disclosure. Indeed the novel methods and systems described herein can be embodied in a variety of other forms without departing from the spirit thereof. Accordingly other combinations omissions substitutions and modifications will be apparent to the skilled artisan in view of the disclosure herein. Thus the present disclosure is not intended to be limited by the disclosed embodiments but is to be defined by reference to the appended claims. The accompanying claims and their equivalents are intended to cover forms or modifications as would fall within the scope and spirit of the inventions.

