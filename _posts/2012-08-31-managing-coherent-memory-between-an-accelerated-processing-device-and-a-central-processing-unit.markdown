---

title: Managing coherent memory between an accelerated processing device and a central processing unit
abstract: Existing multiprocessor computing systems often have insufficient memory coherency and, consequently, are unable to efficiently utilize separate memory systems. Specifically, a CPU cannot effectively write to a block of memory and then have a GPU access that memory unless there is explicit synchronization. In addition, because the GPU is forced to statically split memory locations between itself and the CPU, existing multiprocessor computing systems are unable to efficiently utilize the separate memory systems. Embodiments described herein overcome these deficiencies by receiving a notification within the GPU that the CPU has finished processing data that is stored in coherent memory, and invalidating data in the CPU caches that the GPU has finished processing from the coherent memory. Embodiments described herein also include dynamically partitioning a GPU memory into coherent memory and local memory through use of a probe filter.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09430391&OS=09430391&RS=09430391
owner: ATI Technologies ULC
number: 09430391
owner_city: Markham, ON
owner_country: CA
publication_date: 20120831
---
This application claims the benefit of U.S. Provisional Application No. 61 617 479 filed on Mar. 29 2012 which is incorporated herein by reference in its entirety.

The present invention is generally directed to computing operations performed in computer systems. More particularly the present invention is directed to a coherent memory model that is shared between processors.

The desire to use a graphics processing unit GPU for general computation has become much more pronounced recently due to the GPU s exemplary performance per unit power and or cost. The computational capabilities for GPUs generally have grown at a rate exceeding that of the corresponding central processing unit CPU platforms. This growth coupled with the explosion of the mobile computing market e.g. notebooks mobile smart phones tablets etc. and its necessary supporting server enterprise systems has been used to provide a specified quality of desired user experience. Consequently the combined use of CPUs and GPUs for executing workloads with data parallel content is becoming a volume technology.

However GPUs have traditionally operated in a constrained programming environment available primarily for the acceleration of graphics. These constraints arose from the fact that GPUs did not have as rich a programming ecosystem as CPUs. Their use therefore has been mostly limited to two dimensional 2D and three dimensional 3D graphics and a few leading edge multimedia applications which are already accustomed to dealing with graphics and video application programming interfaces APIs .

With the advent of multi vendor supported OpenCL and DirectCompute standard APIs and supporting tools the limitations of the GPUs in traditional applications has been extended beyond traditional graphics. Although OpenCL and DirectCompute are a promising start there are many hurdles remaining to creating an environment and ecosystem that allows the combination of a CPU and a GPU to be used as fluidly as the CPU for most programming tasks.

Existing computing systems often include multiple processing devices. For example some computing systems include both a CPU and a GPU on separate chips e.g. the CPU might be located on a motherboard and the GPU might be located on a graphics card or in a single chip package. Both of these arrangements however still include significant challenges associated with i efficient scheduling ii providing quality of service QoS guarantees between processes iii programming model iv compiling to multiple target instruction set architectures ISAs and v separate memory systems all while minimizing power consumption.

The existing multiprocessor computing systems often have insufficient memory coherency and consequently are unable to efficiently utilize the separate memory systems. For example the CPU cannot effectively write to a block of memory and then access that memory from the GPU device unless the GPU explicitly synchronizes or flushes its caches. Otherwise the write will not be made visible to the GPU device. This is because a GPU is optimized for a weak consistency memory model. In particular load commands may be reordered after other load commands and store commands may be reordered after other store commands.

In addition in existing multiprocessor computing systems the CPU is forced to statically split memory locations between two different memory heaps one is private to the CPU private and the other is shared coherently with the CPU. As result of statically splitting memory locations between two memory heaps existing multiprocessor computing systems are unable to efficiently utilize the separate memory systems.

What is needed therefore are methods and systems that provide sufficient memory coherency to facilitate efficient use of separate memories in a multiprocessor computing system

Although GPUs accelerated processing units APUs and general purpose use of the graphics processing unit GPGPU are commonly used terms in this field the expression accelerated processing device APD is considered to be a broader expression. For example APD refers to any cooperating collection of hardware and or software that performs those functions and computations associated with accelerating graphics processing tasks data parallel tasks or nested data parallel tasks in an accelerated manner compared to conventional CPUs conventional GPUs software and or combinations thereof.

Embodiments of the present invention in certain circumstances provide systems and methods for managing a coherent memory between an APD and a CPU. According to a first embodiment a method is provided for receiving a notification within the APD that the CPU has finished processing data that is stored in the coherent memory. The method also includes invalidating data in the CPU caches that the APD has finished processing from the coherent memory. According to a second embodiment a method is provided for dynamically partitioning APD memory into APD coherent memory and APD local memory through use of a probe filter.

Additional features and advantages of the invention as well as the structure and operation of various embodiments of the invention are described in detail below with reference to the accompanying drawings. It is noted that the invention is not limited to the specific embodiments described herein. Such embodiments are presented herein for illustrative purposes only. Additional embodiments will be apparent to persons skilled in the relevant art s based on the teachings contained herein.

In the detailed description that follows references to one embodiment an embodiment an example embodiment etc. indicate that the embodiment described may include a particular feature structure or characteristic but every embodiment may not necessarily include the particular feature structure or characteristic. Moreover such phrases are not necessarily referring to the same embodiment. Further when a particular feature structure or characteristic is described in connection with an embodiment it is submitted that it is within the knowledge of one skilled in the art to affect such feature structure or characteristic in connection with other embodiments whether or not explicitly described.

The term embodiments of the invention does not require that all embodiments of the invention include the discussed feature advantage or mode of operation. Alternate embodiments may be devised without departing from the scope of the invention and well known elements of the invention may not be described in detail or may be omitted so as not to obscure the relevant details of the invention. In addition the terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. For example as used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will be further understood that the terms comprises comprising includes and or including when used herein specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

In one example system also includes a memory an operating system and a communication infrastructure . The operating system and the communication infrastructure are discussed in greater detail below.

The system also includes a kernel mode driver KMD a software scheduler SWS and a memory management unit such as input output memory management unit IOMMU . Components of system can be implemented as hardware firmware software or any combination thereof. A person of ordinary skill in the art will appreciate that system may include one or more software hardware and firmware components in addition to or different from that shown in the embodiment shown in .

In one example a driver such as KMD typically communicates with a device through a computer bus or communications subsystem to which the hardware connects. When a calling program invokes a routine in the driver the driver issues commands to the device. Once the device sends data back to the driver the driver may invoke routines in the original calling program. In one example drivers are hardware dependent and operating system specific. They usually provide the interrupt handling required for any necessary asynchronous time dependent hardware interface.

CPU can include not shown one or more of a control processor field programmable gate array FPGA application specific integrated circuit ASIC or digital signal processor DSP . CPU for example executes the control logic including the operating system KMD SWS and applications that control the operation of computing system . In this illustrative embodiment CPU according to one embodiment initiates and controls the execution of applications by for example distributing the processing associated with that application across the CPU and other processing resources such as the APD .

APD among other things executes commands and programs for selected functions such as graphics operations and other operations that may be for example particularly suited for parallel processing. In general APD can be frequently used for executing graphics pipeline operations such as pixel operations geometric computations and rendering an image to a display. In various embodiments of the present invention APD can also execute compute processing operations e.g. those operations unrelated to graphics such as for example video operations physics simulations computational fluid dynamics etc. based on commands or instructions received from CPU .

For example commands can be considered as special instructions that are not typically defined in the ISA. A command may be executed by a special processor such a dispatch processor command processor or network controller. On the other hand instructions can be considered for example a single operation of a processor within a computer architecture. In one example when using two sets of ISAs some instructions are used to execute x86 programs and some instructions are used to execute kernels on an APD compute unit.

In an illustrative embodiment CPU transmits selected commands to APD . These selected commands can include graphics commands and other commands amenable to parallel execution. These selected commands that can also include compute processing commands can be executed substantially independently from CPU .

APD can include its own compute units not shown such as but not limited to one or more SIMD processing cores. As referred to herein a SIMD is a pipeline or programming model where a kernel is executed concurrently on multiple processing elements each with its own data and a shared program counter. All processing elements execute an identical set of instructions. The use of predication enables work items to participate or not for each issued command.

In one example each APD compute unit can include one or more scalar and or vector floating point units and or arithmetic and logic units ALUs . The APD compute unit can also include special purpose processing units not shown such as inverse square root units and sine cosine units. In one example the APD compute units are referred to herein collectively as shader core .

Having one or more SIMDs in general makes APD ideally suited for execution of data parallel tasks such as those that are common in graphics processing.

A work item is distinguished from other executions within the collection by its global ID and local ID. In one example a subset of work items in a workgroup that execute simultaneously together on a SIMD can be referred to as a wavefront . The width of a wavefront is a characteristic of the hardware of the compute unit e.g. SIMD processing core . As referred to herein a workgroup is a collection of related work items that execute on a single compute unit. The work items in the group execute the same kernel and share local memory and work group barriers.

Within the system APD includes its own memory such as graphics memory although memory is not limited to graphics only use . Memory provides a local memory for use during computations in APD . Individual compute units not shown within shader core can have their own local data store not shown . In one embodiment APD includes access to local graphics memory as well as access to the memory . In another embodiment APD can include access to dynamic random access memory DRAM or other such memories not shown attached directly to the APD and separately from memory .

In the example shown APD also includes one or n number of command processors CPs . CP controls the processing within APD . CP also retrieves commands to be executed from command buffers in memory and coordinates the execution of those commands on APD .

In one example CPU inputs commands based on applications into appropriate command buffers . As referred to herein an application is the combination of the program parts that will execute on the compute units within the CPU and APD. A plurality of command buffers can be maintained with each process scheduled for execution on the APD .

CP can be implemented in hardware firmware or software or a combination thereof. In one embodiment CP is implemented as a reduced instruction set computer RISC engine with microcode for implementing logic including scheduling logic.

APD also includes one or n number of dispatch controllers DCs . In the present application the term dispatch refers to a command executed by a dispatch controller that uses the context state to initiate the start of the execution of a kernel for a set of work groups on a set of compute units. DC includes logic to initiate workgroups in the shader core . In some embodiments DC can be implemented as part of CP .

System also includes a hardware scheduler HWS for selecting a process from a run list for execution on APD . HWS can select processes from run list using round robin methodology priority level or based on other scheduling policies. The priority level for example can be dynamically determined. HWS can also include functionality to manage the run list for example by adding new processes and by deleting existing processes from run list . The run list management logic of HWS is sometimes referred to as a ran list controller RLC .

APD can have access to or may include an interrupt generator . Interrupt generator can be configured by APD to interrupt the operating system when interrupt events such as page faults are encountered by APD . For example APD can rely on interrupt generation logic within IOMMU to create the page fault interrupts noted above.

APD can also include preemption and context switch logic for preempting a process currently running within shader core . Context switch logic for example includes functionality to stop the process and save its current state e.g. shader core state and CP state .

Memory can include non persistent memory such as DRAM not shown . Memory can store e.g. processing logic instructions constant values and variable values during execution of portions of applications or other processing logic. For example in one embodiment parts of control logic to perform one or more operations on CPU can reside within memory during execution of the respective portions of the operation by CPU .

In this example memory includes command buffers that are used by CPU to send commands to APD . Memory also contains process lists and process information e.g. active list and process control blocks . These lists as well as the information are used by scheduling software executing on CPU to communicate scheduling information to APD and or related scheduling hardware. Access to memory can be managed by a memory controller which is coupled to memory . For example requests from CPU or from other devices for reading from or for writing to memory are managed by the memory controller .

As used herein context can be considered the environment within which the kernels execute and the domain in which synchronization and memory management is defined. The context includes a set of devices the memory accessible to those devices the corresponding memory properties and one or more command queues used to schedule execution of a kernel s or operations on memory objects.

In the example above communication infrastructure interconnects the components of system as needed. Communication infrastructure can include not shown one or more of a peripheral component interconnect PCI bus extended PCI PCI E bus advanced microcontroller bus architecture AMBA bus advanced graphics port AGP or other such communication infrastructure. Communications infrastructure can also include an Ethernet or similar network or any suitable physical communications infrastructure that satisfies an application s data transfer rate requirements. Communication infrastructure includes the functionality to interconnect components including components of computing system .

In some embodiments based on interrupts generated by an interrupt controller such as interrupt controller operating system invokes an appropriate interrupt handling routine. For example upon detecting a page fault interrupt operating system may invoke an interrupt handler to initiate loading of the relevant page into memory and to update corresponding page tables.

In some embodiments SWS maintains an active list in memory of processes to be executed on APD . SWS also selects a subset of the processes in active list to be managed by HWS in the hardware. Information relevant for running each process on APD is communicated from CPU to APD through process control blocks PCB . is an illustration of an exemplary block diagram of a computing system based upon a coherent memory model in accordance with embodiments of the present invention. As used herein a coherent memory model broadly describes the permissible interactions of memory operations from multiple wavefronts operating in a computing system such as computing system using CPU and APD .

Computing system provides a more detailed view of the internal architecture of computing system shown in . For example computing system includes CPU APD memory and APD memory of computing system . Computing system also includes a flag register . Flag register is associated with a synchronization variable stored in system memory . In the exemplary computing system CPU can include an execution engine a CPU cache and a memory controller . APD can include an instruction set execution engine APD cache and a memory controller . System memory and APD memory can include coherent memories A and B respectively. As would be appreciated by those skilled in the relevant arts computing system is not limited to the components shown in .

Execution engine executes a variety of commands during the operation of CPU . Many of these commands require that execution engine perform operations on data stored in coherent memory A. Execution engine can determine where the data is stored by accessing address processing device that contains a virtual address for the data. The address processing device contains address pointers to data that are shared between CPU and APD . Once the address pointers are retrieved the virtual addresses can be translated into physical addresses. For example if the physical address for the data is located in coherent memory A CPU cache is queried to determine whether it is holding the data. If CPU cache is not holding the requested data memory controller retrieves the data stored in coherent memory A based upon the physical address. Execution engine then processes the retrieved data.

The processed data is written to CPU cache and coherent memory A. After processing CPU writes a flag to flag register informing APD that data is available for manipulation. Furthermore the synchronization variable associated with flag register is set to confirm the data is valid.

APD periodically monitors flag register for a notification when data is available for processing commands. When notification is received execution engine executes a load acquire command.

As understood by those of skill in the art the load acquire command guarantees that all subsequent loads occur after the load acquire and that all subsequent loads read coherent memory. This requirement ensures that subsequent loads are not serviced by stale data in the APD cache. In the example above when the load acquire command is executed APD waits to execute any read requests initiated after the load acquire command. Once all outstanding read requests have been fenced APD monitors flag register to ensure that the data to be operated on includes valid updates.

In one embodiment execution engine determines where the data is stored by acquiring an address pointer from address processing device . The address pointer can be a virtual address that is translated into a physical address. Once the physical address is received memory control can retrieve the data from coherent memory A. Execution engine performs operations on data retrieved from coherent memory A within system memory . Once the data is processed execution engine executes a store release command which guarantees that all previous memory writes are visible to other devices.

For example when the store release command is executed within execution engine APD flushes all data within APD cache to coherent memory B to ensure the data is valid. In other words APD waits for all store commands executed prior to the store release command to complete. After validity has been insured APD writes a flag to flag register providing notification that data within coherent memory B is available to other devices such as CPU . Furthermore the synchronization variable associated with flag register is set to confirm validity of the data. At this point the store release command can complete execution.

As with computing system at of computing system includes CPU APD system memory and APD local memory . Computing system also includes flag register .

CPU includes an execution engine an address processing device CPU cache and a memory controller . APD includes execution engine an address processing device APD cache a probe filter and a memory controller . In the exemplary embodiment of system memory includes non coherent memory and system coherent memory . APD memory includes ADP local memory and APD coherent memory . As would be appreciated by those skilled in the relevant arts computing system is not limited to the components shown in .

In the embodiment execution engine receives a command to perform an operation within CPU . Execution engine acquires an address pointer from address processing device . Address processing device translates the address pointer into a physical address for use by memory controller . Memory controller uses the physical address to check CPU cache and system memory to determine whether the requested data can be located. Once the data has been located execution engine processes the data by executing commands.

The processed data can be stored to CPU cache and system coherent memory . By way of example CPU cache can be a level 1 L1 cache level 2 L2 cache or a level 3 L3 cache. Memory controller can also store the processed data to system coherent memory . System coherent memory is accessible to CPU and APD via a PCI PCIE or any other suitable interconnection. Frequently used data can also be stored on CPU cache . Once the data is stored memory controller sets a synchronization variable within flag register .

The setting and operation of flag register of discussed above also applies to flag register . Therefore flag register will not be discussed in addition detail.

Computing system also includes probe filter . Probe filter is a mechanism for monitoring and recording the addresses of cache lines used by CPU or an agent thereof. The embodiment of probe filter is configured to optimize the performance of a computing system by reducing the number of times APD searches CPU cache and system coherent memory to retrieve requested data.

For example when APD receives the address of the requested data probe filter determines whether that data was recently exported by CPU . To make this determination the address of the requested data is compared to addresses recorded within probe filter . If the comparison produces a match a probe is sent to CPU memory controller to retrieve the data. For example memory controller can search CPU cache and system coherent memory to locate the exported data. When CPU memory controller finds the data commands are executed to ensure that the data is valid. For example synchronization variables are checked to ensure the data is current.

Conversely if the comparison fails to produce a match i.e. the data was not recently exported by CPU a driver not shown may elect to process the data as if the data was stored in non coherent local memory such as non coherent memory . Responsive to the comparisons with the probe filter a driver not shown can dynamically partition APD memory into APD coherent memory and APD local memory . Alternatively the driver can store a portion of the allocated APD coherent memory into APD local memory .

In this example the received data is compared to the addresses of cache lines recently exported by CPU that are recorded within probe filter . If the address has not been previously exported the driver can store a portion of the APD coherent memory containing those cache lines into APD local memory . This effectively partitions and prevents the APD coherent memory from being used as a shared resource between CPU and APD . In this example APD coherent memory may be only visible to ADP for the duration of its allocation as managed by the driver.

Alternatively APD memory can be used as a dynamic resource to allocate regions within APD local memory treating these regions within APD as an extension of APD coherent memory . In this manner APD coherent memory will be available to both CPU and ADP . In the embodiment APD coherent memory is mapped into the application virtual address space using x86 page tables. The operating system e.g. operating system is responsible for maintaining currency of the APD table lookup buffers TLBs . In the embodiment as described above APD coherent memory does not require additional software such as consistency semantics to facilitate coherent operation. For example CPU can store processed data to APD coherent memory in the same manner it would store processed data to system coherent memory .

In operation a processor receives a notification indicating a data is available in a memory. For example APD receives a notification that CPU has finished processing data stored in the coherent memory A as shown in .

In operation a processor locates outstanding requests associated with data stored in the memory. For example APD executes a load acquire instruction to locate outstanding read requests for data stored in coherent memory A. In operation a processor such as APD waits for the requests to complete.

In operation the data is processed by the processor. For example an execution engine that is located within APD processes the data by executing commands. In operation the processor returns the processed data to the memory. For example APD executes a store release instruction for the data processed by APD . When the store release instruction is executed APD flushes processed data stored within APD cache to coherent memory B as illustrated in . In other words APD waits for all store commands executed prior to the store release command to complete. After validity has been insured APD writes a flag to flag register providing notification that data within coherent memory B is available to other devices such as CPU .

In operation the processor sets a synchronize variable when all data is returned to the memory. For example a synchronization variable is set by APD to confirm that all the data flushed to coherent memory B is valid.

In operation a processor receives an address for data that is available to be processed. For example APD receives an address for data that is available to be processed. In operation the processor determines if another device has recently used the data. For example APD uses a probe filter to determine whether the address of the associated data was previously exported by CPU as described above in relation to . If the data was recently exported a probe is sent to retrieve the data from the other device as depicted in operation . For example if probe filter determines that data was recently exported by APD a probe is sent to CPU to retrieve the exported data. In this example APD searches CPU cache to locate the exported data.

If the data was not recently used by another device the memory is partitioned as depicted in operation . For example APD uses probe filter o determine if data was recently exported. If the data was recently exported APD memory is partitioned as described above.

It is to be appreciated that the Detailed Description section and not the Summary and Abstract sections is intended to be used to interpret the claims. The Summary and Abstract sections may set forth one or more but not all exemplary embodiments of the present invention as contemplated by the inventor s and thus are not intended to limit the present invention and the appended claims in any way.

The Summary and Abstract sections may set forth one or more but not all exemplary embodiments of the present invention as contemplated by the inventor s and thus are not intended to limit the present invention and the appended claims in any way.

The present invention has been described above with the aid of functional building blocks illustrating the implementation of specified functions and relationships thereof. The boundaries of these functional building blocks have been arbitrarily defined herein for the convenience of the description. Alternate boundaries can be defined so long as the specified functions and relationships thereof are appropriately performed.

The foregoing description of the specific embodiments will so fully reveal the general nature of the invention that others can by applying knowledge within the skill of the art readily modify and or adapt for various applications such specific embodiments without undue experimentation without departing from the general concept of the present invention. Therefore such adaptations and modifications are intended to be within the meaning and range of equivalents of the disclosed embodiments based on the teaching and guidance presented herein. It is to be understood that the phraseology or terminology herein is for the purpose of description and not of limitation such that the terminology or phraseology of the present specification is to be interpreted by the skilled artisan in light of the teachings and guidance.

The breadth and scope of the present invention should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents.

