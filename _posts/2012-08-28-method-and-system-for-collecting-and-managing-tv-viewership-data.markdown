---

title: Method and system for collecting and managing TV viewership data
abstract: A computer-implemented method for collecting and managing TV viewership data from multiple TV metering data providers is disclosed. The method includes: receiving an event log file at a distributed computer system that includes multiple computers; dynamically selecting one or more computers according to a predefined sharding function; at each of the selected computers: allocating a set of compressed event records, which corresponds to a subset of the event log file, at predetermined locations within the memory of the computer; and in accordance with a predefined schedule, replicating the compressed event records from a respective one of the selected computers to one or more other computers of the distributed system such that there are at least two replicas of any event record on at least two computers of the distributed computer system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09392310&OS=09392310&RS=09392310
owner: GOOGLE INC.
number: 09392310
owner_city: Mountain View
owner_country: US
publication_date: 20120828
---
This Application is a United States National Stage Application filed under 35 U.S.C. 371 of PCT Patent Application Serial No. PCT US2012 052717 filed on Aug. 28 2012 which claims the benefit of and priority to U.S. Provisional Patent Application No. 61 529 808 filed on Aug. 31 2011 which are hereby incorporated by reference in their entireties.

The disclosed implementations relate generally to TV viewership ratings and in particular to system and method for collecting and managing TV viewership data.

Nowadays people can get the same TV content from different vendors through different communication channels such as satellite cable phone line and Internet. The choice of communication channels often has many geographic and demographic considerations. For example satellite receivers may be the most commonly used devices for households in the rural areas to receive TV signals. But it is probably more common for households in big metropolitan areas to use cable connections or over the air OTA antennas to receive TV signals. Although many people still watch TV programs on TVs younger viewers may increasingly choose to watch TV programs on a computer that is coupled to the Internet or even a smartphone supporting 3G 4G wireless communications. The existence of diversified communication channels for receiving TV programs is that it is more challenging to provide an efficient accurate estimate of the viewership rating for a particular TV program at a large population level e.g. at a national level .

In accordance with some implementations described below a computer implemented method for collecting and managing TV viewership data is disclosed. The method is implemented at a distributed computer system including multiple computers each computer having one or more processors and memory storing multiple event records each event record including a predefined time interval. The method includes receiving an event log file that includes a plurality of data source IDs each data source ID having an associated set of event log entries and each event log entry including a time interval dynamically selecting one or more computers by applying the plurality of data source IDs to a predefined sharding function at each of the selected computers identifying at least one of the plurality of data source IDs that matches at least one of the event records stored on the computer for each identified data source ID and each of the set of event log entries associated with the identified data source ID determining a location in the memory of the computer in accordance with the event log entry s time interval and the time intervals of the event records stored on the memory of the computer generating a new event record for the event log entry wherein the new event record includes the time interval associated with the event log entry storing the new event record at the determined location within the memory of the computer and in accordance with a predefined schedule replicating the compressed event records from a respective one of the selected computers to one or more other computers of the distributed system such that there are at least two replicas of any event record on at least two computers of the distributed computer system.

In accordance with some implementations described below a distributed computer system for collecting and managing TV viewership data is disclosed the distributed computer system including multiple computers each computer having one or more processors and memory storing multiple event records each event record including a predefined time interval. The one or more programs include instructions for receiving an event log file that includes a plurality of data source IDs each data source ID having an associated set of event log entries and each event log entry including a time interval dynamically selecting one or more computers by applying the plurality of data source IDs to a predefined sharding function at each of the selected computers identifying at least one of the plurality of data source IDs that matches at least one of the event records stored on the computer for each identified data source ID and each of the set of event log entries associated with the identified data source ID determining a location in the memory of the computer in accordance with the event log entry s time interval and the time intervals of the event records stored on the memory of the computer generating a new event record for the event log entry wherein the new event record includes the time interval associated with the event log entry storing the new event record at the determined location within the memory of the computer and in accordance with a predefined schedule replicating the compressed event records from a respective one of the selected computers to one or more other computers of the distributed system such that there are at least two replicas of any event record on at least two computers of the distributed computer system.

In accordance with some implementations described below a non transitory computer readable storage medium storing one or more programs for execution by one or more processors of a computer system for collecting and managing TV viewership data is disclosed. The one or more programs include instructions for receiving an event log file that includes a plurality of data source IDs each data source ID having an associated set of event log entries and each event log entry including a time interval dynamically selecting one or more computers by applying the plurality of data source IDs to a predefined sharding function at each of the selected computers identifying at least one of the plurality of data source IDs that matches at least one of the event records stored on the computer for each identified data source ID and each of the set of event log entries associated with the identified data source ID determining a location in the memory of the computer in accordance with the event log entry s time interval and the time intervals of the event records stored on the memory of the computer generating a new event record for the event log entry wherein the new event record includes the time interval associated with the event log entry storing the new event record at the determined location within the memory of the computer and in accordance with a predefined schedule replicating the compressed event records from a respective one of the selected computers to one or more other computers of the distributed system such that there are at least two replicas of any event record on at least two computers of the distributed computer system.

TV viewership in national markets can be quite fragmented. In some implementations a TV viewership projection system receives raw viewership data from a variety of TV content providers e.g. cable and satellite companies over the air broadcasters and Internet streaming sites . The TV viewership projection system aggregates the raw data from each of the different content providers for different geodemographic groups i.e. particular viewer demographics geographic regions and or some combination of both characteristics and computes viewership share information for particular groups at a level that is statistically significant. For example the TV viewership projection system computes per minute share information when there is enough data e.g. in metropolitan areas and per hour share information when there is not enough data to reliably determine per minute share information e.g. in sparsely populated areas where there are few subscribers for a particular service content provider . The TV viewership projection system then combines the share information from disparate content providers by weighting the different components in order to produce reliable share information for larger areas than covered by the information from the disparate providers. In some situations the viewership share information covers the same geodemographic groups e.g. viewership information for the same geographical regions from a satellite provider and a cable provider . Also by combining and weighting viewership share information for different content providers it becomes possible to generate reliable information for geodemographic groups that are not adequately represented in either group individually e.g. share information for a cable provider A and a satellite provide B might not include adequate information for the same geo demographic group X individually but when combined they do .

In some implementations the households for which account and viewership information is retained in the database are participants in TV viewership panels who have agreed that their TV viewing account and demographic information can be collected aggregated and analyzed to allow the distributed measurement server to determine TV viewing data for participant households . The account information database generally includes an account number a name and a billing address for each participant household. In some implementations the account information also includes a physical address such as a residence address for a household and or an IP address associated with one or more Internet enabled devices used by the household to access and view streaming TV content and or Internet based services generally. In some cases the household has a fixed IP address in which case the fixed IP address is associated with the household s account in the database . In other cases the household has a dynamically allocated IP address which can change on a regular basis e.g. every time a household member dials up or makes a new connection to the Internet service provider ISP . In this case the broadcaster provider tracks the changes to the household s IP address and updates the record in the database accordingly. In some cases the account information database includes an account profile associated with each household. The account profile may include one or more demographic parameters that characterize the members of the household including but not limited to the number of household members and the age gender educational level income and profession of at least one household member. As described below information in the account profile is used for querying the viewership data in response to a client request or minimizing the bias associated with the viewership data collected by one provider when projecting the TV viewership rating from the collected viewership data. In some cases the account information database includes the TV viewership data that represents the television viewing activity of the household associated with each account. For example the TV viewing activity can include information on every program viewed by the household including for each program a name and description of the program the channel that played the program the date time of the viewing etc. In other implementations the TV viewing activity saved in the database includes only programs that are viewed for at least a threshold amount of time e.g. 1 minute or 5 minutes as well as the start time of a program and the end time of the program. In some implementations the viewing activity tracked includes only premium content. The TV viewership data may include either the raw data sample collected from a household such as the date and time when the data sample was collected and information about the TV program being broadcasted in the household when the data sample was collected or the pre processed data sample such as the broadcasting duration of the TV program in the household. As shown in the database may include the TV viewership data collected from multiple TV broadcasters or TV metering data providers . A data pre processing procedure may be applied to the data from different sources if their formats are different from the one used by the database . The terms like viewership data and metering data are used interchangeably throughout this application.

In some implementations the TV broadcaster is only responsible for broadcasting TV signals while a separate TV metering data provider is in charge of collecting TV metering data for different TV programs from the households. In some other implementations the TV broadcaster and the TV metering data provider operate as single entity that is responsible from both broadcasting TV signals and collecting the TV metering data. But the implementations of the present application apply to either configuration. For simplicity the rest of the present application will use the term TV metering data provider to represent both entities. From the context in which the term appears one of ordinary skill in the art would understand which entity it refers to.

As show in the TV metering data providers may transmit the television programs to the household over a cable by transmission from a satellite or by streaming over the communication networks e.g. Internet . In the case of satellite transmissions the household has a receiver antenna to receive the signal. In the household there is a receiver or converter to process or decode the incoming TV signals. The decoded TV signals are transmitted to a set top box STB which allows household members to control what is being displayed on the television . In some implementations the receiver converter is combined with the STB . In general a household member such as member or controls the STB with a remote control device. In some implementations there is additional communication between the TV metering data provider and the STB over a telephone line . For example the STB may provide information about what television programs are being viewed or may receive further information from the TV metering data provider for interactive television programs. The TV metering data provider processes information about the household members viewing activity from the STB and stores the processed information in the database .

In some implementations the household members viewing activity is identified by the receiver converter and transmitted back to the TV metering data provider through the STB which is connected to the communication network e.g. the Internet through a wired or wireless home router . In other implementations the STB is able to ascertain the program viewed by evaluating the signal received from the receiver converter . In these implementations the STB transmits the viewing information e.g. program channel date time etc. to the TV metering data provider again via the household router . Because the viewing information is transmitted through the household router the IP address of the household router is also transmitted to the TV metering data provider along with the viewing information. In some implementations the IP address and or the viewing information is transmitted to the TV metering data provider on a periodic basis e.g. from once an hour to once a day or even once a week . Between two consecutive transmissions the data is stored in the STB . As noted above in some alternative implementations the STB transmits data to the TV metering data provider over a phone line . In these implementations the STB is able to retrieve the IP address from the router and transmit it with the viewing data.

The actual television program signals are generally transmitted by satellite over a cable or via terrestrial TV transmissions i.e. conventional TV broadcast . In some implementations the television programs are streamed over the communications network such as the Internet. In these implementations the process of selecting a television program may be performed by a computer the STB or a receiver converter that is connected directly to the household router not shown in . The household router is the gateway to the Internet from the household . Inside the household the router is connected to the STB and in some cases to a number of computers smartphones or digital audio players or game consoles such as XBOX PLAYSTATION or WII. The router is connected to the communication network through an Internet service provider that assigns the IP address to the home router .

The computers in the household can access the Internet to perform myriad activities such as watching TV programs streamed from the TV metering data provider through the communication networks the Internet service provider and the household router shopping viewing videos online e.g. on YouTube playing online games participating in online social networks or engaging in many other activities. The TV program viewing activities are logged by the TV metering data provider in the database and tracked by the IP Address of the household because it is readily available and it is a unique identifier at least at a specific point in time . One of ordinary skill in the art would recognize that the data identified in the account information database could all be found in a single database or distributed to a different number of databases depending on the implementation.

In some implementations the distributed measurement server is coupled to the TV metering data providers or the account information database or both for receiving many households metering data collected by the TV metering data providers as well as their demographic data. In some other implementations the distributed measurement server receives metering data from the household s STB via the communication networks the ISP and the household router . As noted below the distributed measurement server manages a copy of the metering and demographic data of its own for estimating TV viewership ratings in response to queries from its clients and providing such rating information to the requesting clients. As will be described below in detail the distributed measurement server applies a set of criteria to the TV metering data stored on its memory to determine a reasonable estimate of a client requested viewership rating within a short time frame ranging from e.g. less than a second to a few minutes. In some implementations the distributed measurement server allocates different weights to the data from different providers to eliminate or reduce the associated bias the weights being a function of one or more geodemographic factors including location gender age income education etc.

The root node of the hierarchical architecture is referred to as a root shard . Depending on the total number of computers within a distributed measurement server the hierarchical architecture may include zero or more layers of intermediate nodes. In the example shown in the distributed measurement server includes one layer of intermediate nodes which are referred to as mixer shards M . Each mixer shard manages a predefined number e.g. 16 of leaf shards e.g. or . When the distributed measurement server imports new metering data from the data sources e.g. STB and TV metering data provider s the root shard is responsible for selecting one or more leaf shards in accordance with the predefined sharding function to allocate the new metering data within the distributed measurement server . A more detailed description of the data importation process in the distributed measurement server is provided below in connection with . When the distributed measurement server receives a query from a client the root shard is responsible for selecting one or more leaf shards in accordance with the predefined sharding function for processing the query. The leaf shards submit the query results to the respective mixer shards . The mixer shards aggregate the query results and submit the aggregated query results to the root shard to be returned to the requesting client . A more detailed description of the query processing in the distributed measurement server is provided below in connection with . In some implementations the root mixer leaf shards correspond to three types of software modules such that different types of shards shown in may operate on the same computer within the distributed measurement server . For example a computer that is responsible for managing a shard of metering data like the leaf shard may also act as the mixer shard for receiving query results from other leaf shards N and maybe even the root shard .

In some implementations the sharding function is a hash table function that defines a mapping relationship from a TV viewership data record and one or more leaf shards such that for a given data record the root shard can determine which leaf shard s stores the data record when responding to a query or which leaf shard s is to store the data record when receiving new metering data . As an example an exemplary sharding function is defined as Shard Number hash  TV ID STB ID Broadcaster ID wherein hash K refers to a predefined hash table function that maps the input metering data or query to a respective leaf shard with a serial number in the range of 1 . . . K and K is the total number of unique leaf shards i.e. not counting the redundant replicas for each unique leaf shard within the distributed measurement server which serves as the modulo of the hash table function. The parameter TV ID identifies a unique TV within a household and the parameter STB ID identifies a unique STB that the TV is connected to. The parameter Broadcaster ID identifies a unique TV content provider that provides the TV programs to the STB. In some implementations one household may have multiple TVs connected to multiple STBs in order to receive contents from different TV content providers. Based on these three parameters the root shard determines which of the leaf shards is responsible for allocating the metering data generated for a particular TV within a particular household. For illustrative purposes some of the examples described below in connection with assume that one household has one STB such that the household ID is equivalent to the STB ID.

In some implementations the distributed measurement server is configured through the sharding function such that metering data from one data source are spread over multiple leaf shards which may be associated with the same or different mixer shards. For example one leaf shard is not allowed to store more than 5 of the metering data from the same data provider. By doing so the distributed measurement server can provide an estimated rating in response to a query even if one or more leaf shards identified as having the metering data associated with the query is temporarily or permanently unavailable because of system maintenance or other reasons. In some implementations the distributed measurement server is configured to maintain a predefined number of replicas of the same metering data on different computers associated with different leaf nodes. The root shard is configured to monitor the usage of the metering data replicas at different leaf shards such that upon receipt of a query for the metering data the root shard can not only find which leaf shards have the requested metering data but also select those leaf shards of less work load to process the query. On the other hand the metering data associated with the same household is stored on the same computer s memory and managed by the same leaf shard. In other words when the distributed measurement server finds a first leaf shard as having the metering data associated with a household identified by a query it does not need to look for a second leaf shard because any other leaf shard has either the same metering data as the first leaf shard i.e. a replica or nothing at all.

The configuration of spreading the metering data from the same data source over multiple leaf shards and maintaining multiple replicas of the same leaf shard not only improves the load balance of the distributed measurement server but also makes it more fault tolerant. For a given query there may be multiple leaf shards as having the metering data related to the query. The root shard may identify a subset of the multiple leaf shards for responding to the query based on the current work load at these leaf shards. If one of the identified leaf shards is unavailable the root shard can choose another leaf shard that has a replica of the requested metering data to replace the original leaf shard. This is especially useful if the client accepts a partial query result e.g. the client may specify in the query that a query result is acceptable if its accuracy is above a predefined threshold or if it results from processing a predefined amount of metering data but less than the entire metering data set .

In some implementations the distributed measurement server provides a predefined application programming interface API for the client to interact with the server . This API may support an existing query language such as SQL or define a more flexible new query language based on the organization of the metering data on the leaf shards. For example both the API and a client application may be implemented in Java. Upon receipt of a query submitted by the client application the root shard examines the query to make sure that it is correct and has no grammatical error and then interprets the query to determine which leaf shards may have the metering data that may match the query. For example if the query is to determine the TV viewership rating for a particular show broadcasted by a particular broadcaster the root shard first identifies one or more leaf shards that store the metering data provided by this particular broadcaster and then submits the query to one or more of the identified leaf shards based on the factors such as a leaf shard s current load balance the amount of related metering data stored on the leaf shard and the rating accuracy if specified by the client.

At each chosen leaf shard a query engine is responsible for applying the query to the metering data stored on the leaf shard. This process is to compare the query s conditions with each metering data record in the memory of the computer and determine whether this data record satisfies the query s conditions or not. In some implementations the query engine maintains a count of data records that satisfy the query such that the count increases by one whenever a new metering data record is found satisfying the query until the last data records is examined.

Because the metering data is distributed on multiple leaf shards each leaf shard having its own query engine multiple counts of data records that satisfy the query are generated independently by different leaf shards and then submitted to respective mixer shards associated with these leaf shards. In some implementations the mixer shards aggregate the multiple counts into one and submit the aggregated total count of data records that satisfy the query to the root shard which is returned to the requesting client as at least part of the response to the query.

As noted above the distributed measurement server may include multiple computers and each computer may support multiple types of shards. is a block diagram illustrating the components of a computer running as a root mixer leaf shard member of the distributed measurement server in accordance with some implementations. The computer includes one or more processing units CPU s for executing modules programs and or instructions stored in memory and thereby performing processing operations one or more network or other communications interfaces memory and one or more communication buses for interconnecting these components. In some implementations the computer includes a user interface comprising a display device and one or more input devices e.g. keyboard or mouse . In some implementations the memory includes high speed random access memory such as DRAM SRAM DDR RAM or other random access solid state memory devices. In some implementations memory includes non volatile memory such as one or more magnetic disk storage devices optical disk storage devices flash memory devices or other non volatile solid state storage devices. In some implementations memory includes one or more storage devices remotely located from the CPU s . Memory or alternately the non volatile memory device s within memory comprises a non transitory computer readable storage medium. In some implementations memory or the computer readable storage medium of memory stores the following elements or a subset of these elements and may also include additional elements 

In order to provide service to clients the distributed measurement server needs to have the TV metering data of many households. As noted above in connection with there are multiple ways for the distributed measurement server to access the TV metering data. For example the TV metering data providers may receive the metering data e.g. in the form of STB logs from respective subscribing households. Because the distributed measurement server is coupled to the TV metering data providers the TV metering data providers can forward the STB logs to the distributed measurement server. In some implementations the TV metering data providers stores the metering data in the account information database e.g. the viewership data and provides the access to the database to the distributed measurement server . In this case the distributed measurement server can receive the new metering data directly from the account information database . In some implementations the distributed measurement server can also receive the viewership data directly from individual households . In this case the STB in the household provides the STB log generated by the box to the distributed measurement server through the household router the ISP and the communication networks . As will be explained below the distributed measurement server can find a location for hosting an incoming new metering data record regardless of whether the data record comes from a TV metering data provider or an STB in a household.

In some implementations different TV metering data providers may have different schedules for providing the metering data. Some provider may provide new metering data to the distributed measurement server after a predefined time interval ranging from a day or a week. This relatively long delay may be related to the fact that the TV metering data provider is usually associated with a large number of households e.g. ranging from tens of thousands to multi millions and it may need to wait longer for collecting the STB logs from all the subscribers and then apply predefined processing procedures to the logs for multiple purposes. On the other hand an STB in a household may report the new metering data it has accumulated to the distributed measurement server more frequently e.g. from every hour or every minute . The metering data from the different sources may serve different purposes from the perspective of the distributed measurement server . For example the large volume of metering data from the TV metering data providers can help the distributed measurement server produce more accurate and less biased TV viewership rating estimates. But the metering data directly coming from individual household set top boxes can be used for making nearly real time rating estimates.

The distributed measurement server includes one or more root shards for requesting receiving the metering data from different data sources and a plurality of leaf shards for allocating the metering data see e.g. . As shown in upon receipt of the incoming metering data the root shard may apply the metering data to a predefined sharding function and choose one or more leaf shards to process the metering data. In some implementations the predefined sharding function is implemented as a hash table. For a given data provider ID there is a predefined set of leaf shards e.g. to N to host the metering data associated with this data provider ID. In order to improve the server s performance the distributed measurement server spreads the metering data from the same data source across multiple leaf shards. For example the server may set a maximum threshold such that one leaf shard should have no more than a predefined percentage or absolute amount of data from a particular data source e.g. a TV metering data provider . In some implementations the server may also set a minimum threshold for a leaf shard on the amount of data from a particular data source. In some implementations the root shard forwards to each of the identified leaf shards e.g. to N a copy of the incoming metering data it receives from the TV metering data provider so that the leaf shards can process the data in parallel. Each leaf shard screens the STB log records within the metering data one by one and chooses a subset of the STB log records to be stored in a data structure e.g. or stored in the memory of the leaf shard. A more detailed description of the data importation process at the distributed measurement server is described below in connection with .

At each leaf shard identified by the root shard a data importer module receives the plurality of event records or at least a subset thereof. In some implementations different leaf shards selected by the root shard each have independent access to the incoming metering data. For each new event record the data importer module generates a new metering data record compresses the data record and stores the compressed data record in its memory . Different leaf shards process the event records in parallel so that every record ends up being stored by one of the leaf shards. In some implementations the data importer module returns the processing result to the root shard and the processing result may identify the subset of event records stored at this particular leaf shard. The root shard may use such information to update its sharding function so that e.g. upon receipt of a query the root shard can determine which leaf shard may have the metering data matching the query. In some implementations the data importer module also propagates the newly generated data records to the other leaf shards if each of them is designated to have a replica of the data records.

In some implementations different leaf shards may have replicas of metering data associated with different data sources or even different sets of households to improve the distributed measurement server s load balance and avoid one of them being the bottleneck that adversely affects the server s overall performance. For example assuming that there are three leaf shards and three data sources the metering data from the three data sources may be stored on the three leaf shards according to the following relationship table 

One goal of the present invention is for the distributed measurement server to provide answers to queries like how many household people are watching a TV show and their geodemographic distribution in a real time fashion. The distributed measurement server can achieve this goal by responding to queries from clients even if it stores billions of TV viewership metering data records.

The query above can be further modified such that the query result is grouped by another parameter associated with the in memory STB records e.g. STB Type as follows 

The query above can be further modified such that the query result is derived by processing at least a predefined amount e.g. 50 of applicable TV metering data stored at the distributed measurement server as follows 

The query above can be further modified such that the query result is limited to a particular viewing type e.g. DVR only viewership type as follows 

Upon receipt of the query the root shard performs a sanity check on the query to make sure that the query is grammatically correct. If the query fails the sanity check the root shard returns an error message to the requesting client and may also explain the error s identified in the query. For each query that passes the sanity check the root shard applies the query to a predefined sharding function and selects one or more leaf shards for processing the query . In some implementations this step is similar to the step of selecting leaf shards for handling the incoming metering data i.e. the root shard not only considers whether a leaf shard has the metering data associated with the query but also takes into account of the load balances at different leaf shards. For example if a leaf shard does not have metering data related to the TV broadcaster identified in the query the root shard will not choose this leaf shard. But even if a leaf shard has the metering data the root shard still needs to consider the leaf shard s overall health condition e.g. whether the current work load at the leaf shard makes it the most qualified leaf shard for handling this query. Finally the root shard issues the query to each of the selected leaf shards.

At a particular leaf shard the query engine module receives the query and then applies the query to the data record stored at the leaf shard to determine a count of data records that satisfy the query. A more detailed description of how the leaf shard applies the query to the metering data stored at the leaf shard is provided below in connection with . Finally the leaf shard returns the determined count to the root shard . As noted above in connection with the distributed measurement server is implemented in a tree like hierarchical architecture. Different leaf shards process the same query in parallel and return the respective determined counts of metering data records that satisfy the query. Because different leaf shards deal with different sets of metering data records no data record would be counted twice and the final query result is achieved by aggregating the partial query results from different leaf nodes. Depending on whether the hierarchical architecture includes any mixer shard the aggregation may be performed by the root shard alone or by one or more mixer shards and then the root shard .

As shown in assuming that there is no intermediate mixer shard involved in the aggregation process the root shard receives the partial counts of data records from the respective leaf shards and aggregates the partial counts into a total count of data records that satisfy the data records. In some implementations there is no need for the distributed measurement server to examine every possible metering data record in order to provide an accurate query result. This is especially true if the client e.g. an advertiser is more interested in a query result that is statistically accurate enough to his or her satisfaction. In some implementations it is very difficult for the distributed measurement server to provide an extremely precise query result because there might be too much data to process such that the client has to wait for a long time e.g. a few hours to get any query result. To satisfy the needs by different types of clients the final query result processing module of the distributed measurement server implements a statistical projection function to project the query results returned by the leaf shards into a query result statistically estimated from the entire metering data set managed by the server . Finally the root shard returns the estimated query result to the requesting client .

In some implementations the root shard uses the measurement server s sharding function shard health and load information to identify a subset of data sample within a cluster of leaf shards within the server for evaluating the query. After counting a total sample size within the cluster the root shard propagates the total sample size per cluster across all the other leaf shards along with the original query as defined by the hierarchical architecture shown in . In particular each leaf shards determines a respective cluster weight defined as k K wherein kand Kare the total sample size as determined before and the total size of the target population in the cluster c respectively. For each cluster the root shard determines a weighted average scales the weighted average to the entire target population and adds the scaled values together for the total population. In some other implementations for each STB a leaf shard computes a weight defined as k K where kand Kare the sample size and the total size of the target population in the leaf shard. Next the root shard performs the weighted average among all the leaf shards based on the sample size of each leaf shard. For other implementations of the statistical projection please refer to U.S. Provisional Patent Application No. 61 501 105 and U.S. patent application Ser. No. 12 055 906 each of which is incorporated by reference in its entirety.

Although some of the various drawings illustrate a number of logical stages in a particular order stages that are not order dependent may be reordered and other stages may be combined or broken out. While some reordering or other groupings are specifically mentioned others will be obvious to those of ordinary skill in the art and so do not present an exhaustive list of alternatives. Moreover it should be recognized that the stages could be implemented in hardware firmware software or any combination thereof.

The foregoing description for purpose of explanation has been described with reference to specific implementations. However the illustrative discussions above are not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The implementations were chosen and described in order to best explain principles of the invention and its practical applications to thereby enable others skilled in the art to best utilize the invention and various implementations with various modifications as are suited to the particular use contemplated. Implementations include alternatives modifications and equivalents that are within the spirit and scope of the appended claims. Numerous specific details are set forth in order to provide a thorough understanding of the subject matter presented herein. But it will be apparent to one of ordinary skill in the art that the subject matter may be practiced without these specific details. In other instances well known methods procedures components and circuits have not been described in detail so as not to unnecessarily obscure aspects of the implementations.

