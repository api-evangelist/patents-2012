---

title: 3D layering of map metadata
abstract: Techniques and tools are described for rendering views of a map in which map metadata elements are layered in 3D space through which a viewer navigates. Layering of metadata elements such as text labels in 3D space facilitates parallax and smooth motion effects for zoom-in, zoom-out and scrolling operations during map navigation. A computing device can determine a viewer position that is associated with a view altitude in 3D space, then render for display a map view based upon the viewer position and metadata elements layered at different metadata altitudes in 3D space. For example, the computing device places text labels in 3D space above features associated with the respective labels, at the metadata altitudes indicated for the respective labels. The computing device creates a map view from points of the placed labels and points of a surface layer of the map that are visible from the viewer position.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08681149&OS=08681149&RS=08681149
owner: Microsoft Corporation
number: 08681149
owner_city: Redmond
owner_country: US
publication_date: 20121121
---
This application is a continuation of U.S. patent application Ser. No. 12 842 880 filed Jul. 23 2010 which is hereby incorporated by reference.

Computer aided map navigation tools have achieved widespread acceptance. A user can find an address or directions with map navigation tools available at various Web sites. Some software programs allow a user to navigate over a map zooming in towards the ground or zooming out away from the ground or moving between different geographical positions. In cars GPS devices have provided rudimentary road navigation for years. More recently map navigation software for cellular telephones and other mobile computing devices has allowed users to zoom in zoom out and move around a map that shows details about geographical features town city county and state locations roads and buildings.

Map navigation tools typically present metadata about map features as being baked into a flat two dimensional 2D view of the map. For example in a top down map view text labels are written over road details or image details at appropriate locations and the text labels are effectively presented at the same ground level as the road or image details. This can cause excessive visual complexity due to the density of information that needs to be displayed at any given level of viewing the map.

To reduce the density of detail many map navigation tools hide or reveal metadata depending on the view level of the map. For example if the view is close to a small scale feature such as a building a text label about the feature is revealed but the text label about the feature is hidden if the view is far from the small scale feature. On the other hand a text label about a large scale feature such as a country or state is shown at a high level view but hidden at a view closer to ground level. At any given view level however the revealed text labels are still baked into a flat 2D view of the map. And the transitions between different view levels can be abrupt as one 2D view is replaced with the next 2D view showing different metadata. As a result the viewer can lose context during transitions and become disoriented.

Techniques and tools are described for rendering views of a map in which map metadata elements are layered in three dimensional 3D space through which a viewer navigates. Layering of map metadata elements in 3D space facilitates smooth motion effects for zoom in zoom out and scrolling operations in map navigation. In many cases the techniques and tools help a viewer maintain context through transitions between different view levels which improves the overall experience of using a map navigation tool.

According to one aspect of the techniques and tools described herein a computing device determines a viewer position that is associated with a view altitude in 3D space. The computing device also determines one or more map metadata elements such as text labels that indicate titles distances or other details about features on the map. A map metadata element has a metadata altitude in the 3D space and is associated with a feature of the map e.g. a building street neighborhood city or state . The computing device renders for display a view of a map based at least in part upon the viewer position and layering of map metadata elements at different altitudes in 3D space.

The rendering of a view of the map depends at least in part on how the view altitude for the viewer position relates to the different metadata altitudes of the map metadata elements in 3D space. For example the computing device places metadata text labels in 3D space above features associated with the labels respectively at the metadata altitudes indicated for the labels. The computing device creates the view of the map from points of a surface layer of the map and points of the placed labels that are visible from the viewer position e.g. not obscured by another feature or label within a threshold distance from the viewer position to be rendered with one or more pixels . In some cases the placed labels are parallel with the surface layer in 3D space and below the altitude of the viewer position. In other cases some of the placed labels are perpendicular to the surface layer in 3D space while other labels are parallel to the surface layer in 3D space and above the viewer position.

For navigation as viewer position changes the computing device repeatedly renders map views for viewer positions that can have different view altitudes in 3D space and or different geographical positions e.g. in terms of location at the surface layer . For metadata elements associated with different metadata altitudes in 3D space the elements can appear to be displaced by different distances from view to view so as to provide parallax effects when accounting for a change in geographic position between viewer positions in 3D space.

When transitioning between a top down view and birds eye view of the map the rendering can include placing some metadata elements parallel to a surface layer in 3D space for rendering the top down view but placing the elements perpendicular to the surface layer in 3D space for rendering the birds eye view. At the same time other metadata elements can be placed parallel to the surface layer and above the viewer when rendering the birds eye view. Between birds eye views of the map metadata elements can appear to be rotated and scaled in context with features in the map to account for change between viewer positions in 3D space. Upon switching to a photographic view of a specific feature e.g. building landmark on the map the rendering can present additional metadata text details pertaining to the specific feature.

To provide smooth transitions between views of a map and thereby help the viewer maintain context the computing device can repeatedly determine viewer positions between an initial viewer position and destination viewer position in 3D space and render new views of the map at the new viewer positions. In particular this can facilitate smooth motion effects for zoom in zoom out and scrolling operations in navigation. For example to provide a smooth zoom in effect while transitioning between top down views as view altitude decreases in 3D space a metadata text label appears to become larger and darker as the view altitude approaches a target altitude or distance but appears to become larger and lighter as the view altitude continues to fall below the target altitude or distance resulting in a gradual fading or dissolving effect for the label. As another example to provide a smooth transition between top down and birds eye views of a map a label appears to rotate away from the viewer as view altitude decreases towards the metadata altitude of the label then appears to flip at the metadata altitude of the label.

According to another aspect of the techniques and tools described herein a client computing device and server computing device exchange information to facilitate map navigation. The client computing device sends a request for map information for a map. In some scenarios the request indicates one or more search terms. In other scenarios the request simply indicates a viewer position associated with a view altitude in 3D space. In response the client computing device receives map metadata elements. Individual map metadata elements are associated with individual features of the map. When the client computing device sent search terms the client computing device can also receive a viewer position associated with a view altitude in 3D space. The client computing device then renders map views for example as described above.

Conversely the server computing device receives a request for map information from a client computing device where for example the request indicates one or more search terms or a viewer position associated with a view altitude in 3D space. The server computing device determines one or more map metadata elements with altitudes in 3D space the map metadata elements being usable to render views of the map depending at least in part on how altitude of viewer position relates to the different metadata altitudes of the map metadata elements as layered in 3D space. When the request indicates one or more search terms the server computing device determines the map metadata elements based at least in part on search results for the one or more search terms and can also determine the viewer position based on the one or more search terms. The server computing device sends the one or more map metadata elements and in some cases the viewer position to the client computing device.

To facilitate navigation as viewer position changes the server computing device can receive a second request for map information from the client computing device where for example the second request indicates one or more other search terms or a second viewer position in 3D space. The server computing device can then determine additional map metadata elements with different altitudes in 3D space and send the additional map metadata elements to the client computing device in some cases along with a second viewer position . Typically the first batch of map metadata elements and additional map metadata elements are sufficient for the client computing device to render new views of the map between the first viewer position as an initial position and second viewer position as a destination position and thereby provide smooth transitions between the two viewer positions.

The foregoing and other objects features and advantages of the invention will become more apparent from the following detailed description which proceeds with reference to the accompanying figures.

Techniques and tools are described for rendering views of a map in which map metadata elements are separated from the base layer of the map. The map metadata elements can be layered in 3D space above the base layer for the map. For example map metadata elements are associated with features such as buildings roads towns cities states in the map and the map metadata elements are placed in 3D space above the features with which the map metadata elements are associated at altitudes that depend on the scale of the features lower altitude for buildings and roads higher altitudes for cities and so on . In various scenarios 3D layering of map metadata elements improves the overall experience of using a map navigation tool.

Separating map metadata elements from the base layer of a map and using 3D layering of map metadata elements on top of the map simplify the process of deciding which map metadata elements to reveal or hide for a given task. For example depending on the terms of a search different subsets of map metadata elements are selected for rendering. Moreover altitude values provide a natural hierarchy of metadata elements when deciding which metadata elements to reveal or hide.

3D layering of map metadata elements on a map also facilitates smooth motion effects for zoom in zoom out and scrolling operations as a viewer navigates through 3D space over the map. To help a user maintain context through transitions between different view levels map metadata elements placed in 3D space can appear to be scaled and or rotated in context to the user s view on the map. To provide parallax effects as a user moves over map metadata elements in 3D space map metadata elements can be displaced by different amounts from view to view depending on altitude in 3D space. These and other graphical transformations of map metadata placed in 3D space add visual depth and context for the user during navigation through the map.

In conventional map views presented by map navigation tools the density of metadata presented on a map view can create visual complexity. Selectively hiding and revealing map metadata in different views can help limit the density of metadata presented but transitions between different views are problematic. Techniques and tools described herein layer map metadata elements according to altitude in a 3D space. This 3D layering provides a natural hierarchy of map metadata for rendering. It also provides a straightforward way to relate viewer position to the selection of which map metadata elements to hide or reveal when rendering a map view.

Generally map metadata elements are associated with an altitude depending on the scale of the feature annotated by the map metadata. Large scale features such as countries and big states are associated with higher altitudes. In the highest altitude map metadata is a text label for Washington associated with an altitude of 50 miles. Small scale features such as buildings and small streets are associated with lower altitudes. In the lowest altitude map metadata is a text label for Occidental associated with an altitude of 2000 feet. Map metadata text labels for Jackson St. and King St. are associated with an altitude of 6000 feet and a map metadata text label for Pioneer Square is associated with an altitude of 18000 feet. For progressively larger scale features text labels for International District and Seattle are associated with altitudes of 36000 feet and 20 miles respectively. Typically metadata altitude for a metadata element increases with geographic scale of the feature annotated by the metadata element.

For the sake of simplicity shows only one or two examples of map metadata at each of the indicated altitudes. Real world examples of map metadata will typically include many more labels for features of the map. For example labels for many other streets in the map could be associated with altitudes of 2000 feet and 6000 feet in . On the other hand in some scenarios the map metadata rendered in a map view is deliberately limited to focus on metadata most likely of interest to the viewer. This could be the case for example when map metadata is selected responsive to a search or otherwise selected for a task specific purpose.

As shown in the map metadata can be text labels for place names of states cities neighborhoods and streets. The map metadata can also be text labels for buildings stores within buildings landmarks or other features in the map. Aside from names the map metadata can be distances between features. Or the map metadata can provide additional details for a given feature such as contact information e.g. phone number Web page address reviews ratings other commentary menus photos advertising promotions or games e.g. geo caching geo tagging . Links can be provided for Web pages to launch a Web browser and navigate to information about the feature.

In terms of data organization for storage and transmission an individual element of the map metadata can have an attribute or property for altitude. Assigning an altitude per map metadata element facilitates fine grain specification of different altitudes for different features. Alternatively different map metadata elements are organized by layer such that all map metadata elements are at 2000 feet are organized together all map metadata elements at 6000 feet are organized together and so on. Organizing map metadata by altitude layer can facilitate operations that affect a whole layer of metadata. Regardless of how map metadata are organized for storage and transmission 3D layering of map metadata elements can be part of the rendering process for the metadata elements in a map view.

The number of different altitudes of map metadata and the values used for the different altitudes depend on implementation. To illustrate the concept of different altitudes for map metadata shows six different altitudes that are widely separated but there can be more or fewer different altitudes in 3D space and altitudes can have values different than the ones shown in . In actual use scenarios layers of map metadata will typically have altitudes that are closer together especially in situations when elements from the layers are concurrently rendered for display. The altitudes for map metadata elements can be static or the altitudes can be dynamically adjusted from initially assigned values for the sake of presentation. For example as a user navigates in 3D space the altitude values for map metadata elements can be brought closer together so that parallax effects are more gradual. Concurrently rendering metadata elements with widely spaced altitudes can result in parallax effects that are too abrupt as the viewer changes geographic positions. 

The number of different altitudes for map metadata potentially affects the quality of the user experience. If too few different altitudes are used transitions may become abrupt as many details are revealed or hidden at the same time and a given map view may become crowded with metadata details that suddenly appear. On the other hand having too many different altitudes can result in excessive computational complexity as a computing device determines the appropriate scale sample adjustments etc. for map metadata on an element by element basis from view to view. Further if elements with close altitude values are processed in a markedly different way in rendering decisions the results can be unexpected or disorienting for the user.

In the altitudes of the map metadata are given in terms of feet and miles. The units used to represent altitude values depend on implementation metric units or units in any other measurement system can be used. The altitude values can be real world altitudes above the surface layer of the map or above sea level. Alternatively the altitude values are more abstractly values along a third dimension of a 3D space in which a viewer position has a value indicating placement along the third dimension.

To start the tool determines a viewer position associated with an altitude in 3D space here the view altitude or altitude of the viewer position . For example the viewer position is initially a default viewer position the last viewer position reached in a previous navigation session a previously saved viewer position or a viewer position indicated as the result of a search. In subsequent iterations the viewer position can be a destination viewer position in 3D space or an intermediate viewer position between the previous viewer position and a destination viewer position in 3D space.

The tool renders a view of the map for display. The tool renders the view based on view altitude of the viewer position and layering of map metadata elements in 3D space potentially at metadata altitudes that is the altitudes of the metadata elements that are different for different metadata elements. For example the map metadata elements are text labels at different altitudes in 3D space and the rendered view depends on how the view altitude of the viewer position relates to the different metadata altitudes for the map metadata text labels in 3D space. The tool can get the map metadata elements from the client computing device and or from a server computing device e.g. using the techniques described with reference to and .

The exact operations performed as part of the rendering depend on implementation. In some implementations the tool determines a field of view e.g. a volume that originates at the viewing position and extends towards a surface layer and identifies features of the map that are in the field of view or for distant features have space above them that is in the field of view considering the altitude geographical position and angle of the viewer position. Then for those features the tool selects map metadata elements. This may include any and all of the map metadata elements for the identified features that are potentially visible in the field of view. E.g. within a threshold distance from the viewer position. Or it may include a subset of those potentially visible map metadata elements which are relevant to the navigation scenario e.g. search . The tool places the selected map metadata elements in the 3D space at their respective altitudes above the features marked by the elements and assigns resolutions for the elements depending on metadata altitude and or distance away from the viewer position. For example the assigned resolutions indicate size of the elements and how light or dark the elements are. For example for rendering a top down view of the map the tool places the elements in 3D space parallel to a surface layer of the map. Or for rendering a birds eye view of the map the tool places some elements in 3D space perpendicular to the surface layer at or near the surface layer. For rendering the birds eye view other elements can be placed parallel to the surface layer and above the viewer position to provide the effect of looking up to the elements in the sky. Finally the tool creates the map view from points of the surface layer and points of the placed labels that are visible e.g. not obscured by another feature or label within a threshold distance from the viewer position to be rendered with one or more pixels from the viewer position. For example the tool starts with the surface layer and empty air space then composites metadata elements moving upward from surface layer to the viewer position or moving outward from the viewer position. This stage provides the effects of rotation scaling shrinking towards a perspective point etc. for elements and features when rendering a birds eye view. Alternatively the tool implements the rendering using acts in a different order using additional acts or using different acts.

Although the tool primarily adjusts rendering of map metadata elements in ways that depend on metadata altitudes the tool can also adjust the metadata altitudes themselves before or during rendering. For example the tool adjusts metadata altitudes for map metadata elements to bring the altitudes closer together. If rendered metadata elements are too far apart in altitude a small change in the geographic position of the viewer can cause an extreme apparent displacement of one map metadata element while causing a tiny apparent displacement of another map metadata element. To avoid such abrupt parallax effects the metadata altitudes of elements can be brought closer together while maintaining the relative ordering and relative distances between elements. This results in more gradual changes in scale and apparent positions of metadata elements as the viewer navigates through 3D space.

Generally for the rendering the tool performs various operations on map metadata elements layered in 3D space including rotation scaling adjustment of sample values and selective suppression or addition of map metadata details. and illustrate examples of these and other operations for map metadata text labels. Adjustment of how and when map metadata elements are presented during navigation helps a viewer stay oriented during navigation. and show examples of top down views rendered during navigation through map metadata text labels layered in 3D space in which metadata altitude is the main indicator of which map metadata details to render and how to render such details. show examples of top down views and birds eye views. In the top down views and transitions from top down to birds eye perspective metadata altitude is still considered as a parameter to determine level of detail and scale for map metadata text labels. In the birds eye views the tool also considers distance from the viewer extending away from the viewer towards the horizon to determine level of detail and scale for map metadata text labels.

Returning to the tool determines whether it has reached a destination viewer position for rendering map views. The destination viewer position can be the initial and only viewer position or it can be a different viewer position. If the tool has not reached the destination viewer position the tool repeats the acts of determining and rendering for another viewer position. The other viewer position can differ from the previous viewer position in terms of geographical position over a 2D surface and or view altitude in 3D space. In this way the tool can repeat the acts of determining and rendering for first second third etc. viewer positions in succession or the tool can keep the current map view.

The tool can react to user input that indicates a change in viewer position. For example the user input is gesture input from a touchscreen or keystroke input from a keyboard. In this situation the tool determines a new viewer position indicated by the input or a new viewer position to transition towards a destination indicated by the input and renders a view for that new viewer position. In particular to provide smooth motion effects when zooming between altitudes and or scrolling over geographic positions from an initial viewer position to the destination viewer position the tool can repeat the determining and rendering acts for viewer positions between the initial and destination viewer positions. The number of map views rendered for smooth motion effects depends on implementation. For example 4 map views per second are rendered. During this transition towards a given destination viewer position the tool can assign a new destination viewer position if interrupted e.g. by user input indicating the new destination viewer position .

The architecture includes a device operating system and map navigation framework . The device OS manages user input functions output functions storage access functions network communication functions and other functions for the device. The device OS provides access to such functions to the map navigation framework .

A user can generate user input that affects map navigation. The device OS includes functionality for recognizing user gestures and other user input and creating messages that can be used by map navigation framework . The interpretation engine of the map navigation framework listens for user input event messages from the device OS . The UI event messages can indicate a panning gesture flicking gesture dragging gesture or other gesture on a touchscreen of the device a tap on the touchscreen keystroke input or other user input e.g. voice commands directional buttons trackball input . The interpretation engine translates the UI event messages into map navigation messages sent to a positioning engine of the map navigation framework .

The positioning engine considers a current viewer position possibly provided as a saved or last viewer position from the map settings store any messages from the interpretation engine that indicate a desired change in viewer position and map metadata with different metadata altitudes in 3D space. From this information the positioning engine determines a viewer position in 3D space. The positioning engine provides the viewer position as well as map metadata in the vicinity of the viewer position to the rendering engine .

The positioning engine gets map metadata for a map from a map metadata store . The map metadata store caches recently used map metadata. As needed the map metadata store gets additional or updated map metadata from local file storage or from network resources. The device OS mediates access to the storage and network resources. The map metadata store requests map metadata from storage or a network resource through the device OS which processes the request receives a reply and provides the requested map metadata to the map metadata store . In some scenarios the request for map metadata takes the form of a search query and map metadata responsive to the search is returned.

The rendering engine processes the viewer position and map metadata in 3D space and renders a map view. Depending on the use scenario the rendering engine can render map metadata from local storage map metadata from a network server or a combination of map metadata from local storage and map metadata from a network server. The rendering engine provides display commands for the rendered map view to the device OS for output on a display.

In the altitude of the viewer position has decreased as the viewer zooms in towards Seattle. In the map view of the text label for Washington is larger but fainter as the altitude of the viewer position approaches the altitude associated with the Washington label. Compared to the label for Seattle is larger and darker in the map view of

As the viewer continues to zoom towards Seattle the size of the label for Seattle increases in the map views of and . Below a target altitude associated with the label here not the altitude in 3D space at which the label is placed or target distance from the label the label for Seattle becomes successively lighter. Eventually the label for Seattle is displaced out of the map view or the label dissolves if the viewer position passes directly through it. shows a top down map view even closer to the surface layer of the map as the altitude of the viewer position continues to decrease. In the labels for Washington and Seattle have zoomed past the field of view and metadata details within the city have zoomed into view.

In summary for the zoom in operation illustrated in a metadata text label appears small then successively larger and darker sharper between views. After reaching a peak darkness sharpness the text label continues to grow between views but also fades dissolves or floats out of view.

For a zoom out operation rendering of map metadata text labels generally mirrors the rendering for zoom in operations. show example map views as view altitude increases during navigation through map metadata text labels at different altitudes in 3D space. In the map view shows metadata text labels such as the label for Eastlake which shrink in size from view to view as the viewer zooms out from the surface layer. As the viewer zooms out through map views in labels at higher altitudes are brought into view and rendered in large size from view to view and labels at lower altitudes in a wider geographic area are brought into view and rendered in small size. When directly passing through a label e.g. the label for Washington the label is at first faint but becomes darker from view to view as altitude increases over the label.

The map view of shows a metadata text label for Montlake for example that appears over a highway. As the viewer moves north northeast the position of the label for Montlake appears to move south southwest between the map views of . Some other metadata text labels exhibit parallax effects to a greater or less extent. For example the metadata text label for Capitol Hill is displaced entirely out of view by the rendered map view of . Text labels associates with low altitudes which are further from the viewer are displaced by smaller distances between the rendered map views.

Generally when geographic position of the viewer changes the distances that different metadata text labels are displaced between the rendered top down views depend on the metadata altitudes associated with the labels relative to the view altitude. Labels close to the viewer are displaced more between map views and labels further from the viewer closer to the ground are displaced less between map views.

In addition parallax effects can appear between views when altitude changes. In for example the label for Queen Anne appears to shift due to change in perspective for the viewer position as view altitude increases. As view altitude changes labels closer to the sides are displaced more between map views and labels in the middle closer to directly under the viewer are displaced less between map views.

As view altitude decreases a metadata text label close to the viewer such as the text label for Pioneer Square is rotated away from the viewer from view to view as shown in the transitional views of and . As the view altitude passes below the altitude of the label in 3D space see the view of the text orientation of the label is flipped between views so that the top of the text label is close to the viewer instead of far from the viewer. Eventually the text label is rendered consistently with an orientation that is parallel to the surface layer of the map in 3D space with the effect that the text label floats in the air space above the surface of the map in rendered map views.

The text labels for streets the labels being parallel to the surface layer in 3D space for rendering of top down views dissolve between views of and . The text labels re materialize in views of and after being placed perpendicular to the surface layer in 3D space. Placing the street labels perpendicular to the map surface and just above the map surface aids viewing of the street names after the labels are rendered in birds eye views. Generally the street labels also run parallel to the streets they mark respectively. As shown in other labels such as the label for Pioneer Square can remain parallel to the surface layer in 3D space even for rendering of birds eye views.

In the birds eye views of text labels are rotated and scaled along with map features as the viewer position changes in terms of view altitude and or geographic position. In addition the text labels shrink towards a distant perspective point away from the viewer. The rotation and scaling as viewer position changes cause parallax effects even though the text labels do not move in 3D space they appear to move between the rendered views. Generally in text labels that are perpendicular to the surface map run parallel in 3D space to the features they mark and this parallel orientation in 3D space is maintained by rotation from view to view as the viewer position changes.

For rendering in the birds eye view the size of the metadata text labels in a view and the level of metadata detail to reveal in the view depend on distance away from the viewer. The level of detail and size can also depend on other factors e.g. minor street vs. major street metadata element relevant to search result or not .

The viewer can transition to a photographic view for example by selecting an individual feature of the map. Or the viewer can transition to a photographic view by navigating directly into a specific feature on the map.

To start the client computing device determines a request for map information. The request can include a viewer position in 3D space or the request can include one or more search terms for a search that can yield a viewer position in 3D space. The client computing device sends the request to a server computing device.

The server computing device receives a request for map information for a map e.g. specifying a viewer position in 3D space or specifying search terms responsive to which a viewer position in 3D space may be identified . From information in the request the server computing device determines map metadata elements with different metadata altitudes in 3D space. For example the server computing device finds map metadata elements visible from a viewer position specified in the request using metadata altitude as a control parameter for which elements are visible. Or the server computing device finds a viewer position from search results for search term s specified in the request finds map metadata elements from the search results and selects some or all of those map metadata elements which are visible from the viewer position using metadata altitude as a level of detail control parameter for the search results. The map metadata elements are usable to render views of the map depending at least in part on how altitude of viewer position relates to the different altitudes of the map metadata elements as layered in the 3D space according to their respective metadata altitudes.

The server computing device sends the map metadata elements to the client computing device. The server computing device can receive a second request for map information from the same client computing device. The server computing device then determines additional map metadata elements and sends the additional elements to the client computing device. In some cases the initial and additional map metadata elements are sufficient for the client computing device to render a new view of the map for each of multiple new viewer positions between the first viewer position and a second viewer position.

Returning to the client computing device receives the map metadata from the server computing device and renders one or more views of the map based upon view altitude of viewer position and metadata altitudes of the map metadata elements in 3D space. For example the client computing device renders views according to the method of or another method. When rendering the map metadata elements received from the server computing device the client computing device can combine map metadata elements from other server computing devices and or the client computing device itself.

The server computing device when providing map metadata or client computing device when placing map metadata elements can rank map metadata elements based on what the user is likely to want. For a search the server computing device can give higher rank to metadata more likely to be useful to viewer so that such metadata is revealed earlier and or given more prominence. For example if the search seeks information about a specific kind of restaurant or shop the server computing device assigns selected metadata for them higher rank than metadata for other restaurants shops and also increases the rank of the selected metadata relative to other kinds of map metadata e.g. for streets . Or if the search seeks information about a particular street the server computing device increases the rank of metadata for streets and directions relative to other kinds of metadata. This makes the increased rank metadata appear sooner and or with more prominence during scenario based rendering that combines the search results with 3D layering of map metadata.

Although and show acts of a single client computing device and single server computing device one server computing device can service requests from multiple client computing devices. Moreover a given client computing device can select between multiple server computing devices. Server processing can be stateless or a server computing device can remember state information for specific client computing devices from request to request.

The illustrated mobile device can include a controller or processor e.g. signal processor microprocessor ASIC or other control and processing logic circuitry for performing such tasks as signal coding data processing input output processing power control and or other functions. An operating system can control the allocation and usage of the components and support for one or more application programs . In addition to map navigation software the application programs can include common mobile computing applications e.g. email applications calendars contact managers web browsers messaging applications or any other computing application.

The illustrated mobile device can include memory . Memory can include non removable memory and or removable memory . The non removable memory can include RAM ROM flash memory a hard disk or other well known memory storage technologies. The removable memory can include flash memory or a Subscriber Identity Module SIM card which is well known in GSM communication systems or other well known memory storage technologies such as smart cards. The memory can be used for storing data and or code for running the operating system and the applications . Example data can include web pages text images sound files video data or other data sets to be sent to and or received from one or more network servers or other devices via one or more wired or wireless networks. The memory can be used to store a subscriber identifier such as an International Mobile Subscriber Identity IMSI and an equipment identifier such as an International Mobile Equipment Identifier IMEI . Such identifiers can be transmitted to a network server to identify users and equipment.

The mobile device can support one or more input devices such as a touch screen microphone camera e.g. capable of capturing still pictures and or video images physical keyboard and or trackball and one or more output devices such as a speaker and a display . Other possible output devices not shown can include piezoelectric or other haptic output devices. Some devices can serve more than one input output function. For example touchscreen and display can be combined in a single input output device.

A wireless modem can be coupled to an antenna not shown and can support two way communications between the processor and external devices as is well understood in the art. The modem is shown generically and can include a cellular modem for communicating with the mobile communication network and or other radio based modems e.g. Bluetooth or Wi Fi . The wireless modem is typically configured for communication with one or more cellular networks such as a GSM network for data and voice communications within a single cellular network between cellular networks or between the mobile device and a public switched telephone network PSTN .

The mobile device can further include at least one input output port a power supply a satellite navigation system receiver such as a Global Positioning System GPS receiver an accelerometer a transceiver for wirelessly transmitting analog or digital signals and or a physical connector which can be a USB port IEEE 1394 FireWire port and or RS 232 port. The illustrated components are not required or all inclusive as any components can deleted and other components can be added.

The mobile device can implement the technologies described herein. For example the processor can determine viewer positions and render views of a map during map navigation in 3D space. The processer can also process user input to determine changes in viewer position. As a client computing device the mobile computing device can send requests to a server computing device and receive map metadata in return from the server computing device.

In example environment the cloud provides services for connected devices with a variety of screen capabilities. Connected device represents a device with a computer screen e.g. a mid size screen . For example connected device could be a personal computer such as desktop computer laptop notebook netbook or the like. Connected device represents a device with a mobile device screen e.g. a small size screen . For example connected device could be a mobile phone smart phone personal digital assistant tablet computer and the like. Connected device represents a device with a large screen. For example connected device could be a television screen e.g. a smart television or another device connected to a television e.g. a set top box or gaming console or the like. One or more of the connected devices can include touch screen capabilities. Devices without screen capabilities also can be used in example environment . For example the cloud can provide services for one or more computers e.g. server computers without displays.

Services can be provided by the cloud through service providers or through other providers of online services not depicted . For example cloud services can be customized to the screen size display capability and or touch screen capability of a particular connected device e.g. connected devices . The service providers can provide a centralized solution for various cloud based services. The service providers can manage service subscriptions for users and or devices e.g. for the connected devices and or their respective users .

In example environment the 3D map navigation techniques and solutions described herein can be implemented with any of the connected devices as a client computing device. Similarly any of various computing devices in the cloud or for a service provide can perform the role of server computing device and deliver map metadata to the connected devices .

Although the operations of some of the disclosed methods are described in a particular sequential order for convenient presentation it should be understood that this manner of description encompasses rearrangement unless a particular ordering is required by specific language set forth below. For example operations described sequentially may in some cases be rearranged or performed concurrently. Moreover for the sake of simplicity the attached figures may not show the various ways in which the disclosed methods can be used in conjunction with other methods.

Any of the disclosed methods can be implemented as computer executable instructions or a computer program product stored on one or more computer readable storage media e.g. non transitory computer readable media such as one or more optical media discs such as DVD or CD volatile memory components such as DRAM or SRAM or nonvolatile memory components such as hard drives and executed on a computer e.g. any commercially available computer including smart phones or other mobile devices that include computing hardware . Any of the computer executable instructions for implementing the disclosed techniques as well as any data created and used during implementation of the disclosed embodiments can be stored on one or more computer readable media e.g. non transitory computer readable media . The computer executable instructions can be part of for example a dedicated software application or a software application that is accessed or downloaded via a web browser or other software application such as a remote computing application . Such software can be executed for example on a single local computer e.g. any suitable commercially available computer or in a network environment e.g. via the Internet a wide area network a local area network a client server network such as a cloud computing network or other such network using one or more network computers.

For clarity only certain selected aspects of the software based implementations are described. Other details that are well known in the art are omitted. For example it should be understood that the disclosed technology is not limited to any specific computer language or program. For instance the disclosed technology can be implemented by software written in C Java Perl JavaScript Adobe Flash or any other suitable programming language. Likewise the disclosed technology is not limited to any particular computer or type of hardware. Certain details of suitable computers and hardware are well known and need not be set forth in detail in this disclosure.

Furthermore any of the software based embodiments comprising for example computer executable instructions for causing a computing device to perform any of the disclosed methods can be uploaded downloaded or remotely accessed through a suitable communication means. Such suitable communication means include for example the Internet the World Wide Web an intranet software applications cable including fiber optic cable magnetic communications electromagnetic communications including RF microwave and infrared communications electronic communications or other such communication means.

The disclosed methods apparatus and systems should not be construed as limiting in any way. Instead the present disclosure is directed toward all novel and non obvious features and aspects of the various disclosed embodiments alone and in various combinations and sub combinations with one another. The disclosed methods apparatus and systems are not limited to any specific aspect or feature or combination thereof nor do the disclosed embodiments require that any one or more specific advantages be present or problems be solved. In view of the many possible embodiments to which the principles of the disclosed invention may be applied it should be recognized that the illustrated embodiments are only preferred examples of the invention and should not be taken as limiting the scope of the invention. Rather the scope of the invention is defined by the following claims. We therefore claim as our invention all that comes within the scope and spirit of these claims.

