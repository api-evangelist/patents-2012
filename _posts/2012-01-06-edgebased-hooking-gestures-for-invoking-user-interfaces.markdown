---

title: Edge-based hooking gestures for invoking user interfaces
abstract: Presented herein are techniques for invoking user interfaces in graphical computing environments involving “hooking” gestures applicable to an edge of a display. These gestures involve positioning a pointer near an edge of the display depicting the environment, and then moving the cursor to a second location that is distanced from but near the first location (e.g., moving the pointer into a corner of the display, and then toward the center of the display, or to a midpoint of an adjacent edge of the display), resulting in the presentation of the user interface under the pointer at the second location. Variations include presenting a preview of the user interface (e.g., a subset of a user interface menu) at the second location while the pointer is in the first location, and refraining from presenting the user interface upon failing to complete the gesture or receiving extraneous input before completing the gesture.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09141262&OS=09141262&RS=09141262
owner: Microsoft Technology Licensing, LLC
number: 09141262
owner_city: Redmond
owner_country: US
publication_date: 20120106
---
Within the field of computing many scenarios involve the presentation of user interfaces within a graphical computing environment such as windows icons menu bars toolbars and widgets as well as a background region such as a desktop presented as a backdrop for the graphical computing environment. In particular such user interfaces may present various types of visual controls for interacting with the graphical computing environment such as adjustable properties of the environment adjustable settings of respective applications and properties of regions associated with the applications e.g. opening closing and repositioning windows for respective applications . A user may manipulate a pointing device e.g. a mouse trackball touchpad drawing tablet stylus or touchscreen device to manipulate a pointer within the graphical environment in order to interact with the user interfaces presented therein.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key factors or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

Although the presentation of visual controls within the graphical computing environment may enable various operations for respective applications or the environment such presentation may consume valuable space on the display of the device. In some scenarios such consumption may be regarded as an inefficient use of display space that diminishes the amount of display space usable by the application. For example settings may be accessible through interaction with a visual control such as a button or menu but such settings may be infrequently or rarely accessed by the user and it may not be desirable to present such visual controls in a continuous manner that reduces the size of the display used to present other aspects of the graphical computing environment such as the output of the applications. Moreover such visual controls are often presented in a large non interactive display element such as a menu bar displayed along the entire top width of the region where only a small portion of the menu bar is used to display a small set of options. Therefore it may be desirable to reduce the amount of chrome e.g. attached visual controls and embedding visual elements presented within the graphical computing environment in order to maximize the area of display space that is available for other uses. However it is still desirable to enable the user to access the operations for which the visual elements may have been presented.

Presented herein are techniques for enabling users to invoke a presentation of a user interface within a graphical computing environment through the detection of gestures performed with the pointer. Such techniques may particularly useful when implemented in a chromeless manner e.g. where the graphical computing environment refrains from indicating the availability of the user interface before the start of the performance of the gesture. These techniques involve detecting a positioning of the pointer at a first location near an edge of the display followed by a movement of the pointer toward a second location that is located near but at a distance from the first location e.g. the first location comprising a corner of the display adjoining two edges and the second location comprising a midpoint of one of the edges . This gesture may invoke a presentation of a user interface region of the user interface such that the user interface region that is positioned near the first location but that does not include the first location such that the user interface does not occlude other visual controls that may be positioned at the first location e.g. other visual controls positioned in the corner of the display . Additionally upon detecting a hovering of the pointer at the first location the device may present a preview of the user interface at the second location thus encouraging the user to move the pointer toward the second location where the full user interface is presented.

The configuration of the device to detect such gestures as requests to invoke the user interface may present several advantages. As a first example the edges of a display and even more so the corners of the display are often easily accessible to a user according to some consequences of Fitts Law and a user may rapidly invoke such gestures and possibly without regard to the current position of the pointer and even without looking at the display. As a second example such gestures are not currently associated with any known functionality and users may be easily and intuitively trained to utilize such gestures to invoke the user interface. As a third example such gestures may be detectable with high accuracy a low degree of ambiguity and a comparatively simple analytic process as contrasted with gestures that involve complex shape analysis such as the drawing of symbols such as letters . These and other advantages may be achievable through the implementation of the techniques presented herein.

To the accomplishment of the foregoing and related ends the following description and annexed drawings set forth certain illustrative aspects and implementations. These are indicative of but a few of the various ways in which one or more aspects may be employed. Other aspects advantages and novel features of the disclosure will become apparent from the following detailed description when considered in conjunction with the annexed drawings.

The claimed subject matter is now described with reference to the drawings wherein like reference numerals are used to refer to like elements throughout. In the following description for purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of the claimed subject matter. It may be evident however that the claimed subject matter may be practiced without these specific details. In other instances structures and devices are shown in block diagram form in order to facilitate describing the claimed subject matter.

Within the field of computing many scenarios involve a graphical computing environment comprising various user interfaces such as windows menu bars toolbars notification dialogs icons representing applications or documents and widgets embedded in a desktop. Such graphical computing environments may be generated on a device e.g. by first rendering a background color or background image by rendering one or more user interfaces for each application executing on the device and by presenting the rendering on a display component of the device such as a monitor. Some user interfaces may be associated with an application executing on the device and may be configured to present visual output from the application and or convey input through the user interface to the application. Such applications may include user applications renderings of media objects configuration dialogs games operating system applications e.g. a file explorer and graphical interfaces for background service e.g. a notification icon stored in a notification icon area .

A user may be permitted to interact with the user interfaces presented in the graphical computing environment. As one such example the device may comprise a pointing device such as a mouse a trackball a trackpad a drawing tablet a touchscreen a stylus a targeting element such as an infrared emitter and or a camera configured to detect a manual pointing gesture of a user at the display component. These pointing devices may be used to control a pointer presented on the display e.g. represented by a cursor such as an arrow and the user may utilize the pointer to select activate and interact with various user interfaces e.g. the user may slide a mouse on a flat surface to achieve corresponding movement of the pointer may position the pointer over a user interface of interest and may click a button on the mouse to indicate a selection of the user interface. The device may be configured to support many such operations through the pointing device e.g. clicking with one or more particular buttons double clicking dragging an item such as an icon or creating a marquee or lasso to select an area or items of interest . The device may also evaluate the input of the pointing device to identify gestures performed with the pointer e.g. a shaking of the cursor and or of the pointing device e.g. a single or multi touch gesture performed on a touchpad that is to be processed in lieu of corresponding movements on the touchpad . Such manipulations may enable the execution of many types of activities including selecting and or unselecting items invoking switching to or terminating particular applications and zooming into or out of a portion of the graphical computing environment.

In addition to operations invoked solely through the input of the pointing device pointer based user interfaces are frequently used in conjunction with various types of visual controls embedded in the user interface such as depressable buttons selectable radio buttons checkboxes sliders lists numeric up down controls menus and pickers. For example a window of an application is often presented with a visual frame a set of buttons at a top corner of the window for frequently invoked window management functions such as maximizing restoring minimizing and closing an application and a top menu bar providing a set of invokable operations exposed by the application. These controls may be applied by default to the window by the graphical computing environment or may be specified by the application associated with the window and may be positioned in an independent floating detachable dockable and or attached manner with respect to the window. Some familiar examples involve techniques for achieving a repositioning of a window on the display. Such repositioning may involve e.g. altering the location of a window on the display resizing the window switching the window to a particular view mode e.g. a maximized fullscreen windowed or hidden view mode and moving the window to a different display e.g. from a first monitor to a second monitor connected to the same device from a first virtual display space to a second virtual display space of the same device or to a display component of a different device . Because windows are frequently repositioned in window based graphical computing environments such devices often provide several ways to invoke the repositioning of a window. For example the maximize restore and minimize buttons attached to a window enable the repositioning of the window at predefined sizes and locations the graphical frame of a window enables a user to position the pointer over an edge of the window and drag to adjust the size of the window and menu options in a standard application menu enabling the maximizing restoring and minimizing of the region. Additional input options may be included to reposition the window without the use of a visual repositioning control e.g. keyboard shortcuts may be provided to invoke the maximize restore and minimize commands and a throw pointer gesture may be implemented to enable a user to grab a window with the pointer and rapidly move the pointer to a particular location of the display in order to throw it into that location.

However the implementation of visual controls for respective windows may consume space within the graphical computing environment . For example in the exemplary scenario of a significant amount of the display space is consumed by visual controls such as the top menu bar the application launcher button the options button and the visual controls embedded in the window . Moreover some of these elements are embedded in non interactive visual elements in order to provide visual alignment and or consistency among windows e.g. a menu bar may span the entire width of a window but may be only partially utilized by embedded visual controls and the remainder may present non interactive and unhelpful visual elements within the graphical computing environment . Moreover the functionality provided by a visual control may be redundant with other visual controls may be invoked only occasionally or rarely despite the ubiquitous presentation of the visual control and may be handled entirely by the logic of the application e.g. the application may present an option within the window to exit the application at an appropriate time thereby rendering a close button irrelevant and confusing .

These and other considerations may encourage a reduction in the amount of redundant infrequently invoked and or non functional visual elements chrome within a graphical computing environment . However users may nevertheless wish to invoke the functionality provided by the omitted visual controls . Therefore it may be desirable to design the graphical computing environment to provide such functionality other than through the use of chrome. One such technique is the implementation of gestures whereby a particular motion of the pointer may be associated with a particular functionality such as the marquee or lasso gesture whereby the drawing of a rectangle enclosing a set of items in a list results in a selection of the enclosed items or double clicking a button on a pointing device to activate an item indicated by the pointer . Moreover it may be desirable to devise gestures that may be distinctively entered by the user that may be efficiently and accurately distinguished by computational analysis from other gestures and other forms of input and that are intuitively associated with the invoked functionality e.g. a recognizable multi touch gesture may involve a spreading or pinching of two locations may be intuitively associated respectively with a zoom in operation or a zoom out operation . The configuration of the device to recognize a distinctive set of gestures each enabling access to frequently invoked functionality in an unambiguous manner may enable a considerable reallocation of the display space of the graphical computing environment from chrome to the output of applications .

Presented herein are techniques for facilitating the invocation of a user interface through gestures that are not presently associated with other functionality that are readily distinguished from other gestures and forms of input and that are intuitively associated with the associated functionality of invoking a user interface . In particular it may be desirable to devise a gesture that is associated with a location near an edge of a display of the device which according to Fitts Law is readily accessible to users . However due to this advantageous accessibility many graphical computing environments may position the user interface near an edge of the display and or may implement gestures or other forms of input with respect to an edge. These alternate uses of the edge of a display may cause two undesirable consequences first the gesture may already be associated with other forms of input that are associated with the same edge second presenting the user interface at the first location where the gesture begins may result in an overlapping of other visual elements that are also presented near the same edge. Moreover it may be further desirable to present the invoked user interface near the edge only after the gesture is completed because presenting the user interface near the first location before the gesture is completed may result in an overlapping of other visual elements near the edge even if the user did not intend to invoke the user interface .

For at least these reasons it may be desirable to devise gestures that conclude at a second location that is near but distanced from the first location where the gesture is initiated where the movement of the pointer from the first location to the second location may unambiguously identify a request to invoke the user interface . As an additional option while the invoked user interface may be presented only after the gesture is completed the graphical computing environment may present a preview of the user interface at the second location. This preview presentation may both suggest the availability of the user interface to the user when the gesture is commenced and may prompt the user with the movement to complete the gesture. Moreover the presentation at the second location near but distanced from the first location near the edge of the display may avoid overlapping visual elements presented at the first location particularly before the intent of the user to invoke the user interface is unambiguously established through the completion of the gesture.

Still another embodiment involves a computer readable medium comprising processor executable instructions configured to apply the techniques presented herein. Such computer readable media may include e.g. computer readable storage media involving a tangible device such as a memory semiconductor e.g. a semiconductor utilizing static random access memory SRAM dynamic random access memory DRAM and or synchronous dynamic random access memory SDRAM technologies a platter of a hard disk drive a flash memory device or a magnetic or optical disc such as a CD R DVD R or floppy disc encoding a set of computer readable instructions that when executed by a processor of a device cause the device to implement the techniques presented herein. Such computer readable media may also include as a class of technologies that are distinct from computer readable storage media various types of communications media such as a signal that may be propagated through various physical phenomena e.g. an electromagnetic signal a sound wave signal or an optical signal and in various wired scenarios e.g. via an Ethernet or fiber optic cable and or wireless scenarios e.g. a wireless local area network WLAN such as WiFi a personal area network PAN such as Bluetooth or a cellular or radio network and which encodes a set of computer readable instructions that when executed by a processor of a device cause the device to implement the techniques presented herein.

An exemplary computer readable medium that may be devised in these ways is illustrated in wherein the implementation comprises a computer readable medium e.g. a CD R DVD R or a platter of a hard disk drive on which is encoded computer readable data . This computer readable data in turn comprises a set of computer instructions configured to operate according to the principles set forth herein. In one such embodiment the processor executable instructions may be configured to perform a method of presenting a user interface within a graphical computing environment such as the exemplary method of . Some embodiments of this computer readable medium may comprise a nontransitory computer readable storage medium e.g. a hard disk drive an optical disc or a flash memory device that is configured to store processor executable instructions configured in this manner. Many such computer readable media may be devised by those of ordinary skill in the art that are configured to operate in accordance with the techniques presented herein.

The techniques discussed herein may be devised with variations in many aspects and some variations may present additional advantages and or reduce disadvantages with respect to other variations of these and other techniques. Moreover some variations may be implemented in combination and some combinations may feature additional advantages and or reduced disadvantages through synergistic cooperation. The variations may be incorporated in various embodiments e.g. the exemplary method of to confer individual and or synergistic advantages upon such embodiments.

A first aspect that may vary among embodiments of these techniques relates to the scenarios wherein such techniques may be utilized.

As a first variation of this first aspect these techniques may be utilized with many types of devices such as servers workstations laptop or palmtop computers tablets phones personal data assistants media players game consoles and appliances.

As a second variation of this first aspect these techniques may be utilized with many types of displays such as active or passive matrix liquid crystal displays LCD organic or traditional light emitting diode LED displays cathode ray tube CRT displays and projectors. Such displays may also exhibit various features including various physical sizes aspect ratios resolutions pixel densities adjustable properties and three dimensional image simulation. The device may also have one or multiple displays arranged in various ways e.g. an aggregate desktop presented seamlessly across the displays a set of distinct and isolated display spaces presented on different displays or a mirrored desktop that is partially or wholly redundantly presented on several displays . Additionally in some variations a first device may apply the techniques presented herein for windows presented on a display of a second device e.g. a terminal services client presenting on a display the graphical computing environment generated by a terminal server .

As a third variation of this first aspect these techniques may be utilized with many types of pointing devices such as a mouse trackball trackpad pointing stick joystick or drawing tablet. The pointing device may also be incorporated in the display such as a magnetic or capacitative touch sensitive display capable of detecting touch by a finger of a user or by a stylus. Other pointing devices may include a camera and may correlate movements of the pointer with movements of a user e.g. applying image evaluation techniques to detect the body position and physical gestures of the user or a pointing device such as an infrared emitter. Such pointing devices may also include various properties that may affect the pointer such as sensitivity acceleration the inclusion of buttons and or wheels the detection of various axes of movement possibly including a third dimensional axis and or gyroscopic sensors detecting device attitude such as tilt and sensitivity to the touch of the user as in a touch sensitive mouse. As a further variation such pointing devices may generally correlate with the pointer in various modes such as an absolute correlation e.g. on a touch sensitive display the location touched by the user is the location of the point or on a touchpad the size and dimensions of the touchpad may be scaled to the size and dimensions of the display such that touching a particular location on the touchpad positions the pointer at the corresponding scaled position on the display and a relative correlation e.g. input from a mouse or touchpad may be interpreted not as a location of the pointer on the display but as directional motion of the pointer within the graphical computing environment .

As a fourth variation of this first aspect these techniques may be utilized to invoke many types of user interfaces . As a first such example the user interfaces may be associated with the graphical computing environment and or one or more applications . As a second such example the user interfaces may include many types of visual elements such as modal or modeless windows interactive and non interactive dialogs toolbars menu bars renderings of media objects such as documents and images icons and widgets embedded in a desktop. Those of ordinary skill in the art may devise many scenarios wherein the techniques presented herein may be utilized.

A second aspect that may vary among embodiments of these techniques relates to variations in the gestures detectable by such techniques.

As a first variation of this second aspect the gestures may be associated with particular edges of a display . As a first such example the first location may comprise a location near a corner of the display adjoining two selected edges of the display and the second location may comprise a midpoint of a selected edge or vice versa . As a second such example the first location may comprise a location near an edge of the display and the second location may comprise a location between the first location and the center of the display .

As a second variation of this second aspect the gesture may comprise additional forms of input combined with the positioning of the pointer in the first location and the movement of the pointer to the second location . As a first such example the gesture may involve hovering the pointer at the first location and or the second location . As a second such example the gesture may involve activating the pointer at the first location and or the second location and or therebetween e.g. clicking a button of a mouse at the first location and or the second location or performing a drag operation from the first location to the second location . As a third such example the gesture may also involve other forms of input through other input devices such as depressing a key on a keyboard while performing the gesture.

As a third variation of this second aspect the graphical computing environment may visually indicate the availability of the gesture in various ways. As a first such example the graphical computing environment may present a visual indicator at the first location of the user interface or the availability of the gesture such as an interactive or non interactive target. Alternatively the graphical computing environment may refrain from presenting visual indicators e.g. a chromeless implementation of the gesture . As a second such example between the initiation of the gesture at the first location and the completion of the gesture at the second location the graphical computing environment may present a visual indication of the second location in order to encourage the completion of the gesture or may refrain from presenting any such visual indication in order to promote the chromelessness of the graphical computing environment .

As a particular example of this third variation of this second aspect and as illustrated in the exemplary scenario of the visual indication of the second location may comprise a preview of the user interface such as a portion of the user interface e.g. a textual or pictorial description of the user interface a subset of visual controls of the user interface that indicate the type of functionality exposed by the user interface a non interactive version of the user interface such as a thumbnail or a partially transparent version of the user interface and or a presentation of the visual controls of the user interface without an isolating background of the user interface . As the movement of the pointer from the first location to the second location is detected the preview may be replaced with the user interface . As a further example if the user interface comprises a user interface region extending to the edge of the display the preview may be positioned at the second location such that it does not extend to the edge of the display thereby avoiding an occlusion of any other visual elements that are positioned near the edge of the display . As an additional option the preview may only be presented after detecting a hovering of the pointer within the first location measured as a hover duration from the arrival of the pointer at the first location . This option may inform an inexperienced user of the availability of the user interface and the completion of the associated gesture while also enabling an experienced user to invoke the user interface without having to hover the pointer at the first location or view the preview .

As a fourth variation of this second aspect the detection of the gesture within the graphical computing environment may be performed in many ways. As a first such example the graphical computing environment may endeavor to detect an initiation of the gesture by while not in the user interface suggestion mode continuously comparing the location of the pointer with the first location of the gesture and while in the user interface suggestion mode continuously evaluating pointer input of the pointer to detect a movement to the second location . As a second such example the graphical computing environment may not continuously monitor the pointer input of the pointer but may include a trigger that when activated indicates input correlated with the gesture. For example the graphical computing environment may be particularly configured to display a set of regions e.g. windows and may additionally include a window covering the area of the first location and having no visual appearance. The graphical computing environment may already be generally configured to detect the entry of a pointer into the areas of respective windows and to notify such windows of such pointer input. The non visible window positioned at the first location may interpret notification of the entry of the pointer into the area of the window as an initiation of the gesture. Moreover the non visible window may send the pointer input to any visual controls near e.g. below the window in case such pointer input is intended for such underlying visual controls and not as part of the gesture.

As a fifth variation of this second aspect if the user does not complete an initiated gesture the graphical computing environment may construe the input as an inadvertent initiation of the gesture and a cancellation of the user interface suggestion mode . As a first such example upon failing to detect the movement of the pointer to the second location within a gesture completion duration from entering the user interface suggestion mode the graphical computing environment may cancel the user interface suggestion mode . As a second such example upon detecting input other than the movement of the pointer to the second location while in the user interface suggestion mode e.g. movement of the pointer away from the second location an activation of the pointer such as a depression of a mouse button or input from another input device such as a keyboard the graphical computing environment may cancel the user interface suggestion mode .

As a sixth variation of this second aspect the details of such gestures may be adjusted for execution on a device having two or more displays . In many such scenarios the displays may be logically arranged to reflect a single logical display. For example a logical display may comprise a first logical display space and a second logical display space that are logically adjacent such that moving the pointer past the right edge of the first logical display space causes the pointer to emerge from the left edge of the second logical display space. The arrangement of the logical display spaces within the logical display may or may not be consistent with the physical arrangement of such displays e.g. while it may be advantageous to position a first display presenting the first logical display space adjacent to and to the left of a second display presenting the second logical display space the displays may be located in other physical arrangements such as the first display above below or to the right of the second display . In relation to the techniques presented herein in multi display scenarios an edge of a logical display space may operate not as a hard boundary that resists overshooting of the pointer past the edge but rather as a portal to a second logical display space such that even minimal overshooting of the first location causes the pointer to move past the first location and onto a second logical display space. Accordingly the edge may be less readily accessible due to Fitts Law and the gesture may be adapted to improve the accessibility of the associated functionality.

As a first example of this sixth variation the gesture may be adjusted in order to distinguish an initiation of a gesture from a transition between the logical display spaces presented by different displays that happens to pass through the first location . For example the gesture may be initiated only upon detecting a movement from within the logical display space of a display to a first location within the same logical display space of the same display .

As a second example of this sixth variation in order to adjust for inadvertently overshooting the first location within a first logical display space of a first display that is logically adjacent to a second logical display space of a second display the first location may be extended into the second logical display space of the second display . For example the first location may be adapted to include an edge region comprising a first region portion near an edge of the first logical display space of the first display and a second region portion near an adjacent edge of the second logical display space of the second display having a second region portion edge that is adjacent to a first region portion edge of the first region portion and the detection of the pointer within either region portion of the edge region may be construed as the positioning of the pointer at the first location . This detection followed by the movement of the pointer to the second location may cause the user interface to appear in the first logical display space of the first display .

Additional variations of this second variation of this sixth aspect may be included to account for the inclusion of the second region portion in the edge region. As a first such example if the presentation of a preview is preceded by a hovering of the pointer in the first region portion the hover duration may be different when the pointer is positioned in the second region portion on the second display e.g. by defining a first hover duration for the first region portion and a second hover duration for the second region portion that is different from the first hover duration . As a second such example if the device is configured to terminate the user interface suggestion if the pointer is not moved from the first region portion to the second location within a gesture completion duration this gesture completion duration may be extended if the pointer is positioned within the second region portion e.g. to account for the further length of the movement when initiated from a second display . Thus the device may define a first gesture completion duration when the gesture is initiated in the first region portion and a second gesture completion duration which is greater than the first gesture completion duration when initiated in the second region portion. As a third such example the completion of this gesture may have various effects within the second logical display space of the second display . For example in addition to causing the device to present the first user interface at the second location on the first display the gesture may cause the same user interface to be presented at a location on the second display a different user interface to be presented on the second display or no effect with respect to the second display . Those of ordinary skill in the art may devise many such variations of the gestures in accordance with the techniques presented herein.

Although not required embodiments are described in the general context of computer readable instructions being executed by one or more computing devices. Computer readable instructions may be distributed via computer readable media discussed below . Computer readable instructions may be implemented as program modules such as functions objects Application Programming Interfaces APIs data structures and the like that perform particular tasks or implement particular abstract data types. Typically the functionality of the computer readable instructions may be combined or distributed as desired in various environments.

In other embodiments device may include additional features and or functionality. For example device may also include additional storage e.g. removable and or non removable including but not limited to magnetic storage optical storage and the like. Such additional storage is illustrated in by storage . In one embodiment computer readable instructions to implement one or more embodiments provided herein may be in storage . Storage may also store other computer readable instructions to implement an operating system an application program and the like. Computer readable instructions may be loaded in memory for execution by processing unit for example.

The term computer readable media as used herein includes computer storage media. Computer storage media includes volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions or other data. Memory and storage are examples of computer storage media. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM Digital Versatile Disks DVDs or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by device . Any such computer storage media may be part of device .

Device may also include communication connection s that allows device to communicate with other devices. Communication connection s may include but is not limited to a modem a Network Interface Card NIC an integrated network interface a radio frequency transmitter receiver an infrared port a USB connection or other interfaces for connecting computing device to other computing devices. Communication connection s may include a wired connection or a wireless connection. Communication connection s may transmit and or receive communication media.

The term computer readable media may include communication media. Communication media typically embodies computer readable instructions or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term modulated data signal may include a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal.

Device may include input device s such as keyboard mouse pen voice input device touch input device infrared cameras video input devices and or any other input device. Output device s such as one or more displays speakers printers and or any other output device may also be included in device . Input device s and output device s may be connected to device via a wired connection wireless connection or any combination thereof. In one embodiment an input device or an output device from another computing device may be used as input device s or output device s for computing device .

Components of computing device may be connected by various interconnects such as a bus. Such interconnects may include a Peripheral Component Interconnect PCI such as PCI Express a Universal Serial Bus USB firewire IEEE 1394 an optical bus structure and the like. In another embodiment components of computing device may be interconnected by a network. For example memory may be comprised of multiple physical memory units located in different physical locations interconnected by a network.

Those skilled in the art will realize that storage devices utilized to store computer readable instructions may be distributed across a network. For example a computing device accessible via network may store computer readable instructions to implement one or more embodiments provided herein. Computing device may access computing device and download a part or all of the computer readable instructions for execution. Alternatively computing device may download pieces of the computer readable instructions as needed or some instructions may be executed at computing device and some at computing device .

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

As used in this application the terms component module system interface and the like are generally intended to refer to a computer related entity either hardware a combination of hardware and software software or software in execution. For example a component may be but is not limited to being a process running on a processor a processor an object an executable a thread of execution a program and or a computer. By way of illustration both an application running on a controller and the controller can be a component. One or more components may reside within a process and or thread of execution and a component may be localized on one computer and or distributed between two or more computers.

Furthermore the claimed subject matter may be implemented as a method apparatus or article of manufacture using standard programming and or engineering techniques to produce software firmware hardware or any combination thereof to control a computer to implement the disclosed subject matter. The term article of manufacture as used herein is intended to encompass a computer program accessible from any computer readable device carrier or media. Of course those skilled in the art will recognize many modifications may be made to this configuration without departing from the scope or spirit of the claimed subject matter.

Various operations of embodiments are provided herein. In one embodiment one or more of the operations described may constitute computer readable instructions stored on one or more computer readable media which if executed by a computing device will cause the computing device to perform the operations described. The order in which some or all of the operations are described should not be construed as to imply that these operations are necessarily order dependent. Alternative ordering will be appreciated by one skilled in the art having the benefit of this description. Further it will be understood that not all operations are necessarily present in each embodiment provided herein.

Moreover the word exemplary is used herein to mean serving as an example instance or illustration. Any aspect or design described herein as exemplary is not necessarily to be construed as advantageous over other aspects or designs. Rather use of the word exemplary is intended to present concepts in a concrete fashion. As used in this application the term or is intended to mean an inclusive or rather than an exclusive or . That is unless specified otherwise or clear from context X employs A or B is intended to mean any of the natural inclusive permutations. That is if X employs A X employs B or X employs both A and B then X employs A or B is satisfied under any of the foregoing instances. In addition the articles a and an as used in this application and the appended claims may generally be construed to mean one or more unless specified otherwise or clear from context to be directed to a singular form.

Also although the disclosure has been shown and described with respect to one or more implementations equivalent alterations and modifications will occur to others skilled in the art based upon a reading and understanding of this specification and the annexed drawings. The disclosure includes all such modifications and alterations and is limited only by the scope of the following claims. In particular regard to the various functions performed by the above described components e.g. elements resources etc. the terms used to describe such components are intended to correspond unless otherwise indicated to any component which performs the specified function of the described component e.g. that is functionally equivalent even though not structurally equivalent to the disclosed structure which performs the function in the herein illustrated exemplary implementations of the disclosure. In addition while a particular feature of the disclosure may have been disclosed with respect to only one of several implementations such feature may be combined with one or more other features of the other implementations as may be desired and advantageous for any given or particular application. Furthermore to the extent that the terms includes having has with or variants thereof are used in either the detailed description or the claims such terms are intended to be inclusive in a manner similar to the term comprising. 

