---

title: Systems, methods, and interfaces for vector input/output operations
abstract: Data of a vector storage request pertaining to one or more disjoint, non-adjacent, and/or non-contiguous logical identifier ranges are stored contiguously within a log on a non-volatile storage medium. A request consolidation module modifies one or more sub-requests of the vector storage request in response to other, cached storage requests. Data of an atomic vector storage request may comprise persistent indicators, such as persistent metadata flags, to identify data pertaining to incomplete atomic storage requests. A restart recovery module identifies and excludes data of incomplete atomic operations.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09274937&OS=09274937&RS=09274937
owner: Longitude Enterprise Flash S.a.r.l.
number: 09274937
owner_city: Luxembourg
owner_country: LU
publication_date: 20121221
---
The application claims priority to U.S. Provisional Patent Application Ser. No. 61 625 475 filed Apr. 17 2012 and entitled Composite Vectored Storage Operations U.S. Provisional Patent Application Ser. No. 61 637 155 filed Apr. 23 2012 and entitled Systems Methods and Interfaces for Composite Vectored Atomic Storage Operations U.S. patent application Ser. No. 13 539 235 entitled Systems Methods and Interfaces for Managing Persistent Data of Atomic Storage Operations filed Jun. 29 2012 and to U.S. patent application Ser. No. 13 335 922 entitled Methods and Apparatuses for Atomic Storage Operations filed Dec. 22 2011 which claims priority to U.S. Provisional Patent Application Ser. No. 61 579 627 filed Dec. 22 2011 and entitled Methods and Apparatuses for Atomic Storage Operations each of which is hereby incorporated by reference.

The disclosure relates to input output IO operations and more particularly to IO operations configured to operate on one or more IO vectors.

A storage controller may be configured to perform input output IO operations in response to requests from one or more storage clients. The storage controller may be configured to implement vector storage operations on respective logical identifier ranges. The vector storage operations may be atomic such that the storage operation completes for each I O vector or none of the I O vectors.

Disclosed herein are systems and apparatuses configured to service vector storage requests which may include a request consolidation module configured to modify one or more storage requests of a vector storage request wherein the storage requests corresponds to respective logical identifier ranges of the vector storage request in response to one or more other pending storage requests and a storage controller configured to store one or more data packets pertaining to the vector storage request on the non volatile storage medium.

The request consolidation module may be configured to combine two or more storage requests including a storage request of the vector storage request. The two or more storage requests pertain to logical identifiers that are adjacent and or overlap. The two or more storage requests may comprise trim storage requests that pertain to overlapping and or adjacent logical identifier ranges in a logical address space. The request consolidation module may be further configured to remove one or more of the storage requests of the vector storage request in response to determining that the one or more storage requests are obviated by one or more pending storage requests. The request consolidation module may be configured to remove a storage request to trim one or more logical identifiers in response to a pending storage request to write data to the one or more logical identifiers.

The apparatus may further comprise a log storage module configured to append the one or more data packets pertaining to an atomic vector storage request contiguously within a log on the non volatile storage medium and an atomic storage module configured to include a persistent indicator in one or more the data packets of the atomic vector storage request to indicate that the one or more data packets pertain to an atomic storage operation that is incomplete. The atomic storage module may be configured to include a persistent indicator in a last one of the data packets of the atomic vector storage request to indicate that the atomic storage request is complete.

Disclosed herein are systems and apparatus configured to service atomic vector storage requests which may comprise a non volatile storage medium a log storage module configured to append one or more data packets pertaining to an atomic vector storage request in a contiguous log format on the non volatile storage medium and an atomic storage module configured to include respective persistent metadata flags in one or more of the data packets of the atomic storage request within the log on the non volatile storage medium to indicate that that the one or more data packets correspond to an atomic storage request that is in process. The atomic storage module may be configured to include a persistent metadata flag in one of the data packets of the atomic vector storage request to indicate that the atomic storage request is complete. The persistent metadata flags may comprise single bits. The log storage module may be configured to append the one or more data packets to non contiguous physical storage locations within a physical address space of the non volatile storage medium. The log storage module may be configured to append data packets sequentially from an append point within a physical address space of the non volatile storage medium and to associate the data packets with respective sequence indicators and wherein the sequential order and the sequence indicators of the data packets determine a log order of the data packets.

The atomic vector storage request may comprise a plurality of sub requests each sub request comprising an operation pertaining to a respective set of one or more logical identifiers and wherein the storage controller is configured to defer updating a forward index comprising any to any mappings between logical identifiers and physical storage locations until each of the sub requests of the atomic vector storage operation are complete.

The atomic vector storage request comprises a plurality of sub requests each sub request comprising an operation pertaining to a respective set of one or more logical identifiers wherein two or more of the sub requests comprise different types of storage operations.

A restart recovery module may be configured to reconstruct a forward index comprising mappings between logical identifiers of a logical address space and physical storage locations of the non volatile storage medium wherein the restart recovery module is configured to identify a data packet of an incomplete atomic vector storage request in response to accessing a data packet that comprises a persistent metadata flag indicating that the data packet corresponds to an atomic vector storage request that is in process at an append point.

The storage controller may be configured to update an inflight index in response to completing a subcommand of the atomic vector storage operation and to update the forward index with the inflight index in response to completing each of the subcommands of the atomic vector storage operation.

Subcommands of the atomic vector storage request may be queued in an ordered queue configured to complete the subcommands and the other storage requests according to an order in which the subcommands and the other storage requests were received at the ordered queue.

A request consolidation module may be configured to modify one of the subcommands based on one or more of the other plurality of subcommands of the atomic vector storage request. The request consolidation module may delete a subcommand in response to determining that the subcommand is overridden by one or more other subcommands of the atomic vector storage request and or combine one or more subcommands into a single composite subcommand.

Disclosed herein are systems and apparatus for consolidating storage requests comprising a request buffer configured to buffer and or queue one or more storage requests a request consolidation module configured to modify one or more of the storage requests in the request buffer based on one or more other storage requests in the request buffer and a storage controller configured to service storage requests in the request buffer. The request consolidation module may be configured to delete a storage request to trim one or more logical identifiers from the request buffer in response to receiving a storage request configured to store data to the one or more logical identifiers at the storage controller. The request consolidation module may be further configured to consolidate two or more storage requests to trim logical identifiers that overlap and or are contiguous in a logical address.

The system may further comprise a storage controller . The storage controller may comprise a storage management layer logical to physical translation module storage metadata log storage module media interface and or one or more media controllers . Portions of the storage controller may operate on or in conjunction with the computing device . Portions of the storage controller may be implemented separately from the computing device for example portions of the storage controller may be connected using a system bus such as a peripheral component interconnect express PCI e bus a Serial Advanced Technology Attachment serial ATA bus universal serial bus USB connection an Institute of Electrical and Electronics Engineers IEEE 1394 bus FireWire an external PCI bus Infiniband or the like.

The storage controller may comprise a media interface configured to couple to the storage controller to a non volatile storage media by use of one or more media controllers and bus . The non volatile storage media may comprise any suitable storage medium including but not limited to flash memory nano random access memory nano RAM or NRAM nanocrystal wire based memory silicon oxide based sub 10 nanometer process memory graphene memory Silicon Oxide Nitride Oxide Silicon SONOS Resistive Random Access Memory RRAM Programmable Metallization Cell PMC Conductive Bridging RAM CBRAM Magneto Resistive RAM MRAM Dynamic RAM DRAM Phase change RAM PRAM magnetic media e.g. one or more hard disks optical media or the like.

The media controller s may be configured to write data to and or read data from the non volatile storage media via a bus . The bus may comprise a storage I O bus for communicating data to and from the non volatile storage media and may further comprise a control I O bus for communicating addressing and other command and control information to the non volatile storage media .

The storage controller may be configured to service storage requests for one or more storage clients A N. The storage clients A N may include but are not limited to operating systems A file systems B databases C user applications D and so on. The storage clients A N may operate locally on the computing device and or may operate on other remote computing devices e.g. remote storage client s E .

The storage clients A N may access services provided by the storage controller via the storage management layer . The storage management layer may comprise one or more drivers libraries modules interfaces block device interfaces interface extensions e.g. input output control IOCTL interfaces Application Programming Interfaces API application binary interfaces ABI object classes remote interfaces e.g. Remote Procedure Call Simple Object Access Protocol or the like and so on.

The storage management layer may be configured to present and or expose a logical address space to the storage clients A N. As used herein a logical address space refers to a logical representation of I O resources such as storage resources. The logical address space may comprise a plurality e.g. range of logical identifiers. As used herein a logical identifier refers to any identifier for referencing an I O resource e.g. data stored on the non volatile storage media including but not limited to a logical block address LBA cylinder head sector CHS address a file name an object identifier an inode a Universally Unique Identifier UUID a Globally Unique Identifier GUID a hash code a signature an index entry a range an extent or the like.

The storage management layer may comprise a logical to physical translation layer configured to map and or associate logical identifiers in the logical address space and referenced by the storage clients A N with physical storage locations e.g. physical addresses on the non volatile storage media . The mappings may be any to any such that any logical identifier can be associated with any physical storage location and vice versa . As used herein a physical address refers to an address or other reference of one or more physical storage location s on the non volatile storage media . Accordingly a physical address may be a media address. As used herein physical storage locations include but are not limited to sectors pages logical pages storage divisions e.g. erase blocks logical erase blocks and so on or the like.

In some embodiments the logical address space maintained by the storage management layer may be thinly provisioned or sparse. As used herein a thinly provisioned or sparse logical address space refers to a logical address space having a logical capacity that is independent of physical address space of the non volatile storage media . For example the storage management layer may present a very large logical address space e.g. 2 64 bits to the storage clients A N which exceeds the physical address space of the non volatile storage media .

The storage management layer may be configured to maintain storage metadata pertaining to the non volatile storage media including but not limited to a forward index comprising any to any mappings between logical identifiers of the logical address space and storage resources a reverse index pertaining to the non volatile storage media one or more validity bitmaps atomicity and or translational metadata and so on. Portions of the storage metadata may be stored on the volatile memory and or may be periodically stored on a persistent storage medium such as the non transitory storage medium and or non volatile storage media .

In some embodiments the storage controller may leverage the arbitrary any to any mappings of the logical to physical translation module to store data in a log format such that data is updated and or modified out of place on the non volatile storage media . As used herein writing data out of place refers to writing data to different media storage location s rather than overwriting the data in place e.g. overwriting the original physical location of the data . Storing data in a log format may result in obsolete and or invalid data remaining on the non volatile storage media . For example overwriting data of logical identifier A out of place may result in writing data to new physical storage location s and updating the storage metadata to associate A with the new physical storage locations s e.g. in a forward index described below . The original physical storage location s associated with A are not overwritten and comprise invalid out of date data. Similarly when data of a logical identifier X is deleted or trimmed the physical storage locations s assigned to X may not be immediately erased but may remain on the non volatile storage media as invalid data.

The storage controller may comprise a groomer module configured to groom the non volatile storage media which may comprise reclaiming physical storage location s comprising invalid obsolete or trimmed data as described above. As used herein grooming the non volatile storage media may include but is not limited to wear leveling removing invalid and or obsolete data from the non volatile storage media removing deleted e.g. trimmed data from the non volatile storage media refreshing and or relocating valid data stored on the non volatile storage media reclaiming physical storage locations e.g. erase blocks identifying physical storage locations for reclamation and so on. The groomer module may be configured to operate autonomously and in the background from servicing other storage requests. Accordingly grooming operations may be deferred while other storage requests are being processed. Alternatively the groomer module may operate in the foreground while other storage operations are being serviced. Reclaiming a physical storage location may comprise erasing invalid data from the physical storage location so that the physical storage location can be reused to store valid data. For example reclaiming a storage division e.g. an erase block or logical erase block may comprise relocating valid data from the storage division erasing the storage division and initializing the storage division for storage operations e.g. marking the storage division with a sequence indicator . The groomer may wear level the non volatile storage media such that data is systematically spread throughout different physical storage locations which may improve performance data reliability and avoid overuse and or underuse of particular physical storage locations. Embodiments of systems and methods for grooming non volatile storage media are disclosed in U.S. Pat. No. 8 074 011 issued Dec. 6 2011 and entitled Apparatus System and Method for Storage Space Recovery After Reaching a Read Count Limit which is hereby incorporated by reference.

In some embodiments the storage controller may be configured to manage asymmetric write once non volatile storage media such as solid state storage media. As used herein a write once refers to storage media that is reinitialized e.g. erased each time new data is written or programmed thereon. As used herein asymmetric refers to storage media having different latencies and or execution times for different types of storage operations. For example read operations on asymmetric solid state non volatile storage media may be much faster than write program operations and write program operations may be much faster than erase operations. The solid state non volatile storage media may be partitioned into storage divisions that can be erased as a group e.g. erase blocks in order to inter alia account for these asymmetric properties. As such modifying a single data segment in place may require erasing an entire erase block and rewriting the modified data on the erase block along with the original unchanged data if any . This may result in inefficient write amplification which may cause excessive wear. Writing data out of place as described above may avoid these issues since the storage controller can defer erasure of the obsolete data e.g. the physical storage location s comprising the obsolete data may be reclaimed in background grooming operations .

The forward index comprises a plurality of entries A N each representing one or more logical identifiers in the logical address space entry A references logical identifiers entry B references logical identifiers entry C references logical identifiers and so on. The logical to physical translation module may enable independence between logical identifiers and physical storage locations such that data may be stored sequentially in a log based format and or updated out of place on the non volatile storage media . As such there may be no correspondence between logical identifiers and the physical storage locations.

The entries A N may comprise assignments between logical identifiers and physical storage locations on the non volatile storage media . Accordingly one or more of the entries A N may reference respective physical storage locations for example entry A assigns logical identifiers to physical addresses entry B assigns logical identifiers to physical addresses and so on. In some embodiments references to the physical storage locations may be indirect as depicted in entries D F and G.

The physical address es of the entries A N may be updated in response to changes to the physical storage location s associated with the corresponding logical identifiers due to inter alia grooming data refresh modification overwrite or the like. In some embodiments one or more of the entries A N may represent logical identifiers that have been allocated to a storage client A N but have not been assigned to any particular physical storage locations e.g. the storage client has not caused data to be written to the logical identifiers as depicted in entry E .

The entries A N may be indexed to provide for fast and efficient lookup by logical identifier. For clarity the example depicts entries A N comprising numeric logical identifiers. However the disclosure is not limited in this regard and the entries A N could be adapted to include suitable logical identifier representation including but not limited to alpha numerical characters hexadecimal characters binary values text identifiers hash codes or the like.

The entries A N of the index may reference ranges or vectors of logical identifiers of variable size and or length a single entry A may reference a plurality of logical identifiers e.g. a set of logical identifiers a logical identifier range a disjoint non adjacent and or non contiguous set of logical identifiers or the like . For example the entry B represents a contiguous range of logical identifiers . Other entries of the index may represent a non contiguous sets or vectors of logical identifiers entry G represents a non contiguous disjoint logical identifier range and each range being assigned to respective physical storage locations by respective references G and G. The forward index may represent logical identifiers using any suitable technique for example the entry D references a logical identifier range by starting point and length logical identifier and length which corresponds to a range of logical identifiers .

The index may be used to efficiently determine whether particular logical identifiers are assigned to physical storage location s and or are allocated to one or more storage clients A N. The storage controller may determine that logical identifiers that are not included in the index are available to be allocated to a storage client A N. Similarly the storage controller may determine that physical storage locations that are not associated with a logical identifier in the index do not comprise valid data and can be reclaimed. For example modifying data of the logical identifiers may result in associating the entry C with a new set of physical storage location s e.g. the storage locations comprising the data as modified out of place on the non volatile storage media . As a result the old physical addresses are no longer associated with an entry A N in the index and may be identified as invalid and ready for reclamation.

The reverse index comprises a plurality of entries depicted as rows in the table datastructure of the reverse index each of which corresponds to one or more physical storage locations on the non volatile storage media . Accordingly each entry may correspond to one or more physical addresses . In some embodiments the entries may be of variable length and or may comprise compressed and or encrypted data. As such one or more of the entries may comprise a data length . A valid tag indicates whether the physical address es of the entry comprise valid or invalid data e.g. obsolete or trimmed data .

The reverse index may further comprise references and or links to the first index such as a logical identifier field data length from the perspective of the storage clients A N e.g. uncompressed and or decrypted data length and the like e.g. miscellaneous . In some embodiments the reverse index may include an indicator of whether the physical address stores dirty or clean data or the like.

The reverse index may be organized according to the configuration and or layout of a particular non volatile storage media . In embodiments comprising solid state non volatile storage media the reverse index may be arranged by storage divisions e.g. erase blocks physical storage locations e.g. pages logical storage locations or the like. In the example the reverse index is arranged into a plurality of erase blocks and each comprising a plurality of physical storage locations e.g. pages logical pages or the like .

The entry ID may comprise an address reference virtual link or other data to associate entries in the reverse index with entries in the forward index or other storage metadata . The physical address indicates a physical address on the non volatile storage media . Together the physical address and data length may be referred to as destination parameters e.g. parameters pertaining to the physical storage location s of the entries . The logical identifier and data length may be referred to as source parameters . The logical identifier associates entries with respective logical identifier s of the logical address space e.g. in the forward index .

The valid tag indicates whether the data of the entry is valid e.g. whether the physical storage location s of the entry comprise valid up to date data of a logical identifier . Entries marked invalid in tag may comprise invalid obsolete and or deleted e.g. trimmed data. The reverse index may track the validity status of each physical storage location of the non volatile storage device. The groomer module may use the reverse index to identify physical storage locations to reclaim and or to distinguish data that needs to be retained from data that can be removed from the non volatile storage media .

The reverse index may also include other miscellaneous data such as a file name object name source data storage client security flags atomicity flag transaction identifier or the like. While physical addresses are depicted in the reverse index in other embodiments physical addresses or other destination parameters may be included in other locations such as in the forward index an intermediate table or data structure or the like.

The reverse index may be adapted to the characteristics and or partitioning of the non volatile storage media . In the example the reverse index is adapted for use with solid state storage media that is partitioned into a plurality of erase blocks. The groomer module may traverse the index to identify valid data in a particular erase block or logical erase block and to quantify an amount of valid data or conversely invalid data therein. The groomer may select storage divisions for recovery based in part on the amount of valid and or invalid data in each erase block.

In some embodiments the groomer module is restricted to operating within certain portions of the non volatile storage media . For example portions of the storage metadata may be periodically persisted on the non volatile storage media or other persistent storage and the groomer module may be limited to operating on physical storage locations corresponding to the persisted storage metadata . In some embodiments storage metadata is persisted by relative age e.g. sequence with older portions being persisted while more current portions are retained in volatile memory . Accordingly the groomer module may be restricted to operating in older portions of the physical address space and as such are less likely to affect data of ongoing storage operations. Therefore in some embodiments the groomer module may continue to operate while vector and or atomic storage requests are being serviced. Alternatively or in addition groomer module may access the storage metadata and or inflight index disclosed in further detail below to prevent interference with atomic storage operations. Further embodiments of systems methods and interfaces managing a logical address pace such as the logical address space and or storing data in a log based format are disclosed in U.S. patent application Ser. No. 12 986 117 filed on Jan. 6 2011 entitled Apparatus System and Method for a Virtual Storage Layer and published as United States Patent Application Publication No. 20120011340 on Jan. 12 2012 and U.S. patent application Ser. No. 13 424 333 filed on Mar. 19 2012 and entitled Logical Interface for Contextual Storage each of which is hereby incorporated by reference.

Referring back to the storage controller may be configured to leverage the arbitrary any to any mappings maintained by the logical to physical translation module to manage data on the non volatile storage media independent of the logical interface of the data e.g. independent of the logical identifier s associated with the data . For example the storage controller may leverage the logical to physical translation layer to store data on the non volatile storage media in a log format as described below.

The storage controller may comprise a log storage module configured to store data on the non volatile storage media in a log format e.g. an event log . As used herein a log format refers to a data storage format that defines an ordered sequence of storage operations performed on the non volatile storage media . Accordingly the log format may define an event log of storage operations performed on the non volatile storage media . In some embodiments the log storage module is configured to store data sequentially from an append point on the non volatile storage media . The log storage module may be further configured to associate data and or physical storage locations on the non volatile storage media with respective sequence indicators. The sequence indicators may be applied to individual data segments packets and or physical storage locations and or may be applied to groups of data and or physical storage locations e.g. erase blocks . In some embodiments sequence indicators may be applied to physical storage locations when the storage locations are reclaimed e.g. erased in a grooming operation and or when the storage locations are first used to store data.

In some embodiments the log storage module may be configured to store data according to an append only paradigm. The storage controller may maintain a current append point within a physical address space of the non volatile storage media . As used herein an append point refers to a pointer or reference to a particular physical storage location e.g. sector page storage division offset or the like . The log storage module may be configured to append data sequentially from the append point. As data is stored at the append point the append point moves to a next available physical storage location on the non volatile storage media . The log order of data stored on the non volatile storage media may therefore may be determined based upon the sequence indicator associated with the data and or the sequential order of the data on the non volatile storage media . The log storage module may identify the next available storage location by traversing the physical address space of the non volatile storage media e.g. in a reverse index as described below to identify a next available physical storage location.

Each physical storage location may be assigned a respective physical address ranging from zero 0 to N. The log storage module may be configured to store data sequentially from an append point within the physical address space . The append point moves sequentially through the physical storage space . After storing data at the append point the append point advances sequentially to the next available physical storage location. As used herein an available physical storage location refers to a physical storage location that has been initialized and is ready to store data e.g. has been erased . Some non volatile storage media such as solid state storage media can only be programmed once after erasure. Accordingly as used herein an available physical storage location may refer to a storage location that is in an initialized or erased state. If the next storage division in the sequence is unavailable e.g. comprises valid data has not been erased or initialized is out of service etc. the append point selects the next available physical storage location. In the embodiment after storing data on the physical storage location the append point may skip the unavailable physical storage locations of storage division and continue at the next available physical storage location e.g. physical storage location of storage division .

After storing data on the last storage location e.g. storage location N of storage division the append point wraps back to the first division or the next available storage division if is unavailable . Accordingly the append point may treat the physical address space as a loop or cycle.

Referring back to the log based format of the storage controller may further comprise storing data in a contextual format. As used herein a contextual data refers to a self describing data format from which the logical interface of the data may be determined. As used herein the logical interface of data may include but is not limited to a logical identifier of the data a range and or extent of logical identifiers a set of logical identifiers a name for the data e.g. file name object name or the like or the like. Accordingly the contextual format may comprise storing self descriptive persistent metadata with the data on the non volatile storage media the persistent metadata may comprise the logical identifier s associated with the data and or provide sequence information pertaining to the sequential ordering of storage operations performed on the non volatile storage media . In some embodiments contextual data may be stored in data packets on the non volatile storage media . As used herein a data packet refers to any data structure configured to associate a data segment and or other quantum of data with metadata pertaining to the data segment. A data packet may comprise one or more fields configured for storage as a contiguous unit on the non volatile storage media . Alternatively a data packet may comprise a plurality of different portions and or fragments stored at different noncontiguous storage locations of one or more non volatile storage medium .

In certain embodiments the packet may include persistent metadata that is stored on the non volatile storage media with the data segment . In some embodiments the persistent metadata is stored with the data segment as a packet header footer of other packet field. The persistent metadata may include a logical identifier indicator that identifies the logical identifier s to which the data segment pertains. As described below the persistent metadata and the logical identifier indicator may be used to reconstruct the storage metadata such as the forward index and or reverse index . The persistent metadata may further comprise one or more persistent metadata flags . As disclosed below the persistent metadata flags may be used to support atomic storage operations transactions or the like.

In some embodiments the packet may comprise and or be associated with a sequence indicator . The sequence indicator may be persisted with the packet on the non volatile storage media for example the sequence indicator may be stored on the same storage division as the packet . Alternatively the sequence indicator may be persisted in a separate storage location. In some embodiments a sequence indicator is applied when a storage division is made available for use e.g. when erased when the first or last storage location is programmed or the like . The sequence indicator may be used to determine the log order of the packet relative to other packets on the non volatile storage media .

The letters A L of may represent data stored on physical storage locations of the non volatile storage media . Data A is initially stored at a physical storage location . When the data A is persisted at location the physical storage location reference in the forward index entry is updated to reference the physical storage location . In addition a reverse index entry may be updated to indicate that the physical storage location comprises valid data and or to associate the physical storage location with logical identifiers not shown . For clarity other portions of the forward index and or reverse index are omitted from . 

Data A may be modified and or overwritten out of place such that the updated data is not be stored on the original physical storage location . Instead the updated data A is stored sequentially out of place at storage location which may correspond to the current position of the append point at the time data A was modified. The storage metadata is updated accordingly. The forward index entry is updated to associate the logical identifiers with the physical storage location comprising A . The entry of the reverse index is updated to mark physical storage location as invalid and to indicate that the physical storage location comprises valid data. Marking the physical storage location as invalid may allow the storage location to be reclaimed by the groomer module as described above.

The data A may be further modified and or overwritten with data A . The updated data A may be stored at the current append point physical storage location . The storage metadata is updated as described above the forward index entry is updated to associate the entry with the physical storage location and a reverse index entry is updated to indicate that the physical storage address comprises valid data and that the physical address comprises invalid data . The obsolete versions A and A may be retained on the non volatile storage media until the corresponding physical storage locations and or are reclaimed e.g. erased in a grooming operation.

The data A A and A may be stored in the sequential log based format an event log format described above. Referring back to the storage controller may be configured to reconstruct the storage metadata from the contents of the non volatile storage media e.g. from the contextual log format of the data . The storage controller may access persistent metadata of packets to identify the logical identifier s associated with corresponding data segments . The storage controller may be further configured to distinguish valid up to date data from obsolete out of date versions based on the log order of the data on the non volatile storage medium e.g. based on sequence indicator s associated with the data and or relative order of the data within the physical address space of the non volatile storage media .

In the logical identifier indicator of the persistent metadata stored with data A A and or A may indicate that the data stored at the physical storage locations and corresponds to logical identifiers . A sequence indicator of the data A A and or A and or the position of the append point indicates that the physical storage location comprises the current valid copy of the data. Therefore the forward index entry may be reconstructed to associate the logical identifiers with the physical storage location . In addition the reverse index entries and or may be reconstructed to indicate that the physical storage locations and comprise invalid data and that the physical storage location comprises valid data. Further embodiments of systems and methods for crash recovery and or data integrity despite invalid shutdown conditions are described in U.S. patent application Ser. No. 13 330 554 filed Dec. 19 2011 and entitled Apparatus System and Method for Persistent Data Management on a Non Volatile Storage Media which is hereby incorporated by reference.

In the embodiment the non volatile storage media may comprise one or more non volatile storage devices such as one or more hard disks one or more solid state storage elements or the like. The non volatile storage media and or corresponding devices may be selectively coupled to the media controller via the bus and or multiplexer . Alternatively or in addition one or more of the non volatile storage media or devices may be a remote storage device accessible via a network e.g. network .

The media controller may comprise a storage request receiver module configured to receive storage requests from the storage controller and or other storage clients A N. The request module may be configured to perform storage operations on the non volatile storage media in response to the requests which may comprise transferring data to and from the storage controller and or storage clients A N. Accordingly the request module may comprise one or more direct memory access DMA modules remote DMA modules controllers bridges buffers and the like.

The media controller may comprise a write pipeline that is configured to process data for storage on the non volatile storage media . In some embodiments the write pipeline comprises one or more write processing stages which may include but are not limited to compression encryption packetization media encryption error encoding and so on.

Packetization may comprise encapsulating data in a contextual data format such as the self describing packet format described above. Accordingly the write pipeline may be configured to store data with persistent metadata which may include indicators of the logical identifier s associated with the data. As described above the restart recovery module may leverage the contextual data format to reconstruct the storage metadata . As used herein restart recovery comprises the act of a system apparatus or computing device commencing processing after an event that can cause the loss of data stored within volatile memory of the system apparatus or computing device e.g. a power loss reset hardware failure software fault or the like . Restart recovery may also comprise power cycle recovery such as commencing processing after an invalid shutdown a hard reset or a disconnection or separation of the powered device from a power supply such as physically disconnecting a power supply for the device .

Error encoding may comprise encoding data packets or other data containers in an error correcting code ECC . The ECC encoding may comprise generating ECC codewords each of which may comprise a data segment of length N and a syndrome of length S. For example the write pipeline may be configured to encode data segments into 240 byte ECC chunks each ECC chunk comprising 224 bytes of data and 16 bytes of ECC data. In other embodiments the write pipeline may be configured to encode data in a symbolic ECC encoding such that each data segment of length N produces a symbol of length X. The write pipeline may encode data according to a selected ECC strength. As used herein the strength of an error correcting code refers to the number of errors that can be detected and or corrected by use of the error correcting code. In some embodiments the strength of the ECC encoding may be adaptive and or configurable the strength of the ECC encoding may be selected according to the reliability and or error rate of the non volatile storage media .

The write buffer may be configured to buffer data for storage on the non volatile storage media . In some embodiments the write buffer may comprise one or more synchronization buffers to synchronize a clock domain of the media controller with a clock domain of the non volatile storage media and or bus .

As described above the log storage module may be configured to store data in a log format on the non volatile storage media . The log storage module may be configured to store data sequentially from an append point within the physical address space of the non volatile storage media as described above. The log storage module may therefore select physical storage location s for data to maintain a log order on the non volatile storage media which may comprise providing addressing and or control information to the media controller and or write pipeline .

The media controller may further comprise a read pipeline that is configured to read data from the non volatile storage media in response to requests received via the request module . The requests may comprise and or reference the logical interface of the requested data such as a logical identifier a range and or extent of logical identifiers a set of logical identifiers or the like. The physical addresses associated with data of a read request may be determined based at least in part upon the logical to physical translation layer and or storage metadata maintained by the storage controller . Data may stream into the read pipeline via the read buffer and in response to addressing and or control signals provided via the bus . The read buffer may comprise one or more read synchronization buffers for clock domain synchronization as described above.

The read pipeline may be configured to process data read from the non volatile storage media and provide the processed data to the storage controller and or a storage client A N. The read pipeline may comprise one or more data processing stages which may include but are not limited to error correction media decryption depacketization decryption decompression and so on. Data processed by the read pipeline may flow to the storage controller and or storage client A N via the request module and or other interface or communication channel e.g. the data may flow directly to and from a storage client via a DMA or remote DMA module of the storage controller 

The read pipeline may be configured to detect and or correct errors in data read from the non volatile storage media using inter alia the ECC encoding of the data e.g. as encoded by the write pipeline parity data e.g. using parity substitution and so on. The ECC encoding may be capable of detecting and or correcting a pre determined number of bit errors in accordance with the strength of the ECC encoding. Further embodiments of apparatus systems and methods for detecting and or correcting data errors are disclosed in U.S. Pat. No. 8 195 978 issued on Apr. 5 2012 and entitled Apparatus System and Method for Detecting and Replacing a Failed Data Storage which is hereby incorporated by reference.

The logical storage element may comprise 25 solid state storage elements connected in parallel by the bus . The logical storage element may be partitioned into logical storage units such as logical storage divisions logical erase blocks and or logical storage units logical pages . Each logical erase block comprises an erase block of a respective storage element 25 erase blocks and each logical page comprises a page of a respective storage element 25 pages .

Storage operations performed on the logical storage element may operate across the constituent solid state storage elements an operation to read a logical page comprises reading from as many as 25 physical pages e.g. one storage unit per solid state storage element an operation to program a logical page comprises programming as many as 25 physical pages an operation to erase a logical erase block comprises erasing as many as 25 physical erase blocks and so on.

As disclosed above the groomer module may be configured to reclaim storage resources on the non volatile storage media . In some embodiments the groomer module may be configured to interleave grooming operations with other storage operations and or requests. For example reclaiming a storage resource such as a physical erase block PEB or logical erase block e.g. set of two or more physical erase blocks may comprise relocating valid data to another storage location on the non volatile storage media . The groomer write and groomer read bypass modules and may be configured to allow data packets to be read into the read pipeline and then be transferred directly to the write pipeline without being routed out of the media controller .

The groomer read bypass module may coordinate reading data to be relocated from a storage resource that is being reclaimed e.g. an erase block logical erase block or the like . The groomer module may be configured to interleave the relocation data with other data being written to the non volatile storage media via the groomer write bypass . Accordingly data may be relocated without leaving the media controller . In some embodiments the groomer module may be configured to fill the remainder of the write buffer with relocation data which may improve groomer efficiency while minimizing the performance impact of grooming operations.

The media controller may further comprise a multiplexer that is configured to selectively route data and or commands between the write pipeline and read pipeline and the non volatile storage media . In some embodiments the media controller may be configured to read data while filling the write buffer and or may interleave one or more storage operations on one or more banks of solid state storage elements . Further embodiments of write and or read pipelines are disclosed in U.S. patent Ser. No. 11 952 091 filed Dec. 6 2007 entitled Apparatus System and Method for Managing Data Using a Data Pipeline and published as United States Patent Application Publication No. 2008 0141043 on Jun. 12 2008 which is hereby incorporated by reference.

Many storage clients A N rely on atomic storage operations. As used herein an atomic operation refers to an operation that either completes or fails as a whole. Accordingly if any portion of an atomic storage operation does not complete successfully the atomic storage operation is incomplete or failed and other portions of the atomic storage operation are invalidated or rolled back. As used herein rolling back an incomplete atomic storage operation refers to undoing any completed portions of the atomic storage operation. For example an atomic storage operation may comprise storing six data packets on the non volatile storage media five of the packets may be stored successfully but storage of the sixth data packet may fail rolling back the incomplete storage operation may comprise ignoring and or excluding the five packets as described below.

Some atomic operations may be limited to a relatively small fixed sized data e.g. a single sector within a block storage device . Atomic storage operations may require a copy on write operation to ensure consistency e.g. to allow the atomic storage operation to be rolled back if necessary which may significantly impact the performance of the atomic storage operations. Moreover support for atomic storage operations may typically be provided by a layer that maintains its own separate metadata pertaining to atomic storage operations resulting in duplicative effort increased overhead and or decreased performance. Some atomic operations may be more complex and may involve multiple storage operations or sub requests or subcommands e.g. may involve storing a plurality of data packets on the non volatile storage media . The storage controller may be configured to efficiently service complex atomic storage operations such that the atomic operations are crash safe and packets of incomplete failed atomic operations can be identified and rolled back.

In some embodiments the storage controller is configured to leverage and or extend the storage metadata to provide efficient atomic storage operations through the storage management layer . Consistency of the storage metadata may be maintained by deferring updates to the storage metadata until the one or more storage operations comprising the atomic storage request are complete. In some embodiments the atomic storage module maintains metadata pertaining to atomic storage operations that are in process e.g. ongoing operations that are not yet complete in separate inflight metadata . Accordingly in certain embodiments the state of the storage metadata is maintained until the atomic operation successfully completes obviating the need for extensive rollback processing. In response to completion of the atomic storage operation the atomic storage module updates the storage metadata with the corresponding contents of the inflight metadata .

Alternatively or in addition the atomic storage module may comprise an ordered queue that is configured to maintain ordering of storage requests directed to the storage controller . The ordered queue may be configured to queue both atomic storage requests and non atomic storage requests. In some embodiments the ordered queue may be configured to retain the order in which the storage requests were received e.g. in a first in first out configuration . The ordering may prevent data hazards such as read before write or the like. The ordered queue may therefore simplify processing of storage requests and or obviate the need for example for the separate inflight metadata disclosed below in connection with . Consequently certain embodiments may include an ordered queue and not inflight metadata or vice versa . In addition some embodiments may leverage the ordered queue to avoid potential problems that may be caused by interleaving of data packets which may occur if multiple atomic requests are processed simultaneously. As will be explained below in connection with FIGS. B and A C if data packets for each atomic request are stored contiguously in the log without interleaving packets associated with other write requests a single bit within each data packet may be utilized to identify whether an atomic write was successfully completed. Accordingly in certain embodiments the ordered queue may provide significant advantages by reducing the persistent metadata overhead associated with atomic storage operations. In alternative embodiments the ordered queue may process either atomic storage requests or non atomic storage requests but not both and or the storage controller may comprise separate queues for atomic storage requests and non atomic storage requests.

The storage management layer may comprise a vector module configured to perform vector I O operations e.g. service vector storage requests . As used herein a vector I O operation or vector storage request refers to an I O operation pertaining to one or more vectors. A vector may comprise one or more parameters which may include but are not limited to one or more source identifiers pertaining to a source of an I O operation one or more destination identifiers pertaining to a destination of the I O operation one or more flags to indicate a type of I O operation and or properties of the I O operation and so on. Accordingly as used herein a vector may define an I O operation e.g. a storage request pertaining to a set of disjoint and or non contiguous identifiers a range of identifiers an extent of identifiers or the like. The identifiers of a vector may include but are not limited to memory addresses memory references physical storage locations logical identifiers names offsets or the like. A vector may specify a storage request and or I O operation. As such as used herein a vector may be referred to as a storage request storage vector and or I O vector. A vector storage request may comprise a plurality of vectors and may therefore define a plurality of storage requests e.g. a separate I O vector and or storage request for each vector of the vector storage request . The storage requests of a vector storage request may be referred to as subcommands or sub requests each of which may correspond to a respective vector of the vector storage request. Servicing and or executing a vector storage request comprising a plurality of vectors may comprise servicing and or executing the subcommands and or sub requests of the vector storage request. Accordingly in certain embodiments servicing and or executing a vector storage request may comprise generating and or determining storage requests corresponding to each vector of the vector storage request generating and or determining the subcommands and or sub requests of the vector storage request . Servicing and or executing an atomic vector storage request may comprise successfully completing all of the storage requests of the atomic vector storage request or none of the storage requests of the atomic vector storage request e.g. rolling back and or excluding completed portions of a failed atomic vector storage request .

As disclosed above a vector storage request refers to a request to perform an I O operation s on one or more vectors. The vector s of a vector storage request may pertain to logical identifier sets and or ranges that are contiguous or non contiguous with respect to the logical address space . For example an operation to TRIM one or more logical identifier ranges in the logical address space may be implemented as a single vector storage request e.g. a vector storage request to TRIM logical identifiers through through and through .

The storage layer may further comprise an atomic module configured to implement atomic operations. As described in additional detail below the storage layer may leverage the log format implemented by the log storage module and the independence between logical identifiers and physical storage locations to efficiently service vector and or atomic operations.

As disclosed above the logical to physical translation module may enable arbitrary any to any mappings between logical identifiers and physical storage locations. The storage controller may leverage the flexibility provided by these mappings to store data out of place and in a log based format and to efficiently manage vector storage requests. A vector storage request may comprise a request to perform I O operation s on two or more vectors which may be disjoint non adjacent and or non contiguous with respect to the logical address space . However due to the independence between logical identifiers and physical storage locations the storage controller may store data pertaining to the vector storage operations contiguously in the log on the non volatile storage media e.g. by use of the log storage module as described above .

Storing data contiguously within the log may simplify atomic storage operations including atomic vector storage operations. Referring to an atomic vector storage request may comprise a request to write data to two or more disjoint non adjacent and or non contiguous vectors such that either all of the write requests complete successfully or none of the write requests complete e.g. any partial sub requests are rolled back . The atomic storage module may be configured to decompose the atomic vector storage request into sub requests e.g. a separate write request for each logical identifier range to store data packets and of the write requests contiguously in the log from a starting append point A to an end append point B as described above e.g. by use of the vector storage module and or the log storage module .

The storage controller may leverage persistent metadata of the packet format or other suitable data format to identify data that pertains to atomic storage operations. In some embodiments the persistent metadata may be used to identify and exclude data packets pertaining to incomplete failed atomic storage operations e.g. during reconstruction of the storage metadata by the restart recovery module . The persistent metadata may ensure that atomic storage operations including atomic vector storage operations are crash safe such that data packets of failed atomic operations can be identified and rolled back during restart and or recovery processing.

In some embodiments data pertaining to atomic operations may be identified by use of persistent indicators stored on the non volatile storage media . For example data pertaining to an incomplete and or in process atomic storage operation may be identified by use of a persistent metadata indicator in a first state. As used herein data of an incomplete or in process atomic storage request refers to data pertaining to an ongoing atomic storage operation such as data stored on the non volatile storage media as part of one or more sub requests of an atomic vector operation and or other multi packet operation. Persistent metadata in a second state may be used to signify completion of the atomic storage operation. The indicators may be stored at a pre determined order within the log which as disclosed in further detail herein may allow data of failed atomic storage operations to be detected excluded and or rolled back.

In some embodiments the packet format of may be leveraged to identify data packets of atomic storage operations. Data packets pertaining to incomplete and or in processes atomic storage operations may comprise a persistent metadata flag in a first state. Data packets pertaining to non atomic operations and or data packets that represent completion of an atomic storage operation may comprise a persistent metadata flag in a second state. The metadata flag may comprise a single bit the first state may be a 0 and the second state may be a 1 or vice versa .

In the example the atomic storage module may configure the write pipeline to store the data packets with the persistent metadata flag in the first state e.g. the state indicating that the data packets are part of an in progress atomic storage request . The atomic storage module may further configure the write pipeline to set the persistent metadata flag of the data packet of the atomic vector storage request to the second state e.g. non atomic or closed state indicating that the atomic storage operation was successfully completed. The data packet comprising the persistent metadata flag in the second state may be the last final and or terminating data packet of the atomic storage request within the log . This data packet may be configured to signify completion of the atomic storage operation. As such the last data packet may be stored at the head of the log with respect to the other packets of the atomic storage operation. Accordingly when traversing the log in reverse log order from completion append point B the first packet encountered will indicate that the atomic vector storage request is complete and that the other data packets of the atomic storage request should be retained .

The storage controller may be configured to identify data pertaining to incomplete atomic storage operations using the persistent metadata flags which certain embodiments may include in the packets and . The restart recovery module may be configured to identify data of an incomplete atomic storage operation in response to identifying one or more data packets comprising a persistent metadata flag in the first state that do not have corresponding data packets with a persistent metadata flag in the second state e.g. the log ends with packets comprising persistent metadata flags in the first state . In the embodiment a failure condition may occur at the append point C before the data packet was stored in the log . The restart recovery module may be configured to traverse the log from the failure append point C in reverse log sequence which results in encountering packets comprising a persistent metadata flag in the first state without first encountering a packet having a persistent metadata flag in the second state indicating that the packets are part of an incomplete atomic vector storage request and should be ignored and or invalidated as described below .

Although depict the logs and in contiguous physical storage locations e.g. contiguous sectors pages erases blocks etc. the disclosure is not limited in this regard. As described above in conjunction with in some embodiments the logs and or may not be contiguous in the physical address space of the non volatile storage media . Referring to as the log storage module appends data sequentially from the append point the log storage module may skip over certain physical storage locations that are not available for storing data e.g. the erase block of . A physical storage location may be unavailable for a number of different reasons including but not limited to the physical storage location is currently being used to store other valid data the physical storage location is not ready to store data e.g. has not been reclaimed or erased by the groomer module a failure condition e.g. the physical storage location has been taken out of service or the like. However notwithstanding any non contiguity in the physical address space the log format of the log storage module generates a contiguous log of storage operations as defined by the sequence indicators and sequential storage order of data on the non volatile storage media . Therefore referring back to the logs and are contiguous with respect to the sequence of storage operations on the non volatile storage media regardless of whether the data packets and or are stored on contiguous physical storage locations of the non volatile storage media .

As described above the storage controller may leverage the contiguous log format to ensure that atomic storage operations are crash safe with minimal persistent metadata overhead on the non volatile storage media . For example if a data packet of a non atomic storage operation were interleaved within the data packets in the log one or more of the data packets could be misidentified as being part of a completed atomic storage operation. However the log format of the storage controller may ensure that data of atomic storage operations are stored contiguously within the log without interleaving other packets therein which may ensure that incomplete atomic operations are crash safe and can be accurately identified and rolled back.

As described above in some embodiments the storage controller may be configured to defer updates to the storage metadata pertaining to an atomic storage operation until completion of the atomic storage operation. Metadata pertaining to storage operations that are in process may be maintained in separate inflight metadata . Accordingly in certain embodiments the state of the storage metadata is maintained until the atomic storage operation successfully completes obviating the need for extensive post failure rollback operations.

Metadata pertaining to in process atomic storage operations may be maintained in an inflight metadata which may be separate from other storage metadata . The inflight metadata may be accessed to identify read and or write hazards pertaining to the atomic storage request.

An atomic vector storage request may comprise and or reference one or more vectors pertaining to one or more disjoint non adjacent and or non contiguous ranges of logical identifiers e.g. an atomic vector storage request . In the example the atomic vector storage request comprises a request to store data pertaining to two logical identifier ranges and portions of which overwrite existing data in the forward index . The existing data is referenced by entries B and E of the forward index . The entries B and E may comprise references to physical storage locations of the data and or may reference the physical storage locations and of the data using the entries and of a reverse index for clarity only a portion of the reverse index and reverse index entries is depicted . As illustrated in the atomic vector storage request expands the logical identifier range of to . Servicing the atomic storage request may therefore comprise allocating additional logical identifiers in the logical address space . Completion of the atomic vector storage request may be predicated on the availability of the additional logical identifiers. The new logical identifiers may be allocated in the forward index in an unassigned entry not shown or as depicted in in the inflight index .

As disclosed above the storage metadata may be updated as data is stored on the non volatile storage media which may comprise updating entries in the forward index to assign logical identifiers to updated physical storage locations adding and or removing entries. Updating the storage metadata may further comprise updating the reverse index to invalidate previous versions of overwritten modified data and to track the physical storage locations of the updated data. These updates modify the state of the storage metadata which may make it difficult to roll back a failed atomic storage operation. Moreover the updates may cause previous versions of the data to be removed from the non volatile storage media by the groomer module or other process such as a cache manager or the like. Removal of the previous version of data overwritten by data of an atomic storage request may make it difficult or impossible to roll back the atomic storage request in the event of a failure.

Use of the inflight index may provide additional advantages over tracking in process storage operations using the forward index alone. For example as a storage request is performed the inflight index may be updated via an exclusive or locked operation. If these updates were performed in the forward index or other shared storage metadata the lock may preclude other storage requests from being completed. Isolating these updates in a separate datastructure may free the storage metadata for use in servicing other potentially concurrent storage requests. In addition the inflight index may track in process operations that may be rolled back in the event of failure e.g. atomic storage operations . Furthermore isolating the in process metadata within the inflight index allows the storage metadata e.g. forward index to be maintained in a consistent state until the storage request is fully complete and may allow for more efficient rollback of failed and or incomplete storage requests.

In some embodiments the state of the storage metadata is preserved until completion of an atomic storage request. The progress of the atomic vector storage request may be tracked in the inflight index . Modifications to the inflight index may be applied to the storage metadata forward index and or reverse index upon completion of the atomic storage request and or upon reaching a point after which the atomic storage operation is guaranteed to complete .

Entries B and E are added to the inflight index in response to the atomic vector storage request . The entries B and E identify logical identifiers pertaining to the atomic vector storage request . As illustrated in the atomic vector storage request comprises writing data to two vectors pertaining to respective disjoint non adjacent and or non contiguous logical identifier ranges and . The inflight index comprises respective entries B and E representing the logical identifier ranges of each vector. The disclosure is not limited in this regard however and could be adapted to generate entries for each logical identifier for sub ranges of logical identifiers of the atomic vector storage request and so on.

The inflight index is updated in response to completion of one or more portions of the atomic vector storage request . depicts the inflight index after storing a first portion of the data of the atomic vector storage request . The entry E indicates that the data corresponding to logical identifiers has been successfully stored at physical storage locations . Alternatively or in addition the physical storage locations may be referenced using a secondary datastructure such as a separate reverse index or the like. The forward index and reverse index of the storage metadata remain unchanged. The inflight index is further updated in response to completion of other portions of the atomic vector storage request . depicts the inflight index as the atomic storage request is completed. The inflight index entry B is updated to assign physical storage locations to the logical identifiers . The forward index and or reverse index remain unchanged.

The storage metadata may be updated in response to detecting completion of the atomic vector storage request and or determining that the atomic vector storage request will successfully complete e.g. data of the atomic vector storage request has been received within a crash power safe domain such as within the write pipeline or at write buffer .

In some embodiments the inflight index is used to avoid write and or read hazards. As shown in a storage request pertaining to a logical identifier of an atomic vector storage request may be received after or concurrently with the atomic vector storage request but before completion of the atomic vector storage request . For example the subsequent storage request may pertain to logical identifiers that are to be overwritten by the atomic vector storage request . If the subsequent storage request is to read data of the request may pose a read hazard e.g. read before write since reading the physical storage location of the entry B will return obsolete data. The read hazard may be identified in the inflight index which indicates that the target of the request is in the process of being modified. The storage management layer may be configured to delay and or defer the subsequent storage request until completion or failure of the atomic vector storage request and removal of the in process entry B from the inflight index . Write hazards may also be detected and addressed by use of the inflight index .

The inflight index may also be used to prevent a subsequent storage request from writing data to the logical identifiers of the atomic vector storage request . For example the entry B of the inflight index may be accessed to prevent another storage client from allocating logical identifiers .

As described above the storage controller may be configured to mark data packets pertaining to atomic storage operations that are in process vectored or otherwise . Accordingly atomic storage operations may be crash safe such that data of incomplete storage operations can be identified within the log the log format stored on the non volatile storage media . Absent these indicators data packets pertaining to failed atomic storage operation may appear to be valid. This potential issue is illustrated in . Data A B C are stored on physical storage locations and respectively. Other data D is subsequently stored in the log . The data A B and C are modified overwritten in a subsequent atomic storage request. The atomic storage request stores a portion of the atomic storage request updated data A is stored in packet and updated B is stored in packet . A failure occurs with the append point at physical storage location before the atomic storage operation is complete for example before writing C to packet . The failure may require the storage metadata e.g. forward index and or reverse index through power loss or data corruption to be reconstructed from the log .

The restart recovery module may be configured to reconstruct the storage metadata e.g. forward index from data stored on the non volatile storage media in the self describing log format described above. The restart recovery module may be configured to access the log from the last known append point which corresponds to the most recent operations in the log . In some embodiments the append point location is periodically stored to the non volatile storage media or other non transitory storage medium . Alternatively or in addition the append point may be determined using sequence indicators within the log e.g. sequence indicators on erase blocks or other physical storage locations of the non volatile storage media . The storage metadata may be reconstructed by traversing the log in a pre determined order e.g. from storage operation performed furthest in the past to the most recent storage operations tail to head or from the most recent storage operations to older storage operations head to tail .

As disclosed above the storage controller may be configured to store data of atomic storage requests contiguously in the log. The storage controller may be further configured to mark data packets with persistent metadata flags to identify data pertaining to in process atomic storage operations e.g. by use of the atomic storage module . The log order of the data A at and B of the failed atomic storage request in the log may indicate that data packets and comprise the most up to date versions of the data A and B rendering obsolete the previous version s of A at and B at . However the atomic storage request should have been rolled back to preserve the original data A B and C. If the failed atomic storage request is not identified and reconciled this may result in reconstructing invalid entries A and B in the forward index that associate A and B with data of the failed atomic storage request e.g. data packets and or . The reverse index may comprise entries and that improperly invalidate the A data at and the B data at and entries and that improperly indicate that the data of the failed atomic storage request at and is valid.

In some embodiments persistent indicators stored on the non volatile media are used to track in process storage requests on the non volatile storage device and or to account for loss of storage metadata . As used herein a persistent indicator refers to an indicator that is stored persisted on a non volatile storage medium e.g. the non volatile storage media . A persistent indicator may be associated with the data to which the indicator pertains. In some embodiments the persistent indicators are persisted with the data in a packet format such as the packet format described above. The persistent indicators may be stored with the data in a single storage operation and or in the smallest write unit supported by the non volatile storage media . Accordingly persistent storage indicators will be available when the storage metadata is reconstructed from the log . The persistent indicators may identify incomplete and or failed atomic storage requests despite an invalid shutdown and or loss of storage metadata . For example and as described above the packets and may comprise persistent metadata flags in the first state indicating that the packets and are part of an in process atomic storage operation. The packet comprising the metadata flag in the second state was not stored in the log therefore when traversing the log from the append point the restart recovery module may determine that the packets and are part of an incomplete atomic storage request and should be rolled back e.g. excluded from the storage metadata which may comprise invaliding the association between A and packet and B and packet reverting to the associations to and respectively and invalidating packets and in the reverse index .

An atomic storage request is received to store data in association with one or more disjoint non adjacent and or non contiguous logical identifiers LIDs and . In some embodiments an atomic storage request is formed by combining one or more storage requests as described above for example the storage requests may be combined into a single atomic vector storage request that is implemented as a whole.

In some embodiments data of the atomic storage request is stored contiguously in the log such that data that does not pertain to the atomic storage request is not interleaved with data of the atomic storage request . The logical identifiers of the atomic storage request however may be disjoint non adjacent non contiguous out of order or the like. Accordingly while data of the atomic storage request is being appended to the log other data that does not pertain to the request such as groomer bypass data data of other storage requests and the like may be suspended. In some embodiments suspension is not required if write requests including grooming are processed utilizing the ordered queue described above.

The persistent metadata flag stored with the data on physical storage locations and indicates that the physical storage locations and comprise data pertaining to an incomplete atomic storage operation because the first encountered persistent metadata flag is a 0 rather than a 1 reading in reverse log order reading to the left from the append point as illustrated in . If the first persistent metadata flag preceding the append point A is set to a 1 as shown in this indicates that the atomic storage operation was successfully completed. The persistent metadata flag may be stored with the data on the physical storage locations and .

If a failure were to occur the persistent metadata flags are used together with the contiguous placement of data for the atomic storage request in the log to identify data pertaining to the incomplete atomic storage request . When the event log of is traversed in reverse log order e.g. right to left as shown in or in other words from the tail to the head of the sequence the first persistent metadata flag will be a 0 indicating that the data pertains to a failed atomic storage request. The data at storage location may therefore be invalidated and may not result in reconstructing invalid storage metadata . The data may continue to be invalidated or ignored until a 1 flag is encountered at physical storage location . This approach relies on data of the atomic storage request being stored contiguously in the log . If data comprising a 1 persistent metadata flag were interleaved with the atomic storage data before completion of the atomic storage request the data at and or could be misidentified as being valid e.g. pertaining to a complete atomic storage request .

If a failure were to occur subsequent to persisting the data at physical storage location the storage metadata could be correctly reconstructed. When traversing the event log in reverse sequence e.g. moving left from the append point the first persistent metadata flag encountered would be the 1 flag on the physical storage location indicating that the data at physical storage locations and pertain to a successfully completed atomic storage request.

In some embodiments the data of such an atomic storage request may be limited by storage boundaries of the non volatile storage media e.g. page boundaries logical page boundaries storage divisions erase blocks logical erase blocks etc. . Alternatively the size of the data for an atomic storage request may require that the atomic storage request wait until the append point is on a storage division with sufficient free space to fit the atomic storage request before reaching a logical erase block boundary. Accordingly the size of an atomic storage request may be limited to a logical page size. Additionally in some embodiments atomic storage requests do not cross logical erase block boundaries. In another example the persistent metadata flag may comprise an identifier which may allow data to be interleaved with atomic storage requests and or allow atomic storage requests to be serviced concurrently. In some embodiments data of atomic storage operations may be allowed to cross storage boundaries as described below in conjunction with .

In some embodiments the persistent metadata flags A of data packets pertaining to atomic storage operations may be modified in response to grooming operations. For example a grooming operation on a storage division comprising physical addresses and comprising data of an atomic storage operation may comprise relocating the data to another storage division data of logical identifiers and . When the data is relocated after completion of the atomic storage operation the persistent metadata flags of the corresponding data packets may be modified to indicate that the data is part of a complete atomic operation and or a non atomic operation which may comprise updating the persistent metadata flags of the data packets to a 1 state. Accordingly when storage metadata is reconstructed from an updated append point B the relocated data on storage division will not be misidentified as being part of a failed and or incomplete atomic storage operation.

In some embodiments the groomer module may be configured to control grooming operations on storage divisions that comprise persistent metadata indicating completion of atomic storage operation s . The groomer module may be configured to prevent such storage divisions from being groomed until other storage divisions comprising data of the corresponding atomic storage operation s have been relocated and or updated to indicate that the atomic storage operation s are complete. As described in further detail below in conjunction with prohibiting grooming operations on such storage divisions may inter alia prevent loss of the completion indicators due to grooming failures.

The storage management layer may be configured to manage subsequent storage operations pertaining to data of atomic storage operations. For example an operation to TRIM data of logical identifier may result in trimming e.g. invalidating the data packet at physical address which indicates completion of the atomic storage request . If the data packet at physical address were to be completely invalidated and or erased the corresponding persistent metadata flag indicating completion of the atomic storage request may also be lost which may allow the data at physical addresses and or to be misidentified as being part of a failed and or incomplete atomic storage operation. The storage layer may be configured to implement TRIM operations while preserving information pertaining to atomic storage operations e.g. persistent metadata flags . In response to the TRIM request the storage management layer may be configured to invalidate the data at physical address while retaining the completion indicator e.g. the persistent metadata flag . The storage management layer may be configured to invalidate the data within the index and or reverse index while retaining storage metadata indicating successful completion of the atomic storage operation. Accordingly the storage management layer may invalidate the data of logical identifier while retaining the effect of the persistent metadata flag associated with the data.

In some embodiments an operation trimming data comprises storing a persistent indicator corresponding to the trim operation e.g. a persistent TRIM note packet or the like . During a restart and recovery operation the restart recovery module may be configured to exclude trimmed data in response to such indicators e.g. exclude data stored at physical address in response to a persistent indicator that the data was trimmed . The restart recovery module may be further configured to preserve the persistent metadata of the invalidated data e.g. apply and or effectuate the persistent metadata flag such that the data of logical identifiers and at physical addresses and are not misidentified as being part of a failed and or incomplete atomic storage operation. Accordingly the restart recovery module may utilize the persistent metadata flag of the invalidated data while excluding the data itself.

The disclosure is not limited to preserving persistent metadata through TRIM operations. As disclosed herein a data packet may be invalidated in response to a number of different storage operations including but not limited to overwriting modifying and or erasing the data. As disclosed above performing any of these types of operations in relation to logical identifier may result in invalidating the data stored at physical address e.g. the data comprising the persistent metadata flag indicating completion of the atomic storage request . In response to any such operation the storage management layer and or restart reconstruction module may be configured to preserve the effect of the persistent metadata flag s while invalidating the corresponding data. As described above preserving the persistent metadata flag s may comprise retaining storage metadata indicating that data at physical address is invalid but that the corresponding atomic storage operation was successfully completed excluding data at physical address while preserving and or applying the persistent metadata flag s at physical address and so on. Accordingly the storage management layer may be configured to invalidate a portion of data comprising persistent metadata flags indicating completion of the atomic storage request a particular data packet data segment or the like and to utilize the persistent metadata flags of the invalidated data despite the invalidation operation s . Preserving the persistent metadata flags of the invalidated data may comprise identifying other data of the atomic storage request e.g. other portions of data such as data packets data segments or the like as being part of a completed atomic storage request or non atomic storage request . Preserving the persistent metadata flags may further comprise the restart recovery module excluding the invalidated portion of data while identifying other portions of the corresponding atomic storage request as valid e.g. by applying the persistent metadata flags of the invalidated data portion .

The ID  persistent metadata flag A on physical storage locations and identifies data pertaining to the atomic storage operation ID that has not yet been completed. The persistent metadata flag A ID   on the physical storage location indicates successful completion of the atomic storage operation ID. Another persistent metadata flag A ID  identifies data pertaining to a different interleaved atomic storage operation. The persistent metadata flag A ID  of physical storage location indicates successful completion of the atomic storage request ID. Data that does not pertain to an atomic storage operation may comprise a 1 persistent metadata flag A or other pre determined identifier. When reconstructing storage metadata from the event log at the append point A if an atomic storage request identifier comprising a 0 flag e.g. ID  is encountered before or without encountering a completion persistent metadata flag A e.g. ID  all data associated with the persistent metadata flag A ID may be invalidated. By contrast after encountering the ID  flag all data associated with the ID persistent metadata flag A may be identified as pertaining to a completed atomic storage request. The persistent metadata A of data pertaining to atomic storage operations may be updated in response to grooming operations as described above. Accordingly relocating data of logical identifiers and to storage division after completion of the atomic storage operation ID may comprise updating the respective persistent metadata flags A of the corresponding data packets to indicate that the data is part of a completed atomic storage operation or non atomic storage operation . Although the extended persistent metadata flags A of may provide for more robust support for atomic storage operations they may impose additional overhead.

As indicated in data associated with logical identifiers and may comprise and or be associated with persistent metadata B that indicates that the data pertains to the atomic storage operation ID. In some embodiments the persistent metadata B may comprise persistent metadata flag s within a packet header. The disclosure is not limited in this regard however the persistent metadata B may be embodied in other forms. In some embodiments for example the persistent metadata B may be embodied in a persistent index reverse index separate data packet or segment or the like.

In the embodiment completion of the atomic storage operations ID and ID may be indicated by persistent metadata   and  . The persistent metadata   and   may be embodied as persistent metadata within the log . The persistent metadata   and or   may be embodied as separate data packets data segments persistent flags within other data packets or the like. The completion indicators   and or   may be configured to indicate completion of one or more atomic storage operations the completion indicator   may indicate completion of the atomic storage operation ID and the completion indicator   may indicate completion of the atomic storage operation ID. Accordingly the completion indicators   and or   may comprise and or reference the identifier s of one or more completed atomic storage operations ID and ID. Data of a failed and or incomplete atomic storage operation may be detected in response to identifying data comprising an atomic storage operation identifier that does not have a corresponding completion indicator.

In some embodiments the completion indicators   and or   may be configured to indicate completion of an atomic storage operation regardless of the log order of the indicator s   and or   within the log . The atomic storage module may be configured to append the persistent metadata   and or   to the log in response to completing the respective atomic storage operations ID and or ID. Completion of an atomic storage operation may comprise transferring data of the atomic storage operation into a powercut and or crash safe domain such as the media controller write buffer media write buffer queue described below request buffer described below or the like. Accordingly an atomic storage operation may be considered to be complete before all of the data pertaining the atomic storage operation has been actually written to the non volatile storage medium which may result in storing the completion indicator s   and or   before data of the corresponding atomic operations within the log . The restart recovery module may be configured to apply and or effectuate completion indicators   and or   regardless of their order within the log .

In some embodiments completion indicators   and or   may be consolidated. As described above grooming data pertaining to an atomic operation may comprise modifying persistent metadata of the data which may comprise updating persistent metadata flags B to indicate that the data packets are part of a completed atomic storage operation and or non atomic storage operation. Grooming may further comprise combining and or coalescing persistent metadata   and or  . For example the persistent metadata   and   may be combined into a single persistent metadata entry persistent note or data packet  N that indicates completion of a plurality of atomic storage operations e.g. atomic storage operations ID and ID . The persistent indicator s     and or  N may be removed from the log in response to updating the persistent metadata B of the data corresponding to the atomic storage operations e.g. updating the respective persistent metadata flags B of the data packets in grooming operation s as described above such that the persistent indicator s are no longer required to determine that the corresponding atomic storage operations were successfully completed.

As illustrated in two data packets are stored in a first logical erase block and two different data packets are stored in a second logical erase block . In the illustrated embodiment all four of the data packets are stored as a result of a single atomic storage request e.g. an atomic vector storage request . As indicated above the append point indicates where additional data may be written to the storage media .

Each logical erase block comprises two or more physical erase blocks e.g. erase blocks as depicted in . A logical erase block boundary separates each logical erase block . The logical erase block boundary may comprise a virtual or logical boundary between each logical erase block 

As illustrated in the embodiment of each data packet includes a header . Each header may comprise persistent metadata related to data within each packet . The data may comprise user data to be stored on and potentially retrieved from the storage media in response to requests by for example storage clients A N. In some embodiments a header and its associated data are both stored to the storage media in a single write operation e.g. in a packet format .

In a header of a first data packet is illustrated. The header may comprise persistent metadata including various flags . For example one or more bits of the header may comprise a data packet flag that when set to a particular value indicates when an associated data packet comprises user data. The position and number of the bits for each data packet flag within the header may be varied within the scope of the disclosed subject matter. Also in one embodiment the data packet flag may be located in the same position i.e. the same bit position within each header of each data packet 

The illustrated headers also include either a first persistent metadata flag in a first state or the first persistent metadata flag in a second state . The first persistent metadata flag may comprise a single bit within each header . For example the first persistent metadata flag in the first state may comprise a particular bit position such as the 56th bit within a header set to a high value a 1 while the first persistent metadata flag in the second state may comprise the same bit position in a different header set to a low value a 0 . Alternatively the first persistent metadata flag in the first state may comprise a particular bit position within the header set to a low value while the first persistent metadata flag in the second state may comprise the same bit position in a different header set to a high value. In one embodiment the first persistent metadata flag in the first or second state may each comprise a pattern of multiple bits or separate and distinct bit positions. Use of a single bit within each packet when data packets associated with an atomic storage request are stored contiguously provides the advantage that a very small amount of data is used on the storage media to indicate whether an atomic write operation failed or succeeded.

As illustrated in each header of the first three data packets comprises the first persistent metadata flag in the first state while the last data packet comprises the first persistent metadata flag in the second state . In one embodiment each of data packets except the last data packet stored on the storage media pursuant to an atomic storage request comprises the first persistent metadata flag in the first state . As illustrated the last packet includes the first persistent metadata flag in the second state which signals the end or completion of data written pursuant to an atomic write request. This embodiment is advantageous in that only one bit within each packet is needed to signal whether an atomic storage request was completed successfully. The first persistent metadata flags in the first and second states indicate not only that the data of these packets pertain to an atomic storage request but also identify a beginning and end or successful completion of the data associated with the atomic storage request.

However a problem may arise if the third and fourth data packets of the second logical erase block are erased. Some background information may be helpful to understand this problem. For example during a recovery or other process the event log e.g. the data stored sequentially together with persistent metadata as illustrated in the event of may be accessed to reconstruct a logical sequence of logical erase blocks e.g. from head to tail . This may be achieved through a scan of the erase blocks and in particular through examination and processing of metadata and sequence indictors stored in the erase block headers of the event log . The logical sequence of erase blocks may be formulated before performing recovery following an invalid shutdown or a restart operation such as a shutdown resulting from a power failure using either a forward or reverse sequence scan of the logical erase blocks stored on the media . After the logical sequence of erase blocks has been formulated reverse sequence scanning the event log or logical sequence of logical erase blocks based on the event log from the append point i.e. the tail in reverse sequence toward the head or beginning of the log in certain embodiments is initiated to identify failed atomic requests. In such a case if third and fourth data packets of the second logical erase block are erased the reverse sequence scanning from an append point could erroneously identify the first and second data packets as being associated with a failed atomic storage request because the first encountered packet does not include the first persistent metadata flag in the second state . Accordingly in one embodiment grooming or deletion of a logical erase block that includes an endpoint is prohibited.

As used in this application an endpoint may comprise the point immediately after the last packet which may be stored or identified in a volatile memory. Alternatively the final or last packet of an atomic write operation may comprise the endpoint.

As an alternative to prohibiting grooming or deletion of a logical erase block that includes an endpoint an incorrect determination that the first and second data packets relate to a failed atomic storage request is avoided by reference to sequence indicators such as the sequence indicators illustrated in . As noted above the sequence indicators identify or specify a log order of physical storage locations e.g. erase blocks . In particular in one embodiment sequence indicators of each erase block header comprise monotonically increasing numbers spaced at regular intervals. In view of the foregoing if a sequence indicator for a next logical erase block in the event log moving from left to right from the head to the tail of logical chain of erase blocks as specified by the event log is not a next sequence number in the sequence then for example the storage management layer recognizes that prior logical erase block does not end with a failed atomic request i.e. the first and second packets do not comprise a part of a failed atomic write.

In some embodiments excluding from the index may comprise bypassing each data packet associated with the failed atomic storage request during a scan of a log based structure e.g. the event log illustrated in or the ordered sequence of logical erase blocks specified by the log used to create the index . In another embodiment excluding from the index may further comprise removing each logical identifier that maps to each data packet associated with the failed atomic storage request from the index created by way of a scan of the log based structure. In yet another embodiment excluding from the index may further comprise erasing each data packet associated with the failed atomic storage request from the storage media by way of a storage space recovery operation which will be explained further below . Of course one or more of the foregoing embodiments may be combined or used with other embodiments for excluding the data packets from the index .

Thereafter a recovery grooming operation may be initiated to transfer the valid data packets but not the invalid data packets from the first logical erase block to the third logical erase block . More specifically the grooming operation for example may involve transfer of valid packets from the first logical erase block to the third logical erase block with a newly assigned sequence number e.g. a logical erase block immediately after the append point while data packets that are associated with a failed atomic write are not transferred to the logical erase block with the newly assigned sequence number. The recovery grooming operation may be performed as part of a storage recovery operation in response to a storage request e.g. a request to TRIM and or erase data on the erase block or the like .

As noted above a sequence number may be assigned to each erase block . The sequence numbers may be stored in logical erase block headers as illustrated in or at another location on the non volatile solid state storage media . The sequence numbers are utilized to create an ordered sequence of the logical erase blocks . The ordered sequence may be identified or specified by the log . The sequence numbers for each logical erase block in one embodiment are spaced at regular intervals. For example a consecutive series of logical erase blocks may be assigned the following sequence numbers 1 65 129 193 257 321 385 and 449. When it is determined that a new logical erase block needs be to utilized for the storage of data the new logical erase block may be assigned the next available sequence number in the series of sequence numbers . Accordingly in such an embodiment if the last sequence number assigned to a logical erase block is the sequence number a newly assigned erase block may be assigned the sequence number . Of course in alternative embodiments spacing between the sequence numbers may be at an interval other than 64 such as 32 or at irregular or varying intervals. Also the sequence numbers may be assigned in the cyclic fashion such that when the highest sequence number is utilized given the number of bits of metadata allocated for the sequence numbers the lowest sequence number no longer in use may be assigned to a newly identified erase block

In view of this background as illustrated in during the recovery grooming operation which is intended to transfer the valid data packs from the first logical erase block to the third logical erase block a second power failure may occur resulting in a failure of the grooming operation . Accordingly a technique for identification of such a failure would be helpful to prevent use of the invalid or partially written data saved in the third logical erase block or confusion as to whether the data in the first logical erase block or the third logical erase block should be utilized.

One such technique involves assigning a subsequence number rather than a sequence number to the logical erase block to which the valid data will be or is intended to be transferred. As indicated above in one embodiment the sequence numbers are spaced at regular intervals such as at intervals of 64 or at intervals of 32 as illustrated in . For example consecutive sequence numbers may increment the most significant bits of a fixed size sequence number by a particular increment while leaving the least significant bits unchanged. The subsequence number may be derived from a sequence number by incorporating the most significant bits of the sequence number from which the subsequence number is derived and altering such as incrementing or decrementing the least significant bits of the sequence number . As illustrated in the subsequence number may incorporate the most significant bits of the first sequence number and increment the least significant bits of the first sequence number to yield the subsequence number e.g. 1010001000001 comprising the same high order bits and incremented low order bits . By assigning the subsequence number to the third logical erase block the sequencing order of the erased blocks is maintained because the subsequence number is greater than the first sequence number from which the subsequence number is derived and is less than the next sequence number . Accordingly the subsequence number maintains an ordered sequence among logical erase blocks of the log based structure e.g. the log illustrated in such that an ordered sequence of storage operations completed on the storage media is preserved on the storage media .

It should also be noted that a subsequence number may be derived in various ways from a sequence number . For example a subsequence number could decrement the most significant bits of the first sequence number from which the subsequence number is derived and increment the least significant bits of the sequence number from which the subsequence number is derived.

In due course all of the data packets of the first logical erase block will be erased including erase block header from the storage media if the grooming operation were completed successfully. However erasure of the data packets and the erase block header of the first logical erase block may not occur immediately if the grooming operation is completed successfully. Moreover if the second power failure occurs during grooming e.g. while relocating the valid data from the first logical erase block to the third logical erase block the data packets in the third logical erase block could potentially be corrupt or incomplete.

Accordingly during a power on operation following the second power failure a restart recovery process may be initiated. During the restart recovery process the log will be created to formulate an ordered sequence of the logical erase blocks . During this process it may be determined that the first logical erase block has been assigned the first sequence number and the third logical erase block has been assigned the subsequence number derived from the first sequence number . As explained above this may indicate that either the data of the first logical erase block was not erased or that a grooming operation was interrupted. In either case the data packets of the third logical erase block are potentially corrupted or incomplete and should not be relied on as being valid. As a result the data packets erase block header and any other data stored in the third logical erase block should be erased or scheduled for erasure and should be excluded from the index . As indicated previously the index maps logical identifiers to physical locations or addresses and may comprise or be based on metadata stored on the media . 

Thereafter the append point would be positioned immediately to the right of invalid data packet as shown in . Reverse sequence scanning of the non volatile storage media from the append point would be commenced and would identify data packets of the first logical erase block and data packets of the second logical erase block as comprising a portion of a failed atomic write operation as a result of the first power failure . The valid data packets of first logical erase block will be groomed to the third logical erase block without transferring the invalid data packets to the third logical erase block . In one embodiment when the valid data packets are groomed to the third logical erase block the first persistent metadata flag for each of the valid data packets is set to a second state

In view of the foregoing it should also be observed that excluding from the forward or logical index during a restart recovery may comprise erasing each logical erase block of the non volatile solid state storage media comprising one or more data packets associated with the failed atomic storage request and transferring data packets e.g. valid data packets from the each logical erase block to a different location or logical erase block on the storage media . Also erasing each logical erase block during restart recovery may comprise assigning a subsequence number to a destination logical erase block configured to store transferred data packets i.e. valid data . Further erasing each logical erase block during a restart recovery process may comprise in response to identifying a first logical erase block having a sequence number and a third logical erase block having a subsequence number grooming the first logical erase block and as described above excluding each data packet of the first logical erase block associated with the failed atomic storage request from the index . Again the invalid data packets of the first logical erase block may immediately or eventually be erased from the media after the grooming operation is performed.

The recovery grooming operation if completed before normal input output operations commence in one embodiment avoids a scenario in which data packets associated with a failed atomic write operation could be considered valid because those data packets are removed from the media by the recovery grooming operation . The following example illustrates this point.

First a failed atomic write operation commences and is interrupted resulting in the invalid data packets being stored on the storage media . Second a power on operation is performed and through a scan the event log is formulated without engaging in the recovery grooming operation such that the invalid data packets are included in the event log and forward index . Third a second atomic write operation is commenced and successfully completed. Finally a reverse sequence scan from the append point which is positioned after the data packets associated with the second successful atomic write operation is subsequently initiated to identify packets associated with a failed atomic write operation. In this scenario the invalid packets will not be identified and removed from the storage media . This is because the reverse sequence scanning from the append point will encounter the packets associated with the second successful atomic write operation and determine that the second atomic write operation was successfully completed. In certain embodiments identifying the second successful atomic write operation may result in termination of the reverse sequence scanning and the invalid data packets will not be identified as being associated with a failed atomic write operation. Accordingly the invalid data packets will not be removed or otherwise excluded from the forward index or from the storage media .

Although A and depict embodiments for managing atomic storage operations using inter alia persistent metadata flags e.g. persistent metadata flags and so on the disclosure is not limited in this regard. The embodiments disclosed herein may be adapted to use other mechanisms for managing atomic storage operations. For example in some embodiments an atomic storage operation may comprise storing one or more persistent notes on the non volatile storage medium e.g. in the log . An open persistent note may indicate the start of an atomic operation and a close persistent note may indicate completion of the atomic storage operation. Packets of the atomic storage operation may be stored contiguously between the open and close persistent notes. If a close persistent note is not found packets after the open persistent note may be identified as part of an incomplete atomic storage operation and may be excluded as described above. depicts one embodiment of persistent notes for managing an atomic storage operation. The persistent note identifies the beginning of an atomic storage operation on the non volatile storage medium log . Accordingly the packets following the open persistent note are identified as part of an atomic storage operation. A close persistent note may be stored on the non volatile storage medium in response to completion of the atomic storage operation. If an open persistent note is not closed with a corresponding close persistent note the packets may be identified as being part of an incomplete atomic storage operation and excluded as described above.

In some embodiments the packets may comprise respective headers as described above e.g. headers . The headers may comprise persistent metadata indicating that the packets are part of an atomic storage operation. Alternatively persistent flags indicating membership in an atomic storage operation may be omitted since this information may be determined based upon the open persistent note . However in some embodiments a persistent flag indicating membership in the atomic storage operation may be included e.g. a persistent metadata flag in a first state . Other packets that are not part of the atomic storage operation may be interleaved with the packets . These packets may comprise respective persistent metadata flags to indicate that the packets are not part of the atomic storage operation e.g. persistent metadata flags in a second state . Accordingly when excluding packets due to a failed or incomplete atomic storage request the interleaved packets that were not part of the atomic storage operation may be retained not excluded as described above .

The embodiments disclosed herein may be configured to efficiently process vector storage requests. As disclosed herein a vector storage request refers to a storage request pertaining to one or more vectors I O vectors . A vector may pertain to a group set and or range of identifiers e.g. logical identifiers physical addresses buffer addresses or the like . A vector may be defined in terms of a base identifier e.g. starting point and length range and or extent. Alternatively a vector may be defined in set notation e.g. a set of one or more identifiers or ranges of identifiers . A vector storage request may therefore refer to a storage request comprising a plurality of sub requests or subcommands each of which pertains to a respective one of the vectors. For example a vector write operation may comprise writing data to each of a plurality of vectors each vector pertaining to a respective logical identifier range or extent. As described above in conjunction with the storage controller may be configured to store data of vector storage requests contiguously within a log on the non volatile storage media . Therefore data packets pertaining to disjoint non adjacent and or non contiguous vectors with respect to the logical address space may be stored contiguously within the log on the non volatile storage media .

The storage management layer may provide an interface through which storage clients may issue vector storage requests. In some embodiments the vector storage request interface provided by the storage management layer may include but is not limited to API library remote procedure call user space API kernel space API block storage interface or extension e.g. IOCTL commands and or extensions or the like. A vector may be defined as a data structure such as 

The iov base parameter may reference a memory or buffer location comprising data of the vector iov len may refer to a length or size of the data buffer and dest lid may refer to the destination logical identifier s for the vector e.g. base logical identifier the length of the logical identifier range may be implied and or derived from the input buffer iov len .

The vector write operation above may be configured to gather data from each of the vector data structures referenced by the iov pointer and or specified by the vector count parameter iov cnt and write the data to the destination logical identifier s specified in the respective iovect structures e.g. dest lid . The flag parameter may specify whether the vector write operation should be implemented as an atomic vector operation.

As illustrated above a vector storage request may comprise performing the same operation on each of a plurality of vectors e.g. implicitly perform a write operation pertaining to one or more different vectors . In some embodiments a vector storage request may specify different I O operations for each constituent vector. Accordingly each iovect data structure may comprise a respective operation indicator. In some embodiments the iovect structure may be extended as follows 

The iov flag parameter may specify the storage operation to perform on the vector. The iov flag may specify any suitable storage operation which include but is not limited to a write a read an atomic write a trim or discard request a delete request a format request a patterned write request e.g. request to write a specified pattern a write zero request or an atomic write operation with verification request allocation request or the like. The vector storage request interface described above may be extended to accept vector structures 

The flag parameter may specify whether the vector operations of the vector request are to be performed atomically.

The IO Count parameter may specify the number of vector storage operations encapsulated within the IO Vector e.g. the number of vector identifiers . The flag parameter may identify the storage operation to be performed on the IO Vector s . The flag parameter may specify any storage operation including but not limited to a write a read an atomic write a trim or discard request a delete request a format request a patterned write request e.g. request to write a specified pattern a write zero request or an atomic write operation with verification request allocation request or the like. The atomic write operation with verification request completes the atomic write operation and then verifies that the data of the request was successfully written to the storage media. As illustrated above the flag parameter may specify either atomic or non atomic storage operations.

The storage operation specified by the flag may be implemented on each of the IO Vector s . Accordingly the interface may be used to minimize the number of calls needed to perform a particular set of operations. For example an operation to store data pertaining to several contiguous or disjoint non adjacent and or non contiguous ranges may be encapsulated into a single vector storage request through the interface . Moreover the use of a flag parameter provides flexibility such that the interface may be utilized for various purposes such as atomic writes a trim or discard request a delete request a format request a patterned write request a write zero request or an atomic write operation with verification request.

In some embodiments an interface may provide for specifying a different storage operation for each IO Vector . The interface may include vector identifier s comprising respective flag parameters . The flag parameter s may specify a storage operation to perform on a particular IO Vector the flag parameters may be different for each IO Vector . Accordingly the interface may be configured to implement vector storage operations such that each sub request and or sub operation of the vector storage request may involve a different type of storage operation. For example the flag of a first IO Vector may specify a TRIM operation the flag of second IO Vector may specify a write operation and so on. The interface may comprise a top level flag parameter which may be used to specify default and or global storage flag parameters e.g. specify that the vector storage request is to be performed atomically as described above .

In some embodiments one or more of the operations of a vector storage request may comprise operations that do not directly correspond to storage operations on the non volatile storage media . For example the vector storage request may comprise a request to allocate one or more logical identifiers in the logical address space e.g. expand a file deallocate logical identifiers e.g. TRIM or delete data and so on. If the vector storage request is atomic the allocation deallocation operation s may not be reflected in the storage metadata until other operations of the atomic vector storage request are complete. In another example a TRIM subcommand may comprise modifying the storage metadata to indicate that data of one or more logical identifiers no longer needs to be preserved on the non volatile storage media . Modifying the storage metadata may comprise removing one or more entries from a forward index invaliding one or more packets and so on. These metadata operations may not be reflected in the storage metadata until other operations of the request are complete e.g. index entries may not be removed until other operations of the atomic storage request are complete . In some embodiments the allocation deallocation and or TRIM operations may be maintained in inflight metadata until completion of the atomic vector storage request as described above.

In some embodiments flags and or may specify an order of the vector storage request. For example the flags and or may indicate that operations of the vector storage request are to be completed in a particular order and or may be completed out of order. Ordering of the vector storage requests may be enforced by the storage management layer by use of the ordered queue request buffer described below or the like.

As described above in conjunction with the storage controller may be configured to store data packets pertaining to disjoint non adjacent and or non contiguous logical identifier ranges vectors contiguously within a log on the non volatile storage media . depicts execution of an atomic vector storage request which comprises appending data packets to a log on a non volatile storage media . In the example an atomic vector storage request may specify atomic write operations pertaining to a plurality of vectors including a vector at LID length a vector at LID length a vector at LID length and a vector at LID length . As illustrated in the index the vectors of the request correspond to disjoint non adjacent and non contiguous ranges with respect to the logical address space .

In response to the request the storage management layer may queue the sub requests of the atomic vector storage request which may comprise a TRIM storage request write storage request zero storage request. The storage requests may be queued in an ordered queue and or in a request buffer described below . Alternatively if the request is not an atomic operation or is being managed using an inflight index as described above the ordered queue may not be used.

The storage controller may be configured to service the atomic vector storage request by executing the sub requests of the vector storage request . The log storage module may be configured to append data packets pertaining to the vector storage request to the log on the non volatile storage medium .

For clarity of illustration in the example each logical identifier corresponds to data of a respective data packet e.g. each logical identifier references the same or less data as stored in a data packet segment described above . The disclosure however is not limited in this regard and could be adapted to implement any fixed and or variable mapping between logical identifiers and data segment size.

The logical to physical translation module may be configured to associate physical storage locations of the data packets with respective logical identifiers in the index . The index may comprise entries A D corresponding to the vectors of the request . The any to any mappings between logical identifiers and physical storage locations may allow data of the disjoint non adjacent non contiguous vectors to be stored contiguously within the log as illustrated in the entries A D may comprise respective mappings to arbitrary physical storage locations on the non volatile media such that the logical identifier ranges map to packets that are arranged contiguously within the log . The packets may comprise self describing persistent metadata e.g. headers to persist the association between the logical identifier s and the packets such that the any to any mappings of entries A D can be reconstructed.

The contiguous log format of the packets may facilitate tracking completion of the atomic vector storage request . As described above the packets may comprise a persistent metadata flag in a first state indicating that the packets are part of an incomplete or in process atomic storage request. The last final or termination packet written as part of the atomic vector storage request may comprise a persistent metadata flag in a second state indicating successful completion of the atomic vector storage request . As disclosed above the last packet may be the final data packet pertaining to the atomic vector storage request within the log . In some embodiments the packet may be the termination data packet of the atomic storage request e.g. the final packet written to the non volatile storage medium as part of the atomic vector storage request . Accordingly the packet may the last packet pertaining to the atomic vector storage request with respect to the log order of the packets . Alternatively or in addition the data packet may comprise separate persistent metadata such as a persistent note data packet and or data segment configured to indicate completion of the atomic vector storage request as described above in conjunction with .

As described above the contiguous layout of the packets and the corresponding flags in the log may allow incomplete atomic storage requests to be identified and rolled back such that data pertaining to the incomplete atomic storage requests are excluded from the storage metadata e.g. excluded from the index . For example if the persistent metadata flag in the second state is not found on the non volatile storage media the entries A D may be removed or omitted from the index and the packets may be invalidated as described above. The persistent metadata may be further leveraged to allow atomic storage operations to cross media boundaries e.g. erase block boundaries allow TRIM and or grooming operations and so on as described herein.

The persistent note and other persistent notes and or data of the atomic vector storage request may comprise and or reference persistent metadata flags which as described above may indicate that the persistent note and or data is part of an atomic storage operation. If a corresponding persistent metadata flag in a state indicative of completing the atomic storage operation is not found e.g. persistent flag does not exist on the medium the TRIM operation of the persistent note as well as other operations may be rolled back or excluded. Accordingly in the absence of the persistent metadata flag in the appropriate state or other condition indicating closure of the atomic storage operation the entry may not be removed from the index and the data packet may not be invalidated e.g. the TRIM operation will be rolled back .

The other storage operations of the atomic vector storage request may proceed as described above. The ZERO operation may comprise associating LID with a particular data pattern e.g. zeros by storing the data pattern in one or more packets on the log and or storing an indicator of the pattern e.g. a persistent note as described above. Completion of the composite atomic storage request may comprise storing a packet or other persistent data comprising a persistent metadata flag indicating completion of the request as described above.

The storage management layer may be configured to modify a storage request within the request buffer in response to one or more other storage requests by use of a request consolidation module . The consolidation module may be configured to selectively modify storage requests in response to other pending storage requests e.g. other storage requests in the request buffer . In some embodiments modifying a storage request comprises consolidating and or combining two or more storage requests removing or deleting one or more storage requests modifying the range extent and or set of logical identifiers pertaining to a storage request or the like. Modifying a vector storage request may comprise modifying one or more vectors provided in the vector storage request in response to other pending storage requests within the request buffer and or in response to other vectors within the vector storage request itself. The storage request consolidation module may improve efficiency by consolidating and or removing certain storage requests. For example certain storage clients A N such as file system storage clients B may make heavy use of certain types of storage requests e.g. TRIM storage requests . The storage requests may pertain to adjacent and or overlapping logical identifier ranges in the logical address space . Accordingly one or more storage requests and or portions thereof may be overridden subsumed made obsolete and or made redundant by other pending storage requests within the same logical address range or namespace e.g. other pending storage requests within the request buffer . The request consolidation module may modify the storage requests in the request buffer e.g. join combine and or remove buffered storage requests to thereby reduce the overall number of storage requests processed by the storage controller which may improve performance and reduce wear on the non volatile storage media . In some embodiments modifying a storage request comprises acknowledging completion of the storage request without actually performing and or implementing the storage request e.g. acknowledging a TRIM storage request made redundant by one or more other pending storage requests without actually implementing the redundant TRIM request .

The storage management layer may be configured to selectively buffer and or modify storage requests. In some embodiments the storage management layer may be configured to receive storage requests from different storage clients A N both within the same host or on other hosts . The storage management layer may be configured to buffer and or modify the storage requests of select storage client s A N to the extent that the storage client s A N are configured to operate using the same logical identifiers namespace and or the like. Storage requests of other unselected storage clients e.g. file system storage client B may not be buffered in the request buffer and or modified by the request consolidation module . In some embodiments the storage management layer may be configured to selectively buffer storage requests of a particular type. For example the request buffer may be configured to only buffer TRIM storage requests. Alternatively or in addition the request buffer may comprise a plurality of separate request buffers for different storage client s A N and or different types of storage requests. For example the request buffer may be configured to buffer sub requests or subcommands of vector storage requests and or atomic vector storage requests. The request consolidation module may be configured to consolidate the sub requests and or subcommands as described herein.

In some embodiments the request consolidation module may be configured to modify a vector storage request and or one or more vectors of a vector storage request e.g. one or more sub requests and or subcommands of the vector storage request . The request consolidation module may be configured to identify and or analyze the respective vectors of the vector storage request by use of the vector storage module and or atomic storage module . The storage requests corresponding to the vector storage request may be buffered in the request buffer along with or separately from similar other non vector storage requests and or storage requests of other vector storage requests. Buffering a vector storage request may therefore comprise generating sub requests and or subcommands separate storage requests corresponding to each of the vectors of the vector storage request. For example a vector storage request to TRIM data in vectors . . . N may correspond to N separate storage requests wherein each of the N storage requests is configured to TRIM a range of logical identifiers specified in a respective one of the . . . N vectors. The constituent storage requests of atomic vector storage requests may be similarly buffered in the request buffer . The storage requests of an atomic vector storage request may be buffered in an ordered queue and or ordered buffer as described above.

The request consolidation module may be configured to modify one or more storage requests in the request buffer based on one or more other storage requests within the request buffer . The storage requests may comprise storage requests of vector storage requests and or non vector storage requests. Modifying a storage request may comprise combining and or coalescing two or more of the storage requests. For example individual storage requests pertaining to overlapping and or contiguous sets of logical identifiers in the logical address space may be combined into a single storage request which may include and or combine the overlapping ranges. depicts one embodiment of a request buffer . The request buffer may be ordered such that storage requests are executed and or serviced by the request execution module described below in the order in which the storage requests were received e.g. in a first in first out FIFO configuration in which storage requests are pushed into the request buffer at the incoming end of the request buffer and are popped for execution at the outgoing end of the request buffer .

Storage requests may be added to the request buffer as they are received at the storage controller . Adding a vector storage request to the request buffer may comprise adding storage requests corresponding to each of a plurality of vectors of the vector storage request to the request buffer . The storage controller may be configured to execute and or service the storage requests as described herein which may comprise appending one or more data packets to a log on the non volatile storage media modifying the storage metadata and so on. In some embodiments the storage controller comprises a request execution module configured to service and or execute storage requests in the request buffer . The request execution module may be configured to execute buffered storage requests in a particular order e.g. in the order in which the storage requests were received for example the request execution module may be configured to pop buffered storage requests from an end of an ordered queue FIFO or the like. Alternatively or in addition the request execution module may be configured to service and or execute storage requests out of order. Alternatively or in addition the request execution module may be configured to change the order of storage requests within the request buffer based on criteria that optimizes use of the storage media and preserves the integrity of the storage operations. Executing or servicing a storage request may comprise performing one or more storage operations specified by the store request which as described herein may comprise appending one or more data packets to a log on the non volatile storage medium by use of the log storage module reading portions of the non volatile storage medium transferring data pertaining to a storage request updating storage metadata and so on. The request execution module may be further configured to execute and or service atomic storage requests by use of the atomic storage module which may comprise storing persistent metadata on the non volatile storage medium to track completion of the atomic storage request s as described herein.

In some embodiments the request execution module is configured to execute storage requests according to a particular interval and or schedule. The scheduling may be adaptive according to operating conditions of the storage controller and or in response to trigger conditions such as filling the request buffer and or ordered queue buffering a threshold number of storage requests and so on.

As disclosed above the request consolidation module may be configured to modify one or more of the storage requests within the request buffer . The request consolidation module may be configured to modify the storage requests in response to other pending storage requests within the request buffer which may comprise combining and or joining two or more storage requests into a single storage request that operates on a logical union of the overlapping and or adjacent set s of logical identifiers. In the example the storage request buffer comprises TRIM storage requests pertaining to logical identifiers . . . . The request consolidation module may be configured to aggregate the TRIM storage requests in the request buffer to form a single combined TRIM storage request . The storage request to TRIM logical identifier is not adjacent with and or overlap the logical identifiers . . . and as such may remain as a separate storage request. Coalescing the TRIM storage requests as described herein may reduce wear on the non volatile storage media . For example if the TRIM storage requests are persistent e.g. comprise storing a persistent note on the non volatile storage media forming the aggregate TRIM storage request may reduce the total number of persistent notes stored on the non volatile storage medium . In some embodiments a persistent TRIM note may be configured to TRIM one or more disjoint non adjacent and or non contiguous logical identifier ranges or vectors. Accordingly the storage request consolidation module may be configured to join the trim storage request pertaining to logical identifier into a vector TRIM storage request e.g. request to TRIM logical identifiers . . . and not shown in .

The request consolidation module may be configured to modify storage requests in the request buffer such that the modifications do not affect other pending storage requests. As illustrated in the request buffer may comprise a storage request to read data of logical identifier . The request consolidation module may be configured to schedule the read storage request before the combined storage request to TRIM logical identifiers . . . such that the read storage request can be completed scheduling the read storage request after the combined TRIM storage request would result in losing access to the data of logical identifier .

The request consolidation module may be further configured to remove and or delete one or more storage requests from the request buffer . A storage request may be removed and or deleted from the request buffer in response to determining that the storage request s would be obviated by one or more other pending storage requests in the request buffer . As illustrated in the request buffer comprises a plurality of storage requests to TRIM and write to various logical identifiers in the logical address space . The request consolidation module may determine that one or more of the TRIM and or write storage requests are obviated by other pending storage requests in the request buffer the write request to logical identifier . . . overlaps several of the TRIM storage requests and the write request to logical identifiers . . . . The request consolidation module may be configured to remove and or delete the storage requests that are obviated by the write storage request. Storage requests that are not obviated by the write storage request may be retained and or modified e.g. the storage request to TRIM logical identifiers . . . may be modified to TRIM only logical identifier which is not obviated by the write storage request . As described above the request consolidation module may configure the modification such that other pending storage requests are not affected. For example the write operation to logical identifiers . . . may not be deleted if there is a storage request to read data of one or more of the logical identifiers . . . before the write to . . . in the request buffer . Removing a storage request may further comprise acknowledging completion of the storage request. The storage request may be acknowledged even if the storage request is not actually implemented e.g. is obviated by another storage request in the request buffer .

As described above the request buffer may be configured to buffer storage requests received from one or more storage clients A N including vector storage requests and or atomic vector storage requests. The request consolidation module may be configured to modify an atomic vector storage request and or the constituent storage requests thereof in response to other pending storage requests in the request buffer and or within the atomic vector storage request itself . In some embodiments however the request consolidation module may only modify storage requests within respective atomic vector storage operations without regard to other non atomic storage requests in the request buffer . For example the request consolidation module may consolidate adjacent and or overlapping write and or TRIM requests within an atomic vector storage request as described above. However the request consolidation module may not modify the sub requests of the atomic vector storage request in response to other storage requests in the request buffer that are not part of the atomic vector storage request.

As the method begins an atomic storage request is received for example at the storage management layer . The atomic storage request may be received for example through an interface such as the storage management layer by use of one or more of the interfaces . The atomic storage request may involve a single atomic storage operation or a plurality of vector storage operations. The storage request may pertain to disjoint non adjacent and or non contiguous ranges and or sets of logical identifiers in the logical address space .

Step may comprise storing and or appending data pertaining to the atomic storage request contiguously to a log on the non volatile storage media . In some embodiments the data may be appended in a packet format such as the packet format described above in conjunction with . Step may further comprise storing the data with persistent metadata e.g. persistent metadata flags to track completion of the atomic storage request as illustrated for example in FIGS. A and B C. The persistent metadata may comprise persistent metadata flags configured to identify data that is part of an incomplete atomic storage operation. The persistent metadata may comprise persistent metadata flags of one or more data packets. The persistent metadata may further comprise one or more persistent indicators that the atomic storage request is complete. In some embodiments a completion indicator may comprise storing a persistent metadata flag in a last data packet stored as part of the atomic vector storage request e.g. the final data packet within the log wherein the persistent metadata flag is configured to indicate completion of the atomic storage request. In some embodiments the atomic storage request may involve a plurality of storage operations each of which may encompass storage operations in a plurality of different logical erase blocks . The log storage module may be configured to store persistent metadata such as a header and associated user data within a data packet or other persistent note on the storage media in one or more write operations i.e. as part of one or more operations performed on the storage media .

Step may comprise acknowledging completion of the atomic storage request to a storage client A N or the like. The atomic storage module may be configured to send acknowledgment asynchronously via a callback or other mechanism. Alternatively the atomic storage request may be synchronous and the atomic storage module may transmit acknowledgment by a return from a synchronous function or method call.

In some embodiments acknowledgment is provided as soon as it can be assured that the data of the atomic storage request will be persisted to the non volatile storage media but before the data is actually stored thereon. For example the atomic storage module may send acknowledgment upon transferring data of the atomic storage request into a buffer of the non volatile storage device or into a write data pipeline transferring the data to a storage controller e.g. within a protection domain of a storage controller or the like. Alternatively acknowledgment is performed after the data of the atomic storage request has been persisted on the non volatile storage media .

The restart recovery module may be configured to identify data packets of incomplete atomic storage requests in response to a data packet preceding the append point comprising a persistent indicator that satisfies an incomplete atomic write criteria. For example the persistent indicator may satisfy the incomplete atomic write criteria if the preceding data packet comprises the first persistent metadata flag in the first state e.g. a state indicating that the packet is part of an incomplete or in process atomic storage request .

The restart recovery module may be further configured to identify one or more data packets associated with the incomplete atomic storage request by for example identifying data packets including the first persistent metadata flag in a first state . The one or more data packets associated with the incomplete atomic storage request may be positioned sequentially within the log based structure . One example of an incomplete atomic storage request involving sequentially positioned packets is illustrated in i.e. the data packets of are associated with the incomplete atomic storage request and are positioned sequentially in a log based structure . It should be noted that identifying the incomplete atomic storage request and identifying one or more packets associated with the incomplete atomic storage request may be performed consecutively or concurrently.

Step comprises excluding the data packet associated with the incomplete atomic storage request from an index such as a forward index or a reverse index . The restart recovery module may exclude by bypassing each data packet associated with the incomplete atomic storage request during a scan of the log based structure used to create the index . In addition the exclusion module may exclude by removing each logical identifier that maps to each data packet associated with the incomplete atomic storage request from the index created by way of a scan of the log based structure .

Step may comprise grooming e.g. erasing the data packets associated with the incomplete atomic storage request by way of the storage space recovery operation. The groomer module may be further configured to exclude by erasing each logical erase block of the solid storage media comprising one or more data packets associated with the incomplete atomic storage request and transferring data packets from each logical erase block to a different location on the non volatile storage media as illustrated for example in . The groomer module may also erase by assigning a subsequence number to a destination logical erase block configured to store the preserved data packets as is also illustrated for example in . During a power on operation of the storage device the groomer module may erase by identifying a first logical erase block having a sequence number and another logical erase block having a subsequence number derived from the sequence number and grooming the first logical erase block as illustrated in and excluding each data packet associated with the failed atomic storage request from the index . Excluding may further comprise storing a physical TRIM note identifying the data packet s of the incomplete atomic storage request.

Step may comprise resuming input output operations after restart recovery is complete. Performing exclusion before commencing normal input output operations in one embodiment simplifies the restart recovery process by preventing normal input output operations from interfering with the restart recovery process and or propagating errors in data stored on the media .

As disclosed above a vector storage request may comprise a request to perform one or more operations on one or more vectors which may pertain to respective sets and or ranges within a logical address space . A portion of one or more of the vectors may overlap and or may be logically adjacent and or one or more operations may negate e.g. overlay one or more other operations. For example a vector storage request may comprise a request to perform a TRIM operation on two vectors. The vectors may pertain to overlapping and or adjacent sets of logical identifiers e.g. the operations may TRIM logical identifiers and respectively . The request consolidation module may identify the overlapping TRIM operations within the vector storage request and in response may modify the vector storage requests. Modifying the vector storage request may comprise modifying one or more of the vectors of the vector storage request e.g. combining the TRIM requests into a single request to TRIM logical identifiers . In another example a vector storage request may comprise requests to TRIM the same set of logical identifiers the request consolidation module may be configured to remove one or more of the overlapping vectors of the vector storage request. For example a vector storage request comprising multiple requests to TRIM logical identifiers may be combined into a single TRIM request comprising the vector . The request consolidation module may be configured to consolidate or join logically adjacent requests and or vectors. For example a vector storage request may comprise requests to TRIM logical identifiers and the request consolidation module may be configured to consolidate these two separate vectors into a single vector .

The request consolidation module may be further configured to consolidate atomic vector storage requests e.g. requests received via the interface described above . For example an atomic vector storage request may comprise a vector configured to TRIM a particular range of logical identifiers followed by a vector configured to write to the same vector or a portion of the same vector . The request consolidation module may be configured to detect that the vector pertaining to the TRIM operation is obviated by the vector pertaining to the write operation and in response may omit storage request s of the TRIM vector and or omit the portion of the TRIM operation that is obviated by the write .

The request consolidation module may be configured to modify storage requests by examining the vectors within respective vector storage requests comparing vectors of different vector storage requests examining storage requests in a storage request buffer identifying I O vectors for consolidation and or modifying the buffered storage requests and so on as described above.

Step may comprise buffering one or more storage requests. As described above buffering storage requests may comprise adding the storage requests to a buffer the request buffer queuing storage requests e.g. adding storage requests to an ordered queue holding storage requests delaying storage requests and or the like. Step may comprise buffering storage requests buffering vector storage requests buffering atomic vector storage requests and so on. Buffering a vector storage request and or atomic vector storage request may comprise extracting one or more vector s from the storage request and or generating storage requests corresponding to each of the vectors within the vector storage request e.g. buffering a storage request for each vector within the vector storage request . Step may comprise retaining an order of the storage requests within the buffer queue or other data structure. Accordingly the buffering of step may be configured to maintain the storage requests in the same or equivalent order as the storage requests were received. For example in some embodiments the request buffer comprises an ordered queue such as a first in first out FIFO or the like. Storage requests may flow through the ordered queue e.g. by first in first out processing as disclosed above.

Step may comprise modifying one or more of the storage requests vector storage requests and or vectors. The modification of step may comprise removing joining combining and or modifying one or more storage requests vector storage requests and or vectors as described above. Step may comprise identifying storage requests and or vectors that pertain to overlapping and or adjacent ranges of logical identifiers within the logical address space . Accordingly step may comprise comparing pending storage requests and or vectors of pending vector storage requests atomic and or otherwise to other pending storage requests and or vectors within the request buffer . Step may further comprise identifying storage requests and or vectors that can be combined modified and or removed. As disclosed above storage requests that pertain to overlapping ranges of logical identifiers may be combined which may comprise modifying the storage request to reference a vector and or modifying the set range extent and or logical identifiers of one or more vectors. Step may further comprise identifying storage requests and or vectors that are made redundant by one or more other pending storage requests and or vectors as disclosed above.

In some embodiments the modification of step may operate within the vectors of a particular vector storage request. Accordingly the buffering of step may be omitted and step may operate within an individual vector storage request and or an individual atomic vector storage request . Alternatively or in addition the request consolidation module may treat some storage requests separately. For example atomic vector storage requests may be buffered and or consolidated separately from other storage requests. In other embodiments steps and or may comprise buffering and or modifying storage requests of a particular storage client A N e.g. storage requests of a file system storage client B buffering and or modifying storage requests of a particular type e.g. only TRIM storage requests or the like

Step may comprise servicing the buffered storage requests. Step may comprise servicing one or more of the storage requests and or vectors modified at step . Step may be performed at a predetermined time and or operation interval. In some embodiments step is performed in response to a trigger condition which may include but is not limited to filling the request buffer e.g. a FIFO ordered queue or the like buffering a predetermined number of storage requests a user request to flush the request buffer or the like. Step may further comprise acknowledging completion of one or more storage requests. The request s may be acknowledged after all of the storage requests of a particular vector storage request or atomic vector storage request are complete. In some embodiments step may comprise acknowledging completion of a storage request that was modified at step . The acknowledgement may pertain to a storage request and or vector that was removed or omitted at step .

Step may comprise identifying a plurality of storage requests of a vector storage request e.g. a plurality of sub requests or sub operations of the vector storage request . The vector storage request may pertain to a plurality of vectors each vector corresponding to a range of one or more logical identifiers of a logical address space . Two or more of the vectors may pertain to logical identifiers that are disjoint non adjacent and or non contiguous with respect to the logical address space . The storage requests identified at step may correspond to respective vectors of the vector storage request and or may comprise different types of storage operations e.g. in accordance with a vector flag parameter or vector storage request flag parameter .

Step may comprise modifying one or more of the storage requests of the vector storage request based on and or in response to other pending storage requests by use of the request consolidation module described above . Step may comprise buffering the identified storage requests in a request buffer which may comprise other storage requests of other storage clients A N in addition to the storage requests identified at step . Alternatively step may comprise modifying the storage requests in response to the vector storage request as identified at step without regard to other storage requests buffered or otherwise . Accordingly the other storage requests may comprise other storage requests within the vector storage request as identified at step and or other storage requests buffered in the request buffer that are independent of the vector storage request e.g. in addition to the storage requests of the vector storage request of step .

Modifying a storage request may comprise joining and or combining two or more storage requests removing or deleting one or more storage requests that are obviated e.g. negated by one or more other pending storage requests modifying the logical identifier s and or vector of the storage request and so on as described above. The modifications of step may be configured to maintain consistency with other storage requests as described above the request consolidation module may be configured to modify and or order the storage requests such that the modifications do not affect other pending storage requests.

Step may comprise servicing the storage requests of the vector storage request as modified at step . Step may comprise storing data packets of the vector storage request contiguously within a log on the non volatile storage media e.g. by use of the log storage module . Storing the data packets contiguously may comprise appending the data packets at an append point storing the data packets sequentially from the append point and or associating the data packets with respective sequence indicators on the non volatile storage media such that a log order of the data packets is retained on the non volatile storage media .

In some embodiments the vector storage request of step may be an atomic vector storage request. Accordingly step may further comprise storing one or more persistent indicators on the non volatile storage media to identify data pertaining to the atomic vector storage request and or to indicate completion of the atomic vector storage request. Step may comprise configuring one or more data packets of the atomic vector storage request to include respective persistent indicators e.g. persistent metadata flags that indicate that the one or more data packets pertain to an atomic storage request that is incomplete and or in process. Step may further comprise configuring a last data packet of the atomic storage request to include a persistent indicator e.g. persistent metadata flag that indicates that the atomic storage operation is complete.

Reference throughout this specification to features advantages or similar language does not imply that all of the features and advantages that may be realized are included in any single embodiment. Rather language referring to the features and advantages is understood to mean that a specific feature advantage or characteristic described in connection with an embodiment is included in at least one embodiment. Thus discussion of the features and advantages and similar language throughout this specification may but do not necessarily refer to the same embodiment.

Furthermore the features advantages and characteristics described herein may be combined in any suitable manner in one or more embodiments. One skilled in the relevant art will recognize that the disclosed embodiments may be practiced without one or more of the specific features or advantages of a particular embodiment. In other instances additional features and advantages may be recognized in certain embodiments that may not be present in all embodiments. These features and advantages of the disclosed embodiments will become more fully apparent from the following description and appended claims or may be learned by the practice of the embodiments as set forth hereinafter.

Many of the functional units described in this specification have been labeled as modules in order to more particularly emphasize their implementation independence. For example a module may be implemented as a hardware circuit comprising custom VLSI circuits or gate arrays off the shelf semiconductors such as logic chips transistors or other discrete components. A module may also be implemented in programmable hardware devices such as field programmable gate arrays programmable array logic programmable logic devices or the like.

Modules may also be implemented in software for execution by various types of processors. An identified module of executable code may for instance comprise one or more physical or logical blocks of computer instructions which may for instance be organized as an object procedure or function. Nevertheless the executables of an identified module need not be physically located together but may comprise disparate instructions stored in different locations which when joined logically together comprise the module and achieve the stated purpose for the module.

Indeed a module of executable code may be a single instruction or many instructions and may even be distributed over several different code segments among different programs and across several memory devices. Similarly operational data may be identified and illustrated herein within modules and may be embodied in any suitable form and organized within any suitable type of data structure. The operational data may be collected as a single data set or may be distributed over different locations including over different storage devices and may exist at least partially merely as electronic signals on a system or network. Where a module or portions of a module are implemented in software the software portions are stored on one or more computer readable media.

Reference throughout this specification to one embodiment an embodiment or similar language means that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment. Thus appearances of the phrases in one embodiment in an embodiment and similar language throughout this specification may but do not necessarily all refer to the same embodiment.

Reference to a computer readable medium may take any form capable of storing machine readable instructions on a digital processing apparatus. A computer readable medium may be embodied by a compact disk digital video disk a magnetic tape a Bernoulli drive a magnetic disk a punch card flash memory integrated circuits or other digital processing apparatus memory device.

Furthermore the features structures or characteristics disclosed herein may be combined in any suitable manner in one or more embodiments. In the following description numerous specific details are provided such as examples of programming software modules user selections network transactions database queries database structures hardware modules hardware circuits hardware chips etc. to provide a thorough understanding of the disclosed embodiments. One skilled in the relevant art will recognize however that the teachings of the disclosure may be practiced without one or more of the specific details or with other methods components materials and so forth. In other instances well known structures materials or operations are not shown or described in detail to avoid obscuring aspects of the disclosed embodiments.

The schematic flow chart diagrams included herein are generally set forth as logical flow chart diagrams. As such the depicted order and labeled steps are indicative of one embodiment of the presented method. Other steps and methods may be conceived that are equivalent in function logic or effect to one or more steps or portions thereof of the illustrated method. Additionally the format and symbols employed are provided to explain the logical steps of the method and are understood not to limit the scope of the method. Although various arrow types and line types may be employed in the flow chart diagrams they are understood not to limit the scope of the corresponding method. Indeed some arrows or other connectors may be used to indicate only the logical flow of the method. For instance an arrow may indicate a waiting or monitoring period of unspecified duration between enumerated steps of the depicted method. Additionally the order in which a particular method occurs may or may not strictly adhere to the order of the corresponding steps shown.

