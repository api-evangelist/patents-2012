---

title: Open station canonical operator for data stream processing
abstract: Customizing functions performed by data flow operators when processing data streams. An open-executor(s) is provided as part of the data stream analytics platform, wherein such open-executor allows for both of: 1) customizing user plug-ins for the operators, to accommodate changes in user requirements; and 2) predefining templates that are based on specific meta-properties of various operators and that are common therebetween.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09524184&OS=09524184&RS=09524184
owner: Hewlett Packard Enterprise Development LP
number: 09524184
owner_city: Houston
owner_country: US
publication_date: 20120731
---
This application is related to U.S. patent application Ser. No. 13 563 176 filed Jul. 31 2012 and entitled DATABASE RETRIEVAL IN ELASTIC STREAMING ANALYTICS PLATFORM . The entirety of the above referenced applications is incorporated herein by reference.

As large scale computations are becoming more and more affordable distributed stream processing is receiving significant attention by various commercial enterprises. As such real time stream analytics has increasingly gained popularity since enterprises need to capture and update business information just in time analyze continuously generated moving data from sensors mobile devices social media of all types and gain live business intelligence.

A stream analytics process with continuous and graph structured dataflow can include multiple operators with various pipes that connect such operators. In this regard many dataflow operators can share common execution patterns in I O blocking characteristics data grouping characteristics which can be categorized for introducing unified system support and the like. Such commonalities among dataflow operators can be considered as their meta properties and hence categorized for providing unified system support e.g. operators that are placed in a same category can share such meta properties execution patterns . At the same time operators for stream analysis can further be defined based on application logic which if written manually can lead to fragile code erroneous results jeopardizing operational semantics and complicate user efforts interactions.

The requirement for categorizing stream operators and their running patterns to provide automatic support can further be employed to ensure that such operators are executed optimally and consistently. To this end users efforts for managing these properties manually which are deemed tedious and error prone can further be facilitated.

Moreover additional requirements exist to elastically and accurately parallelize a stateful operator which is history sensitive by relying on prior state and data processing results. Furthermore a demand exists for manner of analyzing unbounded stream granularly to ensure sound semantics e.g. aggregation .

For example an infinite input data stream can be processed chunk by chunk wherein each operator may punctuate data based on different chunking criteria. Such chunking criteria can be a predetermined time period e.g. 1 minute or 1 hour time windows which may require compliance with defined constraints e.g. the frame of a downstream operator being same as or some integral number of the frame of its upstream operator . As such granulizing dataflow analytics represents another kind of common behavior of stream operators which further requires systematic support. Failure to properly address and abstract such issues in stream processing systems and instead shifting them to user programs can result in fragile code disappointing performance and incorrect results.

Various implementations of the subject disclosure standardize operational pattern of stream operators and support operator execution patterns automatically and systematically via an open executor in a stream analytics platform that includes continuous real time data flow with graph structured topology. Such open executor can provide designated system support while being receptive or open for accepting customized application logic to be plugged in hence the term open executor . Furthermore the open executor can represent a container for a stream operator that is classified based on predetermined meta properties.

In a related implementation a station class hierarchy can be defined wherein operations for each class is further facilitated based on its association with a respective open executor and its related system utilities. For example in an object oriented OO programming context the open executor of the subject disclosure can be coded by invoking certain abstract functions or processes which are to be implemented by users based on their application logic.

In a related implementation the open executor of the subject disclosure can facilitate developing streaming operations in different characteristic categories and further provide for canonical mechanisms to parallelize stateful and granule dataflow analytics. Accordingly a data flow s can be handled chunk wise and group wise for each vertex that represents a logical operator in its dataflow graph. Moreover operation parallelization or launching multiple instances can occur with input data partition or grouping wherein input data partitioning and data buffering remain consistent at each operation instance.

In this regard the open executor can include a dynamic behavior component and a template behavior component . As such the operators on a parallel and distributed dataflow infrastructure can be performed by both the infrastructure associated with the template behavior component and the user programs associated with the dynamic behavior component . The template behavior of a stream operator can further depend on its meta properties and running pattern.

In the data analytics platform of a dataflow element or tuple may either be originated from a data source or derived by a logical operator wherein the operator is stationed and continuous. Such logical operator can have multiple instances threads over multiple machine nodes. Moreover streams from the instances of operator A to the instances of operator B can be grouped e.g. partitioned in the same manner and as described in detail below wherein multiple logical operators B B . . . Bcan exist for receiving the output stream of A yet each with a different data partition criterion.

In context of framework for processing parallel problems across substantially large data set the open executor of the subject disclosure can provide a substantially flexible and elastic approach e.g. as compared to systems such as Hadoop when handling dynamically parallelized operations in a general graph structured dataflow topology. Moreover the subject disclosure can support the template behavior or operation patterns automatically and systematically.

In a related implementation of the subject disclosure the open executor can represent a container for a stream operator wherein unlike applying an operator to data stream processing can be characterized by flowing of data through a stationed operator. In this regard stream operators with predetermined common meta properties can be executed by the class of open stations which are specific to such operators.

As explained earlier the open station s can be classified into a station hierarchy wherein each class can be associated with an open executor and its related system utilities. To this end the station provides designated system support while being receptive and open for accepting the application logic to be plugged in and received by the dynamic behavior component .

In a related aspect the subject disclosure can facilitate safe parallelization in data stream processing. Typically a key to ensuring safe parallelization is to handle data flow group wise for each vertex representing a logical operator in the dataflow graph wherein the operation parallelization with multiple instances occurs with input data partition grouping which further remains consistent with the data buffering at each operation instance. Such may ensure that in the presence of multiple execution instances of an operator O every stream tuple is processed once and only once by one of the execution instances of O. Furthermore the historical data processing states of every group of the partitioned data are buffered with one and only one execution instance of O.

Likewise the open executor of the subject disclosure facilitates performing granule semantics and managing dataflow in a chunk wise manner by punctuating and buffering data consistently. In general the proposed canonical operation framework enables standardizing various operational patterns of stream operators and support such patterns systematically and automatically. In this regard the open executor of the subject disclosure facilitates such operation in real time continuous elastic data parallel and topological stream analytics. Accordingly the analytics platform of the subject disclosure can be characterized by real time and continuous with the capability of parallel and distributed computation on real time and infinite streams of messages events and signals. Moreover the analytics platform of the subject disclosure can be characterized by topological features to manage data flows in complex graph structured topology and not limited to the map reduce scheme. Furthermore and unlike a statically configured Hadoop platform the analytics platform with the open executor of the subject disclosure can scale out over a grid of computers elastically for parallel computing.

In general such can be deemed similar to reorganizing redistributing of data e.g. data shuffling from a Map node to a Reduce node. Furthermore there can be multiple logical operators B B . . . B for receiving the output stream of A but each with different data partition criterion referred to as inflow grouping attributes e.g. as in SQL group by . In this regard tuples that are grouped together and in the same partition can possess the same inflow group keys .

As described in detail with the examples below the tuples representing the traffic status of a express way xway direction dir and segment seg can be partitioned and grouped by attributes wherein tuples of each group have identical inflow group key derived from the values of xway dir and seg. It is noted that an operation instance may receive multiple groups of data.

Moreover for history sensitive data parallel computation an operation instance may maintain a state computed from its input tuples for example. Such state can be generally provided as a Key Value KV store wherein keys that are referred to as caching group keys are Objects e.g. String extracted from the input tuples. Likewise values can be deemed Objects that are derived from the past and present tuples such as numerical objects e.g. sum count list objects certain values derived from each tuple and the like. The multiple instances of a logical operation can run in data parallel provided that the inflow group keys are employed as the caching group keys. Accordingly the KV store can be represented as Group wise KV store GKV .

In addition and as described in detail with respect to the following example for continuous yet granular computation the input data can be punctuated into chunks e.g. 1 minute time windows. Moreover the processing of the tuples associated with a chunk can be referred to as an epoch. In this regard predetermined operations e.g. aggregations can be applied to the data chunk wise by the end of each epoch and other operations may be deemed tuple wise. Essentially any operation may update the GKV for chunk wise aggregation sliding window based calculation and the like.

Such combination of group wise and chunk wise stream analytics provides a generalized abstraction for parallelizing and granulizing the continuous and incremental dataflow analytics. In a dataflow graph created accordingly every vertex can be characterized by the inflow grouping and the corresponding GKV caching which remains consistent with the inflow grouping.

In particular a task can be contained in a station with task characteristics specified as the properties of that station. For example a station can employ a Java object serving as the continuous and repeated executor of a task. In one implementation mechanisms for capturing characteristics of tasks can include 1 specializing customizing station classes sub classing by specific task characteristics and 2 providing abstract methods to be implemented for each particular type station such as a method for setting group wise exec epoch. For example specified in a station class there can exist two kinds of predefined processes namely 1 the system defined methods and 2 the user defined methods. Accordingly the system defined methods can include the open executor that is open to plugging in application logic wherein abstract methods can be invoked or implemented by users according to the application logic. Similarly the station class can include abstract methods that can be implemented by the user based on the application logic.

In a related example a mechanism for introducing application context can include implementing predetermined abstract methods with the application logic. For instance an open executor or station execution engine can be specified with the inflow grouping attributes and the granule criterion for group wise and chunk wise computation.

Accordingly a GKV can be automatically generated wherein GKV operations such as gkv.dump key to list gkv.clear gkv.add and the like are system provided for example.

Such operation can be applied tuple by tuple iteratively as supported by the system function execute which can further invoke several abstract functions to be implemented by users based on the app logic. Examples can include 

In a related example the EpochStation function can extend the BasicBlockStation and be employed to support chunk wise stream processing wherein the framework provided functions that are hidden from user programs can include 

The three functions of getGroupKey updateState processChunkByGroup can further be implemented based on the application logic whereas the other functions are deemed system defined for encapsulating the chunk wise stream processing semantics.

The abstract functions can be inherited level by level as required by the level of implementation as illustrated in that depicts an example for such hierarchy.

Hence the open executor of the subject disclosure can further supply canonical mechanisms to parallelize stateful and granule dataflow process via processing data flow based on chunks and groups. In this regard for each vertex representing a logical operator in the dataflow graph the operation parallelization launching multiple instances occurs with input data partition grouping which can remain consistent with the data buffering at each operation instance.

In the above topology specification hints for parallelization can be supplied to the operators agg 6 instances my 5 instances toll 4 instances and hourly 2 instances so that the platform can perform adjustments accordingly and based on resource availability.

As illustrated in the operation agg aims to deliver the average speed in each express way s segment per minute. Subsequently an execution of this operation on an infinite stream can be performed in a sequence of epochs one on each stream chunks.

To enable applying such operation to the stream data one chunk at a time and to return a sequence of chunk wise aggregation results the input stream is divided into 1 minute 60 seconds based chunks S S . . . S where i is an integer such that execution semantics of agg is defined as a sequence of one time aggregate operation on the data stream input minute by minute.

For an operator O over an infinite stream of relation tuples S with a criterion for cutting S into an unbounded sequence of chunks e.g. by every 1 minute time window where Sdenotes the i th chunk of the stream according to the chunking criterion then semantics of applying 0 to the unbounded stream S is represented by which continuously generates an unbounded sequence of results one on each chunk of the stream data.

Punctuating input stream into chunks and applying operation epoch by epoch to process the stream data chunk by chunk can be considered as a type of meta property of a class of stream operations. Such operations can be supported automatically and systematically on the epoch station or the ones subclassing it and provide system support in the following aspects.

An epoch station can host a stateful operation that is data parallelizable and therefore the input stream is hash partitioned which remains consistent with the buffering of data chunks as described earlier. Moreover several types of stream punctuation criteria can be specified including punctuation by cardinality by time stamps and by system time period which are covered by the system function of 

If the current tuple belongs to the new chunk the present data chunk can be dumped from the chunk buffer for aggregation group by in terms of the user implemented abstract method processChunkByGroup . Every input tuple or derivation can be buffered either into the present or the new chunk. By specifying additional meta properties and by subclassing the epoch station other aspects of system support can further be introduced.

For example an aggregate of a chunk of stream data can be made once by end of the chunk or tuple wise incrementally. In the latter case an abstract method for per tuple updating the partial aggregate can be provided and implemented by the user.

It is noted that the paces of dataflow with regards to timestamps can be different at different operators. For instance the agg operator and its downstream operators can be applied to the input data minute by minute. Yet when the hourly analysis operator is applied to the input stream minute by minute it generates output stream elements hour by hour. As such combination of group wise and chunk wise stream analytics provide a generalized abstraction for parallelizing and granulizing the continuous and incremental dataflow analytics. An example for the physical instances of these operators for data parallel execution as described above is illustrated in .

As described in detail below many stream operations have common characteristics and fall in the same execution pattern wherein aspects of the subject disclosure can categorize and support them systematically as opposed to hand coding them one by one. Moreover the execution pattern of a class of stream operators can depend on their meta properties as well as the special execution support they may require. Such may be considered as the meta data or the design pattern of operators. Below is an example for a list of characteristics which can include 

I O Characteristics that specifies the number of input tuples and output tuples which the stream operator is designed to handle stream data chunk wise. Examples can include 1 1 one input one output 1 N one input multiple outputs M 1 multiple inputs one output and M N multiple inputs multiple outputs where M N are integers . Accordingly for each chunk of the input one can classify the operators into Scalar 1 1 Table Valued TV 1 N Aggregate N 1 . One can support the following chunking criteria for punctuating the input tuples namely a by cardinality e.g. number of tuples and b by granule as a function applied to an attribute value such as get minute timestamp in second .

Blocking Characteristics indicates that in the multiple input case the operator applies to the input tuple one by one incrementally e.g. per chunk aggregation or first pools the input tuples and then apply the function to all the pooled tuples. Accordingly the block mode can be per tuple or blocking. Specifying the blocking characteristics describes for the system to invoke the operator in the designated way and hence facilitate the user s effort to handle them in the application program.

Caching Characteristics pertains to the 4 levels potential cache states namely 1 per process state that covers the whole dataflow process with certain initial data objects 2 Per chunk state that covers the processing of a chunk of input tuples with certain initial data objects 3 Per input state that covers the processing of an input tuple possibly with certain initial data objects for multiple returns 4 Per return state that covers the processing of a returned tuple.

Grouping Characteristics indicates a topology of how to send tuples between two operators wherein different kinds of stream groupings exists. The simplest kind of grouping can be referred to as a random grouping which can send the tuple to a random task. Such can have the effect of evenly distributing the work of processing the tuples across all of the consecutive downstream tasks. The hash grouping can ensure the tuples with the same value of a given field be directed to the same task. As such hash groupings can be implemented by employing consistent hashing for example.

As explained earlier ensuring the characteristics of stream operators by user programs can often be tedious and not system guaranteed. Alternatively and as provided by the subject disclosure categorizing the common classes of operation characteristics and supporting them automatically and systematically can simplify users efforts and enhance the quality of streaming application development. In this regard the open stations of the subject disclosure can contain stream operators and encapsulate their characteristic and towards the open executor class hierarchy of .

At an open executor execution engine can be supplied that enables users to invoke abstract processes according to application logic. The open executor remains open to plugging in application logic wherein abstract methods can be invoked or implemented by users according to the application logic. For example such can include specifying a station with in flow grouping attributes and the granule criterion for group wise and chunk wise computation. As explained earlier specified in a station class there can exist both the system defined methods and the user defined methods. The system defined methods can include the open executor that is open to plugging in application logic wherein abstract methods can be invoked or implemented by users according to the application logic. Similarly the open executor class or the station class can include abstract methods that can be implemented by the user based on the application logic. A station class hierarchy can be created at with each class facilitated with a respective open executor as well as related system utilities. Next and at data flow can be granulized via chunk wise processing. As such performing granule semantics and managing dataflow can occur in a chunk wise manner by punctuating and buffering data consistently. Subsequently and at tuples associated with a chunk can be processed wherein predetermined operations e.g. aggregations can be applied to the data chunk wise by the end of each epoch and other operations may be deemed tuple wise.

As used herein the term inference refers generally to the process of reasoning about or inferring states of the system environment and or user from a set of observations as captured via events and or data. Inference can identify a specific context or action or can generate a probability distribution over states for example. The inference can be probabilistic that is the computation of a probability distribution over states of interest based on a consideration of data and events. Inference can also refer to techniques employed for composing higher level events from a set of events and or data. Such inference results in the construction of new events or actions from a set of observed events and or stored event data whether or not the events are correlated in close temporal proximity and whether the events and data come from one or several event and data sources.

The inference component can employ any of a variety of suitable AI based schemes as described supra in connection with facilitating various aspects of the herein described subject matter. For example a process for learning explicitly or implicitly how parameters are to be created for training models based on similarity evaluations can be facilitated via an automatic classification system and process. Classification can employ a probabilistic and or statistical based analysis e.g. factoring into the analysis utilities and costs to prognose or infer an action that a user desires to be automatically performed. For example a support vector machine SVM classifier can be employed. Other classification approaches include Bayesian networks decision trees and probabilistic classification models providing different patterns of independence can be employed. Classification as used herein also is inclusive of statistical regression that is utilized to develop models of priority.

The subject application can employ classifiers that are explicitly trained e.g. via a generic training data as well as implicitly trained e.g. via observing user behavior receiving extrinsic information so that the classifier is used to automatically determine according to a predetermined criteria which answer to return to a question. For example SVM s can be configured via a learning or training phase within a classifier constructor and feature selection module. A classifier is a function that maps an input attribute vector x x1 x2 x3 x4 xn to a confidence that the input belongs to a class that is f x confidence class .

Each computing object etc. and computing objects or devices etc. can communicate with one or more other computing objects etc. and computing objects or devices etc. by way of the communications network either directly or indirectly. Even though illustrated as a single element in communications network can include other computing objects and computing devices that provide services to the system of and or can represent multiple interconnected networks which are not shown. Each computing object etc. or computing objects or devices etc. can also contain an application such as applications that might make use of an application programming interface API or other object software firmware and or hardware suitable for communication with or implementation of the various examples of the subject disclosure.

There are a variety of systems components and network configurations that support distributed computing environments. For example computing systems can be connected together by wired or wireless systems by local networks or widely distributed networks. Currently many networks are coupled to the Internet which provides an infrastructure for widely distributed computing and encompasses many different networks though any network infrastructure can be used for exemplary communications made incident to the systems as described in various examples.

Thus a host of network topologies and network infrastructures such as client server peer to peer or hybrid architectures can be utilized. The client can be a member of a class or group that uses the services of another class or group. A client can be a computer process e.g. roughly a set of instructions or tasks that requests a service provided by another program or process. A client can utilize the requested service without having to know all working details about the other program or the service itself.

As used in this application the terms component module engine system executor and the like are intended to refer to a computer related entity either hardware software firmware a combination of hardware and software software and or software in execution. For example a component can be but is not limited to being a process running on a processor a processor an object an executable a thread of execution a program and or a computer. By way of illustration both an application running on a computing device and or the computing device can be a component. One or more components can reside within a process and or thread of execution and a component can be localized on one computer and or distributed between two or more computers. In addition these components can execute from various computer readable storage media having various data structures stored thereon. The components can communicate by way of local and or remote processes such as in accordance with a signal having one or more data packets e.g. data from one component interacting with another component in a local system distributed system and or across a network such as the Internet with other systems by way of the signal .

In a client server architecture particularly a networked system a client can be a computer that accesses shared network resources provided by another computer e.g. a server. In the illustration of as a non limiting example computing objects or devices etc. can be thought of as clients and computing objects etc. can be thought of as servers where computing objects etc. provide data services such as receiving data from client computing objects or devices etc. storing of data processing of data transmitting data to client computing objects or devices etc. although any computer can be considered a client a server or both depending on the circumstances. Any of these computing devices can process data or request transaction services or tasks that can implicate the techniques for systems as described herein for one or more examples.

A server can be typically a remote computer system accessible over a remote or local network such as the Internet or wireless network infrastructures. The client process can be active in a first computer system and the server process can be active in a second computer system communicating with one another over a communications medium thus providing distributed functionality and allowing multiple clients to take advantage of the information gathering capabilities of the server. Any software objects utilized pursuant to the techniques described herein can be provided standalone or distributed across multiple computing devices or objects.

In a network environment in which the communications network bus can be the Internet for example the computing objects etc. can be Web servers file servers media servers etc. with which the client computing objects or devices etc. communicate via any of a number of known protocols such as the hypertext transfer protocol HTTP . Computing objects etc. can also serve as client computing objects or devices etc. as can be characteristic of a distributed computing environment.

As mentioned the techniques described herein can be applied to any suitable device. It is to be understood therefore that handheld portable and other computing devices and computing objects of all kinds are contemplated for use in connection with the various examples. In addition to the various examples described herein it is to be understood that other similar examples can be used or modifications and additions can be made to the described example s for performing the same or equivalent function of the corresponding example s without deviating there from. Still further multiple processing chips or multiple devices can share the performance of one or more functions described herein and similarly storage can be affected across a plurality of devices. The subject disclosure is not to be limited to any single example but rather can be construed in breadth spirit and scope in accordance with the appended claims.

