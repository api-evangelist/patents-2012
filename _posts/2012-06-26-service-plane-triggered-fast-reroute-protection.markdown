---

title: Service plane triggered fast reroute protection
abstract: Techniques are described for detecting failure or degradation of a service enabling technology function independent from an operational state of a service node hosting the service enabling technology function. For example, a service node may provide one or more service enabling technology functions, and service engineered paths may be traffic-engineered through a network to service node network devices that host a service enabling technology function. A monitor component at the service layer of the service node can detect failure or degradation of one or more service enabling technology functions provided by the service node. The monitor component reports detection of failure or degradation to a fault detection network protocol in a forwarding plane of the service node. The fault detection network protocol communicates with an ingress router of a service engineered path to trigger fast reroute by the ingress of traffic flows to bypass the affected service enabling technology function.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08948001&OS=08948001&RS=08948001
owner: Juniper Networks, Inc.
number: 08948001
owner_city: Sunnyvale
owner_country: US
publication_date: 20120626
---
The disclosure relates to packet based computer networks and more particularly to forwarding packets within computer networks.

A computer network is a collection of interconnected computing devices that exchange data and share resources. In a packet based network such as the Internet the computing devices communicate data by dividing the data into small blocks called packets. The packets are individually routed across the network from a source device to a destination device. The destination device extracts the data from the packets and assembles the data into its original form. Dividing the data into packets enables the source device to resend only those individual packets that may be lost during transmission.

Multi protocol Label Switching MPLS is a mechanism used to engineer traffic patterns within Internet Protocol IP networks. By using MPLS a source device can request a path through a network i.e. a Label Switched Path LSP . An LSP defines a distinct path through the network to carry MPLS packets from the source device to a destination device. A short label associated with a particular LSP is affixed to packets that travel through the network via the LSP. Routers along the path cooperatively perform MPLS operations to forward the MPLS packets along the established path. LSPs may be used for a variety of traffic engineering purposes including bandwidth management and quality of service QoS .

A variety of protocols exist for establishing LSPs. For example one such protocol is the label distribution protocol LDP . Another type of protocol is a resource reservation protocol such as the Resource Reservation Protocol with Traffic Engineering extensions RSVP TE . RSVP TE uses constraint information such as bandwidth availability to compute paths and establish LSPs along the paths within a network. RSVP TE may use bandwidth availability information accumulated by a link state interior routing protocol such as the Intermediate System Intermediate System ISIS protocol or the Open Shortest Path First OSPF protocol.

In some instances a node or link along an LSP may no longer be available. For example a link along the LSP or a node may experience a failure event such as when one or more components of a router fail or the router is brought down by a user such as a network operator. When a link or router in the network fails routers using traditional link state protocols such as OSPF and or IS IS may take a long time to adapt their forwarding tables in response to the topological change resulting from node and or link failures in the network. The process of adapting the forwarding tables is known as convergence. This time delay occurs because each node must update its representation of the network topology and execute the shortest path algorithm to calculate the next hop for each destination within the updated network topology. Until the next hops are re computed traffic being sent toward the failed links may be dropped. Some deployments take as much as 500 milliseconds to several seconds for detection and recovery from failures in the network. These large convergence times may adversely affect the performance of applications such as Voice over Internet Protocol VoIP and multimedia applications which often rely on RSVP TE and are extremely sensitive to traffic loss.

In general techniques are described detecting failure or degradation of a service enabling technology function independent from an operational state of a service node hosting the service enabling technology function. For example a service node may provide one or more service enabling technology functions such as applications virtual appliance virtual cache firewall security and others. Service engineered paths may be traffic engineered through a network to one or more service node network devices that host service enabling technology function.

A monitor component at the service layer of the service node can detect failure or degradation of one or more service enabling technology functions provided by the service node. The monitor component can report its detection of such a failure or degradation to a fault detection network protocol in a forwarding plane of the service node. The fault detection network protocol communicates with an router of a service engineered path to trigger Fast Reroute FRR of traffic flows to bypass the service node and thus the affected service enabling technology function.

A fault detection network protocol such as Bidirectional Forwarding Detection BFD can be extended as described herein to receive from the service layer monitor component a notification of a change in a performance status of a service enabling technology function. The fault detection network protocol can in turn output a notification over a BFD session to the ingress router to inform the ingress router of the failure or degradation of the service function. For example the fault detection network protocol can send a control message to the ingress router to notify the ingress router to reroute network traffic requiring the service enabling technology function to a second instance of the service enabling technology function provided at a second service node reachable by a backup service engineered path.

In response to the edge router learning of the change in status of the service enabling technology function the edge router can trigger fast reroute of network traffic onto a backup service engineered path that bypasses the first service node and directs traffic to a different instance of the service enabling technology function that is hosted at a second service node. In this manner when an instance of a service enabling technology function fails even though the first service node may remain operable fast reroute may be triggered to reroute traffic to a different instance of the service enabling technology function.

The techniques of this disclosure may provide one or more advantages. For example the techniques may allow an operator to maintain service integrity and any associated service level agreement SLA requirements for the service in the event of failures not detectable using other link or node protection functions. The techniques provide automatic fast reroute of traffic when a performance degradation of a service is detected and beyond initial configuration of thresholds on the service node do not require any action from the operator to effect fast reroute of traffic by a router to a different service node when a failure or degradation of a service occurs on the service node. This service based fast reroute capability can be provided in addition to and may be used in conjunction with other link interface and node fast reroute protection capabilities of the service node and ingress router.

In one aspect a method includes by a service node applying a service to network traffic received at a service plane of the service node via a primary service engineered path wherein the service node operates as an egress device of the primary service engineered path and detecting at the service plane of the service node that a performance level of the service differs from an expected performance level of the service by at least a threshold amount wherein the performance level of the service is independent of an overall operational state of the service node. The method also includes outputting a notification to a fault detection network protocol in a forwarding plane of the service node when the detected difference in the performance level of the service differs from the expected performance level by at least the threshold amount and in response to receiving notification from the service layer outputting by the fault detection network protocol a control message to a router on the primary service engineered path wherein the control message notifies the router to reroute network traffic to a second instance of the service provided at a second service node reachable by a backup service engineered path.

In another aspect a network device includes a forwarding plane that includes a packet forwarding component a fault detection network protocol executing within the forwarding plane and a service plane comprising one or more service cards. The service plane includes a software process that applies a service to network traffic received at the network device via a primary service engineered path wherein the network device operates as an egress device of the primary service engineered path and a monitor component that communicates with the software process to assess a performance level of the software process in applying the service to the network traffic wherein the performance level of the service is independent of an overall operational state of the network device. The monitor component detects that the performance level of the service differs from an expected performance level of the service by at least a threshold amount wherein the monitor component outputs a notification to the fault detection network protocol when the detected difference in the performance level differs from the expected performance level by at least the threshold amount. In response to receiving the notification from the monitor component the fault detection network protocol outputs a control message to a router on the primary service engineered path wherein the control message notifies the router to reroute network traffic to a second instance of the service provided at a second network device reachable by a backup service engineered path.

In another aspect a system includes a service node and a router along a primary service engineered path between the service node and the router. The service node includes a forwarding plane that includes a packet forwarding component a fault detection network protocol executing within the forwarding plane and a service plane comprising one or more service cards. The service plane includes a software process that applies a service to network traffic received at the network device via the primary service engineered path wherein the network device operates as an egress device of the primary service engineered path and a monitor component that communicates with the software process to assess a performance level of the software process in applying the service to the network traffic wherein the performance level of the service is independent of an overall operational state of the network device. The monitor component detects that the performance level of the service differs from an expected performance level of the service by at least a threshold amount and the monitor component outputs a notification to the fault detection network protocol when the detected difference in the performance level differs from the expected performance level by at least the threshold amount. In response to receiving the notification from the monitor component the fault detection network protocol outputs a control message to the router on the primary service engineered path wherein the control message notifies the router to reroute network traffic to a second instance of the service provided at a second network device reachable by a backup service engineered path. The router receives the control message and in response to receiving the control message reroutes traffic requiring application of the service to the second instance of the service provided at the second network device reachable by the backup service engineered path.

In another aspect a computer readable storage medium includes instructions for causing a programmable processor to apply a service to network traffic received at a service plane of a service node via a primary service engineered path wherein the service node operates as an egress device of the primary service engineered path and detect at the service plane of the service node that a performance level of the service differs from an expected performance level of the service by at least a threshold amount wherein the performance level of the service is independent of an overall operational state of the service node. The computer readable storage medium further includes instructions to output a notification to a fault detection network protocol in a forwarding plane of the service node when the detected difference in the performance level of the service differs from the expected performance level by at least the threshold amount and further includes instructions to in response to receiving notification from the service layer output by the fault detection network protocol a control message to a router on the primary service engineered path wherein the control message notifies the router to reroute network traffic to a second instance of the service provided at a second service node reachable by a backup service engineered path.

The details of one or more examples are set forth in the accompanying drawings and the description below. Other features objects and advantages will be apparent from the description and drawings and from the claims.

Each of subscribers may include one or more computing devices not shown such as personal computers laptop computers handheld computers workstations servers switches printers or other devices. Network may be a service provider network coupled to one or more networks administered by other service providers and may thus form part of a large scale public network infrastructure e.g. the Internet. The service provider may provide subscribers with access to the Internet via network which allows computing devices represented by subscribers to communicate with computing devices within the Internet or with another one of subscribers . may be a simplified view of network . Network may include a variety of other network devices not shown such as routers switches servers and the like and may be connected to other networks.

In this example routers A G are connected to one another by physical links. The physical links may be a physical medium such as a copper wire a coaxial cable any of a host of different fiber optic lines or a wireless connection. In order to maintain an accurate representation of the network routers and service nodes exchange routing information using control plane signaling in accordance with one or more defined protocols such as the Border Gateway Protocol BGP . Another example protocol for exchanging routing information is the Intermediate System to Intermediate System protocol ISIS which is an interior gateway routing protocol for IP networks for communicating link state information within an autonomous system. Other examples of interior routing protocols include the Open Shortest Path First OSPF and the Routing Information Protocol RIP .

The example of illustrates a service enabling technology function SET A hosted by service node A that is accessed by subscribers A N across service engineered path A that extends between edge router A and service node A. As used herein a service enabling technology SET function is a service enabling technology that is physically deployed and hosted at a service node. Examples of service enabling technology functions may include applications session border controller SBC virtual appliance virtual cache network traffic acceleration network address translation NAT firewall deep packet inspection DPI Quality of Service QoS access control and other functions. In some examples services provided by SETs may be composite services composed of two or more services and may form a single externally visible service to subscribers . As one example a SET may be a composite SET consisting of NAT services and firewall services.

As one example edge router A receives a request for a particular service from a subscriber. For example edge router A may receive a request from subscriber A for a parental control application service enabling technology function. Edge router A selects service node A as being able to provide the requested parental control application service enabling technology function. Service node A hosts an instance A of the parental control application service enabling technology function. In response to the request edge router A establishes a service engineered path A to service node A if service engineered path A is not already in place. Service engineered path A may be a traffic engineered network tunnel such as a label switched path LSP established using a resource reservation protocol such as Resource Reservation Protocol with Traffic Engineering Extensions RSVP TE or other type of tunnel. In some examples multiple SETs may be linked together using service engineered paths . As one example router A may be establishing the service engineered paths to carry layer two L2 communications from subscribers to service nodes in the form of MPLS packets that enter the respective service engineered path at ingress router A and exit the service engineered path at service node A or B.

In addition edge router A may also establish backup service engineered path B to a second service node B which is a second service node that edge router A selects as also being able to provide the requested parental control application service enabling technology function. Service engineered path B is a second service engineered path i.e. another traffic engineered network tunnel. SET B may be a second instance of the same SET as SET A. Edge router A may use fast reroute extensions to RSVP TE such as described in P. Pan Fast Reroute Extensions to RSVP TE for LSP Tunnels RFC 4090 May 2005 the entire contents of which are incorporated by reference herein. For example edge router A may serve as a point of local repair for fast reroute. Edge router A may install forwarding state that indicates that service engineered path A is a primary service engineered path for traffic from subscriber A and service engineered path B is a backup service engineered path for traffic from subscriber A. Ingress router A associates the backup service engineered path B with the primary service engineered path A. Traffic normally forwarded through the primary service engineered path A is redirected onto service engineered path B should service engineered path A be marked as inoperable due to failure or degradation of SET A.

After router A finishes establishing backup service engineered path B router A maintains forwarding information in a data plane of router A that allows router A to send traffic through service engineered path B upon receipt of a BFD control packet from service node A indicating failure or degradation of SET A. See for example techniques described by U.S. Pat. No. 8 077 726 entitled Fast Reroute for Multiple Label Switched Paths Sharing a Single Interface issued Dec. 13 2011 the entire contents of which are incorporated by reference herein. In this manner edge router A can automatically trigger fast reroute to send traffic from subscriber A to service node B via service engineered path B in the event that edge router A is notified of a problem with SET A or service node A as described below.

Upon receiving network traffic from subscriber A edge router A determines that a profile of subscriber A requires the traffic to be sent on service engineered path A to service node A for application of the parental control application A. Edge router A then forwards the traffic from subscriber A on service engineered path A such as by appending an MPLS label to the traffic from subscriber A.

A monitor component in the service plane at service node A monitors performance levels of SET A. In the event that the monitor component detects a failure or degeneration of pre set service levels for SET A the monitor component triggers a failure event to BFD. The monitor component in the service plane may trigger an event to BFD in the forwarding plane when a degradation of performance of the service occurs such that the service enabling technology function can no longer provide the services paid for by a service level agreement SLA . In some aspects the monitor component may be a high availability HA monitor component that interacts with the service function. For example the HA monitor may interact with services to collect statistics perform handshaking or carry out other checks to the functionality of services.

A BFD session is enabled between router A and service node A such as described in R. Aggarwal Bidirectional Forwarding Detection BFD for MPLS Label Switched Paths LSPs RFC 5884 June 2010 the entire contents of which are incorporated by reference herein. To use BFD for fault detection on an MPLS LSP such as service engineered path A edge router A and service node A establish a BFD session for that particular MPLS LSP e.g. using LSP Ping messages. BFD control packets are sent along the same data path as the service engineered path A being verified. BFD for MPLS LSPs typically used to detect a data plane failure in the MPLS LSP path even when the control plane of devices associated with the MPLS LSP is functional. Here BFD is being used to communicate from a service node A to an ingress router A that a performance degradation or failure has occurred with a service provided in a service plane of the service node A.

Service node A uses the BFD session to report any change in status of SET A to edge router A. In some examples a BFD module on service node A may set a diagnostic code of a BFD control packet to indicate the performance degradation in the relevant service. Ingress router A builds the backup service engineered path B using a diverse path that bypasses service node A and directs traffic to a different instance of the SET that is hosted at another Service Node e.g. SET B at service node B.

The performance level of the service enabling technology function is independent of an overall operational state of the service node network device. In other words upon notification of this failure event through BFD session edge router A is able to trigger fast reroute and switch traffic from service engineered path A to service engineered path B even though the existing service node A used for SET A remains operable. The failover of network traffic requiring the service enabling technology function occurs without disruption to a subscriber receiving the services of the SET.

In some examples service engineered path A may not be marked as wholly inoperable as a result of receiving the BFD control packet on BFD session but may be marked as inoperable only with respect to the particular SET A that has failed. Service engineered path A may continue to be used by router A for forwarding traffic to service node A on service engineered path A for accessing other SETs hosted by service node A. In this manner upon receiving a notification from service node A of a performance issue with SET A edge router A can do a fast reroute operation to reroute only the traffic needing the failing service on to a backup service engineered path B using fast reroute mechanisms typically used only in case of a link or node failure notification.

If the service that had been having performance issues in this example SET A later resumes its expected performance levels the HA monitor in the service plane of service node A may detect this change and notify the BFD protocol running on the forwarding plane of service node A which in turn can inform the edge router A using BFD session . The edge router A can then determine whether to continue sending the relevant traffic on the service engineered path B or revert to sending the traffic on the service engineered path A.

Although is described for purposes for illustration in part with respect to an edge router of network the techniques may be also applied to a non edge router. Likewise the techniques of this disclosure may be applied by a router that is not the ingress of one or both of service engineered paths but instead may be applied by a non ingress router along the service engineered paths such as a point of local repair router.

The configuration of the network environment illustrated in is merely exemplary. For example network may include any number of edge routers coupled to one or more customer networks. Nonetheless for ease of description only edge router A and access node are illustrated in . As another example network may include a greater number of service nodes than shown and each of service nodes may provide a plurality of services and SETs.

Routing plane may include a routing engine that resolves routes through network in accordance with one or more of a plurality of routing protocols. Routing engine may include a device management interface DMI and a routing daemon . Routing engine is primarily responsible for maintaining routing information to reflect a current topology of network . Routing information describes the topology of a network and in particular routes through the network. Routing information may include for example route data that describes various routes within the network and corresponding next hop data indicating appropriate neighboring devices within the network for each of the routes.

In addition routing engine uses routes stored in routing information to compute best paths for particular network destinations and derive forwarding information forwarding info which includes entries that map destination addresses to outbound interfaces on service node A. Routing engine may push a subset of forwarding information to forwarding engine for storage as forwarding information . Forwarding information may therefore be thought of as a subset of the information contained within routing information . Forwarding information may also include labels for use in forwarding MPLS packets along LSPs. Routing daemon may represent a software module that updates routing information to reflect a current topology of network . While described as a daemon or software module executed by routing engine routing daemon may be implemented as a hardware module or a combination of both hardware and software. Routing engine also includes a service daemon that interfaces with service plane such as to permit configuration of service card by device management interface .

Forwarding plane may include a forwarding engine . Forwarding engine may represent a software and or hardware component such as one or more interface cards not shown that forwards network traffic . Forwarding engine may divert aspects of network traffic as service traffic to service engine . After application of one or more services service plane returns the service traffic to forwarding engine to be merged into outbound network traffic . Forwarding engine may forward some aspects of network traffic as outbound network traffic without diverting to the service plane when application of services is not needed for the traffic.

Forwarding engine may represent a central or distributed forwarding engine where a distributed forwarding engine is distributed across a plurality of interface cards and a central forwarding engine resides in the above described central location e.g. control unit of service node A. Forwarding engine may forward network traffic in accordance with forwarding information forwarding info . Forwarding information may comprise an association or table of mappings identifying an interface by which to forward a particular packet or data unit of network traffic .

Service node A may further provide service plane for applying one or more of a plurality of services to network traffic . Service plane may implement the functionality of a network security device and or other service oriented devices using at least one service card . As further examples service plane may implement service enabling technologies SETs including services and service oriented devices represented by services . Example SETs that may be provided by services include for example applications session border controller SBC virtual appliance virtual cache network traffic acceleration network address translation NAT firewall deep packet inspection DPI Quality of Service QoS access control or other service functions.

Service card may include a service engine . Service engine may represent a software and or hardware component that applies services in accordance with policy rules defined by policy configuration data stored by policies . Policies may be a policy module that stores and maintains policy configuration data for service engine and by extension service plane . In some examples policies may maintain policy configuration data in an XML based encoding. However in some embodiments policies may maintain policy configuration data in the form of one or more tables linked lists databases flat files or any other data structure.

Device management interface DMI may represent a software and or hardware module that presents an interface with which an administrator or an administrative device represented by ADMIN may interact to specify certain operational characteristics of service node A. In response to invocation by admin device management interface interacts with other components of service node A such as to retrieve configure copy and or delete policy configuration data stored in policies update service data via service daemon and to perform other management related functions.

Service engine applies one or more of services to received packets in accordance with policies . As one example for a service comprising a parental control application a packet may be received from a subscriber requesting access to web content associated with a given uniform resource locator URL . The service causes service engine to reference policies to determine whether a profile of a subscriber from which the packet is received allows the request from the subscriber to be allowed through to the specified URL. In another example service engine may apply a QoS policy that classifies packets meeting the policy condition to a particular forwarding equivalence class FEC . In these instances service engine may forward packets differently depending on their classified FEC to meet QoS requirements.

Policy configuration data stored by policies may include policies for any of services applied by service plane . Although described herein with respect to policies stored within service plane in some embodiments policies may be stored within or distributed to additional service cards routing plane or forwarding plane . For example in some embodiments forwarding plane may store one or more policy rules for prompt application by the forwarding elements to network traffic .

Service engine also includes a high availability monitor HA monitor . HA monitor may represent a software and or hardware component that monitors performance of one or more of services in service plane . Service plane may provide an operating environment for running one or more applications including one or more of services created by a third party software development kit SDK . In some examples HA monitor may also be a software application that makes use of the SDK. In some aspects services may each expose an application programming interface API by which HA monitor inspects performance data e.g. loading levels for the respective service. Alternatively HA monitor may expose a universal interface which each of services may invoke to communicate current performance data. As another example HA monitor may periodically ping each service or may monitor output communications from each of the services or operating system level resources consumed by each of the services. HA monitor can monitor any of a variety of parameters associated with services which may be defined via a management plane of service node A e.g. via device management interface . HA monitor can monitor parameters associated with services such as per process central processing unit CPU usage memory usage rate of output number of available connections or any other parameters for detecting whether a service is performing according to expected levels. For example if a service is expected to provide an output at a threshold rate HA monitor can detect when the actual rate of output falls below the threshold rate. An administrator can configure the performance level thresholds via device management interface such as when an application or other service is initially deployed on service node A. The performance level thresholds may be stored as service data . The performance level thresholds may be selected relative to SLA requirements to trigger action when performance levels fall below what required by subscribers .

As one example a network address translation NAT function provided by one of services may support a number of connections. Admin may configure a threshold number of connections below which the NAT service should not fall for an expected performance level. Device management interface configures service card via service daemon to store the configured threshold to service data . HA monitor may continuously or periodically monitor the number of connections being supported by the NAT service and if the number of connections available by the NAT service falls below the threshold number of connections the HA monitor can detect this event and send a notification to BFD module in the forwarding plane.

When the performance level of a given one of services is substantially different than expected e.g. outside of one or more preset threshold ranges HA monitor in service plane notifies a fault detection protocol of service node A such as BFD module of the forwarding plane of service node A. For example HA monitor may notify BFD module through an API or other internal software mechanism. HA monitor can detect when one or more of services is operating outside of expected performance level thresholds and notifies BFD module of the performance level issues of the service . In this manner HA monitor operates in the service plane to monitor services and provides a trigger to the fault detection protocol e.g. BFD in the forwarding plane sometimes referred to as the data plane which as described below causes BFD to send a control packet to ingress router A to trigger fast reroute by router A.

BFD module may be extended as described herein to support the notification of failures or degradation of services which may be service enabling technologies SETs . Consistent with the principles of the disclosure BFD module provides signaling mechanisms for notifying an ingress edge router of a service engineered path of a failure of a service such as an application or other service . In certain embodiments the operations may be carried out automatically i.e. without intervention by a system administrator or a software agent.

BFD module can be implemented in the forwarding plane and can continue to function through disruptions in the control plane of service node A. BFD module uses control packets to indicate information about service node A to other devices such as to edge router A over BFD session . For example BFD module can include diagnostic code in a control packet where the diagnostic code value may be defined to indicate the local system s reason for a change in session state. The Diagnostic field may be used by the remote system edge router A to determine the reason that a session failed for example. BFD module sends BFD control packets in an encapsulation appropriate to the environment. For example a BFD control packet may be encapsulated with an MPLS label and sent by source node A via an LSP of service engineered path A. Further details on BFD can be found in D. Katz Bidirectional Forwarding Detection BFD RFC 5880 June 2010 the entire contents of which are incorporated by reference herein.

The Diagnostic field of the BFD control packet allows remote systems to understand the nature of a change in status of a session on the sender s system. In accordance with one example of the techniques of this disclosure upon detecting a failure or degradation of one of services BFD module may send a BFD control packet that includes a newly defined diagnostic code in a Diagnostic DIAG field of the BFD control packet. As one example BFD module can be extended to include an additional new diagnostic code used to indicate a failure or degradation of an operational state of one or more of services despite that the data plane forwarding plane and control plane of service node A are still working normally. The new diagnostic code may be referred to as a Service Down code for example. Assuming edge router A executes a BFD module that can process the new diagnostic code when the edge router A receives the BFD control message having the new diagnostic code set its BFD module will be informed by the diagnostic code of the BFD control message that a failure or degradation of some service has occurred and a forwarding plane of edge router A can take appropriate action to automatically trigger fast reroute of packets requiring services to a different service node.

As a further example BFD module of service node A may be extended to identify the particular service for which the performance degradation has been detected. For example BFD module can be extended to include both a service down diagnostic code as well as other sub codes that can be used to specify which particular services are experiencing performance problems. In this example BFD may need further modifications so as to allocate enough bits in the BFD control packet to the sub codes. Edge router A receiving the BFD control message having the new diagnostic code and sub code set will be informed by the diagnostic code of the BFD control message that a failure or degradation of a particular identified service has occurred and in response edge router A can take appropriate action to trigger fast reroute of affected packets to a different instance of the service on a different service node. The BFD module on edge router A would likewise be modified in order to process the new diagnostic code and sub codes to carry out the required actions and decide which traffic to reroute. In some aspects edge router A can perform fast reroute for traffic needing the identified service but can leave other traffic to service node A intact over service engineered path A. That is edge router A may not reroute all traffic from service engineered path A but may just update its forwarding state to forward all traffic requiring the identified service onto service engineered path B. This may provide a selective fast reroute function allowing router A to reroute traffic for some services and not others.

In another alternative example in response to notification by HA monitor of a performance issue in a service instead of using a new Service Down diagnostic code BFD module of service node A may use an existing code of BFD to indicate that service node A is down to trigger the fast reroute e.g. a node failure event even though the service node A is not actually down. In response to receiving the BFD control packet indicating a node failure edge router A may reroute all traffic from service node A to one or more alternative service nodes e.g. service node B .

The architecture of service node A illustrated in is shown for exemplary purposes only. The disclosure is not limited to this architecture. In other examples service node A may be configured in a variety of ways. Although described for exemplary purposes in reference to BFD the principles described herein may be applied to extend other protocols such as other protocols for Operations Administration and Maintenance OAM and or fault detection. In some examples BFD module may execute on one or more interface cards of forwarding plane not shown .

Aspects of service plane routing plane or forwarding plane may be implemented solely in software or hardware or may be implemented as a combination of software hardware or firmware. For example service plane routing plane or forwarding plane may include one or more hardware based processors which execute software instructions. In that case the various software modules of service plane routing plane or forwarding plane such as BFD module may comprise executable instructions stored on a computer readable storage medium such as computer memory or hard disk.

The service plane may receive one or more performance level thresholds such as by administrator configuration and may store the thresholds as service data . HA monitor in the service plane also referred to as the service layer of service node A monitors aspects one or more of services to ascertain whether a performance level of the service is within an expected range of performance level based on the performance level thresholds of service data . When the performance level is not substantially different than expected e.g. not outside of preset threshold range s YES of block HA monitor continues to monitor the services . For example HA monitor may interact with services to collect statistics perform handshaking or carry out other checks to the functionality of services .

When the performance level of a given one of services is substantially different than expected e.g. outside of one or more preset threshold ranges NO of block HA monitor notifies a fault detection protocol of service node A such as BFD module of the forwarding plane of service node A . The change in performance level may indicate that the service has failed or degraded. The notification from HA monitor to BFD module may identify the service for which the performance degradation or failure has been detected. In response to receiving the notification from HA monitor of the service plane BFD module outputs a control packet on an existing BFD session to ingress router A to notify router A of the degradation of performance of the service . As described above the control packet can include various forms of information to trigger router A to reroute of some or all traffic from service node A. For example the BFD control packet may include a diagnostic code that specifies that network traffic being sent to service node A for service should be sent elsewhere.

Router A executes a corresponding BFD module that receives the BFD control packet . In response to receiving the BFD control packet edge router A determines which traffic should be rerouted selects a new service node such as service node B having another instance of the required service and triggers fast reroute of the traffic requiring services to the service node B . In some examples edge router A may automatically update stored forwarding information to modify next hops for routes formerly destined for service node A via service engineered path A to now reflect a next hop to reach service node B via service engineered path B. If service engineered path B is not already in existence edge router A may establish service engineered path B at this time e.g. using RSVP TE.

The techniques described in this disclosure may be implemented at least in part in hardware software firmware or any combination thereof. For example various aspects of the described techniques may be implemented within one or more processors including one or more microprocessors digital signal processors DSPs application specific integrated circuits ASICs field programmable gate arrays FPGAs or any other equivalent integrated or discrete logic circuitry as well as any combinations of such components. The term processor or processing circuitry may generally refer to any of the foregoing logic circuitry alone or in combination with other logic circuitry or any other equivalent circuitry. A control unit comprising hardware may also perform one or more of the techniques of this disclosure.

Such hardware software and firmware may be implemented within the same device or within separate devices to support the various operations and functions described in this disclosure. In addition any of the described units modules or components may be implemented together or separately as discrete but interoperable logic devices. Depiction of different features as modules or units is intended to highlight different functional aspects and does not necessarily imply that such modules or units must be realized by separate hardware or software components. Rather functionality associated with one or more modules or units may be performed by separate hardware or software components or integrated within common or separate hardware or software components.

The techniques described in this disclosure may also be embodied or encoded in a computer readable medium such as a computer readable storage medium containing instructions. Instructions embedded or encoded in a computer readable medium may cause a programmable processor or other processor to perform the method e.g. when the instructions are executed. Computer readable media may include non transitory computer readable storage media and transient communication media. Computer readable storage media which is tangible and non transitory may include random access memory RAM read only memory ROM programmable read only memory PROM erasable programmable read only memory EPROM electronically erasable programmable read only memory EEPROM flash memory a hard disk a CD ROM a floppy disk a cassette magnetic media optical media or other computer readable storage media. It should be understood that the term computer readable storage media refers to physical storage media and not signals carrier waves or other transient media.

Various aspects of this disclosure have been described. These and other aspects are within the scope of the following claims.

