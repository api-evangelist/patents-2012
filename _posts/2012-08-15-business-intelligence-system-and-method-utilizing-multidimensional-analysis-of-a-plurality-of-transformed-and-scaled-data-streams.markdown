---

title: Business intelligence system and method utilizing multidimensional analysis of a plurality of transformed and scaled data streams
abstract: A method is provided for qualifying and analyzing business intelligence. At a first part of a data management system receives first, second and third streams of data. The first stream is client provided source data, the second is public source data and the third is data management system internal data previously collected and managed source data in the data management system. The three streams of data are organized into items and their attributes at the data management system. The source data is transformed at a data warehouse where it becomes normalized. Logic is applied to provide multi-dimensional analysis of transformed source data relative to a scale for at least one business intelligence. The data warehouse includes updated data from the multi-dimensional analysis. A user interface communicates with the data management system to create statistical information that illustrates impact over time and value.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08571909&OS=08571909&RS=08571909
owner: Roundhouse One LLC
number: 08571909
owner_city: San Francisco
owner_country: US
publication_date: 20120815
---
This application claims the benefit of U.S. 61 524 422 filed Aug. 17 2011 which application is fully incorporated herein by reference.

The invention relates generally to information storage management and analytic tools and more particularly to a data system for qualifying and analyzing data for at least one business intelligence.

Currently business leaders managing build asset facilities portfolios often make mission critical decisions using 1 no data 2 the wrong data or 3 inaccurate data. Vendors nominally in this space include business intelligence developers consultants integrated workplace management system vendors computer aided facilities management systems providers and others. Clients can use technology and services to optimize efficiency around a wide variety of facilities related business problems from project management to lease administration to space utilization and occupancy. These offerings however are standardized and afford the client only limited ability to customize them. Moreover they are designed for and constrained by the organizational silo in which they reside.

Existing systems specify the data requirements based on the data s expected relationship to a type of outcome or discrete task i.e. energy efficiency lease administration etc. . Where the outcomes are multi dimensional the data points within those dimensions are ill defined. The client value set is often predefined and solving for non standard or multi dimensional definitions of value is not supported. For example available tools fail to provide built asset portfolio planning tools that allow an education client to solve for their own definition of value i.e. maximum teacher retention against declining CapEx and contracting building inventory. In short no existing analytics engine correlates asset related A resources environment related E and culture related C data over time T to illustrate current performance optimum performance and or benchmark performance.

Accordingly there exists a need for modern on demand technology to extract classify validate qualify analyze store enhance and display data related to multi dimensional enterprise decision making with adjustable value definitions. There is a further need to provide systems and methods that take an actuarial approach to predictive modeling related to human performance resource utilization environmental factors and architectural data. Optimal performance is dependent on hundreds if not thousands of factors many of which are E C A T dependent. The complexity of these interactions and correlations calls for powerful methodologies and technology to provide insight and the basis for action.

Accordingly there is a need for improved data systems and their methods of use for qualifying and analyzing data for at least one business intelligence. There is a further need for data systems and their methods of use for qualifying and analyzing data for at least one business intelligence that uses multi dimensional analysis relative to a scale for at least one business intelligence.

An object of the present invention is to provide methods for qualifying and analyzing data for at least one business intelligence.

Another object of the present invention is to provide methods that receives a plurality of different source data that is used for qualifying and analyzing data for at least one business intelligence.

A further object of the present invention is to provide methods for qualifying and analyzing data for at least one business intelligence that uses client source data public source data and data acquired by a data management system.

Still another object of the present invention is to provide methods for qualifying and analyzing data for at least one business intelligence that organizes different streams of data into items and their attributes.

Yet another object of the present invention is to provide methods for qualifying and analyzing data for at least one business intelligence that uses multi dimensional analysis relative to a scale for at least one business intelligence.

These and other objects of the present invention are achieved in a method for qualifying and analyzing business intelligence. At a first part of a data management system receives first second and third streams of data. The first stream is client provided source data the second is public source data and the third is data management system internal data previously collected and managed source data in the data management system. The three streams of data are organized into items and their attributes at the data management system. The source data is transformed at a data warehouse where it becomes normalized. Logic is applied to provide multi dimensional analysis of transformed source data relative to a scale for at least one business intelligence. The data warehouse includes updated data from the multi dimensional analysis. A user interface communicates with the data management system to create statistical information that illustrates impact over time and value.

Referring to in one embodiment of the present invention a data management system and analysis and report preparation system are provided for qualifying and analyzing data for at least one business intelligence. As a non limiting example the business intelligence includes but is not limited to occupancy and utilization optimization at an academic facility looking to understand the financial impact of an expansion in its enrollment. A platform is provided that receives source data. In one embodiment the source data can be three streams of data. The first second and third streams of data can be client source data public source data and data acquired by the data management system . The data management system transforms raw data and stores it. The analysis and report preparation system includes an analytic engine. In operation the data management system receives first second and third streams of source data the first stream of data being client source data the second stream of data being public source data and the third stream being data acquired by the data management system. The data management system organizes the first second and third streams of data into items and their attributes. Examples of item and attribute types include but are not limited to items classroom building individual person department window temperature sensor site city and the like attributes size quantity orientation location identity characteristics material composition construction implementation process maintenance characteristics measurement values and the like. Furthermore the data management system classifies the items and attributes as belonging to at least one of the following dimensions asset culture environment time and value. The analytic engine receives the items with their attributes from the data management system and applies logic to provide single and multi dimensional analysis relative to a scale of study for at least one business intelligence. Multi dimensional analysis is defined as analysis that compares calculates correlates or otherwise operates on items and or attributes from at least two distinct dimensions while uni dimensional analysis refers to the same operations within a single dimension. Scale is defined as the level of detail of the particular analysis relative to the dimension under study. As a non limiting example of scale a high detail study for a school may be at the room level while a less detail study may be at the building level. Dimensions can have unlimited levels of scale.

The overall system also includes analysis and report preparation logic a user interface that a user uses to interact with the data management system a peer network and adaptive intelligence .

As illustrated in the client source data is data provided by a client or secured from the client by others but is owned by the client and is specific to that client. The client source data can be related to at least one facility assets human factors cultural factors user data environmental resource data scheduling data financial data custom data and the like.

The public source data is data accessed from public resources such as public federal government records local city planning databases public utility records census records university sponsored research publicly released business indices and the like. The public source data can be data related to at least one facility assets human factors cultural factors user data environmental resource data scheduling data financial data custom data and the like.

The data management system proprietary data is collected by the data management system based on the needs of the specific project. Data management proprietary data can be collected by a designated team or technology using sensors written notes data basing software photo video capture surveys and the like.

Referring to data management system transforms the raw source data into a format and logic usable by the platform. The data management system includes an extract transform loader ETL and the data warehouse .

The ETL includes one or more of a data transformer and a data validator . The data transformer includes at least one of logic for assigning raw source data to predefined data management system database fields association logic for associating raw data through item attribute relationships classification logic for classifying raw data as asset related environment related culture related dimensions time related or value related logic for relating items to each other based on scale and qualifying logic for qualifying raw data based on data related collection methods. The data validator includes validation logic for determining if the raw data is valid for input into the data management system database. This logic includes a set of rules governing valid data format and valid data value for the item or attribute type.

The Data Classifier and Data Qualifier can further include logic for assigning metadata to the raw data during classification and qualification. The metadata is used for analysis evaluation and reporting. The data warehouse includes a plurality of databases selected from at least one of data management system database distinct client databases qualifier metadata peer network metadata and data management system historical data.

Source data enters the data management system and is first processed by the ETL . The ETL transforms and validates the data before storage. After the data is processed by the ETL it can be sent to the data warehouse s client database and archived in a unique database prior to analysis. The data remains archived until a user creates an executable demand to analyze the data. The data is then passed to the analysis and report preparation where it is analyzed by the analytic engine . The analytic engine outputs tables of processed and correlated data that is sent to an analysis qualifier to determine an over all quality of analysis based on the accuracy of the data collection methodology calibration of collection equipment quantity of data and so forth. From there the data is sent to an report builder for modeling and scenario building or sent directly to the user interface for reporting.

The data that is sent to a report builder and can be correlated with user inputted priorities and divided into short term recurring and long term future and singular impact reports and recommendations. These reports are then sent to the user interface .

The user interface outputs the reports sent to it by the analytic engine and the report builder . Additionally data and information is sent back into the data management system as it is uploaded and configured by client and administration users.

Peer and public network creates report context by showing client data and information in relation to at least one of large scale industry trends peer trends local benchmarks and so forth. This data can be collected manually and uploaded through the user interface it may be collected automatically by the ETL or it can be calculated internally from the historical data database .

Adaptive intelligence makes the data management system and analysis and report preparation system smarter by both increasing information context and calibrating their rules and engines. The single component of this function the feedback Interpreter collects data and information from within the platform from user interaction patterns and from uploaded data without limitation to a particular data stream. This information is processed within the feedback interpreter and updates and refinements are made to at least one of data validation logic association logic the analytic engine logics the report builder logic and the query building and interpretation logic.

Referring now to all of the source data relates to six types of data facility assets data human resources HR user Characteristic data Environmental data schedule and other User Operational data financial and data and other data types as determined by the specific project as well as potential additional data determined through proprietary methods. The client source data comes from a facility manager COO or whoever represents the interests of a client company and has the data available in some format. The public source data comes from public archives such as federal government records local city planning databases publicly available indices public utility records census records and so forth. Types of data collected include historic retrofit costs real estate valuation salary costs absentee rates education level historic energy use rates types and the like. The data management system proprietary data comes from data collected by the data management system using custom installed sensors researcher perceived observation privately secured and maintained rates and indices privately secured client information and other data management system data collection methods and the like.

The associated data is then sent to a data dimension classifier for classification into one of the five dimensions asset culture environment value and or time. Asset related data may refer to items such as rooms mechanical and lighting systems windows buildings and the like. Environment related data may refer to units of energy amount of solar energy collected available water resources weight of a client s waste consumable organizational resources statistics associated with contextual environmental variables interior surface temperature exterior air temperature humidity rainfall acoustics CO2 Levels etc. and the like. Culture related data may refer to characteristics associated with individual people departments and organizations such as health ethnicity education level morale technology adoption rates productivity and the like. Value data may refer to balance sheet information income statement information unit costs financial indices as well as non financial value data such as sustainability renewability brand awareness happiness and crime rate. Time related data may refer to durations of study frequency of data collection milestone for projections impact on schedule historic and projected trends patterns and the like. The data dimension classifier tags all pass through data with metadata classifying it as one of the above dimensions.

The classified data is then sent to the data scale relater to be tagged with metadata relating all of the dimensional items to each other by scale. Scale may be defined as a level of study ranging from a close up more detailed view to a less detailed high level view. Scale may relate across dimensions using non dimensional metadata similar to scale using an architectural ruler or it may be unrelated across dimensions depending on the specifics of the analysis.

The scaled data is then sent to a data qualifier where it is again tagged with metadata related to data collection methods and accuracy. These metadata may include but not be limited to whether or not the data was collected by a client or specialized team whether or not the data was double checked how recent the data was collected the accuracy of the sensors if applicable and so forth. Once the data is qualified it sent to the data validator .

The data validator runs an algorithm matching at least one of the actual data formats types and values to the expected value types value thresholds and so forth for the mapped categories. In other words if a data table is mapped to the data management system data protocol then the data validator expects all of the data values to be formatted as dates. If a batch does not conform to the expected data types then the batch is flagged for review and sent to a data resolver . Additionally if a data value is outside of the expected range for a particular data type then it will be flagged for review as well.

Data sent to the data resolver may be accepted or rejected. Data that is transformed and validated successfully is then sent to the data warehouse for storage in one of several possible databases.

Referring to operation of the ETL is illustrated. Source data is acquired and uploaded and the data transforming process begins. The data transformer assigns data columns to data management system database fields. Data columns are associated using item attribute characterization. The data associator model is coupled to the data transformer and receives the associated data columns. The data associator model takes object A and produces item A with associated attribute C attribute B and attribute A related to the item by time. Items are classified as asset A culture C or environment E using metadata tags. Classified items are transformed into scaled relationships with an A C E dimension. Data is qualified based on source collection methodologies and the like. The data validator determines if the data meets expected data format and value thresholds. If yes the transformed and validated data is sent to the data warehouse. If not the data is flagged for resolution by an admin user. If the data is valid or made valid it is sent to the data warehouse . If not it goes to trash.

Referring to analytic engine runs correlations calculations comparisons and other data management system analysis per user query settings. As a non limiting example of a building the calculations can be occupancy calculation utilization calculation monthly energy use per square foot calculation and the like. As a non limiting example comparisons can include energy use compared to occupancy over one month building maintenance frequency versus monthly sky conditions and the like. As a non limiting example the correlations can be high occupancy correlated with high energy use high student performance correlated with the quantity of natural daylight in a classroom employee productivity per building between specified dates absenteeism rate of a specific function type per floor during a specified weather condition and the like.

The analysis and report preparation system runs multi dimensional analysis on data in the data warehouse and prepares reports relating the analysis to short term recurring and long term future and singular value information related to client priorities. In one embodiment the analysis and report preparation system includes the analytic engine data analysis evaluator and report builder . In one embodiment the analytic engine includes the analytic operator that interprets user queries the data aggregator that aggregates items and attributes at different scales the data analyzer that executes multi dimensional calculations comparisons correlations and other operations on selected data sets.

In one embodiment the analytic operator includes query logic to interpret user queries related to a specific client assignment and retrieval logic to determine which data sets are required from the data warehouse to execute the requested analysis.

The data aggregator has item attribute aggregation logic to aggregate attributes from a more detailed scale into an attribute s for an item in a less detailed scale. For example a room may have multiple temperature sensors in multiple zone locations within the room. A user may want to see the room as an item with a single temperature attribute and see the building with a single temperature attribute. To do so all or a selection of the temperature measurements may be aggregated into a single item and attribute association at the room level. In this case the temperatures are collected by unique sensors each classified as an item in the data management system platform. To attribute a single temperature value to a single item an average of the selected temperatures can be taken and associated with an existing room item or a new item representing the room may be created. Then the attributes may be combined again for all of the rooms in a building to form a single temperature attribute for the entire building. This example shows at least two scale jumps one from the zone level to the room level and another from the room level to the building level.

The data analyzer runs logic selected from at least one of logic to compare data sets within a single dimension or across dimensions over time and related to value comparison logic logic to calculate averages minimums maximums ranges and other mathematical functions on a data set within a single dimension or across dimensions math logic and logic to correlate attributes to outcomes expressed as value data set calculations or other within a single dimension or across dimensions correlation logic . Operation of one of these logics may involve an input that is the result of another logic. For example comparing occupancy to energy use using comparison logic may first require calculating occupancy math logic and using it as an input in the comparison.

In one embodiment the previous logics comparison math and correlation logic may be applied in conjunction with dimensional analysis logic which includes a single dimension or multiple dimensions of data leading to different orders of data management system analysis. These orders consisting of first order data management system analysis with data from a single dimension of asset environment or culture second order data management system analysis with data from at least two dimensions of asset environment and or culture and third order data management system analysis with data from all three dimensions of asset environment and or culture.

The data analysis evaluator has evaluator logic for calculating a quality score for the information generated by the analytic engine with the score being based on qualifier metadata referenced in the ETL s data qualifier . The evaluation logic uses the qualifier metadata possibly with a weighting system to determine an analysis ranking that may be indexed across all reports against other indices or against an ideal standard. The ranking allows users of the data to quickly understand the quality and relative value of the data analysis.

The report builder communicates with the analytic engine and the data analysis evaluator to generate reports related to specific client assignments.

Reports generated by the report builder are packaged for presentation to the user in terms of capital Long term and singular and operational Short term and recurring allocations determined by client priorities.

A diagram of the data analysis evaluator is illustrated in which assigns a relative rating or handicap based on the data s qualifier data to the information created by the analytic engine.

As illustrated in the report builder receives formalized data from the analytic engine and user prioritized lenses obtained from a master set list of lenses from the user interface . A strategic operator receives the data from both components and feeds the data into a scenario builder . The scenario builder uses defined relationships between the user prioritized lenses and the incoming data to determine possible short term and long term responses scenarios to client facilities and operations. The short term responses are developed by the recurring response developer and are passed to the operation decisions report generator for packaging in that the data and models are ordered contextualized and made ready for visualization by the user interface . These responses affect the everyday operations of the facility and therefore relate to non fixed assets. The long range response developer produces models that predict outcomes based on capital responses including but not limited to changes such as sale or renovation of fixed assets relocation of personnel and infusion of new funds. Operation decisions report generator and long range response developer send their reports to the user interface s sub component graphic display for presentation to the user.

The user input portal can include up of three subcomponents that the user interacts with a data uploader priorities builder queries builder . Additionally the user input portal provides a range of services that support the use of these components not limited to default options i.e. pre set selections for both queries and priorities query recommendations and other user centric features.

The data uploader provides data uploading services to platform users. These services provide both automatic upload options such as for a non limiting example downloadable API s that communicate with user computer systems and manual upload options e.g. templates for data input . The user enters the user Input portal selects the data uploader and chooses how to upload the data. The data uploader can receive a pre set list of file types including but not limited to .xls .csv .xlm .kml .rvt .jpg .doc and the like determined by the capabilities of a translator and the application program interfaces API s available for communication through the data uploader .

Data from the data uploader is sent to the translator if the data is from a project that is new to the platform. If the data is coming from a project that has already been analyzed by the platform then the data is sent to the platform refiner for model calibration.

The priorities builder allows the platform user to choose the type and relative importance of the values to optimize their institution related to a variety of objectives including but not limited to sustainability financial annual bottom line employee health and wellness long term productivity improvements brand awareness and the like. These values can be thought of as lenses that an advisor wears when making short term and long term reports. The platform user prioritizes these lenses relative to each other using a numeric scoring reference with variable scores which can be adjusted by the user but will sum to preset total. As a non limiting example if there are ten lenses with a preset total of 100 points then each will have a numeric score attached to it ranging from 1 91 with the other nine lenses having an allocation set by the user for the remainder of the points for a total of 100 points.

Each lens may be made up of sub lenses that deconstruct the primary lenses into simpler value judgments for the user. As a non limiting example a lens of sustainability may be constructed of the sub lenses water use energy use building materials site location and the like where the user rates the importance of these sub lenses as a sub total of the value of the master lens in this case sustainability.

The scenario builder uses the prioritizations from the priorities builder to determine which primary lenses will most influence the model development. Once a primary lens is selected the sub lenses which relate to measurable building operations or capital decisions are used to evaluate long term and short term responses. For instance if sustainability is selected as a top priority and water usage is selected as a top priority sub lens then the scenario builder can solve for optimized water usage above other primary and sub lenses in both its short term and long term responses.

Weightings and correlations of the lenses are numerous and can be reoriented in several different ways. The design allows for flexibility in how these lenses are prioritized how they relate to the data being fed into the Analysis and Report preparation system of the data management system and how they are presented in the final reports.

The queries builder also illustrated in further develops the user input portal by adding a command search functionality to the interface. By default the queries builder may already be set up to request a particular type of output in a particular sequence related to a particular time horizon and so forth. This default request may be project specific that is based on the type of data entered into the platform tagged to a specific project related to global conditions including but not limited to types of output most educational organizations government agencies and healthcare institutions are interested in and the like related to a linked network that is based on the type of output that most of these peer institutions are interested in.

In the above case linked network refers to an opt in network of peer institutions determined by several criteria and available in a multitude of varieties. For instance top ten US Graduate Schools of Business may want to opt in to a linked network of these ten peer institutions that give a varying degree of context to the data output received from the platforms reporting function. For instance members of the linked network may use averages from the network as benchmarks for performance. Additionally the linked network may have query defaults based on user patterns or other smart platform features based on network trends. Thus a query default may be based on the user patterns of these institutions or related to some other platform determined criteria.

Users may decide to search for a specific type of dimensional information or construct a different order of reports or in some way alter the report outputs. To do so the user can change the settings in the queries builder . Changing the settings can be made by but not limited to altering the fields for search typing in a text string question or selecting from a pre set list of report outputs.

Queries generated by the queries builder are sent to the analytic engine or the advisor depending on the type of query. Queries that require sense making i.e. an explanation or user defined prioritization are sent to the advisor and eventually outputted in the form of short term impact and long term impact reports. Whereas queries that require only a noisy uncorrelated data output may be sent directly to the analytic engine and from the analytic engine s output to the graphics display bypassing the advisor .

As illustrated in the reports display is configured to visualize data tables sent to it by the analytic engine and the advisor . The visualization tool may be a proprietary in house tool or an off the self product including but not limited to Data Graph Wonder Graph and the like and customized to receive these data tables and graphically visualize their relationships as directed by the analytic engine and advisor which are preset to output relationships in default ways. These visualizations may include but are not limited to histograms bar charts heat maps bubble maps pie charts architecture drawing overlays time lapsed animation and the like. All charts can have default settings but may be customized by the platform user based on project specifications including but not limited to levels of access which limited functionality for some users and the like.

In one embodiment the graphics display includes three sub components the operations decisions display the capital decisions display and the snap shot display . The operations decisions display receives reports from the operations decisions report generator and visually presents those reports to the platform using a plurality of techniques static display interactive display downloadable documents and the like. The visuals may be singular or exist in relation to several visual and text that make up the report. The capital decisions display receives reports from the long range response developer and presents similar visuals as stated above except that they relate directly to long term fixed asset decisions. These reports both operations and capital can be designed for a range of general executive functions in an organization such as the CEO COO CFO and the like as well as for specialized job functions such as Facilities Director Operations Director Human Resources Director and the like for their interpretation and implementation.

The snap shot display visually displays output from the analytic engine . Data tables coming from the analytic engine may not be correlated with user defined priorities or be contextualized. In general these reports are a snap shot of current conditions as analyzed by the analytic engine . As a non limiting example they may display a specific sub set of room occupancy at a point in time for a specific building or over a set period of time and the like.

The feedback interpreter handles the internal data collection pulling data from the user interface . The noise canceling refiner analyzes all source data coming into the system categorizes it and creates like relationships between data sets i.e. two academic institutions with similar characteristics will be related to each other . These like relationships will be used to refine the validation logic used by the data validator . Using this methodology the data validator s logic will become more accurate in identifying erroneous and invalid data.

The model calibrator pulls data from the data uploader if the data is tagged as a previously uploaded data set including but not limited to the same data such as a non limiting example facilities asset data from the same project was uploaded to the data management system database at an earlier time. As a non limiting example this can be termed data set . This data is then compared to the previously stored data set data set . Patterns and anomalies are analyzed and differences in the two time stepped data sets are analyzed. If enough contextual data is available other data sets uploaded for the second time the model calibrator will automatically use multi variant statistics to determine causalities among data sets and use its findings to refine model prediction in the scenario builder . If there is not enough contextual data to automatically determine causalities then the differences in singular data sets will be compared and conclusions regarding causality will be determined by service technicians working on the projects. As a non limiting example conclusions regarding a data set about water usage differences from January compared to the same water usage data set from March without any other relevant data provided during analysis can be made manually by a technician who can investigate various causes.

A user Interaction trend analyzer pulls data from the user interface while the user is interacting with the interface. This data may come from user query patterns user priority building patterns and the like

A public contextualizer pulls data from public source data and analyzes it in relation to platform projects. This data then provides context to platform data helping platform users understand how their data measures up or relates to large scale public trends and other facilities with information in the public sphere. Within the public contextualizer the large scale trend analyzer pulls macro data that is trending including but not limited to financial market building indices building cost indices human resource trends occupancy trends labor trends and the like. As a non limiting example this component may grab US News and World Report 2012 s Top 10 Business school index and search it for characteristics relevant to the platform s analysis. The public comparator pulls relevant data that is publicly available and like a platform user s project like refers to a variety of similar characteristics between projects that make comparison relevant . This data is used to place the platform user s project performance in context with other project performances.

A peer contextualizer acts in a similar way to the public contextualizer by providing data that contextualizes performance by platform users Assets Environment and Culture. However the peer contextualizer collects data solely from linked network members. This data can contain more relevant and more specific information than the public contextualizer because it is shared within a closed network of peer institutions that have opted to share with each other. A peer benchmarker provides sample data from linked network members to the advisor to contextualize the reports delivered to other linked network members from the same network. A peer query analyzer provides query suggestions to the user interface to help guide the querying by other linked network members including but not limited to the queries builder that adjust query defaults to match querying patterns by linked network members.

The adaptive intelligence performs at least one of recognizing patterns in user interaction measuring predicted verses actual outcomes calibrating data management system proprietary data and adjusting the rules used by the data validator in the data management system . Calibration refers to refining the accuracy of the proprietary data.

The peer network advisor provides a comparison of a first client specific report to one or more different second client specific reports.

The foregoing description of various embodiments of the claimed subject matter has been provided for the purposes of illustration and description. It is not intended to be exhaustive or to limit the claimed subject matter to the precise forms disclosed. Many modifications and variations will be apparent to the practitioner skilled in the art. Particularly while the concept component is used in the embodiments of the systems and methods described above it will be evident that such concept can be interchangeably used with equivalent concepts such as class method type interface module object model and other suitable concepts. Embodiments were chosen and described in order to best describe the principles of the invention and its practical application thereby enabling others skilled in the relevant art to understand the claimed subject matter the various embodiments and with various modifications that are suited to the particular use contemplated.

