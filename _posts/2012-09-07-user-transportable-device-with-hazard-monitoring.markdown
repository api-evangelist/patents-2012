---

title: User transportable device with hazard monitoring
abstract: In some examples, a user transportable device may determine, based at least in part on sensor input, that the device is in motion. For example, the device may determine there is a likelihood that a user of the device is walking, running, traveling in a vehicle, or the like. In response, the device may present, on a display, an image obtained from a camera oriented, at least in part, toward a direction of travel. Further, in some examples, one or more images from the camera and/or sensor input from other sensors on the device may be analyzed to detect whether an object, obstruction or other hazard is in a direction of travel of the user of the device. If the device determines that a hazard may be imminently encountered by the user, the device may provide an alert to the user.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08953841&OS=08953841&RS=08953841
owner: Amazon Technologies, Inc.
number: 08953841
owner_city: Seattle
owner_country: US
publication_date: 20120907
---
People use handheld mobile or other user transportable electronic devices for a variety of purposes such as making telephone calls texting accessing the Internet sending and receiving email playing digital content executing applications playing games navigation and numerous other functions. Users of these devices sometimes use them while the users in motion such as while walking running biking skating skiing horseback riding driving a car or other vehicle and so forth which can distract the users from their surrounding environment. Unfortunately due at least in part to a lack of situational awareness injury or death may occur if a user encounters a hazard when using a user transportable device while the user is moving.

This disclosure includes techniques and arrangements for enabling a user transportable mobile electronic device to warn a user of a potential hazard. For instance the user transportable device may determine that the user transportable device and the user are moving. The user transportable device may further determine that the user is using the user transportable device in a fashion such that the user may be distracted from the user s surroundings. As one example the user may be using an application or function of the user transportable device that makes it likely that the user is looking at the display of the device such as for performing text entry looking at presented information or images applying touch inputs or the like. Thus upon detecting movement of the user transportable device in a manner that indicates that the user is walking running driving etc. the user transportable device may begin monitoring an area or region in front of the user e.g. an area or region in a direction of travel of the user that includes a probable path of the user. In some instances the user transportable device may attempt to identify any hazards in the direction of travel and or the probable path of the user and warn the user if it appears likely that the user will encounter the hazard. Examples of such hazards may include potential collision with a person or other object in the user s path detection of an upcoming curb a set of stairs an intersection or any of numerous other possible obstructions or obstacles.

Some implementations may employ the ability of a user transportable device to detect when the device is in motion and to make a determination that it is likely that the user is also in motion. For example if the user transportable device detects that it is moving between 2 9 mph this may be indicative that the user is walking jogging or running. Further a gait or a walking running cadence of the user may be detected by one or more accelerometers on the user transportable device to provide further indication of walking running as opposed to other types of motion such as riding in a vehicle. Additionally in some examples the user transportable device may detect a current attitude or orientation of the device such as based on inputs from one or more accelerometers or other suitable orientation sensors. For instance if the device is held with the display facing substantially up and between an angle of about 0 to 90 degrees from the horizontal then there is a strong probably that the device is being held in a position that enables the user to view the display.

In some examples one or more cameras may be used by the user transportable device to provide information to the user regarding an area or region toward which the user is detected to be moving. In some cases the device may determine a predicted path of travel of the user. Further images from the camera may be analyzed and processed to recognize objects obstructions and other hazards. Additionally in some cases an auto focusing ability of a camera and or changes between sequential images may be used for determining the distance to objects in a predicted path of the user the probability of a collision and the like.

In some cases some or all of the techniques herein may be invoked manually such as by being turned on by the user at the beginning of a use session. In other examples however the provision of information and or hazard monitoring may be invoked automatically. For instance a hazard monitoring module may run as a background process until movement of the device is detected. Further in some examples the user may not be able to disable the hazard monitoring features such as in the case of parental controls or extremely unsafe behavior e.g. texting while driving at a high rate of speed . In addition Global Positioning System GPS technology may be used to turn on some features automatically when the user enters certain areas and or turn off some features in certain areas. For example use of the camera may be prohibited in certain areas.

In some implementations the device presents an image captured by the camera on the display of the device. This may be an image in a small window on the screen or may be provided as an overlay or underlay to the whole screen e.g. a translucent image a partially transparent image or an alpha channel image a background image etc. As several examples the image may be at least one of a partially transparent image overlaid on a user interface an image displayed under a partially transparent user interface only a portion of an image received from the camera or an augmented image that visually distinguishes one or more objects in the image. For instance when an object in the image has been determined to be a potential hazard the image may be processed or augmented such as by outlining or coloring potential hazards or with other graphic effects. As one example when an object in an image is first identified as a potential hazard the object may be outlined in a first color such as yellow. If the user continues to advance toward the object such that an encounter with the object is imminent the object may be outlined in a different color such as red the outline object or image may flash rapidly and or one or more additional alerts may be provided to the user.

In some cases the image is not provided on the display until the device detects that the user is likely to be in motion such as walking running biking riding in a vehicle etc. For instance if the device detects that the user is walking and that the user is holding the device in a position to view the display and or is actually using the device such as by making touch inputs using a keypad etc. the device may initiate presentation of the image captured by the camera. Typically in the case that the camera is forward facing in the direction of movement of the user the image captured by the camera will include the probable path over which the user will travel. Accordingly the user can view on the display any obstacles objects or other hazards in the path of the user while the user is looking at the display. When the user transportable device detects that the user has stopped moving the image from the camera may no longer be presented on the display.

Additionally in some examples processing of the image may be performed to detect components or portions of the image that may indicate an obstacle or hazard in a direction of travel and or a predicted path of the user. For example computer vision techniques such as image segmentation pattern or object recognition and other image processing may be applied to detect objects in an image such as for distinguishing a trash receptacle from surrounding pavement or grass. Similarly image processing may be used to detect a yellow red white curb or pedestrian crossing marks which may indicate a transition from a safe sidewalk to a dangerous road. Stairs fountains bushes street signs fire hydrants vehicles walls and any of numerous other objects can be detected by the device. Changes in object size from one image to a subsequent image may indicate an approaching pedestrian or vehicle. Additionally sound received by the microphone can further indicate an approaching vehicle such as an automobile bus or train or other hazard. The user transportable device may further calculate a likelihood of collision or other encounter with an identified object or hazard based at least in part on a predicted path of the user. Additionally in some examples an auto focus capability of the camera may be used to help determine a distance to a particular object. Accordingly the probable distance to potential obstacles may be measured and compared with a predicted path of the user. For example an overall predicted path of the user may be determined such as based on changes in GPS position and other inputs to the user transportable device and despite some movement of the device that may occur relative to the user or the user s path. In the case that an encounter with a hazard appears likely the device may provide an alert to the user such as a visual alert an audible alert a tactile or vibratory alert or any combination thereof.

As another example image processing may be applied to recognize that the user is driving a car. For example the device may detect that the device is moving and the device may further recognize from a captured image a steering wheel or car dashboard. In such a case the device may provide a warning or may even disable itself at least in part such as by deactivating one or more hardware components of the device deactivating one or more executable modules software components etc. e.g. blacking out the display deactivating a transceiver displaying a warning screen deactivating a module for sending or receiving text messages or the like until the device detects that it is no longer moving. For example parental controls may include this feature to prevent teens from using mobile devices while driving. Alternatively suppose that the user is a passenger in the car then when the user transportable device detects that the device is moving but the image obtained from the camera shows that the device is in the passenger seat or in a rear seat of the car the device may take no additional action and may operate normally.

For discussion purposes some example implementations are described in the environment of presenting an interface for alerting a user about a hazard. However the implementations herein are not limited to the particular examples provided and may be extended to other types of interface configurations other types of alerts and other types of user interactions as will be apparent to those of skill in the art in light of the disclosure herein.

The user transportable device may include or may have associated therewith a display to present information to a user. In some types of user transportable devices the display may be a touch sensitive display configured with a sensor to sense a touch input received from an input effecter such as a finger of a user a stylus or the like. Thus the touch sensitive display may receive one or more touch inputs stylus inputs selections of text selections of interface components and so forth.

In other implementations the display may be non touch sensitive. Accordingly in addition to or as an alternative to a touch sensitive display the user transportable device may include various external controls and input devices. For example some implementations not shown in of the user transportable device may include a virtual or physical keyboard or keypad a mouse a pointing stick a touchpad a trackball a joystick a remote control buttons and or various other controls for performing various desired inputs and interactions with the user transportable device such as with an interface or other information presented on the display . Additionally in some implementations one or more voice commands may be used to control or interact with the interfaces and user transportable devices herein. Further in some examples a user s eye position or point of focus may be detected to serve as inputs or commands. Thus implementations herein are not limited to any type of input devices techniques or controls.

In addition in some examples the user transportable device may include one or more cameras which may be each be a mono or stereo camera a camera array or other suitable light detection device such as an infrared IR energy detection device. Each camera may include a lens and an image sensor to receive images taken from an angle of view or field of view of the camera . Typically the field of view is dependent upon several factors including lens size aperture zoom image sensor size and so forth. The image sensor may be any suitable type of image sensor such as a CCD charged coupled device or a CMOS complementary metal oxide semiconductor image sensor. Further the image sensor may be configured to receive and detect visible light IR light or any combination thereof.

In the illustrated example the display is located on a first side of the user transportable device and the camera is oriented toward a second side of the user transportable device opposite to the first side . Thus the camera may be a forward facing camera that is operable to obtain one or more images of a region within the camera s field of view on a side opposite to the display . Further in some examples a camera may additionally or alternatively be oriented toward the first side of the user transportable device i.e. a rear facing camera.

In the illustrated example suppose that the user and the user transportable device are moving in a direction of travel indicated by an arrow . The user transportable device may detect that the device is in motion using any of numerous techniques sensors and combinations thereof. For instance the user transportable device may include one or more accelerometers a compass a Global Positioning System GPS device one or more communication interfaces or various other sensors and components that may provide sensor information to the user transportable device to indicate that the device is in motion. As one example the user transportable device may detect that the device is currently being held with the display facing up at an angle that is between about 0 and 90 degrees from a horizontal position or horizontal axis which may correspond the ground in some cases. Such an angle indicating the device s attitude or orientation may imply that the user is looking at the display at least some of the time.

The user transportable device may further detect that the user is walking based on detecting a gait or cadence movement of the user that may be detected for example using one or more accelerometers i.e. movement of the device in an up and down back and forth manner indicative of walking or running. The user transportable device may further detect movement of the device based on one or more inputs from one or more other components. For example communication of the user transportable device with one or more cell towers and or wireless access points may change in nature such as by the signal strength increasing or decreasing as the user is moving from one location to another. Further GPS communications from one or more GPS satellites may indicate movement of the user transportable device . As another option the camera may be operated to collect sequential images which may be processed or analyzed to detect a change in a scene within the field of view that is indicative of movement of the user transportable device and the user.

The user transportable device may use any combination of the inputs from the multiple different sensors components and sources for detecting that the user transportable device and the user are in motion. Further the user transportable device may allow some sensors such as the accelerometers to operate in the background while other more power thirsty sensors such as the GPS device and the camera may remain off until some type of movement is detected by the sensors operating in the background. For instance the accelerometers may provide sensor information that is indicative of a pattern of movement corresponding to walking or running. The device may then wake up the GPS device and or the camera to begin monitoring movement of the device and begin monitoring a region in the direction of travel of the device if the device detects that the user is likely to be using the device while moving.

When the device is in motion and being held in a manner or orientation to indicate that the display is likely to be able to be viewed by the user or if the device is actually being used such as by receiving user inputs e.g. text entry touch inputs etc. the user transportable device may operate the camera to monitor the region in the camera s field of view . For example when the orientation of the display i.e. the presentation area or viewable portion of the display is determined to be between 0 and 90 degrees from horizontal and facing away from the direction of travel the display is likely to be viewable by the user and thus the user may be looking at the display rather than in the direction of travel. Additionally if the device is actually receiving user inputs while the device is determined to be in the motion this is also an indication that the device is being used and that the user may not be looking in the direction of travel. For instance if a user input is received within a threshold period of time before during or after the device is determined to be in motion the device may determine that the device is being used while in motion.

The device may begin operating the camera which is oriented at least in part toward a direction of travel of the user transportable device . The camera may receive an image of a scene including one or more objects in the region in a path of the user. In the illustrated example the objects include a fire hydrant a person a curb and a street sign although in other examples any other object that may be a hazard can be detected by the device . As the user transportable device moves in the direction of travel indicated by the arrow the scene imaged by the camera will change from one image to the next as the objects become closer to the user transportable device or otherwise change position in relation to the device . Additionally in some cases the user transportable device may predict a path of the user based on the information from the sensors and or the changes in multiple images obtained from the camera . In some examples the user transportable device may include an image processing module that employs computer vision and pattern recognition techniques to distinguish objects in a received image and to identify objects that may pose a hazard to the user of the user transportable device . For example as the scene imaged by the camera changes from one image to the next over a period of time the image processing module and or a hazard recognition module may determine based on the predicted path that the user transportable device is approaching a particular object and may be on a collision course or may otherwise encounter the particular object .

In some situations when the user transportable device is monitoring the region and determines that there may be an obstacle such as an object that poses a likelihood of collision or other hazard to the user of the user transportable device the device may provide a warning alert or other notification to the user regarding the presence of the hazard. For instance the user transportable device may provide any combination of a visual alert an audible alert a tactile or haptic alert and so forth. On the other hand when the user transportable device detects that the device is no longer in motion or is no longer being used in a manner that may be distracting to the user the user transportable device may cease monitoring of the region in order to conserve power of the device . For example if the user holds the user transportable device downward to one side or places the user transportable device into a pocket the user transportable device may determine this action and may turn off the camera and otherwise stop monitoring for hazards.

In the illustrated example the texting interface includes a virtual keyboard a message entry area an old messages area and a send button . Further in this example the user transportable device includes various other components such as one or more physical buttons or controls one or more microphones at least one speaker and a front facing camera which may be one of the one or more cameras described above.

The display may further present a monitoring interface that may include an image received by the front facing camera on the second side of the user transportable device . In some examples the monitoring interface may be automatically activated when certain conditions are detected by the user transportable device . For example when the user transportable device is not determined to be in motion then the monitoring interface is not presented. On the other hand when the device is detected to be in motion as discussed above and is being held at an orientation that indicates the user may be able to view the display while moving or the user is actually interacting with the device such as by making one or more touch inputs then the user transportable device may automatically start monitoring for hazards in a direction of travel such as a predicted path of the user and may further present the monitoring interface as a pop up overlay etc. on the display . Accordingly the monitoring interface may provide at least a portion of an image obtained by the camera as a displayed scene . Thus referring to the example of the scene may include the object which in this case is a person located in the predicted path of the user. Thus the user may be able to view directly on the display any hazards in the path of the user without having to look up from the display .

In some examples an operating system of the user transportable device may generate the monitoring interface and display the monitoring interface over top of another interface such as the interface produced by a separate application or module. As several examples the monitoring interface and any associated graphic effects may be generated using any of HTML hypertext markup language JavaScript CSS Cascading Style Sheets widgets C C Java or any combination thereof or any other suitable technology programming language or graphics rendering software. For instance the operating system of the device may include one or more modules that run as background processes to monitor the use of the device detect whether the device is in motion and so forth. When the conditions described above are detected the one or more modules may become more active such as by activating the camera to capture one or more images and processing the one or more images to detect whether any hazards are in a predicted path of the user. The GPS device may also be utilized for tracking movement of the device.

In other examples the monitoring interface may be generated by the application that is generating the current interface . For instance the texting interface may include one or more APIs application programming interfaces able to receive an image captured by the camera and present the image in a pop up or picture in picture window as the monitoring interface . Accordingly in this implementation certain interfaces applications and programs that a user tends to use in a dangerous manner such as while walking running etc. may include the capability to present the monitoring interface .

As still another example the monitoring interface may be generated by a separate application that executes on the user transportable device . For example the monitoring interface may be generated by a monitoring application that may start when the user turns on the user transportable device and that runs as a background operation until one or more conditions indicate that the device is being used while in motion in a manner that may be distracting or dangerous to the user. Accordingly upon detecting these conditions the monitoring application may then become more active and may present the monitoring interface as an overlay on top of the interfaces of any other currently executing applications. Other variations will also be apparent to those of skill in the art having the benefit of the disclosure herein.

When the user transportable device detects that the user has stopped or changed the path of the user to avoid the fire hydrant the device may return the monitoring interface to its previous size and cease the visual alert and or the audible alert and or the tactile alert. Alternatively in the case that the user has stopped altogether the user transportable device may no longer display the monitoring interface and may cease all alerts.

The compass may further indicate motion based on a change in direction of the device . The microphone or microphone array may detect noises that indicate a hazard or that contribute to detection of a hazard such a traffic noise approaching sirens electric vehicle signature noises fountain noises mass transit or train arrival noises to name a few. The communication interfaces can indicate a change in signal strength from a cell tower wireless access point or the like. The GPS device can detect movement of the device from a first location to a second location. In some examples the GPS device may be activated only periodically unless movement of the device has been detected in which case the GPS device may begin monitoring the direction of travel velocity of travel etc. The sonar device can emit an audible or inaudible noise such as in a periodic manner to detect any objects in a path of the user. Furthermore the monitoring module may identify one or more active applications that are currently active on the device such as being used currently by a user of the device or currently displaying an interface on the display . In addition the monitoring module may receive or may access device settings that may specify one or more settings such as user preferences for when the monitoring module displays the monitoring interface and so forth.

The monitoring module may include a situation recognition model which may be a trained statistical model that receives the information from the sensors and other sources and that determines whether the device is in motion such as being carried by a user who is walking jogging running biking skating skiing horseback riding driving a car or other vehicle or the like. As mentioned above the monitoring module may run as a background process and may provide information from one or more of the sensors or sources to the situation recognition model as the information is received. For instance the GPS device and the camera may normally be in a powered down condition to conserve a power supply such as a battery of the user transportable device . Subsequently if the monitoring module receives input from the one or more accelerometers which is determined by the situation recognition model to be indicative that the user is walking or running the monitoring module may activate the GPS device to determine a speed of the user. Various other inputs may also be used to determine movement of the user such as input from the communication interfaces . For instance when the user moves from one location to another the signal from a wireless access point or a cell tower may change in a manner that can be identified by the situation recognition model as being indicative of movement of the user.

Furthermore information from the accelerometers and or from other inputs or sources may indicate a current attitude or angle of the user transportable device with respect to the ground or other horizontal axis. In addition the active application s on the device may indicate that the monitoring module may start operating the camera to monitor a potential path of the user. For instance if the active interface or application is a type in which the user views or interacts with the display of the device and the accelerometers indicate that the device is currently being held at an attitude at which the user is able to view the display and the situation recognition model indicates that the user is likely to be moving e.g. within a confidence level then the monitoring module may begin to operate the camera as indicated at to monitor an area or region that includes a predicted path of the user e.g. within the field of view of the camera .

The camera may capture or obtain one or more images which may be provided to an image processing module . The images discussed herein may be in any suitable electronic image format such as TIFF Tagged Image File Format JPEG Joint Photographic Experts Group BMP Bitmap GIF Graphics Interchange Format PNG Portable Network Graphics Exif Exchangeable image file format RAW and so forth. Thus the disclosure herein is not limited by the image file format. In some examples a single image may be obtained while in other examples a sequence of images may be obtained. For instance multiple frames per second may be obtained and the images compared with one another to determine changes in the position of objects identified in the images which can assist in identification of objects that may pose a hazard to the user a distance to the objects and so forth.

The image processing module may carry out computer vision processing of the captured image to attempt to determine any objects or hazards in the predicted path of the user. As one example the image processing module may include or may access an image segmentation module that may be used to segment the image into components such as connected components made up of connected pixels that share one or more pixel properties. Various types of image segmentation may include thresholding clustering connected component analysis edge detection region growing split and merge methods graph partitioning methods model based segmentation and trainable segmentation.

After the image has been segmented an object recognition model may be applied to attempt to identify one or more objects included in the image . For example the object recognition model may be a trained statistical model that has been trained using a large number of training images including recognizable objects and may use pattern recognition techniques for identifying objects in the image . Examples of suitable pattern recognition models or classifiers that may be used include logistic regression classifiers support vector machines quadratic classifiers linear classifiers kernel estimation classifiers neural networks Bayesian networks hidden Markov models decision tree classifiers and so forth. Further implementations are not limited to any particular type of model or classifier and thus may employ any combination of algorithms learning models statistical models feature vectors and so forth that receive one or more inputs and categorize group cluster organize or otherwise classify the one or more inputs into one or more sets categories groups clusters or the like such as based on one or more quantifiable properties or features of the one or more inputs. Accordingly the object recognition model may receive the segmented image and identify one or more objects included in the image .

An image processing result may be provided to the monitoring module . For example the image processing result may identify one or more objects included in the image . The monitoring module or the image processing module may include a hazard identification model that may receive the image processing result as well as information from the sensors for attempting to identify whether there may be a hazard in the predicted or probably path of travel of the user. For instance the hazard identification model may be a trained statistical model such as discussed above that receives information from sensors including the compass the microphone the accelerometers the communication interfaces and the GPS device for determining a predicted path of travel of the user a speed of travel of the user and so forth. Further the hazard identification model may determine whether there is a hazard in the predicted path of the user such as based on the image processing result . For example the hazard identification model can predict a likelihood of a collision or other encounter with one or more of the objects recognized by the object recognition model . For example if the image s show that a user is approaching a yellow red white curb the edge of a subway platform pedestrian crossing marks an open manhole cover or the like which may indicate a possible hazard the hazard identification model can be trained to provide an output such as an alert to the user. In some cases when a particular type of object reaches a particular size in the image s the device may issue an alert based on the assumption that the object is in the user s path and an encounter is imminent. In other cases the hazard identification model may determine a likely probable path of the user such as based on compass information averaged image information GPS information and so forth and consider which objects in the image are likely to be a hazard to the user. The hazard identification model can provide an output to the user based on whether a hazard is identified in the predicted path of the user . For example if it appears imminent that the user will encounter the hazard the output may be an alert to the user as discussed above.

For example if the network connection of the user transportable device to the remote computing device is sufficiently fast it may be more economical to allow the remote computing device to perform the image processing and or the hazard recognition. For example image processing may be a processor intensive operation and thus by sending one or more images and or sensor information from one or more sensors to a remote computing device the user transportable device may conserve power and avoid processor intensive use of resources. Accordingly the remote computing device s may provide one or more computing services to the user transportable device .

In the illustrated example each remote computing device may include one or more processors one or more computer readable media and one or more communication interfaces . The processor s may be a single processing unit or a number of processing units and may include single or multiple computing units or multiple processing cores. The processor s can be configured to fetch and execute computer readable instructions stored in the computer readable media or other computer readable media.

The computer readable media may include volatile and nonvolatile memory and or removable and non removable media implemented in any type of technology for storage of information such as computer readable instructions data structures program modules or other data. Such computer readable media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape solid state storage magnetic disk storage RAID storage systems storage arrays network attached storage storage area networks cloud storage or any other medium that can be used to store the desired information and that can be accessed by a computing device. Depending on the configuration of the computing device the computer readable media may be a type of computer readable storage media and may be a tangible non transitory storage media.

The communication interface s may include one or more interfaces and hardware components for enabling communication with various other devices such as the user transportable device over the network s . For example communication interface s may facilitate communication through one or more of the Internet cable networks cellular networks wireless networks e.g. Wi Fi cellular and wired networks. Various different approaches to implementations described herein can be implemented in various environments. For instance the network s may include any suitable network including an intranet the Internet a cellular network a LAN WAN VPN or any other network or combination thereof. Components used for such a system can depend at least in part upon the type of network and or environment selected. Protocols and components for communicating via such networks are well known and will not be discussed herein in detail.

In some examples the remote computing device may include an image processing module which may perform image processing of one or more images received from the user transportable device in place of and in a manner similar to the image processing module discussed above with respect to . For example the image processing module may include the image segmentation module and the object recognition model as discussed above.

Further in some examples the remote computing device may also execute a hazard identification module in place of and similar to the hazard identification model discussed above to further save processing resources and or power on the user transportable device . Thus the user transportable device may send sensor information to the remote computing device over the network s to enable processing by the hazard identification module as well. Further since the trainable models such as the object recognition model and the hazard identification model are maintained on a remote server in this example these models may be updated on the remote computing device without having to update the models on each individual user transportable device .

Depending on the configuration of the user transportable device the computer readable media may be an example of tangible non transitory computer storage media and may include volatile and nonvolatile memory and or removable and non removable media implemented in any type of technology for storage of information such as computer readable instructions data structures program modules or other data. Such computer readable media may include but is not limited to RAM ROM EEPROM flash memory or other computer readable media technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape solid state storage and or magnetic disk storage. Further in some cases the user transportable device may access external storage such as RAID storage systems storage arrays network attached storage storage area networks cloud storage or any other medium that can be used to store information and that can be accessed by the processor directly or through another computing device or network. Accordingly the computer readable media may be computer storage media able to store instructions modules or components that may be executed by the processor .

The computer readable media may be used to store and maintain any number of functional components that are executable by the processor . In some implementations these functional components comprise instructions or programs that are executable by the processor and that when executed implement operational logic for performing the actions attributed above to the user transportable device . Functional components of the user transportable device stored in the computer readable media may include the monitoring module executable by the processor for detecting movement of the user transportable device and monitoring for hazards in a predicted path of a user as discussed above. Additional functional components stored in the computer readable media may include the image processing module as described above which may be executed by the processor for segmenting an image and recognizing objects in the image. Other functional components may include an operating system and a user interface module for controlling and managing various functions of the user transportable device and providing basic functionality. Additional modules may include a texting module for sending and receiving text messages an email module for sending and receiving emails a media player module for playing and presenting digital content items a telephone module for making telephone calls and a parental control module for enabling parental control to be exerted over the device such as for preventing use of certain functions of the device when movement of the device is detected or if it is determined that the user is driving a vehicle. Any or all of the modules and may be integrated into the operating system or may be operable separately therefrom. Additionally the computer readable media may include one or more applications such as one or more mobile applications that may be executed to employ the device to perform various functions and uses.

In addition the computer readable media may also store data data structures and the like that are used by the functional components. For example data stored by the computer readable media may include the one or more content items that may be presented by the media player module . In some examples the user transportable device may further include the user or device settings which may control operation of the monitoring module such as when the monitoring module operates what types of alerts are provided and so forth. In some examples the one or more images are stored in the computer readable media at least temporarily during image processing. The user transportable device may also maintain other data which may include for example data used by the monitoring module data used by the operating system and data used by the other modules described above. Depending on the type of the user transportable device the computer readable media may also optionally include other functional components and data such as other modules and data which may include applications programs drivers and so forth and the data used by the functional components. Further the user transportable device may include many other logical programmatic and physical components of which those described are merely examples that are related to the discussion herein.

Further while the figures illustrate the functional components and data of the user transportable device as being present on the user transportable device and executed by the processor s on the user transportable device it is to be appreciated that these components and or data may be distributed across different computing devices and locations in any manner. For example some or all of the various functionality described above for the image processing module and or the monitoring module may distributed in various ways across different computing devices. For instance the images captured by the camera may be sent to a remote server and processor intensive computer vision operations such as scene segmentation object recognition hazard recognition and other image processing and computer modeling techniques may be performed at the remote server with an image processing result and or an output or alert being returned to the monitoring module on the user transportable device .

Other components included in the user transportable device may include one of more cameras and various types of sensors which may include the one or more accelerometers the compass the microphone the GPS device and the sonar device . The one or more communication interfaces may support both wired and wireless connection to various networks such as cellular networks radio WiFi networks short range or near field networks e.g. Bluetooth infrared signals local area networks wide area networks the Internet and so forth. The communication interfaces may further allow a user to access storage on another device such as a remote computing device a network attached storage device cloud storage or the like.

The user transportable device may further be equipped with various other input output I O components . Such I O components may include a touchscreen and various user controls e.g. buttons a joystick a keyboard a keypad etc. one or more speakers a haptic or tactile output device connection ports and so forth. For example the operating system of the user transportable device may include suitable drivers configured to accept input from a keypad keyboard or other user controls and devices included as the I O components . For instance the user controls may include page turning buttons navigational keys a power on off button selection keys and so on. Additionally the user transportable device may include various other components that are not shown examples of which include removable storage a power source such as a battery and power control unit a PC Card component and so forth.

Various instructions methods and techniques described herein may be considered in the general context of computer executable instructions such as program modules stored on computer storage media and executed by the processors herein. Generally program modules include routines programs objects components data structures etc. for performing particular tasks or implementing particular abstract data types. These program modules and the like may be executed as native code or may be downloaded and executed such as in a virtual machine or other just in time compilation execution environment. Typically the functionality of the program modules may be combined or distributed as desired in various implementations. An implementation of these modules and techniques may be stored on computer storage media or transmitted across some form of communication media.

At the device based at least in part on sensor information determines that the device is in motion. For example the user transportable device may receive input from one or more sensors such as one or more accelerometers a compass a GPS device a communication interface a microphone a camera or any of various other types of sensors able to detect information and provide the information to the user transportable device .

At the device determines a current orientation of a display of the user transportable device. For example if the display of the user transportable device is at an angle at which the user normally holds the device when viewing the display such as between 0 and 90 degrees from horizontal then there is a likelihood that the user is viewing or using the display of the device.

At based at least in part on determining that the user transportable devices in motion and further based at least in part on the current orientation of the display the device obtains an image from a camera oriented at least in part in a direction of travel of the user transportable device. For example the device may determine from the one or more sensors a direction of travel and may operate a camera that is oriented toward the direction of travel to obtain an image of a region including a predicted path over which the user and the user transportable device will travel.

At the device presents on the display of the user transportable device the image obtained from the camera. As one example the device may present the image as an overlay or picture in picture presentation superimposed on the currently presented interface on the display.

At the device determines based at least in part on the image that an object in the image is a potential hazard. For instance the device can identify a potential hazard substantially in an area or region in the direction of travel of the user of the user transportable device. For example computer vision techniques and pattern recognition may be applied to the image to identify one or more hazards or potential hazards that may be in the predicted path of travel of the user.

At based at least in part on the determining that the object in the image is a potential hazard the device provides an alert. For example based at least in part on the identification of the potential hazard in the predicted path of travel the user transportable device may provide an alert to the user to warn the user of an impending encounter with the object identified as the potential hazard. The alert may be any of a visual alert an audible alert or a tactile alert as discussed above.

At the device based at least in part on sensor information determines that the device is in motion. For example the user transportable device may receive input from one or more sensors such as one or more accelerometers a compass a GPS device a communication interface a microphone a camera or any of various other types of sensors able to detect information and provide the information to the user transportable device.

At in response to determining that the user transportable device is in motion the user transportable device may monitor a region in a direction of travel of the user transportable device. For example the user transportable device may determine a general direction of travel based on information from one or more sensors. As one example a GPS device may indicate the direction in which the user is traveling. Additionally or alternatively a camera of the device may be used determine a predicted path of travel based on comparison of a sequence of images obtained from the camera to determined the general or average direction of travel. Furthermore in some examples prior to initiating monitoring the user transportable device can determine that the orientation of the display is such that the user may be viewing or is able to view the display.

At the device issues an alert when a hazard is detected in the path of the user and or the region in the direction of travel of the user transportable device. For example image analysis of one or more images obtained from the camera may be performed to detect a hazard in the predicted path of the user. In some cases pattern recognition techniques as discussed herein may be used to identify a hazard from the image.

At the computing device receives an image from a user transportable device. For example the remote computing device may receive one or more images from the user transportable device that were obtained by the device in response to detecting that the device is in motion. Thus in some implementations rather than performing the image processing and pattern recognition locally the user transportable device may offload the processing tasks to a remote computing device.

At the computing device detects an object in the image. For example the computing device may perform image segmentation and pattern recognition to detect one or more objects in the image received from the user transportable device.

At the computing device sends information related to the object to the user transportable device. For example the computing device may send information identifying an object in the image that was received from the user transportable device. Furthermore in some examples the computing device may identify a hazard in the path of travel of the user and may send an output to the user transportable device that may include an identification of the hazard and or an alert.

The example processes described herein are only examples of processes provided for discussion purposes. Numerous other variations will be apparent to those of skill in the art in light of the disclosure herein. Further while the disclosure herein sets forth several examples of suitable frameworks architectures and environments for executing the processes implementations herein are not limited to the particular examples shown and discussed.

Furthermore this disclosure provides various example implementations as described and as illustrated in the drawings. However this disclosure is not limited to the implementations described and illustrated herein but can extend to other implementations as would be known or as would become known to those skilled in the art. Reference in the specification to one implementation this implementation these implementations or some implementations means that a particular feature structure or characteristic described is included in at least one implementation and the appearances of these phrases in various places in the specification are not necessarily all referring to the same implementation.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described. Rather the specific features and acts are disclosed as example forms of implementing the claims.

