---

title: Parallel symbolic execution on cluster of commodity hardware
abstract: A symbolic execution task is dynamically divided among multiple computing nodes. Each of the multiple computing nodes explores a different portion of a same symbolic execution tree independently of other computing nodes. Workload status updates are received from the multiple computing nodes. A workload status update includes a length of a job queue of a computing node. A list of the multiple computing nodes ordered based on the computing nodes' job queue lengths is generated. A determination is made regarding whether a first computing node in the list is underloaded. A determination is made regarding whether a last computing node in the list is overloaded. Responsive to the first computing node being underloaded and the last computing node being overloaded, a job transfer request is generated that instructs the last computing node to transfer a set of one or more jobs to the first computing node.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08863096&OS=08863096&RS=08863096
owner: École Polytechnique Fédérale de Lausanne (EPFL)
number: 08863096
owner_city: Lausanne
owner_country: CH
publication_date: 20120106
---
This application claims the benefit of U.S. Provisional Application No. 61 430 191 filed on Jan. 6 2011 entitled Parallel Automated Testing Platform for Software Systems. 

The invention generally relates to the field of automated software testing and in particular to parallel symbolic execution of a target program using a cluster of commodity hardware.

Symbolic execution is an automated technique for testing software programs. Instead of executing a target program with regular concrete inputs e.g. x 5 symbolic execution executes a target program with symbolic inputs that can take on all values allowed by the type e.g. x where N and N is the set of all numbers . Whenever a conditional branch is encountered that involves a predicate that depends directly or indirectly on x state and execution are forked into two alternatives one following the then branch and another following the else branch . The two executions can now be pursued independently. This approach is efficient because it analyzes code for entire classes of inputs rather than specific concrete inputs. When a failure point e.g. a bug is found a test generator can compute concrete values for target program inputs that take the program to the bug location. A symbolic test specifies families of inputs and environment behaviors for which to test a target program. By encompassing entire families of behaviors symbolic tests cover substantially more test cases than concrete regular tests. Also a symbolic test enables environment conditions to be reproduced which otherwise would have been very difficult or impossible to set up with regular test cases.

In order to test a target program using symbolic execution a symbolic execution engine SEE executes the target program with unconstrained symbolic inputs. When an execution branch involves symbolic values execution forks into two separate executions each with a corresponding clone of the program state. Symbolic values in the clones are constrained to make the branch condition e.g. 

In this way all execution paths in the target program are explored. To ensure that only feasible paths are explored the SEE uses a constraint solver to check the satisfiability of each branch s predicate and the SEE follows only satisfiable branches. If a bug is encountered e.g. a crash or a hang along one of the paths then the solution to the constraints accumulated along that path yields the inputs that take the target program to the bug. These inputs constitute a test case.

One of the challenges faced by symbolic testing is scalability. The phenomenon of path explosion refers to the fact that the number of paths through a program is roughly exponential in program size. Since the size of an execution tree is exponential in the number of branches and the complexity of constraints increases as the tree deepens state of the art SEEs can quickly bottleneck on limited computing resources e.g. central processing unit CPU cycles and memory even for target programs that have only a few thousand lines of code KLOC . Path explosion severely limits the extent to which large software programs can be thoroughly tested. One must be content with either a low percentage of code coverage for large programs or using symbolic execution tools with only small programs.

The above and other issues are addressed by a computer implemented method non transitory computer readable storage medium and computer system for dynamically dividing a symbolic execution task among multiple computing nodes wherein each of the multiple computing nodes explores a different portion of a same symbolic execution tree independently of other computing nodes. An embodiment of the method comprises receiving workload status updates from the multiple computing nodes. A workload status update includes a length of a job queue of a computing node. The method further comprises generating a list of the multiple computing nodes ordered based on the computing nodes job queue lengths determining whether a first computing node in the list is underloaded and determining whether a last computing node in the list is overloaded. The method further comprises responsive to the first computing node being underloaded and the last computing node being overloaded generating a job transfer request that instructs the last computing node to transfer a set of one or more jobs to the first computing node and sending the job transfer request to the first computing node and the last computing node.

An embodiment of the medium stores executable computer program instructions for dynamically dividing a symbolic execution task among multiple computing nodes wherein each of the multiple computing nodes explores a different portion of a same symbolic execution tree independently of other computing nodes. The instructions receive workload status updates from the multiple computing nodes. A workload status update includes a length of a job queue of a computing node. The instructions further generate a list of the multiple computing nodes ordered based on the computing nodes job queue lengths determine whether a first computing node in the list is underloaded and determine whether a last computing node in the list is overloaded. The instructions further generate responsive to the first computing node being underloaded and the last computing node being overloaded a job transfer request that instructs the last computing node to transfer a set of one or more jobs to the first computing node and send the job transfer request to the first computing node and the last computing node.

An embodiment of the computer system for dynamically dividing a symbolic execution task among multiple computing nodes wherein each of the multiple computing nodes explores a different portion of a same symbolic execution tree independently of other computing nodes comprises at least one non transitory computer readable storage medium storing executable computer program instructions. The instructions comprise instructions for receiving workload status updates from the multiple computing nodes. A workload status update includes a length of a job queue of a computing node. The instructions further generate a list of the multiple computing nodes ordered based on the computing nodes job queue lengths determining whether a first computing node in the list is underloaded and determining whether a last computing node in the list is overloaded. The instructions further generate responsive to the first computing node being underloaded and the last computing node being overloaded a job transfer request that instructs the last computing node to transfer a set of one or more jobs to the first computing node and send the job transfer request to the first computing node and the last computing node.

The Figures FIGS. and the following description describe certain embodiments by way of illustration only. One skilled in the art will readily recognize from the following description that alternative embodiments of the structures and methods illustrated herein may be employed without departing from the principles described herein. Reference will now be made to several embodiments examples of which are illustrated in the accompanying figures. It is noted that wherever practicable similar or like reference numbers may be used in the figures and may indicate similar or like functionality.

Instead the environment uses a cluster of commodity hardware. This cluster based approach harnesses the computing resources of the individual commodity machines computing nodes into a distributed computer whose aggregate CPU and memory surpass that of a standard individual machine. The computing nodes can include single core machines and or multi core machines. Cluster based parallel symbolic execution provides the illusion of running a classic SEE on top of a large powerful computer. Without changing the exponential nature of the problem parallel symbolic execution harnesses cluster resources to make it feasible to perform symbolic testing on large programs. The cluster can be for example a private cluster or a cloud infrastructure e.g. the Elastic Compute Cloud EC2 web service from Amazon or the Eucalyptus cloud computing platform from Eucalyptus .

In the environment each computing node in the cluster has direct access to its own local memory. Since the computing nodes do not have access to any shared memory the cluster is referred to as shared nothing and the computing nodes communicate explicitly with each other to exchange information. Parallelizing symbolic execution on a large shared nothing cluster in a way that scales well is difficult. The environment provides cluster based parallel symbolic execution that scales linearly with the number of commodity machines computing nodes in the system. Specifically the environment provides a parallel SEE that executes on a commodity cluster. Since the execution scales linearly a user is able to throw hardware at the problem. 

The environment may be maintained by an enterprise that facilitates software testing such as a corporation university or government agency. As shown the environment includes a network a load balancing system and multiple worker systems . . . . Together the network the load balancing system and the worker systems form a platform for parallel symbolic execution of a target program. While only one load balancing system is shown in the embodiment depicted in for clarity other embodiments can have multiple load balancing systems . Also while only three worker systems are shown in the embodiment depicted in for clarity other embodiments can have other numbers of worker systems .

The network represents the communication pathway between the load balancing system and the worker systems and between the worker systems themselves. In one embodiment the network uses standard communications technologies and or protocols and can include the Internet. Thus the network can include links using technologies such as Ethernet 802.11 worldwide interoperability for microwave access WiMAX 2G 3G 4G mobile communications protocols digital subscriber line DSL asynchronous transfer mode ATM InfiniBand PCI Express Advanced Switching etc. Similarly the networking protocols used on the network can include muitiprotocol label switching MPLS transmission control protocol Internet protocol TCP IP User Datagram Protocol MP hypertext transport protocol HTTP simple mail transfer protocol SMTP file transfer protocol FTP etc. The data exchanged over the network can be represented using technologies and or formats including image data in binary form e.g. Portable Network Graphics PNG hypertext markup language HTML extensible markup language XML etc. In addition all or some of the links can be encrypted using conventional encryption technologies such as secure sockets layer SSL transport layer security TLS virtual private networks VPNs Internet Protocol security IPsec etc. In another embodiment the entities on the network can use custom and or dedicated data communications technologies instead of or in addition to the ones described above. Custom and or dedicated data communications technologies include for example shared storage e.g. a shared storage server a cloud based server and a bug database.

The load balancing system divides a symbolic execution task among worker systems computing nodes which then explore the execution tree independently of each other referred to as distributed exploration . One way to divide the task is to statically split the execution tree and farm off subtrees to computing nodes. However the contents and shape of the execution tree are not known until the tree is actually explored and finding a balanced partition i.e. one that will keep all computing nodes busy of an unexpanded execution tree is undecidable. Besides subtree size the amount of computing resources e.g. CPU and memory required to explore a subtree is also undecidable yet must be taken into account when partitioning the tree. When running on large target programs this approach leads to high workload imbalance among computing nodes making the entire cluster proceed at the pace of the slowest computing node. If this computing node gets stuck e.g. while symbolically executing a loop then the testing process may never terminate.

Instead the load balancing system partitions the execution tree dynamically as the tree is being explored by the worker systems . In one embodiment the load balancing system dynamically partitions the execution tree such that the parts are disjoint to avoid redundant work and together they cover the global execution tree for exploration to be complete . One side effect of dynamic partitioning is the transparent handling of fluctuations in resource quality availability and cost which are inherent to large clusters e.g. in cloud settings .

The load balancing system receives status updates on the worker systems workloads. As needed the load balancing system instructs pairs of worker systems to balance each other s workloads. In one embodiment the load balancing system attempts to minimize the number of work transfers and associated communication overhead. The load balancing system is a computer or set of computers that stores one or more processing modules and or one or more data repositories and is further described below with reference to .

A worker system explores the execution tree independently of other worker systems . Specifically each worker system explores a portion of the execution tree using an independent SEE symbolic execution module . A worker system sends status updates on its workload to the load balancing system . A worker system receives instructions from the load balancing system to balance its workload relative to another worker system . Encoding and transfer of work is handled directly by the two involved worker systems without intervention by the load balancing system . A worker system is a computer or set of computers that stores one or more processing modules and or one or more data repositories and is further described below with reference to .

The storage device includes one or more non transitory computer readable storage media such as a hard drive compact disk read only memory CD ROM DVD or a solid state memory device. The memory holds instructions and data used by the processor . The pointing device is used in combination with the keyboard to input data into the computer system . The graphics adapter displays images and other information on the display device . In some embodiments the display device includes a touch screen capability for receiving user input and selections. The network adapter couples the computer system to the network . Some embodiments of the computer have different and of other components than those shown in . For example the load balancing system and or the worker systems can be formed of multiple blade servers and lack a display device keyboard and other components.

The computer is adapted to execute computer program modules for providing functionality described herein. As used herein the term module refers to computer program instructions and or other logic used to provide the specified functionality. Thus a module can be implemented in hardware firmware and or software. In one embodiment program modules formed of executable computer program instructions are stored on the storage device loaded into the memory and executed by the processor .

The worker statistics repository stores statistics on the worker systems exploration progress. A worker system maintains a queue of exploration jobs each of which represents an unexplored subtree. In one embodiment the statistics stored in the worker statistics repository include lengths e.g. numbers of jobs of various worker systems job queues.

The processing server includes various modules such as an initial job module for sending an initial seed job to a worker system a worker statistics module for storing worker system statistics a load balancing module for instructing pairs of worker systems to balance each other s workloads and an exploration coordination module for coordinating worker system level explorations. In one embodiment the processing server includes a computer or set of computers that communicates with the repository and processes data e.g. by executing the initial job module the worker statistics module and the load balancing module .

The initial job module sends an initial seed job to a worker system . For example a load balancing system starts execution and is eventually contacted by a first worker system . In response to this contact the initial job module sends an initial seed job to that worker system . This initial seed job includes instructions to explore the entire execution tree. The initial seed job will be further discussed below with reference to the initial contact module .

The worker statistics module stores worker system statistics in the worker statistics repository . For example the worker statistics module receives a status update from a worker system regarding that worker system s workload and stores the appropriate statistics in the worker statistics repository e.g. the length of that worker system s job queue .

The load balancing module instructs one or more pairs of worker systems to balance each other s workloads. For example the load balancing module accesses worker statistics stored in the worker statistics repository analyzes these statistics and instructs one or more pairs of worker systems to balance each other s workloads as needed. In one embodiment the load balancing module operates as follows 

The lengths lof each worker system W s job queue Qare obtained from the worker statistics repository . The Ware sorted according to their queue length land placed in a list. The average and standard deviation of the lvalues are computed. Each Wis classified as underloaded l or okay otherwise. is a constant factor. One or more underloaded worker systems from the beginning of the list are matched with one or more overloaded worker systems from the end of the list thereby forming one or more pairs where Wis underloaded and Wis overloaded. For each pair with l

The load balancing module is executed periodically. The load balancing module can be executed more often or less often as desired. In general the more frequently the load balancing module is executed the less chance there is of having a worker system run out of work to perform. In one embodiment the load balancing module is executed after the worker statistics repository has been updated to reflect a job transfer initiated by the previous execution of the load balancing module .

In one embodiment the load balancing module is executed after every other iteration of worker system status updates i.e. after every two iterations where an iteration of status updates includes receiving one status update from each worker system . Note that during one iteration a worker system can send multiple status updates while the load balancing system is waiting for status updates from other worker systems . In that situation only the status update most recently received by the load balancing system is considered. Waiting for two iterations ensures that the status updates reflect all of the job transfers initiated by the previous execution of the load balancing module .

In one embodiment if the load balancing system decides that worker system A needs to transfer a job to worker system B then the load balancing system informs worker system A of this decision as a response to the next status update received from worker system A. In response worker system A initiates a connection to worker system B to perform the job transfer. Worker system A does not send any other status updates to the load balancing system before it finishes the job transfer. Worker system B on the other side continues sending status updates to the load balancing system and uses a separate thread to receive the job front worker system A. After the first iteration of updates worker system A will have received the job transfer request and initiated the transfer while worker system B may have sent its update before or after the transfer and thus cause the set of transferred jobs to be reported both by worker system A and worker system B . After the second iteration worker system A and worker system B will both report the correct new values. These values enable the load balancing module to properly execute its decision algorithm again.

The exploration coordination module coordinates worker system level explorations. Classic symbolic execution relies on heuristics to choose which state on an exploration frontier to explore first so as to efficiently reach a chosen test goal e.g. code coverage or finding a particular type of bug . In a distributed setting like environment local heuristics are coordinated across worker systems to achieve a global goal while keeping communication overhead at a minimum. In one embodiment the exploration coordination module ensures that eventually all paths in the execution tree are explored. In another embodiment the exploration coordination module aids in focusing on the execution paths desired by the global strategy. In this sense the exploration strategies represent policies.

Global strategies are implemented in the environment using a strategy interface for building overlays on the execution tree structure. In one embodiment the strategy interface implements distributed versions of strategies that come with the KLEE symbolic execution tool. KLEE was developed at Stanford University and built on top of the LLVM low level intermediate representation and compiler framework. The strategy interface can also be available to users of the environment .

For example in one embodiment a global strategy is built as follows A code coverage optimized strategy drives exploration so as to maximize code coverage. In the environment code coverage is represented as a bit vector with one bit per line of code. A set bit indicates that a line is covered. If a worker system explores a program state the worker system sets the corresponding bits locally. The current version of the local bit vector is appended to the status update sent to the load balancing system thereby notifying the load balancing system of the worker system s current progress in terms of code coverage. The load balancing system maintains the current global coverage vector. If the load balancing system receives an updated coverage bit vector the load balancing system logically OR s the updated coverage bit vector into the current global coverage vector. The resulting vector is then sent back to the worker system which in turn logically OR s this global bit vector into its own local bit vector in order to enable the worker system s local exploration strategy to make choices consistent with the global goal. The code coverage bit vector is an example of an overlay data structure.

The strategy interface is further discussed below with reference to strategy portfolios and class uniform path analysis. 

The target program is a software program. The environment enables a target program to be tested automatically using symbolic execution that is performed in parallel. The target program can be single threaded multi threaded or distributed. In one embodiment the target program is loaded into a worker system from a network file system not shown . In another embodiment the target program is sent from the load balancing system to a worker system if the worker system joins the cluster. The target program can be stored as executable code e.g. native binary code .

The symbolic test specifies families of program inputs and environment behaviors for which to test the target program . The environment enables a symbolic test to be applied to a target program . A symbolic test resembles a classic unit test in the sense that they are both code portions that exercise the functionality of a target program and check for some properties e.g. through the use of assertions . A symbolic test differs from a classic unit test in various aspects 1 A symbolic test uses a special API to mark parts of the target program input as symbolic and thus allow multiple paths through the target program to be explored and or to control the behavior of the target program s environment. 2 A symbolic test is compiled together with the target program into a special format that is passed to the SEE. 3 A symbolic test requires more resources than a classic unit test e.g. it executes for a longer period of time and requires larger computational power and is time bounded. In one embodiment the symbolic test programmatically controls fine grained aspects of the executing target program s environment and or orchestrates environment related events. For example the symbolic test injects faults at the boundary between the target program and its environment and or controls target program thread schedules.

A symbolic test is executed by a worker system specifically by a symbolic execution module . Symbolic tests are usually written in the programming language of the target program and are executed as a suite e.g. periodically during and after the development phase . The resulting format of a symbolic test is specific to the SEE. In one embodiment the symbolic test adheres to the LLVM format.

In one embodiment the symbolic test is loaded into a worker system from a network file system not shown . In another embodiment the symbolic test is sent from the load balancing system to a worker system if the worker system joins the cluster. The symbolic test can be stored as executable code e.g. native binary code .

The partial symbolic execution tree is a partial view of a global symbolic execution tree. A partial view reveals only a portion of the global symbolic execution tree. The partial view is a subtree of the global tree in the sense that there is an inclusion relationship between the partial view s tree nodes and the global tree s tree nodes. The partial symbolic execution tree has been assigned to a worker system to be explored. A worker system s partial symbolic execution tree includes the root of the global execution tree. A worker system s knowledge is limited to the partial symbolic execution tree that the worker system is exploring locally. As Wexplores and reveals the content of its local partial symbolic execution tree it has no knowledge of W s i j partial symbolic execution tree. Together all the partial trees of the various worker systems completely map the global tree. In one embodiment no element in the environment not even the load balancing system maintains a global execution tree. Recall that disjointness and completeness of the exploration are ensured by the load balancing system .

In one embodiment a symbolic execution tree or partial view thereof is stored using two custom data structures tree node pins and a tree layer. A tree node pin is a type of smart pointer customized for trees. Standard smart pointers can introduce significant performance disruptions when used for linked data structures. Chained destructors can introduce noticeable deallocation latency and can even overflow the stack and crash the system. A tree node pin enables a tree to be treated like a rubber band data structure. Specifically as tree nodes get allocated the rubber band is stretched and some tree nodes act as pins to anchor the rubber band. When such a pin is removed the tree nodes with no incoming references are freed up to the point where the rubber band reaches the pin next closest to the root. Tree nodes between two pins are freed all at once avoiding the use of the stack for recursive destructor calls.

The tree layer is a layer based structure similar to that used in computer aided design CAD tools where the actual tree is a superposition of simpler layers. When exploring the tree one chooses the layer of interest. Switching between layers can be done dynamically at virtually zero cost. In one embodiment separate layers are used for symbolic states imported jobs and other sets of internal information.

The exploration state indicates exploration characteristics regarding the tree nodes of a worker system s partial symbolic execution tree . The tree nodes of a worker system s partial symbolic execution tree are characterized as one of three types 1 internal tree nodes that have already been explored and are thus no longer of interest referred to as dead tree nodes 2 tree nodes that demarcate the portion being explored separating the domains of different worker systems referred to as fence tree nodes and 3 tree nodes that are ready to be explored referred to as candidate tree nodes . A worker system explores only candidate tree nodes. A worker system never explores dead tree nodes or fence tree nodes.

Candidate tree nodes are leaves of the local partial symbolic execution tree and they form the exploration frontier. The load balancing module ensures that exploration frontiers are disjoint between worker systems thus ensuring that no worker system duplicates the exploration performed by another worker system . At the same time the union of all exploration frontiers in the environment corresponds to the frontier of the global execution tree. As explained above when the global exploration frontier becomes poorly balanced across worker systems the load balancing module determines a loaded source worker Wand a less loaded destination worker Wand instructs them to balance workload by sending n jobs from Wto W in the extreme Wis a new worker system i.e. a worker system that recently joined the cluster or a worker system that has finished exploring its partial symbolic execution tree and has zero jobs left.

Regarding tree nodes and jobs a tree node is an element in a symbolic execution tree while a job is the path in the symbolic execution tree that leads to a tree node. The work transferred between worker systems is encoded as a set of one or more jobs. A job can point to a not yet existing tree node in which case the job is replayed by the symbolic execution module discussed below .

The processing server includes various modules such as an initial contact module for initially contacting a load balancing system a status update module for sending a status update to a load balancing system a job transfer module for sending or receiving an exploration job a candidate node selection module for determining a next candidate tree node to explore a symbolic execution module for symbolically executing a target program and a failure information module for gathering information regarding a failure. In one embodiment the processing server includes a computer or set of computers that communicates with the repository and processes data e.g. by executing the initial contact module the status update module the job transfer module the candidate node selection module symbolic execution module and the failure information module .

The initial contact module initially contacts a load balancing system . For example a worker system starts execution and uses the initial contact module to initially contact a load balancing system . In response to this contact the worker system can receive 1 an initial seed job if this contact is the first time that the load balancing system was contacted by any worker system or 2 a job transfer request or 3 nothing. If this contact is not the first time that the load balancing system was contacted by any worker system then the worker system may or may not receive a job transfer request in response to the initial contact depending on the implementation of the load balancing algorithm. The job transfer request can be postponed e.g. until there is sufficient workload in the cluster that the load balancing system can instruct that some of the workload be sent to the worker system . For instance when N worker systems join the cluster the load balancing system will give the seed job to one of them. During the second load balancing iteration only the first worker system will have work to give away and thus only one of the N 1 other worker systems will be selected for job transfer. Generally speaking the number of worker systems needed for work transfer starts with 1 and then it doubles after each load balancing iteration until all the worker systems may be involved in load balancing decisions. Therefore this process requires a number of iterations logarithmic in the number of worker systems in order to converge.

Recall that an initial seed job includes instructions to explore the entire execution tree. If the worker system receives an initial seed job then the initial contact module instructs the symbolic execution module discussed below to replay the initial seed job until the symbolic execution module obtains the starting tree node here the root tree node thereby starting the exploration from the root tree node in the symbolic execution tree. The initial seed job will be further discussed below with reference to the job transfer module .

The status update module sends a status update to a load balancing system . The status update includes information regarding that worker system s workload in terms of exploration jobs e.g. the length of that worker system s exploration job queue . In one embodiment the status update module sends status updates periodically e.g. at configurable time intervals .

The job transfer module sends or receives an exploration job. For example a job transfer module receives from a load balancing system a job transfer request. In one embodiment the job transfer request includes three pieces of information an indication of a source worker system W an indication of a destination worker system W and a number of jobs to move n .

If the job transfer module is part of the source worker system W then the job transfer module determines n of its candidate tree nodes and packages them for transfer to the destination worker system W . In one embodiment the job transfer module determines the n candidate tree nodes randomly from among the candidate tree nodes on the symbolic execution tree s exploration frontier. In another embodiment the job transfer module determines the first n candidate tree nodes encountered during a depth first search traversal of the symbolic execution tree. In this embodiment the n candidate tree nodes belong to the same region in the tree which minimizes the replay effort since these tree nodes will share many common paths that will be replayed only once . Since a candidate tree node sent to another worker system is now on the boundary between the work done by W and the work done by W that candidate tree node becomes a fence tree node at W e.g. as stored in the exploration state . This conversion prevents redundant work.

In one embodiment an exploration job is sent by serializing the content of the chosen tree node and sending it to W. The content of a tree node which represents an execution state is maintained by the symbolic execution module discussed below . The execution state includes the content symbolic or concrete of each memory location the set of processes and threads created including the program counter and stack of each thread and the path constraints accumulated up to that execution point. In another embodiment an exploration job is sent by sending to Wthe path from the root of the global symbolic execution tree to the chosen tree node and relying on Wto replay that path and obtain the contents of the tree node. Any fence tree nodes that border this path will never be explored. Choosing one embodiment versus the other is a trade off between time to encode decode and network bandwidth. The first embodiment requires little work to decode but consumes bandwidth. The state of a real target program is typically at least several megabytes. Encoding a job as a path i.e. the second embodiment requires replay on W. If large commodity clusters are assumed to have abundant CPU but meager bisection bandwidth then it is better to encode jobs as the path from the root to the candidate tree node i.e. the second embodiment . As an optimization common path prefixes can be exploited jobs are not encoded separately but rather the corresponding paths are aggregated into a job tree and sent as such.

If the job transfer module is part of the destination worker system W then the job transfer module will receive n of W s candidate tree nodes. When an exploration job arrives at W it is placed conceptually in a queue referred to as an exploration job queue . If the job transfer module receives a job tree then the job transfer module imports the job tree into W s own partial symbolic execution tree and the leaves of the job tree become part of W s frontier e.g. as stored in the exploration state . At the time of arrival these tree nodes may lie ahead of W s frontier. The tree nodes in the incoming jobs are characterized as virtual tree nodes as opposed to materialized tree nodes which reside in the local partial symbolic execution tree . Paths are replayed only lazily. A materialized tree node is one that contains the corresponding program state whereas a virtual tree node is an empty shell without corresponding program state. In the common case the frontier of a worker system s local partial symbolic execution tree contains a mix of materialized and virtual tree nodes.

Recall that a worker system can receive an initial seed job from the load balancing system . In this situation which is separate from receiving a job transfer request the initial seed job is in the same format as a regular job transferred between worker systems . Specifically the initial seed job is a path to a specific tree node in the symbolic execution tree. In one embodiment the initial seed job is an empty path which refers to the root tree node of the symbolic execution tree. When the first worker system receives the initial seed job the worker system s symbolic execution module replays the seed job until the symbolic execution module obtains the starting tree node i.e. the root tree node .

If a job is transferred from one worker system to another the replay done during materialization must successfully reconstruct the transferred state. Along the reconstruction path the destination must execute the same instructions obtain the same symbolic memory content and receive the same results during constraint solving as on the source worker system. Failing to do so causes the replayed path to be broken i.e. the path either diverges or terminates prematurely . In both cases this means the state cannot be reconstructed and this can affect exploration completeness.

One challenge is that a symbolic execution module based on an underlying KLEE symbolic execution engine SEE relies on a global memory allocator to service the target program s malloc calls. The allocator returns actual host memory addresses which is necessary for executing external system calls that access target program state. Unfortunately this means that buffers are allocated at addresses whose values for a given state depend on the history of previous allocations in other states. Such cross state interference leads to frequent broken replays.

In one embodiment the symbolic execution module s KLEE allocator is replaced with a per state deterministic memory allocator which uses a per state address counter that increases with every memory allocation. To preserve the correctness of external calls that require real addresses this allocator gives addresses in a range that is also mapped in the SEE address space using the memory space allocator of the host operating system e.g. POSIX s mmap . Thus before external calls are invoked the memory content of the state is copied into the mmap ed region.

The candidate node selection module determines a next candidate tree node to explore. At an exploration processing step the candidate node selection module determines which candidate tree node to explore next. This determination is guided by a strategy. Since the set of candidate tree nodes can include both materialized and virtual tree nodes it is possible for the strategy to determine a virtual tree node as the next one to explore. If this happens then the corresponding path in the partial symbolic execution tree is replayed i.e. the symbolic execution module executes that path . At the end of this replay all tree nodes along the path are dead except the leaf tree node which has been converted from virtual to materialized and is now ready to be explored. Note that while exploring the chosen job path each branch produces child program states. Any such state that is not part of the path is marked as a fence tree node because it represents a tree node that is being explored elsewhere so Wshould not pursue it .

In summary a tree node N in worker system W s partial symbolic execution tree has two attributes N materialized virtual and N candidate fence dead. A worker system s frontier Fis the set of all candidate tree nodes on worker system W. The worker system can explore only tree nodes in F i.e. dead tree nodes are off limits and so are fence tree nodes except if a fence tree node needs to be explored during the replay of a job path . The union of Fequals the frontier of the global execution tree ensuring that the aggregation of worker level explorations is complete. The intersection of Fi the empty set thus avoiding redundancy by ensuring that worker systems explore disjoint partial trees.

The symbolic execution module symbolically executes a target program. For example the symbolic execution module symbolically executes the target program according to the test . In one embodiment the symbolic execution module includes a symbolic execution engine SEE based on the KLEE symbolic execution tool specifically KLEE s single computing node symbolic execution engine .

The failure information module gathers information regarding a failure e.g. a bug . For example if a bug is encountered during symbolic execution of a target program the failure information module computes the target program inputs the thread schedule and the system call returns that would take the target program to that bug. In one embodiment the failure information module uses KLEE s cache mechanism for constraint solving results. This constraint cache can significantly improve constraint solver performance. In the environment states are transferred between worker systems without the source worker s constraint cache. While one might expect this to hurt performance significantly in practice the necessary portion of the constraint cache is mostly reconstructed as a side effect of path replay as the path constraints are re sent to the local constraint solver.

Described above was an environment for parallel symbolic execution of a target program. One challenge with symbolic execution is mediating between a target program and its environment i.e. symbolically executing a target program that calls into libraries and or the operating system OS or communicates with other systems neither of which executes symbolically . Real world systems interact heavily with the environment in varied and complex ways e.g. through system calls and library calls and can communicate with other parties e.g. over sockets interprocess communication IPC and shared memory . For a symbolic execution tool to be used in practice the tool must be capable of handling these interactions.

In one embodiment the symbolic execution module allows a call from a target program to go through into the concrete environment e.g. to write a file . Unfortunately this causes the target program s environment to be altered for all forked executions being explored in parallel thus introducing inconsistency.

In another embodiment the symbolic execution module replaces the target program s real environment with a symbolic model i.e. a piece of code linked with the target program that provides the illusion of interacting with a symbolically executing environment . Specifically the real environment is replaced with a quasi complete symbolic model that adheres to the Portable Operating System Interface for Unix POSIX family of standards and that makes it possible to use symbolic execution on real world systems. This symbolic model of a target program s environment supports major aspects of the POSIX interface including processes threads synchronization networking IPC and file input output I O . This symbolic model is sufficiently accurate and complete to enable the testing of complex real software.

The goal of a symbolic model is to simulate the behavior of a real execution environment while maintaining the necessary symbolic state behind the environment interface. A symbolic execution engine SEE can then seamlessly transition back and forth between the target program and the environment. Also symbolic execution with a model can be substantially faster than without. Requirements that complicate a real environment OS implementation such as performance and extensibility can be ignored in a symbolic model.

In one embodiment a symbolic system call interface to the symbolic execution module s SEE provides the building blocks for thread context switching address space isolation memory sharing and sleep operations. These features are difficult to provide solely through an external model. Symbolic system calls are further described below with reference to symbolic engine modifications. 

In some cases it is practical to have the host OS handle parts of the environment via external calls. These are implemented by concretizing the symbolic parameters of a system call before invoking it from symbolically executing code. The symbolic execution module allows external calls for only stateless or read only system calls such as reading a system configuration file. This restriction ensures that external concrete calls do not alter other symbolically executing paths.

The symbolic execution module builds upon the KLEE symbolic execution engine and so the symbolic execution module inherits from KLEE the mechanism for replacing parts of the C Library with model code. The symbolic execution module also inherits the external calls mechanism. The symbolic execution module adds the symbolic system call interface and replaces parts of the C Library with the POSIX model. The resulting architecture is shown in . is a high level block diagram illustrating the architecture of the symbolic execution module s POSIX model according to one embodiment.

Before symbolic execution starts the target program is linked with a special symbolic C Library. This library resembles a standard C library except for some parts that were replaced with the corresponding POSIX model code. The code of the target program need not be modified in any way to enable it to use the POSIX model.

Referring to in the C Library operations related to threads processes file descriptors and network operations were replaced with their corresponding model and the API was augmented with specific extensions . A large portion of the C Library was reused since it worked without modification e.g. any API calls that were implemented solely in user space without requiring direct operating system support such as memory and string operations . Finally parts of the original C Library itself use the modeled code e.g. Standard I O stdio relies on the modeled POSIX file descriptors .

The modeled POSIX components interface with the SEE through symbolic system calls listed in Table 1 below. Occasionally the unmodified part of the C Library invokes external system calls and the model code itself needs support from the host OS . To make sure the external calls do not interfere with the symbolic engine s own operations such access is limited to read only and or stateless operations. This avoids problems like for instance allowing an external close system call to close a network connection or log file that is actually used by the SEE itself.

To support the POSIX interface the symbolic execution module s symbolic execution engine should provide support for multiple address spaces to emulate processes and offer a mechanism to enable the control of thread scheduling. This functionality is accessed by model code through the symbolic system call interface see Table 1 . Additional models of non POSIX environments can be built using this interface. For example the interface enables one to build any user facing API such as the Microsoft Windows API or a GPU environment such as OpenCL or CUDA . These APIs can be modeled on top of the symbolic system call interface.

If KLEE s symbolic execution module is used then KLEE is augmented with multiple address spaces per state and support for scheduling threads and processes. KLEE uses copy on write CoW to enable memory sharing between symbolic states. This functionality is extended in two ways. First multiple address spaces are enabled within a single execution state corresponding to multiple processes encompassed in that state. Address spaces can thus be duplicated both across states as in classic KLEE and within a state when cloud9 process fork is invoked e.g. as used by the POSIX model s fork .

Second the address spaces in an execution state are organized as CoW domains that permit memory sharing between processes. A memory object can be marked as shared by calling cloud9 make shared. The memory object is then automatically mapped in the address spaces of the other processes within the CoW domain. Whenever a shared object is modified in one address space the new version is automatically propagated to the other members of the CoW domain. The shared memory objects can then be used by the model as global memory for inter process communication.

Threads are created in the currently executing process by calling cloud9 thread create. The POSIX threads pthreads model makes use of this primitive in its own pthread create routine.

A cooperative scheduler is implemented. An enabled thread runs uninterrupted atomically until either a the thread goes to sleep b the thread is explicitly preempted by a cloud9 thread preempt call or c the thread is terminated via symbolic system calls for process thread termination. Preemption occurs at explicit points in the model code but it is straightforward to extend the symbolic execution module to automatically insert preemption calls at instruction level as would be necessary for instance when testing for race conditions .

When cloud9 thread sleep is called the SEE places the current thread on a specified waiting queue and an enabled thread is selected for execution. Another thread may call cloud9 thread notify on the waiting queue and wake up one or all of the queued threads.

The symbolic execution module can be configured to schedule the next thread deterministically or to fork the execution state for each possible next thread. The latter case is useful when looking for concurrency bugs but it can be a significant source of path explosion so it should be disabled when not needed.

If no thread can be scheduled when the current thread goes to sleep then a hang is detected the execution state is terminated and a corresponding test case is generated.

Note that parallelizing symbolic execution is orthogonal to providing the multithreading support described above. In the former case the execution engine is instantiated on multiple machines and each instance expands a portion of the symbolic execution tree. In the latter case multiple symbolic threads are multiplexed along the same execution path in the tree. Execution is serial along each path.

The POSIX model uses shared memory structures to keep track of all system objects processes threads sockets etc. . The two most important data structures are stream buffers and block buffers analogous to character and block device types in UNIX. Stream buffers model half duplex communication channels. They are generic producer consumer queues of bytes with support for event notification to multiple listeners. Event notifications are used for instance by the polling component in the POSIX model. Block buffers are random access fixed size buffers whose operations do not block they are used to implement symbolic files.

The symbolic execution engine maintains only basic information on running processes and threads identifiers running status and parent child information. However the POSIX standard mandates additional information such as open file descriptors and permission flags. This information is stored by the model in auxiliary data structures associated with the currently running threads and processes. The implementations of fork and pthread create are in charge of initializing these auxiliary data structures and making the appropriate symbolic system calls.

Modeling synchronization routines is simplified by the cooperative scheduling policy. No locks are necessary and all synchronization can be done using the sleep notify symbolic system calls together with reference counters. illustrates the simplicity this engenders in the implementation of pthread mutex lock and unlock. is a listing of C code illustrating an example implementation of pthread mutex operations in the POSIX environment model.

The POSIX model inherits most of the semantics of the file model from KLEE. In particular one can either open a symbolic file its contents comes from a symbolic block buffer or a concrete file in which case a concrete file descriptor is associated with the symbolic one and all operations on the file are forwarded as external calls on the concrete descriptor.

In addition to file objects the POSIX model adds support for networking and pipes. Currently the TCP and UDP protocols are supported over IP and UNIX network types. Since no actual hardware is involved in the packet transmission the entire networking stack can be collapsed into a simple scheme based on two stream buffers see . is a high level block diagram illustrating a TCP network connection modeled using TX and RX buffers implemented as stream buffers according to one embodiment. The network is modeled as a single IP network with multiple available ports. This configuration is sufficient to connect multiple processes to each other in order to simulate and test distributed systems. The model also supports pipes through the use of a single stream buffer similar to sockets.

The POSIX model supports polling through the select interface. The select model relies on the event notification support offered by the stream buffers that are used in the implementation of blocking I O objects e.g. sockets and pipes .

The constraint solver operates on bit vectors. As a result symbolic formulas refer to contiguous areas of memory. To reduce the constraint solving overhead the amount of intermixing of concrete and symbolic data in the same memory region is reduced. The POSIX model segregates concrete from symbolic data by using static arrays for concrete data and linked lists or other specialized structures for symbolic data. Potentially symbolic data passed by the tested program through the POSIX interface is allocated into separate buffers.

Described above was an environment for parallel symbolic execution of a target program. The symbolic execution tests the target program according to a symbolic test . One challenge with symbolic execution is using an automated test generator in the context of a development organization s quality assurance processes. To take full advantage of the automated exploration of paths a testing tool should provide ways to control aspects of the environment. For example there should be a clean API for injecting failures at the boundary between programs and their environment a way to control thread schedules and so on. There should be a way to programmatically orchestrate all environment related events but doing so should not require deep expertise in the technology behind the testing tools themselves.

In one embodiment a testing platform is used to write symbolic tests. The testing platform includes an easy to use API and primitives that provide a systematic interface for writing symbolic tests. Developers can specify concisely families of inputs and environment behaviors for which to test the target software without having to understand how symbolic execution works which program inputs need to be marked symbolic or how long the symbolic inputs should be. The API enables for instance finding errors in bug patches by reproducing environment conditions which otherwise would have been hard or impossible to set up with regular test cases. The testing platform enables developers to write symbolic tests that concisely specify entire families of inputs and behaviors to be tested thus improving testing productivity. Existing test suites can be used to generate new test cases that capture untested corner cases e.g. network stream fragmentation . The testing platform enables fine grain control over the behavior being tested including the injection of faults and the scheduling of threads.

Software products and systems typically have large handmade test suites. Writing and maintaining these suites requires substantial human effort. The testing platform reduces this burden while improving the quality of testing by offering an easy way to write symbolic test suites. First a symbolic test case encompasses many similar concrete test cases into a single symbolic one. Each symbolic test a developer writes is equivalent to many concrete ones. Second a symbolic test case explores conditions that are hard to produce reliably in a concrete test case such as the occurrence of faults concurrency side effects or network packet reordering dropping and delay. Furthermore symbolic test suites can easily cover unknown corner cases as well as new untested functionality.

A symbolic testing API see Tables 2 and 3 enables a symbolic test to programmatically control events in the environment of the target program . A symbolic test suite need only include a cloud9.h header file and make the requisite calls.

The generality of a test case can be expanded by introducing bytes of symbolic data. This is done by calling cloud9 make symbolic a wrapper around klee make symbolic with an argument that points to a memory region. klee make symbolic is a primitive provided by KLEE to mark data symbolic. In addition to wrapping this call several new primitives are added to the testing API Table 2 . Symbolic data can be written to read from files can be sent received over the network and can be passed via pipes. Furthermore the SIO SYMBOLIC ioctl code Table 3 turns on off the reception of symbolic bytes from individual files or sockets.

Delay reordering or dropping of packets causes a network data stream to be fragmented. Fragmentation can be turned on or off at the socket level using one of the ioctl extensions. Symbolic fragmentation can be used for example to prove that a bug fix for a web server was incomplete.

Calls in a POSIX system can return an error code when they fail. Most programs can tolerate such failed calls but even high quality production software misses some. Such error return codes are simulated by the testing platform whenever fault injection is turned on.

The testing platform provides multiple scheduling policies that can be controlled for purposes of testing on a per code region basis. Currently the testing platform supports a round robin scheduler and two schedulers specialized for bug finding a variant of the iterative context bounding scheduling algorithm and an exhaustive exploration of all possible scheduling decisions.

Consider a scenario in which one wants to test the support for a new X NewExtension HTTP header just added to a web server. Tests for this new feature can be written as follows 

A symbolic test suite typically starts off as an augmentation of an existing test suite. In this scenario the existing boilerplate setup code is reused and a symbolic test case is written that marks the extension header symbolic. Whenever the code that processes the header data is executed the symbolic execution engine forks at all the branches that depend on the header content. Similarly the request payload can be marked symbolic to test the payload processing part of the system 

The web server may receive HTTP requests fragmented in a number of chunks returned by individual invocations of the read system call. The web server should run correctly regardless of the fragmentation pattern. To test different fragmentation patterns one simply enables symbolic packet fragmentation on the client socket ioctl ssock SIO PKT FRAGMENT RD 

To test how the web server handles failures in the environment one can selectively inject faults when the server reads or sends data on a socket by placing in the symbolic test suite calls of the form 

Fault injection can be enabled disabled globally for all file descriptors within a certain region of the code using calls to cloud9 fi enable and cloud9 fi disable. For simulating low memory conditions a cloud9 set max heap primitive can be used to test the web server with different maximum heap sizes. Strategy Portfolios

Described above was an environment for parallel symbolic execution of a target program. Recall that the exploration coordination module coordinates worker system level explorations and global strategies are implemented in the environment using a strategy interface for building overlays on the execution tree structure. Since the search space in symbolic execution is roughly exponential in the size of the program and potentially infinite it is helpful to devise a strategy to explore only the relevant subset of paths that are likely to lead to the exploration goal e.g. code coverage or uncovering new bugs . In practice the available strategies are heuristics whose efficiencies are unknown a priori and vary across target programs.

In one embodiment explorations based on different strategies are executed simultaneously and the results obtained independently by the explorations are then combined e.g. by collecting all bugs found and or aggregating all the coverage information . This approach is referred to as a strategy portfolio. In one embodiment the strategy portfolio approach is characterized by one or more of the following features 

1. Inspired by the Modern Portfolio Theory a strategy portfolio is a composite strategy where multiple member strategies maintain independent queues of states and each member strategy selects one state to explore next. This portfolio of concurrent strategies can be thought of in analogy to financial investment portfolios. If a strategy is a stock worker systems represent cash and the portfolio s return is measured in results per unit of time then the problem is allocating cash to stocks so as to maximize overall return. By casting the exploration problem as an investment portfolio optimization problem portfolio theory results can be reused such as diversification and speculation as well as quantitative techniques for improving returns such as efficient frontier and alpha beta coefficients. For example a small number of worker systems can be speculatively devoted to a strategy that works exceptionally well but for only a small fraction of programs. Running this exploration on a copy of the execution tree in parallel with a classic strategy that bears less risk may improve the expected time of reaching the overall goal.

2. Each strategy starts from the same initial state of the system but the execution tree and state queue evolve differently and independently under each strategy.

3. The strategies run in parallel. One can imagine this as the composite strategy selecting an n tuple of states to explore simultaneously rather than one at a time. Each state in the tuple corresponds to the selection of each member strategy.

4. In a goal oriented exploration if any of the states in the n tuple matches the exploration goal the exploration ends. In a coverage oriented exploration the exploration progress of each state in the n tuple is aggregated into a global composite progress.

5. A strategy portfolio benefits from diversification. For each target program the performance of the portfolio is better than the maximum performance of its individual components.

6. A strategy portfolio offers parallelization opportunities. Each strategy member in the portfolio can run on separate computing resources either single computing node or an instance of environment thus offering trivial parallelism.

When the strategy portfolio approach is used different information is stored in the load balancing system s repository . For example rather than storing only one worker statistics repository multiple worker statistics repositories are stored one for each strategy containing information regarding the worker systems executing that strategy . A global work progress repository is also stored which includes work progress information over the entire cluster e.g. a bit vector representing overall code coverage for the entire cluster .

Additional information is stored in the worker system s repository . For example a strategy is stored. The strategy instructs the worker system regarding how to determine which candidate tree node to explore next.

In step K pairs are initialized. Each pair includes one seed job and one exploration strategy. One pair is initialized for each strategy that is part of the strategy portfolio.

In step in response to a worker system joining the cluster that worker system is assigned a strategy and appropriate instructions are sent to the worker system. If that worker system is the first worker system to be assigned a particular strategy then the appropriate seed job is also sent to the worker system.

In step workload information is maintained for each strategy independently e.g. using the multiple worker statistics repositories .

In step work progress information is maintained globally e.g. using the global work progress repository .

In step in response to a first worker system A running out of work a work transfer request is sent that instructs a second worker system B which is executing the same strategy as the first worker system A to send some of its work to the first worker system A.

In step an initial contact is sent to the load balancing system . An exploration strategy is then received. If that strategy is being assigned for the first time then a seed job is also received.

In step exploration states are received from and or sent to worker systems e.g. in response to job transfer requests received from the load balancing system .

Status updates are sent to the load balancing system not shown . The status updates contain information regarding workload e.g. length of job queue and work progress specific to the configured strategy such as a bit vector representing code coverage .

Note that the load balancing system partitions a cluster of worker systems into multiple independent teams and assigns a different strategy to each team. Also an individual worker system is not aware that it is running within a portfolio of strategies as part of a team. So the only aspect that changes from the perspective of a worker system is that the worker system no longer executes a pre determined strategy. Instead the worker system executes a strategy indicated by the load balancing system when the worker system first joins the cluster and sends an initial contact to the load balancing system.

Note also that the work progress is maintained globally to combine the effects of each strategy and thus obtain the sum better than individual parts effect of the portfolio. For instance one can execute different code coverage strategies on different worker systems and let the load balancing system inform each worker system about the overall code coverage progress. This enables the worker systems to optimize their searches for the remaining uncovered code.

The role of the search strategies is to find the best candidate tree node to explore among a potentially large pool of states. When a state executes it may fork at a branching condition thus adding one more elements to the pool. In practice forked states tend to share similar traits for instance the position in the target program and then in a cascading effect some traits end up dominating the entire pool of states although the state with less dominant traits would have also had an important contribution to the exploration.

For example assume that the trait is the state s target program location. In this case some points in the program such as loops may cause significantly more forks than in the rest of the code. The states inside the loop will keep forking and they will eventually dominate the entire pool of states. The progress of the states in other parts of the code is slowed down since those states are selected less frequently.

Strategy portfolios introduce the idea of diversification in the state search space. The performance of the portfolio lies in the choice of its member strategies the more orthogonal to each other the strategies are the more effective the portfolio is. However as more strategy members are added it becomes increasingly difficult to find a strategy orthogonal to all the others e.g. that would explore a different portion of the symbolic execution tree . This causes redundancy in the exploration since the state queues of each member strategy have a higher degree of overlapping.

In one embodiment the set of available program states is partitioned into multiple classes. States are selected from each class at the same rate e.g. by picking classes in a round robin fashion in a sequential symbolic execution engine . Within each class states are selected according to an underlying strategy. There can be a single strategy instantiated for each class or different strategies for each class. This approach is referred to as class uniform path analysis CUPA . CUPA dynamically groups program paths into classes e.g. based on a developer specified criterion and balances analysis resources uniformly across the classes thus ensuring that all classes of paths in the target program get a fair chance to be explored.

CUPA can be viewed as a heuristic for choosing the paths to explore when the resources e.g. developer time CPU and memory are limited compared to the size of the search space. CUPA works orthogonally to an exploration search strategy by splitting the search space into partitions and ensuring that each partition gets the same share of computational resources. By encasing the explosion of states inside a partition the other states can progress faster to the exploration goal.

The effects of path explosion are highly non uniform. When applying symbolic execution to real world target programs it is often the case that path explosion originates primarily from a small set of target program statements referred to as hot spots. Hot spots are for instance loop headers whose exit conditions depend on symbolic input. Those statements cause the states executing them to fork significantly more often than in the other portions of the code. In effect the code around that statement is explored over and over again taking time that could be otherwise used to explore other regions of the program.

When the goal is not to exhaustive explore all paths the allocation of CPU resources to states becomes important. A naive uniform allocation of computation resources to symbolic states may lead to starvation. For instance when the goal is to achieve line coverage the fork points that generate most of the states will dominate the exploration search space. This will prevent states that fork in other points of the program from progressing.

We propose instead to partition the exploration search space according to a criterion relevant for the testing goal. In one embodiment CUPA is used to optimize the search for code coverage and states are partitioned according to their fork point in the target program. The exploration process selects one state from each partition in a round robin fashion thus ensuring that each partition gets roughly the same amount of resources. is a conceptual diagram illustrating effects of partitioning a set of states based on the region they belong to in a target program. In the arrows represent the progress rate of the states. A path explosion path bomb in worker Wlocally slows down the progress of each state. However the regions belonging to Wand Ware unaffected and states advance faster.

Within each partition another strategy is used to select the next state to explore. It may be any strategy that would also be used in a non partitioned scenario such as randomly selecting the next state or weighting state selection according to the estimated proximity to uncovered code.

Within a partition states can appear and disappear for multiple reasons. When a new state forks it is first assigned to a corresponding partition. When a state terminates it is removed from the partition. A state can also transition from one partition to another as it traverses and forks through the target program.

The net result of applying this partitioning scheme is an increase in the fluidity of state exploration which in turn helps achieve the testing goal faster. The strategy favors the case where there are states advancing in all program regions. The decision of which states to select next within each region is orthogonal to the partitioning strategy and is left to the lower level state selection heuristics.

CUPA scales well in a parallel environment. In one embodiment each worker system reports to a load balancing system the total number of states per partition instead of the total number of states on the worker system . The load balancing system then aggregates this information and computes the per partition distribution of states across worker systems . Then for each partition the load balancing system performs load balancing by issuing work transfer requests for states belonging to that partition. The resulting two layered architecture is illustrated in . is a conceptual diagram illustrating a layered organization of search heuristics in the presence of CUPA partitioning. In the states under their control are represented by circles. The heuristics may share global information through the load balancing system the gray arrows .

In effect the load balancing algorithm ensures that on each partition the states are evenly distributed across worker systems . Since each worker system gives equal weight to each partition the average CPU resources allocated in the cluster for each partition is uniform.

1. As program states evolve in the system they can move from one partition to the other. The strategies of each class are informed on the departure arrival of a state in order to be excluded included in the selection process.

2. A particularly useful partitioning criterion is the location in the target program of the last point a state branched. The rationale is that this partitioning attaches each state to one of the program points where execution states branch. In practice a few of such branching points are the source of most of the states. They are for instance loop headers depending on symbolic input. These branching points referred to as hot spots are a major cause of state explosion. By containing such points in classes and employing CUPA such hot spots are prevented from polluting the selection space.

3. CUPA can be generalized to multiple partitioning schemes executing simultaneously. They can be member strategies in a portfolio or applied in a round robin fashion on the same queue of states.

4. The CUPA approach is orthogonal to the strategy portfolio approach since it offers an alternative way of diversification. For example one can execute independent CUPA enabled strategies as part of a larger portfolio and even mixed with other strategies as well .

5. There is a duality between the CUPA approach and the strategy portfolio approach regarding how workload is split among worker systems . A strategy portfolio partitions the worker systems in multiple teams and load balancing is performed for each team independently. CUPA partitions states into multiple classes and load balancing is performed for each class independently across all worker systems .

When the CUPA approach is used different information is stored in the load balancing system s repository . For example rather than storing only one worker statistics repository multiple worker statistics repositories are stored one for each program state partition class containing information regarding the worker systems executing that program state partition .

Additional information is stored in the worker system s repository . For example program state partition information is stored. The worker system chooses program states e.g. candidate tree nodes from among various partitions. The program state partition information depends on the worker system s configured classification and stores for each partition one or more candidate tree nodes assigned to that partition. The worker system s processing server includes an additional module referred to as a partition module. The partition module keeps the program state partition information up to date as stored in the repository and determines which partition to select next. After the partition is selected a state e.g. candidate tree node is selected from that partition using the candidate node selection module . 

In step worker systems workloads are balanced on a per partition basis. For example in response to a first worker system A s partition becoming empty e.g. that partition contains zero states a work transfer request is sent that instructs a second worker system B which is executing the same partition as the first worker system A to send some of its work to the first worker system A.

In step an initial contact is sent to the load balancing system . A classification is then received. If this contact is the first time that the load balancing system has been contacted by any worker system since the load balancing system started executing then the load balancing system sends an initial seed job to the worker system and the worker system receives the seed job.

In step program states are partitioned into disjoint sets partitions according to the received classification. This program state partition information is stored.

In step a next state to explore e.g. a candidate tree node is selected from the partition that was selected in step .

In step the program state partition information is updated as necessary e.g. to add new states or change the classifications of existing states .

In step program states are received from and or sent to worker systems on a per partition basis e.g. in response to job transfer requests received from the load balancing system .

Exploration is performed according to the configured classification not shown . Status updates are sent to the load balancing system not shown . The status updates contain information regarding workload e.g. length of job queue on a per partition basis.

The above description is included to illustrate the operation of certain embodiments and is not meant to limit the scope of the invention. The scope of the invention is to be limited only by the following claims. From the above discussion many variations will be apparent to one skilled in the relevant art that would yet be encompassed by the spirit and scope of the invention.

