---

title: Middlebox reliability
abstract: The discussion relates to middlebox reliability. One example can apply event filters to a dataset of middlebox error reports to separate redundant middlebox error reports from a remainder of the middlebox error reports of the dataset. The example can categorize the remainder of the middlebox error reports of the dataset by middlebox device type. The example can also generate a graphical user interface that conveys past reliability and predicted future reliability for an individual model of an individual middlebox device type.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09262253&OS=09262253&RS=09262253
owner: Microsoft Technology Licensing, LLC
number: 09262253
owner_city: Redmond
owner_country: US
publication_date: 20120628
---
Datacenters deploy a variety of intermediary devices or middleboxes such as load balancers firewalls intrusion detection and prevention systems IDPS and virtual private networks VPNs . These devices provide broad range functionalities to secure manage and improve the performance of hosted services as well as support new traffic classes and applications. Even in enterprise networks the number of middleboxes is becoming comparable to the number of routers. The middleboxes constitute a significant fraction of the network capital costs and operational expenses e.g. human experts to manage them.

The patent relates to middlebox reliability One example can apply event filters to a dataset of middlebox error reports to separate redundant middlebox error reports from a remainder of the middlebox error reports of the dataset. The example can categorize the remainder of the middlebox error reports of the dataset by middlebox device type. The example can also generate a graphical user interface that conveys past reliability and predicted future reliability for an individual model of an individual middlebox device type.

The above listed example is intended to provide a quick reference to aid the reader and is not intended to define the scope of the concepts described herein.

This discussion relates to understanding middlebox reliability. The information from the present techniques can relate to reliability of different types of middleboxes such as failures e.g. failure events caused by load balancers compared to failures caused by firewalls. Further this information can relate to models of devices within a specific type of middlebox. For instance the techniques can provide the relative failure rate of a model a load balancer versus a model b load balancer. Further still these techniques can predict a useful lifespan of a particular model such that replacement plans can be considered in advance as well as cost vs. benefit decision making for spare inventory management. As used herein the term reliability can be defined as the probability that an item will perform its intended function without failure for a specified interval under stated conditions. Failure can be defined as the inability of an item to function within its specified limits of performance.

Some of the present techniques can leverage datacenter middlebox error reports hereinafter error reports . Error reports relate to failure events. These error reports tend to include high percentages of spurious and or duplicate error reports that obscure any understanding of the actual middlebox reliability. The present techniques can filter the error reports to identify the more germane or valuable error reports e.g. those error reports which contain useful information . These filtered error reports can be correlated with other middlebox related data to specific types of devices and or to models of an individual device. The correlated error reports can be used to evaluate and or predict middlebox reliability in various ways some of which are described below.

A monitoring system can monitor reliability of the datacenter network architecture as a whole and or of individual components. An event analysis component can utilize information obtained by the monitoring system to provide useful reliability information relating to datacenter network architecture and or the individual components.

From a logical standpoint the system can be organized into a hierarchy that includes a core layer an L3 aggregation layer and a L2 aggregation layer . This logical organization can be based on the functional separation of Layer 2 trunking VLANs etc. and Layer 3 routing responsibilities.

ToR switches also known as host switches connect the servers to a remainder of the system via a network represented by connecting lines in . Host ports in these ToR switches are often 10 100 1000 Ethernet with the uplinks being Gigabit Ethernet or 10 GE ports. The ToRs can be connected upstream to a pair of aggregation switches that form a redundancy group e.g. the group contains multiple members and individual members can perform the switching function in the event that the other member fails . These aggregation switches can serve as an aggregation point for Layer 2 traffic and typically support high speed technologies such as 10 Gigabit Ethernet to carry large amounts of traffic e.g. data .

Traffic from aggregation switches can be forwarded to the access routers which can be deployed in pairs for redundancy. Access routers can use Virtual Routing and Forwarding VRF to create a virtual Layer 3 environment for each tenant. A tenant is a service or application hosted on servers which uses network devices for connectivity route traffic from to users or other services to from its hosted servers. The access routers can aggregate traffic from up to several thousand servers and route the traffic to core routers that can connect to the rest of the system and Internet .

Load balancers can improve the performance of hosted applications. Redundant pairs of load balancers can connect to each aggregation switch and perform mapping between static IP addresses exposed to clients through DNS and dynamic IP addresses of the servers that process user requests. Load balancers can support different functionalities such as NAT SSL acceleration cookie management and data caching.

Firewalls can protect applications from unwanted traffic e.g. DoS attacks by examining packet fields at IP transport and sometimes even at the application layer against a set of defined rules. While software based firewalls can be attractive to quickly implement new features hardware based firewalls are typically used in datacenters to provide performance critical features.

VPNs can augment datacenter network infrastructure by providing switching optimization and security for web and client server applications. The VPNs can provide secure remote access via SSL VPN.

There can be other middleboxes such as NATs WAN optimizers proxies and media converters among others. A media converter device can perform on the fly conversion of application level data transcoding of existing web pages for display on hand held wireless devices and transcoding between various audio formats for interconnecting mobile phones with VoIP services.

The monitoring system can accept event log streams from syslog for example and perform functions such as reformatting filtering data based on rules and routing messages to any installed rule engines or archival log files. These logs can contain information about what type of network component experienced an event the event type the other end point of this component and a short machine generated description of the event.

Network operators may troubleshoot network faults through problem tracking systems or ticketing systems that coordinate among network engineers working on the problem. Some troubleshooting systems are built around the NOC RFC. In such a case a unique identifier herein referred to as NOC TicketID is assigned to each failure event. These tickets contain structured information about when and how an event was discovered and diaries of steps taken by the engineers in troubleshooting and mitigating the problem.

Operators can use a maintenance tracking and revision control system to track activities such as device provisioning configuration changes and or software upgrades throughout the system . The maintenance tracking and revision control system can be features of the monitoring system or a separate system. Before debugging an outage a network engineer can check the maintenance tracking system for on going and planned maintenance. The network engineer can use the revision control system to detect any recent changes to the device configuration files. Network traffic carried on network interfaces links can be logged using SNMP polling that averages traffic seen every five minutes for example. Other sources of traffic information can be obtained from sampling based approaches such as sFlow. Traffic monitoring systems can use the MIB format to store the data that includes fields such as the interface type token ring Ethernet etc. the other end of the interface the interface status up down and or the number of bytes sent or received by the interface among others.

Event analysis component can utilize event logs obtained by the monitoring system to evaluate system and or component reliability. For instance additionally or alternatively to the event logs obtained from the monitoring system the event analysis component can utilize data collected by network operators. For example network operators can detect faults from network devices and analyze root causes by using monitoring alarms such as syslog and SNMP traps and by monitoring device status via ping and SNMP polling. The event analysis component can obtain any other middlebox related data to use in analyzing middlebox reliability. Non limiting examples of such data can include traffic data configuration data maintenance data prior failure history data and or device meta data.

Event analysis component can correlate the above mentioned data with failure events to extract failures impacting network traffic and to reverse engineer the topology information using link level connectivity as the topology changes from time to time. As used herein a failure can be thought of as an event that causes a device or a link to be unavailable to perform its intended task. Specifically a link failure can be thought of as occurring when the connection between two devices is down. Similarly a device failure can be thought of as occurring when the device is not functioning for routing forwarding traffic.

Some implementations of event analysis component can filter several types of spurious network event logs such as inaccurate event logs duplicate event logs caused by multiple devices reporting the same event single events being recorded as multiple events and shadow reports e.g. chatty events . In regard to inaccurate event logs syslog messages can be spurious with devices sending multiple notifications even though a device is operational. In regards to multiple reporting devices two or more devices e.g. neighbors may send notifications for the same event leading to redundant event logs e.g. multiple redundant middlebox error reports . The error reports can be thought of as redundant if subsequent error reports relate to an error that was reported by an earlier error report. Regarding a single event being recorded as multiple events a flapping device can generate multiple down and up messages which get logged as different events.

Shadow events can be thought of as events being triggered due to devices which are being scheduled for replacement or have been detected as faulty by operators but which are awaiting repairs. In some cases this effect can be severe with some devices e.g. chatty or shadow devices sending more than a thousand device down notifications over a few hours because the notification system did not suppress them during the troubleshooting window. Techniques that the event analysis component can employ to filter several types of spurious network event logs are described below relative to .

To summarize the event analysis component can characterize the reliability across different types of middleboxes and across different series of devices of a given type or those belonging to the same service or application or by data center or any other logical or physical grouping by applying event filters to de noise monitored datasets.

The event analysis component can also correlate the filtered data across multiple dimensions and perform inference analysis. In some cases the correlation can be performed solely on the filtered event logs. For instance such correlation can identify a pattern of failures from the filtered event logs. One such example can compare a number of failure events per device of a given device type to identify individual devices with a high failure rate. Another example can compare downtimes per failure event among different devices of a device type.

In other cases the correlation can be performed on the filtered event logs and other middlebox related data from other sources. As mentioned above examples of such other middlebox related data can include traffic data configuration data maintenance data prior failure history data and or device meta data. Correlating the filtered event logs with other middlebox related data from other sources can enhance the value of the data for reliability determination. The latter case can provide highly reliable data upon which to perform the inference analysis.

In this case computer can include an application layer an operating system layer and a hardware layer . The event analysis component can be manifest as a program or application of the application layer among other configurations. In this example the event analysis component can include a middlebox error report filter module hereinafter filter module a middlebox error report correlation module hereinafter correlation module and a middlebox recommendation module hereinafter recommendation module .

The hardware layer can include a processor storage memory e.g. computer readable storage media medium a display device and or various other elements. For instance the other elements can include input output devices optical disc readers USB ports etc.

Processor can execute data in the form of computer readable instructions to provide a functionality such as an event analysis component functionality. Data such as computer readable instructions can be stored on storage memory and or received from another source such as optical storage device . The storage memory can include any one or more of volatile or non volatile memory hard drives flash storage devices e.g. memory sticks or memory cards and or optical storage devices e.g. CDs DVDs etc. among others. As used herein the term data can include computer readable instructions e.g. program code and or data that is received by a program e.g. user input . The computer may also receive data in the form of computer readable instructions over network that is then stored on the computer and or on a storage device that is communicatively coupled with the computer for execution by its processor. For example the network can connect computer to monitoring system and or a database .

Alternatively to the illustrated configuration of computer the computer can employ a system on a chip SOC type design. In such a case functionality provided by the computer can be integrated on a single SOC or multiple coupled SOCs. For instance the computer can include shared resources and dedicated resources. An interface s can facilitate communication between the shared resources and the dedicated resources. As the name implies dedicated resources can be thought of as including individual portions that are dedicated to achieving specific functionalities. Shared resources can be storage processing units etc. that can be used by multiple functionalities.

Generally any of the functions described herein can be implemented using software firmware hardware e.g. fixed logic circuitry manual processing or a combination of these implementations. The term engine tool component or module as used herein generally represent software firmware hardware whole devices or networks or a combination thereof. In the case of a software implementation for instance these may represent program code that performs specified tasks when executed on a processor e.g. CPU or CPUs . The program code can be stored in one or more computer readable storage memory devices such as computer readable storage media. The features and techniques of the component are platform independent meaning that they may be implemented on a variety of commercial computing platforms having a variety of processing configurations.

As used herein the term computer readable media and computer readable medium can include transitory and non transitory instructions. In contrast the terms computer readable storage media and computer readable storage medium and computer readable storage medium exclude transitory instances and or signals. Computer readable storage media can include computer readable storage devices . Examples of computer readable storage devices include volatile storage media such as RAM and non volatile storage media such as hard drives optical discs and flash memory among others.

In some implementations the filter module can be configured to perform functionality relating to separating duplicate error reports from a remainder of the error reports in the dataset. The correlation module can be configured to perform functionality relating to correlating individual error reports such as by time of occurrence date of occurrence duration of occurrence physical location type property configuration setup and or functional role of involved middleboxes. The recommendation module can be configured to perform functionality relating to predicting future reliability of individual middlebox device models. The recommendation module can also be configured to perform functionality relating to recommending a replacement date for individual middlebox device models.

Stated another way the event analysis component can achieve event filtering denoising correlation within the filtered error report data and or across different data sources cost vs. benefit analysis reliability comparison decision making predicting failures trend analysis etc. These aspects are described in more detail below by way of example.

In this case the event analysis component can generate the GUI screenshot by obtaining error reports from monitoring system and or database . The database can include error reports from one or more datacenters. Error reports obtained from or relating to one or more datacenters can be thought of as a dataset that is evaluated by the event analysis component .

The event analysis component can separate individual error reports relating to middlebox devices and links connecting these devices from those of other datacenter devices. The event analysis component can also identify error reports relating to high priority events from a remainder of the error reports. In some implementations the event analysis component can employ a systematic methodology that uses spatio temporal panoramas of device failures and applies trend analysis to characterize middlebox reliability in datacenters. These aspects are described in more detail below.

To summarize several features that can be offered by the event analysis component are described above and below. These features can include characterizing middlebox reliability from a datacenter site up vs. cost down perspective. Another feature can involve characterizing middlebox reliability by comparing middlebox device type and models within an individual device type. A further feature can involve filtering shadow devices which log a significant number of failures even after their fault has been detected. Still another feature can apply a pipeline of event filters across network datasets to extract meaningful failures from data. A further feature can involve evaluating the effectiveness of middlebox redundancy when a redundancy group has more than two components. Still another feature can involve applying spatial panoramas and trend analysis to identify the most failure prone network elements such as the more or most failure prone middlebox device types and or models within a device type. These features are described more below relative to .

As mentioned above the event analysis component can balance site up vs. cost down considerations. Site up can relate to an availability of services provided by the datacenter. Stated another way what percent of the time are the services available such as may be defined in a service level agreement. Cost down can include all costs associated with providing the services such as datacenter capital costs software maintenance and management costs among others. For example one goal can be to increase and potentially maximize service availability while keeping costs down. For instance keeping a large inventory of spare devices can ensure that failed devices can be quickly replaced to minimize downtime. However this incurs significant costs and spares themselves might be faulty or become obsolete over time. To balance this tradeoff event analysis component can identify highly failure prone and potentially the most failure prone middleboxes for replacement that exhibit high downtime or a high number of failures.

The event analysis component can also indicate sufficient numbers of spares to maintain as backup e.g. recommended number of spares . The event analysis component can also provide information that can aid in understanding the root causes of middlebox failures. This knowledge can be useful for network troubleshooting. Further the event analysis component can analyze the effectiveness of middlebox redundancy in masking failures. The information displayed in can be utilized by the event analysis component when balancing these considerations.

The event analysis component can utilize this information to compare device reliability across load balancer generations. Both hardware and software improvements during a study period can also be captured by the event analysis component .

The event analysis component can identify failure root causes. In the test case the main root causes of load balancer failures due to hardware are faulty power supply units PSUs cabling ASIC and memory problems those due to software are reboot loops software version incompatibility between active and standby devices port flapping maintenance configuration e.g. authentication issues VIP reallocation RFC Implementations and the remaining are unknown problems. For firewalls the root causes were mainly mis configurations software bugs and network card failures. VPN failures were attributed to errors in VLAN reallocation and SNAT port exhaustion. Of course beyond the illustrated example the event analysis component can show root cause type and its contribution to number of failures and downtime of a middlebox device type in any manner that provides useful information.

The event analysis component can determine the effectiveness of middlebox redundancy. For instance the event analysis component can identify the redundancy between load balancers and aggregation switches in reducing the impact of failure on network traffic measured in terms of lost bytes and the percent of the events which experience zero impact. For example in one instance two potentially key root causes of this ineffectiveness could be software version inconsistencies between primary and backup devices and configuration bugs in the failover mechanism.

The event analysis component can identify individual middlebox devices that cause inordinately high amounts of failures. For instance a few devices contributed to a significant number of failures in one device family. Further a new load balancer generation exhibited a higher relative failure rate than its previous generation. In this case the event analysis component determined that one possible cause of the higher failure rate was due to software bugs. The event analysis component also identified a family of load balancers having faulty power supply units PSUs which could have been detected as early problem indicators by aggregating failure root causes across that type. Without the event analysis component the faulty PSUs were repaired in isolation without recognition of the scope of the problem. The event analysis component can identify faulty devices exhibiting unexpected reboots which led to the defective device being sent back to the vendor. The event analysis component can then determine whether the applied fix was effective or whether another eventually solved the problem.

The event analysis component can identify early problem symptoms to enable proactive repair and maintenance of a model of a middlebox device type. Further as evidenced the event analysis component can identify whether a new product line is more reliable than its predecessor. This analysis can further be used to compute a cost of ownership metric. The event analysis component can utilize this information to determine whether it is cost effective to continue with a faulty product line by purchasing additional spares or to replace it the product line at a point in time see description related to . Finally this information can be useful feedback to manufacturers vendors in fixing existing problems and adding new features in future generations.

The following discussion offers further details regarding load balancer reliability. Failing load balancers whose links were not carrying any traffic before the failure can be separated into two categories i inactive no traffic before or during the failure and ii provisioning no traffic before the failure but some traffic during the failure . When links are idle and functioning the links tend to exchange small amounts of control traffic such as 30 bytes sec. Therefore accurate link filtering can utilize 30 bytes sec as a threshold. Thus those load balancers above 30 bytes sec are considered to be carrying traffic.

The event analysis component can create a panorama of how load balancer failures are distributed across a measurement period ordered by datacenters and applications to which they belong. The panorama can represent failure over unit time such as per day. In one configuration the panorama can represent widespread failures as vertical bands and long lived failures can be represented as horizontal bands. Thus event analysis component can recognize vertical bands as failures that are spatially distributed across multiple devices around the same time. Notable reasons for this can include planned software upgrades RFCs. In such a case the device software is being upgraded to a newer version or all devices across multiple datacenters are being upgraded.

The event analysis component can also recognize unplanned inconsistencies. For instance occasionally after a device replacement there is a software mismatch between the new device and the old standby. For example if any control protocols undergo a change as a result of a software upgrade without a proper backward compatibility upgrade as opposed to update then any bootstrapping could subsequently fail. In this scenario all devices involved in the group tend to be taken down for a software upgrade.

The event analysis component can also recognize effect propagation where failure of devices higher up in the topology trigger alerts connected devices in the underlying sub tree. For instance when an access router fails it tends to trigger alerts in all firewall devices connected to it.

The event analysis component can also recognize long lived failures as horizontal bands in the panorama. These horizontal bands can indicate device failures on the same device across time. The event analysis component can detect these as early warnings of impending failures. This scenario can be common when a device is about to fail. For instance one suspected device may have SDRAM ECC errors and try to recover thus triggering multiple failure events but keep being power cycled as a quick fix.

The event analysis component can also recognize standby failover error. For example this can occur when a standby device is not notified of its peer being replaced after a failover. For instance if the standby device is not aware that the active device is being replaced then the standby device will tend to log multiple down events indicating that the active device is down when in fact the active device is physically absent.

The event analysis component can also recognize faux devices. For example an individual middlebox device might be monitored while the middlebox device is not connected to the operational network. In this scenario if the middlebox device is switched over to active it tends to log multiple events indicating that its links are down.

The event analysis component can compare the reliability of load balancers across device generations vendors e.g. across manufacturers models etc. using the probability of failure metric among other techniques. In one implementation this metric is computed by dividing the number of middlebox devices of a given type that observe failure by the total population of that type.

In the illustrated upper graph the failure rate of the five load balancer models tends to vary significantly. In terms of overall failure probability load balancers LB LB and LB are the least reliable with about a one third chance of experiencing a failure. The failure rate increased from an older generation LB to a newer LB indicating potential defects in the newer generation. Lower graph shows the fraction of failures and downtime contributed by each model. Observe that LB exhibits a large failure rate as well as contributing significantly to the total number of failures and downtime. However note that in one comparison to other load balancers the fraction of failures contributed can be higher than the downtime indicating that most problems are short term failures. Or conversely the event analysis component can identify when a particular model is experiencing more long term failures than other models. The event analysis component can validate this observation by using time to repair plots where a short time to repair indicates transient problems. In this instance a majority of these problems were due to link flapping. The event analysis component can also determine the percentage of downtime and failure per device for each model.

In some implementations the time to repair for a middlebox device can be defined as the time between a down notification for a device and when it is next reported as being back online. There are two types of failures short lived transient failures where the network engineer may not always intervene to resolve the failure and long term failures where the middlebox device or its component is usually replaced or decommissioned. Note that for long term failures the failure durations may be skewed by when the network troubleshooting tickets were closed by network operators. For instance support tickets usually stay open until the middlebox device in question is replaced with a spare and this duration can be in the order of days.

The event analysis component can also calculate the time to repair for load balancers. Short lived failures may be attributed to software problems e.g. OS watchdog timer triggering a reboot . Comparing across load balancers the time to repair for the generations increased for LB indicating potential problems with troubleshooting these devices. However for newer generation devices of LB the time to repair is relatively lower indicating that newer generations get fixed quickly.

The event analysis component can calculate annualized downtime for middleboxes. The availability of a device can be estimated using the annualized downtime metric i.e. the degree to which a device or network component is operational and accessible when required for use during the period of observation.

Time to failure of a device can be defined as the time elapsed between two consecutive failures. Since this metric requires that a device fail at least twice devices having only zero or one failures during a measurement period can be excluded. In the analyzed dataset load balancers tend to experience a number of short lived transient failures i.e. irrespective of their device generation a majority of them fail within a relatively short amount of time such as one or two days. However specific percentages of failures such as the 95th percentile may be weeks or months between failures. This indicates that even among devices that fail multiple times there tends to be two types of devices robust ones that failed once in one or two months and failure prone devices that experience numerous failures mostly within 24 hours . The event analysis component can identify notable causes of these failures examples of which are now described. One notable cause of frequent failures can be referred to as link flapping Sync Problems . Link flapping relates to unexpected cases where standby devices are unable to sync with active devices and hence raise alerts indicating link failures. Another frequent cause of failures is neighbor maintenance. Neighbor maintenance refers to devices that are power recycled updated as part of routine maintenance so when a device goes down its neighbors start sending DOWN notifications. Reboot loops are another frequent cause of failures. Devices get stuck in a reboot loop where the devices are either hard power recycled or sent back for replacement. A further frequent cause of failures is field test failures. Field test failures result when network engineers routinely perform EUD End User Diagnostics tests to ensure that the device health is at acceptable levels. During this procedure devices are restarted or tested multiple times.

The next discussion point relates to failure trends across different middlebox device types. The event analysis component can create timeline trends for the number of failure events and the number of devices that failed. In one implementation subplots can show two curves a cumulative curve that sums up the number of failures across time and another curve to show the actual number of failures. In some cases the two curves are similar for both the devices and failure events. Such an occurrence can indicate that the number of devices that fail is on par with the number of failures.

In other instances the number of failure events is far higher than the number of devices that failed indicating that a few bad devices are responsible for a majority of failures. Further a sudden increase in the slope can indicate major problems during that period of observation.

The event analysis component can further identify the major root causes observed for each load balancer type. In the explored dataset unexpected reboots are a common root cause across all device types due to software bugs or faulty hardware components. For reboots due to software problems the main reasons were fault in the switch card control processor licensing bugs and incorrect value setting for the OS watchdog timer. Hardware related reboots were caused due to cache parity errors power disruption hard disk failures faulty EEPROM or dual in line memory modules and or failed network cards. Overall across both planned and unexpected outages hardware problems contributed relatively fewer problems e.g. faulty PSUs cabling problems ASIC and memory problems software contributed relatively more problems e.g. reboot loops active standby version incompatibility and port flapping maintenance configuration contributed relatively few problems e.g. authentication issues VIP reallocation RFC Implementations and the remaining problems were attributed as unknown root causes.

The event analysis component can estimate the Impact of failures. In some cases quantifying the impact of a failure is difficult as it can require attributing discrete outage levels to annotations used by network operators such as severe mild or some impact. To circumvent this problem the event analysis component can correlate the network event logs with link level traffic measurements to estimate impact of a failure event. However it can be difficult to precisely quantify how much data was actually lost during a failure because of two complications i presence of alternate routes in datacenters and ii temporal variations in traffic patterns. In one implementation the event analysis component can estimate the failure impact in terms of lost network traffic that would have been routed on a failed link in the absence of the failure. Specifically the event analysis component can first compute the median number of bytes on the links connected to a failed device in the hours preceding the failure median before and the median bytes during the failure. Then the event analysis component can estimate the amount of data that was potentially lost during the failure event as loss medianbefore medianduring failure duration 1 where duration denotes how long the failure lasted.

The event analysis component can evaluate the effectiveness of network redundancy in mitigating failure impact as it is a de facto technique for fault tolerance in datacenters. Within a redundant group one device is usually designated the primary and the rest as backups. However note that some devices may belong to multiple redundancy groups.

The event analysis component can observe large redundancy groups of up to 12 load balancers connected to a single aggregation switch using link level connectivity. The event analysis component can estimate the effectiveness of redundancy by computing the ratio of median traffic bytes entering a device across all links during a failure and the median traffic entering the device before the failure. The event analysis component can then compute this ratio across all devices in the redundancy group where the failure occurred. Network redundancy is considered 100 effective if this ratio is close to one across a redundancy group. This ratio can be thought of as normalized traffic.

The event analysis component can create a graph to show the distribution of normalized traffic for individual devices and redundancy groups. Several reasons can contribute to why redundancy is not 100 effective in masking the failure impact. A first reason can relate to faulty failovers. In some cases the primary device failed when the backup was experiencing an unrelated problem and hence led to a failed failover. In other cases software protocol bugs prevented the failover from happening correctly often sending devices into a deadlock state. Software mismatch can be another source of faulty failover. In such cases the OS version on the backup device may be different from the primary device leading to differences in the failover protocol. In these cases the network engineer can perform a break fix maintenance to bring both devices to the same software version. A further source is mis configurations or cases where the same configuration error was made on both primary and backup devices.

The event analysis component can analyze the reliability across different types of firewalls and VPN devices which show a higher degree of robustness compared to load balancers. The event analysis component can use the metrics introduced relative to load balancers to compare device reliability across different generations of a model and vendors.

For instance for a given dataset firewall related failures may tend to occur relatively infrequently or more frequently compared to load balancers.

To gain insight into the kind of failures affecting middleboxes notably load balancers due to their large footprint the event analysis component can analyze the network tickets associated with each individual series to develop a more thorough understanding of the problems facing datacenters. The results can be studied with network operators.

As noted above the event analysis component can identify problems grouped by a device type. In some implementations the event analysis component can use the grouped problems to determine early failure indicators of problems with the entire set of devices in that type. In one such example although the operations team had been proactively repairing faulty power supply units PSUs associated with load balancers most repairs replacements were performed in isolation. The analysis from the event analysis component revealed load balancers of type LB to have a relatively high annualized failure rate. Analyzing network tickets associated with this series indicated that i devices were failing due to PSU problems even before the repairs started ii out of all network tickets related to LB a significant portion of them required that the device be replaced. This is somewhat surprising while most devices have extended warranty a device replacement risks operating at reduced or no redundancy until the replacement arrives while keeping a large number of spares incurs significant costs.

In most cases where a device was sent back to the manufacturer for a post mortem their analysis did not prove to yield any useful results. This is likely due to the fact that most problems are believed to be isolated and unless needed troubleshooting does not involve conducting specialized tests that uncover problem patterns. In comparison the present approach of aggregating root causes across a device generation helped identify problem patterns. In this case failing PSUs could have been used as early indicators of problems with the product line. A cost of ownership analysis can then be useful to decide whether to continue using a device model or gracefully upgrade it to newer models.

Another issue that was examined in further detail involved LB load balancers where problematic devices were rebooting unexpectedly. The ticket diaries indicated that i the manufacturer was not able to find the root cause and ii in a large percentage of the problematic cases the defective unit was sent back to the manufacturer for a replacement. The subsequent product line upgrades from the manufacturer s website revealed this to be a software bug related to a watchdog timeout. This timeout got triggered in response to a high volume of network traffic on the management network. In this case the newer generation LB exhibited lower reliability than its predecessor LB . Ideally this bug should have been fixed with a software patch but instead it led to wastage of several weeks of troubleshooting man hours. One way to address this issue with third party middlebox vendors is to perform rigorous functional verification prior to deployment.

Another issue that was examined in further detail related to the LB series. The event analysis component s trend analysis and reliability and downtime estimates revealed that LB suffered from the few bad apples effect i.e. the rate of increase of the slope of the cumulative count of failures curve is higher than the rate of increase of the slope of the cumulative count of devices curve. This indicates that only a few devices are contributing to a majority of failures and can be confirmed by high COV value for LB .

Trend analysis is a useful tool in uncovering these types of problems where a specific set of devices are more failure prone than the rest of the population. There are many reasons why this happens. For instance a fault with the regulated PSU will frequently cause power fluctuations leading to damaged parts. Even if the damaged parts are replaced unless the fault is localized to the power supply unit for example the problem will repeat itself.

Another issue that was examined in further detail related to failover bugs due to version mismatches. This particular case study concerns LB and LB where devices were sent back to the manufacturer for replacement. A significant percentage of the events led to another simultaneous failure event being generated the failover mechanism had failed. Subsequent troubleshooting revealed the root cause as an outdated software version on the standby device leading to failure of the new failover mechanism. Of course this example is intended to represent occurrences in the test data and may not reoccur in other datacenter scenarios.

The event analysis component can consider network redundancy at both the device level and the failover mechanisms. Techniques such as regular failover testing and checking for consistency of network configurations can be used to check the robustness of the failover mechanisms.

This robustness checking can provide additional information that can be useful for datacenter management decision such as replace vs. upgrade control strategies. For instance one of the examples described above involved a newer device generation which had lower reliability compared to its predecessor. More broadly it raises a key question about deciding whether to keep replacing faulty devices in existing deployments or to upgrade to a new product line. Answering this fundamental question can involve a deeper understanding of multiple challenges. For instance upgrading devices with their high speed feature rich counterparts will likely require additional training for network engineers. Thus the degree of familiarization with the upgraded device can be considered in making the decision to upgrade or not. For example an undesirable consequence of this issue is a longer time to repair for devices at least initially due to unfamiliarity.

A further factor in the upgrading equation can be active standby compatibility. For example incorporating new versions of devices into an existing network may lead to unexpected problems e.g. the compatibility problems between primary devices and the standby devices as observed in the case studies. By computing a metric such as the cost of ownership COO decisions such as whether to purchase additional spares for a faulty product line or to gracefully replace a product line can be made. The primary cost elements of the COO model can be i initial investment in purchasing the product ii depreciation iii cost of operation that includes space personnel and or power expenses iv cost of maintenance operations including downtime repair cost failure rates and or cost of spares and or v enhancement costs that include hardware software upgrades and or personnel training. Reliability analysis provided by event analysis component can attempt to answer some of these questions.

The event analysis component can also provide guidance regarding spare device management e.g. what middlebox devices to keep on site as replacements and how many . With existing ad hoc spare management it is common for network engineers to not have replacement devices on site. Delays in obtaining additional inventory can cause decreased system reliability. The event analysis component can leverage several aspects of the above mentioned analysis to determine on site inventory. Examples of two aspects that can be leveraged include i metrics such as annualized failure rates time to failure and time to repair can be used to build device reliability models and ii root cause analysis can be used to identify early problem symptoms in failing devices and to compute a requisite number of replacement parts as spares. This reliability information can be combined to balance cost vs. benefit tradeoffs of extended downtime of a device e.g. situated at a network hotspot because of delay in obtaining a spare against the holding costs of the inventory and the risk that the spares may become obsolete or fail themselves before being used.

The event analysis component can also implement failover stress testing. This can reduce and potentially avoid instances where the primary and the backup device failed simultaneously due to mis configuration of the backup device cabling errors and mismatch between software versions of the primary and backup.

The event analysis component can provide useful information and or services to organizations that own or manage datacenters as well as to vendors that design manufacture middlebox devices. In regards to vendors the event analysis component can provide robust test suites. As noted above post mortem analysis of failed devices by the vendors did not yield conclusive results in some cases. As troubleshooting can involve significant time and personnel it translates into repair costs for the vendor. Having detailed knowledge of the different type of problems observed for a particular device type will help design robust test suites that can automatically perform functional verification and model checking of newer generations of devices. This will help prevent commonly occurring hardware and software problems and meet the expectations of operational readiness quantified in terms of the probability that the equipment will be ready to start its function when called upon to do so.

The event analysis component can also provide information regarding product line life cycles. For instance the event analysis component can generate an accurate estimate of device reliability that can aid the vendor in estimating the cost of spare parts inventory i.e. for a highly reliable device the vendor can produce less spare parts. This information can be inferred from the problem tickets raised by the vendor s customers e.g. datacenter operators.

The technique can employ a timing filter at . The timing filter can be used to fix various timing inconsistencies. In one implementation the timing filter can first group events with the same start and end time originating on the same interface. This process can remove duplicate events. Next the timing filter can pick the earliest start and end times of multiple events that originated within a predefined time window on the same interface. For example any events that happened within a predefined time of 60 seconds on the same interface can be grouped into a single event e.g. characterized as a single event . This process can reduce or avoid any problems due to clock synchronization and log buffering.

Next the technique can group two events originating on the same interface that have the same start time but different end times. These events can be grouped into a single event that is assigned the earlier of the end times. The earliest end times can be utilized since events may not be marked as cleared long after their resolution. The technique can employ a planned maintenance filter at . Events caused by planned maintenance can have less value in understanding device behavior than unplanned events e.g. unexpected outages .

The technique can employ a shadow device filter at . The shadow device filter can be useful to filter events being logged by devices being scheduled for replacement or that have been detected as faulty by operators but are awaiting repairs. The shadow device filter can identify these shadow devices by arranging the devices in the descending order of their number of failures. In one implementation for a top percentage of the devices in this list all events are merged that have the same NOC TicketID field. This constitutes an event in that events with the same ticket ID are likely to have the same symptoms. In one case the top percentage is defined as the top five percent but of course other values can be employed in other implementations.

The technique can employ an impact filter at . An event can be defined as having an impact when the event affects application reliability e.g. throughput loss number of failed connections or increased latency. In implementations without access to application level logs failure impact can be estimated by leveraging network traffic data and computing the ratio of the median traffic on a failed device link during a failure and its value in the recent past. For example the value of the recent past can be set as the preceding eight hour correlation window . Other implementations can use other values. A failure has impact if this ratio is less than one. The above acts can collectively allow the filtering technique to identify the failures with impact at . Of course other filters can alternatively or additionally be utilized.

At the method can separate spurious and duplicate middlebox error reports from a filtered sub set of the middlebox error reports. In one implementation the separating can be accomplished by applying a pipeline of event filters to the set of middlebox error reports to generate the filtered sub set of the middlebox error reports. In some cases the pipeline can be created by selecting individual filters from a set of available event filters.

At the method can correlate the filtered sub set of the middlebox error reports with other middlebox related data to produce correlated data. In one case the correlation can identify failures that take a long time to fix. Further groups of failures that happen at the same time or nearly the same time can be identified. Note that in some implementations the correlation of block can be performed only on the set of middlebox error reports obtained at block . In other implementations the correlation can be performed on the set of middlebox error reports obtained at block and or upon data from other data sources. For instance an example of other data sources that can be drawn upon can include traffic data configuration data maintenance data prior failure history data and or device meta data among others. The correlated data can be especially indicative of middlebox reliability when compared to uncorrelated data. At the method can perform inference analysis on the correlated data to estimate middlebox reliability. Some implementations can further apply spatial panoramas and trend analysis to identify relatively highly failure prone middlebox device types and or models.

The order in which the methods are described is not intended to be construed as a limitation and any number of the described blocks can be combined in any order to implement the method or an alternate method. Furthermore the method can be implemented in any suitable hardware software firmware or combination thereof such that a computing device can implement the method. In one case the method is stored on one or more computer readable storage media as a set of instructions such that execution by a processor of a computing device causes the computing device to perform the method.

Although techniques methods devices systems etc. pertaining to middlebox reliability are described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described. Rather the specific features and acts are disclosed as exemplary forms of implementing the claimed methods devices systems etc.

