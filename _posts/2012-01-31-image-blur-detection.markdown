---

title: Image blur detection
abstract: Among other things, one or more techniques and/or systems are provided for quantifying blur of an image. Blur may result due to motion of a camera while the image is captured. Accordingly, motion measurement data corresponding to motion of the camera during an exposure event may be used to create a camera rotation matrix. A camera intrinsic matrix may be obtained based upon a focal length and principle point of the camera. A transformation matrix may be estimated based upon the camera rotation matrix and/or the camera intrinsic matrix. The transformation matrix may be applied to pixels within the image to determine a blur metric for the image. In this way, blur of an image may be quantified offline and/or in real-time during operation of the camera (e.g., so that the image may be re-acquired (e.g., on the fly) if the image is regarded as being overly blurry).
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08964045&OS=08964045&RS=08964045
owner: Microsoft Corporation
number: 08964045
owner_city: Redmond
owner_country: US
publication_date: 20120131
---
Many services and applications provide functionality associated with images captured by cameras. In one example an online mapping service may provide users with interactive maps derived from images captured by cameras mounted on aircrafts vehicles and or satellites. In another example a web search engine may provide users with search results comprising one or more images captured by cameras. Unfortunately many images acquired by cameras may suffer from blur. For example blur may occur due to motion of a camera while an image is captured e.g. a camera mounted to an aircraft may experience three dimensional movement due to turbulent motion experience by the aircraft during image capture . Substantial blur within an image may render the image unusable for certain applications functions.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key factors or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

Among other things one or more systems and or techniques for determining a blur metric for an image are provided herein. A camera may be associated with a device configured to measure motion of the camera. For example an inertial measurement unit IMU may be configured to measure angular motion of the camera such as camera roll camera pitch and or camera heading e.g. measurements may be captured at 200 Hz . Such information may be received as motion measurement data. The motion measurement data may be used to create a camera rotation matrix e.g. camera rotation matrix of . The camera rotation matrix may define an angular difference e.g. displacement of the camera between a shutter open event e.g. a time at which the camera starts to capture an image and a shutter close event e.g. a time at which the camera finishes capturing the image of an image captured by the camera during an exposure event. In one example the motion measurement data may be interpolated to obtain shutter open camera orientation angles e.g. pitch roll and or heading orientation angles of the camera corresponding to the time of the shutter open event and or shutter close camera orientation angles e.g. pitch roll and or heading orientation angles of the camera corresponding to the time of the shutter close event . The camera rotation matrix may be created based upon an angular difference between the shutter open camera orientation angles and the shutter close camera orientation angles. In this way the camera rotation matrix may be used to take into account motion of the camera during the exposure event of the image when determining the blur metric for the image.

A camera intrinsic matrix e.g. camera intrinsic matrix of comprising conversion information for converting between image plane coordinates e.g. x y image coordinates and object coordinates e.g. X Y Z location coordinates of an object scene depicted within the image may be obtained. The camera intrinsic matrix may be based upon a camera focal length e.g. an attribute of a camera lens associated with image magnification and or angle of view for the camera and or a principle point associated with the camera e.g. the principle point may take into account off centeredness between the center of the camera lens and the center of the image .

A transformation matrix e.g. transformation matrix of may be estimated based upon the camera rotation matrix and or the camera intrinsic matrix. For example the transformation matrix may be a function of the camera rotation matrix e.g. to take into account displacement of the camera during the exposure event of the image and or the camera intrinsic matrix e.g. so that calculations may be performed in image space using pixel units coordinates . The transformation matrix may be applied to ending image coordinates of a pixel e.g. x y corresponding to pixel coordinates of the pixel at the shutter close event which may be obtained from the image to obtain starting image coordinates of the pixel e.g. x y corresponding to pixel coordinates of the pixel at the shutter open event which otherwise may not be apparent from the image . A pixel displacement for the pixel may be determined based upon comparing the ending image coordinates and the beginning image coordinates of the pixel. For example a Euclidean distance between starting image coordinates of the pixel at the shutter open event and ending image coordinates of the pixel at the shutter close event may be determined as the pixel displacement. It may be appreciated that pixel displacement may be measured using a variety of measurement techniques and is not limited to merely a Euclidean distance measurement. A blur metric for the pixel may be determined based upon the pixel displacement e.g. the blur metric may be a function of a distance the pixel is displaced between the shutter open event and the shutter close event which may be due to the motion of the camera .

Blur metrics may be determined for respective pixels within the image. In this way a blur metric for the image may be determined e.g. the blur metric for the image may be a maximum pixel displacement for respective pixels . Accordingly the amount of blur for the image may be quantifiable in real time during operation of the camera and or offline after operation of the camera. For example if the blur metric for the image exceeds an acceptance threshold amount of blur then a notification may be provided to an operator of the camera e.g. an instruction may be provided to a pilot of an aircraft comprising the camera to perform a re flight to recapture the image .

It may be appreciated that blur metrics may be determined for images associated with a variety of imaging devices for which motion measurement data may be obtained. For example a blur metric may be determined for a hand held camera e.g. a digital camera an SLR camera a smart phone camera and or other cameras for which motion measurement data may be available such as through an accelerometer gyroscope etc. a vehicle mounted camera an aerial mounted camera a satellite mounted camera a video camera an image scanner e.g. a camera may be interpreted as a scanner such as used in document imaging etc. . Accordingly the instant application including the scope of the appended claims is not intended to be limited to the examples provided herein e.g. not limited to determining blur metrics for aerial mounted cameras etc. .

To the accomplishment of the foregoing and related ends the following description and annexed drawings set forth certain illustrative aspects and implementations. These are indicative of but a few of the various ways in which one or more aspects may be employed. Other aspects advantages and novel features of the disclosure will become apparent from the following detailed description when considered in conjunction with the annexed drawings.

The claimed subject matter is now described with reference to the drawings wherein like reference numerals are generally used to refer to like elements throughout. In the following description for purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of the claimed subject matter. It may be evident however that the claimed subject matter may be practiced without these specific details. In other instances structures and devices are illustrated in block diagram form in order to facilitate describing the claimed subject matter.

Images captured by cameras may suffer from blur due to camera motion during exposure events. For example a camera may be mounted to an aircraft in order to capture a series of images of a city. The camera may experience three dimensional motion while one or more images are captured by the camera e.g. turbulence experienced by the aircraft may result in three dimensional camera motion during an exposure event which may result in a blurred image . It may be advantageous to identify blurred images so that such images may be discarded and or recaptured as desired. Manual examination of the images for blur may be a cumbersome task if the series of images comprises a significant number of images e.g. it may be a daunting task for a user to manually examine thousands of images for blur . Conventional blur detection techniques may utilize a modular transfer function that may estimate blur based upon the presence of straight edges in an image scene. Unfortunately the modular transfer function may require the image scene to comprise straight edges which may not be available. Furthermore the blur estimate may be a relative value e.g. the modular transfer function may merely indicate blur as a comparison between a first image and a second image . Moreover because the modular transfer function is computationally intensive real time blur analysis of images may not be available.

Accordingly one or more systems and or techniques for determining a blur metric for an image are provided herein. The blur metric may be determined offline and or in real time e.g. determined during operation of the camera such that a notification may be provided to an operator of the camera when a blur metric for an image exceeds an acceptable threshold . The blur metric may be based upon motion measurement data associated with a camera e.g. an inertial measurement unit IMU may measure camera roll orientation angles camera pitch orientation angles and or camera heading orientation angles as angular motion information for inclusion within the motion measurement data . In particular a camera rotation matrix may be created based upon the motion measurement data which may be used in determining the blur metric. The camera rotation matrix e.g. to take into account motion of the camera during an exposure event and or a camera intrinsic matrix e.g. so that calculations may be performed in an image plane using pixel units coordinates may be used to estimate a transformation matrix. The transformation matrix may be used to determine pixel displacement of pixels within the image e.g. a difference in location of a pixel at a shutter open event and a shutter close event . The blur metric for the image may be determined based upon one or more pixel displacements e.g. the blur metric may be identified as a maximum pixel displacement . In this way blur of the image may be quantified by the blur metric.

One embodiment of determining a blur metric for an image is illustrated by an exemplary method in . At the method starts. At motion measurement data associated with a camera may be received. For example the motion measurement data may comprise angular motion information associated with a camera roll orientation angle a camera pitch orientation angle a camera heading orientation angle etc. The motion measurement data may have been captured by a motion measurement device such as an inertial measurement unit IMU e.g. operating at 200 Hz associated with the camera. The motion measurement data may be associated with an exposure event of an image. The exposure event may correspond to a shutter open event e.g. when the camera begins to capture the image and a shutter close event e.g. when the camera finishes capturing the image . In one example the motion measurement data may not correspond directly to a time of the shutter open event and or a time of the shutter close event. Accordingly the motion measurement data may be interpolated to identify motion measurement data corresponding to the time of the shutter open event and or motion measurement data corresponding to the time of the shutter close event. For example the motion measurement data may be interpolated to obtain shutter open camera orientation angles and or shutter close camera orientation angles.

At a camera rotation matrix e.g. camera rotation matrix of may be created based upon the motion measurement data. The camera rotation matrix may define an angular difference e.g. displacement of the camera between the shutter open event and the shutter close event of the image captured by the camera. For example the camera rotation matrix may be created by comparing shutter open camera orientation angles with shutter close camera orientation angles. In this way the camera rotation matrix may be utilized in determining the blur metric to take into account motion of the camera during the exposure event.

At a camera intrinsic matrix e.g. camera intrinsic matrix of may be obtained. The camera intrinsic matrix may comprise conversion information between image plane coordinates e.g. x y pixel coordinates and object coordinates e.g. X Y Z location coordinates of an object scene depicted within the image . The camera intrinsic matrix may be based upon a camera focal length e.g. an attribute of a camera lens associated with image magnification and or angle of view for the camera and or a principle point associated with the camera e.g. the principle point may take into account off centeredness between the center of the camera lens and the center of the image . In this way the camera intrinsic matrix may be utilized in determining the blur metric to convert between image plane coordinates and object coordinates e.g. calculations may be performed within the image plane using pixel units coordinates .

At a transformation matrix e.g. transformation matrix of may be estimated. The transformation matrix may be based upon the camera rotation matrix e.g. to take into account motion of the camera during the exposure event of the image and or the camera intrinsic matrix e.g. so that pixel coordinates within the image plane may be utilized in determining the blur metric . At the blur metric may be determined for the image based upon applying the transformation matrix to one or more pixels of the image. For example the transformation matrix may be applied to ending image coordinates of a pixel at the shutter close event to identify beginning image coordinates of the pixel at the shutter open event. That is ending image coordinates of the pixel at the shutter close event may be evident from the image e.g. the pixel may be depicted within the image at ending image coordinates 4 5 . The transformation matrix may be utilized to determine the beginning image coordinates of the pixel at the shutter open event which may not be apparent from merely the image e.g. the transformation matrix may determine that the pixel was located at 3 3 at the shutter open event . A pixel displacement for the pixel may be determined based upon the ending image coordinates and the beginning image coordinates of the pixel. For example a Euclidean distance between the ending image coordinates and the beginning image coordinates may be calculated as the pixel displacement e.g. pixel displacement of 2.24 may be determined based upon square root over 4 3 5 3 square root over 4 3 5 3 .

Pixel displacements may be determined for one or more pixels within the image using the transformation matrix. The blur metric for the image may be determined based upon one or more of the pixel displacements. In one example the blur metric may be a function of a maximum pixel displacement. It may be appreciated that the blur metric is not limited to a maximum pixel displacement but may be based upon other evaluations of the pixel displacements such as an average pixel displacement etc. for example. In this way the blur metric may quantify blur of the image. The blur metric may be determined offline and or in real time during operation of the camera. The blur metric may be compared to an acceptance threshold to determine whether the blur of the image exceeds such a threshold. If the blur metric exceeds the acceptance threshold then the image may be discarded and or a notification to recapture the image may be provided e.g. a notification in real time may be provided to an operator of the camera . At the method ends.

The camera rotation matrix component may be configured to identify motion measurement data associated with an exposure event of the image e.g. motion measurement data associated with a shutter open event and or a shutter close event . For example the camera rotation matrix component may be configured to interpolate the motion measurement data to obtain shutter open camera orientation angles e.g. pitch roll and or heading angles of the camera during the shutter open event and or shutter close camera orientation angles e.g. pitch roll and or heading angles of the camera during the shutter close event . The camera rotation matrix component may be configured to create a camera rotation matrix based upon the motion measurement data . For example the camera rotation matrix component may create the camera rotation matrix based upon an angular difference between the shutter open camera orientation angles and the shutter close camera orientation angles. In this way the camera rotation matrix component may create the camera rotation matrix defining an angular difference of the camera between the shutter open event and the shutter close event of the image.

The camera intrinsic matrix component may be configured to obtain a camera intrinsic matrix . The camera intrinsic matrix may comprise conversion information between image plane coordinates e.g. x y pixel coordinates of the image and object coordinates e.g. X Y Z location coordinates of an object scene depicted within the image based upon a camera focal length and a principle point for example associated with the camera .

The blur detection component may be configured to estimate a transformation matrix based upon the camera rotation matrix e.g. to take into account motion of the camera during the exposure event of the image and or the camera intrinsic matrix e.g. so that pixel coordinates within an image plane may be utilized in determining the blur metric . The blur detection component may utilize the transformation matrix in determining the blur metric for the image. For example the blur detection component may apply the transformation matrix to image data such as ending image coordinates of pixels within the image at the shutter close event to identify beginning image coordinates of pixels at the shutter open event. That is the blur detection component may apply the transformation matrix to one or more pixels within the image e.g. ending image coordinates specified within the image data in order to identify beginning image coordinates of the one or more pixels. The blur detection component may be configured to determine pixel displacements for the one or more pixels. For example a pixel displacement of a pixel may be determined based upon comparing ending image coordinates of the pixel to starting image coordinates of the pixel e.g. a Euclidean distance may be calculated to determine pixel displacement .

The blur detection component may determine the blur metric for the image based upon one or more pixel displacements. In one example the blur metric may correspond to a maximum pixel displacement of pixels within the image. In another example the blur metric may correspond to an average pixel displacement of pixels within the image although the blur metric is not limited to being determined in this manner and the instant application including the scope of the appended claims is not meant to be so limited . In this way the blur metric may quantify blur of the image. The blur detection component may be configured to compare the blur metric with an acceptance threshold to determine whether to retain discard and or provide a notification regarding blur of the image e.g. an operator of the camera may be provided with a notification to retake the image if the blur metric exceeds the acceptance threshold .

The camera rotation matrix component may be configured to create the camera rotation matrix based upon the motion measurement data e.g. angular differences in roll angle pitch angle and heading angle between a shutter open event and a shutter close event for the image . For example a roll angle difference may be determined based upon calculating a difference between a roll angle at the shutter open event and a roll angle at the shutter close event. A pitch angle difference may be determined based upon calculating a difference between a pitch angle at the shutter open event and a pitch angle at the shutter close event. A heading angle difference may be determined based upon calculating a difference between a heading angle at the shutter open event and a heading angle at the shutter close event. The camera rotation matrix component may utilize the roll angle difference the pitch angle difference and the heading angle difference to create the camera rotation matrix 

In one example the blur detection component may apply the transformation matrix to the ending image coordinates x y to identify the beginning image coordinates x y . Beginning pixel coordinate xmay correspond to x z and beginning pixel coordinate ymay correspond to y z within the transformation matrix . In this way the beginning coordinates x y may be identified so that the blur detection component may compare the beginning image coordinates x y with the ending image coordinates x y to determine a pixel displacement for the pixel e.g. which may be used to determine the blur metric for the image .

The blur detection component may compare the beginning image coordinates x y of the pixel and the ending image coordinates x y of the pixel to determine the pixel displacement experienced by the pixel during an exposure event corresponding to the shutter open event and the shutter close event. The blur metric may be a function of the pixel displacement. For example the blur metric may be a function of a Euclidean distance representing the pixel displacement which may be defined as blur x y square root over x x y y square root over x x y y . In this way the blur metric may be determined.

The blur detection component may be configured to determine the blur metric for the image based upon comparing the pixel coordinates at the shutter close event and the pixel coordinates at the shutter open event. The blur metric may quantify blur of the image associated with displacement of pixels between the shutter open event and the shutter close event e.g. pixels on a right side of a cloud pixels on a top portion of a first tree and pixels of a second tree may be displaced to the right e.g. or otherwise from the shutter open event to the shutter close event .

Still another embodiment involves a computer readable medium comprising processor executable instructions configured to implement one or more of the techniques presented herein. An exemplary computer readable medium that may be devised in these ways is illustrated in wherein the implementation comprises a computer readable medium e.g. a CD R DVD R or a platter of a hard disk drive on which is encoded computer readable data . This computer readable data in turn comprises a set of computer instructions configured to operate according to one or more of the principles set forth herein. In one such embodiment the processor executable computer instructions may be configured to perform a method such as at least some of the exemplary method of for example. In another such embodiment the processor executable instructions may be configured to implement a system such as at least some of the exemplary system of for example. Many such computer readable media may be devised by those of ordinary skill in the art that are configured to operate in accordance with the techniques presented herein.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

As used in this application the terms component module system interface and the like are generally intended to refer to a computer related entity either hardware a combination of hardware and software software or software in execution. For example a component may be but is not limited to being a process running on a processor a processor an object an executable a thread of execution a program and or a computer. By way of illustration both an application running on a controller and the controller can be a component. One or more components may reside within a process and or thread of execution and a component may be localized on one computer and or distributed between two or more computers.

Furthermore the claimed subject matter may be implemented as a method apparatus or article of manufacture using standard programming and or engineering techniques to produce software firmware hardware or any combination thereof to control a computer to implement the disclosed subject matter. The term article of manufacture as used herein is intended to encompass a computer program accessible from any computer readable device carrier or media. Of course those skilled in the art will recognize many modifications may be made to this configuration without departing from the scope or spirit of the claimed subject matter.

Although not required embodiments are described in the general context of computer readable instructions being executed by one or more computing devices. Computer readable instructions may be distributed via computer readable media discussed below . Computer readable instructions may be implemented as program modules such as functions objects Application Programming Interfaces APIs data structures and the like that perform particular tasks or implement particular abstract data types. Typically the functionality of the computer readable instructions may be combined or distributed as desired in various environments.

In other embodiments device may include additional features and or functionality. For example device may also include additional storage e.g. removable and or non removable including but not limited to magnetic storage optical storage and the like. Such additional storage is illustrated in by storage . In one embodiment computer readable instructions to implement one or more embodiments provided herein may be in storage . Storage may also store other computer readable instructions to implement an operating system an application program and the like. Computer readable instructions may be loaded in memory for execution by processing unit for example.

The term computer readable media as used herein includes computer storage media. Computer storage media includes volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions or other data. Memory and storage are examples of computer storage media. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM Digital Versatile Disks DVDs or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by device . Any such computer storage media may be part of device .

Device may also include communication connection s that allows device to communicate with other devices. Communication connection s may include but is not limited to a modem a Network Interface Card NIC an integrated network interface a radio frequency transmitter receiver an infrared port a USB connection or other interfaces for connecting computing device to other computing devices. Communication connection s may include a wired connection or a wireless connection. Communication connection s may transmit and or receive communication media.

The term computer readable media may include communication media. Communication media typically embodies computer readable instructions or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term modulated data signal may include a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal.

Device may include input device s such as keyboard mouse pen voice input device touch input device infrared cameras video input devices and or any other input device. Output device s such as one or more displays speakers printers and or any other output device may also be included in device . Input device s and output device s may be connected to device via a wired connection wireless connection or any combination thereof. In one embodiment an input device or an output device from another computing device may be used as input device s or output device s for computing device .

Components of computing device may be connected by various interconnects such as a bus. Such interconnects may include a Peripheral Component Interconnect PCI such as PCI Express a Universal Serial Bus USB firewire IEEE 1394 an optical bus structure and the like. In another embodiment components of computing device may be interconnected by a network. For example memory may be comprised of multiple physical memory units located in different physical locations interconnected by a network.

Those skilled in the art will realize that storage devices utilized to store computer readable instructions may be distributed across a network. For example a computing device accessible via a network may store computer readable instructions to implement one or more embodiments provided herein. Computing device may access computing device and download a part or all of the computer readable instructions for execution. Alternatively computing device may download pieces of the computer readable instructions as needed or some instructions may be executed at computing device and some at computing device .

Various operations of embodiments are provided herein. In one embodiment one or more of the operations described may constitute computer readable instructions stored on one or more computer readable media which if executed by a computing device will cause the computing device to perform the operations described. The order in which some or all of the operations are described should not be construed as to imply that these operations are necessarily order dependent. Alternative ordering will be appreciated by one skilled in the art having the benefit of this description. Further it will be understood that not all operations are necessarily present in each embodiment provided herein.

Moreover the word exemplary is used herein to mean serving as an example instance or illustration. Any aspect or design described herein as exemplary is not necessarily to be construed as advantageous over other aspects or designs. Rather use of the word exemplary is intended to present concepts in a concrete fashion. As used in this application the term or is intended to mean an inclusive or rather than an exclusive or . That is unless specified otherwise or clear from context X employs A or B is intended to mean any of the natural inclusive permutations. That is if X employs A X employs B or X employs both A and B then X employs A or B is satisfied under any of the foregoing instances. In addition the articles a and an as used in this application and the appended claims may generally be construed to mean one or more unless specified otherwise or clear from context to be directed to a singular form. Also at least one of A and B and or the like generally means A or B or both A and B.

Also although the disclosure has been shown and described with respect to one or more implementations equivalent alterations and modifications will occur to others skilled in the art based upon a reading and understanding of this specification and the annexed drawings. The disclosure includes all such modifications and alterations and is limited only by the scope of the following claims. In particular regard to the various functions performed by the above described components e.g. elements resources etc. the terms used to describe such components are intended to correspond unless otherwise indicated to any component which performs the specified function of the described component e.g. that is functionally equivalent even though not structurally equivalent to the disclosed structure which performs the function in the herein illustrated exemplary implementations of the disclosure. In addition while a particular feature of the disclosure may have been disclosed with respect to only one of several implementations such feature may be combined with one or more other features of the other implementations as may be desired and advantageous for any given or particular application. Furthermore to the extent that the terms includes having has with or variants thereof are used in either the detailed description or the claims such terms are intended to be inclusive in a manner similar to the term comprising. 

