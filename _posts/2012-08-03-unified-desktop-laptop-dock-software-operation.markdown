---

title: Unified desktop laptop dock software operation
abstract: Methods and devices for selectively presenting a user interface or “desktop” across two devices are provided. More particularly, a unified desktop is presented across a device and a computer system that comprise a unified system. The unified desktop acts as a single user interface that presents data and receives user interaction in a seamless environment that emulates a personal computing environment. To function within the personal computing environment, the unified desktop includes a process for docking and undocking the device with the computer system. The unified desktop presents desktops or windows based on a set of pre-determined rules.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09405459&OS=09405459&RS=09405459
owner: Z124
number: 09405459
owner_city: George Town
owner_country: KY
publication_date: 20120803
---
This application is a Continuation In Part and claims the benefit of and priority to U.S. application Ser. No. 13 566 244 filed Aug. 3 2012 U.S. application Ser. No. 13 566 168 filed Aug. 3 2012 U.S. application Ser. No. 13 566 103 filed Aug. 3 2012 U.S. application Ser. No. 13 543 678 filed Jul. 6 2012 U.S. application Ser. No. 13 543 635 filed Jul. 6 2012 now abandoned U.S. application Ser. No. 13 408 530 filed Feb. 29 2012 U.S. application Ser. No. 13 410 931 filed Mar. 2 2012 now U.S. Pat. No. 8 872 727 U.S. application Ser. No. 13 410 958 filed Mar. 2 2012 now U.S. Pat. No. 9 122 441 U.S. application Ser. No. 13 410 983 filed Mar. 2 2012 now U.S. Pat. No. 9 003 311 U.S. application Ser. No. 13 411 034 filed Mar. 2 2012 now U.S. Pat. No. 8 910 061 U.S. application Ser. No. 13 411 075 filed Mar. 2 2012 U.S. application Ser. No. 13 436 593 filed Mar. 30 2012 now U.S. Pat. No. 9 069 518 U.S. application Ser. No. 13 436 823 filed Mar. 30 2012 now U.S. Pat. No. 9 213 516 U.S. application Ser. No. 13 436 826 filed Mar. 30 2012 U.S. application Ser. No. 13 485 734 filed May 31 2012 now U.S. Pat. No. 8 874 894 and U.S. application Ser. No. 13 485 743 filed May 31 2012 now U.S. Pat. No. 8 904 165.

This application also claims the benefits of and priority under 35 U.S.C. 119 e to U.S. Provisional Application Ser. No. 61 539 884 filed Sep. 27 2011. This application is also related to U.S. application Ser. Nos. 13 217 108 filed on Aug. 24 2011 Ser. No. 13 251 427 filed on Oct. 3 2011 Ser. No. 13 247 166 filed on Sep. 28 2011 Ser. No. 13 217 121 filed on Aug. 24 2011 Ser. No. 13 217 130 filed on Aug. 24 2011 Ser. No. 13 247 170 filed on Sep. 28 2011 Ser. No. 13 246 669 filed on Sep. 27 2011 Ser. No. 13 217 099 filed on Aug. 24 2011 Ser. No. 13 247 885 filed on Sep. 28 2011 Ser. No. 13 250 764 filed on Sep. 30 2011 Ser. No. 13 251 434 filed on Oct. 3 2011 13 399 901 filed on Feb. 17 2012 Ser. No. 13 399 929 filed on Feb. 17 2012 13 399 936 filed on Feb. 17 2012 Ser. No. 13 246 118 filed on Sep. 27 2011 Ser. No. 13 246 128 filed on Sep. 27 2011 Ser. No. 13 246 133 filed on Sep. 27 2011 Ser. No. 13 246 675 filed on Sep. 27 2011 and Ser. No. 13 246 665 filed on Sep. 27 2011. The entire disclosures of all applications or patents listed herein are hereby incorporated by reference for all that they teach and for all purposes.

A substantial number of handheld computing devices such as cellular phones tablets and E Readers make use of a touch screen display not only to deliver display information to the user but also to receive inputs from user interface commands. While touch screen displays may increase the configurability of the handheld device and provide a wide variety of user interface options this flexibility typically comes at a price. The dual use of the touch screen to provide content and receive user commands while flexible for the user may obfuscate the display and cause visual clutter thereby leading to user frustration and loss of productivity.

The small form factor of handheld computing devices requires a careful balancing between the displayed graphics and the area provided for receiving inputs. On the one hand the small display constrains the display space which may increase the difficulty of interpreting actions or results. On the other hand a virtual keypad or other user interface scheme is superimposed on or positioned adjacent to an executing application requiring the application to be squeezed into an even smaller portion of the display.

This balancing act is particularly difficult for single display touch screen devices. Single display touch screen devices are crippled by their limited screen space. When users are entering information into the device through the single display the ability to interpret information in the display can be severely hampered particularly when a complex interaction between display and interface is required.

Current handheld computing devices may be connected to larger computing devices e.g. personal computers PCs or peripheral screens to provide more display area. Current handheld devices do not include features that allow it to provide both PC functionality and the functionality associated with the handheld device e.g. phone text or other communication functionality. Instead a peripheral screen connected to a handheld device merely provides more display area for the handheld computing device. When connecting the handheld device to another computing system such as a PC the handheld device is typically recognized by the computing system as a peripheral device. The functionality of the handheld device is typically not integrated with the functionality of the larger computing system.

There is a need for a dual multi display handheld computing device that provides for enhanced power and or versatility compared to conventional single display handheld computing devices. These and other needs are addressed by the various aspects embodiments and or configurations of the present disclosure. Also while the disclosure is presented in terms of exemplary embodiments it should be appreciated that individual aspects of the disclosure can be separately claimed.

Embodiments provide for a handheld device with a unified desktop for integrating the functionality of the handheld device with a larger computing system e.g. a PC. When connected to a peripheral display and or a display of a PC the handheld device provides a unified desktop displayed across the screen s of the handheld device and the additional display. The unified desktop unifies the PC functionality provided on the additional display with the handheld functionality such as communication applications e.g. phone SMS MMS provided on the screen s of the handheld device. A user can seamlessly interact with applications e.g. open drag close receive notifications on the unified desktop whether the applications are displayed on the screens of the handheld device or the peripheral display of the larger computing system. Each portion of the unified desktop i.e. the portion on the handheld device and the portion on the peripheral screen may display different applications information and or have a different layout. Also in embodiments each portion of the desktop may display similar information in different formats. For example battery level of the handheld device wireless network signal strength notifications can be displayed in both portions of the desktop with a larger format being used on the portion of the unified desktop displayed on the peripheral screen and a smaller format used on the screen s of the peripheral device.

Further embodiments provide for a method to providing the device and computer system which are docked to form a unified system. A unified desktop is generated for the unified system wherein the unified desktop includes at least a first user interface associated with the device and a second user interface associated with the computer system and wherein the unified desktop emulates a personal computer environment. While docked a first event is received at the computer system. The computer system sends the first event to the device for processing and subsequently receives output associated with the first event to display on the second user interface of the computer system.

The phrases at least one one or more and and or are open ended expressions that are both conjunctive and disjunctive in operation. For example each of the expressions at least one of A B and C at least one of A B or C one or more of A B and C one or more of A B or C and A B and or C means A alone B alone C alone A and B together A and C together B and C together or A B and C together.

The term a or an entity refers to one or more of that entity. As such the terms a or an one or more and at least one can be used interchangeably herein. It is also to be noted that the terms comprising including and having can be used interchangeably.

The term automatic and variations thereof as used herein refers to any process or operation done without material human input when the process or operation is performed. However a process or operation can be automatic even though performance of the process or operation uses material or immaterial human input if the input is received before performance of the process or operation. Human input is deemed to be material if such input influences how the process or operation will be performed. Human input that consents to the performance of the process or operation is not deemed to be material .

The term computer readable medium as used herein refers to any tangible storage and or transmission medium that participate in providing instructions to a processor for execution. Such a medium may take many forms including but not limited to non volatile media volatile media and transmission media. Non volatile media includes for example NVRAM or magnetic or optical disks. Volatile media includes dynamic memory such as main memory. Common forms of computer readable media include for example a floppy disk a flexible disk hard disk magnetic tape or any other magnetic medium magneto optical medium a CD ROM any other optical medium punch cards paper tape any other physical medium with patterns of holes a RAM a PROM and EPROM a FLASH EPROM a solid state medium like a memory card any other memory chip or cartridge a carrier wave as described hereinafter or any other medium from which a computer can read. A digital file attachment to e mail or other self contained information archive or set of archives is considered a distribution medium equivalent to a tangible storage medium. When the computer readable media is configured as a database it is to be understood that the database may be any type of database such as relational hierarchical object oriented and or the like. Accordingly the disclosure is considered to include a tangible storage medium or distribution medium and prior art recognized equivalents and successor media in which the software implementations of the present disclosure are stored.

The term desktop refers to a metaphor used to portray systems. A desktop is generally considered a surface that typically includes pictures called icons widgets folders etc. that can activate show applications windows cabinets files folders documents and other graphical items. The icons are generally selectable to initiate a task through user interface interaction to allow a user to execute applications or conduct other operations.

The term screen touch screen or touchscreen refers to a physical structure that includes one or more hardware components that provide the device with the ability to render a user interface and or receive user input. A screen can encompass any combination of gesture capture region a touch sensitive display and or a configurable area. The device can have one or more physical screens embedded in the hardware. However a screen may also include an external peripheral device that may be attached and detached from the device. In embodiments multiple external devices may be attached to the device. Thus in embodiments the screen can enable the user to interact with the device by touching areas on the screen and provides information to a user through a display. The touch screen may sense user contact in a number of different ways such as by a change in an electrical parameter e.g. resistance or capacitance acoustic wave variations infrared radiation proximity detection light variation detection and the like. In a resistive touch screen for example normally separated conductive and resistive metallic layers in the screen pass an electrical current. When a user touches the screen the two layers make contact in the contacted location whereby a change in electrical field is noted and the coordinates of the contacted location calculated. In a capacitive touch screen a capacitive layer stores electrical charge which is discharged to the user upon contact with the touch screen causing a decrease in the charge of the capacitive layer. The decrease is measured and the contacted location coordinates determined. In a surface acoustic wave touch screen an acoustic wave is transmitted through the screen and the acoustic wave is disturbed by user contact. A receiving transducer detects the user contact instance and determines the contacted location coordinates.

The term display refers to a portion of one or more screens used to display the output of a computer to a user. A display may be a single screen display or a multi screen display referred to as a composite display. A composite display can encompass the touch sensitive display of one or more screens. A single physical screen can include multiple displays that are managed as separate logical displays. Thus different content can be displayed on the separate displays although part of the same physical screen.

The term displayed image refers to an image produced on the display. A typical displayed image is a window or desktop. The displayed image may occupy all or a portion of the display.

The term display orientation refers to the way in which a rectangular display is oriented by a user for viewing. The two most common types of display orientation are portrait and landscape. In landscape mode the display is oriented such that the width of the display is greater than the height of the display such as a 4 3 ratio which is 4 units wide and 3 units tall or a 16 9 ratio which is 16 units wide and 9 units tall . Stated differently the longer dimension of the display is oriented substantially horizontal in landscape mode while the shorter dimension of the display is oriented substantially vertical. In the portrait mode by contrast the display is oriented such that the width of the display is less than the height of the display. Stated differently the shorter dimension of the display is oriented substantially horizontal in the portrait mode while the longer dimension of the display is oriented substantially vertical.

The term composite display refers to a logical structure that defines a display that can encompass one or more screens. A multi screen display can be associated with a composite display that encompasses all the screens. The composite display can have different display characteristics based on the various orientations of the device.

The term gesture refers to a user action that expresses an intended idea action meaning result and or outcome. The user action can include manipulating a device e.g. opening or closing a device changing a device orientation moving a trackball or wheel etc. movement of a body part in relation to the device movement of an implement or tool in relation to the device audio inputs etc. A gesture may be made on a device such as on the screen or with the device to interact with the device.

The term module as used herein refers to any known or later developed hardware software firmware artificial intelligence fuzzy logic or combination of hardware and software that is capable of performing the functionality associated with that element.

The term gesture capture refers to a sense or otherwise a detection of an instance and or type of user gesture. The gesture capture can occur in one or more areas of the screen A gesture region can be on the display where it may be referred to as a touch sensitive display or off the display where it may be referred to as a gesture capture area.

A multi screen application or multiple display application refers to an application that is capable of multiple modes. The multi screen application mode can include but is not limited to a single screen mode where the application is displayed on a single screen or a composite display mode where the application is displayed on two or more screens . A multi screen application can have different layouts optimized for the mode. Thus the multi screen application can have different layouts for a single screen or for a composite display that can encompass two or more screens. The different layouts may have different screen display dimensions and or configurations on which the user interfaces of the multi screen applications can be rendered. The different layouts allow the application to optimize the application s user interface for the type of display e.g. single screen or multiple screens. In single screen mode the multi screen application may present one window pane of information. In a composite display mode the multi screen application may present multiple window panes of information or may provide a larger and a richer presentation because there is more space for the display contents. The multi screen applications may be designed to adapt dynamically to changes in the device and the mode depending on which display single or composite the system assigns to the multi screen application. In alternative embodiments the user can use a gesture to request the application transition to a different mode and if a display is available for the requested mode the device can allow the application to move to that display and transition modes.

A single screen application refers to an application that is capable of single screen mode. Thus the single screen application can produce only one window and may not be capable of different modes or different display dimensions. A single screen application may not be capable of the several modes discussed with the multi screen application.

The term window refers to a typically rectangular displayed image on at least part of a display that contains or provides content different from the rest of the screen. The window may obscure the desktop.

The terms determine calculate and compute and variations thereof as used herein are used interchangeably and include any type of methodology process mathematical operation or technique.

It shall be understood that the term means as used herein shall be given its broadest possible interpretation in accordance with 35 U.S.C. Section 112 Paragraph 6. Accordingly a claim incorporating the term means shall cover all structures materials or acts set forth herein and all of the equivalents thereof. Further the structures materials or acts and the equivalents thereof shall include all those described in the summary of the invention brief description of the drawings detailed description abstract and claims themselves.

The preceding is a simplified summary of the disclosure to provide an understanding of some aspects of the disclosure. This summary is neither an extensive nor exhaustive overview of the disclosure and its various aspects embodiments and or configurations. It is intended neither to identify key or critical elements of the disclosure nor to delineate the scope of the disclosure but to present selected concepts of the disclosure in a simplified form as an introduction to the more detailed description presented below. As will be appreciated other aspects embodiments and or configurations of the disclosure are possible utilizing alone or in combination one or more of the features set forth above or described in detail below.

In the appended figures similar components and or features may have the same reference label. Further various components of the same type may be distinguished by following the reference label by a letter that distinguishes among the similar components. If only the first reference label is used in the specification the description is applicable to any one of the similar components having the same first reference label irrespective of the second reference label.

Presented herein are embodiments of a device. The device can be a communications device such as a cellular telephone or other smart device. The device can include two screens that are oriented to provide several unique display configurations. Further the device can receive user input in unique ways. The overall design and functionality of the device provides for an enhanced user experience making the device more useful and more efficient.

Primary screen also includes a configurable area that has been configured for specific inputs when the user touches portions of the configurable area . Secondary screen also includes a configurable area that has been configured for specific inputs. Areas and have been configured to receive a back input indicating that a user would like to view information previously displayed. Areas and have been configured to receive a menu input indicating that the user would like to view options from a menu. Areas and have been configured to receive a home input indicating that the user would like to view information associated with a home view. In other embodiments areas and may be configured in addition to the configurations described above for other types of specific inputs including controlling features of device some non limiting examples including adjusting overall system power adjusting the volume adjusting the brightness adjusting the vibration selecting of displayed items on either of screen or operating a camera operating a microphone and initiating terminating of telephone calls. Also in some embodiments areas and may be configured for specific inputs depending upon the application running on device and or information displayed on touch sensitive displays and or .

In addition to touch sensing primary screen and secondary screen may also include areas that receive input from a user without requiring the user to touch the display area of the screen. For example primary screen includes gesture capture area and secondary screen includes gesture capture area . These areas are able to receive input by recognizing gestures made by a user without the need for the user to actually touch the surface of the display area. In comparison to touch sensitive displays and the gesture capture areas and are commonly not capable of rendering a displayed image.

The two screens and are connected together with a hinge shown clearly in illustrating a back view of device . Hinge in the embodiment shown in is a center hinge that connects screens and so that when the hinge is closed screens and are juxtaposed i.e. side by side as shown in illustrating a front view of device . Hinge can be opened to position the two screens and in different relative positions to each other. As described in greater detail below the device may have different functionalities depending on the relative positions of screens and .

Device also includes a number of buttons . For example illustrates the left side of device . As shown in the side of primary screen includes three buttons and which can be configured for specific inputs. For example buttons and may be configured to in combination or alone control a number of aspects of device . Some non limiting examples include overall system power volume brightness vibration selection of displayed items on either of screen or a camera a microphone and initiation termination of telephone calls. In some embodiments instead of separate buttons two buttons may be combined into a rocker button. This arrangement is useful in situations where the buttons are configured to control features such as volume or brightness. In addition to buttons and device also includes a button shown in which illustrates the top of device . In one embodiment button is configured as an on off button used to control overall system power to device . In other embodiments button is configured to in addition to or in lieu of controlling system power control other aspects of device . In some embodiments one or more of the buttons and are capable of supporting different user commands. By way of example a normal press has a duration commonly of less than about 1 second and resembles a quick tap. A medium press has a duration commonly of 1 second or more but less than about 12 seconds. A long press has a duration commonly of about 12 seconds or more. The function of the buttons is normally specific to the application that is currently in focus on the respective display and . In a telephone application for instance and depending on the particular button a normal medium or long press can mean end call increase in call volume decrease in call volume and toggle microphone mute. In a camera or video application for instance and depending on the particular button a normal medium or long press can mean increase zoom decrease zoom and take photograph or record video.

There are also a number of hardware components within device . As illustrated in device includes a speaker and a microphone . Device also includes a camera . Additionally device includes two position sensors A and B which are used to determine the relative positions of screens and . In one embodiment position sensors A and B are Hall effect sensors. However in other embodiments other sensors can be used in addition to or in lieu of the Hall effect sensors. An accelerometer may also be included as part of device to determine the orientation of the device and or the orientation of screens and . Additional internal hardware components that may be included in device are described below with respect to .

The overall design of device allows it to provide additional functionality not available in other communication devices. Some of the functionality is based on the various positions and orientations that device can have. As shown in device can be operated in an open position where screens and are juxtaposed. This position allows a large display area for displaying information to a user. When position sensors A and B determine that device is in the open position they can generate a signal that can be used to trigger different events such as displaying information on both screens and . Additional events may be triggered if accelerometer determines that device is in a portrait position as opposed to a landscape position not shown .

In addition to the open position device may also have a closed position illustrated in . Again position sensors A and B can generate a signal indicating that device is in the closed position. This can trigger an event that results in a change of displayed information on screen and or . For example device may be programmed to stop displaying information on one of the screens e.g. screen since a user can only view one screen at a time when device is in the closed position. In other embodiments the signal generated by position sensors A and B indicating that the device is in the closed position can trigger device to answer an incoming telephone call. The closed position can also be a preferred position for utilizing the device as a mobile phone.

Device can also be used in an easel position which is illustrated in . In the easel position screens and are angled with respect to each other and facing outward with the edges of screens and substantially horizontal. In this position device can be configured to display information on both screens and to allow two users to simultaneously interact with device . When device is in the easel position sensors A and B generate a signal indicating that the screens and are positioned at an angle to each other and the accelerometer can generate a signal indicating that device has been placed so that the edge of screens and are substantially horizontal. The signals can then be used in combination to generate events that trigger changes in the display of information on screens and .

Transitional states are also possible. When the position sensors A and B and or accelerometer indicate that the screens are being closed or folded from open a closing transitional state is recognized. Conversely when the position sensors A and B indicate that the screens are being opened or folded from closed an opening transitional state is recognized. The closing and opening transitional states are typically time based or have a maximum time duration from a sensed starting point. Normally no user input is possible when one of the closing and opening states is in effect. In this manner incidental user contact with a screen during the closing or opening function is not misinterpreted as user input. In embodiments another transitional state is possible when the device is closed. This additional transitional state allows the display to switch from one screen to the second screen when the device is closed based on some user input e.g. a double tap on the screen .

As can be appreciated the description of device is made for illustrative purposes only and the embodiments are not limited to the specific mechanical features shown in and described above. In other embodiments device may include additional features including one or more additional buttons slots display areas hinges and or locking mechanisms. Additionally in embodiments the features described above may be located in different parts of device and still provide similar functionality. Therefore and the description provided above are nonlimiting.

A third region of the touch sensitive screens and may comprise a configurable area . The configurable area is capable of receiving input and has display or limited display capabilities. In embodiments the configurable area may present different input options to the user. For example the configurable area may display buttons or other relatable items. Moreover the identity of displayed buttons or whether any buttons are displayed at all within the configurable area of a touch sensitive screen or may be determined from the context in which the device is used and or operated. In an exemplary embodiment the touch sensitive screens and comprise liquid crystal display devices extending across at least those regions of the touch sensitive screens and that are capable of providing visual output to a user and a capacitive input matrix over those regions of the touch sensitive screens and that are capable of receiving input from the user.

One or more display controllers may be provided for controlling the operation of the touch sensitive screens and including input touch sensing and output display functions. In the exemplary embodiment illustrated in a separate touch screen controller or is provided for each touch screen and . In accordance with alternate embodiments a common or shared touch screen controller may be used to control each of the included touch sensitive screens and . In accordance with still other embodiments the functions of a touch screen controller may be incorporated into other components such as a processor .

The processor may comprise a general purpose programmable processor or controller for executing application programming or instructions. In accordance with at least some embodiments the processor may include multiple processor cores and or implement multiple virtual processors. In accordance with still other embodiments the processor may include multiple physical processors. As a particular example the processor may comprise a specially configured application specific integrated circuit ASIC or other integrated circuit a digital signal processor a controller a hardwired electronic or logic circuit a programmable logic device or gate array a special purpose computer or the like. The processor generally functions to run programming code or instructions implementing various functions of the device .

A communication device may also include memory for use in connection with the execution of application programming or instructions by the processor and for the temporary or long term storage of program instructions and or data. As examples the memory may comprise RAM DRAM SDRAM or other solid state memory. Alternatively or in addition data storage may be provided. Like the memory the data storage may comprise a solid state memory device or devices. Alternatively or in addition the data storage may comprise a hard disk drive or other random access memory.

In support of communications functions or capabilities the device can include a cellular telephony module . As examples the cellular telephony module can comprise a GSM CDMA FDMA and or analog cellular telephony transceiver capable of supporting voice multimedia and or data transfers over a cellular network. Alternatively or in addition the device can include an additional or other wireless communications module . As examples the other wireless communications module can comprise a Wi Fi BLUETOOTH WiMax infrared or other wireless communications link. The cellular telephony module and the other wireless communications module can each be associated with a shared or a dedicated antenna .

A port interface may be included. The port interface may include proprietary or universal ports to support the interconnection of the device to other devices or components such as a dock which may or may not include additional or different capabilities from those integral to the device . In addition to supporting an exchange of communication signals between the device and another device or component the docking port and or port interface can support the supply of power to or from the device . The port interface also comprises an intelligent element that comprises a docking module for controlling communications or other interactions between the device and a connected device or component.

An input output module and associated ports may be included to support communications over wired networks or links for example with other communication devices server devices and or peripheral devices. Examples of an input output module include an Ethernet port a Universal Serial Bus USB port Institute of Electrical and Electronics Engineers IEEE 1394 or other interface.

An audio input output interface device s can be included to provide analog audio to an interconnected speaker or other device and to receive analog audio input from a connected microphone or other device. As an example the audio input output interface device s may comprise an associated amplifier and analog to digital converter. Alternatively or in addition the device can include an integrated audio input output device and or an audio jack for interconnecting an external speaker or microphone. For example an integrated speaker and an integrated microphone can be provided to support near talk or speaker phone operations.

Hardware buttons can be included for example for use in connection with certain control operations. Examples include a master power switch volume control etc. as described in conjunction with . One or more image capture interfaces devices such as a camera can be included for capturing still and or video images. Alternatively or in addition an image capture interface device can include a scanner or code reader. An image capture interface device can include or be associated with additional elements such as a flash or other light source.

The device can also include a global positioning system GPS receiver . In accordance with embodiments of the present invention the GPS receiver may further comprise a GPS module that is capable of providing absolute location information to other components of the device . An accelerometer s may also be included. For example in connection with the display of information to a user and or other functions a signal from the accelerometer can be used to determine an orientation and or format in which to display that information to the user.

Embodiments of the present invention can also include one or more position sensor s . The position sensor can provide a signal indicating the position of the touch sensitive screens and relative to one another. This information can be provided as an input for example to a user interface application to determine an operating mode characteristics of the touch sensitive displays and or other device operations. As examples a screen position sensor can comprise a series of Hall effect sensors a multiple position switch an optical switch a Wheatstone bridge a potentiometer or other arrangement capable of providing a signal indicating of multiple relative positions the touch screens are in.

Communications between various components of the device can be carried by one or more buses . In addition power can be supplied to the components of the device from a power source and or power control module . The power control module can for example include a battery an AC to DC converter power control logic and or ports for interconnecting the device to an external source of power.

As illustrated in there are twelve exemplary physical states closed transition or opening transitional state easel modified easel open inbound outbound call or communication image video capture transition or closing transitional state landscape docked docked and landscape . Next to each illustrative state is a representation of the physical state of the device with the exception of states and where the state is generally symbolized by the international icon for a telephone and the icon for a camera respectfully.

In state the device is in a closed state with the device generally oriented in the portrait direction with the primary screen and the secondary screen back to back in different planes see . From the closed state the device can enter for example docked state where the device is coupled with a docking station docking cable or in general docked or associated with one or more other devices or peripherals or the landscape state where the device is generally oriented with the primary screen facing the user and the primary screen and the secondary screen being back to back.

In the closed state the device can also move to a transitional state where the device remains closed but the display is moved from one screen to another screen based on a user input e.g. a double tap on the screen . Still another embodiment includes a bilateral state. In the bilateral state the device remains closed but a single application displays at least one window on both the first display and the second display . The windows shown on the first and second display may be the same or different based on the application and the state of that application. For example while acquiring an image with a camera the device may display the view finder on the first display and displays a preview for the photo subjects full screen and mirrored left to right on the second display .

In state a transition state from the closed state to the semi open state or easel state the device is shown opening with the primary screen and the secondary screen being rotated around a point of axis coincidence with the hinge. Upon entering the easel state the primary screen and the secondary screen are separated from one another such that for example the device can sit in an easel like configuration on a surface.

In state known as the modified easel position the device has the primary screen and the secondary screen in a similar relative relationship to one another as in the easel state with the difference being one of the primary screen or the secondary screen are placed on a surface as shown.

State is the open state where the primary screen and the secondary screen are generally on the same plane. From the open state the device can transition to the docked state or the open landscape state . In the open state the primary screen and the secondary screen are generally in the portrait like orientation while in landscaped state the primary screen and the secondary screen are generally in a landscape like orientation.

State is illustrative of a communication state such as when an inbound or outbound call is being received or placed respectively by the device . While not illustrated for clarity it should be appreciated the device can transition to the inbound outbound call state from any state illustrated in . In a similar manner the image video capture state can be entered into from any other state in with the image video capture state allowing the device to take one or more images via a camera and or videos with a video capture device .

Transition state illustratively shows primary screen and the secondary screen being closed upon one another for entry into for example the closed state .

In the Key indicates that H represents an input from one or more Hall Effect sensors A represents an input from one or more accelerometers T represents an input from a timer P represents a communications trigger input and I represents an image and or video capture request input. Thus in the center portion of the chart an input or combination of inputs are shown that represent how the device detects a transition from a first physical state to a second physical state.

As discussed in the center portion of the chart the inputs that are received enable the detection of a transition from for example a portrait open state to a landscape easel state shown in bold HAT. For this exemplary transition from the portrait open to the landscape easel state a Hall Effect sensor H an accelerometer A and a timer T input may be needed. The timer input can be derived from for example a clock associated with the processor.

In addition to the portrait and landscape states a docked state is also shown that is triggered based on the receipt of a docking signal . As discussed above and in relation to the docking signal can be triggered by the association of the device with one or more other device accessories peripherals smart docks or the like.

With reference to a first type of gesture a touch gesture is substantially stationary on the screen for a selected length of time. A circle represents a touch or other contact type received at particular location of a contact sensing portion of the screen. The circle may include a border the thickness of which indicates a length of time that the contact is held substantially stationary at the contact location. For instance a tap or short press has a thinner border than the border for a long press or for a normal press . The long press may involve a contact that remains substantially stationary on the screen for longer time period than that of a tap . As will be appreciated differently defined gestures may be registered depending upon the length of time that the touch remains stationary prior to contact cessation or movement on the screen.

With reference to a drag gesture on the screen is an initial contact represented by circle with contact movement in a selected direction. The initial contact may remain stationary on the screen for a certain amount of time represented by the border . The drag gesture typically requires the user to contact an icon window or other displayed image at a first location followed by movement of the contact in a drag direction to a new second location desired for the selected displayed image. The contact movement need not be in a straight line but have any path of movement so long as the contact is substantially continuous from the first to the second locations.

With reference to a flick gesture on the screen is an initial contact represented by circle with truncated contact movement relative to a drag gesture in a selected direction. In embodiments a flick has a higher exit velocity for the last movement in the gesture compared to the drag gesture. The flick gesture can for instance be a finger snap following initial contact. Compared to a drag gesture a flick gesture generally does not require continual contact with the screen from the first location of a displayed image to a predetermined second location. The contacted displayed image is moved by the flick gesture in the direction of the flick gesture to the predetermined second location. Although both gestures commonly can move a displayed image from a first location to a second location the temporal duration and distance of travel of the contact on the screen is generally less for a flick than for a drag gesture.

With reference to a pinch gesture on the screen is depicted. The pinch gesture may be initiated by a first contact to the screen by for example a first digit and a second contact to the screen by for example a second digit. The first and second contacts may be detected by a common contact sensing portion of a common screen by different contact sensing portions of a common screen or or by different contact sensing portions of different screens. The first contact is held for a first amount of time as represented by the border and the second contact is held for a second amount of time as represented by the border . The first and second amounts of time are generally substantially the same and the first and second contacts generally occur substantially simultaneously. The first and second contacts generally also include corresponding first and second contact movements respectively. The first and second contact movements are generally in opposing directions. Stated another way the first contact movement is towards the second contact and the second contact movement is towards the first contact . More simply stated the pinch gesture may be accomplished by a user s digits touching the screen in a pinching motion.

With reference to a spread gesture on the screen is depicted. The spread gesture may be initiated by a first contact to the screen by for example a first digit and a second contact to the screen by for example a second digit. The first and second contacts may be detected by a common contact sensing portion of a common screen by different contact sensing portions of a common screen or by different contact sensing portions of different screens. The first contact is held for a first amount of time as represented by the border and the second contact is held for a second amount of time as represented by the border . The first and second amounts of time are generally substantially the same and the first and second contacts generally occur substantially simultaneously. The first and second contacts generally also include corresponding first and second contact movements b respectively. The first and second contact movements are generally in a common direction. Stated another way the first and second contact movements are away from the first and second contacts . More simply stated the spread gesture may be accomplished by a user s digits touching the screen in a spreading motion.

The above gestures may be combined in any manner such as those shown by to produce a determined functional result. For example in a tap gesture is combined with a drag or flick gesture in a direction away from the tap gesture . In a tap gesture is combined with a drag or flick gesture in a direction towards the tap gesture .

The functional result of receiving a gesture can vary depending on a number of factors including a state of the device display or screen a context associated with the gesture or sensed location of the gesture. The state of the device commonly refers to one or more of a configuration of the device a display orientation and user and other inputs received by the device . Context commonly refers to one or more of the particular application s selected by the gesture and the portion s of the application currently executing whether the application is a single or multi screen application and whether the application is a multi screen application displaying one or more windows in one or more screens or in one or more stacks. Sensed location of the gesture commonly refers to whether the sensed set s of gesture location coordinates are on a touch sensitive display or a gesture capture region whether the sensed set s of gesture location coordinates are associated with a common or different display or screen and or what portion of the gesture capture region contains the sensed set s of gesture location coordinates.

A tap when received by an a touch sensitive display can be used for instance to select an icon to initiate or terminate execution of a corresponding application to maximize or minimize a window to reorder windows in a stack and to provide user input such as by keyboard display or other displayed image. A drag when received by a touch sensitive display can be used for instance to relocate an icon or window to a desired location within a display to reorder a stack on a display or to span both displays such that the selected window occupies a portion of each display simultaneously . A flick when received by a touch sensitive display or a gesture capture region can be used to relocate a window from a first display to a second display or to span both displays such that the selected window occupies a portion of each display simultaneously . Unlike the drag gesture however the flick gesture is generally not used to move the displayed image to a specific user selected location but to a default location that is not configurable by the user.

The pinch gesture when received by a touch sensitive display or a gesture capture region can be used to minimize or otherwise increase the displayed area or size of a window typically when received entirely by a common display to switch windows displayed at the top of the stack on each display to the top of the stack of the other display typically when received by different displays or screens or to display an application manager a pop up window that displays the windows in the stack . The spread gesture when received by a touch sensitive display or a gesture capture region can be used to maximize or otherwise decrease the displayed area or size of a window to switch windows displayed at the top of the stack on each display to the top of the stack of the other display typically when received by different displays or screens or to display an application manager typically when received by an off screen gesture capture region on the same or different screens .

The combined gestures of when received by a common display capture region in a common display or screen can be used to hold a first window stack location in a first stack constant for a display receiving the gesture while reordering a second window stack location in a second window stack to include a window in the display receiving the gesture. The combined gestures of when received by different display capture regions in a common display or screen or in different displays or screens can be used to hold a first window stack location in a first window stack constant for a display receiving the tap part of the gesture while reordering a second window stack location in a second window stack to include a window in the display receiving the flick or drag gesture. Although specific gestures and gesture capture regions in the preceding examples have been associated with corresponding sets of functional results it is to be appreciated that these associations can be redefined in any manner to produce differing associations between gestures and or gesture capture regions and or functional results.

The memory may store and the processor may execute one or more software components. These components can include at least one operating system OS an application manager a desktop and or one or more applications and or from an application store . The OS can include a framework one or more frame buffers one or more drivers previously described in conjunction with and or a kernel . The OS can be any software consisting of programs and data which manages computer hardware resources and provides common services for the execution of various applications . The OS can be any operating system and at least in some embodiments dedicated to mobile devices including but not limited to Linux ANDROID iPhone OS IOS WINDOWS PHONE 7 etc. The OS is operable to provide functionality to the phone by executing one or more operations as described herein.

The applications can be any higher level software that executes particular functionality for the user. Applications can include programs such as email clients web browsers texting applications games media players office suites etc. The applications can be stored in an application store which may represent any memory or data storage and the management software associated therewith for storing the applications . Once executed the applications may be run in a different area of memory .

The framework may be any software or data that allows the multiple tasks running on the device to interact. In embodiments at least portions of the framework and the discrete components described hereinafter may be considered part of the OS or an application . However these portions will be described as part of the framework but those components are not so limited. The framework can include but is not limited to a Multi Display Management MDM module a Surface Cache module a Window Management module an Input Management module a Task Management module an Application Model Manager a Display Controller one or more frame buffers a task stack one or more window stacks which is a logical arrangement of windows and or desktops in a display area and or an event buffer .

The MDM module includes one or more modules that are operable to manage the display of applications or other data on the screens of the device. An embodiment of the MDM module is described in conjunction with . In embodiments the MDM module receives inputs from the other OS components such as the drivers and from the applications to determine continually the state of the device . The inputs assist the MDM module in determining how to configure and allocate the displays according to the application s preferences and requirements and the user s actions. Once a determination for display configurations is made the MDM module can bind the applications to a display. The configuration may then be provided to one or more other components to generate a window with a display.

The Surface Cache module includes any memory or storage and the software associated therewith to store or cache one or more images of windows. A series of active and or non active windows or other display objects such as a desktop display can be associated with each display. An active window or other display object is currently displayed. A non active windows or other display objects were opened and at some time displayed but are now not displayed. To enhance the user experience before a window transitions from an active state to an inactive state a screen shot of a last generated image of the window or other display object can be stored. The Surface Cache module may be operable to store a bitmap of the last active image of a window or other display object not currently displayed. Thus the Surface Cache module stores the images of non active windows or other display objects in a data store.

In embodiments the Window Management module is operable to manage the windows or other display objects that are active or not active on each of the displays. The Window Management module based on information from the MDM module the OS or other components determines when a window or other display object is visible or not active. The Window Management module may then put a non visible window or other display object in a not active state and in conjunction with the Task Management module Task Management suspends the application s operation. Further the Window Management module may assign through collaborative interaction with the MDM module a display identifier to the window or other display object or manage one or more other items of data associated with the window or other display object . The Window Management module may also provide the stored information to the application the Task Management module or other components interacting with or associated with the window or other display object . The Window Management module can also associate an input task with a window based on window focus and display coordinates within the motion space.

The Input Management module is operable to manage events that occur with the device. An event is any input into the window environment for example a user interface interactions with a user. The Input Management module receives the events and logically stores the events in an event buffer . Events can include such user interface interactions as a down event which occurs when a screen receives a touch signal from a user a move event which occurs when the screen determines that a user s finger is moving across a screen s an up event which occurs when the screen determines that the user has stopped touching the screen etc. These events are received stored and forwarded to other modules by the Input Management module . The Input Management module may also map screen inputs to a motion space which is the culmination of all physical and virtual display available on the device.

The motion space is a virtualized space that includes all touch sensitive displays tiled together to mimic the physical dimensions of the device . For example when the device is unfolded the motion space size may be 960 800 which may be the number of pixels in the combined display area for both touch sensitive displays . If a user touches on a first touch sensitive display on location 40 40 a full screen window can receive touch event with location 40 40 . If a user touches on a second touch sensitive display with location 40 40 the full screen window can receive touch event with location 520 40 because the second touch sensitive display is on the right side of the first touch sensitive display so the device can offset the touch by the first touch sensitive display s width which is 480 pixels. When a hardware event occurs with location info from a driver the framework can up scale the physical location to the motion space because the location of the event may be different based on the device orientation and state. The motion space may be as described in U.S. patent application Ser. No. 13 187 026 filed Jul. 20 2011 entitled Systems and Methods for Receiving Gesture Inputs Spanning Multiple Input Devices which is hereby incorporated by reference in its entirety for all that it teaches and for all purposes.

A task can be an application and a sub task can be an application component that provides a window with which users can interact to do something such as dial the phone take a photo send an email or view a map. Each task may be given a window in which to draw a user interface. The window typically fills a display for example touch sensitive display but may be smaller than the display and float on top of other windows. An application usually consists of multiple sub tasks that are loosely bound to each other. Typically one task in an application is specified as the main task which is presented to the user when launching the application for the first time. Each task can then start another task or sub task to perform different actions.

The Task Management module is operable to manage the operation of one or more applications that may be executed by the device. Thus the Task Management module can receive signals to launch suspend terminate etc. an application or application sub tasks stored in the application store . The Task Management module may then instantiate one or more tasks or sub tasks of the application to begin operation of the application . Further the Task Management Module may launch suspend or terminate a task or sub task as a result of user input or as a result of a signal from a collaborating framework component. The Task Management Module is responsible for managing the lifecycle of applications tasks and sub task from when the application is launched to when the application is terminated.

The processing of the Task Management Module is facilitated by a task stack which is a logical structure associated with the Task Management Module . The task stack maintains the state of all tasks and sub tasks on the device . When some component of the operating system requires a task or sub task to transition in its lifecycle the OS component can notify the Task Management Module . The Task Management Module may then locate the task or sub task using identification information in the task stack and send a signal to the task or sub task indicating what kind of lifecycle transition the task needs to execute. Informing the task or sub task of the transition allows the task or sub task to prepare for the lifecycle state transition. The Task Management Module can then execute the state transition for the task or sub task. In embodiments the state transition may entail triggering the OS kernel to terminate the task when termination is required.

Further the Task Management module may suspend the application based on information from the Window Management Module . Suspending the application may maintain application data in memory but may limit or stop the application from rendering a window or user interface. Once the application becomes active again the Task Management module can again trigger the application to render its user interface. In embodiments if a task is suspended the task may save the task s state in case the task is terminated. In the suspended state the application task may not receive input because the application window is not visible to the user.

The frame buffer is a logical structure s used to render the user interface. The frame buffer can be created and destroyed by the OS kernel . However the Display Controller can write the image data for the visible windows into the frame buffer . A frame buffer can be associated with one screen or multiple screens. The association of a frame buffer with a screen can be controlled dynamically by interaction with the OS kernel . A composite display may be created by associating multiple screens with a single frame buffer . Graphical data used to render an application s window user interface may then be written to the single frame buffer for the composite display which is output to the multiple screens . The Display Controller can direct an application s user interface to a portion of the frame buffer that is mapped to a particular display thus displaying the user interface on only one screen or . The Display Controller can extend the control over user interfaces to multiple applications controlling the user interfaces for as many displays as are associated with a frame buffer or a portion thereof. This approach compensates for the multiple physical screens that are in use by the software component above the Display Controller .

The Application Manager is an application that provides a presentation layer for the window environment. Thus the Application Manager provides the graphical model for rendering by the Task Management Module . Likewise the Desktop provides the presentation layer for the Application Store . Thus the desktop provides a graphical model of a surface having selectable application icons for the Applications in the Application Store that can be provided to the Window Management Module for rendering.

Further the framework can include an Application Model Manager AMM . The Application Manager may interface with the AMM . In embodiments the AMM receives state change information from the device regarding the state of applications which are running or suspended . The AMM can associate bit map images from the Surface Cache Module to the tasks that are alive running or suspended . Further the AMM can convert the logical window stack maintained in the Task Manager Module to a linear film strip or deck of cards organization that the user perceives when the using the off gesture capture area to sort through the windows. Further the AMM may provide a list of executing applications to the Application Manager .

An embodiment of the MDM module is shown in . The MDM module is operable to determine the state of the environment for the device including but not limited to the orientation of the device whether the device is opened or closed what applications are executing how the applications are to be displayed what actions the user is conducting the tasks being displayed etc. To configure the display the MDM module interprets these environmental factors and determines a display configuration as described in conjunction with . Then the MDM module can bind the applications or other device components to the displays. The configuration may then be sent to the Display Controller and or the other components within the OS to generate the display. The MDM module can include one or more of but is not limited to a Display Configuration Module a Preferences Module a Device State Module a Gesture Module a Requirements Module an Event Module and or a Binding Module .

The Display Configuration Module determines the layout for the display. In embodiments the Display Configuration Module can determine the environmental factors. The environmental factors may be received from one or more other MDM modules or from other sources. The Display Configuration Module can then determine from the list of factors the best configuration for the display. Some embodiments of the possible configurations and the factors associated therewith are described in conjunction with .

The Preferences Module is operable to determine display preferences for an application or other component. For example an application can have a preference for Single or Dual displays. The Preferences Module can determine an application s display preference e.g. by inspecting the application s preference settings and may allow the application to change to a mode e.g. single screen dual screen max etc. if the device is in a state that can accommodate the preferred mode. However some user interface policies may disallow a mode even if the mode is available. As the configuration of the device changes the preferences may be reviewed to determine if a better display configuration can be achieved for an application .

The Device State Module is operable to determine or receive the state of the device. The state of the device can be as described in conjunction with . The state of the device can be used by the Display Configuration Module to determine the configuration for the display. As such the Device State Module may receive inputs and interpret the state of the device. The state information is then provided to the Display Configuration Module .

The Gesture Module is shown as part of the MDM module but in embodiments the Gesture module may be a separate Framework component that is separate from the MDM module . In embodiments the Gesture Module is operable to determine if the user is conducting any actions on any part of the user interface. In alternative embodiments the Gesture Module receives user interface actions from the configurable area only. The Gesture Module can receive touch events that occur on the configurable area or possibly other user interface areas by way of the Input Management Module and may interpret the touch events using direction speed distance duration and various other parameters to determine what kind of gesture the user is performing. When a gesture is interpreted the Gesture Module can initiate the processing of the gesture and by collaborating with other Framework components can manage the required window animation. The Gesture Module collaborates with the Application Model Manager to collect state information with respect to which applications are running active or paused and the order in which applications must appear when a user gesture is performed. The Gesture Module may also receive references to bitmaps from the Surface Cache Module and live windows so that when a gesture occurs it can instruct the Display Controller how to move the window s across the display . Thus suspended applications may appear to be running when those windows are moved across the display .

Further the Gesture Module can receive task information either from the Task Manage Module or the Input Management module . The gestures may be as defined in conjunction with . For example moving a window causes the display to render a series of display frames that illustrate the window moving. The gesture associated with such user interface interaction can be received and interpreted by the Gesture Module . The information about the user gesture is then sent to the Task Management Module to modify the display binding of the task.

The Requirements Module similar to the Preferences Module is operable to determine display requirements for an application or other component. An application can have a set display requirement that must be observed. Some applications require a particular display orientation. For example the application Angry Birds can only be displayed in landscape orientation. This type of display requirement can be determined or received by the Requirements Module . As the orientation of the device changes the Requirements Module can reassert the display requirements for the application . The Display Configuration Module can generate a display configuration that is in accordance with the application display requirements as provided by the Requirements Module .

The Event Module similar to the Gesture Module is operable to determine one or more events occurring with an application or other component that can affect the user interface. Thus the Event Module can receive event information either from the event buffer or the Task Management module . These events can change how the tasks are bound to the displays. The Event Module can collect state change information from other Framework components and act upon that state change information. In an example when the phone is opened or closed or when an orientation change has occurred a new message may be rendered in a secondary screen. The state change based on the event can be received and interpreted by the Event Module . The information about the events then may be sent to the Display Configuration Module to modify the configuration of the display.

The Binding Module is operable to bind the applications or the other components to the configuration determined by the Display Configuration Module . A binding associates in memory the display configuration for each application with the display and mode of the application. Thus the Binding Module can associate an application with a display configuration for the application e.g. landscape portrait multi screen etc. . Then the Binding Module may assign a display identifier to the display. The display identifier associated the application with a particular display of the device . This binding is then stored and provided to the Display Controller the other components of the OS or other components to properly render the display. The binding is dynamic and can change or be updated based on configuration changes associated with events gestures state changes application preferences or requirements etc.

With reference now to various types of output configurations made possible by the device will be described hereinafter.

It may be possible to display similar or different data in either the first or second portrait configuration . It may also be possible to transition between the first portrait configuration and second portrait configuration by providing the device a user gesture e.g. a double tap gesture a menu selection or other means. Other suitable gestures may also be employed to transition between configurations. Furthermore it may also be possible to transition the device from the first or second portrait configuration to any other configuration described herein depending upon which state the device is moved.

An alternative output configuration may be accommodated by the device being in a second state. Specifically depicts a third portrait configuration where data is displayed simultaneously on both the primary screen and the secondary screen . The third portrait configuration may be referred to as a Dual Portrait PD output configuration. In the PD output configuration the touch sensitive display of the primary screen depicts data in the first portrait configuration while the touch sensitive display of the secondary screen depicts data in the second portrait configuration . The simultaneous presentation of the first portrait configuration and the second portrait configuration may occur when the device is in an open portrait state . In this configuration the device may display one application window in one display or two application windows one in each display and one application window and one desktop or one desktop. Other configurations may be possible. It should be appreciated that it may also be possible to transition the device from the simultaneous display of configurations to any other configuration described herein depending upon which state the device is moved. Furthermore while in this state an application s display preference may place the device into bilateral mode in which both displays are active to display different windows in the same application. For example a Camera application may display a viewfinder and controls on one side while the other side displays a mirrored preview that can be seen by the photo subjects. Games involving simultaneous play by two players may also take advantage of bilateral mode.

The device manages desktops and or windows with at least one window stack as shown in . A window stack is a logical arrangement of active and or inactive windows for a multi screen device. For example the window stack may be logically similar to a deck of cards where one or more windows or desktops are arranged in order as shown in . An active window is a window that is currently being displayed on at least one of the touch sensitive displays . For example windows and are active windows and are displayed on touch sensitive displays and . An inactive window is a window that was opened and displayed but is now behind an active window and not being displayed. In embodiments an inactive window may be for an application that is suspended and thus the window is not displaying active content. For example windows and are inactive windows.

A window stack may have various arrangements or organizational structures. In the embodiment shown in the device includes a first stack associated with a first touch sensitive display and a second stack associated with a second touch sensitive display . Thus each touch sensitive display can have an associated window stack . These two window stacks may have different numbers of windows arranged in the respective stacks . Further the two window stacks can also be identified differently and managed separately. Thus the first window stack can be arranged in order from a first window to a next window to a last window and finally to a desktop which in embodiments is at the bottom of the window stack . In embodiments the desktop is not always at the bottom as application windows can be arranged in the window stack below the desktop and the desktop can be brought to the top of a stack over other windows during a desktop reveal Likewise the second stack can be arranged from a first window to a next window to a last window and finally to a desktop which in embodiments is a single desktop area with desktop under all the windows in both window stack and window stack . A logical data structure for managing the two window stacks may be as described in conjunction with .

Another arrangement for a window stack is shown in . In this embodiment there is a single window stack for both touch sensitive displays . Thus the window stack is arranged from a desktop to a first window to a last window . A window can be arranged in a position among all windows without an association to a specific touch sensitive display . In this embodiment a window is in the order of windows. Further at least one window is identified as being active. For example a single window may be rendered in two portions and that are displayed on the first touch sensitive screen and the second touch sensitive screen . The single window may only occupy a single position in the window stack although it is displayed on both displays .

Yet another arrangement of a window stack is shown in . The window stack is shown in three elevation views. In the top of the window stack is shown. Two sides of the window stack are shown in . In this embodiment the window stack resembles a stack of bricks. The windows are stacked on each other. Looking from the top of the window stack in only the top most windows in the window stack are seen in different portions of the composite display . The composite display represents a logical model for the entire display area of the device which can include touch sensitive display and touch sensitive display . A desktop or a window can occupy part or all of the composite display .

In the embodiment shown the desktop is the lowest display or brick in the window stack . Thereupon window window window and window are layered. Window window window and window only occupy a portion of the composite display . Thus another part of the stack includes window and windows through shown in section . Only the top window in any portion of the composite display is actually rendered and displayed. Thus as shown in the top view in window window and window are displayed as being at the top of the display in different portions of the window stack . A window can be dimensioned to occupy only a portion of the composite display to reveal windows lower in the window stack . For example window is lower in the stack than both window and window but is still displayed. A logical data structure to manage the window stack can be as described in conjunction with .

When a new window is opened the newly activated window is generally positioned at the top of the stack. However where and how the window is positioned within the stack can be a function of the orientation of the device the context of what programs functions software etc. are being executed on the device how the stack is positioned when the new window is opened etc. To insert the window in the stack the position in the stack for the window is determined and the touch sensitive display to which the window is associated may also be determined. With this information a logical data structure for the window can be created and stored. When user interface or other events or tasks change the arrangement of windows the window stack s can be changed to reflect the change in arrangement. It should be noted that these same concepts described above can be used to manage the one or more desktops for the device .

A logical data structure for managing the arrangement of windows or desktops in a window stack is shown in . The logical data structure can be any data structure used to store data whether an object record file etc. The logical data structure can be stored in any type of database or data storage system regardless of protocol or standard. In embodiments the logical data structure includes one or more portions fields attributes etc. that store data in a logical arrangement that allows for easy storage and retrieval of the information. Hereinafter these one or more portions fields attributes etc. shall be described simply as fields. The fields can store data for a window identifier dimensions a stack position identifier a display identifier and or an active indicator . Each window in a window stack can have an associated logical data structure . While only a single logical data structure is shown in there may be more or fewer logical data structures used with a window stack based on the number of windows or desktops in the stack as represented by ellipses . Further there may be more or fewer fields than those shown in as represented by ellipses .

A window identifier can include any identifier ID that uniquely identifies the associated window in relation to other windows in the window stack. The window identifier can be a globally unique identifier GUID a numeric ID an alphanumeric ID or other type of identifier. In embodiments the window identifier can be one two or any number of digits based on the number of windows that can be opened. In alternative embodiments the size of the window identifier may change based on the number of windows opened. While the window is open the window identifier may be static and remain unchanged.

Dimensions can include dimensions for a window in the composite display . For example the dimensions can include coordinates for two or more corners of the window or may include one coordinate and dimensions for the width and height of the window. These dimensions can delineate what portion of the composite display the window may occupy which may the entire composite display or only part of composite display . For example window may have dimensions that indicate that the window will occupy only part of the display area for composite display as shown in through E. As windows are moved or inserted in the window stack the dimensions may change.

A stack position identifier can be any identifier that can identify the position in the stack for the window or may be inferred from the window s control record within a data structure such as a list or a stack. The stack position identifier can be a GUID a numeric ID an alphanumeric ID or other type of identifier. Each window or desktop can include a stack position identifier . For example as shown in window in stack can have a stack position identifier of identifying that window is the first window in the stack and the active window. Similarly window can have a stack position identifier of representing that window is the third window in the stack . Window can also have a stack position identifier of representing that window is the first window in the second stack . As shown in window can have a stack position identifier of window rendered in portions and can have a stack position identifier of and window can have a stack position identifier of . Thus depending on the type of stack the stack position identifier can represent a window s location in the stack.

A display identifier can identify that the window or desktop is associated with a particular display such as the first display or the second display or the composite display composed of both displays. While this display identifier may not be needed for a multi stack system as shown in the display identifier can indicate whether a window in the serial stack of is displayed on a particular display. Thus window may have two portions and in . The first portion may have a display identifier for the first display while the second portion may have a display identifier for the second display . However in alternative embodiments the window may have two display identifier that represent that the window is displayed on both of the displays or a display identifier identifying the composite display. In another alternate embodiment the window may have a single display identifier to represent that the window is displayed on both of the displays .

Similar to the display identifier an active indicator may not be needed with the dual stack system of as the window in stack position is active and displayed. In the system of the active indicator can indicate which window s in the stack is being displayed. Thus window may have two portions and in . The first portion may have an active indicator while the second portion may also have an active indicator . However in alternative embodiments window may have a single active indicator . The active indicator can be a simple flag or bit that represents that the window is active or displayed.

An embodiment of a method for creating a window stack is shown in . While a general order for the steps of the method is shown in . Generally the method starts with a start operation and ends with an end operation . The method can include more or fewer steps or can arrange the order of the steps differently than those shown in . The method can be executed as a set of computer executable instructions executed by a computer system and encoded or stored on a computer readable medium. Hereinafter the method shall be explained with reference to the systems components modules software data structures user interfaces etc. described in conjunction with .

A multi screen device can receive activation of a window in step . In embodiments the multi screen device can receive activation of a window by receiving an input from the touch sensitive display or the configurable area or a gesture capture region or or some other hardware sensor operable to receive user interface inputs. The processor may execute the Task Management Module may receive the input. The Task Management Module can interpret the input as requesting an application task to be executed that will open a window in the window stack.

In embodiments the Task Management Module places the user interface interaction in the task stack to be acted upon by the Display Configuration Module of the Multi Display Management Module . Further the Task Management Module waits for information from the Multi Display Management Module to send instructions to the Window Management Module to create the window in the window stack.

The Multi Display Management Module upon receiving instruction from the Task Management Module determines to which touch portion of the composite display the newly activated window should be associated in step . For example window is associated with the a portion of the composite display In embodiments the device state module of the Multi Display Management Module may determine how the device is oriented or in what state the device is in e.g. open closed portrait etc. Further the preferences module and or requirements module may determine how the window is to be displayed. The gesture module may determine the user s intentions about how the window is to be opened based on the type of gesture and the location of where the gesture is made.

The Display Configuration Module may use the input from these modules and evaluate the current window stack to determine the best place and the best dimensions based on a visibility algorithm to open the window. Thus the Display Configuration Module determines the best place to put the window at the top of the window stack in step . The visibility algorithm in embodiments determines for all portions of the composite display which windows are at the top of the stack. For example the visibility algorithm determines that window window and window are at the top of the stack as viewed in . Upon determining where to open the window the Display Configuration Module can assign a display identifier and possibly dimensions to the window. The display identifier and dimensions can then be sent back to the Task Management Module . The Task Management Module may then assign the window a stack position identifier indicating the windows position at the top of the window stack.

In embodiments the Task Management Module sends the window stack information and instructions to render the window to the Window Management Module . The Window Management Module and the Task Management Module can create the logical data structure in step . Both the Task Management Module and the Window Management Module may create and manage copies of the window stack. These copies of the window stack can be synchronized or kept similar through communications between the Window Management Module and the Task Management Module . Thus the Window Management Module and the Task Management Module based on the information determined by the Multi Display Management Module can assign dimensions a stack position identifier e.g. window window etc. a display identifier e.g. touch sensitive display touch sensitive display composite display identifier etc and an active indicator which is generally always set when the window is at the top of the stack. The logical data structure may then be stored by both the Window Management Module and the Task Management Module . Further the Window Management Module and the Task Management Module may thereinafter manage the window stack and the logical data structure s .

An embodiment of a method for executing an application such as a phone application is shown in . While a general order for the steps of the method is shown in . Generally the method starts with a start operation and ends with an end operation . The method can include more or fewer steps or can arrange the order of the steps differently than those shown in . The method can be executed as a set of computer executable instructions executed by a computer system and encoded or stored on a computer readable medium. Hereinafter the method shall be explained with reference to the systems components modules software data structures user interfaces etc. described in conjunction with .

An application such as a phone application is executed in step . In embodiments a processor receives indication to execute an application through a user interface etc. The indication can be a selection of an icon associated with the application. In other embodiments the indication can be a signal generated from another application or event such as receiving a phone call or other communication which causes the application to execute automatically. The processor can retrieve the application from the application store and begin its execution. In executing the application a user interface can be generated for a user.

In creating a user interface the application can begin executing to create a manifest in step . A manifest is a data structure that indicates the capabilities of the application . The manifest can generally be created from the resources in the resources directory of the application . The resources directory can indicate the types of modes locations or other indications for how the user interface should be configured in the multi display device . For example the several modes can include classic mode that indicates that the application is capable of being displayed on a single screen or display dual mode that indicates that the application is capable of being displaced on two or more displays and max mode that indicates the application is capable of being displayed or desires to be displayed across multiple displays and and or bilateral mode that indicates that the application is capable of being displayed on 2 or more displays and when the device is in easel mode see .

Similarly the manifest can include a desired or allowed location within the displays . The possible locations can include left which indicates that the application desires to be displayed on the left display right which indicates that the application desires to be displayed on the right display and or other indications of where a location should be including possible top and or bottom of one or more of the displays .

The application can also indicate that it desires to be displayed in a minimum window which is a window that occupies less than the full area of a single display. There may be other modes possible for the application which may be included in the manifest. The manifest can be sent from the application to the multi display management module .

The multi display management module can receive the manifest in step . In receiving the manifest the multi display management module can use the information to determine a display binding for the application . The manifest may be received more than once from the application based on changes in how the application is being executed where the application desires to have a different display setting for the new mode. Thus with the manifest the application can indicate to the multi display management module how best to or what is the desired for the application s user interface. The multi display management module can use the information in the manifest to determine the best fit for the user interface depending on how the device is currently configured.

The multi display management module can determine the application display mode in step . Here the multi display management module receives or retrieves an indication of the device configuration. For example the multi display management module can determine if the device is in single display configuration see or E dual display configuration see bilateral display configuration see or one of the other display configurations see .

Further the multi display management module can determine if the device is in a portrait or landscape orientation. With this information the multi display management module may then consider the capabilities or preferences listed for the application in the received manifest. The combined information may then allow the multi display management module to determine a display binding. The display binding can include which of the one or more displays and or are going to be used to display the application s user interface s . For example the multi display management module can determine that the primary display the secondary display or all displays and of the device will be used to display the application s user interface.

The display modes setting can be assigned by creating or setting a number in the display binding. This number can be for the primary display for the secondary display or for dual displays and . The display mode setting can also indicate if the application should display the user interface in portrait or landscape orientation. Further there may be other settings for example providing a max mode or other setting that may indicate how the application is to be displayed on the device. The display binding information is stored in a data structure to create and set a binding in step .

The established display binding may then be provided by the multi display management module to the application in step . The provided display binding data structure can become an attribute of the application . An application may thereinafter store the display binding attribute in the memory of the device . The application with the display binding may then generate a user interface based on this display binding. The application may be unaware of the position of the display but may be able to determine from the display binding the size of the available user interface to generate a window that has particular characteristics for that display setting.

When a configuration change happens to the device the multi display management module may change the display binding and send a new display binding to the application . In embodiments the multi display management module may indicate to the application that there is a new binding or in other embodiments the application may request a display configuration change or a new display binding in which case the multi display management module may send a new display binding to the application . Thus the multi display management module can change the configuration of the display for the application by altering the display binding for the application during the execution of that application

The multi display management module thereinafter while the application is executing can determine if there has been a configuration change to the device in step . The configuration change may be an event see triggered by one or more signals from one or more hardware sensor etc. For example if the device is changed from portrait to landscape orientation Hall effect sensors may indicate to the framework that a display configuration change has been made. Other changes may include transitions from a single display to a dual display configuration by opening the device. Other types of configuration changes may be possible and may be signaled to alert the multi display management module of the configuration change. If a configuration change has been made the method proceeds YES to step so that the multi display management module can determine new application display mode settings and create a new display binding which may be passed to the application . If there are no configuration changes the method precedes NO to step .

In step a new application mode change may be determined. Application mode changes can also occur in the application and thus the application can determine if something has occurred within the application that requires a different display setting. Modes are described hereinafter with respect to . The mode change can create a desire to change the display and thus require the application to generate a new manifest. If the application does sense a mode change or an event has occurred that requires a change in display setting the method proceeds YES back to step . At step a new manifest or preference is created by the application that may be received by the multi display management module to determine if the multi display management module can change the display binding. If it is possible to provide the preferred display the multi display management module can create a new display binding and send display binding back to the application and allow the application to alter its user interface. If no mode change is sensed or an event is not received to create a mode change the method proceeds NO to end operation .

An embodiment of a unified system is shown in . In embodiments the unified system includes a computer system and the device . The computer system may be as described in conjunction with . The device may be as described herein in conjunction with through . The device may be physically connected to the computer system with a docking cradle or other wired connection. In other embodiments the device and computer system may communicate or be connected wirelessly using a wireless system and or protocol e.g. Bluetooth 802.11g etc. . Upon connection of the device and the computer system the device can recognize the connection either manually through user input or automatically and functionally connect the device with the computer system to form the unified system. The unified system functions as to allow the device to communicate with interact with and or control the function of the computer system when functionally connected. As such when executed the unified system appears to be a single system where the device and the computer system function in concert. The components and or software that enable the unified system the communications or functions of the unified system and other description of the unified system is described in U.S. Provisional Applications 61 507 206 61 507 201 61 507 199 61 507 209 61 507 203 and 61 389 117 which are each incorporated herein by reference for all that they teach and for all purposes. Further the unified system is also described in U.S. patent application Ser. Nos. 12 948 585 and 12 948 676 which are each incorporated herein by reference for all that they teach and for all purposes.

The computer system may additionally include a computer readable storage media reader a communications system e.g. a modem a network card wireless or wired an infra red communication device etc. and working memory which may include RAM and ROM devices as described above. In some embodiments the computer system may also include a processing acceleration unit which can include a DSP a special purpose processor and or the like

The computer readable storage media reader can further be connected to a computer readable storage medium together and optionally in combination with storage device s comprehensively representing remote local fixed and or removable storage devices plus storage media for temporarily and or more permanently containing computer readable information. The communications system may permit data to be exchanged with the network and or any other computer described above with respect to the system . Moreover as disclosed herein the term storage medium may represent one or more devices for storing data including read only memory ROM random access memory RAM magnetic RAM core memory magnetic disk storage mediums optical storage mediums flash memory devices and or other machine readable mediums for storing information.

The computer system may also comprise software elements shown as being currently located within a working memory including an operating system and or other code such as program code implementing the components and software described herein. It should be appreciated that alternate embodiments of a computer system may have numerous variations from that described above. For example customized hardware might also be used and or particular elements might be implemented in hardware software including portable software such as applets or both. Further connection to other computing devices such as network input output devices may be employed.

An embodiment of a unified desktop is shown in . The unified desktop is the user interface for the unified system . Thus the unified desktop is formed from the user interface in the screen associated with the computer system and the user interface in the screen s associated with the device . As a unified desktop the user interface and function together to provide parallel displays exchange windows or other user interface elements and generally present a cohesive user interface across both the computers system and the device . In other words the unified desktop spans or is provided over at least one of the screens of the device and the screen of the computer system . The device can assume the form factor and function of both device and the computer system the design of the unified desktop can provide a seamless user experience across the device and the computer system enabling the user to access shared content manage applications and peripherals regardless of which system or presents the user interface action. Hereinafter specific user interface actions related to the unified desktop shall be presented.

A portion of the unified desktop is shown in . More particularly the user interface of the unified desktop is shown where a set of status indicators are provided in a portion of the unified desktop . The status indicators in the portion provide information about both the device and the computer system . In embodiments the portion is provided in only one of the user interface or user interface . Thus status indicators associated with both the device and the computer system are shown together in a screen or associated with only the device or the computer system .

The status indicators shown in include a wireless fidelity Wi Fi indicator a Bluetooth indicator a Network Traffic indicator a signal strength indicator a battery indicator a power menu indicator and a time indicator . In embodiments one or more of the indicators may be selectable to provide further information or user interface devices that may be selected to configure the device and or the computer system . For example as shown in the battery indicator has been selected which caused the device to provide a drop down user interface device in the user interface . The drop down user interface device provides further information about the amount of device battery life and a user interface device that can be selected to access the power settings for the device . Thus the user can configure the device or the computer system by accessing menus or other user interface through the status indicators through . Some of the possible status indicators that may be displayed for the device and the menus or other information associated with the status indicators is provided in the table below 

Some of the possible status indicators that may be displayed for the computer system and the menus or other information associated with the status indicators is provided in the table below 

Other possible status indicators are contemplated and included as one skilled in the art would understand.

A user interface with other status indicators is shown in . User interface is associated with the device . The user interface may also include a portion that displays status indicators. In embodiments the portion may include the same or different status indicators from portion and may display the status indicators substantially simultaneously with portion . For example portion may include a Network Traffic indicator a signal strength indicator a battery indicator and a time indicator . However portion may include an alarm indicator that is not shown in portion . Thus the portions and can show the same indicators different indicators and in some embodiments indicators associated with only the device or the computer system .

An embodiment of a method for providing status indicators is shown in . While a general order for the steps of the method is shown in . Generally the method starts with a start operation and ends with an end operation . The method can include more or fewer steps or can arrange the order of the steps differently than those shown in . The method can be executed as a set of computer executable instructions executed by a computer system and encoded or stored on a computer readable medium. Hereinafter the method shall be explained with reference to the systems components modules software data structures user interfaces etc. described in conjunction with .

A computer system and a device can be connected physically electrically wirelessly etc. The device can automatically recognize the connection and functionally connect the device with the computer system to form a unified system in step . In other embodiments a user may provide input to functionally connect the device with the computer system . In response to the function connection the device can generate a unified desktop in step . The unified desktop can expand or create a single user interface for the unified system across the screens of the device and the computer system .

The device may then determine status indicators that would need to be displayed for the user in step . In embodiments the device can request or discover the status indicators associated with the computer system . The status indicators associated with the computer system may be incorporated into a data structure or combined with the status indicators associated with the device . The combined set of status indicators may then be provided in the unified desktop in step . For example the combined set of status indicators can be displayed in a portion of the user interface . Thus in a single area of the unified desktop and possibly only on one screen of either the device or the computer system the device can provide status indicators for both the device and the computer system .

An embodiment of a method for providing status indicators is shown in . While a general order for the steps of the method is shown in . Generally the method starts with a start operation and ends with an end operation . The method can include more or fewer steps or can arrange the order of the steps differently than those shown in . The method can be executed as a set of computer executable instructions executed by a computer system and encoded or stored on a computer readable medium. Hereinafter the method shall be explained with reference to the systems components modules software data structures user interfaces etc. described in conjunction with .

The processor of the device can receive a selection of a status indicator in step . A selection can be a user interface action conducted on a status indicator. For example a user may hover a pointer over status indicator . In another example the user may click the pointer on the status indicator to select it. Regardless the user conducts a user interface action on a status indicator to generate an input into the unified desktop . In response to receiving the selection by the user the device can provide information and or a user interface device in the unified desktop in step . Information can include more detail about a status as described in the tables above. For example upon selecting the status indicator the device can provide more detailed information that the battery charge is at 50 as shown in information in the drop down . A user interface device may be a selectable icon or other user interface display that will allow the user to access further functionality. For example the user interface display in the drop down display can be selected by the user s pointer. Selection of the user interface display allows the user to access power settings for the device . Other menus or functions can be accessed through the status indicators as shown in the tables above.

The processor of the device can receive a selection of a user interface device in step . For example the user may select user interface display by clicking on or near the user interface display . Upon receiving this selection the device can provide a menu or other functionality associated with the status indicator in step . In embodiments the device presents a menu that can alter or address the status indicated by the status indicator. The menus or functionality that may be accessed may be as described in the tables above. For example by selecting user interface display the device may provide a power settings menu to the user that allows the user to change the power settings of the device . The user may modify a function of the device or the computer system by interaction with the provided menu or functionality. For example the user may change the duration of inactivity required before a screen blacks out in the power settings menu.

A unified desktop interface is shown with a window showing a personal computing application user interface and a freeform window in . A freeform window is a shell that can encapsulate a user interface for a device application. A device application can be any application that typically executes on the device . Thus the device application may be configured to execute in a mobile device environment but not in a personal computing environment. The device application can be specific to the device and does not execute on the computer system. For example gestures received with the device can affect the device application but may not be received or understood in the personal computing environment. Device applications can include a phone application a text messaging application a utilities application a game application or a mobile application. The device application may be executed by the device receiving a user selection of the device application. The selection may be a user selection of a shortcut displayed in the unified desktop. An example of a shortcut is shown in the user interface . The shortcut allows a device application to be opened in the personal computing environment like other personal computing applications.

The freeform window allows the device application user interface to behave as a typical window in a computer system environment. For example as shown in freeform window allows the user interface of the device application to display at least partially over another window . In other embodiments the freeform window allows the user interface of the device application to display at least partially behind another window not shown . Thus the freeform window provides display functionality to the user interface that would not normally be functional for a device application.

Upon executing the device application the device application can generate a user interface which can be incorporated into the freeform window . An embodiment of the freeform window is shown in . The freeform window includes a portion that includes controls . The controls can cause the user interface to complete actions associated with a personal computing environment. For example control can cause the device to close the user interface and stop execution of the device application associated therewith. Control can cause the device to minimize or shrink the user interface of the device application. Control can cause the device to maximize the user interface the user interface of the device application. A user may also user handles or other controls to expand or contract the freeform window and thus the user interface .

The freeform window can provide one or more user interface devices that allow the user interface to provide functionality such as features or controls in the freeform window . For example a first user interface device may be a control or device to access previous information displayed by the user interface of the device application. A second user interface device can provide a control to exit or stop execution of the device application. A third user interface device can provide a feature to expand the user interface the device application into the entire available space of the freeform window . A fourth user interface device may provide a control to change the user interface to a second user interface that displays other information. Other user interface devices that provide other functionality are contemplated as one skilled in the art would understand.

An embodiment of a method for providing a freeform window is shown in . While a general order for the steps of the method is shown in . Generally the method starts with a start operation and ends with an end operation . The method can include more or fewer steps or can arrange the order of the steps differently than those shown in . The method can be executed as a set of computer executable instructions executed by a computer system and encoded or stored on a computer readable medium. Hereinafter the method shall be explained with reference to the systems components modules software data structures user interfaces etc. described in conjunction with .

A computer system and a device can be connected physically electrically wirelessly etc. The device can automatically recognize the connection and functionally connect the device with the computer system to form a unified system in step . In other embodiments a user may provide input to functionally connect the device with the computer system . In response to the function connection the device can generate a unified desktop in step . The unified desktop can expand or create a single user interface for the unified system across the screens of the device and the computer system .

The device may then execute a device application in step . In embodiments the device may receive a selection of a device application from the unified desktop . For example the user may select a shortcut . Regardless the device can execute the selected device application in the computer system environment. When the device executes the device application a freeform window may be created to encapsulate or display the user interface of the device application. Thus the device displays the user interface of the device application in the freeform window in step .

An embodiment of a method for providing status indicators is shown in . While a general order for the steps of the method is shown in . Generally the method starts with a start operation and ends with an end operation . The method can include more or fewer steps or can arrange the order of the steps differently than those shown in . The method can be executed as a set of computer executable instructions executed by a computer system and encoded or stored on a computer readable medium. Hereinafter the method shall be explained with reference to the systems components modules software data structures user interfaces etc. described in conjunction with .

The processor of the device can receive a selection of a device application in step . A selection can be a user interface action conducted on a shortcut or other user interface device. Regardless the user conducts a user interface action to execute a device application. In response to receiving the selection by the user the device can determine any device application feature controls or other functionality to be provided for the user interface in step . The features controls etc. can be functions that conduct and action or complete a process for the device application and may not function in a typical computer system environment. These functions can include expanding or contracting the window providing alternative displays entering information executing sub processes etc. Some possible features or controls are as discussed with .

The processor of the device can provide a freeform window for the user interface of the device application in step . The freeform window can be as described in . As part of the freeform window the device may provide one or more user interface devices that enable the features or controls determined by the device in step . The user interface devices may be as described in conjunction with .

A table containing at least some of the possible sleep states and how to interact with the system in the state is shown in . In embodiments there may be three states a simple sleep state represented in column a screen lock state represented in column and or a passcode lock state represented in column . In embodiments there may be more or fewer states. A sleep state may be chosen by a user and affected when the device receives a selection of the sleep state from the user.

The table shows rules that govern the sleep state and how to wake from the selected sleep state. In a sleep state the user interface is off without anything being displayed and the wake action is any interaction with the user interfaces of the unified desktop. In a screen lock state the user interface is off and the user interface of at least the device requires a specified and predetermined user interaction e.g. moving a bar on the user interface to unlock the state. Generally the bar or other user interface device that receives the specified user interaction is presented to the user after the user provides an initial user interface interaction to the unified desktop. In a passcode state the system requires the entry of a passcode e.g. a numeric code a visual code etc. to exit the state. As with the screen lock state a passcode user interface device that receives the passcode is presented to the user after the user provides an initial user interface interaction to the unified desktop. Thus all the states appear as that shown in but require different input to exit the state.

At least two events can cause the system to leave the states mentioned above. The first event may be a wake event represented in row . The second event may be a phone call represented in row . When these events occur with the correct user input shown in the body of table the system may exit from the state and allow the user to use or interact with the system.

An embodiment of a method for waking the system is shown in . While a general order for the steps of the method is shown in . Generally the method starts with a start operation and ends with an end operation . The method can include more or fewer steps or can arrange the order of the steps differently than those shown in . The method can be executed as a set of computer executable instructions executed by a computer system and encoded or stored on a computer readable medium. Hereinafter the method shall be explained with reference to the systems components modules software data structures user interfaces etc. described in conjunction with .

A system may sleep in step . The sleep state may include the states as described in conjunction with or other states that render the system inactive. The system may appear as shown in where in user interface for the computer system and the user interface for the device are inactive and do not display anything. The sleep state may be entered after the expiration of a period of time automatically in response to an action or event or in response to user interaction. Thus the sleep state can render the system unusable until it is awakened or unlocked.

The device or the computer system may receive an event in step . An event may be as described in conjunction with . For example the event may be a wake action received by a user. In alternative embodiments the event may be a received phone call. Either event may be received with the correct user input to wake the system . The correct user input is based on the state in which the device currently resides.

Thus the system may determine the state of the system in step . The state may be set by the user or be automatically set. For example a user may determine that a passcode lock is necessary and enter the passcode with the setting. However if the user makes no change to the desired sleep state the device may determine to use the sleep state . Regardless the system determines which state has been set when the device sleeps.

The system may then determine if the correct user interface input associated with the determined state has been entered to wake the system in step . As shown in table there is different input based on the event and the state. If the system is in a simple sleep state then any input to the device or computer system may wake the system . Thus the user may hit a key on the keyboard push a user interface device on the device touch the touch sensitive display the gesture capture area or another touch sensitive area on the device or make some other input into the system . In the screen lock state the device may require the user to complete some form of user input e.g. moving a slider bar or other orchestrated movement on the touch sensitive display either in addition to a first interaction or by itself to unlock the device . The computer system may not require any additional input. Thus a first gesture may wake the computer system while a second gesture is needed to unlock the device interface . If the correct input is received the method proceeds YES to step .

However if the incorrect input is received the method proceeds NO to step where the system may inform the user of the incorrect input. Thus a user interface may be provided that indicates that the input was incorrect for example a pop up message that states The password or username is incorrect. In other embodiments the system may revert to the locked state without unlocking the device which can indicate to the user that the input was incorrect. In step the system may wake. Thus the system can transition for example for the display shown in to that shown in . The user may then have complete access to the system and view the desktop or open application after waking the system .

An embodiment of a method for docking the device with the computer system to form a unified system is shown in . While a general order for the steps of the method is shown in . Generally the method starts with a start operation and ends with an end operation . The method can include more or fewer steps or can arrange the order of the steps differently than those shown in . The method can be executed as a set of computer executable instructions executed by a computer system and encoded or stored on a computer readable medium. Hereinafter the method shall be explained with reference to the systems components modules software data structures user interfaces etc. described in conjunction with and . In particular show embodiments of user interfaces associated with the process of docking.

A device and a computer system may be provided in step . The device may be a mobile device as described herein. In embodiments the device may display one or more user interfaces and or before docking occurs. The computer system can include a display that presents a user interface . The user interface can include any window or other display. In embodiments the user interface shows a slide show in before docking. After presenting the device and the computer system the device may dock with the computer system in step .

Docking the device may include electrically connecting the device with the computer system . The electrical connection may be made with a docking cradle a wire interface a wireless interface or by other device or connection. Once docked the computer system may be controlled or managed by the device . Thus actions on the computer system may be handled by the device . Thus docking the device creates the unified system and presents the unified interface for the computer system and the device .

The behavior of the unified system as shown in and after docking may be governed by a set of docking rules as described in conjunction with . In embodiments the computer system display may hide one or more displays or windows presented before docking. Rather the computer system may present a desktop after docking in step . The desktop can display a unified desktop theme with icons or other user interface devices for example icon providing access to unified system functionality. Thus the computer system hides pre existing windows or displays to provide the unified desktop .

After docking the device software as described in may determine if any application displays or windows were displayed on the device before docking in step . The determination may include the processor checking the display configuration in the frame buffers or other type of check. If there were displays presented on the device before docking the method may proceed YES to step . If there were on displays presented on the device before docking the method may proceed NO to step where the unified desktop is also provided on the display s of the device as seen in .

If there were on displays e.g. display and or display presented on the device before docking the displays and or may be migrated from the device interface to the computer system display as shown in . In embodiments new freeform windows or big brother applications are invoked to migrate the displays. Thus a new instance of the application window or interface is opened in the display of the computer system. In other embodiments the display buffer is simply changed to reflect the migration. Thus displays on the device preempt any displays on the computer system after docking.

An embodiment of a method for undocking the device with the computer system to is shown in . While a general order for the steps of the method is shown in . Generally the method starts with a start operation and ends with an end operation . The method can include more or fewer steps or can arrange the order of the steps differently than those shown in . The method can be executed as a set of computer executable instructions executed by a computer system and encoded or stored on a computer readable medium. Hereinafter the method shall be explained with reference to the systems components modules software data structures user interfaces etc. described in conjunction with and . In particular show embodiments of user interfaces associated with the process of undocking.

A device and a computer system may be docked. The device may be a mobile device as described herein. In embodiments the device may display one or more user interfaces before undocking occurs. The computer system can include a display that presents a display before undocking. Thereinafter the device may be undocked from the computer system in step .

Undocking the device may include electrically disconnecting the device from the computer system . The electrical connection may be made with a docking cradle a wire interface a wireless interface or by other device or connection. To disconnect the device the device may be separated from the docking cradles or wire interface or the wireless interface may be disconnected. Once undocked the computer system may no longer be controlled or managed by the device . Thus the computer system may resume control of its own actions. Thus undocking the device dismantles the unified system and presents separate interfaces for the computer system and the device .

The behavior of the unified system as shown in after undocking may be governed by a set of undocking rules as described in conjunction with . In embodiments the computer system display may hide the unified desktop in step and display a window or other display that was hidden during docking as shown in . After undocking the device software as described in may determine if any application displays or windows were displayed on the device before undocking in step . The determination may include the processor checking the display configuration in the frame buffers or other type of check. If there were displays presented on the device before undocking the method may proceed YES to step . If there were on displays presented on the device before undocking the method may proceed NO to step where a device desktop is provided on the display s of the device .

If there were on displays e.g. display presented on the device before undocking the display s may be maintained on the device interface as shown in . However no windows or displays are migrated from the computer system to the device . Thus the device maintains its display characteristics after undocking.

A table containing at least some of the docking and undocking rules and how to interact with the system during docking and undocking is shown in . In embodiments there two sets of rules one set for docking represented in column and another for undocking represented in column . In embodiments there may be more or fewer rules. Further the docking rules may include a rule governing visible to visible transitions represented in column and a stickiness rule represented in column . The table shows rules that govern the components represented by column of the unified system. Thus the rules govern how the displays of the device and computer system are managed during docking and undocking. The device interfaces are represented in row while the computer system interfaces are represented in row .

During docking the user interface may display interfaces or a desktop for the computer system. However upon docking the interfaces and computer system desktop are hidden according to the visible to visible rules for the computer system. Further if there was a previous docking and undocking any application interfaces previously displayed during a past docking are re displayed according to the stickiness rules for the computer system. It should be noted that the device or computer system may store an indicator of the previously displayed interfaces at the last docking and access the indicators to reinstate the interfaces upon re docking. The device during docking will have interfaces displayed on the device moved to the display of the computer system according to the visible to visible rule.

During undocking any interfaces displayed on the device remain displayed on the device according to the undocking rules for the device. For the computer system applications displayed in the unified desktop are hidden but available if the device is re docked. The computer system may then display other interfaces associated with the computer system or the computer system desktop according to the undocking rules for the computer system.

An embodiment of a method for applying a visible to visible rule during docking of a device is shown in . While a general order for the steps of the method is shown in . Generally the method starts with a start operation and ends with an end operation . The method can include more or fewer steps or can arrange the order of the steps differently than those shown in . The method can be executed as a set of computer executable instructions executed by a computer system and encoded or stored on a computer readable medium. Hereinafter the method shall be explained with reference to the systems components modules software data structures user interfaces etc. described in conjunction with .

Device docks with a computer system PC . As shown in the device can include two displays and . The PC can include one display . When docked the displays and or can display a unified desktop which may display one or more application windows. As explained in conjunction with the unified system maintains visible windows from the device on the unified desktop. Before docking the PC and device may appear as shown in . As shown in the device may display a first window and a second window . The PC may display a first window and a second window . Further the device may display an application manager interface that displays windows e.g. on the first display of the device and an application manager interface that displays windows on the second display of the device . The application manager interfaces can also display windows e.g. that are not currently displayed on the displays .

A representation of a display buffer or data structure associated with the display is shown in . The data structure can store information about which windows are displayed before docking. For example data object is associated with window and data object is associated with window . Further a data object may be associated with a desktop displayed on the display . A window stack can represent the windows open active or inactive on the displays before docking the device . For example data object can represent window data object can represent window and data objects can represent the desktop on the device .

At some time thereinafter the device is docked with the PC to create the unified desktop as shown in . In response to docking the device the windows previously displayed on the PC can be hidden or closed in step . The desktop may then be provided on the PC display in step . A processor on the device can then determine if there were any windows active and displayed on the device before docking in step . The processor can scan the window stack to determine that windows and were open substantially simultaneously with the docking of the device .

After determining the windows that were open on the device before docking the processor can instruct the PC to create windows that are similar to or associated with the programs or applications that were being executed on the device . For example if an Internet Browser was being executed on the device a browser application can be opened on the PC . After creating the windows the active windows are moved to the created windows. In embodiments state information for the active windows can be used to provide a new instance of the window on the PC . Thus while to the user it appears the window is moved a new window is actually instantiated. As shown in windows are now open in display . The unified desktop may be shown on the PC and the device in step .

In response to the movement of the windows after docking the processor modifies the window stack in step . For example the processor removes the windows from the window stack as shown in . Further the window stack is changed to reflect that the unified desktop is displayed on the device display . Further the processor modifies the computer system display buffer in step . For example the display buffer is changed to reflect that windows are open on the PC display . The unified desktop may also be provided on the display as shown in the window stack.

While docked a window may be moved from the PC display to the device display as shown in . The display buffer can be changed to reflect the move as shown in . Thus the window is no longer shown in the window stack but shown in the window stack in . Upon undocking the window remains visible on the device . The other window is no longer shown. The display buffer for the PC can now display the PC desktop but no other windows. The window stack may remain unchanged as shown in .

An embodiment of a method for maintaining window stickiness during a re dock of a device is shown in . While a general order for the steps of the method is shown in . Generally the method starts with a start operation and ends with an end operation . The method can include more or fewer steps or can arrange the order of the steps differently than those shown in . The method can be executed as a set of computer executable instructions executed by a computer system and encoded or stored on a computer readable medium. Hereinafter the method shall be explained with reference to the systems components modules software data structures user interfaces etc. described in conjunction with .

Device docks with a computer system PC in step . The docking is as described herein. During the docking at least one window is opened on the unified desktop of the computing system in step . For example at least one of window or window are opened during the docked session. The docking of the unified system is represented by line . In embodiments a desktop is shown on the device . However the device might also have one or more open windows displayed on the device .

At some time thereinafter the device is undocked from the PC in step . The undocked device and computer system is shown in as system . The undocked system may be represented by the absence of line . Upon the occurrence of the undock event recognized by the device and or the computer system the device and or the computer system may save a data structure in step . The data structure is as shown in .

The data structure may include one or more data fields and may be stored as any type of data structure as described herein. In embodiments the data structure can include a timestamp of the last docking an identifier ID for a window and or status and or state information . The data structure can include more or fewer data items than that shown in as represented by ellipses . In embodiments the information contained in data structure is associated all windows that were open on the computer system before the last undocking. For example the data structure may include information about window and window as shown in .

Further other data fields such as window ID and state information may represent window which was open on the computer system before undocking. There may be more or fewer data fields for more or fewer windows that may have been open on the computer system before undocking as represented by ellipses . This data structure can be stored with the device and or the computer system . The undocked system as shown in may have one or more actions occur on the computer system or on the device while undocked. For example window may be opened on the device while the device is undocked from the unified system.

At some time thereinafter the device may be re docked with the computer system to recreate the unified system as shown in in system . Upon the occurrence of a re dock the computer system and or the device may recognize the re dock event and may access the data structure . Information from the data structure may be read to reopen one or more windows in step . For example as shown in window and window may be reopened as having been open during the last docked session. Thus the open windows at undocking are sticky or re open the next time that the device docks with the computer system . Further windows that were open on the device during undock may be moved to the computer system as represented by window being displayed on the computer system display . If a window is moved from the device to the computer system the desktop may be displayed on the device .

Embodiments of systems for creating a unified system for the device and PC are shown in . The software components or modules that provide for the unified system on a computer system are shown in . The systems and or for the device and the PC may be stored and executed in hardware as described herein. The software modules can include a first operating system and a second operating system . The two operating systems and may interact to create and manage the unified system. In embodiments the second operating system may control the functions of the device . The first operating system may control or direct the operations of the computer system . Thus the first operating system may communicate with the computer system interface that sends signals to the computing system through the docking hardware. Embodiments of the dual operating system are described in U.S. Provisional Patent Applications 61 507 199 filed Jul. 13 2011 entitled Dockable Mobile Software Architecture 61 507 201 filed Jul. 13 2011 entitled Cross environment communication framework 61 507 203 filed Jul. 13 2011 entitled Multi operating system 61 507 206 filed Jul. 13 2011 entitled Auto configuration of a docked system in a multi OS environment and 61 507 209 filed Jul. 13 2011 entitled Auto waking of a suspended secondary OS in a dockable system .

The modules on the computer system may be installed or stored upon the first docking of the device to the computer system . The modules can include a device interface that communicates with the computer interface . Thus the device interface can receive signals from the first operating system and may send signals or events to the first operating system . The device interface can communicate with an application programming interface . In turn the application programming interface API can communicate with the operating system for the computer system. The API can act as an intermediary that both controls and directs the computing system OS or changes the operation thereof. Thus the API can both subordinate normal computer system events for the PC and promote the events or signals sent from the device .

In embodiments the API may include one or more modules. For example the API can include an interceptor module a relay module an injector module and or a receiver module . The interceptor module may be operable to intercept events or processor executions that are put on the stack for the computer system processor. Thus the interceptor can erase delete or change the stack for the PC thus controlling what actions are conducted by the PC . Any events that occur on the PC that are placed into the stack may be intercepted by the interceptor and provided to the relay which may then relay the event through the device interface to the first operating system . The information sent from the relay allows the first operating system to respond to the event s for the PC .

Likewise signals from the OS to the PC may be received by a receiver . When the first operating system wants to control or have the personal computer conduct some action the first operating system may send a signal through the PC interface to the receiver . The receiver may then pass the signal onto the injector which may place the event or instruction into the stack for the computer operating system . Thus the injector communicates signals to the computer system OS to control its actions.

An embodiment of the method for receiving an event for the unified desktop at the device is shown in . While a general order for the steps of the method is shown in . Generally the method starts with a start operation and ends with an end operation . The method can include more or fewer steps or can arrange the order of the steps differently than those shown in . The method can be executed as a set of computer executable instructions executed by a computer system and encoded or stored on a computer readable medium. Hereinafter the method shall be explained with reference to the systems components modules software data structures user interfaces etc. described in conjunction with . The process may be described particularly in conjunction with .

The operating system may receive an event at the device in step . An event may be any type of user interface input or other occurrence that may require attention from the device . An event may require action by the device or some type of response. Once an event is received the second operating system can determine if the event is associated with the device in step . For example if the event is a user interface input that requires user interface output at the device then the event is related to the device . However if the event may be related to output on the personal computer the second operating system may determine that the event is related to the personal computer . If the event is related to the device the method proceeds YES to step . In contrast if the event is not associated with the device the method proceeds NO to step .

In step the second operating system can process the event for the device . The processing of an event by an operating system may be as understood in the art. Thus the processing of the event may involve user interface output or completing an instruction for an application. If the processing requires output the output may be displayed on the interface of the device in step .

If the event is related to the computer system operating system may send the event to the first operating system in step . The first operating system may process events similarly to the second operating system but may process the events for the personal computer . Thus the processing may include one or more steps or actions that may be unique to the computer system as opposed to the device . Thus the operating system processes the event for the computer system in step and sends the processed or executed actions through the computing system interface to the device interface . The instructions or completed processing actions may be received by the receiver and sent to the injector . The injector may send the completed instructions or actions for the personal computer operation system OS to create a display or some other output. The personal computer OS can then display the output in step . In this way the device can both control the functions of the device and the personal computer for an event received by the device but applying to either the device or the personal computer .

An embodiment of a method for processing an event received on a personal computer is shown in . While a general order for the steps of the method is shown in . Generally the method starts with a start operation and ends with an end operation . The method can include more or fewer steps or can arrange the order of the steps differently than those shown in . The method can be executed as a set of computer executable instructions executed by a computer system and encoded or stored on a computer readable medium. Hereinafter the method shall be explained with reference to the systems components modules software data structures user interfaces etc. described in conjunction with .

An event may be received on the personal computer in step . The event may be as described previously in conjunction with . The event may be received in the user interface of the personal computer . Upon receiving the event the interceptor may intercept event information from the memory stack of the PC OS and provide that information to the relay . The relay may then package the information to send to the device in step . The information may be sent in a message provided by the device interface to the computer interface and arriving at the first OS . The first OS may then process the event in step . The processing of the event may be the same or similar as that described in conjunction with . Further the output provided by the first OS may then be sent back to the PC OS as described in conjunction with .

An embodiment of the method for processing an event that may cross the personal computer and device is shown in . While a general order for the steps of the method is shown in . Generally the method starts with a start operation and ends with an end operation . The method can include more or fewer steps or can arrange the order of the steps differently than those shown in . The method can be executed as a set of computer executable instructions executed by a computer system and encoded or stored on a computer readable medium. Hereinafter the method shall be explained with reference to the systems components modules software data structures user interfaces etc. described in conjunction with .

An event may be received that crosses both the personal computer and the device in step . For example a user may desire to move a window from the device interface to the computer interface by dragging that window from one interface to the other. This type of event would cross the boundary of the personal computer and device and thus create an event that may occur in both the device and the personal computer .

The event may be received in the second OS . Further the interceptor may receiver or intercept the event in the personal computer OS . The intercepted event may be sent through the relay to the first OS in step . The first and second OS and may then communicate about the events to coordinate the timing of the information. Thus the determination may be made as to where an event started or stopped and which of the outputs should be processed first. Thus both the second OS and the first OS may process the event or events in step . The output may then be coordinated so as to provide a seamless user interaction for such an event. The second OS or the first OS may throttle the other operating system in the device to coordinate or to time the processing and then output of data as described in conjunction with .

An embodiment of the method for the device to be master of the unified system upon docking as described and shown in . While a general order for the steps of the method is shown in . Generally the method starts with a start operation and ends with an end operation . The method can include more or fewer steps or can arrange the order of the steps differently than those shown in . The method can be executed as a set of computer executable instructions executed by a computer system and encoded or stored on a computer readable medium. Hereinafter the method shall be explained with reference to the systems components modules software data structures user interfaces etc. described in conjunction with .

A docking event may be received in step . The docking event may be created when the device is connected to the computer system as shown by the line in . The docking event may occur in both the computer system and the device . In the device the second OS may begin communicating with the first OS and instruct the first OS to begin signaling the computer system to control the computer system s actions. Further in the computer system the application programming interface may begin to be executed and begin scanning or monitoring the stack of the personal computer OS to intercept or inject instructions into the memory stack for the operating system . In embodiments the first OS may send an instruction to the application programming interface to be executed. In other embodiments the docking signal or event may cause the PC OS to begin executing the application programming interface . Upon the execution of the API and the first OS the device controls the personal computer as the master in step . Thus any actions being conducted on either the device or the computer system can be executed or handled with the device .

Upon the device becoming master over the personal computer the computer system subordinates any functions the computer system normally executes independently in step . For example any windows or applications being executed by the personal computer before docking are hidden and those functions are paused while the device is docked. Thus any functions normally executed on the computer system are subordinated to the master control of the device . One such subordination may be the computer system hiding any display in step . The computer system displays a master or unified desktop on the display after having the computer system s other functions. Any windows or the previous desktop are hidden behind the unified system desktop or are replaced by the unified system desktop.

An embodiment of the unified system showing a triad control interface is shown in . The unified system includes the device interface and the computer system interface . As shown in the device interface a control window may be provided by user interface input or automatically through action of an application or other event. The user interface provides direct access to certain functionality including but not limited to a phone application an application launcher a file browser etc. The provided functionality in the unified desktop may be ported to the computer system display in a new and novel user interface area called the triad control. The triad control area may be a separate popup bar that is navigable by a user interface device for example a mouse that is hovered over a specific area of the user interface . Upon the recognition that the user desires to view the triad controls the triad control may appear on the user interface as if sliding from of the screen and onto the top of the display .

The triad control can include at least some of the same functionality as in user interface . For example the area shows several user selectable icons in the triad control . The user selectable icon can execute a file browser. The user selectable icon can execute an application launcher. The user selectable icon can execute an application manager. Further the user selectable icon can provide a phone application that is executed by the device but appears on the computer system display . Likewise a user selectable may provide a browser window that allows the user to browse the internet through the phone device . The selectable device icons provide functionality typically associated with the phone over the unified desktop by allowing access to those functions using a new selectable area for user icons in the triad control bar .

A series of windows shown in show a visual representation of how the triad control may be provided to a user in the user interface . As shown in a user interface device may be shown in user interface . The user interface device may be controllable by a mouse a finger on a touch sensor display or by some other hardware means. The represented arrow icon can be moved from the device s position as shown in to a position shown in . The position of the user interface device in can be recognized as being within a triad control area . Upon recognizing the presence and persistence of the user interface device in the triad control area the triad control bar may begin to present itself by appearing to slide from the side of the screen. Thus as the triad control bar slides the triad control bar will become completely displayed as shown in . If the user persists in maintaining the user interface device in the triad control area the triad control bar can continue to be displayed and to be selectable for the user.

An embodiment of the application launcher presented to the user after user selectable icon is selected in triad control bar is shown in . The user may select the user interface device or user selectable icon in the triad control . Upon selecting icon by user interface action the window may be presented the window presents selectable icons associated with applications that can be launched by the user. The applications may be applications executed in the phone or may be applications executed on a computer system. Thus the available applications for the user presented in the window on the unified desktop encompass applications executed both on the device and the computer system.

For example the group of applications provide icons associated with PC productivity applications that may be executed by the computer system. In alternative embodiments the PC productivity applications are applications executed by the device that may emulate computing system functionality. The applications shown in the group may be Android applications or other applications executed solely by the device. Thus by providing the application launcher menu the user can select one of the icons shown in either area or to launch an application in the unified desktop. Further the window may provide a search area where the user may provide search criteria that can search for a certain application within the application launcher .

An embodiment of a process for using the search function in the application launcher window is shown in . The user may have typed a letter c in the search window . Upon typing the letter within the search window the window may present a more limited set of applications that contain or begin with the letter c. For example the number of PC productivity applications shown in area has been reduced to the fewer number of applications as shown in area . Further the applications shown in area have been reduced to a less numerous number of applications shown in area . The applications shown in areas or may have a c in the name of the application or may have a c that begins the application name. In this way the searching function in allows for the user to quickly discern what applications are important and to select a more limited set of applications from the application launcher window .

Embodiments of a process for using an application manager initiated by selecting icon is shown in . In embodiments the user may move a user interface device to select the application manager icon in the triad control . At the time of the selection of the icon the computing system interface may be displaying at least one window or . Upon selecting the application manager icon in the triad control a new menu or display shown in may be presented within the display . The new display may contain two or more areas e.g. area and area . The separation of the areas may be delineated by a bar shown between area and area . Each area may display different information.

Area of display can display applications currently executing or being displayed on the device. Likewise area may display applications being executed or being displayed on the computing system. For example windows and are shown in area . Thus the display captures the windows that were currently being displayed in . In further embodiments an application window that was not being displayed but being executed on the device may be shown in area . The executed application is represented by window representation in area .

A user may manage the currently executing applications by selecting or conducting actions on the representations of the applications shown in windows . Thus if the user wishes to select execute or move the display of a window being operated on the device e.g. or the user may select or conduct the user interface action on those windows in area . In embodiments when the user moves a user interface device to select a window in the display such that the window may then become displayed in the personal computing interface as shown on . When selected in windows and or windows the displayed windows in the interface s becomes highlighted and or receives focus. Thus in embodiments the application manager window allows the user to manage applications currently executing in the unified desktop.

When the user selects the icon a file manager window may appear within the user interface device as shown in . Thus the triad control provides access to files that may be on the device or the computer system. The user interface window may provide an area for recently used files the area displays information associated with recently used files e.g. filename date when the file was last opened the size of the file etc. . The icons or user interface devices may be selectable to open the represented files in area . Further window may provide an area that includes a list of folders either automatically generated for the user or populated by user input. The folder icons in area may also be selectable to open a folder and access a file within the data storage of either the device or the computer system.

A phone window presented after icon is selected in the triad control is shown on . The phone window may allow the user to select one or more icons displayed within the window to conduct a communication session. For example the phone icon may allow the user to begin a telephone call or send a text message by selecting the icon . Further in area a call log may be listed that shows recent calls either received or sent by the device. In area one or more icons may be provided that represent data about frequently called or contacted contacts. In area one or more contacts that are communicated with often may be listed in a favorite s field. In area all contacts may be listed. The list of contacts may be searchable or navigable by the user in window . Any of these different fields may be searchable by search function presented in windows . Each icon in embodiments within window may be selectable to conduct a communication session using the device regardless of the fact that the icon was selected in window presented in the computing system interface .

Like the phone functionality described in conjunction with if a user selects a browser icon within the triad control bar a web browser window may be presented. The browser window can present one or more items of information that are associated with the web browsing functionality of the device. Thus icon may be selected to begin or conduct a web browser session using the web browser application and network communication capability of the device. Area may include bookmarks for the web browser bookmarked by the user in the device. Area may provide most visited sites that may be selectable by the user. Area may provide a list of downloads that have been recently downloaded by the device. Area may provide a history of recently visited sites using the web browser. Any of these areas may be searchable by search function provided in windows . Each of the icons within windows may be selected to be begin a web browsing session using the functionality of the phone while selectable within the computer system interface .

An embodiment of a method for selecting or executing functions within the unified desktop using the triad control is shown in . An embodiment of the method for receiving an event for the unified desktop at the device is shown in . While a general order for the steps of the method is shown in . Generally the method starts with a start operation and ends with an end operation . The method can include more or fewer steps or can arrange the order of the steps differently than those shown in . The method can be executed as a set of computer executable instructions executed by a computer system and encoded or stored on a computer readable medium. Hereinafter the method shall be explained with reference to the systems components modules software data structures user interfaces etc. described in conjunction with . The method may be described with particular attention paid to .

A user interface event may be received in display in step . The user interface event may occur within the triad control region . Upon receiving the user interface action that persists within the area the unified desktop may display a triad control in step . In embodiments the triad control display may be conducted as described in conjunction with .

Thereinafter the unified desktop system may determine if an event is conducted within the triad control bar . An event may be a selection of a user selectable icon within region or some other icon presented within the triad control . If no action is conducted the triad control may continue to be displayed on user interface device in step . If an event does occur within the triad control such as the selection of the one of the users selectable icons in area the selected function may be provided in step .

The function provided may be the display of a window as described in conjunction with . The menus provided may include further functionality that may be accessed through the triad control . The unified desktop must then determine if a UA device is still in the triad control or in one of the menus provided from the triad control in step . If the user interface device remains within the triad control area the triad control may continue to be displaced in step . However if the user interface device is no longer within the triad control area the triad control may be hidden in step . If a functionality provided within the menu is selected by a user for example selecting a user selectable icon within a menu presented in the triad control may be hidden and the function provided.

While the exemplary aspects embodiments and or configurations illustrated herein show the various components of the system collocated certain components of the system can be located remotely at distant portions of a distributed network such as a LAN and or the Internet or within a dedicated system. Thus it should be appreciated that the components of the system can be combined in to one or more devices such as a tablet like device or collocated on a particular node of a distributed network such as an analog and or digital telecommunications network a packet switch network or a circuit switched network. It will be appreciated from the preceding description and for reasons of computational efficiency that the components of the system can be arranged at any location within a distributed network of components without affecting the operation of the system. For example the various components can be located in a switch such as a PBX and media server gateway in one or more communications devices at one or more users premises or some combination thereof. Similarly one or more functional portions of the system could be distributed between a telecommunications device s and an associated computing device.

Furthermore it should be appreciated that the various links connecting the elements can be wired or wireless links or any combination thereof or any other known or later developed element s that is capable of supplying and or communicating data to and from the connected elements. These wired or wireless links can also be secure links and may be capable of communicating encrypted information. Transmission media used as links for example can be any suitable carrier for electrical signals including coaxial cables copper wire and fiber optics and may take the form of acoustic or light waves such as those generated during radio wave and infra red data communications.

Also while the flowcharts have been discussed and illustrated in relation to a particular sequence of events it should be appreciated that changes additions and omissions to this sequence can occur without materially affecting the operation of the disclosed embodiments configuration and aspects.

In yet another embodiment the systems and methods of this disclosure can be implemented in conjunction with a special purpose computer a programmed microprocessor or microcontroller and peripheral integrated circuit element s an ASIC or other integrated circuit a digital signal processor a hard wired electronic or logic circuit such as discrete element circuit a programmable logic device or gate array such as PLD PLA FPGA PAL special purpose computer any comparable means or the like. In general any device s or means capable of implementing the methodology illustrated herein can be used to implement the various aspects of this disclosure. Exemplary hardware that can be used for the disclosed embodiments configurations and aspects includes computers handheld devices telephones e.g. cellular Internet enabled digital analog hybrids and others and other hardware known in the art. Some of these devices include processors e.g. a single or multiple microprocessors memory nonvolatile storage input devices and output devices. Furthermore alternative software implementations including but not limited to distributed processing or component object distributed processing parallel processing or virtual machine processing can also be constructed to implement the methods described herein.

In yet another embodiment the disclosed methods may be readily implemented in conjunction with software using object or object oriented software development environments that provide portable source code that can be used on a variety of computer or workstation platforms. Alternatively the disclosed system may be implemented partially or fully in hardware using standard logic circuits or VLSI design. Whether software or hardware is used to implement the systems in accordance with this disclosure is dependent on the speed and or efficiency requirements of the system the particular function and the particular software or hardware systems or microprocessor or microcomputer systems being utilized.

In yet another embodiment the disclosed methods may be partially implemented in software that can be stored on a storage medium executed on programmed general purpose computer with the cooperation of a controller and memory a special purpose computer a microprocessor or the like. In these instances the systems and methods of this disclosure can be implemented as program embedded on personal computer such as an applet JAVA or CGI script as a resource residing on a server or computer workstation as a routine embedded in a dedicated measurement system system component or the like. The system can also be implemented by physically incorporating the system and or method into a software and or hardware system.

Although the present disclosure describes components and functions implemented in the aspects embodiments and or configurations with reference to particular standards and protocols the aspects embodiments and or configurations are not limited to such standards and protocols. Other similar standards and protocols not mentioned herein are in existence and are considered to be included in the present disclosure. Moreover the standards and protocols mentioned herein and other similar standards and protocols not mentioned herein are periodically superseded by faster or more effective equivalents having essentially the same functions. Such replacement standards and protocols having the same functions are considered equivalents included in the present disclosure.

The present disclosure in various aspects embodiments and or configurations includes components methods processes systems and or apparatus substantially as depicted and described herein including various aspects embodiments configurations embodiments subcombinations and or subsets thereof. Those of skill in the art will understand how to make and use the disclosed aspects embodiments and or configurations after understanding the present disclosure. The present disclosure in various aspects embodiments and or configurations includes providing devices and processes in the absence of items not depicted and or described herein or in various aspects embodiments and or configurations hereof including in the absence of such items as may have been used in previous devices or processes e.g. for improving performance achieving ease and or reducing cost of implementation.

The foregoing discussion has been presented for purposes of illustration and description. The foregoing is not intended to limit the disclosure to the form or forms disclosed herein. In the foregoing Detailed Description for example various features of the disclosure are grouped together in one or more aspects embodiments and or configurations for the purpose of streamlining the disclosure. The features of the aspects embodiments and or configurations of the disclosure may be combined in alternate aspects embodiments and or configurations other than those discussed above. This method of disclosure is not to be interpreted as reflecting an intention that the claims require more features than are expressly recited in each claim. Rather as the following claims reflect inventive aspects lie in less than all features of a single foregoing disclosed aspect embodiment and or configuration. Thus the following claims are hereby incorporated into this Detailed Description with each claim standing on its own as a separate preferred embodiment of the disclosure.

Moreover though the description has included description of one or more aspects embodiments and or configurations and certain variations and modifications other variations combinations and modifications are within the scope of the disclosure e.g. as may be within the skill and knowledge of those in the art after understanding the present disclosure. It is intended to obtain rights which include alternative aspects embodiments and or configurations to the extent permitted including alternate interchangeable and or equivalent structures functions ranges or steps to those claimed whether or not such alternate interchangeable and or equivalent structures functions ranges or steps are disclosed herein and without intending to publicly dedicate any patentable subject matter.

