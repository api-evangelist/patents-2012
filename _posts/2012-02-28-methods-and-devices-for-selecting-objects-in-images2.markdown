---

title: Methods and devices for selecting objects in images
abstract: Methods and devices for selecting objects in images are described. In one example aspect, a method includes: receiving stereoscopic image data, the stereoscopic image data includes a first image obtained from a first camera and a second image obtained from a second camera; identifying an object in the first image by analyzing the first image and the second image; displaying the first image, the identified object in the first image being selectable.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09558575&OS=09558575&RS=09558575
owner: BlackBerry Limited
number: 09558575
owner_city: Waterloo, Ontario
owner_country: CA
publication_date: 20120228
---
The present disclosure relates to image manipulation and more particularly to methods and electronic devices for selecting objects in images.

Electronic devices such as smartphones and tablet computers may be equipped with an application to manipulate images. For example an image editing application may allow a user to manipulate an image by changing properties associated with the image. The image editing application may for example allow a user to modify the visual properties of the image by removing portions of the image changing the colour of portions of the image adding graphics to the image etc.

An image editing application may not provide a user with an easy way of selecting objects within an image for manipulating the image. An object may include an identifiable item such as an individual within an image. A user desiring to select an object within an image may have to manually outline the boundaries around the object in order to select the object. This can be a cumbersome process and may not accurately capture the whole object within the image.

In one example aspect the present application describes a method implemented by a processor of an electronic device for allowing selection of an object in an image. The method includes receiving stereoscopic image data the stereoscopic image includes a first image obtained from a first camera and a second image obtained from a second camera identifying an object in the first image by analyzing the first image and the second image displaying the first image the identified object in the first image being selectable.

In another example aspect the present application describes an electronic device. The electronic device includes a memory a display a first camera a second camera and a processor coupled with the memory the display the first camera and the second camera. The processor is configured to receive stereoscopic image data the stereoscopic image data includes a first image obtained from the first camera and a second image obtained from the second camera identifying an object in the first image by analyzing the first image and the second image display the first image on the display the identified object in the first image being selectable.

In yet another example aspect the present application describes a computer readable storage medium. The computer readable storage medium includes computer executable instructions for allowing selection of an object in an image. The computer executable instructions includes instructions for receiving stereoscopic image data the stereoscopic image data includes a first image obtained from a first camera and a second image obtained from a second camera identifying an object in the first image by analyzing the first image and the second image displaying the first image the identified object in the first image being selectable.

Other example embodiments of the present disclosure will be apparent to those of ordinary skill in the art from a review of the following detailed description in conjunction with the drawings.

Example embodiments of the present disclosure are not limited to any particular operating system electronic device architecture server architecture or computer programming language.

Reference is first made to which illustrates an example electronic device in which example embodiments described in the present disclosure can be applied. In the example embodiment illustrated the electronic device is a mobile communication device. That is the electronic device is configured to communicate with other electronic devices servers and or systems i.e. it is a communication device and the electronic device is portable and may be easily moved between different physical locations i.e. it is a mobile device . However in other example embodiments the electronic device may not be portable i.e. may not be a mobile device and or may not be configured to communicate with other systems i.e. may not be a communication device .

Depending on the functionality provided by the electronic device in various example embodiments the electronic device may be a multiple mode communication device configured for both data and voice communication a mobile telephone such as a smartphone a wearable computer such as a watch a tablet computer such as a slate computer a personal digital assistant PDA or a computer system. The electronic device may take other forms apart from those specifically listed above. The electronic device may also in various example embodiments be referred to as a mobile communications device a communication device a mobile device an electronic device and in some cases as a device.

The electronic device includes a controller including at least one processor such as a microprocessor which controls the overall operation of the electronic device . The processor interacts with device subsystems such as a wireless communication subsystem for exchanging radio frequency signals with a wireless network to perform communication functions. The processor may be communicably coupled with additional device subsystems including one or more output interfaces such as a display and or a speaker one or more input interfaces such as a first camera a second camera a microphone a keyboard not shown control buttons not shown a touch sensitive overlay not shown associated with a touchscreen display and or other input interfaces memory such as flash memory random access memory RAM read only memory ROM etc. auxiliary input output I O subsystems a data port which may be a serial data port such as a Universal Serial Bus USB data port a short range wireless communication subsystem and other device subsystems generally designated as . Some of the subsystems shown in perform communication related functions whereas other subsystems may provide resident or on device functions.

In at least some example embodiments the electronic device may include a touchscreen display which acts as both an input interface i.e. touch sensitive overlay and an output interface i.e. display . The touchscreen display may be constructed using a touch sensitive input surface which is connected to an electronic controller and which overlays the display . The touch sensitive overlay and the electronic controller provide a touch sensitive input interface and the processor interacts with the touch sensitive overlay via the electronic controller.

The electronic device may include a communication subsystem which allows the electronic device to communicate over a wireless network . The communication subsystem includes a receiver a transmitter and associated components such as one or more antenna elements local oscillators LOs and a processing module such as a digital signal processor DSP . The antenna elements may be embedded or internal to the electronic device and a single antenna may be shared by both receiver and transmitter . The particular design of the wireless communication subsystem depends on the wireless network in which the electronic device is intended to operate.

In at least some example embodiments the electronic device may communicate with any one of a plurality of fixed transceiver base stations of the wireless network within its geographic coverage area. The electronic device may send and receive communication signals over the wireless network after the required network registration or activation procedures have been completed. Signals received by the antenna through the wireless network are input to the receiver which may perform such common receiver functions as signal amplification frequency down conversion filtering channel selection etc. as well as analog to digital A D conversion. A D conversion of a received signal allows more complex communication functions such as demodulation and decoding to be performed in the DSP . In a similar manner signals to be transmitted are processed including modulation and encoding for example by the DSP . These DSP processed signals are input to the transmitter for digital to analog D A conversion frequency up conversion filtering amplification and transmission to the wireless network via the antenna . The DSP not only processes communication signals but may also provide for receiver and transmitter control. For example the gains applied to communication signals in the receiver and the transmitter may be adaptively controlled through automatic gain control algorithms implemented in the DSP .

In at least some example embodiments the auxiliary input output I O subsystems may include an external communication link or interface for example an Ethernet connection. The electronic device may include other wireless communication interfaces for communicating with other types of wireless networks for example a wireless network such as an orthogonal frequency division multiplexed OFDM network. The auxiliary I O subsystems may include a vibrator for providing vibratory notifications in response to various events on the electronic device such as receipt of an electronic communication or incoming phone call or for other purposes such as haptic feedback touch feedback .

In at least some example embodiments the electronic device also includes a removable memory module which may be flash memory such as a removable memory card and a memory interface . Network access may be associated with a subscriber or user of the electronic device via the memory module which may be a Subscriber Identity Module SIM card for use in a GSM network or other type of memory card for use in the relevant wireless network type. The memory module is inserted in or connected to the memory card interface of the electronic device in order to operate in conjunction with the wireless network .

The data port may be used for synchronization with a user s host computer system not shown . The data port enables a user to set preferences through an external device or software application and extends the capabilities of the electronic device by providing for information or software downloads to the electronic device other than through the wireless network . The alternate download path may for example be used to load an encryption key onto the electronic device through a direct reliable and trusted connection to thereby provide secure device communication.

The electronic device includes a first camera and a second camera . The cameras are capable of capturing camera data such as images in the form of still photo and or motion data. The camera data may be captured in the form of an electronic signal which is produced by an image sensor associated with each of the cameras . The cameras may be collectively capable of capturing stereoscopic images for display. That is the cameras may collectively produce stereoscopic image data which defines a stereoscopic image. Stereoscopic images may provide an illusion of depth in the images to a user i.e. three dimensional 3 D images .

To produce stereoscopic image data the cameras are oriented in generally the same direction. For example as will be discussed below in at least some embodiments the cameras may both be rear facing. That is the cameras are in some embodiments both arranged to capture an image of a subject on a rear side of the electronic device .

The first camera and the second camera are mounted in spaced relation to one another. That is there may be a space between the cameras . The first camera and the second camera are spaced apart in order to capture stereoscopic images. The distance between the cameras may be approximately the same as the distance between a standard person s eyes which is around 6.35 cm . In at least some example embodiments the distance between the cameras may be smaller or larger than the distance between a person s eyes. A larger distance between the cameras may allow for capturing stereoscopic images that produces an enhanced effect of stereoscopy for a user.

Since the cameras are offset from one another so too are the images which they produce. That is the first camera captures a first image and the second camera captures a second image that is offset from the first image. The first image is captured at a different position than the second image due to the positioning of the first camera and the second camera . The offset between the first image and the second image is defined by the distance referred to as the intra axial distance between the first camera and the second camera

Accordingly stereoscopic image data may be produced by the cameras . Such stereoscopic image data includes two images a first image produced using the first camera and a second image produced using the second camera . The first image and the second image may be captured at the same time or approximately the same time but may be offset in appearance due to the spacing between the cameras 

The cameras may be both configured as front facing cameras or may both be configured as rear facing cameras. Front facing cameras are provided by cameras which are located to obtain images near a front face of the electronic device . The front face is typically the face on which a main display is mounted. That is when front facing cameras are provided on the electronic device the display is configured to display content which may be viewed from a side of the electronic device where the cameras are directed.

The cameras of the front facing cameras may be located above or below the display . In at least some example embodiments the camera may be provided in a central location relative to the display to facilitate image acquisition of a face. For example the cameras may be located centrally above the display .

Rear facing cameras are provided by cameras which are located to obtain images of a subject near a rear face of the electronic device . The rear face is typically a face which does not include the main display of the electronic device . In at least some embodiments the electronic device may operate in one operating mode in which the display acts as a viewfinder displaying image data associated with rear facing cameras.

The rear facing cameras may obtain images which are not within the field of view of the front facing cameras. The field of view of the front facing and rear facing cameras may generally be in opposing directions.

While illustrates a first camera and a second camera the electronic device may include more than two cameras . For example in at least some example embodiments the electronic device may include both front facing cameras and rear facing cameras.

In at least some example embodiments the electronic device is provided with a service routing application programming interface API which provides an application with the ability to route traffic through a serial data i.e. USB or Bluetooth Bluetooth is a registered trademark of Bluetooth SIG Inc. connection to a host computer system using standard connectivity protocols. When a user connects their electronic device to the host computer system via a USB cable or Bluetooth connection traffic that was destined for the wireless network is automatically routed to the electronic device using the USB cable or Bluetooth connection. Similarly any traffic destined for the wireless network is automatically sent over the USB cable or Bluetooth connection to the host computer system for processing.

The electronic device also includes a battery as a power source which is typically one or more rechargeable batteries that may be charged for example through charging circuitry coupled to a battery interface such as the data port . The battery provides electrical power to at least some of the electrical circuitry in the electronic device and the battery interface provides a mechanical and electrical connection for the battery . The battery interface is coupled to a regulator not shown which provides power V to the circuitry of the electronic device .

The electronic device stores data in an erasable persistent memory which in one example embodiment is the flash memory . In various example embodiments the data includes service data including information used by the electronic device to establish and maintain communication with the wireless network . The data may also include user application data such as email messages address book and contact information camera data calendar and schedule information notepad documents image files and other commonly stored user information stored on the electronic device by its user and other data. The data stored in the persistent memory e.g. flash memory of the electronic device may be organized at least partially into one or more databases or data stores. The databases or data stores may contain data items of the same data type or associated with the same application. For example email messages contact records and task items may be stored in individual databases within the memory.

The electronic device may in at least some example embodiments be a mobile communication device which may provide two principal modes of communication a data communication mode and a voice communication mode. In the data communication mode a received data signal such as a text message an email message or a web page download will be processed by the communication subsystem and input to the processor for further processing. For example a downloaded web page may be further processed by a browser application or an email message may be processed by an email messaging application and output to the display . A user of the electronic device may also compose data items such as email messages for example using the input devices in conjunction with the display . These composed items may be transmitted through the wireless communication subsystem over the wireless network .

In the voice communication mode the electronic device provides telephony functions and operates as a typical cellular phone. The overall operation is similar to the data communication mode except that the received signals would be output to the speaker and signals for transmission would be generated by a transducer such as the microphone . The telephony functions are provided by a combination of software firmware i.e. a voice communication module and hardware i.e. the microphone the speaker and input interfaces . Alternative voice or audio I O subsystems such as a voice message recording subsystem may also be implemented on the electronic device . Although voice or audio signal output is typically accomplished primarily through the speaker the display may also be used to provide an indication of the identity of a calling party duration of a voice call or other voice call related information.

The processor operates under stored program control and executes software modules stored in memory such as persistent memory for example in the flash memory . As illustrated in the software modules include operating system software and other software applications .

The software applications on the electronic device may also include a range of additional applications including for example a notepad application Internet browser application voice communication i.e. telephony application mapping application or a media player application or any combination thereof. Each of the software applications may include layout information defining the placement of particular fields and graphic elements e.g. text fields input fields icons etc. in the user interface e.g. the display according to the application.

The software modules or parts thereof may be temporarily loaded into volatile memory such as the RAM . The RAM is used for storing runtime data variables and other types of data or information as will be apparent. Although specific functions are described for various types of memory this is merely one example and it will be appreciated that a different assignment of functions to types of memory could also be used.

A predetermined set of applications that control basic device operations including data and possibly voice communication applications will normally be installed on the electronic device during or after manufacture. Additional applications and or upgrades to the operating system or software applications may also be loaded onto the electronic device through the wireless network the auxiliary I O subsystem the data port the short range communication subsystem or other suitable device subsystem . The downloaded programs or code modules may be permanently installed for example written into the program memory i.e. the flash memory or written into and executed from the RAM for execution by the processor at runtime.

The processor may be electrically connected to the cameras to allow the processor to receive electronic signals representing camera data from the cameras 

In at least some embodiments the software modules may include one or more camera applications or software modules which are configured for handling the electronic signals representing camera data from the cameras . The camera application may for example be configured to provide a viewfinder on the display by displaying in real time or near real time images defined in the electronic signals received from the cameras . The camera application may also be configured to capture images or videos by storing images or videos defined by the electronic signals received from the cameras . For example the camera application may be configured to store the images or videos to memory for example the flash memory of the electronic device . The images may be stored in various formats including JPEG RAW BMP etc.

The camera application may also be configured to control options or preferences associated with the cameras . For example the camera application may be configured to control camera lens apertures and or shutter speeds associated with the cameras . The control of such features may in at least some embodiments be automatically performed by the camera application based on output received from a light exposure meter.

In at least some example embodiments the camera application may be configured to control flashes associated with the cameras and or to control zooms associated with the cameras . In at least some example embodiments the camera application is configured to provide digital zoom features. The camera application may provide digital zoom features by cropping an image down to a centered area with the same aspect ratio as the original. In at least some example embodiments the camera application may interpolate within the cropped image to bring the cropped image back up to the pixel dimensions of the original. The camera application may in at least some example embodiments provide image stabilization for the cameras . Image stabilization may reduce blurring associated with movement of the cameras 

In at least some embodiments the camera application may be configured to focus the cameras on an object i.e. an identifiable item such as an individual or thing . More particularly the camera application may be configured to control actuators of the cameras to move lenses a lens may be comprised of one or more lens elements in the camera relative to image sensors in the cameras . For example when capturing images of objects which are very close to the cameras e.g. object at macro position the camera application may control the actuators to cause the actuators to move the lenses away from the image sensors.

In at least some embodiments the camera application may provide for auto focusing capabilities. For example the camera application may analyze received electronic signals to determine whether the images captured by the cameras are in focus. That is the camera application may determine whether the images defined by electronic signals received from the cameras are focused properly on the object of such images. The camera application may for example make this determination based on the sharpness of such images. If the camera application determines that the images are not in focus then the camera application may cause the processor to adjust the actuators which controls the lenses to focus the images.

In at least some example embodiments the camera application or another application may be configured to process electronic signals of images captured by cameras for stereoscopic display. That is the camera application may analyze stereoscopic images defined by the electronic signals received from the cameras 

In at least some embodiments the camera application may be configured to display a first image obtained from a first camera and a second image obtained from a second camera separately to a left eye and a right eye of a user viewing the display . That is the first image may be displayed to only the left eye of the user and the second image may be displayed to only the right eye of the user. The offset between the first image and the second image may provide information to the brain of a user to give the perception of depth when the first image and the second image are combined i.e. stereoscopic images and viewed by the user.

Accordingly in one operating mode the camera application may be configured to display stereoscopic images defined by stereoscopic image data to a user via the display to achieve a stereoscopic effect. The camera application may employ any one of a number of stereoscopic display techniques in order to achieve the stereoscopic effect. In at least some example embodiments the stereoscopic images are displayed as alternate frame sequencing. Alternative frame sequencing occurs when the display alternatively displaying a different image for each eye of a user.

That is a first image for the left eye is displayed and then a second image for the right eye is displayed. In such example embodiments a user may wear 3 D liquid crystal glasses in order to experience the effect of stereoscopy when viewing the stereoscopic images. In some embodiments the 3 D liquid crystal glasses may include a communication subsystem for receiving timing signals from the electronic device .

The timing signals control the visibility of the 3 D liquid crystal glasses. For example the timing signals may allow the 3 D liquid crystal glasses to alternatively darken over one eye and then the other eye in synchronization with a refresh rate i.e. the rate at which the images are alternatively displayed of the display . A user wearing a 3 D liquid crystal glass may view a first image with only the left eye and then alternatively view a second image with only the right eye experiencing the effect of stereoscopy.

In some example embodiments the stereoscopic images are displayed through polarizing filters on the display . The polarizing filters are optical filters that allow light of a specific polarization to pass through and prevent light of other polarization to pass through. The polarizing filters may be placed underneath the display . The stereoscopic images are displayed superimposed through the polarizing filters on the display . In such example embodiments a user may wear 3 D polarized glasses in order to experience the effect of stereoscopy when viewing the stereoscopic images. The 3 D polarized glasses contain a pair of different polarizing filters. Each polarizing filter on the 3 D polarized glass passes light that is similarly polarized to the light passing through the polarizing filters on the display and blocks light that is not similarly polarized to the light passing through the polarizing filters on the display . For example a 3 D polarizing glass with a blue light polarizing filter for a first lens and an alternate red light polarizing filter for the a second lens allows only blue light transmitted from the display to pass through the first lens and only red light transmitted from the display to pass through the second lens. A user wearing a 3 D polarized glass may view the superimposed images simultaneously experiencing the effect of stereoscopy.

In other example embodiments the stereoscopic images are displayed as auto stereoscopy. Auto stereoscopy provides a user viewing stereoscopic images to experience the effect of stereoscopy without having to wear any form of 3 D glasses. In such example embodiments the images displayed on the display are split directionally into a user s eyes. That is a first image is only viewed by a user s left eye and a second image is only viewed by a user s right eye. In such example embodiments a parallax barrier may be implemented to split the view of the images. The parallax barrier may overlay the display and include a series of slits allowing each eye of a user to view a specific image. A user viewing the images may experience the effect of stereoscopy.

It will be appreciated that stereoscopic images may be presented by the electronic device on the display in order for a user to experience the effect of stereoscopy when viewing the images by other example embodiments not specifically described herein.

In at least some example embodiments the camera application may be configured to allow for simultaneous operation of the cameras . That is the camera application may allow the cameras to simultaneously capture images. For example a user may input an instruction to the electronic device via an input interface instructing the electronic device to capture an image. In response the electronic device may simultaneously capture an image using both the first camera and the second camera a first image is captured using the first camera and a second image is captured using the second camera . The first image and the second image may be captured at approximately the same time. These images may collectively form stereoscopic image data.

In at least some embodiments the camera application or another application may be configured to allow selection of an identified object in an image. Objects may be identified by the camera application or another application using stereoscopic image data which includes a first image obtained from a first camera and a second image obtained from a second camera . In some embodiments in response to receiving the stereoscopic image data from the cameras the camera application analyzes the first image and the second image to identify an object in the first image. The camera application or another application such as an image editing application may be configured to then display the first image as a two dimensional i.e. non stereoscopic image and to allow selection of the identified object in the first image. Specific functions and features of the camera application will be discussed in greater detail below with reference to .

While the embodiment discussed above includes a processor coupled with a camera application which collectively act as an image signal processor to provide image related functions such as auto focusing in other example embodiments not shown another processor such as a dedicated image signal processor may provide some or all of these functions.

In at least some example embodiments the operating system may perform some or all of the functions of the camera application . In other example embodiments the functions or a portion of the functions of the camera application may be performed by one or more other applications. For example in at least some embodiments the manipulation functions and or the identification of objects which will be described below in greater detail with reference to may be performed within an image editing application not shown . The image editing application may be an application which allows a user to edit an image. The image editing application may contain processor executable instructions which when executed cause the processor to perform object manipulation and or object identification.

Further while the camera application has been illustrated as a stand alone application in other example embodiments the camera application may be implemented as part of the operating system or another application . Furthermore in at least some example embodiments the functions of the camera application may be provided by a plurality of software modules. In at least some example embodiments these software modules may be divided among multiple applications.

As discussed above the electronic device may take a variety of forms. For example in at least some example embodiments the electronic device may be a smartphone.

Referring now to a front view of an example electronic device which is a smartphone is illustrated. The smartphone is a mobile phone which offers more advanced computing capability than a basic non smartphone cellular phone. For example the smartphone may have the ability to run third party applications which are stored on the smartphone.

The smartphone may include the components discussed above with reference to or a subset of those components. The smartphone includes a housing which houses at least some of the components discussed above with reference to .

In the example embodiment illustrated the smartphone includes a display which may be a touchscreen display which acts as an input interface . The display is disposed within the smartphone so that it is viewable at a front side of the smartphone . That is a viewable side of the display is disposed on the front side of the smartphone. In the example embodiment illustrated the display is framed by the housing .

The example smartphone also includes other input interfaces such as one or more buttons keys or navigational input mechanisms. In the example embodiment illustrated at least some of these additional input interfaces are disposed for actuation at a front side of the smartphone.

The example smartphone also includes rear facing cameras on a rear side of the smartphone which are illustrated with reference to showing a rear view of the smartphone . That is the rear facing cameras are located on a side of the smartphone which does not include the display . The rear facing cameras may include a first camera and a second camera as illustrated in .

The rear facing cameras may be located on a central axis of the smartphone which is located midway between a top side and a bottom side of the electronic device when the electronic device is held in a landscape orientation where its width is longer than its height. The rear facing cameras are located so that they may capture images of objects which are located in the rear of and or surrounding the rear side of the electronic device . In at least some example embodiments the electronic device may operate in one operating mode in which the display on the front side of the electronic device acts as a viewfinder displaying image data associated with the rear facing cameras on the rear side of the electronic device .

The rear facing cameras are spaced apart by a distance in order to capture stereoscopic images. In at least some example embodiments the distance between the rear facing cameras is greater than the distance illustrated in . For example the first camera may be located at one end of the rear side e.g. the leftmost end illustrated in of the electronic device and the second camera may be located at the other end of the rear side of the electronic device e.g. the rightmost end illustrated in . A greater distance between the cameras may allow for the capture stereoscopic images that produce an enhanced effect of stereoscopy for a user of the electronic device .

In at least some example embodiments the smartphone may also include one or more front facing cameras instead of or in addition to the rear facing cameras . The front facing cameras may be located on the front side of the smart phone . The front facing cameras are located so that they may capture images of objects which are located in front of and or surrounding the front side of the smartphone .

Referring now to a flowchart of an example method of allowing selection of an identified object in an image is illustrated. The electronic device may be configured to perform the method of . In at least some example embodiments the processor of the electronic device is configured to perform the method of . One or more applications or modules on the electronic device may contain computer readable instructions which cause the processor of the electronic device to perform the method of . In at least some example embodiments the camera application stored in memory of the electronic device is configured to perform the method of . More particularly the camera application may contain computer readable instructions which when executed cause the processor to perform the method of . It will be appreciated that the method of may in at least some example embodiments be provided by other software applications or modules apart from those specifically discussed above for example the operating system . Accordingly any features which are referred to as being performed by the electronic device may be performed by any one or more of the software applications or modules referred to above or other software modules.

In at least some example embodiments at least some of the method of may be performed by or may rely on other applications or modules. For example in some embodiments an image editing application may be configured to perform the method of . That is an image editing application may contain computer readable instructions which when executed cause the processor to perform the method of .

Referring to a flowchart of an example method of allowing selection of an object in an image is illustrated. At the electronic device receives stereoscopic image data. The stereoscopic image data includes a first image obtained from a first camera and a second image obtained from a second camera

In at least some example embodiments the stereoscopic image data may be received in response to receiving an instruction to capture an image on the first camera and or an instruction to capture an image on the second camera . In at least some embodiments the electronic device may cause the first camera and the second camera to capture stereoscopic image data in response to the receipt of an instruction from a user to capture a two dimensional i.e. non stereoscopic image. That is even where a user instructs the device to capture a two dimensional non stereoscopic image the electronic device may capture stereoscopic image data. In at least some embodiments in which the camera captures stereoscopic image data in response to the receipt of an instruction to capture a two dimensional image the stereoscopic image data may not be used for the purposes of displaying an image in a stereoscopic mode. Rather in at least some such embodiments one of the captured images in the stereoscopic image data may be displayed in a two dimensional i.e. non stereoscopic mode and the other of the captured images may be used to analyze the displayed two dimensional image. For example as will be explained below the second image may be used to identify objects in the first image.

Accordingly in at least some embodiments stereoscopic image data may be received in response to the receipt of an instruction to capture a non stereoscopic image. The instruction may be received for example from an input interface associated with the electronic device . For example the instruction may be received from a navigational input device such as a trackball track pad or touchscreen display.

In at least some embodiments in response to receiving an instruction to capture a non stereoscopic image and or in response to receiving an instruction to capture a stereoscopic image the first camera is triggered to capture a first image and the second camera is triggered to capture a second image. In at least some example embodiments the first image and the second image are captured simultaneously. That is the first camera captures the first image at the same time as the second camera captures the second image.

In some example embodiments the first image is not captured simultaneously as the second image. There may be a time difference between the capture of the first image by the first camera and the second image by the second camera . For example the first camera may capture the first image and subsequently the second camera captures the second image. However in order to ensure the stereoscopic image data is not compromised due to moving of the camera or subject the time delay between the acquisition of the first image and the acquisition of the second image is small.

Accordingly in some embodiments at stereoscopic image data is received from the cameras . In other embodiments the electronic device may receive the stereoscopic image data from memory for example from flash memory of the electronic device . For example the electronic device may retrieve the first image and the second image from the data area of memory. In such example embodiments the first image and the second image may have been stored in memory of the electronic device after capture of the first image by the first camera and the second image by the second camera

In at least some example embodiments the first image and the second image are two dimensional 2 D images. For example the first image and the second image may only include a length and a width component along a planar surface. The first image and the second image may not include a depth component along a planar surface.

After receiving the stereoscopic image data the electronic device at may analyze the first image and the second image to identify an object in the first image. In at least some example embodiments analyzing may include identifying one or more boundary associated with the object in the first image. An object may include an identifiable item such as an individual or a thing with one or more identifiable boundary within an image. For example an object may include a person within an image of a landscape at the background. The electronic device may identify the boundaries of the person within the image of the landscape. The boundaries may in at least some embodiments represent a perimeter of an object. That is the boundary may be a continuous line which forms the boundary of a geometric figure i.e. the object .

In at least some example embodiments identifying boundaries may include performing image segmentation. Image segmentation is a process of partitioning an image defined by electronic signals into multiple segments such as pixels a pixel is the smallest unit of an image that can be represented and is a sample of the image. Each pixel has its own address and the address may correspond to its coordinates in relation to the image . A label is assigned to every segment in the image such that same labels share certain visual characteristics for example colour intensity texture depth etc. The image segmentation results in a set of segments that collectively cover the entire image. Each of the segments within a set are similar with respect to a certain visual characteristic resulting in differentiating sets of visual characteristics within the image. The differentiating sets may be used to locate boundaries of an object in an image. For example the region of an object in an image may be identified as having a different set of visual characteristics than the rest of the regions of the image. Image segmentation may be applied to multiple images including stereoscopic images. The images are segmented and the resulting segments may be used to create 3 D reconstructions of detected objects within the images by calculating the size density etc. of the detected objects.

In at least example embodiments in performing image segmentation the electronic device may use the second image to obtain depth information to identify the object in the first image. For example the electronic device may perform image segmentation based on depth information. That is the electronic device calculates depth based characteristics and applies these characteristics as a pixel label to each pixel of a segmented first image. The depth based characteristics may be calculated by using a segmented first image and a segmented second image to obtain depth information of objects within the images. The object boundary in the first image may then be identified as the region of the object may have a different set of depth based characteristics than the rest of the regions of the image.

In at least some example embodiments the electronic device may perform edge detection in order to identify the one or more boundaries of the object. Edge detection is a process of identifying points in an image at which the image brightness has discontinuities. Discontinuities in the image brightness may correspond to discontinuities in depth illumination etc. of the image. Identification of the points may result in connecting lines that indicate the boundaries of objects in an image as the boundaries are areas often associated with discontinuities in image brightness in an image.

In at least some example embodiments in performing edge detection the electronic device may use the second image to obtain depth information to identify the object in the first image. For example the electronic device may use the first image and the second image to obtain depth information of objects within the images. The depth information may then be used to identify points in the first image defining discontinuities in brightness in the first image. The identified points may form the boundaries of the object in the first image.

In at least some example embodiments the electronic device may discard the second image after analyzing the first image and the second image. For example in at least some example embodiments the electronic device may permanently remove the second image from the electronic device . That is the second image may no longer be retrievable by a user of the electronic device . In such example embodiments if the second image is already stored in the electronic device for example in the memory of the electronic device prior to analyzing the first image and the second image the camera application may permanently delete the second image from the electronic device . That is after having used the second image to identify one or more objects in the first image the second image may be discarded to save space on the electronic device.

In at least example embodiments the electronic device may not permanently remove the second image from the electronic device . In some such example embodiments the second image may be sent to an application such as a recycle bin which temporarily stores removed files. The removed files may be stored in the data area of memory being accessible by the recycle bin or other applications. A user may still retrieve the removed second image by accessing the recycle bin. In such example embodiments if the second image is already stored in the electronic device for example in the memory of the electronic device prior to analyzing the first image and the second image the camera application sends the second image to the recycle bin.

At at the electronic device displays the first image. The first image may be displayed on the display of the electronic device . When displayed the first image may occupy the complete display or may occupy a portion of the display .

In at least some embodiments the first image is displayed as a non stereoscopic image. That is the first image is displayed as a two dimensional image. During the display of the first image the second image is not used to provide a stereoscopic effect. That is the second image is not displayed.

The electronic device then at allows selection of the identified object in the first image. The identified object may be defined by the boundaries associated with the object in the first image. That is the electronic device may allow selection of the identified object in the first image using boundaries identified at . That is the electronic device may define the area within the boundaries as being selectable. Selection of the area within the boundaries e.g. using an input interface such as a touchscreen may then be interpreted as being a selection of the object associated with the boundaries.

In at least some example embodiments after allowing selection of the identified object in the first image the electronic device may receive at an instruction for copying the identified object copying is the duplication of information . For example the instruction may be an instruction to duplicate the identified object without removing the identified object from the first image. In at least some example embodiments the instruction is an instruction to copy the whole object defined by its boundaries in the first image. The copying instruction may be received for example from an input interface associated with the electronic device . For example the copying instruction may be received from a navigational input device such as a trackball track pad or touchscreen display or a physical keyboard associated with the electronic device to instruct the electronic device to copy the identified object in the first image.

In response to receiving the instruction in at least some example embodiments at the electronic device copies the identified object to an application. For example the identified object may be duplicated without being removed from the first image. In at least some example embodiments the object may be copied to an application that temporarily stores copied data such as files images objects etc. such as a clipboard.

In at least some example embodiments the identified object may be further copied from the clipboard to other applications. For example the identified object may be copied to an application that is capable of receiving object formats such as JPEG RAW BMP etc. . For example the application may include a word processing application an image editing application etc. In at least some example embodiments the application may be located on a different device such as a server. In such example embodiments the electronic device may interact with the server via the device subsystems such as the wireless communication subsystem to copy the identified object to the application hosted by the server. For example the electronic device may copy the identified object to an online image repository such as Picasa Web Albums application by Google Inc. which may be hosted by a web server accessible via the internet.

While described the use of a copy instruction for copying the object in other embodiments a move instruction which may also be referred to as a cut instruction may be used to place the object in another location such as in another application . Much like the copy instruction when a move instruction is received the object may be copied to another location. That is may be performed. However unlike the copy instruction when a move instruction is received the object is removed from the first image when it is copied to the other location.

After an object is identified in an image the object may in some embodiments be manipulated. In at least some example embodiments manipulating may include cropping the identified object moving the identified object and or associating a caption with the identified object. Examples of such manipulation will now be described.

Reference is now made to which illustrates a flowchart of another example method of allowing selection of an identified object. The electronic device may be configured to perform the method of . In at least some example embodiments the processor of the electronic device is configured to perform the method of . One or more applications or modules on the electronic device may contain computer readable instructions which cause the processor of the electronic device to perform the method of . In at least some example embodiments the camera application stored in memory of the electronic device is configured to perform the method of . More particularly the camera application may contain computer readable instructions which when executed cause the processor to perform the method of . It will be appreciated that the method of may in at least some example embodiments be provided by other software applications or modules apart from those specifically discussed above for example the operating system . Accordingly any features which are referred to as being performed by the electronic device may be performed by any one or more of the software applications or modules referred to above or other software modules.

In at least some example embodiments at least some of the method of may be performed by or may rely on other applications or modules. For example in some embodiments an image editing application may be configured to perform the method of . That is an image editing application may contain computer readable instructions which when executed cause the processor to perform the method of .

Referring to a flowchart of an example method of allowing selection of an object in an image is illustrated. The method includes at receiving stereoscopic image data and at analyzing the first image and the second image to identify an object in the first image. The method also includes at displaying the first image and at allowing selection of the identified object. and are discussed in greater detail above with reference to .

After the object is identified at the electronic device receives a selection of the identified object. The selection may be received for example from an input interface associated with the electronic device . For example the selection may be received from a navigation input device such as a trackball track pad or touchscreen display or a physical keyboard associated with the electronic device to select the identified object. For example in some embodiments a user may select an object by using an input interface to interact with a portion of the object which is inside the boundaries of that object. That is selection of an object may be caused when a user selects any portion of the first image which is associated with that object.

In at least some example embodiments the selection of the identified object includes the selection of the identified object defined by the boundaries associated with identified the object in the first image. That is the area of the image within the boundaries associated with the identified object is selected and the area of the image outside of the boundaries associated with the identified object is not selected.

In at least some example embodiments selection of the identified object may graphically change the display of the identified object in the first image displayed on the display . For example a selection of the associated interface element may graphically bolden color highlight or outline the first identified object.

After receiving selection of the identified object the electronic device at manipulates the first image based on the selection of the identified object. That is the electronic device may perform an operation on the first image based on the object.

Manipulating may include varying visual features associated with the first image. That is the first image may visually appear different after manipulation of the first image. The selected object may either be varied during the manipulation or excluded from variation during the manipulation. That is in some embodiments the object may be moved rotated resized or may have an effect applied to it for example by changing the color of the object sharpening the object darkening the object applying a black and white effect to the object applying a color filter to the object applying a sepia effect to the object . In some embodiments the object may be excluded from variation. That is a background portion of the image which is the portion of the image which does not include the object may be varied and the object may be protected from the varying. For example an effect may be applied to portions of the first image which do not include the object for example by changing the color of the background sharpening the background making the background blurry darkening the background applying a black and white effect to the background applying a color filter to the background applying a sepia effect to the background etc. . Similarly in some embodiments the object may be used as the boundary for a cropping operation. That is the object may be used in order to perform a crop operation. During a crop operation the edges of the image may be cut based on the boundaries of the object.

The manipulation may be performed based on the boundaries of the selected object. That is the boundaries may be used to define the portion of the image which will be manipulated. For example in some embodiments only the portion of the image which is within the boundaries identified for an object will be manipulated. In such embodiments the portion of the image which is outside of the boundaries may be excluded from manipulation. Thus the boundaries may enable the object to be manipulated and the background portion of the image to be excluded.

The manipulation may occur in response to receiving specific instructions for manipulation. For example the selected identified object may be cropped in the first image in response to the electronic device receiving a cropping instruction. The instructions for manipulation may be received from an input interface such as a navigational input device.

Accordingly instructions may be received from a user i.e. via an input interface which instructs the electronic device to perform a specific type of manipulation. For example an instruction may be received at the electronic device instructing the electronic device to crop the first image based on the object remove the identified object from the first image move the identified object to another location in the image assign the object to a specific layer within the image apply an effect to the object for example by changing the color of the object sharpening the object darkening the object applying a black and white effect to the object applying a color filter to the object applying a sepia effect to the object rotating the object and or associating a caption with the object. Instructions for other types of manipulation of the object may be received in other embodiments.

In some embodiments at the electronic device may manipulate the first image based on the selected object in accordance with user instructions.

In at least some example embodiments the manipulated first image may be stored in the memory for example the flash memory of the electronic device .

An example image and object therein will now be discussed with reference to . shows an example image displayed on the smartphone referred to in . The image may be a first image obtained from a first camera provided by the electronic device . The first image includes a background and an identified object for example the car is the identified object and the tree the road and the remaining portion of the image form the background . The identified object is defined by its boundaries within the background of the first image .

The electronic device allows selection of the identified object in the first image . For example in some embodiments the object may be selected using a touchscreen display or another input interface . The selection for example may be received by a finger interacting with the display which is a touchscreen display. The selection of the identified object may occur when an area within the boundaries associated with the identified object in the first image is selected. That is the area of the first image within the boundaries associated with the identified object is selected and the area of the first image outside of the boundaries associated with the identified object is not selected for example the area of the car within the boundaries associated with the car is selected and the area outside of the boundaries associated with the car is not selected i.e. the background which includes the tree the road and the remaining portion of the image is not selected . The selection may be made by selecting a portion of the object and the selection may graphically change the display of the identified object on the display for example the car is selected and the selection boldens the boundaries of the car .

As noted above the first image may be manipulated based on the selection of the identified object . The manipulation may occur in response to receiving specific instructions for manipulation for example from a user of the smartphone via the touchscreen display.

In at least some example embodiments manipulating may include cropping the identified object in the first image . In at least some example embodiments cropping may include accentuating a specific portion of an image and removing portions of the image surrounding the specific portion. For example cropping may include accentuating the identified object and removing portions of the first image surrounding the identified object . That is cropping the identified object may manipulate the first image to accentuate the identified object and the manipulated first image may include the identified object as a greater portion in the manipulated image for example the car is accentuated and takes up a greater portion of the image in the manipulated first image than in the non manipulated first image when displayed on the display .

In at least some example embodiments cropping may include varying the aspect ratio. The aspect ratio is the proportional relationship between the height and the width of an image. For example manipulating the first image by cropping the identified object may vary the aspect ratio of the manipulated first image from the non manipulated first image. That is the manipulated first image may have a changed height and width ratio than the non manipulated first image .

In at least some example embodiments manipulating may include moving the identified object . The identified object may be moved within the first image or may be moved out of the first image . For example in at least some example embodiments the identified object may be moved from its present location in the first image to another location in the first image . That is the manipulated first image includes the identified object in a different location than the non manipulated first image for example the car may be moved from being on the road in the non manipulated first image to adjacent to the road next to the tree in the manipulated first image .

In at least some example embodiments the selected identified object may be removed from the first image . That is the manipulated first image no longer includes the identified object that is included in the non manipulated first image for example the car may no longer be present in the manipulated first image .

In at least some example embodiments the removed identified object may be moved to an application that temporary stores data such as the clipboard i.e. a cut function . In such example embodiments the stored identified object may be further moved to another application i.e. a paste function that is capable of receiving object formats such as JPEG RAW BMP etc. for further manipulation. For example the identified object may be moved to a paintbrush application and portions of the identified object may be graphically manipulated.

In at least some example embodiments manipulating includes associating a caption with the identified object in the first image. A caption may include text for example text describing the identified object in the first image . Associating a caption with the identified object may include adding text to a portion of the image. For example text may be added to the first image . The text may be included near the portion of the identified object or in a different portion of the first image for example the manipulated first image may include a caption above the car stating MY CAR .

While the present application is primarily described in terms of methods a person of ordinary skill in the art will understand that the present application is also directed to various apparatus such as an electronic device including a mobile communications device. The electronic device includes components for performing at least some of the aspects and features of the described methods which may be by way of hardware components such as the memory and or the processor software or any combination of the two or in any other manner. Moreover an article of manufacture for use with the apparatus such as a pre recorded storage device or other similar computer readable medium including program instructions recorded thereon or a computer data signal carrying computer readable program instructions may direct an apparatus to facilitate the practice of the described methods. It is understood that such apparatus articles of manufacture and computer data signals also come within the scope of the present application.

The term computer readable medium as used herein means any medium which can store instructions for use by or execution by a computer or other computing device including but not limited to a portable computer diskette a hard disk drive HDD a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or flash memory an optical disc such as a Compact Disc CD Digital Versatile Disc DVD or Blu ray Disc and a solid state storage device e.g. NAND flash or synchronous dynamic RAM SDRAM .

Example embodiments of the present application are not limited to any particular operating system system architecture mobile device architecture server architecture or computer programming language.

The various embodiments presented above are merely examples and are in no way meant to limit the scope of this application. Variations of the innovations described herein will be apparent to persons of ordinary skill in the art such variations being within the intended scope of the present application. In particular features from one or more of the above described example embodiments may be selected to create alternative example embodiments including a sub combination of features which may not be explicitly described above. In addition features from one or more of the above described example embodiments may be selected and combined to create alternative example embodiments including a combination of features which may not be explicitly described above. Features suitable for such combinations and sub combinations would be readily apparent to persons skilled in the art upon review of the present application as a whole. The subject matter described herein and in the recited claims intends to cover and embrace all suitable changes in technology.

