---

title: System and method for performing packet queuing on a client device using packet service classifications
abstract: A client device having a networking layer and a network driver layer for transmitting network packets comprising: a plurality of transmit queues configured at the network layer, each of the transmit queues having different packet service classifications associated therewith, packets being queued in one of the transmit queues according to traffic service classifications assigned to the packets; a classifier module for classifying packets according to the different packet service classifications, wherein a packet to be transmitted is stored in one of the transmit queues based on the packet service classifications; and a network layer packet scheduler for scheduling packets for transmission from each of the transmit queues at the networking layer, the network layer packet scheduler scheduling packets for transmission according to the packet service classifications.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08873394&OS=08873394&RS=08873394
owner: Apple Inc.
number: 08873394
owner_city: Cupertino
owner_country: US
publication_date: 20120915
---
This application is related to and claims the benefit of U.S. Provisional Patent Application No. 61 595 003 filed Feb. 3 2012 entitled SYSTEM AND METHOD FOR INTELLIGENT NETWORK QUEUE MANAGEMENT by Cahya Masputra et al. which is hereby incorporated by reference herein in its entirety.

This application is related to concurrently filed U.S. application Ser. Nos. 13 620 920 13 620 988 13 621 027 13 621 043 13 621 056 13 621 079 13 621 091 13 621 100 and 13 621 110 filed Sep. 15 2012 by Cahya Masputra et al. assigned to Apple Inc.

An embodiment of the invention is directed to managing data network communications in a client device. Other embodiments are also described.

A data network allows people to communicate with each other and obtain information from various sources on the network using their respective client devices that are on the network. For example a Web browser application program running in a user s workstation or laptop computer can connect with a Web server to download a Web page. The connection may span several intermediate nodes or hops of the network which may include specialized computers such as routers. These devices can discover routes between the end nodes through which they can forward messages that have been broken up into packets of data. Each node may be assigned a unique or global address such as an Internet Protocol IP address. The Internet is a well known global inter network in which networks of computers are connected to each other via routers.

Computer network protocols have a layered architecture. Typically the upper most layer includes the functionality provided by an application program such as a Web browser. This is the layer that at least in the end nodes may initiate a connection between two computers over a network. Thus for example a user may select a desired Website on his computer. The Web browser running in that computer starts a procedure that results in a connection being made with a server that is associated with the selected Website. The Web browser sends the request down through a series of functions referred to as an Internet protocol suite or Transport Control Protocol Internet protocol TCP IP stack. This stack of protocols is typically implemented in software at its higher layers often as part of an operating system OS program running in the client device. Once the selected Website has been translated into an IP address of a Web server the server is contacted over the Internet and an appropriate connection is made with an upper layer program of a similar protocol suite implemented in the Web server.

To use the connection the TCP IP stack in the user s computer encapsulates a request message from the Web browser in this example a request identifying the Web page. The message may be encapsulated more than once by several vertical layers on its way down in the protocol stack including a network access layer. It finally arrives at the lowest layer of the client device namely the physical layer which is typically deemed to be a part of the network access layer .

After leaving the physical layer of the user s computer and then making its way through one or more hops in the network the message from the Web browser arrives in the Web server and is passed up the protocol stack in the Web server to a program that is deemed a peer of the Web browser. The peer program may then respond to the message by causing the data for the requested Web page to be collected and sent back to the user s computer through the existing network connection. The data is broken up into multiple messages or packets and is sent in a manner analogous to how the request message was sent.

An application program can have several applications or processes that are executed by one or more processors in the user s client computer. Each individual application may generate different types of network data traffic which may have different packet loss latency and flow elasticity requirements. By way of example a social networking application may communicate control data text audio and video over the network each of which have different requirements with respect to the above variables. Each application is typically provided with its own port or group of ports to communicate this data though they may all share the same lower layer network resources in the user s computer. In current implementations the routers interconnecting each client device to a particular destination node i.e. another client or a server over the network include large transmit and receive buffers. As such there is little or no packet loss and client devices are typically permitted to transmit packets without regard to flow control resulting in buffer bloat within the router queues. Protocols such as TCP are self tuning protocols which determine congestion and modify transmission speed based on detected packet loss. When packet loss is mitigated using large buffers the TCP protocol

Additionally in current client side implementations the buffering of packets occurs at the driver level. The TCP IP stack simply pushes packets down to the driver and the driver manages its own transmit and receive queues. Because of the large amount of buffering performed at the driver level within the client an Ethernet driver can buffer up to 4000 packets in a queue prior to transmission the networking stack is not provided with accurate network congestion information. As such what is needed is a more intelligent mechanism for performing network queuing within a client device in which a feedback channel is utilized used between the driver layer and the network stack layer.

A client device having a networking layer and a network driver layer for transmitting network packets comprising a plurality of transmit queues configured at the network layer each of the transmit queues having different packet service classifications associated therewith packets being queued in one of the transmit queues according to traffic service classifications assigned to the packets a classifier module for classifying packets according to the different packet service classifications wherein a packet to be transmitted is stored in one of the transmit queues based on the packet service classifications and a network layer packet scheduler for scheduling packets for transmission from each of the transmit queues at the networking layer the network layer packet scheduler scheduling packets for transmission according to the packet service classifications.

An embodiment of the invention is directed to a computer implemented method for active queue management for networking applications executed on a client device.

The networking stack of this embodiment includes queue management logic for managing a plurality of networking queues on behalf of each of the applications . A packet scheduler in the networking stack schedules packets to be transmitted and received from to each of the queues based on packet classifications as described in greater detail below . Although illustrated as separate modules in it will be appreciated that the queue management logic and scheduler may be implemented as a single integrated software module.

In one embodiment each of the queues managed by the queue management logic includes a send queue for storing outgoing network packets e.g. TCP IP packets and a receive queue for storing incoming network packets. The send and receive queues are managed differently for each of the applications based on the networking requirements of each of the applications . For example different applications may have different packet loss latency and flow elasticity requirements all of which are monitored and managed by the queue management logic . In one embodiment the networking requirements of each application are specified ahead of time e.g. when the application initially registers via the API and the networking packets for that application are managed by the queue management logic based on the specified requirements. By way of example a Web browsing application is typically more latency tolerant than a real time video chat application. Consequently the Web browsing application will be associated with a different queue having different specified service level than the real time video chat application.

In one embodiment each driver installed on the client device includes queue management logic for managing a set of driver queues each of which may also be associated with a different service level. In addition each driver may have its own scheduler for performing packet scheduling at the driver level and its own queue management logic for managing the queues . As with the networking stack the driver queue management logic and driver scheduler may be implemented as a single logical module rather than separate modules as illustrated in . In one embodiment each driver may either choose to manage packet scheduling on its own referred to herein as driver managed scheduling or may rely on the packet scheduler and queue management logic of the networking stack for packet scheduling queuing.

By way of example and not limitation Ethernet drivers and cellular wireless e.g. 3G 4G drivers may rely on the packet scheduling and queuing provided by the scheduler of the networking stack while 802.11n Wi Fi drivers may use the driver scheduler to manage packet scheduling and queuing. In one embodiment the Wi Fi driver implements Wireless Multimedia Extensions WME also known as Wi Fi Multimedia WMM standard to schedule network traffic according to the four priority levels of voice video best effort and background. However the underlying principles of the invention are not limited to any particular networking standard.

In one embodiment the Wi Fi driver is capable of dynamically switching between driver managed scheduling and scheduling at the network layer. For example when on an 802.11n network which supports WME the driver may choose driver level scheduling but when on an 802.11b or 802.11g network the driver may choose scheduling at the network layer. In one embodiment when utilizing network stack managed scheduling the network stack will notify the driver when a packet is ready to be dequeued. The driver will then dequeue and transmit the packet as described in greater detail below .

As illustrated in in contrast to prior implementations in which packets are pushed from the networking stack to the driver and buffered in the driver regardless of network conditions in one embodiment of the invention continuous feedback is provided between the driver layer and the networking stack . The feedback from the driver to the networking stack ensures that the networking stack is aware of networking conditions of the communication link managed by the driver and may perform packet scheduling queuing based on this knowledge. In one embodiment the networking stack implements packet scheduling and queuing intelligently as described herein based on the detected networking conditions. Similarly the feedback signal from the networking stack to the driver notifies the driver of the conditions within the transmit receive queues of the networking stack e.g. such as when a new packet is ready to be transmitted from a particular queue .

As illustrated in scheduling may be employed for at the network stack layer for some communication channels and at the driver layer for others e.g. some WiFi channels as discussed above . In particular in the illustrated embodiment packet scheduling may be performed in the network stack layer for the communication channels to servers and and client whereas driver managed scheduling may be performed for the communication channel to server . In addition as shown in a single application may have different types of data traffic assigned to different queues which support different packet loss latency and flow elasticity requirements. For example a particular application may open a TCP UDP socket to communicate control data text audio and video each of which have different requirements with respect to the above variables. As such one type of data e.g. control data may be queued in queue associated with a first service class and a second type of data e.g. interactive video may be queued in queue associated with a second service class. Additionally different applications may queue data in the same queue associated with the same service class. For example applications and may queue control data in queue associated with a service class for control data and applications and may queue interactive video data in queue associated with a service class for interactive video data.

Additionally It will be understood that depending on network connectivity e.g. whether the client is coupled to Ethernet or Wifi and other network variables the client device may utilize only network layer queue management and or scheduling or only driver managed queue management and or scheduling while still complying with the underlying principles of the invention.

A method in accordance with one embodiment of the invention is illustrated in . At a packet to be transmitted is received at the networking layer of a protocol stack on a client device. If the packet is associated with a network link that uses driver managed scheduling determined at then at the packet is provided to the driver layer. The driver layer then queues schedules and transmits the packet at . If however the packet is associated with a network link that performs packet scheduling at the network layer then at the network stack queues and schedules the packet for transmission. At the driver is notified when the packet is ready to be transmitted and at the driver transmits the packet.

The client computing device may be a desktop computer a notebook or laptop computer a video game machine or other consumer electronic device. In some embodiments described herein the client device is a portable wireless device that may include two way voice and video email messaging and media playback functions. The communication path between the client device and a server in this example has a wireless segment between the client device and a wireless base station e.g. a cell tower or Wifi access point . In the Internet reference model the networking stack client device communicates with a network access gateway via the base station in accordance with any suitable wireless communications network access protocol some examples of which are given below. The other client device may be reached via the combination of another base station and gateway. On top of the network access layer are the internetworking layer e.g. defining an Internet Protocol IP address for each node on the network the transport layer e.g. Transport Control Protocol TCP performing host to host flow control and the opening and closing of connections and the application layer e.g. application programs and process protocols such as HTTP SMTP and SSH .

Regardless of how the packet is queued it may be dequeued differently depending on whether the driver or network stack managed scheduling. For driver managed scheduling determined at the driver performs a dequeue operation from a specified service class at . For example if the driver is implementing 802.11n then it may choose to perform the scheduling using the four service classes defined by WMM see e.g. illustrating the 10 4 mapping between service classes and queue instances . Alternatively for other network interface types e.g. Ethernet 3G etc scheduling may be performed at the network layer see e.g. illustrating a 1 1 mapping between service classes and queue instances . Thus at the network layer performs the dequeue operation from the selected service class. At the packet is provided to the driver layer which transmits the packet at .

Thus it can be seen from the above that in one embodiment when a packet needs to be transmitted the packet is passed into a network layer scheduler configured for the network interface. The scheduler extracts the service class from packet service class determines the queue instance to enqueue the packet on. The packet then gets enqueued onto the corresponding queue instance the packet may be dropped if queue is full or flow controlling the decision to drop enqueue is left to the queueing discipline algorithm e.g. SFB as described below . The driver is notified that there is work to do. At some point the driver dequeues a packet. The queue instance needs to be identified. If the interface is configured for network stack scheduling the scheduler selects eligible queue to be serviced. If the interface is configured for driver scheduling the driver indicates to the scheduler the queue to be chosen for service. Once a queue instance is identified a packet if available is dequeued from the queue. The packet is then handed to the driver for transmission over medium which transmits the packet.

As indicated in in one embodiment continuous feedback is provided from the networking stack to each of the applications as indicated by the dotted arrows and is used to provide flow control for the network flow to from each of the applications . For example when the transmit queue for a particular TCP or UDP socket has reached a specified threshold a feedback signal is generated to instruct the respective application to suspend or reduce new packet transmissions.

In the network stack managed model illustrated in the application sends packets to be transmitted to the network stack which then classifies the packet using the classification schemes described below and places the classified packet in an appropriate send queue . In one embodiment there are as many different send queues as there are packet classifications e.g. 10 different send queues for 10 different packet classifications . The networking stack notifies the driver layer when a new packet is ready for transmission in one of the queues. The IO networking interface of the driver then dequeues the packet and passes the dequeued packet to the driver for transmission .

Allocate an ifnet instance which supports the new output model. This is an extended private version of the public ifnet allocate KPI which requires the newly defined ifnet init eparams structure to be filled by the caller. This structure is analogous to the ifnet init params with several new fields that are related to the new output model 

Enqueue a packet to the output queue of an interface which implements the new driver output model. This is provided for a driver family which implements a pre enqueue callback.

Dequeue one or more packets from the output queue of an interface which implements the new driver output model and that the scheduling model is set to normal. 

Dequeue one or more packets from the output queue of an interface which implements the new driver output model and that the scheduling model is set to driver managed. 

Trigger the transmission at the driver layer on an interface which implements the new driver output model. This may result in the driver s start callback to be invoked if not already.

Set and get the uplink and downlink link rates of the interface. The rates may be set by the driver at anytime after the ifnet is attached whenever the information is available at its layer.

Alternative mechanisms to estimate the uplink link rate when the driver is not able to easily retrieve such information from the hardware. These inform the networking stack about the beginning and end of transmission of a burst.

In one embodiment a driver that has registered itself as supporting the new output model i.e. the network stack managed scheduling is flagged with the IFEF TXSTART flag.

An interface which supports the new output model i.e. network layer scheduling uses a dedicated kernel thread the starter thread whose job is to invoke the driver s start callback. In one embodiment this thread is signaled to run if not already whenever ifnet start is called as part of enqueuing a packet via ifnet enqueue allowing the application thread to return immediately upon enqueuing the packet to the output queue. It provides a form of serialization for the driver s start callback so that the dequeue can happen in order. It also reduces complexity at the driver layer as the driver may perform certain operations hardware related or not that may momentarily block the execution of the thread without worrying too much about the impact as no lock is held by the networking stack when it executes the driver s start callback from the context of this starter thread.

Additionally the network layer managed output model allows for a form of uplink rate limiting at the ifnet layer when a Token Bucket Regulator TBR is configured for the interface. By default an interface does not have a TBR configured enabling TBR requires manual configuration via ifconfig or pfctl . When TBR is enabled the starter thread will periodically wakeup every 10 ms whenever the output queue is non empty as illustrated at in . During each period the driver is allowed to dequeue as many bytes as there are available tokens the tokens get refilled at the beginning of each period. The number of tokens are computed according to the rate for which the TBR is configured for. One particular TBR implementation does not require callouts to be allocated unlike the approach taken by BSD because of this it can accommodate extremely high rates tens of Gbps with very low CPU overhead as the interval is fixed and thus independent of the callout resolution 10 ms is achievable across different platforms. 

The if snd member of ifnet holds the transmit queues for the interface. This data structure contains information about the built in scheduler type instance callbacks TBR and optionally an alternative scheduler.

By default in one embodiment the system creates a built in instance of packet scheduler ifcq instance . As mentioned the choice of a packet scheduler and its parameters depends on the type of the network interface and in some instance the topology of the network as well. In one embodiment when a packet scheduler is attached it stores its instance in ifcq disc and configures the enqueue dequeue and request callbacks to the scheduler s corresponding routines. For interfaces which requires a driver managed model a special scheduler is attached which provides an alternative dequeue sc instead of a dequeue callback. Certain embodiments of these callbacks are as follows 

One embodiment of the scheduler instantiates N number of classes each class correlates to a service class and manages a queue instance . Packets are enqueued in one of these queue instances depending on how they are classified. When PF ALTQ support is configured the built in scheduler and its parameters may be overridden via the Packet Filter PF infrastructure e.g. by way of pfctl . This provides for a convenient way for different characteristics of the packet scheduling to be modeled e.g. trying out different schedulers and or parameters .

One embodiment of the packet scheduler module provides entry points for enqueuing and dequeuing packets to and from one of its class queue instances . In one embodiment each class corresponds to a queue. It manages all of its queues depending upon the scheduling algorithm and parameters.

In one embodiment a scheduler gets configured and attached to an ifnet through one of the following techniques 

When an interface is attached to the networking stack a scheduler is chosen based upon the queue scheduling model that is requested by the driver. For a normal model the stack creates a scheduler with 10 classes hence queues . For a driver managed model an instance of a special scheduler with 4 classes is created instead.

In one embodiment the built in static configuration may be overridden by configuring the scheduler and its parameters via the PF framework. This requires the PF ALTQ configuration option to be enabled and altq 1 boot args NVRAM option to be present. In one embodiment it is not enabled allowed by default. However when enabled it allows for a convenient and expedient mechanism for experimenting with different schedulers and parameters.

As illustrated in the QFQ configuration used in one embodiment for network layer managed scheduling provides a 1 1 mapping between packet service classes and packet queue instances respectively. As illustrated the 10 service levels are roughly divided into 4 groups and prioritization is provided within each group. The groups are defined based upon the characteristics of the classified traffics in terms of the delay tolerance low high loss tolerance low high elastic vs. inelastic flow as well as other factors such as packet size and rate. As described herein an elastic flow is one which requires a relatively fixed bandwidth whereas an inelastic flow is one for which a non fixed bandwith is acceptable. The illustrated 1 1 mapping allows for the networking stack to achieve full control over the behavior and differentiation of each service class a packet is enqueued directly into one of the queues according to how it was classified during dequeue the scheduler determines the packet that is to be transmitted from the most eligible queue.

As illustrated in the TCQ configuration used for driver managed scheduling in one embodiment provides a 10 4 mapping between 10 service classes and 4 queue instances . This scheduler is passive in the sense that it does not perform any kind of packet scheduling but instead simply provides the queue instances and maps the service classes to the queues. Because the networking stack of this embodiment has no control over the scheduling the queues cannot be defined with similar characteristics as those for the 1 1 mapping. Instead the could be perceived as having priorities ranging from low L to highest H . A packet is enqueued into directly into one of the queues which represents the service class that it was originally classified with. During dequeue the driver is responsible for choosing the most eligible service class for transmission. The number of queues is set to 4 and is an implementation artifact as mentioned this also happens to be the number of 802.11 WMM access categories.

In one embodiment a queuing discipline or algorithm module manages a single instance of a class queue a queue simply consists of one or more packets mbufs . The algorithm is responsible for determining whether or not a packet should be enqueued or dropped.

In one embodiment a queuing algorithm gets configured and attached to an instance of a scheduler class through one of the following ways 

When a scheduler class gets instantiated as part of configuring a packet scheduler on an ifnet a queuing algorithm is chosen. All classes of the scheduler share the same queue algorithm each with its own unique instance. 

Alternatively the built in static configuration may be overridden by configuring the scheduler and its parameters including the queuing algorithms for the classes via the packet filter PF framework.

In one embodiment the following queuing algorithms are not used by default and are available only via PF 

As mentioned in one embodiment each outbound packet is enqueued in a class queue instance that corresponds to the packet s service class. The service class assignment or packet classification occurs in several places throughout the networking stack. In general packet classification can be explicit opted in by the application or implicit set or overridden by the system .

In one embodiment an application may classify its traffics by issuing the SO TRAFFIC CLASS option either sticky via setsockopt or on a per message basis with sendmsg using one of the following traffic service class values which are illustrated mapped to service classes in 

Thus it can be seen from the above that in one embodiment the system assigns network control packets to the highest priority classification thereby ensuring that control packets are forwarded ahead of packets having all lesser classifications. This is an improvement over prior systems in which certain control packets e.g. such as TCP acknowledgement ACK packets could become stuck in a queue behind other types of non control packets thereby reducing system performance .

In one embodiment any packets classified as background system initiated SO TC BK SYS will be suspended in queues while a voice call is occurring on the client device. As such this embodiment provides significant benefits over prior systems in which a voice call could be degraded or dropped as the result of low priority packets e.g. background system initiated packets being transmitted. Thus in this embodiment a user s photo stream or other data to be backed up to a service e.g. such as iCloud will not interfere with voice calls.

One embodiment of the system can prevent traffic marked as background system initiated so that they do not interfere with an incoming phone call there by increasing the reliability of the call. When a phone call is initiated the network layer e.g. TCP IP layer will receive a flow control notification to suspend all background system initiated traffic. In response the network layer may stop sending down any more packets to the interface. It may also stop the application from writing any more data down the network stack. This will help to improve CPU utilization because the application is quiesced and it also improves reliability of voice calls. If the voice call completes in a reasonable duration of time the applications can resume data communication.

In one embodiment when a particular lower priority application is suspended the network stack will periodically probe the communication link e.g. via feedback signal to determine when it may resume transmission and will communicate this information to the respective application. When the link is no longer loaded packet transmission will then resume.

In summary the continuous flow control feedback between the driver layer and the networking stack and the feedback between the networking stack and the applications provides for a more intelligent efficient allocation of network channels and bandwidth.

In one embodiment the above values do not imply guarantees but are rather hints from the application to the system about the characteristics of its traffic. The system will do its best in providing some form of differentiations on the traffics based on their classifications but no guarantee is made due to the varying factors ranging from packet scheduling parameters to network topology or media conditions.

In one embodiment traffic generated by a socket associated with one of the above values will carry the corresponding service class value in the queue buffer mbuf there is a 1 1 mapping between SO TC and MBUF SC values as illustrated in .

This form of classification is possible via the packet filter PF framework. It allows for the classification rules to be installed via PF and take effect for all IP traffics regardless of how they were originally classified at the origin. PF and pfctl have been enhanced with service class related keywords illustrates an example of a PF configuration file that can be processed by pfctl to override the built in settings.

Thus in the explicit classification case an application opens a socket with the default service class BE . The application may set the service class of the socket via the SO TRAFFIC CLASS socket option so that all future send write operations will automatically cause packets to be marked with the corresponding service class. The application may choose to selectively associate each datagram sent down the socket with a SO TRAFFIC CLASS ancillary message option so that the associated packet will be marked with the corresponding service class but will not affect other current or future packets. In this case we can easily have many different service classes associated with this socket.

In the implicit classification case classification rules are installed in the packet filter engine. Each rule contains a signature e.g. protocol ports etc which upon a match would result in the packet to be marked with a service class.

In addition to marking the queue buffer mbuf with a MBUF SC value in one embodiment the module performing packet classification also associates one or more tags with the packet in order to assist the rest of the system in identifying the type or flow of the packet. In one embodiment these tags reside within the built in pf mtag sub structure of the mbuf and are set regardless of how the classification is performed explicit or implicit . The tags employed in one embodiment are as follows 

As illustrated in in one embodiment each mbuf that is being sent out through an interface is tagged with a flow hash hence marked with PF TAG FLOWHASH which will help to identify all the packets that belong to a particular flow at the interface layer. In one embodiment a flow hash is a 32 bit integer and it is calculated in one of the following places 

In one embodiment when a socket is connected the flow hash for the socket is computed and stored. Further transmissions on this socket will cause the hash value to be carried within the mbuf structure of the packets.

In one embodiment when a packet enters the driver a flow hash will be computed and stored in the associated PF rule and states unless it is already classified. If the packet is passed back to IP successfully it will carry with it the flow hash associated with the rule or state which it was matched against in the mbuf structure.

In one embodiment the hashing algorithms used to compute the flow hash differs across computing system platforms depending on performance. The following table illustrates exemplary platforms and corresponding hashes 

Using the queue management techniques described herein applications sending using TCP are flow controlled when the number of packets per flow queued at the interface reaches an upper limit. Instead of using an indicator like explicit congestion notification ECN or packet drops the interface provides a flow advisory feedback to the transport layer. This can be done without any packet loss.

A flow advisory on a connection is received from AQM when one of the following two conditions is true 

In both of these cases sending more packets will accumulate packets in the interface queue and will increase the latency experienced by the application. Otherwise it might cause packet drops which will reduce the performance because the TCP sender will have to retransmit those packets. By using the flow advisory mechanism the TCP senders can adapt to the bandwidth available without seeing any packet loss or any loss of performance. The interface queue will never drop a TCP packet but it will only send a flow advisory to the connection. Because of this mechanism buffering in device drivers was reduced by a significant amount resulting in improved latency for all TCP connections on the device.

The main response of a TCP connection to a flow advisory is to reduce its congestion window which will in effect reduce its sending rate. This is done by backing off the slow start threshold and allowing the connection to enter congestion avoidance phase. But if the connection is already in recovery it means that the connection has already experienced packet loss in that round trip time and has already lowered its sending rate. In this case the congestion window is not reduced any further.

A connection that is flow controlled will avoid making the socket writable until flow control is lifted. This will prevent the application from writing more data that might just get buffered up in the network stack when it can not send packets out on the interface. This will help interactive applications that need to send only the latest updates and would rather throw away the older updates.

While in a flow controlled state if there is an indication of packet loss in TCP acknowledgements received in the form of duplicate acknowledgements or SACK information then the connection will abort flow control and start fast recovery to retransmit the lost data right away. This will not increase latency any more because the rate of packets sent during recovery is limited. Since the interface is guaranteed to not drop any TCP packets the connection will be able to retransmit the lost data as quickly as possible.

When a connection is in a flow controlled state it means that packets are leaving the interface slower than before. In this situation there can be packets waiting in the interface queue ready to be sent. Usually these are the packets at the end of the last send window. If this wait time is more than the retransmit timeout calculated on the connection then a timeout will fire. At this point retransmitting already sent data might create duplicate copies of the same packet in the interface queue. This might generate duplicate acknowledgements later and cause the connection to go into recovery unnecessarily.

To avoid this confusion a flow controlled TCP connection will avoid retransmitting packets from a retransmit timer until later. If the wait is too long then the connection might get timed out instead of waiting forever and an error will be returned to the application.

Every time a retransmit timeout fires the timer is backed off before trying again. This will help to detect a sudden increase in delay on the link. But for flow controlled sockets the delay might be a result of the flow being blocked temporarily. When the connection comes out of the flow controlled state the back off is undone. This will help to fire the retransmit timer in a timely fashion from then on.

When the packets in the interface queue flow out and the queue levels fall below a threshold the interface will generate a flow advisory to let all the flows that were flow controlled to start sending data again. At this point the TCP sockets also become writable and the applications can start writing data.

When flow control is lifted the connection will send new data that was never sent before. This will generate new acknowledgements and will start the ACK timer. It will also help to detect if there was any data loss prior to flow control that was not already detected. If there is no new data to be sent the retransmit timer will fire soon and it will trigger retransmission of any outstanding data that has not been acknowledged.

Using Flow advisory and flow control mechanism a TCP connection will be able to adapt to the variations in link bandwidth and will be able to minimize delay induced by buffering packets at multiple levels on the host.

In one embodiment a UDP socket is capable of flow control only if it is connected to the peer using a connect system call. When the number of packets from a UDP flow in the interface queue is more than the limit for flow control an advisory is generated. The socket is marked as flow controlled at that point. Unlike TCP the interface queue will drop all UDP packets generated thereafter. The socket will not be writeable which means an application waiting for write event using select or poll or kevent system classes will not get the event until flow control is lifted. If an application writes data to the socket anyways the packet will be dropped by the socket layer and an error ENOBUFS will be returned.

This is different from the previous behavior where all UDP writes succeeded only to drop the packet later by the driver. The UDP flow control and advisory will give immediate feedback to the applications so that they can reduce their send rate immediately. For instance a video application can change it s encoding to send less data over the network.

Since the packet is dropped at the socket layer on a flow controlled UDP socket it saves a lot of CPU utilization compared to the previous where the packet was processed and sent all the way to the driver only to be dropped. Another advantage is that a flow controlled UDP flow can not overwhelm the interface. This will reduce cause packet loss and improve latency for the other flows on the host.

In one embodiment the tracking of flows at the interface layer is made possible due to the use of Stochastic Fair Blue SFB see Appendix B to this patent application as the queuing algorithm. In one embodiment the implementation of SFB uses a 2 level bloom filter whereby a flow as indicated by the flow hash value maps to exactly one bin at each SFB level . Each bin of this embodiment tracks the number of packets as well as the flow drop mark probability. In one embodiment SFB also tracks the list of flows being flow controlled. The thresholds for flow control and flow advisory are based upon the bin allocation currently set to of the queue limit . The bin probability is updated accordingly but it is currently not used for rate limiting.

In one embodiment certain sockets that are marked as opportunistic are suspended when a network interface is throttled. Packets generated by such sockets will be dropped in one embodiment when they are enqueued on the affected queues. In one embodiment a NOTE SUSPENDED event will be generated on the socket in order to inform the application that traffics on the socket are indefinitely blocked. The Application may then decide whether or not to abort the connection. When the interface is no longer throttled the affected queues will no longer block packets and a NOTE RESUMED event will be generated on the affected sockets. Internally the same mechanism may be 0 used by flow control and advisory is used for implementing suspend and resume.

Opportunistic polling of one embodiment uses a network driver input model as illustrated in . The driver component polls the hardware for incoming packets and the IO networking interface polls the driver. Each receive queue instance polls the IP networking interface of the driver to determine if there are any packets associated with that receive queue ready to be dequeued and passed up to the network stack and subsequently up to the requesting application which as illustrated at polls the receive queue instance with which it is associated for new packets .

Thus with this new model inbound packets are no longer pushed up to the networking stack by the driver family as illustrated by operations in . Instead inbound packets reside in the driver s receive queue until they are dequeued by the networking stack . In one embodiment this involves turning off the client device hardware s receive interrupt IRQ . One reason the described embodiment is unique is that the networking stack in conjunction with the driver alternates between polling and the legacy model depending upon the load factor. In one embodiment when the load reaches a predetermined threshold e.g. a specified level of packets built up in the driver s queues then the system may transition to the legacy model . When transitioning to the legacy model the hardware s receive IRQ is turned on and the driver pushes the packets up from the IO networking interface to the appropriate receive queue instance and ultimately to the requesting application via the network stack .

Analogous to ifnet input except that the driver provides the networking stack with all of the information related to the beginning and end of the packet chain as well as the total packet and byte counts. Drivers which already possesses this information are encouraged to utilize this new variant as it allows for better efficiency. This may be used regardless of whether the driver adopts the new model .

In one embodiment a driver that has registered itself as supporting the new input model is flagged with the IFEF RXPOLL flag.

In one embodiment input packet processing throughout the networking stack occurs within the context of a DLIL input thread. Some interfaces have their own dedicated input threads while others share a common main input thread. In one embodiment there are 3 variants of DLIL input threads 

In one embodiment the main input thread is used by the loopback interface as well as other interfaces which don t get their own dedicated input threads i.e. anything but Ethernet PDP or those that don t support RXPOLL . This thread is also used for handling all protocol registrations and packet injections. This is implemented in dlil maininput thread func .

In one embodiment legacy is used by Ethernet PDP interfaces which do not adopt the RXPOLL model implemented in dlil input thread func .

In one embodiment RXPOLL is used by any interface which adopts the RXPOLL model implemented in dlil rxpoll input thread func .

In one embodiment an interface which supports the new input model RXPOLL uses a dedicated kernel thread the poller thread illustrated at in whose job is to invoke the driver s input poll callback in order to retrieve one or more packets from the driver this occurs when polling is ON otherwise the poller thread stays dormant. This thread is analogous to the work loop thread illustrated at in where they both end up calling ifnet input in order to deposit the packets to the receive queue of the RXPOLL capable DLIL input thread.

The packets are then sent up the networking stack for further processing from the context of this DLIL input thread.

In one embodiment RXPOLL capable interfaces transition between IFNET MODEL INPUT POLL OFF and IFNET MODEL INPUT POLL ON modes. In one embodiment the former is the default initial mode the network stack chooses this mode for the interface when it determines that the load factor is low. The load factor is currently determined by looking at the EWMA of the packets and bytes in the DLIL receive queue P avg B avg and the EWMA of the DLIL input thread wakeup requests W avg .

Referring to the DLL input thread in in one embodiment switching to the IFNET MODEL INPUT POLL ON is done when P avg P hiwat B avg B hiwat W avg W hiwat where P hiwat B hiwat and W hiwat are the high watermark values for the packets bytes and the wakeup requests respectively. Conversely switching to the IFNET MODEL INPUT POLL OFF is done when P avg P lowat B avg B lowat W avg W lowat where P lowat B lowat and W lowat are the low watermark values for the variables.

In one embodiment these low and high watermark values are currently chosen arbitrarily based on certain workloads and they should be adjusted accordingly to accommodate future workloads and varying link speeds. 

In one embodiment The bulk of the hybrid polling logic resides within dlil rxpoll input thread func where the transitions between the modes take place by calling the driver s input ctl callback based upon the above logic. Care is taken to rate limit the transitions such that they do not occur too often the hold time is set to 1 sec by default. 

In one embodiment the main difference between polling OFF ON mode lies in the context and frequency in calling ifnet input or ifnet input extended .

In one embodiment when polling is OFF the work loop thread gets scheduled as part of the host CPU handling a receive IRQ from the hardware this IRQ signals the host that the device has transferred one or more packets to the host e.g. via DMA . Regardless of the level of IRQ coalescing done at the hardware the frequency of having this IOKit work loop thread scheduled is driven by the rate of inbound packets. The costs associated with this scheduling context switches etc. are quite significant especially given the fact that our system architecture routes all IRQs to CPU. Therefore in one embodiment upon detecting a high load factor polling is turned ON.

When polling is ON the work loop thread is quiesced by virtue of turning off the receive IRQ. Packets still get transferred to the host from the device and they accumulate in the driver s s receive buffer until they are retrieved by networking stack via the input poll callback. The poller thread which is now active performs the equivalent functionalities of the work loop thread except that the frequency of which this thread gets scheduled is tightly controlled by the networking stack .

In one embodiment polling results in improved performance given the amortizing of per packet processing costs related to receiving packets from the medium. When polling is turned ON the network stack instructs the driver to go into polling mode. While in polling mode the driver would turn OFF its receive interrupts or trap handlers associated with the notification of packets arriving from the hardware. Packets will keep on coming to the host s memory from the device via DMA or equivalent except that the CPU will not be interrupted. This reduces the load on the CPU as each interrupt would normally trigger a series of work to process it and it would have some negative effects on performance as it preempts whatever is running on the CPU at the time. The network stack then polls at 1 millisecond interval by default this is configurable and pulls packets from the driver during each interval. If it detects that the packet rate has dropped polling mode is exited and interrupt is reenabled.

In one embodiment polling may be employed to reduce power consumption e.g. when in low power mode or based on user activity or inactivity . For example if the system is in low power mode this information is supplied to the network stack and the network stack may then choose to enter polling mode on all eligible network interfaces. The network stack would then be informed when the system is no longer in low power mode so that polling mode can be exited.

With respect to use activity if the system is busy handling User Interface inputs this information is supplied to the network stack and the network stack may then choose to enter polling mode on all eligible network interfaces. The network stack would be informed when the system is no longer busy handling UI inputs so that polling mode can be exited.

In one embodiment the if inp member of ifnet holds the receive queue for the DLIL input thread . In one embodiment this data structure contains the information illustrated in .

Unlike its transmit counterpart the receive queue is associated with a DLIL input thread instance rather than with an interface. As mentioned above certain types of interfaces share the common main input thread while others get their own dedicated input threads. Also unlike transmit where there may be up to N transmit queues there is currently only 1 receive queue instance per input thread. This structure also holds the information about the actual kernel threads used for input work loop as well as the poller threads . In one embodiment all of these threads are configured to share the same thread affinity tag in order for them to be scheduled in the same processor set for better cache locality . The parameters needed for opportunistic polling e.g. mode P B W avg P B W lo hiwat also reside within this structure in one embodiment.

In one embodiment events related to the interface are sent from the networking stack to the attached scheduler and queue management logic and further onto all class queue instances . This allows for the scheduler queue management logic and its classes to adapt their parameters accordingly if needed. The events are as follows 

As mentioned above embodiments of the invention include support for two different modes of scheduling 1 scheduling at the network stack layer and 2 scheduling at the driver layer. The driver can choose which type of scheduling to use. In one embodiment if the driver is implementing 802.11n then it may choose to perform the scheduling using the four service classes defined by WMM see e.g. illustrating the 10 4 mapping between service classes and queue instances while if the driver is implementing any other interface e.g. 102.11b g 3G Ethernet etc it may choose to have scheduling performed at the network layer see e.g. illustrating a 1 1 mapping between service classes and queue instances .

In one embodiment in the driver managed model all of the queues may be set up as with the network stack managed model but the scheduling is performed by the driver scheduler . As such the driver based scheduler will then request a number of packets for each dequeue operation for a particular class based on priority i.e. using the 4 classes for WMM .

While the schedulers decide which queue from which to dequeue a packet the queuing algorithm implemented by the queue management logic also referred to as the dropper because it has the option to either drop or queue packets determines which queue into which a packet should be queued prior to dequeue. In one embodiment the scheduler hands the packet to the queue management logic which then determines whether the packet is dropped or enqueued.

As mentioned above in one embodiment the scheduler used by the network stack uses quick fair queuing QFQ while the driver scheduler uses traffic class queuing TCQ each of which are described in detail above.

In one embodiment the same queue management logic is used regardless of the type of scheduler selected. Stochastic fair blue SFB may be used as the default queueing algorithm implemented by the queue management logic for both the driver managed scheduler and the network level scheduler . As indicated in flow hashing allows packets from different flows sockets to be easily tracked within both the network and driver layer and utilized by the SFB algorithm. When a socket is initially connected the hash value is computed stored in the socket data structure and is used for the lifetime of the connected socket . Any time data is sent down from the socket to the lower layers e.g. transport IP sendq and driver the flow hash value is sent with the packet and is used to uniquely identify the socket flow. Once the flow is identified a counter is incremented within one or more of the SFB bins indicated by variable C in as each new packet is queued and decremented as each packet is dequeued. Thus the number of packets currently queued for each flow is known and is used to perform flow control for that flow. In one embodiment if a particular flow has packets queued beyond a specified threshold as indicated by counter C FADV THRESHOLD in then a probability value for that flow is incremented by some interval. Once the probability reaches its limit e.g. 1 this indicates that there are too many packets i.e. the application is sending too fast and further packets for this flow are dropped.

Once the number of packets in the queue drops below a threshold the queuing algorithm wakes a flow advisor thread which wakes up and turns off flow control for the socket . Consequently the socket may then transmit packets down through the lower layers at will until it is placed in a flow controlled state again. Thus illustrates a mechanism which provides feedback and allows flows on the same host to be moved between a flow controlled normal state without relying on internal mechanisms in TCP such as explicit congestion notification ECN . This improves performance because the unnecessary work done by sending data down to the network stack just to be dropped is reduced. In addition it allows the queue size in the driver to be decreased without packet drops. This is in contrast to the older mechanisms in which the application blindly transmits packets down the stack only to be dropped by the driver. In contrast in since the network stack knows what is going on the link because of queue monitoring it can use this information to intelligently change the behavior of the transport mechanism.

In one embodiment illustrated in greater detail in the SFB algorithm takes the hash value of the packet and computes another hash which is unique for SFB and which is used to pick the bits for the flow. As illustrated the original hash value computed by the socket is 4 bytes this may be provided to the hash generator of the SFB queuing logic . Certain flow hashes may be provided with a value of 0 and used to identify flows for which the flow control mechanisms described in here are not used. In one embodiment SFB generates a random number and computes the new hash using the random number A B C D . It uses these four values to populate two sets of bins a current set and a shadow set each of which has two levels as shown in . The two levels are implemented as an array which is indexed using the new hash values each slot is 256 bits in one embodiment . Thus the flow is mapped into the SFB bins using the flow hash value. To check whether a flow exists the SFB bins may be checked. The bloom filter functionality of the SFB algorithm is capable of checking whether a particular flow does not exist within the SFB bins. If one exists it can be indexed to determine the data associated with that flow e.g. the probability and counters mentioned above . This data is maintained by SFB as long as a flow still exists. The shadow set is used because at some interval SFB rehashes. Because of the property of a bloom filter many things can map to the same bin. If there are a lot of sockets some sockets that are sending data too much can impact other sockets that are not sending a lot of data which are mapped to the same bin. Thus SFB periodically re computes the internal hash based on a new random number. After a rehash the bins may move around for each flow. Additional details of the SFB algorithm can be found in the reference which is attached as Appendix B to this patent application and incorporated herein by reference. In this case the shadow set may be used to move the data to the new bins.

Application Driven Traffic Classifications and Flow Feedback Including Opportunistic Behavior Flow Control Traffic Suspension and Transport Layer Improvements

The application driven traffic classifications described above with respect to may be used to enable various opportunistic behaviors. As mentioned each application may explicitly state what the traffic class should be for its sockets. For example say one application is backing up date using a low priority traffic class such as BK SYS but the user wants to make a voice call. Because of the way GSM works if a voice call is in progress while network data is being sent then this increases the probability of the voice call to deteriorate or drop. Similarly if a user opens a web browser while on a call then it may also impact the call. To alleviate this in one embodiment traffic classes are used to suppress unnecessary traffic when a user is on a voice call. Any non urgent traffic e.g. background traffic BKS SYS may be suspended during the voice call. In one embodiment for suspended sockets when a dequeue happens the system pretends that there is nothing to dequeue and when an enqueue happens packets are dropped. Thus the transmitting application will see this as if the network has stopped and may wait before trying again. If the call is relatively quick then it will resume when the call has ended without shutting down the socket entirely . These techniques may be used for traffic classes other that BK SYS e.g. BK BE etc . In one embodiment the flow control advisory mechanism described above is also used to suspend sockets in these situations e.g. if suspended a flow control indication is sent . Thus by classifying traffic for certain sockets e.g. BK SYS an intelligent feedback mechanism is provided for handling traffic suspension and opportunistic behavior for these sockets is enabled in the kernel.

Note that the suspended state is different from the flow controlled state. In the flow controlled state the application may still be sending data but in the suspended state the link is blocked e.g. because a voice call takes a long time . Thus in the suspended state it is beneficial to stop sending any more packets because they will just get dropped because the queue is suspended . In one embodiment the suspended application may choose to start a timer and if suspended too long simply close the connection.

 1 An authoritative entity in the system configures the cellular network interface s into opportunistic throttling mode.

 3 The scheduler goes through all transmit queues that are to be suspended in this opportunistic mode at present this applies only to BK SYS transmit queue. Each affected transmit queue goes into a suspended mode all existing packets are flushed and further enqueues will cause drops.

 1 An authoritative entity in the system removes opportunistic throttling mode from the cellular network interface s .

 3 The scheduler goes thru all transmit queues that were suspended in this opportunistic mode at present this applies only to BK SYS transmit queue. Each affected transmit queue is resumed further enqueues are allowed.

In addition performance optimizations are realized for the receive side referred to herein as large receive offload. In one embodiment this works by reducing the per packet cost by calling functions in the network stack. For example 10 packets are received rather than dealing with all 10 only 1 or 2 may be processed. For certain classes such as the AV class streaming video we can enable this optimization. With AV video applications the software buffers video for several seconds before it starts to play. So because of this this application may receive performance benefits using the large receive overload techniques because they not delay sensitive.

In one embodiment because of the feedback provided up the stack the manner in which TCP sends and receives may be adjusted for certain sockets. For example if the application classifies a flow as BK or BK SYS TCP will be less aggressive. For example if a socket is BK and high congestion on this link is detected the application may back off and receive a better response time for other flows e.g. a socket backing up to the network may be delayed to receive HTTP on another socket . All of this is possible because applications are able to explicitly specify socket traffic classes.

As previously mentioned in one embodiment the highest classification CTL is used for network control packets which may include ARP ACK responses neighbor discovery for IPV6 multicast join and leave DCP related packets IPV6 station router advertisement packets and DNS packets. Consequently DNS operations will not be delayed if the user is performing a large upload using a lower priority socket. Similarly ACK responses will not be delayed as the result of activity on lower priority sockets. Control packets of this type are very small and can go out without any delay.

In addition as mentioned the techniques and architecture described above provide for built in TCP and UDP backoff mechanisms during flow control and suspension states. Usually TCP responds to congestion itself by backing off e.g. using ECN as described above . TCP sends a packet and if does not get an acknowledgement ACK it sends the packet again and again and backs off with each retransmission. This retransmission is a waste of resources if it is known that the packet is still in the interface queue and will be sent only after the link becomes available. Thus in this situation there is no need to back off and retransmit using the built in TCP or UDP mechanisms. Instead using the techniques described herein a flow control advisory may be issued to disable the retransmit function.

In addition in one embodiment the retransmit timer used for TCP may be tweaked to operate more efficiently. Typically TCP backs off the retransmit timer when no ACKs are received e.g. increasing by 2 each time . Thus retransmit timer can become several seconds. Since feedback from the interface is provided as described above this backoff does not need to occur. Once we get a flow control advisory we can automatically wait because we know the packet is queued and don t want to keep resending . However in one embodiment the retransmit timer may be used after the flow control has been turned off for the socket.

In addition the flow advisory is propagated all the way to the application which does not write any more data to TCP. The application could therefore drop all of the stale data e.g. old video frames and send newer data when flow control is turned off. This is particularly beneficial for interactive video data where audio and video may be out of sync if the application does not drop stale data.

Similar principles may be applied to applications which transmit with UDP. Without the flow advisory techniques described herein UDP does not provide any feedback to its applications. Thus if the interface drops the application does not know and continues to transmit packets which must therefore be buffered at the lower levels of the stack wasting resources. By contrast when in a flow controlled state all of the writes by the application may be dropped right away thereby saving buffering resources . Doing the above reduces a significant amount of delay which would result by buffering packets.

Opportunistic Polling Mechanisms for Handling Inbound Network Traffic Improving Network Performance and Handling Denial of Service Attacks

As mentioned above in one embodiment RXPOLL capable interfaces transition between an input polling off mode and an input polling on or legacy mode of operation e.g. IFNET MODEL INPUT POLL OFF and IFNET MODEL INPUT POLL ON. In one embodiment the input polling off mode is used as a default initial mode and is selected by the network stack when it determines that the load factor is low. The load factor may be determined by evaluating the variables P avg B avg and W avg the values for the number of packets bytes and the wakeup requests respectively . Referring to the DLL input thread in in one embodiment switching to the input poling on mode is done when P avg P hiwat B avg B hiwat W avg W hiwat where P hiwat B hiwat and W hiwat are where P hiwat B hiwat and W hiwat are low watermark values for the variables.

Conversely switching to the polling off mode is done when P avg P lowat B avg B lowat W avg W lowat where P lowat B lowat and W lowat are the low watermark values for the variables.

In one embodiment these low and high watermark values may be chosen arbitrarily based on certain workloads and they should be adjusted accordingly to accommodate future workloads and varying link speeds. 

Turning polling on in this manner based on a high load factor improves performance and prevents denial of service attacks. This is because using the polling mechanism the receive queue at the network layer will only request packets from the queues at the driver layer when it has sufficient room to queue them. When it does not have room it will not poll to request more packets and packets to be dropped by the interface i.e. when the driver layer queues are filled up . Thus denial of service is prevented at the driver layer and is not propagated up to the network layers.

The API implemented in one embodiment is an interface implemented by a software component hereinafter API implementing software component that allows a different software component hereinafter API calling software component to access and use one or more functions methods procedures data structures and or other services provided by the API implementing software component. For example an API allows a developer of an API calling software component which may be a third party developer to leverage specified features provided by an API implementing software component. There may be one API calling software component or there may be more than one such software component. An API can be a source code interface that a computer system or program library provides in order to support requests for services from a software application. An API can be specified in terms of a programming language that can be interpretative or compiled when an application is built rather than an explicit low level description of how data is laid out in memory.

The API defines the language and parameters that API calling software components use when accessing and using specified features of the API implementing software component. For example an API calling software component accesses the specified features of the API implementing software component through one or more API calls sometimes referred to as function or method calls exposed by the API. The API implementing software component may return a value through the API in response to an API call from an API calling software component. While the API defines the syntax and result of an API call e.g. how to invoke the API call and what the API call does the API typically does not reveal how the API call accomplishes the function specified by the API call. Various function calls or messages are transferred via the one or more application programming interfaces between the calling software API calling software component and an API implementing software component. Transferring the function calls or messages may include issuing initiating invoking calling receiving returning or responding to the function calls or messages. Hence an API calling software component can transfer a call and an API implementing software component can transfer a call.

By way of example the API implementing software component and the API calling software component may be an operating system a library a device driver an API an application program or other software module it should be understood that the API implementing software component and the API calling software component may be the same or different type of software module from each other . The API calling software component may be a local software component i.e. on the same data processing system as the API implementing software component or a remote software component i.e. on a different data processing system as the API implementing software component that communicates with the API implementing software component through the API over a network. It should be understood that an API implementing software component may also act as an API calling software component i.e. it may make API calls to an API exposed by a different API implementing software component and an API calling software component may also act as an API implementing software component by implementing an API that is exposed to a different API calling software component.

The API may allow multiple API calling software components written in different programming languages to communicate with the API implementing software component thus the API may include features for translating calls and returns between the API implementing software component and the API calling software component however the API may be implemented in terms of a specific programming language.

It will be appreciated that the API implementing software component may include additional functions methods classes data structures and or other features that are not specified through the API and are not available to the API calling software component . It should be understood that the API calling software component may be on the same system as the API implementing software component or may be located remotely and accesses the API implementing software component using the API over a network. While illustrates a single API calling software component interacting with the API it should be understood that other API calling software components which may be written in different languages or the same language than the API calling software component may use the API .

The API implementing software component the API and the API calling software component may be stored in a machine readable medium which includes any mechanism for storing information in a form readable by a machine e.g. a computer or other data processing system . For example a machine readable medium includes magnetic disks optical disks random access memory read only memory flash memory devices etc.

In Software Stack an exemplary embodiment applications can make calls to Services or using several Service APIs and to Operating System OS using several OS APIs. Services and can make calls to OS using several OS APIs.

Note that the Service has two APIs one of which Service API receives calls from and returns values to Application and the other Service API receives calls from and returns values to Application . Service which can be for example a software library makes calls to and receives returned values from OS API and Service which can be for example a software library makes calls to and receives returned values from both OS API and OS API . Application makes calls to and receives returned values from OS API .

As illustrated in the computer system which is a form of a data processing system includes the bus es which is coupled with the processing system power supply memory and the nonvolatile memory e.g. a hard drive flash memory Phase Change Memory PCM etc. . The bus es may be connected to each other through various bridges controllers and or adapters as is well known in the art. The processing system may retrieve instruction s from the memory and or the nonvolatile memory and execute the instructions to perform operations as described above. The bus interconnects the above components together and also interconnects those components to the optional dock the display controller display device Input Output devices e.g. NIC Network Interface Card a cursor control e.g. mouse touchscreen touchpad etc. a keyboard etc. and the optional wireless transceiver s e.g. Bluetooth WiFi Infrared etc. .

According to one embodiment of the invention the exemplary architecture of the data processing system may used for the mobile devices described above. The data processing system includes the processing system which may include one or more microprocessors and or a system on an integrated circuit. The processing system is coupled with a memory a power supply which includes one or more batteries an audio input output a display controller and display device optional input output input device s and wireless transceiver s . It will be appreciated that additional components not shown in may also be a part of the data processing system in certain embodiments of the invention and in certain embodiments of the invention fewer components than shown in may be used. In addition it will be appreciated that one or more buses not shown in may be used to interconnect the various components as is well known in the art.

The memory may store data and or programs for execution by the data processing system . The audio input output may include a microphone and or a speaker to for example play music and or provide telephony functionality through the speaker and microphone. The display controller and display device may include a graphical user interface GUI . The wireless e.g. RF transceivers e.g. a WiFi transceiver an infrared transceiver a Bluetooth transceiver a wireless cellular telephony transceiver etc. may be used to communicate with other data processing systems. The one or more input devices allow a user to provide input to the system. These input devices may be a keypad keyboard touch panel multi touch panel etc. The optional other input output may be a connector for a dock.

Embodiments of the invention may include various steps as set forth above. The steps may be embodied in machine executable instructions which cause a general purpose or special purpose processor to perform certain steps. Alternatively these steps may be performed by specific hardware components that contain hardwired logic for performing the steps or by any combination of programmed computer components and custom hardware components.

Elements of the present invention may also be provided as a machine readable medium for storing the machine executable program code. The machine readable medium may include but is not limited to floppy diskettes optical disks CD ROMs and magneto optical disks ROMs RAMs EPROMs EEPROMs magnetic or optical cards or other type of media machine readable medium suitable for storing electronic program code.

Throughout the foregoing description for the purposes of explanation numerous specific details were set forth in order to provide a thorough understanding of the invention. It will be apparent however to one skilled in the art that the invention may be practiced without some of these specific details. For example it will be readily apparent to those of skill in the art that the functional modules and methods described herein may be implemented as software hardware or any combination thereof. Moreover although embodiments of the invention are described herein within the context of a mobile computing environment i.e. using mobile devices the underlying principles of the invention are not limited to a mobile computing implementation. Virtually any type of client or peer data processing devices may be used in some embodiments including for example desktop or workstation computers. Accordingly the scope and spirit of the invention should be judged in terms of the claims which follow.

