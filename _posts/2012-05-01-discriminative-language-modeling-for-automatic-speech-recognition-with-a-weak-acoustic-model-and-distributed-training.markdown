---

title: Discriminative language modeling for automatic speech recognition with a weak acoustic model and distributed training
abstract: Training data from a plurality of utterance-to-text-string mappings of an automatic speech recognition (ASR) system may be selected. Parameters of the ASR system that characterize the utterances and their respective mappings may be determined through application of a first acoustic model and a language model. A second acoustic model and the language model may be applied to the selected training data utterances to determine a second set of utterance-to-text-string mappings. The first set of utterance-to-text-string mappings may be compared to the second set of utterance-to-text-string mappings, and the parameters of the ASR system may be updated based on the comparison.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08965763&OS=08965763&RS=08965763
owner: Google Inc.
number: 08965763
owner_city: Mountain View
owner_country: US
publication_date: 20120501
---
The present application claims priority to U.S. provisional patent application No. 61 594 068 filed on Feb. 2 2012 and herein incorporated by reference in its entirety.

A goal of automatic speech recognition ASR technology is to map a particular utterance to an accurate textual representation of that utterance. For instance ASR performed on the utterance my dog has fleas would ideally be mapped to the text string my dog has fleas rather than the nonsensical text string my dog has freeze or the reasonably sensible but inaccurate text string my bog has trees. However ASR may be challenging due to different individuals having different speech patterns e.g. different accents phrasings and word choice . Additionally any background noise that is recorded along with an utterance can make it more difficult to discern between the utterance and the background noise.

In an example embodiment a plurality of utterance to text string mappings may be obtained. Each utterance to text string mapping may include a respective utterance and a respective text string that an ASR system has output for the respective utterance using a first acoustic model a language model and a weight vector. A first set of utterance to text string mappings may be selected from the plurality of utterance to text string mappings. Each utterance to text string mapping in the first set may be associated with a respective confidence level that is above a threshold confidence level. A second acoustic model and the language model may be applied to utterances in the first set of utterance to text string mappings to determine a second set of utterance to text string mappings. The second acoustic model may be statistically less accurate than the first acoustic model. The first set of utterance to text string mappings may be compared to the second set of utterance to text string mappings. The weight vector may be updated based on the comparison of the first set of utterance to text string mappings to the second set of utterance to text string mappings.

In another example embodiment a plurality of storage computing devices may each be configured to store a respective set of feature vectors and to have access to a weight vector. The respective sets of feature vectors and the weight vector may be based on a plurality of utterance to text string mappings. Each utterance to text string mapping may include a respective utterance and a respective text string that an ASR system has output for the respective utterance using a first acoustic model and a language model. A plurality of training computing devices may each have access to a respective partition of a first set of utterance to text string mappings that i was selected from the plurality of utterance to text string mappings and ii is associated with a respective confidence level that is above a threshold confidence level. Each training computing device may be configured to apply a second acoustic model and the language model to the respective utterances of the first set to determine a second set of utterance to text string mappings. Each training computing device may also be configured to compare the first set of utterance to text string mappings to the second set of utterance to text string mappings. At least one combining computing device may be configured to update the weight vector based on the comparison of the first set of utterance to text string mappings to the second set of utterance to text string mappings.

These as well as other aspects advantages and alternatives will become apparent to those of ordinary skill in the art by reading the following detailed description with reference where appropriate to the accompanying drawings. Further it should be understood that the description provided in this summary section and elsewhere in this document is intended to illustrate the claimed subject matter by way of example and not by way of limitation.

Perfectly accurate ASR may not be practical in many scenarios. Thus the quality of an ASR system may be measured by its error rate. An ASR system with a lower error rate may be considered to be superior to an ASR system with a higher error rate. This error rate may be measured at the phoneme level word level sentence level or in some other fashion.

In order to reduce ASR error rate a corpus of utterances may be manually transcribed to corresponding text strings. The utterances and these text strings may then be decomposed into a model that is used for ASR. Based on this model the ASR system may be used to estimate the N best text string mappings for any new utterance presented to the system. However the transcription of a large corpus of utterances is a labor intensive task that is subject to a non negligible error rate due to human transcription mistakes. Further even a relatively large corpus of utterances and mapped transcriptions may not contain mappings for all possible sentences of a given language.

Thus some ASR systems include a learning component that allows the system to dynamically adapt based on feedback. Thus for a given utterance an ASR system may assign each text string mapping a respective confidence level. These confidence levels may be represented as a measure of the probability that the associated text string mapping is an accurate transcription of the given utterance. For example suppose that for the utterance my dog has fleas the text string mapping my dog has fleas has a confidence level of 0.95 the text string mapping my dog has trees has a confidence level of 0.03 and the text string mapping my dog has knees has a confidence level of 0.02.

If the ASR system detects a new utterance that it evaluates as the words my dog has fleas the ASR system may transcribe this new utterance as the text string my dog has fleas. The ASR system may present the user with this string and may also present the user with the two other strings just in case. If the user selects my dog has trees as the best transcription the ASR system may adjust its weights accordingly. Consequently for the utterance my dog has fleas the ASR system may set the confidence of the text string mapping my dog has fleas to 92 and the confidence of the text string mapping my dog has trees to 5 . In this way the ASR system can learn from its users.

An ASR system may adjust its weights in either an online or offline fashion. For example if an operating e.g. running ASR system receives feedback regarding one or more transcriptions that it has generated the ASR system may adjust its weights while operating to incorporate this feedback. Therefore the updated weights may go into effect while the ASR system is operating. On the other hand the ASR system or a different adjunct system or device may receive this feedback and the weights may be updated offline. For instance after some amount of time has passed or some volume of feedback has been received the ASR system may update its weights offline to reflect some or all of this feedback. Then a new ASR system with the updated weights may become operational.

ASR systems have been deployed in various environments. Some ASR systems are just a single machine e.g. a personal computer into which a user speaks utterances and the ASR system transcribes the utterances into text. Other ASR systems are client server based in which the user speaks an utterance into a client device and the client device may encode the utterance and transmit it to a server device. Then the server device may perform speech recognition on the encoded utterance and transmit one or more text string mappings to the client device for presentation to the user. Particularly on wireless communication devices such as mobile phones client server based ASR can be supported by Internet search applications geo location and mapping applications text messaging and instant messaging applications and by virtually any third party application as well.

The server component of an ASR system may include just a single server device or may be distributed in various ways across a number of server devices. The following section describes example client and server device s and an example communication system that could be used for client server communication and processing for ASR.

The methods devices and systems described herein can be implemented using client devices and or so called cloud based server devices. Under various aspects of this paradigm client devices such as mobile phones and tablet computers may offload some processing and storage responsibilities to remote server devices. At least some of the time these client services are able to communicate via a network such as the Internet with the server devices. As a result applications that operate on the client devices may also have a persistent server based component. Nonetheless it should be noted that at least some of the methods processes and techniques disclosed herein may be able to operate entirely on a client device or a server device.

This section describes general system and device architectures for such client devices and server devices. However the methods devices and systems presented in the subsequent sections may operate under different paradigms as well. Thus the embodiments of this section are merely examples of how these methods devices and systems can be enabled.

Network may be for example the Internet or some other form of public or private Internet Protocol IP network. Thus client devices and may communicate using packet switching technologies. Nonetheless network may also incorporate at least some circuit switching technologies and client devices and may communicate via circuit switching alternatively or in addition to packet switching.

A server device may also communicate via network . Particularly server device may communicate with client devices and according to one or more network protocols and or application level protocols to facilitate the use of network based or cloud based computing on these client devices. Server device may include integrated data storage e.g. memory disk drives etc. and may also be able to access a separate server data storage . Communication between server device and server data storage may be direct via network or both direct and via network as illustrated in . Server data storage may store application data that is used to facilitate the operations of applications performed by client devices and and server device .

Although only three client devices one server device and one server data storage are shown in communication system may include any number of each of these components. For instance communication system may comprise millions of client devices thousands of server devices and or thousands of server data storages. Furthermore client devices may take on forms other than those in .

User interface may comprise user input devices such as a keyboard a keypad a touch screen a computer mouse a track ball a joystick and or other similar devices now known or later developed. User interface may also comprise user display devices such as one or more cathode ray tubes CRT liquid crystal displays LCD light emitting diodes LEDs displays using digital light processing DLP technology printers light bulbs and or other similar devices now known or later developed. Additionally user interface may be configured to generate audible output s via a speaker speaker jack audio output port audio output device earphones and or other similar devices now known or later developed. In some embodiments user interface may include software circuitry or another form of logic that can transmit data to and or receive data from external user input output devices.

Communication interface may include one or more wireless interfaces and or wireline interfaces that are configurable to communicate via a network such as network shown in . The wireless interfaces if present may include one or more wireless transceivers such as a BLUETOOTH transceiver a Wifi transceiver perhaps operating in accordance with an IEEE 802.11 standard e.g. 802.11b 802.11g 802.11n a WiMAX transceiver perhaps operating in accordance with an IEEE 802.16 standard a Long Term Evolution LTE transceiver perhaps operating in accordance with a 3rd Generation Partnership Project 3GPP standard and or other types of wireless transceivers configurable to communicate via local area or wide area wireless networks. The wireline interfaces if present may include one or more wireline transceivers such as an Ethernet transceiver a Universal Serial Bus USB transceiver or similar transceiver configurable to communicate via a twisted pair wire a coaxial cable a fiber optic link or other physical connection to a wireline device or network.

In some embodiments communication interface may be configured to provide reliable secured and or authenticated communications. For each communication described herein information for ensuring reliable communications e.g. guaranteed message delivery can be provided perhaps as part of a message header and or footer e.g. packet message sequencing information encapsulation header s and or footer s size time information and transmission verification information such as cyclic redundancy check CRC and or parity check values . Communications can be made secure e.g. be encoded or encrypted and or decrypted decoded using one or more cryptographic protocols and or algorithms such as but not limited to the data encryption standard DES the advanced encryption standard AES the Rivest Shamir and Adleman RSA algorithm the Diffie Hellman algorithm and or the Digital Signature Algorithm DSA . Other cryptographic protocols and or algorithms may be used instead of or in addition to those listed herein to secure and then decrypt decode communications.

Processor may include one or more general purpose processors e.g. microprocessors and or one or more special purpose processors e.g. digital signal processors DSPs graphical processing units GPUs floating point processing units FPUs network processors or application specific integrated circuits ASICs . Processor may be configured to execute computer readable program instructions that are contained in data storage and or other instructions to carry out various functions described herein.

Data storage may include one or more non transitory computer readable storage media that can be read or accessed by processor . The one or more computer readable storage media may include volatile and or non volatile storage components such as optical magnetic organic or other memory or disc storage which can be integrated in whole or in part with processor . In some embodiments data storage may be implemented using a single physical device e.g. one optical magnetic organic or other memory or disc storage unit while in other embodiments data storage may be implemented using two or more physical devices.

Data storage may also include program data that can be used by processor to carry out functions described herein. In some embodiments data storage may include or have access to additional data storage components or devices e.g. cluster data storages described below .

Server device and server data storage device may store applications and application data at one or more places accessible via network . These places may be data centers containing numerous servers and storage devices. The exact physical location connectivity and configuration of server device and server data storage device may be unknown and or unimportant to client devices. Accordingly server device and server data storage device may be referred to as cloud based devices that are housed at various remote locations. One possible advantage of such could based computing is to offload processing and data storage from client devices thereby simplifying the design and requirements of these client devices.

In some embodiments server device and server data storage device may be a single computing device residing in a single data center. In other embodiments server device and server data storage device may include multiple computing devices in a data center or even multiple computing devices in multiple data centers where the data centers are located in diverse geographic locations. For example depicts each of server device and server data storage device potentially residing in a different physical location.

In some embodiments each of the server clusters A B and C may have an equal number of server devices an equal number of cluster data storages and an equal number of cluster routers. In other embodiments however some or all of the server clusters A B and C may have different numbers of server devices different numbers of cluster data storages and or different numbers of cluster routers. The number of server devices cluster data storages and cluster routers in each server cluster may depend on the computing task s and or applications assigned to each server cluster.

In the server cluster A for example server devices A can be configured to perform various computing tasks of server device . In one embodiment these computing tasks can be distributed among one or more of server devices A. Server devices B and C in server clusters B and C may be configured the same or similarly to server devices A in server cluster A. On the other hand in some embodiments server devices A B and C each may be configured to perform different functions. For example server devices A may be configured to perform one or more functions of server device and server devices B and server device C may be configured to perform functions of one or more other server devices. Similarly the functions of server data storage device can be dedicated to a single server cluster or spread across multiple server clusters.

Cluster data storages A B and C of the server clusters A B and C respectively may be data storage arrays that include disk array controllers configured to manage read and write access to groups of hard disk drives. The disk array controllers alone or in conjunction with their respective server devices may also be configured to manage backup or redundant copies of the data stored in cluster data storages to protect against disk drive failures or other types of failures that prevent one or more server devices from accessing one or more cluster data storages.

Similar to the manner in which the functions of server device and server data storage device can be distributed across server clusters A B and C various active portions and or backup redundant portions of these components can be distributed across cluster data storages A B and C. For example some cluster data storages A B and C may be configured to store backup versions of data stored in other cluster data storages A B and C.

Cluster routers A B and C in server clusters A B and C respectively may include networking equipment configured to provide internal and external communications for the server clusters. For example cluster routers A in server cluster A may include one or more packet switching and or routing devices configured to provide i network communications between server devices A and cluster data storage A via cluster network A and or ii network communications between the server cluster A and other devices via communication link A to network . Cluster routers B and C may include network equipment similar to cluster routers A and cluster routers B and C may perform networking functions for server clusters B and C that cluster routers A perform for server cluster A.

Additionally the configuration of cluster routers A B and C can be based at least in part on the data communication requirements of the server devices and cluster storage arrays the data communications capabilities of the network equipment in the cluster routers A B and C the latency and throughput of the local cluster networks A B C the latency throughput and cost of the wide area network connections A B and C and or other factors that may contribute to the cost speed fault tolerance resiliency efficiency and or other design goals of the system architecture.

As shown in client device may include a communication interface a user interface a processor and data storage all of which may be communicatively linked together by a system bus network or other connection mechanism .

Communication interface functions to allow client device to communicate using analog or digital modulation with other devices access networks and or transport networks. Thus communication interface may facilitate circuit switched and or packet switched communication such as POTS communication and or IP or other packetized communication. For instance communication interface may include a chipset and antenna arranged for wireless communication with a radio access network or an access point. Also communication interface may take the form of a wireline interface such as an Ethernet Token Ring or USB port. Communication interface may also take the form of a wireless interface such as a Wifi BLUETOOTH global positioning system GPS or wide area wireless interface e.g. WiMAX or LTE . However other forms of physical layer interfaces and other types of standard or proprietary communication protocols may be used over communication interface . Furthermore communication interface may comprise multiple physical communication interfaces e.g. a Wifi interface a BLUETOOTH interface and a wide area wireless interface .

User interface may function to allow client device to interact with a human or non human user such as to receive input from a user and to provide output to the user. Thus user interface may include input components such as a keypad keyboard touch sensitive or presence sensitive panel computer mouse trackball joystick microphone still camera and or video camera. User interface may also include one or more output components such as a display screen which for example may be combined with a presence sensitive panel CRT LCD LED a display using DLP technology printer light bulb and or other similar devices now known or later developed. User interface may also be configured to generate audible output s via a speaker speaker jack audio output port audio output device earphones and or other similar devices now known or later developed. In some embodiments user interface may include software circuitry or another form of logic that can transmit data to and or receive data from external user input output devices. Additionally or alternatively client device may support remote access from another device via communication interface or via another physical interface not shown .

Processor may comprise one or more general purpose processors e.g. microprocessors and or one or more special purpose processors e.g. DSPs GPUs FPUs network processors or ASICs . Data storage may include one or more volatile and or non volatile storage components such as magnetic optical flash or organic storage and may be integrated in whole or in part with processor . Data storage may include removable and or non removable components.

Generally speaking processor may be capable of executing program instructions e.g. compiled or non compiled program logic and or machine code stored in data storage to carry out the various functions described herein. Therefore data storage may include a non transitory computer readable medium having stored thereon program instructions that upon execution by client device cause client device to carry out any of the methods processes or functions disclosed in this specification and or the accompanying drawings. The execution of program instructions by processor may result in processor using data .

By way of example program instructions may include an operating system e.g. an operating system kernel device driver s and or other modules and one or more application programs e.g. address book email web browsing social networking and or gaming applications installed on client device . Similarly data may include operating system data and application data . Operating system data may be accessible primarily to operating system and application data may be accessible primarily to one or more of application programs . Application data may be arranged in a file system that is visible to or hidden from a user of client device .

Application programs may communicate with operating system through one or more application programming interfaces APIs . These APIs may facilitate for instance application programs reading and or writing application data transmitting or receiving information via communication interface receiving or displaying information on user interface and so on.

In some vernaculars application programs may be referred to as apps for short. Additionally application programs may be downloadable to client device through one or more online application stores or application markets. However application programs can also be installed on client device in other ways such as via a web browser or through a physical interface e.g. a USB port on client device .

ASR system may include several computational models that operate on various levels to transcribe an utterance into text. Words and or phonemes may be evaluated by acoustic model while sentences and or phrases may be evaluated by language model . Search module may communicate with acoustic model and language model to determine each transcribed word of an input utterance where this transcription may be based on acoustic model and language model .

As part of the transcription process an utterance may pass through acoustic model . Acoustic model may sample every s milliseconds of the utterance and produce respective output vectors for each sample. These output vectors may be interpreted to estimate phonemes contained therein.

A phoneme may be considered to be the smallest segment of an utterance that encompasses a meaningful contrast with other segments of utterances. Thus a word typically includes one or more phonemes. For purposes of simplicity phonemes may be thought of as utterances of letters but this is not a perfect analogy as some phonemes may present multiple letters. An example phonemic spelling for the American English pronunciation of the word cat is kaet consisting of the phonemes k ae and t. 

Each phoneme may be associated with a different set of nominal output vector values. Thus acoustic model may be able to estimate the phoneme in the sample by analyzing the sample in the frequency domain and finding the phoneme with nominal output vector values e.g. frequency characteristics that best match the output vector values of the sample. Once two or more phonemes are estimated acoustic model may use a pre established mapping e.g. from a dictionary of tens or hundreds of thousands of phoneme pattern to word mappings to put these phonemes together into words.

In interpretation acoustic model correctly interprets the utterance to the text string my dog has fleas. However in interpretation some phonemes are incorrectly interpreted. Consequently acoustic model interprets the utterance as the text string my bog has trees. In interpretation just one phoneme is incorrectly interpreted resulting in an interpretation of the text string my dog has freeze. 

The incorrect phonemic interpretations and may occur for various reasons. One of the factors that may contribute to these errors is that recordings of utterances are often imperfect. For example a user in a noisy environment may record an utterance for purposes of speech recognition. Acoustic model may incorrectly interpret one or more phonemes of this utterance because the acoustic model cannot reliably filter the user s voice from the noise.

Another factor contributing to acoustic model error is the quality of the acoustic model itself. Simply put some acoustic models result in statistically better performance than others on a particular type of input utterance. For instance an acoustic model for American English is likely to outperform an acoustic model for British English on utterances spoken in American English. Therefore selection of an appropriate acoustic model can impact the quality of ASR system .

One way of implementing an acoustic model such as acoustic model is by using a hidden Markov model HMM . Some HMM based acoustic models may also consider context when performing this mapping. For example acoustic model may consider the phoneme that precedes the current sample to provide a better estimate of the phoneme represented by the current sample. The use of context in this fashion can account for certain phoneme combinations e.g. aet being more common that other phoneme combinations e.g. tk . But HMMs are just one technology that can be employed to develop an acoustic model and acoustic model can be based on technology other than HMMs.

Furthermore acoustic model may operate on a level other than words. For instance acoustic model may interpret a series of phonemes as syllables or as one or more words. For purposes of simplicity throughout this specification and the accompanying drawings it is assumed that acoustic models interpret one or more phonemes as words. However acoustic models that perform other types of interpretations are within the scope of the embodiments herein.

After acoustic model performs its interpretation language model may adjust this interpretation based observed patterns of a language. Thus a language model may operate on n grams which are sequences of n units of output from acoustic model . As noted above these units may be for example phonemes syllables words or series of words.

An n gram based language model may define the conditional probability of iv the nth word in an n gram given the values of the pattern of n 1 previous words in the n gram. More formally language model may define In practice language models with values of n greater than 5 are rarely used because of their computational complexity and also because smaller n grams e.g. 3 grams which are also referred to as tri grams tend to yield acceptable results. In the example described below trigrams are used for purposes of illustration. Nonetheless any value of n may be may be used with the embodiments herein. For purposes of example the following discussion assumes the use of tri grams.

One possible way of determining tri gram probabilities is to use an existing training corpus of utterance to text string mappings the text strings may also be referred to as transcriptions . As noted above these text strings may have been manually transcribed in order to increase accuracy. Then tri gram probabilities can be estimated based on their respective number of appearances in the training corpus. In other words if C w w w is the number of occurrences of the word pattern w w win the training corpus then

However this technique is not perfect as some acceptable tri grams may not appear in the training corpus and may therefore be assigned a probability of zero. Consequently when given a zero probability tri gram at run time the language model may instead attempt to map this tri gram to a different tri gram associated with a non zero probability.

In order to reduce this likelihood the language model may be smoothed so that zero probability tri grams have small non zero probabilities and the probabilities of the tri grams in the training corpus are reduced accordingly. In this way tri grams not found in the training corpus can still be recognized by the language model.

Alternatively or additionally the language model may employ a back off. With this option if a tri gram has a probability of zero or a sufficiently low non zero probability the language model may ignore the most distant previous word in the tri gram and evaluate the resulting bi gram. If the bi gram has a probability of zero or a sufficiently low non zero probability the language model may back off again and evaluate the resulting one gram possibly without considering its context. However the back off operation itself may have an associated cost per n gram so that in some cases backing off is unfavorable. For instance if a bi gram is common in the training corpus e.g. the phrase and the a high back off cost may be associated with the bi gram. Consequently the language model is more likely to consider the context of the words in the bi gram. On the other hand if a bi gram is uncommon or not present in the training corpus e.g. the phrase the and a low back off cost may be associated with the bi gram. As a result the language model is less likely to consider the context of the words in this bi gram and may evaluate the words individually instead.

The WFA of represents an example partial WFA of a language model that uses tri grams. Thus in state the two previously evaluated words of an utterance are dog has and the language model is attempting to estimate the N best possible next words. Transitions for two of these next words freeze and fleas are shown with their respective transitions to states and . Since there is likely to be more occurrences of the phrase dog has fleas in the training corpus than the phrase dog has freeze a lower cost may be associated with the transition from state to state a cost of 10 than the transition from state to state a cost of 50 .

Additionally the transition from state to state represents the language model backing off from using tri grams to using bi grams. Since tri grams starting with dog has are fairly common the cost of this back off transition may be high a cost of 100 .

Each of states and may be terminal states for utterances that end after the mapping of either freeze or fleas. On the other hand for utterances that continue the WFA may also include further states and transitions that are omitted from . Similarly from state bi grams starting with the word has may be evaluated. Thus there may be many transitions from state not shown in . Nonetheless a transition from state to state for the word fleas is shown with a cost of 20 . The WFA may or may not include a transition from state to state for the word freeze. 

Search module may be used to determine a sequence of one or more words that matches an input utterance. More specifically search module may calculate argmax where a is a stream of feature vectors derived from the input utterance P w a represents the probability of those feature vectors being produced by a word sequence w and P w is the probability assigned to w by language model . For example P w may be based on n gram conditional probabilities as discussed above. The function argmaxmay return the value of w that maximizes P a w P w .

One way to train a language model is to use the aforementioned training corpus of utterance to text string mappings to iteratively define a function y F x j 1 . . . N that maps input utterance xto N candidate text strings y. One way of iteratively defining F is to build an initial version F based on known e.g. manually transcribed utterance to text string mappings. Then F may be improved by introducing yet more training data in a number of training epochs thereby creating F F and so on. After some number perhaps a large number of training epochs F may converge to an acceptably accurate ASR system.

Formally given a set of training examples x y i 1 . . . m of utterances xto text strings y where yis a reference transcription of x a feature vector x y may be derived. This feature vector may be a multi dimensional representation of the acoustic model s and or the language model s parameterization of the utterances xand or the text strings y. For example the parameters in the feature vector may include costs associated with yfrom the language model s WFA counts of associated n grams and so on. As noted above the values of y i 1 . . . m may be manual transcriptions or best known transcriptions of their respective utterances x. Thus in some embodiments ymay be considered to be a ground truth transcription of x.

A possible goal of the training process may be to determine a weight vector w such that for a new input utterance of x argmax produces an accurate perhaps the best text string transcription of x. The argument x w may be evaluated as the inner product of the two input vectors and the function argmaxmay return the value of that maximizes this inner product. Particularly each feature in x may be paired with a weight from w.

Thus each training example x y run through the ASR system may result in w being updated to reflect what the language model has learned from the training example. Particularly if y y then F x produces the best known transcription of xand w need not be changed. However if y y then w may be updated to increase the weights corresponding to the features of yand decrease the weights corresponding to the features of y . In this way as more and more training samples are run through this process w may converge so that F x is more likely to produce y.

Algorithm 1 illustrates an example embodiment of the training process for T training iterations or epochs. At line 1 weight vector w is initialized to all zeros. The operations for lines 3 through 9 are performed T times once per training epoch. These operations include at line 5 determining y F x the ASR system s best text string transcription of x.

At line 6 y is compared to y. If y y then F x has chosen the reference transcription. However if y y F x has not chosen the reference transcription and at line 7 the weight vector for the current epoch wmay be updated in an attempt to bring F x closer to y. Particularly wmay be adjusted to increase the weights corresponding to the features in y and to decrease the weights corresponding to the features in y . Thus x y is added to wand x y is subtracted from w.

At line 9 the weight vector w is updated with the weight adjustments of the current epoch w. In this way over an appropriate number of epochs w may converge to values that result in F x selecting a transcription that is the reference transcription or close thereto for a majority of input utterances x. Further for an input utterance xthat is not in x i 1 . . . m F x may be able to produce a text string transcription that is an acceptably accurate estimate of the ground truth transcription of x.

Algorithm 1 is depicted visually in . Block of represents epoch t of Algorithm 1. The function F receives weight vector w input utterances x where x i 1 . . . m transcriptions y where y i 1 . . . m and the feature vector . Applying F to these inputs block produces adjusted weight vector w. At block weight vector w may be updated based on w x y and . Then at block the process of block may be repeated using the updated version of weight vector w. This process may continue through any number of epochs potentially hundreds thousands or millions of epochs or more.

Each particular transcription produced by ASR system may be associated with a respective confidence level. In some embodiments this confidence level may be a value between zero and one inclusive that represents an estimate of the accuracy of the particular transcription. Therefore a confidence level may estimate a probability that the transcription is correct. In other embodiments confidence levels may take on values in other ranges or may be represented with different values.

For a particular utterance some transcriptions will have a higher confidence level than others. Table 1 provides some examples based on a single audio utterance of dog has fleas. This utterance may be evaluated by the acoustic model e.g. acoustic model . In this case the acoustic model provides a correct mapping of phonemes to words producing the text string dog has fleas. 

However Table 1 shows that there may be three possible language model transcriptions for this acoustic model output. For example the language model e.g. language model may account for errors that the acoustic model made in the past such as evaluating the utterance dog has trees as dog has fleas and evaluating the utterance dog has knees as dog has fleas. 

Accordingly the language model may assign the text string dog has fleas a confidence level of 0.95 the text string dog has trees a confidence level of 0.03 and the text string dog has knees a confidence level of 0.02. These confidence levels may be based on the conditional probabilities discussed in Section 3B.

Thus the language model may attempt to correct for likely acoustic model failures. Alternatively as also discussed in Section 3B the language model may smooth the conditional probabilities in order to facilitate support for phrases that the language model has not processed as part of its training data.

In some embodiments confidence levels may only be assigned to the best highest confidence transcription. The confidence level value of this best transcription may determine whether the associated utterance to text string mapping x y is retained as part of the training corpus.

In addition to the functionality described above other features may be employed in an ASR system that could result in improving the ASR system s performance in at least some situations. Some of these features include training a language model with a weak acoustic model and distributing e.g. parallelizing the language model training process over multiple computing devices. As noted in Section 1 the training processes described herein may occur online to dynamically update an operational ASR system or offline to update an ASR system that later becomes operational.

As noted in Section 3A some acoustic models exhibit statistically better performance than others. For example the accuracy of an acoustic model may be measured in terms of word error rate. Thus given a set of utterances the word error rate of the acoustic model may be the number of words misinterpreted by an acoustic model divided by the total number of words in the utterances. Thus a strong acoustic model will likely have a lower word error rate than a weak acoustic model.

Nonetheless perhaps once the ASR system has already been trained with a strong acoustic model it may be beneficial to continue training the ASR system using a weak acoustic model. Alternatively the ASR system may be initially trained using a weak acoustic model. Regardless the weak acoustic model is likely to simulate common misinterpretations that the strong acoustic model might make in a noisy environment. Thus by training the ASR system to be able to produce better transcriptions in the presence of these misinterpretations the overall quality of the ASR system may increase.

Further for an ASR system that has already been trained this process may not require additional manual transcriptions of utterances. Instead a set of utterances that has respective transcriptions with a confidence level above of a given threshold e.g. 0.70 0.75 0.80 0.85 0.90 0.95 etc. may be selected. This set may represent utterances for which the transcription is likely to be the ground truth transcription.

Then the utterances in this set may be run through the weak acoustic model. While the weak acoustic model may correctly interpret some utterances it is likely to misinterpret others. The output of the weak acoustic model may then be passed through the language model which in turn may produce one or more possible transcriptions and the respective confidence levels of these transcriptions.

Table 2 shows an example at each step of this process. The input utterance dog has fleas is misinterpreted by the weak acoustic model as dog has freeze. The language model then provides three possible transcriptions dog has fleas dog has knees and dog has freeze with confidence levels of 0.30 0.20 and 0.50 respectively.

As illustrated by Table 2 in some embodiments the N best utterance to text string mappings may include acoustic model and or language model scores. These scores may indicate the quality or confidence level of the respective mappings. These scores may be later augmented for example by the process illustrated in Algorithm 1.

These transcriptions may be compared to the reference transcription of dog has fleas to determine that dog has fleas is most likely the correct transcription. Accordingly the weight vector w of the language model may be adjusted to increase the weights corresponding to the features in my dog has fleas and to decrease the weights corresponding to the features in my dog has knees and or my dog has freeze. 

Consequently the confidence levels of the language model transcriptions for acoustic model output dog has freeze may change. For instance the transcriptions for dog has fleas dog has knees and dog has freeze may end up with confidence levels of 0.60 0.10 and 0.30 respectively. In this way the language model may adapt to properly transcribe the utterance dog has fleas even when uttered in a noisy environment.

Whether training an ASR system with a strong acoustic model or a weak acoustic model it is generally beneficial to perform the training with a large number of input utterances and their respective transcriptions. For example a training corpus may include thousands tens of thousands hundreds of thousands or over a million hours of speech and thus may include tens of millions or hundreds of millions of individual words.

Given a training corpus of this size it may be impractical to perform the training steps outline above see e.g. Algorithm 1 on a single computing device. By distributing the training across multiple computing devices the training may occur in parallel thus reducing the overall time used for training

In some embodiments the training computing devices may perform the function y argmax x w where xis the ith input utterance of partition c 1 . . . C and wis a local version of weight vector wthat is being manipulated by the training computing device s associated with partition c of the input utterances. The training computing devices may also perform the operation w w x y x y to update w. Further the training computing devices may from time to time retrieve ASR parameters or portions of these ASR parameters from the storage computing devices. For instance rather than retrieving all values of the training computing devices may communicate with the storage computing devices to retrieve x y and x y as needed.

Block represents one or more combining computing devices that receive from the computing devices of block wfor each partition c. The combining computing devices may then combine these weights in various ways to determine a new weight vector wfor epoch t 1 for the training process. Consequently block represents the ASR parameters for epoch t 1 such as the feature vector weight vector w x and or y.

The preceding descriptions of the blocks of are provided for purposes of illustration and should not be viewed as limiting. Thus the data and functions described in reference to may include aspects not explicitly discussed herein. Additionally these data and functions may be distributed amongst training storage and combining computing devices in arrangements not explicitly discussed herein. Further each of the training storage and combining computing devices may be a server device such as server device and or a server cluster such as server cluster A. Moreover the functions of the training storage and combining computing devices may be shared by one or more server devices or server clusters.

Regardless of the exact arrangement of computing devices in the functions of the combining computing devices may vary. Particularly the combining computing devices may mix the updated weights they receive from the training computing devices in different ways.

Algorithm 2 is a possible implementation of distributed training of an ASR system. At line 1 the weight vector w is initialized to all zeros. At line 2 the training samples x y of utterances xand their respective transcriptions yare divided into C partitions . . . .

The operations for lines 4 through 14 are performed T times once per training epoch. Additionally the operations for lines 5 through 12 are performed C times one per partition. The operations for each of these partitions may be parallelized by distributing them to between two and C training computing devices.

At line 5 a per epoch per partition weight vector wmay be initialized to the value of w. At line 6 a per epoch per partition iterator vector may be initialized to all zeros.

The operations for lines 8 through 11 are performed times once for each utterance and its associated transcription. These operations include at line 8 determining y F x argmax x w the ASR system s best text string transcription of utterance x. At line 9 y is compared to reference transcription y. If y y F x has chosen the reference transcription. However if y y F x has not chosen the reference transcription and at line 10 the iterator vector may be updated to increase the weights corresponding to the features in y and to decrease the weights corresponding to the features in y . At line 11 the weight vector wmay be set to the most recent value of w plus . In this way reflects the total change made to w by the processing of partition c and may be maintained on a per partition basis by a training computing device.

At line 14 the combining computing device s may collect the C versions of from the training computing devices. The combining computing device s may update w by using mixing function m . At a minimum m may take each of the iterator vectors of the current epoch t.

In a first example embodiment m may perform a sum over all of the iterator vectors. In this case m w . In a second example embodiment m may perform a sum over all of the iterator vectors but average this sum over the total number of partitions C. In this case m w 1 C . In a third example embodiment

Nonetheless these mixing functions may be modified without departing from the scope of the embodiments disclosed herein. Further other mixing functions may be used instead of the three example mixing functions shown above.

At step a plurality of utterance to text string mappings may be obtained. Each utterance to text string mapping in the plurality may include a respective utterance and a respective text string that an ASR system has output for the respective utterance using a first acoustic model a language model and a weight vector. The first acoustic model may map utterances to phonemes. The language model may use n gram representations of phrases to determine utterance to text string mappings.

At step a first set of utterance to text string mappings may be selected from the plurality of utterance to text string mappings. Each utterance to text string mapping in the first set may be associated with a respective confidence level that is above a threshold confidence level.

At step a second acoustic model and the language model may be applied to utterances in the first set of utterance to text string mappings to determine a second set of utterance to text string mappings. The second acoustic model may also map utterances to phonemes. In some embodiments the second acoustic model may be statistically less accurate than the first acoustic model. For example the second acoustic model may have a higher word error rate than the first acoustic model.

Additionally each mapping of the first set of utterance to text string mappings may be associated with a respective feature vector. The respective feature vector may define at least one characteristic related to the respective utterance to text string mapping. Applying the second acoustic model and the language model to utterances of the first set may involve determining respective inner products of the weight vector and each of the respective feature vectors.

At step the first set of utterance to text string mappings may be compared to the second set of utterance to text string mappings. At step the weight vector may be updated based on the comparison of the first set of utterance to text string mappings to the second set of utterance to text string mappings.

Partitions of the first set of utterance to text string mappings may be distributed to a plurality of training computing devices. Each of the plurality of training computing devices may apply the second acoustic model and the language model to the partitions of the first set to produce a respective weight vector adjustment. At least one combining computing device may update the weight vector in accordance with the weight vector adjustments. Updating the weight vector may involve the at least one combining computing device summing the respective weight vector adjustments to update the weight vector averaging the respective weight vector adjustments to update the weight vector and or using a moving average over the weight vector and the respective weight vector adjustments to update the weight vector.

In some embodiments applying the second acoustic model and the language model to utterances of the first set of utterance to text string mappings may involve estimating a text string transcription for a particular utterance in the first set of utterance to text string mappings. A first feature vector may characterize the estimated text string transcription of the particular utterance and comparing the first set of utterance to text string mappings to the second set of utterance to text string mappings may include comparing the estimated text string transcription to a reference text string transcription of the particular utterance. Further a second feature vector may characterize the reference text string transcription of the particular utterance and updating the weight vector based on the comparison of the first set of utterance to text string mappings to the second set of utterance to text string mappings may include adding the second feature vector to the weight vector and subtracting the first feature vector from the weight vector.

Further a computing device may receive an input utterance from a client device. Possibly in response to receiving the input utterance the computing device may apply the ASR system to the input utterance to determine an output text string. In doing so the ASR system may use the updated weight vector. The computing device may transmit the output text string to the client device.

In another example embodiment that is illustrated by both a system may include a plurality of storage computing devices a plurality of training computing devices and at least one combining computing device.

Each of the storage computing devices may be configured to store a respective set of feature vectors and to have access to a weight vector. The respective sets of feature vectors and the weight vector may be based on a plurality of utterance to text string mappings. Each utterance to text string mapping in the plurality may include a respective utterance and a respective text string that an ASR system has output for the respective utterance using a first acoustic model and a language model. The first acoustic model may map utterances to phonemes and the language model may use n gram representations of phrases to determine utterance to text string mappings.

Each of the training computing devices may have access to a respective partition of a first set of utterance to text string mappings that i was selected from the plurality of utterance to text string mappings and ii is associated with a respective confidence level that is above a threshold confidence level.

Each training computing device may also be configured to apply a second acoustic model and the language model to the respective utterances of the first set to determine a second set of utterance to text string mappings. In some embodiments the second acoustic model and the language model may be applied to at least one of the partitions of the first set to produce a respective weight vector adjustment. Applying the second acoustic model and the language model to the respective utterances of the first set may involve determining respective inner products of the weight vector and one of the sets of feature vectors.

The second acoustic model may map utterances to phonemes and may be statistically less accurate than the first acoustic model. For example the second acoustic model may have a higher word error rate than the first acoustic model. On the other hand the second acoustic model may be the same as the first acoustic model.

Each training computing device may additionally be configured to compare the first set of utterance to text string mappings to the second set of utterance to text string mappings.

The combining computing device s may be configured to update the weight vector based on the comparison of the first set of utterance to text string mappings to the second set of utterance to text string mappings. The combining computing device s may also be configured to sum the respective weight vector adjustments to update the weight vector average the respective weight vector adjustments to update the weight vector and or use a moving average over the weight vector and the respective weight vector adjustments to update the weight vector.

In some embodiments applying the second acoustic model and the language model to the respective utterances of the first set may involve estimating a text string transcription for a particular utterance in the first set of utterance to text string mappings where a first feature vector characterizes the estimated text string transcription of the particular utterance. In these embodiments comparing the first set of utterance to text string mappings to the second set of utterance to text string mappings may include comparing the estimated text string transcription to a reference text string transcription of the particular utterance where a second feature vector characterizes the reference text string transcription of the particular utterance. Additionally updating the weight vector based on the comparison of the first set of utterance to text string mappings to the second set of utterance to text string mappings may include adding the second feature vector to the weight vector and subtracting the first feature vector from the weight vector.

The above detailed description describes various features and functions of the disclosed systems devices and methods with reference to the accompanying figures. In the figures similar symbols typically identify similar components unless context dictates otherwise. The illustrative embodiments described in the detailed description figures and claims are not meant to be limiting. Other embodiments can be utilized and other changes can be made without departing from the spirit or scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure as generally described herein and illustrated in the figures can be arranged substituted combined separated and designed in a wide variety of different configurations all of which are explicitly contemplated herein.

With respect to any or all of the message flow diagrams scenarios and flow charts in the figures and as discussed herein each step block and or communication may represent a processing of information and or a transmission of information in accordance with example embodiments. Alternative embodiments are included within the scope of these example embodiments. In these alternative embodiments for example functions described as steps blocks transmissions communications requests responses and or messages may be executed out of order from that shown or discussed including in substantially concurrent or in reverse order depending on the functionality involved. Further more or fewer steps blocks and or functions may be used with any of the message flow diagrams scenarios and flow charts discussed herein and these message flow diagrams scenarios and flow charts may be combined with one another in part or in whole.

A step or block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein described method or technique. Alternatively or additionally a step or block that represents a processing of information may correspond to a module a segment or a portion of program code including related data . The program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique. The program code and or related data may be stored on any type of computer readable medium such as a storage device including a disk or hard drive or other storage media.

The computer readable medium may also include non transitory computer readable media such as computer readable media that stores data for short periods of time like register memory processor cache and or random access memory RAM . The computer readable media may also include non transitory computer readable media that stores program code and or data for longer periods of time such as secondary or persistent long term storage like read only memory ROM optical or magnetic disks and or compact disc read only memory CD ROM for example. The computer readable media may also be any other volatile or non volatile storage systems. A computer readable medium may be considered a computer readable storage medium for example or a tangible storage device.

Moreover a step or block that represents one or more information transmissions may correspond to information transmissions between software and or hardware modules in the same physical device. However other information transmissions may be between software modules and or hardware modules in different physical devices.

While various aspects and embodiments have been disclosed herein other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting with the true scope and spirit being indicated by the following claims.

