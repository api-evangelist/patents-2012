---

title: Binding of network flows to process threads
abstract: A method of routing internal network traffic within a computing system, comprises receiving a network packet at a configurable logic device (CLD), parsing the network packet to obtain a source address and a destination address, searching a predetermined range of a routing table wherein each row of the routing table specifies a range of possible destination addresses and a thread group identifier, identifying a matching row of the routing table wherein the destination address falls within the range of possible destination addresses of the matching row, calculating a hash value based at least in part on the source and destination addresses, and determining a thread identifier based at least in part on the hash value and the thread group identifier.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08811401&OS=08811401&RS=08811401
owner: BreakingPoint Systems, Inc.
number: 08811401
owner_city: Austin
owner_country: US
publication_date: 20120621
---
This application is a member of a family of related U.S. Co Pending Applications filed Jun. 21 2012 including Ser. Nos. 13 529 207 13 529 248 13 529 289 13 529 339 13 529 372 13 529 423 13 529 479 13 529 535 13 529 575 13 529 745 13 529 786 13 529 821 13 529 859 13 529 910 13 529 932 13 529 970 13 529 983 13 529 998 13 530 019 13 530 052 13 530 059 and 13 530 094.

The present disclosure relates to systems and methods for testing communications networks services and devices e.g. testing the traffic handling performance and or security of the network network accessible devices cloud services and data center services.

Organizations are increasingly reliant upon the performance security and availability of networked applications to achieve business goals. At the same time the growing popularity of latency sensitive bandwidth heavy applications is placing heavy demands on network infrastructures. Further cyber attackers are constantly evolving their mode of assault as they target sensitive data financial assets and operations. Faced with these performance demands and increasingly sophisticated security threats network equipment providers NEPs and telecommunications service providers SPs have delivered a new generation of high performance content aware network equipment and services.

Content aware devices that leverage deep packet inspection DPI functionality have been around for several years and new content aware performance equipment is coming to market each year. However recent high profile performance and security failures have brought renewed focus to the importance of sufficient testing to ensure content aware network devices can perform under real world and peak conditions. The traditional approach of simply reacting to attacks and traffic evolution has cost organizations and governments billions. Today s sophisticated and complex high performance network devices and the network they run on require a more comprehensive approach to testing prior to deployment than traditional testing tools are able to provide. NEPs SPs and other organizations require testing solutions capable of rigorously testing simulating and emulating realistic application workloads and security attacks at line speed. Equally important these testing tools must be able to keep pace with emerging and more innovative products as well as thoroughly vet complex content aware DPI capable functionality by emulating a myriad of application protocols and other types of content at ever increasing speeds and feeds to ensure delivery of an outstanding quality of experience QoE for the customer and or subscriber.

Network infrastructures today are typically built on IP foundations. However measuring and managing application performance in relation to network devices remain challenges. To make matters worse content aware networking mandates controls for Layers 4 7 as well as the traditional Layer 2 3 attributes. Yet to date the bulk of the IP network testing industry has focused primarily on testing of Layers 2 3 with minimal consideration for Layers 4 7. Now with the rise of content driven services Layers 4 7 are increasingly strategic areas for network optimization and bulletproofing.

Even as NEPs and SPs rush to introduce newer more sophisticated content aware DPI capable devices to reap the associated business and recreational benefits these products deliver the testing of these devices has remained stagnant. Legacy testing solutions and traditional testing practices typically focus on the IP network connection especially routers and switches and do not have sufficient functionality or capability to properly test this new class of devices. Nor are they aligned with content driven approaches such as using and applying test criteria using stateful blended traffic and live security strikes at line speeds. The introduction of content aware functionality into the network drives many new variables for testing that resist corner case approaches and instead require realistic randomized traffic testing at real time speeds. The inability to test this new set of content aware and software driven packet inspection devices contributes to the deployment challenges and potential failure of many of them once they are deployed.

In one embodiment a method of routing internal network traffic within a computing system comprises receiving a network packet at a configurable logic device CLD parsing the network packet to obtain a source address and a destination address searching a predetermined range of a routing table wherein each row of the routing table specifies a range of possible destination addresses and a thread group identifier identifying a matching row of the routing table wherein the destination address falls within the range of possible destination addresses of the matching row calculating a hash value based at least in part on the source and destination addresses and determining a thread identifier based at least in part on the hash value and the thread group identifier.

In another embodiment a tangible non transitory computer readable media comprising a configuration file that when loaded by the CLD configures the CLD to receive a network packet at a configurable logic device CLD parse the network packet to obtain a source address and a destination address search a predetermined range of a routing table wherein each row of the routing table specifies a range of possible destination addresses and a thread group identifier identify a matching row of the routing table wherein the destination address falls within the range of possible destination addresses of the matching row calculate a hash value based at least in part on the source and destination addresses and determine a thread identifier based at least in part on the hash value and the thread group identifier.

In yet another embodiment a computing system comprises a CLD and a processor on the same card wherein the CLD is configured to receive a network packet at a configurable logic device CLD parse the network packet to obtain a source address and a destination address search a predetermined range of a routing table wherein each row of the routing table specifies a range of possible destination addresses and a thread group identifier identify a matching row of the routing table wherein the destination address falls within the range of possible destination addresses of the matching row calculate a hash value based at least in part on the source and destination addresses and determine a thread identifier based at least in part on the hash value and the thread group identifier.

Preferred embodiments and their advantages over the prior art are best understood by reference to below in view of the following general discussion.

Network testing system may be configured to test the performance e.g. traffic handling performance of devices the security of a test system e.g. from security attacks or both the performance and security of a test system . In some embodiments network testing system configured to simulate a realistic combination of business recreational malicious and proprietary application traffic at sufficient speeds to test both performance and security together using the same data and tests. In some embodiments network testing system is configured for testing content aware systems devices and or content unaware systems .

Network may include any one or more networks which may be implemented as or may be a part of a storage area network SAN personal area network PAN local area network LAN a metropolitan area network MAN a wide area network WAN a wireless local area network WLAN a virtual private network VPN an intranet the Internet or any other appropriate architecture or system that facilitates the communication of signals data and or messages generally referred to as data via any one or more wired and or wireless communication links.

Devices may include any type or types of network device e.g. servers routers switches gateways firewalls bridges hubs databases or data centers workstations desktop computers wireless access points wireless access devices and or any other type or types of devices configured to communicate with other network devices over a communications medium. Devices may also include any hardware software and or firmware components of any such network device e.g. operating systems applications CPUs configurable logic devices CLDs application specific integrated circuits ASICs etc.

In some embodiments network testing system is configured to model and simulate network traffic. The network testing system may act as virtual infrastructure and simulate traffic behavior of network devices e.g. database server Web server running a specific application. The resulting network traffic originated from the network testing system may drive the operation of a test system for evaluating the performance and or security of the system . Complex models can be built on realistic applications such that a system can be tested and evaluated under realistic conditions but in a testing environment. Simultaneously network testing system may monitor the performance and or security of a test system and may collect various metrics that measure performance and or security characteristics of system .

In some embodiments network testing system comprises a hardware and software based testing and simulation platform that includes of a number of interconnected subsystems. These systems may be configured to operate independently or in concert to provide a full spectrum solution for testing and verifying network performance application and security traffic scenarios. These subsystems may be interconnected in a manner to provide high performance highly accurate measurements and deep integration of functionality.

For example as shown in network testing system may comprise any or all of the following testing and simulation subsystems a high speed high resolution network packet capture subsystem a high speed packet generation and measurement subsystem an application level simulation and measurement subsystem a security and exploit simulation and analysis subsystem and or a statistics collection and reporting subsystem . Subsystems are discussed below in greater detail. In some embodiments the architecture of network testing system may allow for some or all of subsystems to operate simultaneously and cooperatively within the same software and hardware platform. Thus in some embodiments system is configured to generate and analyze packets at line rate while simultaneously capturing that same traffic performing application simulation and security testing. In particular embodiments system comprises custom hardware and software arranged and programmed to deliver performance and measurement abilities not achievable with conventional software or hardware solutions.

Network testing system may be connected to the test system in any suitable manner e.g. according to any suitable topology or arrangement. In some embodiments or arrangements network testing system may be connected on both sides of a system to be tested e.g. to simulate both clients and servers passing traffic through the test system. In other embodiment or arrangements network testing system may be connected to any entry point to the test system e.g. to act as a client to the test system . In some embodiment or arrangements network testing system may act in both of these modes simultaneously.

In some embodiment network testing system may be configured to simulated various components of an LTE network in order to test other components or communication links of the LTE network . illustrate three example arrangements in which system simulates different portions or components of the LTE network in order to test other components or communication links of the LTE network i.e. the tested system . In each figure the portions or components of the LTE network that are simulated by system are indicated by a double line outline and connections between network testing system and the tested components of the LTE network are indicated by dashed lines and reference number .

In the example arrangement shown in network testing system may be configured to simulate user equipment UEs and eNodeB interfaces at one end of the LTE network and a public data network e.g. Internet devices connected to the other end of the LTE network. As shown network testing system may be connected to the tested portion of the LTE network by connections that simulate the following LTE network connections a S1 MME connections between eNodeB interfaces and the MME b S1 U connection between eNodeB interfaces and the SGW and c SGi connection between the PGW and public data network e.g. Internet devices .

The example arrangement shown in is largely similar to the example arrangement of but the MME is also simulated by network testing system and the LTE network is connected to an actual public data network e.g. real Internet servers rather than simulating the public data network using system . Thus as shown network testing system is connected to the tested portion of the LTE network by connections that simulate the following LTE network connections a S1 U connection between eNodeB interfaces and the SGW and b S11 connection between the MME and SGW.

In the example arrangement shown in network testing system is configured to simulate all components of the LTE network with the expectation that a deep packet inspection DPI device e.g. a firewall intrusion detection or prevention device e.g. IPS or IDS load balancer etc. will be watching and analyzing the traffic on interfaces S1 U and S11. Thus network testing system may test the performance of the DPI device.

Each card may be plugged into a backplane which may include physical connections for communicatively connecting cards to each other as discussed below. While cards may be interconnected each card is treated for some purposes as an independent unit. Communications within a card are considered to be local communications. Two different cards attached to the same backplane may be running different versions of software so long as the versions are compatible.

Each card may include any architecture of hardware software and or firmware components for providing the functionality of network testing system . For example card may include an architecture card may include an architecture . . . and card n may include an architecture . The architecture of each card may be the same as or different than the architecture of each other card e.g. in terms of hardware software and or firmware and arrangement thereof.

Each architecture may include a system controller one or more network processors and one or more CLDs connected to a management switch and any other suitable components e.g. memory devices communication interfaces etc. . Cards may be communicatively coupled to each other via the backplane and management switches of the respective cards as shown in . In some embodiments backplane include physical connections for connecting each card directly to each other card . Thus each card may communicate with each other card via the management switches of the respective cards regardless of whether one or more slots are empty or whether one or more cards are removed.

In some embodiments each card may be configured to operate by itself or cooperatively with one or more other cards to provide any of the functionality discussed herein.

In general controller is programmed to initiate and coordinate many of the functions of network testing system . In some embodiments controller may be a general purpose central processing unit CPU such as an Intel x86 compatible part. Controller may run a general purpose multitasking or multiprocessing operating system such as a UNIX or Linux variant.

In general network processors are programmed to generate outbound network data in the form of one or more data packets and are programmed to receive and process inbound network data in the form of one or more data packets. In some embodiments network processors may be general purpose CPUs. In other embodiments network processors may be specialized CPUs with instruction sets and hardware optimized for processing network data. For example network processors may be selected from the Netlogic XLR family of processors.

Configurable logic devices CLDs provide high performance specialized computation data transfer and data analysis capabilities to process certain data or computation intensive tasks at or near the network line rates.

As used herein the term configurable logic device CLD means a device that includes a set of programmable logic units internal memory and high speed internal and external interconnections. Examples of CLDs include field programmable gate arrays FPGAs e.g. ALTERA STRATIX family XILINX VIRTEX family as examples programmable logic devices PLDs programmable array logic devices PAL and configurable programmable logic devices CPLDs e.g. ALTERA MAXII as an example . A CLD may include task specific logic such as bus controllers Ethernet media access controllers MAC and encryption decryption modules. External interconnections on a CLD may include serial or parallel data lines or busses. External interconnections may be specialized to support a particular bus protocol or may be configurable general purpose I O connections. Serial and parallel data connections may be implemented via specialized hardware or through configured logic blocks.

Memory within a configurable logic device may be arranged in various topologies. Many types of configurable logic devices include some arrangement of memory to store configuration information. In some devices individual programmable logic units or clusters of such units may include memory blocks. In some devices one or more larger shared banks of memory are provided that are accessible to programmable logic units via internal interconnections or busses. Some configurable logic devices may include multiple arrangements of memory.

A configurable logic device may be configured or programmed at different times. In some circumstances a configurable logic device may be programmed at the time of manufacture of the configurable logic device or of a device containing the configurable logic device . This manufacture time programming may be performed by applying a mask to the device and energizing a light or other electromagnetic wave form to permanently or semi permanently program the device. A configurable logic device may also be programmed electronically at manufacture time initialization time or dynamically. Electronic programming involves loading configuration information from a memory or over an input output connection. Some configurable logic devices may include onboard non volatile memory e.g. flash memory for storing configuration information. Such an arrangement allows the configurable logic device to program itself automatically when power is applied.

As used herein the terms processor and CPU mean general purpose computing devices with fixed instruction sets or microinstruction sets such as x86 processors e.g. the INTEL XEON family and the AMD OPTERON family as examples only POWERPC processors and other well known processor families. The terms processor and CPU may also include graphics processing units GPUs e.g. NVIDIA GEFORCE family as an example and network processors NPs e.g NETLOGIC XLR and family INTEL IXP family CAVIUM OCTEON for example . Processors and CPUs are generally distinguished from CLDs as defined above e.g. FPGAs CPLDs etc. Some hybrid devices include blocks of configurable logic and general purpose CPU cores e.g. XILINX VIRTEX family as an example and are considered CLDs for the purposes of this disclosure.

An application specific integrated circuit ASIC may be implemented as a processor or CLD as those terms are defined above depending on the particular implementation.

As used herein the term instruction executing device means a device that executes instructions. The term instruction executing device includes a processors and CPUs and b CLDs that have been programmed to implement an instruction set.

Management switch allows and manages communications among the various components of testing architecture A as well as communications between components of testing architecture A and components of one or more other cards e.g. via backplane as discussed above with respect to . Management switch may be a Ethernet layer 2 multi port switch.

As discussed above the components of example architecture A may be provided on a single blade and multiple blades may be connected together via backplane to create larger systems. The various components of example architecture A are now discussed according to example embodiments.

Test interfaces may comprise any suitable communication interfaces for connecting architecture A to a test system e.g. network or device . For example test interfaces may implement Ethernet network connectivity to a test system . In one embodiment interfaces may work with SFP modules which allow changing the physical interface from 10 Mbps 10 BaseT twisted pair copper wiring to 10 Gbps long range fiber. The test interfaces may include one or more physical layer devices PHYa and SFP modules. The PHYs and SFP modules may be configured using low speed serial buses implemented by the capture and offload CLDs A e.g. MDIO and I2C .

An CLD Field Programmable Gate Array is a reprogrammable device that can be modified to simulate many types of hardware. Being reprogrammable it can be continually expanded to offer new acceleration and network analysis functionality with firmware updates. Example testing and simulation architecture A includes various CLDs designated to perform different functions including two capture and offload CLDs A capturing data packets two router CLDs B for routing data between components of architecture A and a traffic generation CLD C for generating traffic that is delivered to the test system .

The capture and offload CLDs A have the following relationships to other components of testing and simulation architecture A 

1. Each capture and offload CLDs A is connected to one or more test interfaces . Thus CLDs A are the first and last device in the packet processing pipeline. In some embodiments Ethernet MACs Media Access Controllers required to support 10 100 1000 and 10000 Mbps Ethernet standards are implemented within CLDs A and interact with the physical layer devices PHYs that implement with the test interfaces .

2. Each capture and offload CLDs A is also connected to a capture memory device A that the CLD A can write to and read from. For example each CLD A may write to capture memory when capturing network traffic and read from memory when performing capture analysis and post processing.

3. Each capture and offload CLDs A is connected to the traffic generation CLD C. In this capacity the CLDs A is a pass through interface packets sent by the traffic generation CLD C are forwarded directly to an Ethernet test interface for delivery to the test system

4. Each capture and offload CLDs A is connected to a router CLD B for forwarding packets to and from the NPs and the controller .

5. Each capture and offload CLDs A is connected to the management switch which allows for configuration of the CLD A and data extraction in the case of capture memory from the controller or a network processor .

Each capture and offload CLDs A may be programmed to implement the following functionality for packets received from test interfaces . First each capture and offload CLD A may capture and store a copy of each packet received from a test interface in the capture memory attached to CLD A along with a timestamp for when that packet arrived. Simultaneously the capture and offload CLD A may determine if the packet was generated originally by the traffic generation CLD C or some other subsystem. If CLD A determines that the packet was generated originally by the traffic generation CLD C the CLD A computes receive statistics for the high speed packet generation and measurement subsystem of system e.g. refer to . In some embodiments the packet is not forwarded to any other subsystem in this case. Alternatively if capture and offload CLD A determines that a packet was not generated originally by the traffic generation CLD C the capture and offload CLD A may parse the packet s layer 2 3 4 headers validate all checksums up to 2 layers insert a receive timestamp and forward the packet to the closest router CLD B for further processing.

Each capture and offload CLDs A may also be programmed to implement the following functionality for packets that it transmits to a test interface for delivery to the test system . Packets received at a capture and offload CLD A from the traffic generation CLD C are forwarded by the CLD A as is to the test interface for delivery to the test system . Packets received at a capture and offload CLD A from a router CLD B may have instructions in the packet for specific offload operations to be performed on that packet before it is sent out trough a test interface . For example packets may include instructions for any one or more of the following offload operations a insert a timestamp into the packet b calculate checksums for the packet on up to 2 layers of IP and TCP UDP ICMP headers and or c split the packet into smaller TCP segments via TCP segmentation offload. Further a capture and offload CLD A may forward a copy of each packet or particular packets for storage in the capture memory B attached to the CLD A along with a timestamp indicating when each packet was sent.

In addition to forwarding packets out a test interface each capture and offload CLD A may be configured to simulate a packet being sent and instead of actually transmitting the packet physically on a test interface . This loopback mode may be useful for calibrating timestamp calculations for the rest of architecture A or system by providing a fixed known latency on network traffic. It may also be useful for debugging hardware and network configurations.

As discussed above each capture and offload CLDs A may be connected to capture memory device A that the CLD A can write to and read from. Capture memory device A may comprise any suitable type of memory device e.g. DRAM SRAM or Flash memory hard dive or any other memory device with sufficient bandwidth. In some embodiments a high speed double data rate SDRAM e.g. DDR2 or DDR3 memory interface is provided between each capture and offload CLDs A and its corresponding capture memory device A. Thus data may be written at near maximum theoretical rates to maintain an accurate representation of all packets that arrived on the network within the limits of the amount of available memory.

Router CLDs B may have similar flexibility as the capture and offload CLD A. Router CLDs B may implement glue logic that allows the network processors and controller the ability to send and receive packets on the test network interfaces . Each router CLD B may have the following relationships to other components of testing and simulation architecture A 

1. Each router CLD B is connected to a capture and offload CLD A which gives it a set of local test interface e.g. Ethernet interfaces with which it can send and receive packets.

2. The router CLDs B are also connected to each other by an interconnection . Thus packets can be sent and received on remote test interfaces via an interconnected router CLD B. For example the router CLDs B shown on the right side of may send and receive packets via the test interface shown on the left side of by way of interconnection between the two CLDs B.

3. A network processor may connect to each router CLD B via two parallel interfaces e.g. two parallel interfaces 10 gigabit interfaces . These two connections may be interleaved to optimize bandwidth utilization for network traffic. For example they may be used both for inter processor communication e.g. communications between network processors and between controller and network processors and for sending traffic to and from the test interfaces .

4. Controller also connects to each router CLD B. For example controller may have a single 10 gigabit connection to the each router CLD B which may serve a similar purpose as the network processor connections . For example they may be used both for inter processor communication and for sending traffic to and from the test interfaces .

5. Each router CLD B may include a high speed low latency SRAM memory. This memory may be used for storing routing tables statistics TCP reassembly offload or other suitable data.

6. Each router CLD B is connected to the management switch which may allow for configuration of the router CLD B and extraction of statistics for example.

In some embodiments for packets sent from a network processor or controller the sending processor first specifies a target address in a special internal header in each packet. This address may specify a test interface or another processor . The router CLD B may use the target address to determine where to send the packet next e.g. it may direct the packet to the another router CLD B or to the nearest capture and offload CLD A.

For incoming packets from the test system that arrive at a router CLD B more processing may be required because the target address header is absent for packets that have arrived from the test system . In some embodiments the following post processing is performed by a router CLD B for each incoming packet from the test system 

1. The router CLD B parses the packet is parsed to determine the VLAN tag and destination IP address of the packet.

2. The router CLD B consults a programmable table of IP addresses e.g. implemented using memory built in to the CLD B to determine the address of the target processor . This contents of this table may be managed by software of controller .

3. The router CLD B computes a hash function on the source and destination IP addresses and port numbers of the packet.

4. The router CLD B inserts a 32 bit hash value into the packet along with any latency checksum status or other offload information inserted by the respective offload and capture CLD A .

5. The router CLD B then uses the hash value to determine the optimal physical connection to use for a particular processor address because a network processor has two physical connections as shown in .

6. If the packet is not IP has no matching VLAN or has no other specific routing information the router CLD B consults a series of default processor addresses in an auxiliary table e.g. implemented using memory built in to the CLD B .

In some embodiments the router CLD B also implements TCP reassembly offloads and extra receive buffering using attached memory e.g. attached SRAM memory . Further it can be repurposed for any other suitable functions e.g. for statistics collection by network processor .

Each network processor NP may be a general purpose CPU with multiple cores security and network acceleration engines. In some embodiments each network processor may be an off the shelf processor designed for network performance. However it may be very flexible and may be suitable to perform tasks ranging from low level high speed packet generation to application and user level simulation. Each network processor may have the following relationships to other components of testing and simulation architecture A 

1. Each network processor may be connected to a router CLD B. The router CLD B may provide the glue logic that allows the processor to send and receive network traffic to the rest of the system and out the test interfaces to the test system .

2. Each network processor may be also connected to the management switch . In embodiments in which the network processor has no local storage e.g. a disk drive it may load its operating system and applications from the controller via the management network. As used herein the management network includes management switch CLDs A B and C backplane and controller .

3. Because the CLDs are all connected to the management switch the network processors may be responsible for managing and configuring certain aspects of the router CLDs B and offload and capture CLDs A.

2. IP and Ethernet layer address allocation and routing protocols are handled by the network processor .

4. The network processor works with software on the controller to collect statistics which may subsequently be used by the statistics and reporting engine of subsystem .

5. The network processor may also collect statistics from CLDs A B and C and report them to the controller . In an alternative embodiment the controller itself is configured to collect statistics directly from CLDs A B and C.

Controller may compare any suitable controller programmed to control various functions of system architecture A. In some embodiments controller may be a general purpose CPU with multiple cores with some network but no security acceleration. For example controller may be an off the shelf processor designed primarily for calculations and database performance. However it can also be used for other tasks in the system A and can even be used as an auxiliary network processor due to the manner in which it is connected to the system. Controller may have the following relationships to other components of testing and simulation architecture A 

1. Controller manages a connection with a removable disk storage device or other suitable memory device .

2. Controller may connect to the management switch to configure boot and manage all other processors and CLDs in the system A.

3. Controller is connected to each router CLD B for the purpose of high speed inter processor communication with network processors e.g. to provide a 10 Gbps low latency connection to the network processors in addition to the 1 Gbps connection provided via the management switch as well as generating network traffic via test interfaces .

Controller may be the only processor connected directly to the removable disk storage . In some embodiments all firmware or software used by the rest of the system A except for firmware required to start the controller itself BIOS resides on the disk drive . A freshly manufactured system A can self program all other system components from the controller .

2. Controller runs the middle ware and server applications that coordinates the rest of the system operation.

4. Controller hosts the database statistics and reporting engine of statistics collection and reporting subsystem .

The of the traffic generation CLD C is to generate traffic at line rate. In some embodiment traffic generation CLD C is configured to generate layer 2 layer 3 traffic thus traffic generation CLD C may be referred to as an L2 L3 traffic CLD.

In an example embodiment traffic generation CLD C is capable of generating packets at 10 Gbps using a small packet size e.g. the smallest possible packet size for the four test interfaces simultaneously or 59 523 809 packets per second. In some embodiments this functionality may additionally or alternatively be integrated into each capture and offload CLD A. Traffic generation CLD C may have the following relationship to other components of testing and simulation architecture A 

1. Traffic generation CLD C is connected to capture and offload CLDs A. For example traffic generation CLD C may be connected to capture and offload CLDs A via two 20 Gbps bi directional links. Traffic generation CLD C typically only sends traffic but is may also be capable of receiving traffic or other data.

2. Traffic generation CLD C is connected to the management switch which allows for configuration of CLD C for generating traffic. Controller may be programmed to configure traffic generation CLD C via management switch .

Like other CLDs traffic generation CLD C is reconfigurable and thus may be reconfigured to provide other functions as desired.

A buffer reassembly memory device B may be coupled to each router CLDs B. Each memory device B may comprise any suitable memory device. For example each memory device B may comprise high speed low latency QDR quad data rate SRAM memory attached to the corresponding router CLD B for various offload purposes e.g. statistics collection packet buffering TCP reassembly offload etc.

A suitable memory device may be coupled to controller . For example memory device may comprise a removable solid state drive SSD in a custom carrier that allows hot swapping and facilitates changing software or database contents on an installed board. Disk drive may store various data including for example 

2. An operating system applications and statistics and reporting database utilized by the controller and

The management switch connects to every CLD network processor and control CPU in the system A. In some embodiments management switch comprises a management Ethernet switch configured to allow communication of for 1 10 Gbit traffic both between blades and between the various processors and CLDs on each particular blade . Management switch may route packets based on the MAC address included in each packet passing through switch . Thus management switch may essentially act as a router allowing control CPUs to communication with network processor and CLD on the same card and other cards in the system . In such embodiment all subsystems are controllable via Ethernet such that additional processors and CLDs may be added by simply chaining management switches together.

In an alternative embodiment control CPU of different cards may be connected in any other suitable manner e.g. by a local bus or PCI for example. However in some instances Ethernet connectivity may provide certain advantages over a local bus or PCI e.g. Ethernet may facilitate more types of communication between more types of devices than a local bus or PCI.

Network testing system may be configured to support any suitable number of cards or blades . In one embodiment system is configured to support between 1 and 14 cards in a single chassis . Backplane may provide a system for interconnecting the management Ethernet provided by the management switches of multiple cards as well as system monitoring connections for measuring voltages and temperatures on cards and for debugging and monitoring CPU status on all cards for example. Backplane may also distribute clock signals between all cards in a chassis so that the time stamps for all CPUs and CLDs remain synchronized.

In some embodiments network testing system may provide an integrated solution that provides some or all of the following functions 1 high speed high resolution network packet capture 2 high speed packet generation and measurement 3 application level simulation and measurement 4 security and exploit simulation and analysis and 5 statistics collection and reporting. Thus as discussed above with respect to network testing system may comprise a high speed high resolution network packet capture subsystem a high speed packet generation and measurement subsystem an application level simulation and measurement subsystem a security and exploit simulation and analysis subsystem and or a statistics collection and reporting subsystem . The architecture of system e.g. example architecture A discussed above or example architecture B discussed below may allow for some or all of these subsystems to operate simultaneously and cooperatively within the same software and hardware platform. Thus system may be capable of generating and analyzing packets at line rate while simultaneously capturing that same traffic performing application simulation and security testing.

Modern digital networks involve two or more or nodes that send data between each other over a shared physical connection using units of data called packets. Packets contain information about the source and destination address of the nodes application information. A network packet capture is the observing and storage of packets on the network for later debugging and analysis.

Network packet capture may be performed for various reasons e.g. lawful intercept tapping performance analysis and application debugging for example. Packet capture devices can range in complexity from a simple desktop PC most PCs have limited capture abilities built into their networking hardware to expensive purpose built hardware. These devices vary in both their capacity and accuracy. A limited capture system is typically unable to capture all types of network packets or sustain capture at the maximum speed of the network.

In contrast network packet capture subsystem of network testing system may provide high speed high resolution network packet capture capable of capturing all types of network packets e.g. Ethernet TCP UDP ICMP IGMP etc. at the maximum speed of the tested system e.g. 4.88 million packets per second transmit and receive per test interface .

 b An Ethernet MAC Media Access Controller implemented inside CLD A per physical interface which can be programmed to enter promiscuous mode in which the Ethernet MAC can be instructed to snoop all network packets even those not addressed for it. Normally an Ethernet MAC will only see packets on a network that include its local MAC Address or that are addressed for broadcast or multicast groups. A MAC Address may be a 6 byte Ethernet media access control address. A capture system should be able to see all packets on the network even those that are not broadcast multicast or addressed with the MAC s local MAC address. In some embodiments it may be desirable to enter a super promiscuous mode in order to receive even erroneous packets. Typical Ethernet MACs will drop malformed or erroneous packets even if in promiscuous mode on the assumption that a malformed or erroneous packet is likely damaged and the sender should resend a correct packet if the message is important. These packets may be of interest in a network testing device such as system to identify and diagnose problem connections equipment or software. Thus the Ethernet MAC of CLD A may be configured to enter super promiscuous mode in order to see and capture all packets on the network even including erroneous packets e.g. corrupted packets as defined by Ethernet FCS at end of a packet .

 g Management switch configured to interface and control the capture and offload CLD A from the management processor .

An example network packet capture process is now described. When the packet capture feature is enabled by a user via the user interface provided by the system A see controller may configure the Ethernet MACs and PHYs to accept all packets on the network i.e. to enter promiscuous mode. Controller may then configure the capture and offload CLD A to begin storing all packets sent or received via the Ethernet MAC PHY in the high speed capture memory A attached to the CLD A. When the Ethernet MAC PHY sends or receives a packet it is thus captured in memory A by CLD A. For each captured packet CLD A also generates and records a high resolution e.g. 10 nanosecond timestamp in memory A with the respective packet. This timestamp data can be used to determine network attributes such as packet latency and network bandwidth utilization for example.

Using the architecture discussed herein system can store packets sent and received at a rate equivalent to the maximum rate possible on the network. Thus as long as there is sufficient memory A attached to the CLD A a 100 accurate record of the traffic that occurred on test system may be recorded. If memory A fills up a wrapping mechanism of CLD A allows CLD A to begin overwriting the oldest packets in memory with newer packets.

To achieve optimal efficiency CLD A may store packets in memory in their actual length and may use a linked list data structure to determine where the next packet begins. Alternatively CLD A may assume all packets are a fixed size. While this alternative is computationally efficient a given packet can be found in memory by simply multiplying by a fixed value memory space may be wasted when packets captured on the network are smaller than the assumed size.

CLD A may also provide a tail pointer that can be used to walk backward in the list of packets to find the first captured packet. Once the first captured packet is located the control software can read the capture memory A and generate a diagnostic file called a PCAP Packet CAPture file which can be sent to the user and or stored in disk . This file may be downloaded and analyzed by a user using a third party tool.

Because there can be millions of packets in the capture memory A walking through all of the packets in the packet capture to located the first captured packet based on the tail pointer may take considerable time. Thus CLD A may provide a hardware implementation that walks the linked list and can provide the head pointer directly. In addition copying the capture memory A to a file that is usable for analysis can take additional time. Thus CLD A may implement a bulk memory copy mode that speeds up this process.

At step CLD A may rewind capture memory A e.g. using tail pointers as discussed above or using any other suitable technique. At step controller may read data from capture memory A and write to disk e.g. in the form of a PCAP Packet CAPture file as discussed above which file may then be downloaded and analyzed using third party tools.

Table 1 provides a comparison of the performance of network packet capture subsystem to certain conventional solutions according to an example embodiment of system .

In some embodiments dedicated packet capture memory and hardware may be omitted e.g. for design simplicity cost etc. In such embodiments a software only implementation of packet capture may instead be provided although such implementation may have reduced performance as compared with the dedicated packet capture memory and hardware subsystem discussed above.

Modern networks can transport packets at a tremendous rate. A comparison of various network speeds and the maximum packets second that they can provide is set forth in Table 2.

The data rate for the fastest network of a given era typically exceeds the number of packets second that a single node on the network can practically generate. Thus to test the network at its maximum possible packet rate one might need to employ either many separate machines or a custom solution dedicated to generating and receiving packets at the highest possible rate.

As discussed above network testing system may include a high speed packet generation and measurement subsystem for providing packet generation and measurement at line rate. illustrates relevant components of subsystem . In an example embodiment high speed packet generation and measurement subsystem may utilize the following system components 

 f Controller software of controller configured to manage network resources allowing the CLD generated traffic to co exist with the traffic generated by other subsystems at the same time.

The CLD solution provided by subsystem is capable of sending traffic and analyzing traffic at the maximum packet rate for 10 Gbps Ethernet which may be difficult for even a high end PC. Additionally subsystem can provide diagnostic information at the end of each packet it sends. This diagnostic information may include for example 

The checksum may be placed at the end of each packet. This checksum covers a variable amount of the packet because as a packet traverses the network it may be expected to change in various places e.g. the time to live field or the IP addresses . The checksum allows verification that a packet has not changed in unexpected ways or been corrupted in transit. In some embodiments the checksum is a 32 bit CRC checksum which is more reliably able to detect certain types of corruption that the standard 16 bit s complement TCP IP checksums.

The sequence number may allow detection of packet ordering even if the network packets do not normally have a method of detecting the sequence number. This sequence number may be 32 bit which wraps less quickly on a high speed network as compared to other standardized packet identifiers e.g. the 16 bit IP ID.

The timestamp may have any suitable resolution. For example the timestamp may have a 10 nanosecond resolution which is fine grained enough to measure the difference in latency between a packet traveling through a 1 meter and a 20 meter optical cable effectively measuring the speed of light. 

The signature field may allows the CLD A to accurately identify packets that need analysis from other network traffic without relying on the simulated packets having any other identifiable characteristics. This signature also allows subsystem to operate without interfering with other subsystems while sharing the same test interfaces .

While high speed packet generation and analysis can be used to illustrate raw network capacity integrity and latency modern networks also analyze traffic beyond individual packets and instead look at application flows. This is known as deep packet inspection. Also it is often desired to measure performance of not only the network itself but individual devices such as routers firewalls load balancers servers and intrusion detection and prevention systems for example.

To properly exercise these systems higher level application data is sent on top of the network. Network testing system may include an application level simulation and measurement subsystem to provide such functionality. illustrates relevant components of subsystem . In an example embodiment application level simulation and measurement subsystem may utilize the following system components 

 d Multiple capture and offload CLDs A and router CLDs B configured to route traffic between the Ethernet MACs and the network processors and to perform packet acceleration offload tasks.

 f Controller software of controller to manage network resources allowing the network processor generated application traffic to co exist with the traffic generated by other subsystems at the same time.

In some embodiments the network processors execute software that implements both the networking stack Ethernet TCP IP routing protocols etc. and the application stack that is typically present on a network device. In this sense the software can simulate network clients e.g. Desktop PCs servers routers and a whole host of different applications. This programmable application engine software is given instructions on how to properly simulate a particular network or application by an additional software layer. This software layer may provide information such as 

4. Details on how to simulate a particular application mid level instructions for application interaction .

The details on how to simulate applications reside in software that runs on the management processor on controller . A user can model an application behavior in a user interface see e.g. that provides high level application primitives such as to make a database query or load a web page for example. These high level behaviors are translated by software into low level instructions such as send a packet expect 100 bytes back which are then executed by the application engine running on the network processor . New applications can be implemented by a user e.g. a customer or in house personnel without any changes to the application engine itself. Thus it is possible to add new functionality without upgrading software.

Physically the network processors connect to multiple CLDs . All packets that leave the network processor first pass through one or more CLDs before they are sent to the Ethernet interfaces and all packets that arrive via the Ethernet interfaces pass through one or more CLDs before they are forwarded to a network processor . The CLDs are thus post and pre processors for all network processor traffic. In addition the packet capture functionality provided by subsystem discussed above is able to capture all network processor generated traffic.

4. Incoming packet routing and load balancing to support multiple network processors using the same physical interface by CLD B .

For timestamp insertion a network processor can request that the CLD A insert a timestamp into a packet originally generated by the network processor before it enters the Ethernet. The CLD A can also supply a timestamp for when a packet arrives before it is forwarded to a network processor . This is useful for measuring high resolution accurate packet latency in a way typically only available to a simple packet generator on packets containing realistic application traffic. Unlike conventional off the shelf hardware that can insert and capture timestamps CLD A is configured to insert a timestamp into any type of packet including any kind of packets e.g. PTP IP TCP UDP ICMP or Ethernet layer packets instead of only PTP Precision Time Protocol packets as part of the IEEE 1588 standard.

TCP IP checksum offload may also be performed by the CLDs A. Unlike a typical hardware offload implemented by an off the shelf Ethernet controller the CLD implementation of system has an additional feature in that any packet can have multiple TCP IP checksums computed by CLD A on more than one header layer in the packet. This may be especially useful when generating packets that are tunneled and thus have multiple TCP IP or UDP checksums. Conventional solutions cannot perform a checksum on more than one header layer in a packet.

For TCP segmentation offload a single large TCP packet can automatically be broken into smaller packets by CLD A to fit the maximum transmission unit MTU of the network. TCP segmentation offload can save a great deal of CPU time when sending data at high speeds. Conventional solutions are typically implemented without restrictions such as all offloaded TCP segments will have the same timestamp. In contrast the CLD implementation of system allows timestamping of individual offloaded TCP segments as if they had been sent individually by the network processor .

Incoming packet routing and load balancing enable multiple network processors to be used efficiently in a single system. Conventional load balancing systems rely on some characteristic of each incoming packet to be unique such as the IP or Ethernet address. In the event that the configured attributes for incoming packets are not unique a system can make inefficient use of multiple processors e.g. all traffic goes to one processor rather than being fairly distributed. In contrast the CLD B implementation of packet routing in system provides certain features not typically available in commodity packet distribution systems such as TCAMs or layer 3 Ethernet switches. For example the CLD B implementation of system may provide any one or more of the following features 

1. The CLD implementation of system can be reconfigured to parse packets two headers deep. If all traffic has a single outer header e.g. tunneled traffic the system can look further to find unique identifiers in the packets.

2. The system may employ a hardware implementation of jhash a hashing algorithm designed by Bob Jenkens available at the URL burtleburtle.net bob c lookup3.c to distribute packets which is harder to defeat than other common implementation such as CRC and efficiently distributes packets that differ by very few bits.

3. Packets can be routed on thousands of arbitrary IP ranges as well using a lookup table built into the CLDs B.

At step the simulation finishes. Thus at step the network processor stops simulation and controller stops data collection regarding the simulation. At step the reporting engine on controller may generate reports based on data collected and stored at step .

In both isolated networks and the public Internet vulnerable users applications and networks continue to be exploited in the form of malware virus worms denial of service DoS distributed denial of service DDoS social engineering and other forms of attack. Network testing system may be configured to generate and deliver malicious traffic to a test system at the same time that it generates and delivers normal background traffic to test system . In particular security and exploit simulation and analysis subsystem of system may be configured to generate such malicious traffic. This may be useful for testing test system according to various scenarios such as for example 

 d Multiple capture and offload CLDs A and router CLDs B configured to route traffic between the Ethernet MACs and the network processors and to perform packet acceleration offload tasks.

 e A security engine comprising software configured to generate malicious application traffic and to verify its effectiveness. Security engine may be provided on a network processor and or controller and is thus indicated by dashed lines in .

 f Controller software of controller to manage network resources allowing the malicious application traffic to co exist with the traffic generated by other subsystems at the same time.

 g A management processor on controller configured to execute the controller software collect and store statistics and or generate malicious application traffic.

As mentioned above security engine may be provided on a network processor and or controller . For example in some scenarios the application engine employed by the network processor is used to generate malicious traffic when high performance is required. In other scenarios the management processor of controller can generate malicious traffic packet by packet and forward these to the network processor as if they were generated locally. This mechanism may be employed for more sophisticated attacks that do not require high performance.

At step the simulation finishes. Thus at step security engine stops simulation and controller stops data collection regarding the simulation. At step the reporting engine on controller may generate reports based on data collected and stored at step .

The management processor of controller in addition to providing a place for much of the control software for various subsystems to execute may also host a statistics database and reporting engine . Statistics database both stores raw data generated by other subsystems as well as derives its own data. For instance subsystem or may report the number and size of packets generated on a network over time. Statistics database can then compute the minimum maximum average standard deviation and or other statistical data regarding the data rate from these two pieces of data. Reporting engine may comprise additional software configured to convert statistics into reports including both data analysis and display of the data in an user readable format.

3. A data collection engine configured to converts raw data from sub components into a normalized form for the database .

Reporting engine and data collection engine may comprise software based modules stored in memory associated with controller e.g. stored in disk and executed by management processor .

In the control and management layer example applications are shown including network resiliency data center resiliency lawful intercept scenario editor and 4G LTE. Network and data center resiliency applications may provide an automated standardized and deterministic method for evaluating and ensuring the resiliency of networks network equipment and data centers. System provides a standard measurement approach using a battery of real world application traffic real time security attacks extreme user load and application fuzzing. That battery may include a blended mix of application traffic and malicious attacks including obfuscations.

Lawful intercept applications may test the capabilities of law enforcement systems to process realistic network traffic scenarios. These applications may simulate the real world application traffic that lawful intercept systems must process including major Web mail P2P VoIP and other communication protocols as well as triggering content in multiple languages. These applications may create needle in a haystack scenarios by embedding keywords to ensure that a lawful intercept solution under test detects the appropriate triggers tax the performance of tested equipment with a blend of application attack and malformed traffic at line rate and emulate an environment s unique background traffic by selecting from more than tens of application protocols e.g. SKYPE VoIP email and various instant messaging protocols.

The scenario editor application may allow modification of existing testing scenarios or the creation of new scenarios using a rules based interface. The scenario editor application may also enable configuration of scenarios based on custom program logic installed on system .

The 4G LTE application may allow testing and validation of mobile networking equipment and systems including mobile specific services like mobile specific web connections mobile device application stores and other connections over modern wireless channels. These applications may create city scale mobile data simulations to test the resiliency of mobile networks under realistic application and security traffic. Tests may measure mobility infrastructure performance and security under extreme network traffic conditions stress test key LTE network components with emulation of millions of user devices and thousands of transmission nodes and validate per device accounting billing and policy mechanisms.

Tcl scripting modules may allow web based user interface design and configuration of existing and user created applications. Reporting modules may allow generation of standardized reports on test results traffic analysis and ongoing monitoring data.

Supporting those applications is the unified control and test automation subsystem including two software modules Tcl scripting and reporting and three hardware modules security attacks protocol fuzzing and application protocols. The latter three modules comprise the application and threat intelligence program. Underlying the applications are three hardware layers including security accelerators network processors and configurable logic devices CLDs .

Security accelerator modules may provide customizable hardware acceleration of security protocols and functions. Security attack modules may provide customizable hardware implementation of specific security attacks that may be timing specific or may require extremely high traffic generation e.g. simulation of bot net and denial of service attacks . Protocol fuzzying modules may test edge cases in networking system implementations. A protocol fuzzying module may target a specific data value or packet type and may generate a variety of different values valid or invalid in turn. The goal of a fuzzer may be to provide malicious or erroneous data or to provide too much data to test whether a device will break and therefore indicate a vunerability. A protocol fuzzying module may also identify constraints by systematically varying as aspect of the input data e.g. packet size to determine acceptable ranges of input data. Application protocols modules may provide customizable hardware implementation or testing of specific network application protocols to increase overall throughput.

The output of one or more of these functional modules along with VLAN processing may be fed into one or more network processors along with the ingress packet. Likewise egress packets generated by the network processors may be processed by one or more of several core functional modules in high speed configurable logic devices including 

Controller provides operational control of one or more blades in architecture A. Controller includes control processor coupled to an electrically erasable programmable read only memory EEPROM containing the basic input and output system BIOS universal serial bus USB interfaces clock source joint test action group JTAG controller processor debug port random access memory RAM and Ethernet medium access controllers MACs A and B coupled to non volatile memories . EEPROM memory may be used to store general configuration options e.g. the MAC address es link types and other part specific configuration options. Flash memory may be used to store configurable applications such as network boot e.g. PXE Boot .

Controller may be an integrated system on a chip or a collection of two or more discrete modules. Control processor may be a general purpose central processing unit such as an INTEL x86 compatible processor. In some embodiments control processor may be an INTEL XEON processor code named JASPER FOREST and may incorporate or interface with additional chipset components including memory controllers and input output controllers e.g. the INTEL IBEX PEAK south bridge. Control processor is coupled e.g. via a serial peripheral interface to non volatile memory containing BIOS software. Note that references in this specification to SPI interfaces for example those interconnecting CLDs and or network processors are references to the system packet interface SPI 4.2 rather than the serial peripheral interface. The BIOS software provides processor instructions sufficient to configure control processor and any chipset components necessary to access storage device . The BIOS also includes instructions for loading or booting an operating system from storage device or a USB memory device connected to interface .

USB interfaces provide external I O access to controller . USB interfaces may be used by an operator to connect peripheral devices such as a keyboard and pointing device. USB interfaces may be used by an operator to load software onto controller or perform any other necessary data transfer. USB interfaces may also be used by controller to access USB connected devices within system A.

Clock source CK is a clock source to drive the operation of the components of controller . Clock source may be driven by a crystal to generate a precise oscillation wave.

JTAG controller is a microcontroller programmed to operate as a controller for JTAG communications with other devices. JTAG provides a fallback debugging and programming interface for various system components. This protocol enables fault isolation and recovery especially where a device has been incompletely or improperly programmed e.g. due to loss of power during programming. In certain embodiments JTAG controller is a CYPRESS SEMICONDUCTOR CY68013 microcontroller programmed to execute JTAG instructions and drive JTAG signal lines. JTAG controller may include or be connected to a non volatile memory to program the controller on power up.

Processor debug port is a port for debugging control processor as well as chipset components. Processor debug port may conform to the INTEL XDB specification.

RAM is a tangible computer readable medium coupled to control processor for storing the instructions and data of the operating system and application processes running on control processor . RAM may be double data rate DDR3 memory.

Ethernet MACs A and B provide logic and signal control for communicating with standard Ethernet devices. These MACS may be coupled to control processor via a PCIe bus. MACS A and B may be INTEL 82599 dual 10 Gbps parts. In some embodiments MACs A and B may be incorporated into control processor or the chipset devices. Ethernet MACs A and B are coupled to non volatile memories .

Controller is coupled to tangible computer readable medium in the form of mass storage device e.g. a solid state drive SSD based on high speed flash memory. In some embodiments controller is coupled to storage device via a high speed peripheral bus such as an SATA bus. Storage device includes an operating system application level programs to be executed on one or more processors within the system and other data and or instructions used to configure various components or perform the tasks of the present disclosure. Storage device may also store data generated by application level programs or by hardware components of the system. For example network traffic captured by capture offload CLDs A may be copied to storage device for later retrieval.

Network processor provides software programmable computing that may be optimized for network applications. Network processor may be a NETLOGIC XLR processor. Network processor is coupled to memory boot flash CPLD and Ethernet transceiver . Memory is a tangible computer readable storage medium for storing the instructions and data of the operating system and application processes running on network processor . RAM may be double data rate DDR3 memory. Boot flash is non volatile memory storing the operating system image for network processor . Boot flash may also store application software to be executed on network processor . CPLD may provide glue logic between network processor and boot flash e.g. because the network processor may be capable of interfacing flash memory directly . CPLD may also provide reset and power sequencing for network processor .

Network processor provides four parallel Ethernet ports e.g. RGMII ports for communicating with other devices via the Ethernet protocol. Ethernet transceiver e.g. MARVELL 88E1145 serializes these four ports to provide interoperability with the multiport management switch . Specifically in some embodiments network processor provides four Reduced Gigabit Media Independent Interface RGMII ports each of which requires twelve pins. The MARVELL 88E1145 transceiver serializes these ports to reduce the pin count to four pins per port.

Routing FPGA B is a configurable logic device configured to route network packets between other devices within the network testing system. Specifically FPGA B is a field programmable gate array and in some embodiments is an ALTERA STRATIX 4 device. FPGAs may also be XILINX VIRTEX ACTEL SMARTFUSION or ACHRONIX SPEEDSTER parts. Routing FPGA B may be coupled to tangible computer readable memory B to provide increased local to the FPGA data storage. In some embodiments memory B is 8 MB of quad data rate QDR static RAM. Static RAM operates at a higher speed than dynamic RAM e.g. as DDR3 memory but has a much lower density.

Offload capture FPGA A is a configurable logic device configured to perform a number of functions as packets are received from external ports or as packets are prepared for transmission on external ports . Specifically FPGA B is a field programmable gate array and in some embodiments is an ALTERA STRATIX 4 device. Offload capture FPGA A may be coupled to tangible computer readable memory A to provide increased local to the FPGA data storage. In some embodiments memory A is two banks of 16 GB of DDR3 RAM. Memory A may be used to store packets as they are received. Offload capture FPGA A may also be coupled e.g. via XAUI or SGMII ports to external interfaces which may be constructed from physical interfaces and transceivers . Physical interfaces convert the XAUI SGMII data format to a gigabit Ethernet signal format. Physical interfaces may be NETLOGIC AEL2006 transceivers. Transceivers convert the gigabit Ethernet signal format into a format suitable for a limited length direct attach connection. Transceivers may be SFP transceivers for copper of fiber optic cabling.

Layer 2 Layer 3 FPGA C is a configurable logic device configured to generate layer 2 or layer 3 egress network traffic. Specifically FPGA B is a field programmable gate array and in some embodiments is an ALTERA STRATIX 4 device.

Management switch is a high speed Ethernet switch capable of cross connecting various devices on a single blade or across blades in the network testing system. Management switch may be coupled to non volatile memory to provide power on configuration information. Management switch may be a 1 Gbps Ethernet switch e.g. FULCRUM INTEL FM4000 or BROADCOM BCM5389. In some embodiments management switch is connected to the following other devices 

Serial port access system provides direct data and or control access to various system components via controller or an external serial port e.g. a physical RS 232 port on the front of the blade. Serial port access system illustrated in detail in and discussed below connects via serial line illustrated in as an S in a circle to each of control processor each network processor external serial port and an I2C backplane signaling system . As discussed below with respect to I2C backplane signaling system may be provided for managing inter card serial connections and may include a management microcontroller or environmental controller I2C connection to backplane and an I2C IO expander . Serial lines may be multipoint low voltage differential signaling MLVDS .

Each sub system of control system may include or have access to any suitable hardware devices software CLD configuration information and or firmware for providing the respective functions of that sub system as disclosed herein. The hardware devices software CLD configuration information and or firmware of each respective sub system may be embodied in a single device of system or distributed across multiple devices of as appropriate. The software CLD configuration information and or firmware including any relevant algorithms code instructions or other logic of each sub system may be stored in any suitable tangible storage media of system and may and executable by any processing device of system for performing functions associated with that sub system.

CLDs in the present disclosure provide specialized functions but require external control and management. In some embodiments of the present disclosure control CPU provides this external control and management for the various CLDs on a board. Control CPU may program any one of the CLDs on the board e.g. A B C or to configure the logic and memory of that CLD. Control CPU may write instructions and or data to a CLD. For example control CPU may send instructions to traffic generating CLD C to have that device generating a specified number of network messages in a particular format with specified characteristics. In another example control CPU may send instructions to capture offload CLD A to read back latency statistics gathered during a packet capture window.

CLDs are usually managed via a local bus such as a PCI bus. Such an approach does not scale to large numbers of CLDs and does not facilitate connectivity between multiple CLDs and multiple CPUs. Some bus designs also require the payment of licensing fees. The present disclosure provides a CLD management solution based on the exchange of specialized Ethernet packets that can read and write CLD memories i.e. CLD registers .

In some embodiments CLDs in the present disclosure contain embedded Ethernet controllers designed to parse incoming specially formatted packets as command directives for memory access to be executed. In this approach the CLD directly interprets the incoming packets to make the access to internal CLD memory without intervention by an intermediate CPU or microcontroller processing the Ethernet packets. Simultaneous requests from multiple originating packet sources e.g. CPUs are supported through the use of a command FIFO that queues up incoming requests. After each command directive is completed by the CLD a response packet is sent back to the originating source CPU containing the status of the operation.

Three layers of packet definition are used to form the full command directive packet source and destination addressing the Ethernet type field and the register access directive payload. The destination MAC Media Access Controller address of each CLD contains the system mapping scheme for the CLDs while the source MAC contains the identity of the originating CPU. Note that in some embodiments the MAC addresses of each CLD is only used within the network testing system and are never used on any external network link. Sub fields within the destination MAC address 6 bytes total in length identify the CLD type an CLD index and a board slot ID. The CLD type refers to the function performed by that particular CLD within the network testing system i.e. traffic generating CLD or capture offload CLD . A pre defined Ethernet Type field is matched to act as a filter to allow the embedded Ethernet controller ignore unwanted network traffic. These 3 fields within the packet conform to the standard Ethernet fields IEEE 802.3 .

This conformance allows implementation of the network with currently available interface integrated circuits and Ethernet switches. Ethernet also requires fewer I O pins than a bus like PCI therefore freeing up I O capacity on the CLD and reducing the trace routing complexity of the circuit board. Following the MAC addressing and Ethernet type fields a proprietary command format is defined for access directives supported by the CLD. Some embodiments support instructions for CLD register reads and writes bulk sequential register reads and writes and a diagnostic loopback or echo command. Diagnostic loopback or echo commands provide a mechanism for instructing a CLD to emulate a network loopback by swapping the source and destination addresses on a packet and inserting the current timestamp to indicate the time the packet was received.

In certain embodiments a directive packet can contain only one type of directive e.g. read or write but can access a large number of register addresses within a CLD. In some embodiments the packet size is limited to the standard maximum transmission unit of 1 500 bytes. In some embodiments jumbo frames of 9 000 bytes are supported. By packing multiple instructions of the same type into a single directive significant performance enhancement has been observed. In one configuration startup time of a board was reduced from approximately a minute to approximately five seconds by configuring CLDs over Ethernet instead of over a PCI bus.

In some embodiments access directives may be used to access the entire memory space accessible to a CLD. Some CLDs have a flat memory space where a range of addresses corresponds to CLD configuration data another range of addresses corresponds to internal CLD working memory and yet another range of addresses corresponds to external memory connected to the CLD such as quad data rate static random access memory QDR or double data rate synchronous dynamic access memory DDR .

Ethernet switch operates as a layer 2 router with multiple ports. Each port is connected to a device as discussed in the previous paragraph or another switch e.g. through the backplane connection . Ethernet switch maintains a memory associating each port with a list of one or more MAC addresses of the device or devices connected to that port. Ethernet switch may be implemented as a store and forward device receiving at least part of an incoming Ethernet packet before making a routing decision. The switch examines the destination MAC address and compares that destination MAC address with entries in the switch s routing table. If a match is found the packet will be resent to the assigned port. If a match is not found the switch may broadcast the packet to all ports. Upon receipt of a packet the switch will also examine the source MAC address and compare that address to the switch s routing table. If the routing table does not have an entry for the source MAC address the switch will create an entry associating the source MAC address with the port on which the packet arrived. In some embodiments the switch may populates its routing table by sending a broadcast message i.e. one with a destination address of FF FF FF FF FF FF to trigger responses from each connected device. In other embodiments each device may include an initialization step of sending an Ethernet message through the switch to announce the device s availability on the system.

Because Ethernet is a simple stateless protocol additional logic is useful to ensure receipt and proper handling of messages. In some embodiments each sending device incorporates a state machine to watch for a response or recognize when a response was not received within a predefined window of time i.e. a timeout . A response indicating a failure or timeout situation is often reported in a system log. In some situations a failure or timeout will cause the state machine to resend the original message i.e. retry . In certain embodiments each process running on control processor needing to send instructions to other devices via Ethernet may use a shared library to open a raw socket for sending instructions and receiving responses. Multiplexing across multiple processes may be implemented by repurposing the sequence number field and setting that field to the process identifier of the requesting process. The shared library routines may include filtering mechanisms to ensure delivery of responses based on this process identifier which may be echoed back by the CLD or network processor when responding to the request .

In certain embodiments controller software includes a software module called an CLD server. The CLD server provides a centralized mechanism for tracking failures and timeouts of Ethernet commands. The CLD server may be implemented as an operating system level driver that implements a raw socket. This raw socket is configured as a handler for Ethernet packets of the type created to implement the CLD control protocol. All other Ethernet packets left for handling by the controller s networking stack or other raw sockets.

At step the network processor sends the directive packet to control CPU via switch . The directive packet is received by the CLD server through a raw port on the network driver of the control server. The CLD server creates a record of the directive packet and includes in that record the current time and at least the source MAC address and the sequence number of the directive packet. The CLD server modifies the directive packet as follows. The source MAC address is set to the MAC address of control CPU and the destination MAC address is set to the MAC address of traffic generating CLD C. In some embodiments the CLD server replaces the sequence number with its own current sequence number. In some embodiments the CLD server may keep a copy of the entire modified directive packet to allow later retransmission.

At step the CLD server transmits the modified directive packet via switch to traffic generating CLD C for execution.

At a regular interval the CLD server examines its records of previously sent directives to and determines whether any are older than a predetermined age threshold. This might indicate that a response from the destination CLD is unlikely due to an error in transmission or execution of the directive. If any directives are older than the threshold then a timeout is recognized at step .

In the case of a timeout the CLD server generates an error message at step to send to the requesting network processor. In some embodiments CLD server may resend the directive one or more times before giving up and reporting an error. The CLD server also deletes the record of the directive packet at this time.

If a response is received prior to a timeout CLD server removes the directive packet record and forwards the CLD response packet to the originating network processor at step . To forward the CLD response packet the CLD server replaces the destination MAC address with the MAC address of the originating network processor. If the sequence number was replaced by the CLD server in step the original sequence number may be restored. Finally the modified response packet is transmitted via switch to the originating network processor.

While the present disclosure describes the use of Ethernet other networking technologies could be substituted. For example a copper distributed data interface CDDI ring or concentrator could be used.

In a typical IEEE 802 network each network endpoint is assigned a unique MAC Media Access Control address. Normally the assigned MAC address is permanent because it is used in layer 2 communications such as Ethernet and unique addressing is a requirement.

As discussed above network testing system may utilize a configuration in which multiple Ethernet configured devices internally communicate with each other over an internal Ethernet interface. In some embodiments system comprises a chassis with multiple slots and each containing a blade with multiple Ethernet devices e.g. CLDs network processors control processor etc.

In some embodiments the control CPU of each blade is the only component of system with connectivity to external networks and is thus the public external Ethernet interface of control CPU is only component of system that is assigned a globally unique public MAC address. Hardware and software of system dynamically assigns each other Ethernet device in system including each network processor each CLD and local internal Ethernet interfaces of control CPU a MAC address that is unique within system but need not be globally unique as the internal Ethernet network of system does not connect with external networks. In some embodiments each of such Ethernet devices is dynamically assigned a unique MAC address based on a set of characteristics regarding that device and its location within the configuration of system . For example in some embodiments each network processor and CLD in system automatically derives a 6 byte MAC address for itself that has the following format 

Each CLD e.g. FPGA derives its own MAC address by reading some strapping IO pins on initialization. For example a four CLD system may have two pins that encode a binary number between 0 and 3. Strapping resistors are connected to these pins for each CLD and the CLD reads the value to derive its MAC address. This technique allows system controller to determine all of the encoded information based on the initial ARP Address Resolution Protocol request received from an Ethernet device on the internal Ethernet network. This flexibility allows new blades to be defined that are compatible with existing devices without causing backwards compatibility problems. For example if a new blade is designed that is compatible with an old blade the model number stays the same. If the new blade adds a new CLD to system then the new CLD is simply assigned a different CLD number for the MAC addressing. However if a new blade is installed in system that requires additional functionality on the system controller the new blade may be assigned a new model number. Compatibility with existing blades can thus be preserved.

In addition the dynamically assigned MAC addresses of Ethernet devices may be used by a DHCP server for booting such devices as discussed below in detail.

Each processor may also have an IP address which may be assigned by the DHCP server based on the MAC address of that device and a set of IP address assignment rules.

As discussed above system may be housed in a chassis that interconnects multiple cards via a backplane . In some embodiments all cards boot a single software image. In other embodiments each card runs a different software image possibly with different revisions in the same chassis .

One challenge results from the fact that the cards in chassis are physically connected to each other via Ethernet over the backplane . In addition some processors in system may obtain their operating system image from other processors across the shared Ethernet using DHCP. DHCP is a broadcast protocol such that a request from any processor on any card can be seen from any other card . Thus without an effective measure to prevent it any processor can boot from any other processor that replies to its DHCP request quickly enough including processors on other cards from the requesting processor. This may be problematic in certain embodiments e.g. embodiments that support hot swapping of cards . For example if a CPU on card boots from a CPU on card and card is subsequently removed from chassis CPU may crash.

Thus in some embodiments e.g. embodiments that support hot swapping of cards to utilize multiple control processors and drives available in a multi card system as well as to allow for each control processor to run an independent operating system while maintaining Ethernet connectivity to the backplane system may be configured such that local network processors boot from the local control processor using DHCP NFS Network File System and TFTP Trivial File Transfer Protocol . This task is divided by a special dynamic configuration for the DHCP server.

First the network processors and control processor on a card determine what physical slot the card is plugged into. The slot number is encoded into the MAC address of local network processors . The MAC address of each network processor is thus dynamic but of a predictable format. The DHCP server on the control processor configures itself to listen only for requests from network processors and other devices with the proper slot number encoded in their MAC addresses. Thus DHCP servers on multiple cards listen for request on the shared Ethernet but will only reply to a subset of the possible MAC addresses that are present in system . Thus system may be configured such that only one DHCP server responds to a DHCP request from any network processor . Each network processor is thus essentially assigned to exactly one DHCP server the local DHCP server. With this arrangement each network processor always boots from a processor on the same card as that network processor i.e. a local processor . In other embodiments one or more network processor may be assigned to the DHCP server on another card such that network processors may boot from a processor on another card.

A more detailed example of a method of addressing and booting devices in system is discussed below with reference to . As discussed above in a typical Ethernet based network each device has a globally unique MAC address. In some embodiments of network testing system the control CPU is the only component of system with connectivity to external networks and is thus the only component of system that is assigned a globally unique MAC address. For example a globally unique MAC address for control CPU may be hard coded into a SPI 4.2 EEPROM see .

Thus network processors and CLDs may generate their own MAC addresses according to a suitable algorithm. The MAC address for each device and on a particular card may identify the chassis slot in which that card is located as well as other identifying information. In some embodiments management switch has no CPU and runs semi independently. In particular management switch may have no assigned MAC address and may rely on control CPU for intelligence.

In some embodiments network testing system is configured such that cards can boot and operate independently if desired and be hot swapped without affecting other the operation of the other cards without the need for additional redundant hardware. Simultaneously cards can also communicate with each across the backplane . Such architecture may improve the scalability and reliability of the system e.g. in high slot count systems. Further the Ethernet based architecture of some embodiments may simplify card layout and or reduce costs.

Cards may be configured to boot up in any suitable manner. illustrate an example boot up process and architecture for a card of system according to an example embodiment. In particular illustrates an example DHCP based boot management system including various components of system involved in a boot up process illustrates an example boot up process for a card and illustrates an example method for generating a configuration file during the boot up process shown in according to an example embodiment.

Referring to a DHCP based boot management system may include control CPU connected to a solid state disk drive storing a DHCP server a software driver a configuration script configured to generate configuration files an operating system a Trivial File Transfer Protocol server TFTP server a Network Time Protocol NTP or Simple Network Time Protocol SNTP server and a Network File System NFS server . Configuration script may communicate with external hardware via software driver and a hardware interface e.g. JTAG . Controller may include management processor controller software a bootflash and an EEPROM .

As discussed below configuration script may be configured to run DHCP server and to automatically and dynamically write new configuration files based on the current configuration of system including automatically generating a list of MAC addresses or potential MAC addresses for various devices for which communications may be monitored. Configuration script may communicate with system hardware via software driver API to determine the physical slot in which the card is located. Configuration file generated by configuration script may include a list of possible valid MAC addresses that may be self generated by network processors as discussed below or other offload processors such that DHCP server can monitor for communications from network processors on the same card . In some embodiments configuration file may also list possible valid MAC addresses for particular devices unable to boot themselves or particular devices on a card located in a particular slot e.g. slot . Thus by automatically generating a configuration file including a list of relevant MAC addresses configuration script may eliminate the need to manually compile a configuration file or MAC address list.

In general control CPU boots itself first then boots management server then loads DHCP server and TFTP server NTP server and NFS server stored on disk . After the control CPU finishes loading its servers each network processor loads itself and obtains address and other information via a DHCP request and response. A more detailed description is provided below.

At step the board is powered. At step management switch reads an EEPROM connected to management switch activates local connections between controller network processors and CLDs etc. on card and deactivates backplane connections such that all local processors and and CLDs are connected.

In some embodiments board disables signaling to the backplane by deactivating backplane connections and keeps such connections deactivated unless and until board determines a need to communicated with another board in system . Enabling an Ethernet transceiver when there is no receiver on the other side on the backplane causes extra electromagnetic radiation emissions which may run counter FCC regulations. Thus disabling backplane signaling may reduce unwanted electromagnetic radiation emissions which may place or keep system within compliance for certain regulatory standards.

In addition in one embodiment each management switch can potentially connect to three other switches on the backplane in other embodiments management switch may connect to more other switches . The switch may also provide a function called loop detection that is implemented via a protocol known as spanning tree. Loops are typically undesirable in Ethernet systems because a packet may get caught in the loop causing a broadcast storm condition. In certain embodiments the backplane architecture of system is such that if every switch comes with its backplane connections enabled and all boards are populated in the system the switches may detect a loop configuration and randomly disable ports depending on which port was deemed to be looped first by system . This may cause boards to become randomly isolated from each other on the backplane . Thus by first disabling all backplane connections and then carefully only enabling the connections in a manner that prevents a loop condition from occurring the possibility of randomly isolating boards from each other may be reduced or eliminated. In other embodiments this potential program is addressed by using a different backplane design e.g. by using a star configuration as opposed to a mesh configuration such that the backplane connections may remain enabled.

At step system controller reads bootflash and loads its operating system from attached disk drive . At step each network processor reads local bootflash and begins a process of obtaining an operating system from attached disk drive via DHCP server by requesting an IP address from DHCP server as discussed below. Each network processor can complete the process of loading an operating system from disk drive after receiving a DHCP response from DHCP server which includes needed information for loading the operating system as discussed below. In some embodiments disk drive stores different operating systems for controller and network processors . Thus each processor controller and individual network processors may retrieve the correct operating system for that processor via DHCP server .

Bootflash and may contain minimal code sufficient to load the rest of the relevant operating system from drive . Each network processor on a card automatically derives a MAC address for itself and requests an IP address by sending out a series of DHCP requests that include the MAC address of that network processor . As discussed above the MAC address derived by each network processor may indicate . . . . To derive the slot identifying MAC address for each network processor instructions in bootflash may interrogate a Complex Programmable Logic Device CPLD to determine which slot the card is located in which may then be incorporated in the MAC address for the network processor . Steps and may occur fully or partially simultaneously.

At step system controller software programs local microcontrollers so it can query system status via USB. At step system controller queries hardware slot information to determine which slot the card is located. At step system controller configures management switch to activate backplane connections . Because slots are connected in a mesh fashion by backplane the backplane connections may be carefully configured to avoid switch loops. For example in an example 3 slot embodiment in slot both backplane connections are activated in slot only one backplane connection is activated and in slot the other backplane connection is activated.

At step system controller software starts internal NFS server TFTP server and NTP server services. At step system controller software queries hardware status generates a custom configuration file for the DHCP server and starts DHCP server . After DHCP server is started each network processor receives a response from DHCP server of the local system controller at step in response to the DHCP requests initiated by that network processor at step . The DHCP response to each network processor may include NFS NTP TFTP and IP address information and identify which operating system to load from drive e.g. by including the path to the correct operating system kernel and filesystem that the respective network processor should load and run .

At step each network processor configures its network interface with the supplied network address information. At step each network processor downloads the relevant OS kernel from drive into its own memory using TFTP server mounts filesystem via NFS server and synchronizes its time with the clock of the local system controller via NTP server .

In one embodiment the NTP time server is modified to lie to the network processors . Network processors have no realtime clock i.e. they always start up with a fixed date . With the NTP protocol before an NTP server will give the correct time to a remote client it must be reasonably sure that its own time is accurate determined via stratum designation. This normally takes several minutes which introduces an undesirable delay e.g. the network processor would need to delay boot . Thus the NTP server immediately advertises itself as a stratum server to fool the NTP client on the network processors to immediately synchronize.

The generalized architecture characteristics of the embodiments of the present disclosure enable allows flexible internal routing of received network messages. However some applications may require routing rules to direct traffic matching certain criteria to a specific network processor. For example in certain embodiments applications or situations when a particular network processor sends a network message to a device under test it is advantageous that the responsive network message is routed back to the originating network processor and in particular to the same core of the originating network processor e.g. to maintain thread affinity. As another example in some embodiments applications or situations all network traffic received on a particular virtual local area network VLAN should be routed to the same network processor.

These solutions differ from conventional Internet Protocol IP routing approaches which utilize a table of prefix based rules In conventional IP routers each rule includes an IP address four bytes in IPv4 and a mask indicating which bits of the IP address should be considered when applying the rule. The IP router searches the list of rules for each received packet and applies the rule with the longest prefix match. This approach works well for IP routing because rules often apply to subnetworks defined by a specific number of most significant bits in an IP address. For example consider a router with the following two rule prefixes 

A packet arriving with a destination address of 128.2.1.237 would match both rules but rule a would be applied because it matches more bits of the prefix.

The conventional rule based approach does not work well for representing rules with ranges. For example a rule applying to IP addresses from 128.2.1.2 to 128.2.1.6 would require five separate entries in a traditional routing table including the entries 128.2.1.2 128.2.1.3 128.2.1.4 128.2.1.5 and 128.2.1.6 each with a mask of 255.255.255.255 .

For certain testing applications system needs to bind ranges of IP addresses to a particular processor e.g. a particular network processor or a particular control CPU . For example in a network simulation each processor may simulate an arbitrary set of hosts on a system. In certain embodiments each packet received must arrive at the assigned processor so that the assigned processor can determine whether responses were out of sequence incomplete or delayed. To achieve this goal routing CLDs A may implement a routing protocol optimized for range matching.

At step capture offload CLD A determines whether to capture the packet in capture buffer A based on capture logic . Prior to the start of the present method controller may instruct capture logic to enable or disable packet capture e.g. for all incoming packets or selected incoming packets e.g. based on specified filters applied to packet header information . Thus at step capture offload CLD A may determine whether to capture the incoming packet i.e. store a copy of the packet including prepend header in capture buffer A based on the current capture enable disable setting specified by capture logic and or header information of the incoming packet. In one embodiment prepend module may include a capture flag in the prepend header at step that indicates e.g. based on capture logic and or header information of the incoming packet whether or not to capture the packet Thus in such embodiment step may simply involve checking for such capture flag in the prepend header.

Based on the decision at step the packet may be copied and stored in capture buffer A as indicated at step . The method may then proceed to the process for routing the packet to a network processor . Processing and routing system may provide both static or basic routing and dynamic routing of packets from ports to network processors . At step system may determine whether to route the packet according to a static routing protocol or a dynamic routing protocol. Routing management module running on control processor may be configured to send instructions to configuration register on CLDs A to select between static and dynamic routing as desired e.g. manually based on user input or automatically by controller . Such selection may apply to all incoming packets or to selected incoming packets e.g. based on specified filters applied to packet header information .

If static or basic routing is determined at step the packet may be forwarded to routing CLD B at which static routing module may apply a static routing algorithm at step to determine a particular destination processor and physical interface e.g. a particular SPI 4.2 port bus and or a particular XAUI port for forwarding the packet to the destination processor . An example static packet routing algorithm is discussed below.

Alternatively if dynamic routing is determined at step the packet may be forwarded to routing CLD B at which dynamic routing module may apply a dynamic routing process at steps through to dynamically route the packet to the proper network processor the proper core within that network processor the proper thread group within that core and the proper thread within that thread group e.g. to route the packet to the thread assigned to the conversation in which that packet is involved based on header information of the packet as well as providing load balancing across multiple physical interfaces e.g. multiple SPI4 interfaces connected to the target network processor .

At step dynamic routing module may determine the proper destination network processor and CPU core of that processor based on dynamic routing algorithms. At step dynamic routing module may determine a thread ID associated with the packet being routed. At step dynamic routing module may determine select a physical interface e.g. a particular SPI4 interface over which to route the packet to the destination network processor e.g. to provide load balancing across multiple physical interfaces. Each of these steps of the dynamic routing process and is discussed below in greater detail. It should also be noted that one or more of these aspects of the dynamic routing process may be incorporated into the static routing process depending on the particular embodiment and or operational situation. For example in some embodiments static routing may incorporate the thread ID determination of step in order to route the packet to a particular thread corresponding to that packet.

Once the static or dynamic routing determinations are made as discussed above routing CLD B may then route packet to the determined network processor over the determined routing path e.g. physical interface s at step . At step the network processor receives the packet and places the packet in the proper thread queue based on the thread ID determined at step . At step the network processor may then process the packet as desired e.g. using any application level processing. Various aspects of the routing method are now discussed in further detail.

In some embodiments once the key has been obtained for an ingress packet routing engine may prepend a destination specific header to the packet. Likewise every packet generated by control processor or network processor for transmission by interface includes a prepend header that will be stripped off by capture offload CLD A prior to final transmission. These prepend headers may be used to route this traffic internally n system .

The prepend header added by capture offload CLD A to ingress packets arriving at interface for delivery to a network processor may contain the following information according to certain embodiments of the present disclosure 

The np extport ingress hdr structure defines the prepend fields set on all packets arriving from an external port to be processed by a network processor according to certain embodiments of the present disclosure. The timestamp field may be set to the time of receipt by the capture offload CLD A receiving the packet from interface . This timestamp may be used to determine all necessary and useful statistics relating to timing as it stops the clock prior to any internal routing or transmission delays between components within the network testing system. The physical interface field which may be set by routing engine contains information sufficient to uniquely identify the physical port on which the packet was originally received. The thread id field contains information sufficient to uniquely identify the software thread on the network processor that will process this incoming packet.

As described elsewhere in this specification maintaining ordering and assigning packets to thread groups ensures that the testing application has complete visibility into all of the packets in a given test scenario. The L3 and L4 offset fields indicate the location within the original packet of the OSI layer three and four packet headers. In some embodiments these offset fields may be is determined by capture offload CLDs A and stored for later use. Header offsets may be time consuming to determine due to the possible presence of variable length option fields and additional embedded protocol layers. Because the header offsets must be determined in order to perform other functions e.g. checksum verification described below this information may efficiently be stored in the prepend header for future reference. For instance parsing VLAN tags can be time consuming because there may be many different values that may be used for VLAN tag identification and because VLAN headers may be stored on unaligned boundaries. However if the capture offload CLD A indicates that the L3 header is at a 14 byte offset this fact may immediately indicate the lack of VLAN tags. In that case routing engine and or network processor may skip VLAN parsing altogether. In another instance if parsing L3 headers IPv4 and IPv6 can be slowed by the presence of option headers which are of variable length. By looking at the L4 header byte offset network processor can immediately determine whether options are present and may skip attempts to parse those options if they are not present.

The flags field indicates additional information about the packet as received. In some embodiments flags may indicate whether the certain checksum values were correct indicating that the data was likely transferred without corruption. For example flags may indicate whether layer 2 3 or 4 checksums were valid or whether an IPv6 tunnel checksum is valid. The hash field is the hash value determined by capture offload CLDs A and stored for later use.

The prepend header for packets generated by a network processor for transmission via interface may contain the following information according to certain embodiments of the present disclosure 

The np extport egress hdr structure defines the prepend fields set on all packets generated by a network processor to be sent on an external port to be processed by a network processor according to certain embodiments of the present disclosure. The physical interface field contains information sufficient to identify the specific physical interface on which the packet was received. The timestamp word offset field indicates the location within the packet of the timestamp field for efficient access by capture offload CLD A.

The prepend header for packets arriving via interface for delivery to a control processor may contain the following information according to certain embodiments of the present disclosure 

The ethtype field is included in the prepend header and set to 0x800 e.g. the value for Internet Protocol Version 4 or IPv4 for ingress and egress traffic though it ignored by the CLD and network processor hardware software. This type value used to fool the Ethernet interface chipset e.g. the INTEL 82599 Ethernet MAC or other suitable device interfaced with the control processor into believing the traffic is regular IP over Ethernet when the system is actually using the area as a prepend header. Because this is a point to point link and because the devices on each end of the communication channel are operating in a raw mode or promiscuous mode the prepend header may be handled properly on both ends without confusing a traditional networking stack. If the ethtype field were set to any value less than 0x600 the value would be treated as length instead under IEEE Standard 802.3x 1997.

The fields of the ingress prepend header for packets arriving on an external port and transmitted to the control processor are listed in the structure named bps export ingress hdr. The timestamp field is set to the time of receipt by the capture offload CLD A receiving the packet from interface . The intf field specifies the specific interface on which the ingress packet arrived. The L3 and L4 offset fields indicate the location within the original packet of the OSI layer three and four packet headers. The flags field indicates additional information about the packet as received. In some embodiments flags may indicate whether the certain checksum values were correct indicating that the data was likely transferred without corruption. For example flags may indicate whether layer 2 3 or 4 checksums were valid or whether an IPv6 tunnel checksum is valid. The hash field is the hash value determined by capture offload CLDs A and stored for later use. The thread id field contains information sufficient to uniquely identify the software thread on the network processor that will process this incoming packet.

The prepend header for packets generated by a control processor for transmission via interface may contain the following information according to certain embodiments of the present disclosure 

The fields of the engress prepend header for packets generated by a control processor for transmission via an external port are listed in the structure named bps export egress hdr. The field l3 tunnel offset identifies the start of the layer 3 tunneled packet header within the packet. The field tcp mss is a maximum segment size value for use by the TCP segmentation offload processing logic in capture offload CLD A. The intf field specifies the specific interface port that should transmit the packet. The field timestamp word offset specifies the location in the packet where the capture offload CLD A should insert the timestamp just prior to transmitting the packet.

The flags field may be used to trigger optional functionality to be performed by e.g. capture offload CLD A prior to transmission of egress packets. For example flag bits may be used to instruct capture offload CLD A to generate and set checksums for the IP header L4 header e.g. TCP UDP or ICMP and or a tunnel header. In another example a flag bit may instruct capture offload CLD A to insert a timestamp at a location specified by timestamp word offset. In yet another example a flag bit may be used to instruct capture offload CLD A to perform TCP segmentation using the tcp mss value as a maximum segment size.

In some embodiments the prepend header is encapsulated in another Ethernet header so a packet would structure be Ethernet header prepend header real Ethernet header . Such embodiments add an additional 14 bytes per packet in overhead to the communication process versus tricking the MAC using 0x800 as the ethtype value.

Basic packet routing mode statically binds a port to a particular to a destination processor and bus port. In some embodiments configuration register takes on the following meaning in the basic packet routing mode 

Packet processing and routing system may provide dynamic packet routing in any suitable manner. For example with reference to steps of method discussed above dynamic routing module may determine the proper destination network processor and CPU core of that processor based on dynamic routing algorithms determine a thread ID associated with the packet being routed and select a physical interface e.g. a particular SPI4 interface over which to route the packet to the destination network processor e.g. to provide load balancing across multiple physical interfaces.

In certain embodiments dynamic routing module is configured to determine ingress routing based on arbitrary IPv4 and IPv6 destination address ranges and VLAN ranges. Routing module examines each ingress packet and generates a destination processor and a thread group identifier associated with that processor. Thread groups are a logical concept on the network processors that each contain some number of software threads i.e. multi processing execution contexts . The second routing stage calculates a hash value e.g. jhash value based on particular header information in each ingress packet namely the source IP address destination IP address source port and destination port. This hash value is used to determine which thread within the thread group determined by the CAM lookup to route the packet. In some embodiments a predefined selected bit e.g. a bit predetermined in nay suitable manner as the least significant bit LSB of the hash is also used to determine which of multiple physical interfaces on the CPU ie SPI 0 or 1 or XAUI 0 or 1 to route the packet e.g. to provide load balancing across the multiple physical interfaces.

At step dynamic routing module may perform a lookup into the VLAN table indexed by the VLAN identifier extracted from the packet to be routed. At step dynamic routing module may search the exception table for an entry matching the destination IP address of the ingress packet or may fall back on a VLAN or system wide default.

This method may be better understood in the context of certain data structures referenced above. Routing entries are stored in IP address ranges on a per VLAN basis. The CAM is made up of a 4 k 32 VLAN table e.g. one entry per possible VLAN value a 16 k 256 Exception table and a 16 k 32 key table. The VLAN table may indicate the default destination for that VLAN and may contain the location in the exception table that contains IP ranges associated with that VLAN. In some embodiments the VLAN table may include start and end indices into the exception table to allow overlap and sharing of exception table entries between VLAN values. Each of these tables may be setup or modified by routing management module . Additional information about these three routing tables is included as follows according to certain embodiments of the present disclosure.

In certain embodiments address bits 13 2 of the VLAN table are the VLAN identifier. So to configure VLAN 12 h2783 for port 0 you would write to location 0x309E0C. Because entries in the VLAN table have a start address into the Exception Table and a count e.g. bits 21 16 it is possible to have VLAN entries with overlapping rules or one VLAN entry may reference a subset of the exception table referenced by another VLAN entry.

The exception table may contain all of the IP ranges for each VLAN. Any given entry in the exception table can contain 1 IPv6 exception or up to 4 IPv4 exceptions. IPv6 and IPv4 cannot be mixed in a single entry however there is no restriction on mixing IPv6 entries with IPv4 entries.

When a match is found for the destination IP address it falls within a range defined in the exception table the key for that entry is returned. If no match is found for that entry the default key for that VLAN is returned. If there is no match for that VLAN then the default key for the test interface is returned. In some embodiments the format of the key is as follows 

Many network analysis mechanisms require knowledge of the order in which packets arrived at system . However with the significant parallelism present in system e.g. multiple processors and multiple cores per processor a mechanism is needed to ensure packet ordering. One approach employed is a method called flow affinity. Under this method packets for a given network traffic flow should always be received and processed by the same CPU thread. Otherwise packets may be processed out of order as a flow ping pongs between CPU threads reducing performance as well as causing false positive detection of packet loss for network performance mechanisms like TCP fast retransmit. The rudimentary hardware support for flow affinity provided by network processor is simply not sufficiently flexible to account for all the types of traffic processed by system . The present disclosure presents a flexible flow affinity solution through a flow binding algorithm implemented in a CLD e.g. routing CLD B .

At step routing module looks up in Table 5 the number of threads value and starting thread value corresponding to the previously determined e.g. at step thread group and processor identifier for the packet. In some embodiments each processor may have up to 16 thread groups. In other embodiments each processor may have up to 32 thread groups. A thread group may have multiple threads associated with it. The routing management module may configure the thread associations e.g. by modifying Table 5 on routing CLDs B.

At step routing module may calculate the thread identifier based on the following formula thread 4 0 Starting Thread hash value MOD Num threads 

At step routing module may update the packet s prepend header to include the thread identifier for subsequent use by the network processor.

Dynamic routing module may perform a hash function in parallel with or alternatively before or after the CAM lookup and or other aspects of the dynamic routing process. Dynamic routing module may extract header information from each ingress packet and calculates a hash value from such header information using the jhash algorithm. In a particular embodiment dynamic routing module extracts a 12 byte 4 tuple namely source IP destination IP source port and dest port from the IP header and UDP header of each ingress packet and applies a jhash algorithm to calculate a 32 bit jhash value from such 4 tuple. Dynamic routing module may parse and calculate the hash value each packet at line rate in the FPGAs which may thereby free up processor cycles in the network processors . For example dynamic routing module may embed the calculated hash value is into the prepend header of each packet so that the network processors can make use of the hash without having to parse the packet or calculate the hash. Dynamic routing module can then use the embedded jhash values for packet routing and load balancing as discussed herein.

As discussed herein system may utilize the jhash function written by Bob Jenkins see the URL burtleburtle.net bob c lookup3.c for various functions. As shown the jhash function may be implemented by CLDs e.g. FPGAs A and or B of the example embodiment of . For example capture offload FPGAs A or routing FPGA B may apply the jhash function to header information of incoming packets as discussed above which may allow increased throughput through system as compared to an arrangement in which the hash functions are implemented by network processors or other CPUs.

In some embodiments dynamic routing module pre processes the 4 tuple information before applying the hash function such that all communications of a particular two way communication flow in both directions receive the same hash value and are thus routed to the same processor core in order to provide flow affinity. Packets flowing in different directions in the same communication flow will have opposite source port and destination port data which would lead to different hash values and thus potentially different routing destinations for the two sides of a particular conversation. Thus to avoid this result in one embodiment dynamic routing module utilizes a tuple ordering algorithm that orders the four items of the 4 tuple in numerical order or at least orders the source port and destination port before applying the hash algorithm such that the ordered tuple to which the hash function is applied is the same for both sides of the conversation. This technique may be useful for particular applications e.g. in bump in the wire configurations where it is desired to monitor both sides of a conversation e.g. for simulating or testing a firewall .

Further dynamic routing module may use jhash value to determine which of multiple physical interfaces e.g. multiple SPI 4.2 interfaces or multiple XAUI interfaces to route each packet e.g. for load balancing across such physical interfaces. For example a predefined selected bit e.g. a bit predetermined in any suitable manner as the least significant bit LSB of the hash may be used to determine which physical interfaces on the CPU e.g. SPI 4.2 port 0 or 1 or XAUI 0 or 1 to route the packet. In an example embodiment bit of the jhash value was selected to determine the port to route each packet. Thus in an example that includes two SPI interfaces SPI 4.2 ports 0 and 1 between routing CLD B and network processor if hash 10 0 for a particular packet routing CLD B will forward the packet on SPI 4.2 port 0 and if hash 10 1 it will send the packet on SPI 4.2 port 1. Using the hash value in this manner may provide a deterministic mechanism for substantially evenly distributing traffic over two or more parallel interconnections e.g. less than 1 difference in traffic distribution over each of the different interconnections due to the substantially random nature of the predefined selected hash value bit.

The process discussed above may ensure that all packets of the same communication flow and regardless of their direction in the flow are forwarded not only to the same network processor but to that processor via the same physical serial interface e.g. the same SPI 4.2 port which may ensure that packets of the same communication flow are delivered to the network processor in the correct order due to the serial nature of such interface.

While many components provide channelized interconnections such as the SPI4 interconnections on the FPGAs general purpose CPUs often do not. General purpose CPUs are designed to operate more as controllers of specialized devices rather than peers in a network of other processors. illustrate an approach to providing a channelized interconnection between the general purpose CPU of controller and the routing CLDs B shown as FPGAs according to some embodiments of the present disclosure.

In INTEL XEON processor labeled Intel Jasper Forrest is configured as control processor . This processor is a quad core x86 compatible processor with a Peripheral Component Interconnect Express PCIe interconnection to two INTEL 82599 dual channel 10 Gbps Ethernet medium access controllers MACs . Rather than operating as traditional network connections these components are configured to provide channelized data over four 10 Gbps connections. In particular direct connections are provided between one of the INTEL 82599 MACs and the two Routing FPGAs B.

In this configuration the prepend header discussed above is used to signal to the MAC that the packet should be passed along as an IP packet. The control processor has a raw packet driver that automatically adds and strips the prepend header to allow software processing of standard Ethernet packets.

As with the two SPI4 ports on the routing CLDs B ingress traffic to the control processor should be load balanced across the two 10 Gbps Ethernet channels connecting the routing CLDs B and the INTEL 82599. The load balancing may operate in the same manner as that described above in the context of the SPI4 ports based on a hash value. However the routing process is more complicated. An ingress packet arriving at the routing CLD B illustrated in will either be routed through the 10 Gbps Ethernet e.g. XAUI connection directly to the INTEL 82599 MAC or will be routed through the routing CLD B illustrated in e.g. via interconnection . In the latter scenario routing CLD B illustrated in will then route the packet through that CLD s 10 Gbps Ethernet e.g. XAUI connection directly to the INTEL 82599 MAC.

In certain applications the complexity of logic to be offloaded from a processors to a CLD becomes too great to efficiently implement in a single CLD. Internal device congestion prevents the device from processing traffic at line rates. Further as the device utilization increases development time increases much faster than a linear fashion as development tools employ more sophisticated layout techniques and spend more time optimizing. Traditional design approaches suggest solving this problem by selecting a more complex and capable CLD part that will provide excess capacity. Fewer components often reduces overall design and manufacturing costs even if more complex parts are individually more expensive.

In contrast certain embodiments of the present invention take a different approach and span functionality across multiple CLDs in a careful deintegration of functionality. This deintegration is possible with careful separation of functions and through the use of low latency high throughput interconnections between CLDs. In some embodiments a proprietary bus e.g. the ALTERA SERIALLITE bus is used to connect two or more compatible CLD devices to communicate with latencies and throughput approximating that of each device s internal I O channels. This approach is referred to herein as pipelining of CLD functionality. Pipelining enables independent design and development of each module and the increased availability of I O pins at the cost of additional processing latency. However certain applications are not sensitive to increased latency. Many network testing applications fall into this category where negative effects of processing latency can be effectively neutralized by time stamping packets as they arrive.

In the embodiments illustrated by CLD functionality is distributed across three CLDs. In these embodiments egress network traffic either flows through routing CLD B and capture offload CLD A or through traffic generating CLD C and capture offload CLD A. Likewise ingress network traffic flows through capture offload CLD A and routing CLD B. The functions assigned to each of these devices is described elsewhere in this disclosure.

In certain embodiments each network processor has a theoretical aggregate network connectivity of 22 Gbps. However this connectivity is split between two 11 Gbps SPI4 interfaces e.g. interfaces . The method of distributing traffic across the two interfaces is a critical design consideration as uneven distribution would result in a significant reduction in the achieved aggregate throughput. For example statically assigning a physical network interface e.g. interface to an SPI4 interface may not allow a single network processor to fully saturate a physical interface with generated network traffic. In another example in some applications it is desirable to have a single network processor saturate two physical network interfaces. The user should not need to worry about internal device topologies in configuring such an application. Another core design constraint is the need to maintain packet ordering for many applications.

In some embodiments software on a network processor assigns SPI4 interfaces to processor cores in the network processor such that all egress packets are sent on the assigned SPI4 interface. In some embodiments processor cores with an odd number send data on SPI4 1 while those with an even number send data on SPI4 0. A simple bit mask operation can be used to implement this approach SPI4 Interface CORE ID 0x1. This approach could be scaled to processors with additional SPI4 ports using a modulus function.

In certain embodiments ingress packets are routed through specific SPI4 interfaces based on the output of an appropriate hashing algorithm for example the jhash algorithm described below. In some embodiments the source and destination addresses of the ingress packet are input into the hashing algorithm.

In situations where the hashing algorithm varies based on the order of the input it may be desirable to route packets between the same two hosts to the same interface on the network processor. For example the network testing device may be configured to quietly observe network traffic between two devices in a bump in the line configuration. In this scenario the routing CLD may first numerically sort the source and destination address along with any other values input into the hash function to ensure that the same hash value is generated regardless of which direction the network traffic is flowing.

In certain embodiments offload capture CLDs A are configured to capture and store packets received on interfaces in capture memory A. Packets may be captured to keep a verbatim record of all communications for later analysis or direct retrieval. Captured packets may be recorded in a standard format e.g. the PCAP format or with sufficient information to enable later export to a standard format.

With modern data rates on the order of 10 Gbps packet capture may consume a significant amount of memory in a very short window of time. In certain embodiments the packet capture facility of offload capture CLDs A may be configurable to conserve memory and focus resources. In some embodiments the packet capture facility may capture a limited window of all received packets e.g. through the use of a circular capture memory described below. In certain embodiments the packet capture facility may incorporate triggers to start and stop the capture process based on certain data characteristics of the received packets.

In some embodiments offload capture CLDs A verify one or more checksum values on each ingress packet. The result of that verification may be used to set one or more flags in the prepend header as discussed elsewhere in this disclosure. Examples of checksums include the layer 2 Ethernet checksum layer 3 IP checksum layer 4 TCP checksum and IPv6 tunneling checksum. Erroneous packets may be captured in order to isolate and diagnose the source of erroneous traffic.

In some embodiments offload capture CLDs A may apply a set of rules against each ingress packet. For example packet sizes may be monitored to look for abnormal distributions of large or small packets. An abnormal number of minimum size or maximum size packets may signal erroneous data or a denial of service attack. Packet types may be monitored to look for abnormal distributions of layer 4 traffic. For example a high percentage of TCP connection setup traffic may indicate a possible denial of service attack. In another example a particular packet type e.g an address resolution protocol packet or a TCP connection setup packet may trigger packet capture in order to analyze and or record logical events.

In some embodiments offload capture CLDs A may include a state machine to enable capture of a set of packets based on a event trigger. This state machine may begin capturing packets when triggered by one or more rules described above. The state machine may discontinue capturing packets after capturing a threshold number of packets at the end of a threshold window of elapsed time and or at when triggered by a rule. In some embodiments offload capture e.g. by adding fields in the packet header CLDs A may capture all ingress traffic into a circular buffer and rules may be used to flag captured packets for later retrieval and analysis. In certain embodiments a triggering event may cause the state machine to walk back through the capture buffer to retrieve a specified number or time window of captured packets to allow later analysis of the events leading up to the triggering event.

In certain embodiments offload capture CLDs A may keep a record of triggering events external to the packet capture data for later use in navigating the packet capture data. This external data may operate as an index into the packet capture data e.g. with pointers into that data .

Existing packet capture devices typically set aside a fixed number of bytes for each packet 16 KB for example . This is very inefficient if the majority of the packets are 64 B since most of the memory is left unfilled. The present disclosure is of a more efficient design in which each packet is stored in specific form of a linked list. Each packet will only use the amount of memory required and the link will point to the next memory address whereby memory is packed with network data and no memory wasted. This allows the storage of more packets with the same amount of memory.

In some embodiments capture offload CLDs A implement a circular capture buffer capable of capturing ingress egress packets storing each in memory A. Some embodiments are capable of capturing ingress and or egress packets at line rate. In some embodiments memory A is subdivided into individual banks and each bank is assigned to an external network interface . In certain embodiments network interface ports are configured to operate at 10 Gbps and each port is assigned to a DDR2 memory interface. In certain embodiments network interface ports are configured to operate at 1 Gbps and two ports are assigned to each DDR2 memory interface. In these embodiments the memory may be subdivided into two ranges exclusive to each port.

Below is a more detailed description of the data format of packet data in memory A according to certain embodiments of the present disclosure. Packet data is written to memory A with a prepend header. The data layout for the first 32 Bytes of a packet captured in memory A may contain 16 bytes of prepended header information and 16 bytes of packet data. Subsequent 32 byte blocks are written in a continuous manner wrapping to address 0x0 if necessary until the entire packet has been captured. The last 32 byte block may be padded if the packet length minus 16 bytes in the first block is not an integer multiple of 32 bytes 

In certain embodiments the following algorithm describes the process of capturing packet data. As a packet arrives at offload capture CLD A via internal interface decisional logic A determines whether or not to capture the packet in memory A. This decision may be based on a number of factors as discussed elsewhere in this disclosure. For example packet capture could be manually enabled for a specific window of time or could be triggered by the occurrence of an event e.g. a packet with an erroneous checksum value . In some embodiments packet capture is enabled by setting a specific bit in the memory of CLD A. If the packet is to be captured the packet is stored locally in egress FIFO A. A similar process applies to packets arriving at external interface though decisional logic B will store captured ingress packets in ingress FIFO B. In each case other processing may occur after the packet arrives and before the packet is copied into the FIFO memory. Specifically information may be added to the packet e.g. in a prepend header such as an arrival timestamp and flags indicating the validity of one or more checksum values.

Buffer logic moves packets from FIFOs A and B to memory A. Buffer logic prioritizes the deepest FIFO to avoid a FIFO overflow. To illustrate the operation of buffer logic consider the operation when packet capture is first enabled. In this initial state both FIFOs are empty tail pointer is set to address 0x0 and memory A has uniform value of 0x0. In embodiments where memory A may have an initial value other than zero capture offload CLD A may store additional information indicating an empty circular buffer. Assume that packet capture is enabled.

At this time an ingress packet arrives at external interface and is associated with an arrival timestamp and flags indicating checksum success. Ingress decisional logic B creates the packet capture prepend header the first 16 bytes of data described above copies the packet with its prepend header into FIFO B. Next buffer logic copies the packet to the location 0x0 as this is the first packet stored in the buffer. In certain embodiments memory A is DDR2 RAM which has an effective minimum transfer unit of 256 bits or 32 Bytes. In these embodiments the packet is copied in 32 Byte units and the last unit may be padded.

When another ingress packet arrives at external interface ingress decisional logic B follows the same steps and copies the packet with its prepend header into FIFO B. Next buffer logic determines that tail pointer points to a valid packet record. The value of tail pointer is copied into the prepend header of the current packet e.g. at Data 91 64 and tail pointer is set to the address of the first empty block of memory B and buffer logic copies the current packet to memory A starting at the address specified by tail pointer .

In certain embodiments ingress packets are linked separately from egress packets as separate threads in the circular buffer. In these embodiments at least one additional pointer will be maintained in CLD A in addition to tail pointer to allow buffer logic to maintain linkage for both threads. In particular if the buffer is not empty tail pointer points to a packet of a particular thread type e.g. ingress or egress . If a new packet to be stored of the same thread type the tail pointer may be used to set the previous packet pointer in the new packet to be stored. If the new packet to be stored is of a different thread type buffer logic will reference a stored pointer to the last packet of the different thread type to set the previous packet pointer value on the new packet to be stored but will still store the new packet after the packet identified by tail pointer .

In some embodiments capture offload CLD A may have three logic layers of trigger programming. The first layer may allow up to five combinatorial inverted or non inverted inputs of any combination of VLAN ID source destination IP address and source destination port address to a single logic gate. All bits may be maskable in each of the five fields to allow triggering on address ranges.

The first level may have four logic gates. Each of the four logic gates may be individually programmed to be a OR NOR or AND gate. The IP addresses may be programmed to trigger on either IPV4 or IPV6 packets. The second level may have two gates and allow the combination of non inverted inputs from the four first layer gates. These two second level gates may be individually programmed for an OR NOR or AND gate. The third level logic may be a single gate that allows the combination of non inverted inputs from the four first layer gates and the two second level gates. This third level may be programmed for OR NOR or AND gate logic.

The logic may also allow for triggering on frame check sequence FCS errors IP checksum errors and UDP TCP checksum errors.

In some embodiments CLD A may include rewind logic e.g. as part of buffer logic to generate a forward linked list in the process of generating a properly formatted PCAP file. This rewind logic is preferably implemented in CLD A due to its direct connection to memory A. The rewind logic when triggered may perform an algorithm such as the following written in pseudo code 

The rewind logic walks backward through the list starting at the tail and changes each packet s previous pointer to be a next pointer thus creating a forward linked list. Once completed the variable cur points to the head of a forward linked list that may be copied to drive for persistent storage. Because the address 0x0 is a valid address there is no value in checking for NULL pointers. Instead buffer logic should be careful to not copy any entries after the last entry identified by tail pointer .

In some embodiments network testing system may provide data loopback functionality e.g. to isolate connectivity issues when configuring test environments. illustrates two loopback scenarios. Scenario provides a general illustration of an internal loopback implemented within a networking device that retains all networking traffic internal to that device. In conventional systems loopback may be provided by connecting a physical networking cable between two ports of the same device in order to route data exiting the device back into the device rather than sending the data to an external network or device. In such a configuration all data sent by one port of the device is immediately subject to speed of light delay delivered to the other port and back into the device. In system internal loopback functionality may be provided by a virtual wire loopback technique in which data originating from system is looped back into the system without exiting system without the need for physical cabling between ports. Such technique is referred to herein as virtual wire loopback. 

Scenario provides a general illustration of an external loopback implemented outside a device e.g. to isolate that device from network traffic. In this arrangement data from an external source is looped back toward the external source or another external target without entering the device. In some embodiments system may implement such external loopback functionality in addition to virtual wire internal loopback and or physical wire internal loopback functionality discussed above.

In particular embodiments system provides internal loopback virtual wire and or physical wire loopback and external loopback functionality in combination with packet capture functionality in a flexible configuration manner to enable analysis of internal or external traffic for comparison analysis and troubleshooting e.g. for latency analysis timestamp zeroing etc. .

Arrangement illustrates an external loopback with capture buffer enabled. In this arrangement the network testing system becomes a transparent packet sniffer. All traffic can be captured as shown the in line capture device. Diagnostic pings or traffic can be sent from the external network equipment to validate the network testing system. Network traffic may be captured and analyzed prior to an actual test run before the network testing system is placed in line. By providing the capability to move the capture interface point to both internal and external loopback paths and capture traffic of both configurations in the same manner system configuration and debug are simplified.

In some embodiments network testing system may include a loopback and capture system configured to provide virtual wire internal loopback and may also allow physical wire internal loopback and external loopback in combination with data capture functionality. illustrates aspects an example loopback and capture system relevant to one of the network processors in system according on one embodiment. Components of the example embodiment shown in correspond to the example embodiments of system shown in . illustrates example data packet routing and or capture for virtual wire internal loopback and external loopback scenarios provided by loopback and capture system as discussed below.

As shown in system may include a capture offload FPGA coupled to a pair of test interfaces A and B a capture buffer A a network processor via a routing FPGA and a traffic generation FPGA . Control processor is coupled to network processor and has access to disk drive . A loopback management module having software or other logic for providing certain functionality of system may be stored in disk drive and loopback logic and capture logic configured to implement instructions from loopback management module may be provided in FPGA

Loopback management module may be configured to send control signals to capture offload FPGA to control loopback logic to enable disable an internal loopback mode and to enable disable an external loopback mode and to capture logic to enable disable data capture in buffer A. Such instructions from loopback management module may be generated automatically e.g. by control processor and or manually from a user e.g. via a user interface of system . Thus a user e.g. a developer may control system to place system or at least a relevant card in an internal loopback mode an external loopback mode or a normal mode i.e. no loopback as desired for various purposes e.g. to execute a simulated test scenario analyze system latency calibrate a timestamp function etc.

Thus loopback logic may be configured to control the routing of data entering capture offload FPGA to enable disable the desired loopback arrangement. For example with virtual wire internal loopback mode enabled loopback logic may receive outbound data from network processor and reroute such data back to network processor or to other internal components of system while capture logic may store a copy of the data in capture buffer A if data capture is enabled. The data routing for such virtual wire internal loopback is indicated in the upper portion of . As another example loopback logic may enable virtual wire internal loopback mode to provide loopback of data generated by traffic generation FPGA . For instance loopback logic may be configured in an internal loopback mode to route data from traffic generation FPGA to network processor or to other internal components of system while capture logic may store a copy of the data in capture buffer A if data capture is enabled instead of routing data from traffic generation FPGA out of system through port s . Control processor and or other components of system may subsequently access captured data from buffer A e.g. via the Ethernet management network embodied in switch of system for analysis.

In some embodiments loopback logic may simulate a physical wire internal loopback at least from the perspective of network processor for a virtual wire internal loopback scenario. indicates using a dashed line the connection of a physical cable between test interfaces A and B that may be simulated by such virtual wire internal loopback scenario. For example loopback logic may adjust header information of the looped back data such that the data appears to network processor to have arrived over a different test port than the test port that the data was sent out on. For example if network processor sends out data packets on port 0 loopback logic may adjust header information of the packets such that it appears to network processor that the packets arrived on port 1 as would result in a physical wire loopback arrangement in which a physical wire was connected between port 0 and port 1. Loopback logic may provide such functionality in any suitable manner. In one embodiment loopback logic includes a port lookup table that specifies for each egress port a corresponding ingress port for which network processor may expect data to be looped back through in an internal loopback mode. For example in a four port system port lookup table may specify 

To implement port lookup table with reference to loopback logic reads the egress port number in this example port 0 from the prepend header PH on each data packet P received from network processor determines the corresponding ingress port number port 1 from table and for each packet P inserts a new prepend header PH that includes the determined ingress port number port 1 . Thus when packets P are received at network processor they appear to have returned on port 1 while in reality they do not even reach the ports .

Internal loopback mode virtual or physical cable based may be used for various purposes. For example latency associated with system and or an external system e.g. test system may be analyzed by sending and receiving data using system with internal loopback mode disabled and measuring the associated latency sending and receiving data using system with internal loopback mode enabled and measuring the associated latency and comparing the two measured latencies to determine the extent of the overall latency that is internal to system versus external to system . As another example internal loopback mode virtual or physical cable based may be used to calibrate a timestamp feature of system e.g. to account for inherent internal latency of system . In one embodiment system uses a 10 nanosecond timestamp and system may use internal loopback to calibrate or zero the timestamp timing to 1 10 of a nanosecond. The zeroing process may be used to measure the internal latency and calibrate the process such that the timestamp measures the actual external arrival time rather than the time the packet propagates through to the timestamp logic. This may be implemented for example by enabling the internal loopback mode and packet capture. When an egress packet arrives at capture offload CLD A the packet is time stamped and captured into packet capture buffer . The egress packet is then converted by the internal loopback logic into an ingress packet and time stamped on arrival. The time stamped ingress packet is also stored in packet capture buffer . The difference in time stamps between the egress and ingress packet is the measure of internal round trip latency. This ability to measure internal latency can be especially valuable for configurable logic devices where an image change may alter the internal latency.

As discussed above loopback and capture system may also provide external loopback functionality. That is loopback management module may instruct loopback logic to route data received on one port e.g. port 0 back out over another port e.g. port 1 instead of forwarding such data into system e.g. to network processor etc. as indicated in with respect to packets P. Also as with internal loopback mode in external loopback mode loopback management module may also instruct capture logic to store a copy of data passing through capture offload FPGA in capture buffer A also indicated in . Control processor and or other components of system may subsequently access captured data from buffer A e.g. via the Ethernet management network embodied in switch of system for analysis of such captured data. Thus using external loopback mode system may essentially act as a bump in the wire sniffer for capturing data into a capture buffer.

Standard implementations of hash tables map a single key domain to a value or set of values depending on how collisions are treated. Certain applications benefit from a hash table implementation with multiple co existent key domains. For example when tracking network device statistics some statistics may be collected with visibility only into the IP address of a device while others may be collected with visibility only into the Ethernet address of that device. Another example application is a host identification table that allows location of a host device record by IP address Ethernet address or an internal identification number. A hash table with N key domains is mathematically described as follows 

An additional requirement is needed to ensure the above model represents a single hash table with N key domains instead of simply N hash tables that use the same value range 

Standard hash table implementations organize data internally so that an entry can only be accessed with a single key. Various approaches exist to extend the standard implementation to support multiple key domains. One approach uses indirection and stores a reference to the value in the hash table instead of the actual value. The model becomes this 

In this model R is the set of indirect references to values in V and a lookup operation returns an indirect reference to the actual value which is stored externally to the hash table. This approach has a negative impact on performance and usability. Performance degradation results from the extra memory load and store operations required to access the entry through the indirect reference. Usability becomes a challenge in multithreaded environments because it is difficult to efficiently safeguard the hash table from concurrent access due to the indirect references.

Certain embodiments of the present disclosure support multiple independent key domains avoid indirect references and avoid the negative performance and usability impact associated with other designs that support multiple key domains. According to certain embodiments of the present invention each hash table entry contains a precisely arranged set of links. Each link in the set is a link for a specific key domain. In some embodiments a software macro is used to calculate the distance from each of the N links to the beginning of the containing entry. This allows the table to find the original object much like a memory allocator finds the pointer to the head of a memory chunk. Defining the hash table automatically generates accessors to get the entry from any of N links inside the entry.

Bucket array may be an array of pointers e.g. 32 bit or 64 bit addresses of length hash length. Bucket array entries through are identified as non NULL entries meaning that each contains a valid pointer to a linked list element in memory. Bucket array entry contains a pointer to linked list element . Linked list element contains a pointer e.g. the List1 to the next element in the linked list if any. In the List1 pointer in element points to . The List1 pointer in element is NULL indicating the end of the list linked to bucket array entry

Similar to bucket array entry entry points to linked list element . However in some embodiments of the present disclosure entry points to the List2 pointer in linked list element which then points to element .

Accordingly linked list element may be located in the same hash table using two different key values without the use of indirection and without addition any additional storage overhead. Adding another key to the same hash table merely requires the addition of two field entries in the linked list element data structure the list pointer and key value.

In some embodiments this multikey hash table implementation relies on two or more sets of accessor functions. Each set of accessor functions includes at least an insert function and a lookup function. The lookup function for Key1 operates as illustrated in and the lookup function for Key 2 operates as illustrated in . The insert functions operate in a similar fashion. The insert function for Key1 performs the hash on Key1 of a new element and if empty points the bucket entry to the new element or adds the new element to the end of the linked list. In some embodiments the new element is added to the beginning of the linked list. The insert function for Key2 performs the hash on Key2 of a new element and if empty points the bucket entry to the List2 pointer of the new element or adds the new element to linked list. The insert function for Key2 always points other entries to the List2 field of the new element rather than the start of that element.

In some embodiments all sets of accessor functions use the same hash function. In other embodiments one set of accessor functions uses a different hash function than a second set of accessor functions.

In certain embodiments the accessor functions are generated programmatically using C C style macros. The macros automatically handle the pointer manipulation needed to implement the pointer offsets needed for the second third and additional keys. A programmer need only reference the provided macros to add a new key to the hash table.

The transmission control protocol TCP is a standard internet protocol e.g. first specified in request for comments RFC 675 published by the Internet Engineering Task Force in 1974 . TCP generally aligns with Layer 4 of the Open Systems Interconnection OSI model of network abstraction layers and provides a reliable stateful connection for networking applications. As an abstraction layer TCP allows applications to create and send datagrams that are larger than the maximum transmission unit MTU of the network route between the end points of the TCP connection. Networking systems support TCP by transparently to the application segmenting over sized datagrams at the sending network device and reassembling the segments at the receiving device. When a TCP channel is requested by an application a setup protocol is performed wherein messages are sent between the two end point systems. Intermediate network nodes provide information during this process about the MTU for each link of the initial route that will be used. The smallest reported MTU is often selected to minimize intermediate segmentation.

Many network interface controllers NICs provide automatic segmentation of a large packet into smaller packets using specialized hardware prior to transmission of that data via an external network connection such as an Ethernet connection. The architecture of system differs from typical network devices because network processors share network interfaces and are not directly assigned NICs with specialized segmentation offload hardware. Further network processors do not include built in TCP segmentation offload hardware. In order to efficiently handle TCP traffic the present disclosure provides a CLD based solution that post processes jumbo packets generated by the network processor and splits those packets into multiple smaller packets as specified in the header of a packet.

In certain embodiments of the present disclosure the network processor includes a prepend header to every egress packet. That prepend header passes processing information to offload capture CLD A. Two fields in the prepend header provide instructions for TCP segmentation. The first is a 14 bit field that passes the packet length information in bytes TCPsegLen . TCP lengths can in theory then be any length from a single byte to a max of 16 KB. The second field is a single bit that enables TCP segmentation TCPsegEn for a given a packet.

If the segmentation flag is set segmentation logic determines the segment length e.g by extracting the 14 bit TCPseglen field from the prepend header and extracts the packet s IP and TCP headers at step . Segmentation logic may also determine whether the packet is an IPv4 or IPv6 packet and may verify that the packet is a properly formed TCP packet.

At step segmentation logic generates a new packet the size of the segment length and copies in the original packet s IP and TCP headers. Segmentation logic may keep a segment counter and set a segment sequence number on new packet. Segmentation logic may then fill the data payload of new packet with data from the data payload portion of original packet . Segmentation logic may update the IP and TCP length fields to reflect the segmented packet length and generate IP and TCP checksums.

Once new packet has been generated that packet may be added to a first in first out FIFO queue at step for subsequent transmission via external interface . At step segmentation logic may determine whether any new packets are needed to transmit all of the data from original packet . If not the process stops at step . If so step is repeated. At step if less data remains than can fill the data portion of a packet of length segment length a small packet may be generated rather than padding the remainder.

When TCP packets arrive at system they may arrive as segments of an original larger TCP packet. These segments may arrive in sequence or may arrive out of order. While a CPU is capable of reassembling the segments this activity consumes expensive interrupt processing time. Many operating systems are alerted to the arrival of a new packet when the NIC triggers an interrupt on the CPU. Interrupt handlers often run in a special protected mode on the processor and switching in and out of this protected mode may require expensive context switching processes. Conventional systems offload TCP segment reassembly to the network interface card NIC . However these solutions require shared memory access between the receiving processor and its network interface card NIC . Specifically some commercially available NICs manipulate packet buffers in shared memory mapped between the host CPU and the NIC. System has no memory shared memory between the network processor and routing CLD B. Furthermore a conventional PCI bus and memory architecture does not provide sufficient bandwidth to enable reassembly at the line rates supported by system . In the present disclosure a TCP reassembly engine is provided in a CLD between external interfaces and the destination network processor. This reassembly engine forwards TCP segment jumbograms to the network processor rather than individual segmented packets. The operation of the reassembly engine can reduce the number of packets processed by the NP by a factor of 5 which frees up significant processing time for performing other tasks.

Network processor may control the operation of assembly logic by altering configuration parameters on the reassembly process. In some embodiments network processor may control the number of receive bucket partitions in memory B and or the depth of each receive bucket partition. In certain embodiments network processor may selectively route certain packet flows through or around the assembly engine based on at least one of the subnet VLAN or port range.

If a match is found the packet is added to the matching bucket at step . Receive state machine may insert the new packet into linked list in the appropriate ordered location based on the packet s TCP sequence number. Receive state machine also checks whether this newest packet completes the sequence for this TCP jumbogram at step . If so receive state machine sets the commit bit on the corresponding packet assembly record at step . If the newest packet does not complete the sequence at step receive state machine updates the corresponding packet assembly record and stops.

If no matching packet assembly record was found at step then space permitting receive state machine creates a new record at step and adds the received packet to the newly assigned reassembly bucket list at step .

When a packet is being committed the transmit state machine will set the lock bit on the packet assembly record marking it unavailable. If the packet is complete at step the transmit state machine will assemble at step a TCP jumbogram including the IP and TCP headers of for example the first packet in the sequence after stripping out the TCP segmentation related fields and the concatenated data portions of each segment packet. The transmit state machine at step adds the newly assembled TCP jumbogram to the transmit FIFO and clears the packet assembly record from memory B making it available for use by the receive state machine.

If the packet is not complete but the current packet aged out or was forced out as the oldest packet in memory B then transmit state machine at step may move each packet segment as is to transmit FIFO and clear out the corresponding packet assembly record.

On 64 bit systems pointers typically consume 8 bytes of computer memory e.g. RAM . This is double the amount needed on 32 bit systems and can pose a challenge when migrating from a 32 bit system to a 64 bit system.

Typical solutions to this problem include increasing the amount of available memory and rewriting the software application to reduce the number of pointers used in that application. The first solution listed above is not always possible. For example when shipping software only upgrades to hardware systems already deployed at customer sites. The second solutions can be cost prohibitive and may not reduce memory requirements enough to enable the use of 64 bit pointers.

The system of the present disclosure specially aligns the virtual memory offsets in the operating system so that virtual addresses all fall under the 32 GB mark. This means that for pointers the upper 29 bits are always zero and only the lower 35 bits are needed to address the entire memory space. At the same time the system aligns memory chunks to an 8 byte alignment. This ensures that the lower 3 bits of an address are also zero.

As a result of these two implementation details it is possible to transform a 64 bit pointer to a 32 bit pointer by shifting right 3 bits and discarding the upper 32 bits. To turn the compressed address back to a 64 bit real address one simply shifts the 32 bit address left by 3 bytes and stores in a 64 bit variable. Certain embodiments of the present disclosure may extend this approach to address 64 GB or 128 GB of memory by aligning memory chunks to 16 or 32 byte chunks respectively.

In some embodiments system comprises a task management engine configured to allocate resources to users and processes e.g. tests or tasks running on system and in embodiments that include multiple network processors to distribute such processes among the multiple network processors to provide desired or maximized usage of resources.

Definitions of certain concepts may be helpful for a discussion of task management engine . A user refers to a human user that has invoked or wishes to invoke a test using system . One or more users may run one or more tests serially or in parallel on one or more network processors . A test may be defined as a collection of tasks also called components to be performed by system which tasks may be defined by the user. Thus a user may specify the two tasks FTP simulation and telnet simulation that define an example test FTP and telnet simulation. Some other example tasks may include SMTP simulation SIP simulation Yahoo IM simulation HTTP simulation SSH simulation and Twitter simulation.

Each task e.g. FTP simulation may have a corresponding task configuration that specifies one or more performance parameters for performing the task. An example task configuration may specify two performance parameters 50 000 sessions second and 100 000 simulations. The task configuration for each task may be specified by the requesting user e.g. by selecting values e.g. 50 000 and 10 000 for one or more predefined parameters e.g. sessions second and number of simulations .

Each task requires a fixed amount of resources to complete the task. A resource refers to any limited abstract quantity associated with a network processor which can be given or taken away. Example resources include CPU resources e.g. cores memory resources and network bandwidth. Each network processor has a fixed set of resources.

A port refers to a test interface to the test system being tested. Thus in example embodiments a particular card may have four or eight ports i.e. interface that may be assigned to user s by task management engine for completing various tests.

In a conventional system when test that requires certain resources is started such resources may be available at the beginning of a test but then become unavailable at some point during the test run e.g. due to other tests e.g. from other users being initiated during the test run. This may be particularly common during long running tests. When this situation occurs the test may have to be stopped or paused as the required resources for continuing the test are no longer available. Thus it may be desirable to pre allocate resources for each user so that it can be determined before starting a particular test if the particular test can run to completion without interruption. Thus task management engine may be programmed to allocate resources to users and or processes tests and components thereof i.e. tasks before such processes are initiated to avoid or reduce the likelihood of such processes being interrupted or cancelled due to lack of resources.

In some embodiments task management engine is programmed to allocate resources to users based on a set of rules and algorithms embodied in software and or firmware accessible by task management engine . In a particular embodiment such rules and algorithms specify the following 

3. The resources on a particular board allocated to each user correspond to the number of ports on the board allocated to reserved by that user.

4. If all ports on a board are allocated to reserved by a particular user then all resources of that board allocated to reserved by that user. For example if a user reserves of 8 ports on a board then 25 of all resources of that board are allocated to that user.

In view of these rules task management engine is programmed with the following algorithm for allocating the resources of a board to one or more users.

The algorithm getMaxResourceUtilization computes the amount of each resource r available to a given user. The total amount of any given resource r will be the sum of that resource r across all network processors on the board . Thus the algorithm getMaxResourceUtilization returns an array UR u r where r is a member of K and u is a member of U . Each element of the array represents the amount of the resource available to the user. The algorithm is as follows 

At step task management engine may assign resources of network processors to users based on the number of ports assigned to each user by executing algorithm getMaxResourceUtilization discussed above. As discussed above task management engine may assign the total quantity of each type of network processor resource to users on a pro rata basis based on the number of ports assigned to each user. For example if a user reserves of 4 ports on a board then 75 of each type of resource is assigned to that user.

As discussed above in some embodiments task management engine is further programmed to distribute tasks i.e. components of tests among the multiple network processors of system to provide desired or maximized usage of resources and to determine whether a particular test proposed or requested by a particular user can be added to the currently running tests on system . In particular task management engine may be programmed to distribute tasks based on a set of rules and algorithms embodied in software and or firmware accessible by task management engine . For example such rules and algorithms specify the following 

5. Each task on a board will be assigned a particular network processor based on the resource usage of that task and the resources on that board allocated to the user.

In view of these rules task management engine is programmed with the following algorithm for allocating the resources of a board to one or more users.

The following table defines and provides examples for the variables used in the task distribution algorithm addRunningTest 

The algorithm addRunningTest determines whether to add and if so adds a proposed test to a list of currently running tests on system . The algorithm addRunningTest assumes that the resources used by the running tests do not exceed the total resources available on system . The algorithm first determines all resources consumed by running tests on system . The algorithm then determines whether it is possible to add all of the tasks of the test to one or more network processors without exceeding a any quotas placed on the user e.g. as specified for the user in the user resource allocation array UR u r determined as described above or b the maximum resources available to the relevant network processor s .

If it is impossible to add any task of the proposed test to any network processor based on the conditions discussed above the algorithm determines not to add the test to the set of tasks running on system and notifies the user that the test cannot be run. Otherwise if all tasks of the proposed test can be added to system the test is added to the list of running tests and the tasks are assigned to their specified network processor s as determined by the algorithm.

In one embodiment the user may define the proposed new test by a selecting one or more tasks to be included in the new test e.g. by selecting from a predefined set of task types displayed by engine e.g. FTP simulation telnet simulation SMTP simulation SIP simulation Yahoo IM simulation HTTP simulation SSH simulation and Twitter simulation etc. and b for each selected task specifying one or more performance parameters e.g. by selecting any of the example performance parameter categories listed above Data Rate Unlimited Data Rate Scope Data Rate Unit Data Rate Type Minimum Maximum data rate Ramp Up Behavior etc. and entering or selecting a setting or value for each selected performance parameter category. Thus for a telnet simulation tasks the user may define the performance parameters of 50 000 sessions second and 100 000 simulations.

At step engine may determine the amount of each type of network processor resource r required for achieving the performance parameters defined in the relevant task configuration for each task of the proposed new test indicated as X nt c r in the algorithm above. For example for a particular task of the new test engine may determine that the task requires 20 of the total CPU resources of network processors 25 of the total memory resources of network processors and 5 of the total network bandwidth resources of network processors . Engine may determine the required amount of each type of network processor resource r in any suitable manner e.g. based on empirical test data defining correlations between particular test performance parameters can empirically determined network processor resource quantities used by the relevant network processor s for achieving the particular performance parameters. In some instances engine may interpolate extrapolate or otherwise analyze such empirical test data to determine the network processor resources X nt c r required for achieving the performance parameters of the particular task of the new test. In some embodiments engine may notify the user of the required network processor resources determined at step .

Task management engine may then execute the addRunningTest nt algorithm disclosed above or other suitable algorithm to determine whether the proposed new test can be added to the set of currently running tests on system i.e. whether all tasks of the proposed new test can be added to system . At step engine may determine the current resources available to each user or at least the current resources available to the requesting user and the current resources available to each network processor e.g. by executing algorithm module shown in . In some embodiments engine may display or otherwise notify the user of the current resources available to that user e.g. by displaying the current resources on a display.

At step engine may determine whether any of the tasks of the proposed new test would exceed the current resources available to the requesting user e.g. by executing algorithm module shown in . This may include a comparison of the required resources for each task as determined at step with the current resources available to the requesting user as determined at step . If any of the tasks of the proposed new test would exceed the requesting user s currently available resources the proposed new test is not added to system as indicated at step . In some embodiments engine may display or otherwise notify the user of the results of the determination. At step engine may determine whether the network processors can accommodate the tasks of the proposed new test e.g. by executing algorithm module shown in . This may include a comparison of the required resources for each task as determined at step with the current resources available to each network processor as determined as determined at step . If it is determined that the network processors cannot accommodate the new test the proposed new test is not added to system as indicated at step . In some embodiments engine may display or otherwise notify the user of the results of the determination.

At step if algorithm module determines that the network processors can accommodate all tasks of the proposed new test engine assign the tasks of the proposed new task to one or more network processors e.g. by executing algorithm module shown in . In some embodiments engine may display or otherwise notify the user of the assignment of tasks to network processor s . At step engine may adds the proposed new test to the set of tests running on system e.g. by executing algorithm module shown in . At step task management engine may then instruct control processor and or relevant network processors to initiate the new test. In some embodiments engine may notify the user of the test initiation.

In some embodiments network testing system may perform statistical analysis of received network traffic in order to measure the quality of service provided under a given test scenario. One measure of quality of service is network performance measured in terms of bandwidth or the total volume of data that can pass through the network and latency i.e. the delay involved in passing that data over the network . Each data packet passing through a network will experience its own specific latency based on the amount of work involved in transmitting that packet and based on the timing of its transmission relative to other events in the system. Because of the huge number of packets transmitted on a typical network measurement of latency may be represented using statistical methods. Latency in network simulation may be expressed in abstract terms characterizing the minimum maximum and average measured value. More granular statistical analysis may be difficult to obtain due to the large number of data points involved and the rate at which new data points are acquired.

In some embodiments a network message may be comprised of multiple network packets and measurement may focus on the complete assembled message as received. In some testing scenarios the focus of the analysis may be on individual network packets while other testing scenarios may focus on entire messages. For the purposes of this disclosure the term network message will be used to refer to a network message that may be fragmented into one or more packets unless otherwise indicated.

This aspect of the network testing system focuses on the measurement of and visibility into the latency observed in the lab environment. The reporting period may be subdivided into smaller periodic windows to illustrate trends over time. A standard deviation of measured latencies may be measured and reported within each measurement window. Counts tracking how many packets fall within each of a set of latency ranges may be kept over a set of standard deviation sized intervals. Latency boundaries of ranges may be modified for one or more subsequent intervals based at least in part on the average and standard deviation measured in the previous interval. Where these enhanced measurements are taken during a simulation they may be presented to a user to illustrate how network latency was affected over time by events within the simulation.

In certain embodiments each packet transmitted has a timestamp embedded in it. When the packet is received the time of receipt may be compared against the transmit time to calculate a latency. A count of packets and a running total of all latency measurements may be kept over the course of a single interval. At the end of the measurement interval an average latency value may be calculated by dividing the running total by the count from that interval and the count and sum may be reset to zero to begin the next interval. In some embodiments a separate counter may be kept to count all incoming packets and may be used to determine the average latency value.

For a subset of the packets e.g. one out of every n packets where n is a tunable parameter the latency may be calculated as above and a running sum of the latency of this subset may be kept. In addition a running sum of the square of the latencies measured for this subset may be calculated. Limiting the calculation to a subset may avoid the problem of arithmetic overflow when calculating the sum of squares. At the end of each interval the standard deviation over the measured packets may be calculated using the sum of squares minus square of sums method or

In certain embodiments a set of counters may be kept. A first pair of counters may represent latencies up to one standard deviation from the average as measured in the previous measurement window. A second pair of counters may represent between one and two standard deviations from the average. A third pair of counters may represent two or more standard deviations from the average. Other arrangements of counters may be valuable. For example additional counters may be provided to represent fractional standard deviation steps for a more granular view of the data. In another example additional counters may be provided to represent three or four standard deviations away to capture the number of extreme latency events. In some embodiments the focus of the analysis is on high latencies. In these embodiments one counter may count all received packets with a latency in the range of zero units of time to one standard deviation above the average.

The counters may be maintained as follows. For each packet received one of the counters is incremented based on the measured latency of that packet. At the end of each interval the counts may be recorded e.g. in a memory database or log before the counters are reset. Also at the end of an interval the boundaries between counters may be adjusted based on the new measured average and standard deviation.

In some embodiments the interval length may be adjusted to adjust the frequency of measurement. For example a series of short intervals may be used initially to calibrate the ongoing measurement and a series of longer intervals may be used to measure performance over time. In another example long intervals may be used most of the time to reduce the amount of data gathered with short intervals interspersed regularly or randomly to observe potentially anomalous behavior. In yet another example the interval length may be adjusted based on an internal or external trigger.

In some embodiments the counters may be implemented within the capture offload CLDs A. Locating the counters and necessary logic with CLDs A ensures maximal throughput of the statistical processing system and maximal precision without the possibility of side effects due to internal transfer delays between components within the network testing system.

Process continues for a specified interval of time e.g. one second . In process a responsive network message is received at step and stamped with a high resolution clock value indicating a time of receipt. This responsive network message is examined and information is extracted that may be used to determine a when a corresponding outbound network message was sent. In some embodiments the responsive network message includes a timestamp indicating when the corresponding outbound network message was sent. In other embodiments a serial number or other unique identifier may be used to lookup a timestamp from a database indicating when the corresponding outbound network message was sent. At step the latency is calculated by subtracting the sent timestamp of the outbound network message from the receipt timestamp.

At step the latency is compared against a series of one or more threshold values to determine which bucket should be incremented. Each bucket is a counter or tally of the number of packets received with a latency falling within the range for that bucket. In certain embodiments the threshold values are represented as a max min pair of latency values representing the range of values associated with a particular bucket. The series of buckets forms a non overlapping but continuous range of latency values. In the example illustrated in in the initial configuration at time equals zero the lowest latency bucket is associated with a range of zero to less than 10 microseconds the second latency bucket is associated with a range of ten microseconds to less than 100 microseconds and so forth. In the illustration in the lowest latency range starts at zero and highest latency range continues to infinity in order to include all possible latency values. In some embodiments the latency ranges may not be all inclusive and extreme outliers may be ignored. As a final step with each received network message two interval totals are incremented. The first is a total latency value. This total latency value is incremented by the latency of each received packet. The second is a sum of squares value which is incremented by the square of the latency of the received packet at step .

At the end of the time interval process stores the current statistics and adjusts the threshold values to better reflect the observed variation in latencies. First the current latency counts and latency threshold range information is stored at step for later retrieval by a reporting tool or other analytical software. In some embodiments the information stored at this step includes all of the information in . Next new threshold latency values are calculated at step .

In some embodiments step adjusts the threshold latency values to fit a bell curve to the data of the most recently captured data. In this process the total received message count maintained independently or calculated by summing the tallies in each bucket and the total latency are used to calculate the average latency or center of the bell curve. Then the sum of squares value is used in combination with the average latency to determine the value of a latency that is one standard deviation away from the average. With the average and standard deviation known the threshold ranges may be calculated to be zero to less than two standard deviations below the average two standard deviations to less than one standard deviations below the average one standard deviation below to less than one standard deviation above one standard deviation above to less than two standard deviations above the average and two standard deviations above the average to infinity. Finally the total latency and total sum of squares latency values are zeroed at step .

In embodiments where the threshold latency values do not encompass all possible latency values outliers may be completely ignored or may be used to only calculate the new threshold latency values. In the former case step will be skipped for each outlier message so as not to skew the average and standard deviation calculation. In the latter case a running tally of all received messages is necessary and step will be performed on all received messages.

Serial ports on various processors in system may need to be accessed during manufacturing and or system debug phases. In conventional single processor systems serial port access to the processor is typically achieved by physically removing the board from the chassis and connecting a serial cable to an on board connector. However this may hinder debug ability by requiring the board to be removed to attach the connector possibly clearing the fault on the board before the processor can even be accessed. Further for multi processor boards of various embodiments of system the conventional access technique would require separate cables for each processor. This may cause increased complexity in the manufacturing setup and or require operator intervention during the test each of which may lengthen the test time and incur additional per board costs. Thus system incorporates a serial port access system that provides serial access to any processor on any card in system without having to remove any cards from chassis .

The crossbar switch on each card may comprise an any to any switch connected to all serial ports on the respective card . As shown in crossbar switch connects serial ports of control processor e.g. Intel X86 processor each network processor e.g. XLR Network processors external RS 232 connection a shared backplane MLVDS connection and management microcontroller to provide direct serial communications between any of such devices. In particular the serial ports may be set up to connect between any two attached serial ports through register writes to the CPLD . Crossbar switch may comprise custom logic stored on each CPLD .

An MLVDS Multipoint LVDS shared bus runs across the multi blade chassis backplane and allows connectivity to the crossbar switch in the CPLD of each other card in the chassis . Thus serial port access system allows access to serial ports on the same blade referred to as intra blade serial connections as well as to serial ports on other blades in the chassis via the MLVDS shared bus referred to as inter blade serial connections .

In addition to setting the registers on the CPLD on the local blade control processor sends a message to the local management microcontroller at step to initiate an I2C based signaling for setting the CPLD crossbar switch on the second blade as follows. At step the management microcontroller uses it s 12C connectivity to the other blades in the system to write to an I2C I 0 expander on the second blade involved in the serial connection i.e. the blade housing the target device . For example the management microcontroller sets 4 bits of data out of the I 0 expander on the second blade that are read by the local CPLD . Based on these 4 bits of data CPLD on the second blade sets the local crossbar configuration registers to connect the backplane serial MLVDS connection on the second blade with the target device on the second blade at step . This creates a direct serial connection between the requesting device on the first blade and the target device on the second blade via the MLVDS serial bus bridging the two blades.

Thus serial port access system a provides each processor in system direct serial access each other processor in system and b provides a user direct serial access to any processor in system either by way of control processor or via external RS 232 serial port . If control processor has booted and is functioning properly a user can access any processor in system by way of the control processor acting as a control proxy e.g. according to the method of for intra blade serial access or the method of for intra blade serial access . Thus control processor can be used as a control proxy to debug other devices in system .

Alternatively a user can access any processor in system via physical connection to external RS 232 serial port at the front of chassis . For example a user may connect to external RS 232 serial port when control processors of system are malfunctioning not booted or otherwise inaccessible or inoperative. Serial ports are primitive peripherals that allow basic access even if EEPROMs or other memory devices in the system are malfunctioning or inoperative. In addition CPLD is booted by its own internal flash memory program and accepts RS 232 signaling commands such that crossbar switch in CPLD may be booted and operational even when control processors and or other devices of system are malfunctioning not booted or otherwise inaccessible or inoperative. As another example a user may connect a debug device or system to external RS 232 serial port for external debugging of devices within system .

Thus based on the above serial port access system including crossbar switch allows single point serial access to all processors in a multi blade system and thus allows debugging without specialized connections to system .

System includes multiple programmable devices e.g. microcontrollers that must be programmed before each can perform its assigned task s . One mechanism for programming a device is to connect it to a non transient programmable memory e.g. EEPROM or Flash such that device will read programming instructions from that memory on power up. This implementation requires a separate non transient programmable memory per device which may significantly increase the part count and board complexity. In addition a software update must be written to each of these non transient programmable memories. This memory update process often called flashing the memory adds further design complexity and if interrupted may result in a non functioning device.

Instead of associating each programmable device with its own memory some embodiments of the present disclosure provide a communication channel between control processor and at least some devices through which processor can program each device from device images stored on drive . In these embodiments updating a program for a device may be performed by updating a file on drive . In some embodiments a universal serial bus USB connection forms the communication channel between control processor and programmable devices through which each device may be programmed.

In an embodiment with one programmable device that device will automatically come out of reset and appear on the USB bus ready to be programmed. Control processor will scan the USB bus for programmable devices and find one ready to be programmed. Once identified control processor will locate a corresponding image on drive and will transfer the contents of image to device e.g. via a set of sequential memory transfers.

Certain embodiments require additional steps in order identify and program specific programmable devices . The programmable devices are not pre loaded with instructions or configuration information and each will appear identical as it comes out of reset even though each must be programmed with a specific corresponding image in order to carry out functions assigned to that device within system . The USB protocol cannot be used to differentiate devices as it does not guarantee which order devices will be discovered or provide any other identifying information about those devices. As a result control processor cannot simply program devices as they are discovered because control processor will not be able to identify the specific corresponding image associated with that device.

In one embodiment each programmable device may be connected to an EEPROM or wired coding system e.g. DIP switches or hardwired board traces encoding a device identifier to provide minimal instructions or identification information. However while this technique may enable device specific programming it involves initial pre programming steps during the manufacturing process which may add time complexity and cost to the manufacturing process. Further this technique may reduce the flexibility of the design precluding certain types of future software updates or complicating design reuse.

In some embodiments system includes a programmable device initiation system that uses one of the programmable devices e.g. a USB connected microcontroller as a reset master for the other programmable devices which allows the slave devices to be brought out of reset and uniquely identified by control processor in a staggered manner to ensure that each programmable device receives the proper software image . These embodiments may eliminate the need for an EEPROM associated each USB device discussed above and may thus eliminate the time and cost of pre programming each EEPROM.

In some embodiments master programmable device has outputs connected to reset lines for each of the slave programmable devices as illustrated in . In other embodiments master programmable device has fewer outputs connected to a MUX to allow control of more slave devices with fewer output pins. In certain embodiments master programmable device has one output controlling the reset line of a single other programmable device . That next programmable device also has an output connected to the reset line of a third programmable device . Additional programmable devices may be chained together in this fashion where each programmable device may be programmed and then used as a master to bring the next device out of reset for programming.

At step control processor releases the next programmable device from reset using reset signaling shown in by driving the output high that is connected to the reset pin on the next programmable device to be programmed e.g. Microcontroller . At step control processor detects this next device on the USB bus as ready to be programmed determines using logic the image on drive corresponding to that programmable device and programs that image onto the programmable device. Using this method control processor can cycle through the programmable devices Microcontrollers one by one in the order specified in logic to ensure that each device is enumerated and programmed for correct system operation. Once control processor determines that all programmable devices have come up the method may end as indicated at step .

Programming from local flash EEPROM This method programs the CLDs immediately on boot so the parts are ready very quickly however it also requires individual flash EEPROM parts at each CLD. Also CLD design files have become quite large e.g. greater than 16 MB and that file size is increasing software update time by requiring as much as five minutes per CLD to overwrite each flash EEPROM memory.

Programming via software through CPLDs This is another standard method to use the Fast Parallel programming method for the CLDs. In this approach software installed on a CPLD from internal flash memory initiates the programming during each boot process. Connectivity to the CPLD from the control processor can be an issue with limited options available. To use a PCI connection between control processor and a CLD to be programmed the CPLD must implement PCI cores which consumes valuable logic blocks and requires a licensing fee. Other communication options require the use of specialized integrated circuits. Moreover this approach requires complex parallel bus routing to connect the CPLD to each CLD to be programmed. Long multi drop parallel busses need to be correctly routed with minimal stubs and the lengths need to be controlled to maintain signal integrity on the bus. Some embodiments have 5 FPGA s placed across an 11 18 printed computer board PCB resulting in long traces.

To enable fast flexible programming of CLDs an arrangement of components is utilized to provide software based programming of CLDs controlled by control processor . In certain embodiments one or more microcontrollers are provided to interface with the programming lines of CLDs e.g. the Fast Parallel Programming bus on an FPGA . Those one or more microcontrollers are also connected to control processor via a high speed serial bus e.g. USB IEEE 1394 THUNDERBOLT . The small size of the microcontroller combined with the simplified trace routing enabled by the serial bus allowed direct high speed programming access without the need for long parallel bus lines. Furthermore adding one or more additional microcontrollers could be accomplished with minimal negative impact to the board layout due to minimal part size and wiring requirements while allowing for further simplification of parallel bus routing.

In certain embodiments two microcontrollers e.g. Cypress FX2 USB Microcontrollers are provided. One is positioned near two CLDs on one side of the board and the other is positioned on the opposite side of the board near the other three CLDs . This placement allows for short parallel bus connections to each CLD to help ensure signal integrity on those busses.

At step control processor communicates with each microcontroller via CLD access logic to place the CLDs in programming mode. Microcontroller may perform this operation by driving one or more individual control signals to initiate a programming mode in one or more CLD . In some embodiments microcontroller may program multiple CLD simultaneously e.g. with an identical image by initiating a programming mode on each prior to transmitting a programming image. In some embodiments microcontroller may program CLD devices individually.

At step control processor locates CLD image corresponding to the next CLD to program. Control processor may locate the corresponding image file based on information hard coded on one or more devices. In some embodiments microcontrollers may have one or more pins hard coded e.g. tied high grounded by a pull down resistor to allow specific identification by control processor . In these embodiments that identification information may be sufficient to allow control processor to control a specific CLD by driving a predetermined individual control signal line. In other embodiments microcontrollers are programmed identically while CLDs may have hard coded pins to allow identification by the corresponding microcontroller . In these embodiments CLD access logic will include logic to control each CLD individually in order to read the hard coded pins and thereby identify that device by type e.g. capture offload CLD or L2 L3 CLD or specifically e.g. a specific CLD within system .

Once the corresponding CLD image has been identified control processor transfers the contents of that image e.g. in appropriately sized sub units to microcontroller via the serial connection. Microcontroller via an individual control signal initiates a programming mode on the CLD being programmed and loads image into the CLD via the shared parallel data bus.

At step control processor determines whether another CLD should be programmed and returns to step until all have been programmed.

The transfer speed of the serial bus e.g. USB is sufficiently fast to transfer even large e.g. 16 MB image files in a matter of seconds to each CLD. This programming arrangement also simplifies updates where replacing CLD image files on drive will result in a CLD programming change after a restart. No complicated flashing and verification process is required.

Any time flash memories or EEPROMs are updated through software there is a risk of corruption that may result in one or more non functional devices. The present disclosure provides a reliable path to both program on board devices such as CLD s as well as on board memories e.g. EEPROMs and flash memory . The present disclosure also provides a reliable path to recover from a corrupted image in most devices without rendering a board into a non functional state a.k.a. bricking a board . The present disclosure additionally provides a path for debugging individual devices.

In system programming of all programmable devices on board is critical for field support and software upgrades. Past products did not have a good method for in system programming some devices and caused field returns when an update was needed or to recover from a corrupted device. The present disclosure provides a method to both update all chips as a part of the software upgrade process and to be able to recover from a corrupted image in an on board memory device e.g. EEPROM or flash .

In addition to image update and field support the present disclosure also provides more convenient access to each CLD for in system debug. Previous designs required boards be removed and cables attached to run the debug tools. The present disclosure provides in place in system debug capability. This capability allows debugging of a condition that may be cleared by removing the board from the system.

To allow for both programming and CLD debug the JTAG chain has been subdivided into two sections. The first section includes each CLD and the second section includes all other JTAG compatible devices in system . This division enables convenient access to and automatic recognition of ALTERA devices by certain ALTERA supplied JTAG debug tools.

In certain embodiments short chain provides JTAG access to the 5 FPGA s and 3 CPLD s on the board. This mode may be used to program the CPLD s on the board to program the Flash devices attached to two CPLD s and to run the ALTERA supplied debug tools. The ALTERA tools are run through a software JTAG server interface. ALTERA tools running on a remote workstation may connect via a network connection to control processor and access the JTAG controller. Control processor may include a modified version of the standard LINUX URJTAG Universal JTAG program to enable CPLD and flash programming. Through that tool control processor may program the CPLD s and through the programmed CPLD s the tool can access each attached flash memory not directly connected to the JTAG bus. The flash memories may contain boot code for one or more network processors. Use of the JTAG bus to program these flash memories enables programming of the boot code without the processor running. Previous designs had to be pre programmed and had the risk of bricking a system if a re flash was interrupted. Recovery from such an interruption required a return of the entire board for lab repair. System allows the boot code to be programmed regardless of the state of the network processor allowing for in field update and recovery.

When attached to the full chain e.g. and the microcontroller has access to all the devices on the JTAG bus. The full chain may be used to program the Serial Flash containing the boot code for the networking switch on the board. To program networking switch the JTAG software on control processor may control the pins of networking switch to write out a new flash image indirectly.

As with many systems drive is a standard size and has a standard interface making it mechanically and electrically interchangeable with commodity hardware. However not all drives have satisfactory performance and reliability characteristics. In particular while a solid state device may provide sufficiently low access times and sufficiently high write throughput to maintain certain applications a physically and electrically compatible 5 400 RPM magnetic drive might not. In some cases high volume purchasers of drives may purchase customized devices with manufacture supplied features for ensuring that only authorized drives are used within a system. To prevent users from operating system with an unauthorized drive control processor may read certain information from drive to verify that the drive is identified as an authorized drive.

In some embodiments drive may be partitioned into two logical units hidden partition including branding information and data partition . In some embodiments hidden partition may be a drive partition formatted for example in a non standard format. In certain embodiments hidden partition may be a standard drive partition formatted as a simple standard file system e.g. FAT . In some embodiments branding information may be a raw data written to a specific block on hidden partition . In some embodiments branding information may data written to a file on hidden partition .

Data partition may be a standard drive partition formatted as a standard file system e.g. FAT ext2 NTFS and may contain operating system and application software CLD images packet capture data and other instructions and data required by system .

Branding process may include the following steps performed by a processor such as processor on a second drive . At step software executing on processor may read the drive serial number from read only memory . At step that software may partition the drive into a hidden partition and a data partition . At step the software may format hidden partition . In some embodiments step may be skipped if formatting is not required e.g. where branding information is written as raw data to a specific block of partition . At step the drive serial number is combined with secret information using a one way function such as the jhash function or a cryptographic hash to obtain branding information . At step branding information is written to hidden partition . At this point the drive will be recognized as authorized by system and data partition may be formatted and loaded with an image of system .

Verification process may include the following steps performed by CPU . At step CPU powers up and loads the basic input output system BIOS instructions stored in SPI EEPROM. At step CPU accesses drive and loads branding information and drive serial number . At step CPU verifies branding information . In some embodiments CPU may apply a public key which pairs with the private key used in step to decrypt branding information . If the decrypted value matches serial number the drive may be recognized as authorized. In other embodiments CPU may combine serial number with the same secret used in step and in the same manner. If the result is the same as branding information the drive may be recognized as authorized.

If the drive is authorized CPU may begin to boot the operating system from partition at step . If the drive is not authorized CPU may report an error at step and terminate the boot process. The error report may be lighting a light emitting diode LED on the control panel of system .

In some embodiments verification process may be performed by software executed by the operating system as part of the operating system initialization process.

As discussed above network testing system may comprise one or more boards or cards arranged in slots defined by a chassis . illustrates one example embodiment of network testing system that includes a chassis having three slots configured to receive three cards . Each card may have any number and types of external physical interfaces. In the illustrated example each card has a removable disk drive assembly that houses a disk drive one or more ports for connection to a test system for management of test system one or more ports e.g. including RS 232 port for connection to controller for managing aspects of card a port e.g. a USB port for inserting a removable drive for performing software upgrades software backup and restore etc. for debugging card e.g. by connecting a keyboard and or mouse to communicate with the card or for any other purpose and a number of ports corresponding to test interfaces . Each card may also include a power button and any suitable handles latches locks etc. for inserting removing and or locking card in chassis .

Heat dissipation presents significant challenges in some embodiments of system . For example CLDs processors and and management switch may generate significant amounts of heat that need to be transferred away from system e.g. out through openings in chassis . In some embodiments limited free space and or limited airflow within chassis present a particular challenge. Further in some embodiments of a multi slot chassis different slots receive different amounts of air flow from one or more fans and or the physical dimensions of individual slots e.g. the amount of free space above the card in each respective slot may differ from each other the amount of volume and speed of air flow. Further in some embodiments the fan or fans within the chassis tend to move air diagonally across the cards rather than directly from side to side or front to back. Further heat generated by one or more components on a card may transfer heat to other heat generating components on the card e.g. by convection or by conduction through the printed circuit board thus further heating or resisting the cooling of such other heat generating components on the card . Thus each card may include a heat dissipation system that incorporates a number of heat transfer solutions including one or more fans heat sinks baffles or other air flow guide structures and or other heat transfer systems or structures.

Turning first to card includes a printed circuit board that houses a pair of capture and offload CLDs and and associated DDR3 SDRAM memory modules DIMMs A and A a pair of routing CLDs and and associated QDR SRAMs and a traffic generation CLD C a pair of network processors and and associated DDR2 SDRAM DIMMs and a control processor and associated DDR3 SDRAM DIMMs a management switch four test interfaces a backplane connector a notch or bay that locates a drive connector for receiving a disk drive assembly that houses a disk drive and various other components e.g. including components shown in . As shown DIMMS A A and may be aligned in the same direction e.g. in order to facilitate air flow from one or more fans across card in that direction e.g. in a direction from side to side across card .

Turning next to a number of heat sinks may be installed on or near significant heat generating devices of card . As shown card includes a dual body heat sink to remove heat from first network processor a heat sink to remove heat from second network processor a heat sink to remove heat from control processor a number of heat sinks to remove heat from each CLD and and a heat sink to remove heat from management switch . Each heat sink may have any suitable shape and configuration suitable for removing heat from the corresponding heat generating devices. As shown each heat sink may include fins pegs or other members extending generally perpendicular to the plane of the card for directing air flow from one or more fans across the card . Thus the fins of the various heat sinks may be aligned in one general direction the same alignment direction as DIMMS A A and in order to facilitate air flow in a general direction across card through the heat sinks and DIMMS. Some heat sinks may include an array of fins in which each individual fins extends in one direction the direction of air flow and with gaps between fins that run in a perpendicular direction which gaps may create turbulence that increases convective heat transfer from the fins to the forced air flow.

As discussed below in greater detail dual body heat sink for removing heat from first network processor includes a first heat sink portion arranged above the network processor and a second heat sink portion physically removed from network processor but connected to the first heat sink portion by a heat pipe . Heat is transferred from the first heat sink portion to the second heat sink portion i.e. away from network processor via the heat pipe. As shown in the second heat sink portion may be arranged laterally between two sets of DIMMs A and and longitudinally in line with another set of DIMMs in the general direction of air flow. Details of dual body heat sink are discussed in more detail below with reference to .

As shown in air baffle may include various structures and surfaces for guiding or facilitating air flow across card as desired. For example first part of air baffle may include a thin generally planar sheet portion arranged above components on card and extending parallel to the plane of the printed circuit board and a number of guide walls extending downwardly and perpendicular to the planar sheet portion . Similarly second part may include a thin generally planar sheet portion arranged above components on card and extending parallel to the plane of the printed circuit board and a number of guide walls extending downwardly and perpendicular to the planar sheet portion . Guide walls and are configured to influence the direction and volume of air flow across various areas and components of card e.g. to promote and distribute air flow through the channels defined between heat sink fins and DIMMs on card .

In addition first part of air baffle may include angled flaps or wings and configured to direct air flow above air baffle downwardly into and through the fins of heat sinks and respectively to promote conductive heat transfer away from such heat sinks. As discussed below with reference to wings and may create a low pressure area that influences air flow downwardly into the respective heat sinks

As discussed above heat dissipation system of card may include a dual body heat sink that functions in cooperation with air baffle to dissipate heat from a network processor e.g. a Netlogic XLR 732 1.4 GHz processor .

First heat sink body is connected to the spaced apart second heat sink body by a heat pipe . As shown in heat sink may include two heat pipes a first heat pipe that connects first heat sink body with second heat sink body and a second heat pipe located within the perimeter of first heat sink body . A thermal interface area in which network processor physically interfaces with heat sink body is indicated in . Both heat pipes and extend through the thermal interface area to facilitate the movement of heat from processor to heat sink bodies and via the thermal interface area . Heat pipe moves heat to the remotely located heat sink body which is cooled by an air flow across heat sink body which causing further heat flow from heat sink body to heat sink body . Two heat sink bodies are used so that memory DIMMs for processor can be placed close to processor . The cooling provided by the dual body design may provide increased or maximized processing performance of processor as compared with certain single body heat sink designs.

As shown both heat pipes and interface with processor via thermal interface area . The co planarity of this interface may be critical to adequate contact. Thus the interface may be milled to a very tight tolerance. Further in some embodiments a phase change thermal material or other thermally conductive material may be provided at the interface to ensure that heat sink body is bonded at the molecular level with processor . This material may ensures extremely high thermal connectivity between processor and heat sink body .

In this embodiment each heat pipe is generally U shaped and is received in rectangular cross section channels milled in heat sink bodies and except for the portion of pipe extending between first and second heat sink bodies and . Each channel may be sized such that a bottom surface of each heat pipe and is substantially flush with bottom surfaces of heat sink bodies and . Thus heat pipes and are essentially embedded in heat sink bodies and . Heat pipes and may have rounded edges. Thus when heat pipes and are installed in channels gaps are formed between the walls of channels and the outer surfaces of heat pipes and . Left empty such gaps would reduce the surface area contact between the heat pipes and the heat sink bodies as well as the contact between the heat pipes heat sink and processor at thermal interface area which may reduce the performance of processor . Thus such gaps between the walls of channels and the outer surfaces of heat pipes and may be filled with a thermally conductive solder or other thermally conductive material to promote heat transfer between heat pipes and and heat sink bodies and and all bottom surfaces may then be machined flat to provide a planar surface with a tight tolerance.

Heat sink bodies and and heat pipes and may be formed from any suitable thermally conductive materials. For example heat sink bodies and may be formed from copper and heat pipes and may comprise copper heat pipes embedded in copper heat sink bodies and .

Fins on bodies and may be designed to provide a desired or maximum amount of cooling for the given air flow and air pressure for the worst case slot of the chassis . The thickness and spacing of fins may be important to the performance of heat sink . Mounting of heat sink to card may also be important. For example thermal performance may be degraded if the pressure exerted on heat sink is not maintained at a specified value or within a specified range. In one embodiment an optimal pressure may be derived by testing and a four post spring based system may be designed and implemented to attach heat sink to the PCB .

In some embodiments fans in chassis create a generally diagonal air flow though the chassis . Due to this diagonal airflow as well as the relatively small cross section of cards and pre heating of processors caused by heat from adjacent processors a special air baffle may be provided to work in conjunction with heat sink and other aspects of heat dissipation system as discussed above. Air baffle has unique features with respect to cooling of electronic and assists the cooling of other components of card as discussed above with reference to and below with reference to .

In some embodiments management switch generates large amounts of heat. For example management switch may generate more heat than any other device on card . Thus aspects of heat dissipation system including the location of management switch relative to other components of card the design of heat sink coupled to management switch and the design of air baffle may be designed to provide sufficient cooling of management switch for reliable performance of switch and other components of card .

As shown in in the desired direction of air flow across card management switch is aligned with network processor . Due to the large amount of heat generated by switch it may be disadvantageous to dissipate heat from management switch into the air flow that subsequently flows across and through heat sink above network processor . That is delivering a significant portion of the heat from switch through the heat sink intended to cool network processor may inhibit the cooling of network processor . Thus heat sink may be configured to transfer heat from management switch laterally out of alignment with network processor in the desired direction of air flow . Thus as shown in heat sink may include a first conductive portion positioned over and thermally coupled to management switch and a second finned portion laterally removed from management switch in order to conductively transfer heat laterally away from management switch and then from the fins of finned portion to the forced air flow by convection. In this example configuration finned portion is aligned in the air flow direction with DIMMs rather than with network processor . Because DIMMs typically generate substantially less heat than network processors DIMMs may be better suited than network processor to receive the heated airflow from switch .

Further as shown in and A B air baffle is configured to direct and increase the volume of air flow across heat sink . For example angled wing directs air flow downwardly through heat sink which then flows through heat sink . Further an angled guide wall of the second part of air baffle essentially funnels the air flow to heat sink thus providing an increased air flow mass and or speed across heat sink .

In one embodiment shell is a sheet metal shell and air deflector serves as a multi vaned air deflector that creates specific channels for air to flow. The parts are assembled as shown in . As discussed above the sheet metal shell may include slanted wing like structures and which act as low pressure generators to direct air flow downwardly as shown in . Similar to an aircraft wing an angle of attack with respect to the plane of the sheet metal in may be set for each wing and indicated as and respectively. The angles and may be selected to provide desired air flow performance and may be the same or different angles. In some embodiments one or both of and are between 20 and 70 degrees. In particular embodiments one or both of and are between 30 and 60 degrees. In certain embodiments one or both of and are between 40 and 50 degrees.

Each wing and creates a low pressure area which deflects a portion of the air flow above the sheet metal plane downwardly into the air baffle . This mechanism captures air flow that would normally move above the heat sink fins and redirects this air flow through the heat sink fins. The redirected airflow may be directed to lower parts of the heat sinks located within the air baffle i.e. below the sheet metal plane thus providing improved cooling performance. An indication of air flow paths provided by air baffle is provided in .

Further as discussed above air baffle may include guide vanes and extending perpendicular from planar sheets and of shell and air deflector i.e. downwardly toward PCB . As discussed above fans may tend to generate a diagonal air flow across card . On a general level guide vanes and may direct this air flow across card in a perpendicular or orthogonal to the sides of card rather than diagonally across card which may promote increased heat dissipation. On a more focused level as shown in particular guide vanes of air deflector may be angled with respect to the perpendicular side to side direction of air flow which may create areas of increased air flow volume and or speed e.g. for increased cooling of management switch as discussed above. In one embodiments vanes and are implemented as a Lexan structure. Thus to summarize in some embodiments vanes and linearize the diagonal air flow supplied by high speed fans in chassis . The vanes cause the air to flow through over the heat sinks within and downstream of air baffle which may provide the air speed and pressure necessary for proper operation of such heat sinks. Further vanes and may be designed to substantially prevent pre heated air from flowing through critical areas that may require or benefit from lower temperature air for desired cooling of such areas e.g. to substantially prevent air heated by management switch by way of heat sink from subsequently flowing across downstream heat sink part arranged above network processor .

As discussed above in some embodiments disk drive is a solid state drive that can be interchanged or completely removed from card e.g. for interchangeability security and ease of managing multiple projects for example. Disk drive may be provided in a drive assembly shown in . Drive assembly includes a drive carrier support that is secured to card and a drive carrier that is removeably received in the drive carrier support . Drive carrier houses solid state disk drive which is utilized by control processor for various functions as discussed above. With reference to and A B drive carrier support may be received in notch formed in PCB and secured to PCB . When drive carrier is fully inserted in drive carrier support connections on one end of disk drive connect with drive connector shown in thus providing connection between drive and control processor and or other processors or devices of card .

Lateral sides of disk housing are configured to be slidably received in guide channels of drive carrier support shown in . Disk housing may also include end flanges that include a groove or other protrusion or detent for engaging with spring tabs at the back portion of drive carrier support shown in . Disk housing may also include a lighted label and a handle for installing and removing drive carrier . Handle may comprise a D shaped finger pull or any other suitable handle.

The components of drive carrier and drive carrier support may be formed from any suitable materials. In some embodiments drive carrier may be formed from materials that provide desired weight conductivity and or EMI shielding e.g. aluminum.

Drive carrier support may be formed from any suitable materials. In some embodiments drive carrier support may be formed from materials that provide low insertion force e.g. low friction force . For example drive carrier support may be formed from polyoxymethylene acetal polyacetal or polyformaldehyde to provide a self lubricating surface rigidity stability and machinability.

In some embodiments drive assembly and or card includes a drive status detection system for automatic detection of the removal or insertion of drive carrier from drive carrier support . For example the drive status detection may include an electrical micro switch configured to detect the presence or absence of the drive carrier or communicative connection disconnection of drive from card . Other embodiments include software for detecting the presence or absence of the drive carrier or communicative connection disconnection of drive from card . Such software may periodically check an ID register on the drive to verify that the drive carrier is still installed. If the drive is not found the software may automatically issue a board reset. A special BIOS function may be provided that periodically or continuously checks for a drive if a drive is not found. Once the drive carrier is installed and the BIOS detects the drive the card will boot normally.

For the purposes of this disclosure the term exemplary means example only. Although the disclosed embodiments are described in detail in the present disclosure it should be understood that various changes substitutions and alterations can be made to the embodiments without departing from their spirit and scope.

