---

title: Archival data flow management
abstract: Methods and systems are provided herein to allow efficient management of data flowing in and out of an archival data storage system. In an embodiment, storage entities keep very little state information in memory to provide higher throughput. Further, storage entities may send data in large chunks to facilitate high throughput. Techniques such as batching and coalescing may be used by various storage entities to provide efficiency.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09563681&OS=09563681&RS=09563681
owner: AMAZON TECHNOLOGIES, INC.
number: 09563681
owner_city: Seattle
owner_country: US
publication_date: 20120808
---
This application incorporates by reference for all purposes the full disclosure of co pending U.S. patent application Ser. No. 13 569 984 filed concurrently herewith entitled LOG BASED DATA STORAGE ON SEQUENTIALLY WRITTEN MEDIA co pending U.S. patent application Ser. No. 13 570 057 filed concurrently herewith entitled DATA STORAGE MANAGEMENT FOR SEQUENTIALLY WRITTEN MEDIA U.S. Pat. No. 9 250 811 filed concurrently herewith entitled DATA WRITE CACHING FOR SEQUENTIALLY WRITTEN MEDIA co pending U.S. patent application Ser. No. 13 570 030 filed concurrently herewith entitled PROGRAMMABLE CHECKSUM CALCULATIONS ON DATA STORAGE DEVICES U.S. Pat. No. 9 213 709 filed concurrently herewith entitled ARCHIVAL DATA IDENTIFICATION U.S. Pat. No. 9 092 441 filed concurrently herewith entitled ARCHIVAL DATA ORGANIZATION AND MANAGEMENT U.S. Pat. No. 8 959 067 filed concurrently herewith entitled DATA STORAGE INVENTORY INDEXING co pending U.S. patent application Ser. No. 13 570 088 filed concurrently herewith entitled ARCHIVAL DATA STORAGE SYSTEM co pending U.S. patent application Ser. No. 13 569 591 filed concurrently herewith entitled DATA STORAGE POWER MANAGEMENT co pending U.S. patent application Ser. No. 13 569 714 filed concurrently herewith entitled DATA STORAGE SPACE MANAGEMENT U.S. Pat. No. 9 225 675 filed concurrently herewith entitled DATA STORAGE APPLICATION PROGRAMMING INTERFACE and U.S. Pat. No. 8 805 793 filed concurrently herewith entitled DATA STORAGE INTEGRITY VALIDATION. 

With increasing digitalization of information the demand for durable and reliable archival data storage services is also increasing. Archival data may include archive records backup files media files and the like maintained by governments businesses libraries and the like. Due to the potentially massive amount of data to be stored storage entities handling archival data flowing into and out of an archival data storage system need to implement optimizations to reduce the can cause costs to be prohibitive using many conventional technologies. Also it is often desired that the durability and reliability of storage for archival data be relatively high which further increases the amount of resources needed to store data thereby increasing the expense. Effective management of data flowing in and out of an archival data storage system is essential for achieving cost effectiveness.

In the following description various embodiments will be described. For purposes of explanation specific configurations and details are set forth in order to provide a thorough understanding of the embodiments. However it will also be apparent to one skilled in the art that the embodiments may be practiced without the specific details. Furthermore well known features may be omitted or simplified in order not to obscure the embodiment being described.

Techniques described and suggested herein includes methods and systems for providing data flow management in an archival data storage system. In an embodiment data flowing into and out of an archival data storage system are handled primarily by one or more storage entities such as storage node manager storage nodes and storage devices. Each storage node manager may be in communication with one or more storage nodes. Each storage node may be in communication with one or more storage devices where data objects are physically stored.

In an embodiment data to be stored in an archival data storage system is first stored in a transient data store and a data storage job corresponding to the data storage request may be created and submitted to a job queue. Subsequently a storage node manager may select the job for execution by retrieving the data from the transient data store redundantly encode the data into encoded data components and send each of the encoded data components to a storage node which stores the encoded data component into a storage device. For data retrieval the process may be reversed. A storage node manager for instance may select a data retrieval job for execution and requests that the storage nodes to provide the encoded data components. Upon receiving the encoded data components the storage manager may decode the encoded data components and provide the restored data to the transient data store.

In various embodiments storage entities may be implemented to efficiently handle data flowing in and out of the system as described above. For example a storage node manager may perform job planning and optimization such as job coalescing and batch processing. In addition data transfers may occur in bigger chunks than individual data components to improve efficiency. For example a storage node manager may provide one or more pages of data components in each storage request sent to a storage node. Further storage entities may maintain little state to improve throughput. For example a storage node manager may deallocate storage allocated for encoded data components soon after they are provided to the storage nodes regardless of whether a response is received indicating whether an encoded data component is successfully stored. Finally storage entities may perform batching request coalescence and other optimizations to improve efficiency.

As illustrated in in some embodiments archival data flows in and out of an archival data storage system in stages. Specifically when a customer requests storage of a data object into the archival data storage system the data object may be first stored in a transient data store . In some embodiments the transient data store may be the payload data cache described in connection with a variation thereof or any suitable data storage component. Next a storage job may be created to eventually move the data object from the transient data store to storage devices . The job may be submitted to a job queue and eventually selected by a storage node manager for execution. The job queue may be a partition in the storage node manager job store described in connection with .

In an embodiment archival data storage system include one or more storage entities such as storage node manager storage nodes and storage devices all of which are described in connection with . Each storage node manager may be in communication with one or more storage nodes . Each storage node may be in communication with one or more storage devices where data objects are physically stored.

Once a storage node manager selects a storage job for execution the storage node manager may retrieve the data object from the transient data store and encode e.g. redundantly the data object into a plurality of encoded data components before sending them to a set of storage nodes for storage. In some embodiment a data object may be divided into multiple parts and each part may be processed by the same or different storage node managers for encoding and storage. The storage nodes upon receiving a request from a storage node manager to store a data component may store the data component in a storage device such as hard disk drives managed by storage nodes described in more detail below.

The process described above is reversed for data retrieval request. Specifically when a customer requests to retrieve a data object a retrieval job may be created submitted to job queue and assigned to a storage node manager . The storage node manager may determine a set of storage nodes that store the encoded data components that are generated as a result of encoding the data object during data storage. Such a set of storage nodes may be a subset of the original set of storage nodes used to store all the encoded data components associated with the data object. The size of the subset may be determined based on the data encoding scheme used to encode the data object e.g. the stretch factor of an erasure encoding scheme . For example the size of the subset may be no less than the minimum number of data components to restore a redundantly encoded data object.

Next the storage node manager may request each of the set of storage nodes to retrieve the encoded data component. The storage nodes upon receiving a request from a storage node manager to retrieve an encoded data component may retrieve the corresponding encoded data component from a storage device and provide it to the storage node manager . The storage node manager may decode the encoded data components received from the storage nodes to obtain the original retrieved data object. The retrieved data object may be provided in transient data store for the customer to download.

As illustrated storage entities such as storage node manager storage nodes and storage devices are configured to be the primary handler of bulk data that flows into and out of the archival data storage system . In various embodiments the storage entities may implement various methods described herein to efficiently manage the data flow so as to improve efficiency and to provide scalability. For example storage nodes may perform heavy batching and request coalescing to reduce storage costs. As another example instead of sending one data component to a storage node in a storage request a storage node manager may send a bigger chunk of data e.g. one or more pages of data in a request to facilitate faster throughput of the storage node . As yet another example storage entities such as storage node manager and storage nodes may keep a shallow buffer to store data associated with outstanding requests to increase throughput. Additional details are provided below in particular in connection with .

In an embodiment archival data storage system provides a multi tenant or multi customer environment where each tenant or customer may store retrieve delete or otherwise manage data in a data storage space allocated to the customer. In some embodiments an archival data storage system comprises multiple subsystems or planes that each provides a particular set of services or functionalities. For example as illustrated in archival data storage system includes front end control plane for direct I O common control plane data plane and metadata plane . Each subsystem or plane may comprise one or more components that collectively provide the particular set of functionalities. Each component may be implemented by one or more physical and or logical computing devices such as computers data storage devices and the like. Components within each subsystem may communicate with components within the same subsystem components in other subsystems or external entities such as customers. At least some of such interactions are indicated by arrows in . In particular the main bulk data transfer paths in and out of archival data storage system are denoted by bold arrows. It will be appreciated by those of ordinary skill in the art that various embodiments may have fewer or a greater number of systems subsystems and or subcomponents than are illustrated in . Thus the depiction of environment in should be taken as being illustrative in nature and not limiting to the scope of the disclosure.

In the illustrative embodiment front end implements a group of services that provides an interface between the archival data storage system and external entities such as one or more customers described herein. In various embodiments front end provides an application programming interface API to enable a user to programmatically interface with the various features components and capabilities of the archival data storage system. Such APIs may be part of a user interface that may include graphical user interfaces GUIs Web based interfaces programmatic interfaces such as application programming interfaces APIs and or sets of remote procedure calls RPCs corresponding to interface elements messaging interfaces in which the interface elements correspond to messages of a communication protocol and or suitable combinations thereof.

Capabilities provided by archival data storage system may include data storage data retrieval data deletion metadata operations configuration of various operational parameters and the like. Metadata operations may include requests to retrieve catalogs of data stored for a particular customer data recovery requests job inquires and the like. Configuration APIs may allow customers to configure account information audit logs policies notifications settings and the like. A customer may request the performance of any of the above operations by sending API requests to the archival data storage system. Similarly the archival data storage system may provide responses to customer requests. Such requests and responses may be submitted over any suitable communications protocol such as Hypertext Transfer Protocol HTTP File Transfer Protocol FTP and the like in any suitable format such as REpresentational State Transfer REST Simple Object Access Protocol SOAP and the like. The requests and responses may be encoded for example using Base64 encoding encrypted with a cryptographic key or the like.

In some embodiments archival data storage system allows customers to create one or more logical structures such as a logical data containers in which to store one or more archival data objects. As used herein data object is used broadly and does not necessarily imply any particular structure or relationship to other data. A data object may be for instance simply a sequence of bits. Typically such logical data structures may be created to meeting certain business requirements of the customers and are independently of the physical organization of data stored in the archival data storage system. As used herein the term logical data container refers to a grouping of data objects. For example data objects created for a specific purpose or during a specific period of time may be stored in the same logical data container. Each logical data container may include nested data containers or data objects and may be associated with a set of policies such as size limit of the container maximum number of data objects that may be stored in the container expiration date access control list and the like. In various embodiments logical data containers may be created deleted or otherwise modified by customers via API requests by a system administrator or by the data storage system for example based on configurable information. For example the following HTTP PUT request may be used in an embodiment to create a logical data container with name logical container name associated with a customer identified by an account identifier accountId .

In an embodiment archival data storage system provides the APIs for customers to store data objects into logical data containers. For example the following HTTP POST request may be used in an illustrative embodiment to store a data object into a given logical container. In an embodiment the request may specify the logical path of the storage location data length reference to the data payload a digital digest of the data payload and other information. In one embodiment the APIs may allow a customer to upload multiple data objects to one or more logical data containers in one request. In another embodiment where the data object is large the APIs may allow a customer to upload the data object in multiple parts each with a portion of the data object.

In response to a data storage request in an embodiment archival data storage system provides a data object identifier if the data object is stored successfully. Such data object identifier may be used to retrieve delete or otherwise refer to the stored data object in subsequent requests. In some embodiments such as data object identifier may be self describing in that it includes for example with or without encryption storage location information that may be used by the archival data storage system to locate the data object without the need for a additional data structures such as a global namespace key map. In addition in some embodiments data object identifiers may also encode other information such as payload digest error detection code access control data and the other information that may be used to validate subsequent requests and data integrity. In some embodiments the archival data storage system stores incoming data in a transient durable data store before moving it archival data storage. Thus although customers may perceive that data is persisted durably at the moment when an upload request is completed actual storage to a long term persisted data store may not commence until sometime later e.g. 12 hours later . In some embodiments the timing of the actual storage may depend on the size of the data object the system load during a diurnal cycle configurable information such as a service level agreement between a customer and a storage service provider and other factors.

In some embodiments archival data storage system provides the APIs for customers to retrieve data stored in the archival data storage system. In such embodiments a customer may initiate a job to perform the data retrieval and may learn the completion of the job by a notification or by polling the system for the status of the job. As used herein a job refers to a data related activity corresponding to a customer request that may be performed temporally independently from the time the request is received. For example a job may include retrieving storing and deleting data retrieving metadata and the like. A job may be identified by a job identifier that may be unique for example among all the jobs for a particular customer. For example the following HTTP POST request may be used in an illustrative embodiment to initiate a job to retrieve a data object identified by a data object identifier dataObjectId. In other embodiments a data retrieval request may request the retrieval of multiple data objects data objects associated with a logical data container and the like.

In response to the request in an embodiment archival data storage system provides a job identifier job id that is assigned to the job in the following response. The response provides in this example a path to the storage location where the retrieved data will be stored.

At any given point in time the archival data storage system may have many jobs pending for various data operations. In some embodiments the archival data storage system may employ job planning and optimization techniques such as batch processing load balancing job coalescence and the like to optimize system metrics such as cost performance scalability and the like. In some embodiments the timing of the actual data retrieval depends on factors such as the size of the retrieved data the system load and capacity active status of storage devices and the like. For example in some embodiments at least some data storage devices in an archival data storage system may be activated or inactivated according to a power management schedule for example to reduce operational costs. Thus retrieval of data stored in a currently active storage device such as a rotating hard drive may be faster than retrieval of data stored in a currently inactive storage device such as a spinned down hard drive .

In an embodiment when a data retrieval job is completed the retrieved data is stored in a staging data store and made available for customer download. In some embodiments a customer is notified of the change in status of a job by a configurable notification service. In other embodiments a customer may learn of the status of a job by polling the system using a job identifier. The following HTTP GET request may be used in an embodiment to download data that is retrieved by a job identified by job id using a download path that has been previously provided.

In response to the GET request in an illustrative embodiment archival data storage system may provide the retrieved data in the following HTTP response with a tree hash of the data for verification purposes.

In an embodiment a customer may request the deletion of a data object stored in an archival data storage system by specifying a data object identifier associated with the data object. For example in an illustrative embodiment a data object with data object identifier dataObjectId may be deleted using the following HTTP request. In another embodiment a customer may request the deletion of multiple data objects such as those associated with a particular logical data container.

In various embodiments data objects may be deleted in response to a customer request or may be deleted automatically according to a user specified or default expiration date. In some embodiments data objects may be rendered inaccessible to customers upon an expiration time but remain recoverable during a grace period beyond the expiration time. In various embodiments the grace period may be based on configurable information such as customer configuration service level agreement terms and the like. In some embodiments a customer may be provided the abilities to query or receive notifications for pending data deletions and or cancel one or more of the pending data deletions. For example in one embodiment a customer may set up notification configurations associated with a logical data container such that the customer will receive notifications of certain events pertinent to the logical data container. Such events may include the completion of a data retrieval job request the completion of metadata request deletion of data objects or logical data containers and the like.

In an embodiment archival data storage system also provides metadata APIs for retrieving and managing metadata such as metadata associated with logical data containers. In various embodiments such requests may be handled asynchronously where results are returned later or synchronously where results are returned immediately .

Still referring to in an embodiment at least some of the API requests discussed above are handled by API request handler as part of front end . For example API request handler may decode and or parse an incoming API request to extract information such as uniform resource identifier URI requested action and associated parameters identity information data object identifiers and the like. In addition API request handler invoke other services described below where necessary to further process the API request.

In an embodiment front end includes an authentication service that may be invoked for example by API handler to authenticate an API request. For example in some embodiments authentication service may verify identity information submitted with the API request such as username and password Internet Protocol IP address cookies digital certificate digital signature and the like. In other embodiments authentication service may require the customer to provide additional information or perform additional steps to authenticate the request such as required in a multifactor authentication scheme under a challenge response authentication protocol and the like.

In an embodiment front end includes an authorization service that may be invoked for example by API handler to determine whether a requested access is permitted according to one or more policies determined to be relevant to the request. For example in one embodiment authorization service verifies that a requested access is directed to data objects contained in the requestor s own logical data containers or which the requester is otherwise authorized to access. In some embodiments authorization service or other services of front end may check the validity and integrity of a data request based at least in part on information encoded in the request such as validation information encoded by a data object identifier.

In an embodiment front end includes a metering service that monitors service usage information for each customer such as data storage space used number of data objects stored data requests processed and the like. In an embodiment front end also includes accounting service that performs accounting and billing related functionalities based for example on the metering information collected by the metering service customer account information and the like. For example a customer may be charged a fee based on the storage space used by the customer size and number of the data objects types and number of requests submitted customer account type service level agreement the like.

In an embodiment front end batch processes some or all incoming requests. For example front end may wait until a certain number of requests has been received before processing e.g. authentication authorization accounting and the like the requests. Such a batch processing of incoming requests may be used to gain efficiency.

In some embodiments front end may invoke services provided by other subsystems of the archival data storage system to further process an API request. For example front end may invoke services in metadata plane to fulfill metadata requests. For another example front end may stream data in and out of control plane for direct I O for data storage and retrieval requests respectively.

Referring now to control plane for direct I O illustrated in in various embodiments control plane for direct I O provides services that create track and manage jobs created as a result of customer requests. As discussed above a job refers to a customer initiated activity that may be performed asynchronously to the initiating request such as data retrieval storage metadata queries or the like. In an embodiment control plane for direct I O includes a job tracker that is configured to create job records or entries corresponding to customer requests such as those received from API request handler and monitor the execution of the jobs. In various embodiments a job record may include information related to the execution of a job such as a customer account identifier job identifier data object identifier reference to payload data cache described below job status data validation information and the like. In some embodiments job tracker may collect information necessary to construct a job record from multiple requests. For example when a large amount of data is requested to be stored data upload may be broken into multiple requests each uploading a portion of the data. In such a case job tracker may maintain information to keep track of the upload status to ensure that all data parts have been received before a job record is created. In some embodiments job tracker also obtains a data object identifier associated with the data to be stored and provides the data object identifier for example to a front end service to be returned to a customer. In an embodiment such data object identifier may be obtained from data plane services such as storage node manager storage node registrar and the like described below.

In some embodiments control plane for direct I O includes a job tracker store for storing job entries or records. In various embodiments job tracker store may be implemented by a NoSQL data management system such as a key value data store a relational database management system RDBMS or any other data storage system. In some embodiments data stored in job tracker store may be partitioned to enable fast enumeration of jobs that belong to a specific customer facilitate efficient bulk record deletion parallel processing by separate instances of a service and the like. For example job tracker store may implement tables that are partitioned according to customer account identifiers and that use job identifiers as range keys. In an embodiment job tracker store is further sub partitioned based on time such as job expiration time to facilitate job expiration and cleanup operations. In an embodiment transactions against job tracker store may be aggregated to reduce the total number of transactions. For example in some embodiments a job tracker may perform aggregate multiple jobs corresponding to multiple requests into one single aggregated job before inserting it into job tracker store .

In an embodiment job tracker is configured to submit the job for further job scheduling and planning for example by services in common control plane . Additionally job tracker may be configured to monitor the execution of jobs and update corresponding job records in job tracker store as jobs are completed. In some embodiments job tracker may be further configured to handle customer queries such as job status queries. In some embodiments job tracker also provides notifications of job status changes to customers or other services of the archival data storage system. For example when a data retrieval job is completed job tracker may cause a customer to be notified for example using a notification service that data is available for download. As another example when a data storage job is completed job tracker may notify a cleanup agent to remove payload data associated with the data storage job from a transient payload data cache described below.

In an embodiment control plane for direct I O includes a payload data cache for providing transient data storage services for payload data transiting between data plane and front end . Such data includes incoming data pending storage and outgoing data pending customer download. As used herein transient data store is used interchangeably with temporary or staging data store to refer to a data store that is used to store data objects before they are stored in an archival data storage described herein or to store data objects that are retrieved from the archival data storage. A transient data store may provide volatile or non volatile durable storage. In most embodiments while potentially usable for persistently storing data a transient data store is intended to store data for a shorter period of time than an archival data storage system and may be less cost effective than the data archival storage system described herein. In one embodiment transient data storage services provided for incoming and outgoing data may be differentiated. For example data storage for the incoming data which is not yet persisted in archival data storage may provide higher reliability and durability than data storage for outgoing retrieved data which is already persisted in archival data storage. In another embodiment transient storage may be optional for incoming data that is incoming data may be stored directly in archival data storage without being stored in transient data storage such as payload data cache for example when there is the system has sufficient bandwidth and or capacity to do so.

In an embodiment control plane for direct I O also includes a cleanup agent that monitors job tracker store and or payload data cache and removes data that is no longer needed. For example payload data associated with a data storage request may be safely removed from payload data cache after the data is persisted in permanent storage e.g. data plane . On the reverse path data staged for customer download may be removed from payload data cache after a configurable period of time e.g. 30 days since the data is staged or after a customer indicates that the staged data is no longer needed.

In some embodiments cleanup agent removes a job record from job tracker store when the job status indicates that the job is complete or aborted. As discussed above in some embodiments job tracker store may be partitioned to enable to enable faster cleanup. In one embodiment where data is partitioned by customer account identifiers cleanup agent may remove an entire table that stores jobs for a particular customer account when the jobs are completed instead of deleting individual jobs one at a time. In another embodiment where data is further sub partitioned based on job expiration time cleanup agent may bulk delete a whole partition or table of jobs after all the jobs in the partition expire. In other embodiments cleanup agent may receive instructions or control messages such as indication that jobs are completed from other services such as job tracker that cause the cleanup agent to remove job records from job tracker store and or payload data cache .

Referring now to common control plane illustrated in . In various embodiments common control plane provides a queue based load leveling service to dampen peak to average load levels jobs coming from control plane for I O and to deliver manageable workload to data plane . In an embodiment common control plane includes a job request queue for receiving jobs created by job tracker in control plane for direct I O described above a storage node manager job store from which services from data plane e.g. storage node managers pick up work to execute and a request balancer for transferring job items from job request queue to storage node manager job store in an intelligent manner.

In an embodiment job request queue provides a service for inserting items into and removing items from a queue e.g. first in first out FIFO or first in last out FILO a set or any other suitable data structure. Job entries in the job request queue may be similar to or different from job records stored in job tracker store described above.

In an embodiment common control plane also provides a durable high efficiency job store storage node manager job store that allows services from data plane e.g. storage node manager anti entropy watcher to perform job planning optimization check pointing and recovery. For example in an embodiment storage node manager job store allows the job optimization such as batch processing operation coalescing and the like by supporting scanning querying sorting or otherwise manipulating and managing job items stored in storage node manager job store . In an embodiment a storage node manager scans incoming jobs and sort the jobs by the type of data operation e.g. read write or delete storage locations e.g. volume disk customer account identifier and the like. The storage node manager may then reorder coalesce group in batches or otherwise manipulate and schedule the jobs for processing. For example in one embodiment the storage node manager may batch process all the write operations before all the read and delete operations. In another embodiment the storage node manager may perform operation coalescing. For another example the storage node manager may coalesce multiple retrieval jobs for the same object into one job or cancel a storage job and a deletion job for the same data object where the deletion job comes after the storage job.

In an embodiment storage node manager job store is partitioned for example based on job identifiers so as to allow independent processing of multiple storage node managers and to provide even distribution of the incoming workload to all participating storage node managers . In various embodiments storage node manager job store may be implemented by a NoSQL data management system such as a key value data store a RDBMS or any other data storage system.

In an embodiment request balancer provides a service for transferring job items from job request queue to storage node manager job store so as to smooth out variation in workload and to increase system availability. For example request balancer may transfer job items from job request queue at a lower rate or at a smaller granularity when there is a surge in job requests coming into the job request queue and vice versa when there is a lull in incoming job requests so as to maintain a relatively sustainable level of workload in the storage node manager store . In some embodiments such sustainable level of workload is around the same or below the average workload of the system.

In an embodiment job items that are completed are removed from storage node manager job store and added to the job result queue . In an embodiment data plane services e.g. storage node manager are responsible for removing the job items from the storage node manager job store and adding them to job result queue . In some embodiments job request queue is implemented in a similar manner as job request queue discussed above.

Referring now to data plane illustrated in . In various embodiments data plane provides services related to long term archival data storage retrieval and deletion data management and placement anti entropy operations and the like. In various embodiments data plane may include any number and type of storage entities such as data storage devices such as tape drives hard disk drives solid state devices and the like storage nodes or servers datacenters and the like. Such storage entities may be physical virtual or any abstraction thereof e.g. instances of distributed storage and or computing systems and may be organized into any topology including hierarchical or tiered topologies. Similarly the components of the data plane may be dispersed local or any combination thereof. For example various computing or storage components may be local or remote to any number of datacenters servers or data storage devices which in turn may be local or remote relative to one another. In various embodiments physical storage entities may be designed for minimizing power and cooling costs by controlling the portions of physical hardware that are active e.g. the number of hard drives that are actively rotating . In an embodiment physical storage entities implement techniques such as Shingled Magnetic Recording SMR to increase storage capacity.

In an environment illustrated by one or more storage node managers each controls one or more storage nodes by sending and receiving data and control messages. Each storage node in turn controls a potentially large collection of data storage devices such as hard disk drives. In various embodiments a storage node manager may communicate with one or more storage nodes and a storage node may communicate with one or more storage node managers . In an embodiment storage node managers are implemented by one or more computing devices that are capable of performing relatively complex computations such as digest computation data encoding and decoding job planning and optimization and the like. In some embodiments storage nodes are implemented by one or more computing devices with less powerful computation capabilities than storage node managers . Further in some embodiments the storage node manager may not be included in the data path. For example data may be transmitted from the payload data cache directly to the storage nodes or from one or more storage nodes to the payload data cache . In this way the storage node manager may transmit instructions to the payload data cache and or the storage nodes without receiving the payloads directly from the payload data cache and or storage nodes . In various embodiments a storage node manager may send instructions or control messages to any other components of the archival data storage system described herein to direct the flow of data.

In an embodiment a storage node manager serves as an entry point for jobs coming into and out of data plane by picking job items from common control plane e.g. storage node manager job store retrieving staged data from payload data cache and performing necessary data encoding for data storage jobs and requesting appropriate storage nodes to store retrieve or delete data. Once the storage nodes finish performing the requested data operations the storage node manager may perform additional processing such as data decoding and storing retrieved data in payload data cache for data retrieval jobs and update job records in common control plane e.g. removing finished jobs from storage node manager job store and adding them to job result queue .

In an embodiment storage node manager performs data encoding according to one or more data encoding schemes before data storage to provide data redundancy security and the like. Such data encoding schemes may include encryption schemes redundancy encoding schemes such as erasure encoding redundant array of independent disks RAID encoding schemes replication and the like. Likewise in an embodiment storage node managers performs corresponding data decoding schemes such as decryption erasure decoding and the like after data retrieval to restore the original data.

As discussed above in connection with storage node manager job store storage node managers may implement job planning and optimizations such as batch processing operation coalescing and the like to increase efficiency. In some embodiments jobs are partitioned among storage node managers so that there is little or no overlap between the partitions. Such embodiments facilitate parallel processing by multiple storage node managers for example by reducing the probability of racing or locking.

In various embodiments data plane is implemented to facilitate data integrity. For example storage entities handling bulk data flows such as storage nodes managers and or storage nodes may validate the digest of data stored or retrieved check the error detection code to ensure integrity of metadata and the like.

In various embodiments data plane is implemented to facilitate scalability and reliability of the archival data storage system. For example in one embodiment storage node managers maintain no or little internal state so that they can be added removed or replaced with little adverse impact. In one embodiment each storage device is a self contained and self describing storage unit capable of providing information about data stored thereon. Such information may be used to facilitate data recovery in case of data loss. Furthermore in one embodiment each storage node is capable of collecting and reporting information about the storage node including the network location of the storage node and storage information of connected storage devices to one or more storage node registrars and or storage node registrar stores . In some embodiments storage nodes perform such self reporting at system start up time and periodically provide updated information. In various embodiments such a self reporting approach provides dynamic and up to date directory information without the need to maintain a global namespace key map or index which can grow substantially as large amounts of data objects are stored in the archival data system.

In an embodiment data plane may also include one or more storage node registrars that provide directory information for storage entities and data stored thereon data placement services and the like. Storage node registrars may communicate with and act as a front end service to one or more storage node registrar stores which provide storage for the storage node registrars . In various embodiments storage node registrar store may be implemented by a NoSQL data management system such as a key value data store a RDBMS or any other data storage system. In some embodiments storage node registrar stores may be partitioned to enable parallel processing by multiple instances of services. As discussed above in an embodiment information stored at storage node registrar store is based at least partially on information reported by storage nodes themselves.

In some embodiments storage node registrars provide directory service for example to storage node managers that want to determine which storage nodes to contact for data storage retrieval and deletion operations. For example given a volume identifier provided by a storage node manager storage node registrars may provide based on a mapping maintained in a storage node registrar store a list of storage nodes that host volume components corresponding to the volume identifier. Specifically in one embodiment storage node registrar store stores a mapping between a list of identifiers of volumes or volume components and endpoints such as Domain Name System DNS names of storage nodes that host the volumes or volume components.

As used herein a volume refers to a logical storage space within a data storage system in which data objects may be stored. A volume may be identified by a volume identifier. A volume may reside in one physical storage device e.g. a hard disk or span across multiple storage devices. In the latter case a volume comprises a plurality of volume components each residing on a different storage device. As used herein a volume component refers a portion of a volume that is physically stored in a storage entity such as a storage device. Volume components for the same volume may be stored on different storage entities. In one embodiment when data is encoded by a redundancy encoding scheme e.g. erasure coding scheme RAID replication each encoded data component or shard may be stored in a different volume component to provide fault tolerance and isolation. In some embodiments a volume component is identified by a volume component identifier that includes a volume identifier and a shard slot identifier. As used herein a shard slot identifies a particular shard row or stripe of data in a redundancy encoding scheme. For example in one embodiment a shard slot corresponds to an erasure coding matrix row. In some embodiments storage node registrar store also stores information about volumes or volume components such as total used and free space number of data objects stored and the like.

In some embodiments data plane also includes a storage allocator for allocating storage space e.g. volumes on storage nodes to store new data objects based at least in part on information maintained by storage node registrar store to satisfy data isolation and fault tolerance constraints. In some embodiments storage allocator requires manual intervention.

In some embodiments data plane also includes an anti entropy watcher for detecting entropic effects and initiating anti entropy correction routines. For example anti entropy watcher may be responsible for monitoring activities and status of all storage entities such as storage nodes reconciling live or actual data with maintained data and the like. In various embodiments entropic effects include but are not limited to performance degradation due to data fragmentation resulting from repeated write and rewrite cycles hardware wear e.g. of magnetic media data unavailability and or data loss due to hardware software malfunction environmental factors physical destruction of hardware random chance or other causes. Anti entropy watcher may detect such effects and in some embodiments may preemptively and or reactively institute anti entropy correction routines and or policies.

In an embodiment anti entropy watcher causes storage nodes to perform periodic anti entropy scans on storage devices connected to the storage nodes. Anti entropy watcher may also inject requests in job request queue and subsequently job result queue to collect information recover data and the like. In some embodiments anti entropy watcher may perform scans for example on cold index store described below and storage nodes to ensure referential integrity.

In an embodiment information stored at storage node registrar store is used by a variety of services such as storage node registrar storage allocator anti entropy watcher and the like. For example storage node registrar may provide data location and placement services e.g. to storage node managers during data storage retrieval and deletion. For example given the size of a data object to be stored and information maintained by storage node registrar store a storage node registrar may determine where e.g. volume to store the data object and provides an indication of the storage location of the data object which may be used to generate a data object identifier associated with the data object. As another example in an embodiment storage allocator uses information stored in storage node registrar store to create and place volume components for new volumes in specific storage nodes to satisfy isolation and fault tolerance constraints. As yet another example in an embodiment anti entropy watcher uses information stored in storage node registrar store to detect entropic effects such as data loss hardware failure and the like.

In some embodiments data plane also includes an orphan cleanup data store which is used to track orphans in the storage system. As used herein an orphan is a stored data object that is not referenced by any external entity. In various embodiments orphan cleanup data store may be implemented by a NoSQL data management system such as a key value data store an RDBMS or any other data storage system. In some embodiments storage node registrars stores object placement information in orphan cleanup data store . Subsequently information stored in orphan cleanup data store may be compared for example by an anti entropy watcher with information maintained in metadata plane . If an orphan is detected in some embodiments a request is inserted in the common control plane to delete the orphan.

Referring now to metadata plane illustrated in . In various embodiments metadata plane provides information about data objects stored in the system for inventory and accounting purposes to satisfy customer metadata inquiries and the like. In the illustrated embodiment metadata plane includes a metadata manager job store which stores information about executed transactions based on entries from job result queue in common control plane . In various embodiments metadata manager job store may be implemented by a NoSQL data management system such as a key value data store a RDBMS or any other data storage system. In some embodiments metadata manager job store is partitioned and sub partitioned for example based on logical data containers to facilitate parallel processing by multiple instances of services such as metadata manager .

In the illustrative embodiment metadata plane also includes one or more metadata managers for generating a cold index of data objects e.g. stored in cold index store based on records in metadata manager job store . As used herein a cold index refers to an index that is updated infrequently. In various embodiments a cold index is maintained to reduce cost overhead. In some embodiments multiple metadata managers may periodically read and process records from different partitions in metadata manager job store in parallel and store the result in a cold index store .

In some embodiments cold index store may be implemented by a reliable and durable data storage service. In some embodiments cold index store is configured to handle metadata requests initiated by customers. For example a customer may issue a request to list all data objects contained in a given logical data container. In response to such a request cold index store may provide a list of identifiers of all data objects contained in the logical data container based on information maintained by cold index . In some embodiments an operation may take a relative long period of time and the customer may be provided a job identifier to retrieve the result when the job is done. In other embodiments cold index store is configured to handle inquiries from other services for example from front end for inventory accounting and billing purposes.

In some embodiments metadata plane may also include a container metadata store that stores information about logical data containers such as container ownership policies usage and the like. Such information may be used for example by front end services to perform authorization metering accounting and the like. In various embodiments container metadata store may be implemented by a NoSQL data management system such as a key value data store a RDBMS or any other data storage system.

As described herein in various embodiments the archival data storage system described herein is implemented to be efficient and scalable. For example in an embodiment batch processing and request coalescing is used at various stages e.g. front end request handling control plane job request handling data plane data request handling to improve efficiency. For another example in an embodiment processing of metadata such as jobs requests and the like are partitioned so as to facilitate parallel processing of the partitions by multiple instances of services.

In an embodiment data elements stored in the archival data storage system such as data components volumes described below are self describing so as to avoid the need for a global index data structure. For example in an embodiment data objects stored in the system may be addressable by data object identifiers that encode storage location information. For another example in an embodiment volumes may store information about which data objects are stored in the volume and storage nodes and devices storing such volumes may collectively report their inventory and hardware information to provide a global view of the data stored in the system such as evidenced by information stored in storage node registrar store . In such an embodiment the global view is provided for efficiency only and not required to locate data stored in the system.

In various embodiments the archival data storage system described herein is implemented to improve data reliability and durability. For example in an embodiment a data object is redundantly encoded into a plurality of data components and stored across different data storage entities to provide fault tolerance. For another example in an embodiment data elements have multiple levels of integrity checks. In an embodiment parent child relations always have additional information to ensure full referential integrity. For example in an embodiment bulk data transmission and storage paths are protected by having the initiator pre calculate the digest on the data before transmission and subsequently supply the digest with the data to a receiver. The receiver of the data transmission is responsible for recalculation comparing and then acknowledging to the sender that includes the recalculated the digest. Such data integrity checks may be implemented for example by front end services transient data storage services data plane storage entities and the like described above.

Each storage node manager server rack may have a storage node manager rack connection to an interconnect used to connect to the interconnection network . In some embodiments the connection is implemented using a network switch that may include a top of rack Ethernet switch or any other type of network switch. In various embodiments interconnect is used to enable high bandwidth and low latency bulk data transfers. For example interconnect may include a Clos network a fat tree interconnect an Asynchronous Transfer Mode ATM network a Fast or Gigabit Ethernet and the like.

In various embodiments the bandwidth of storage node manager rack connection may be configured to enable high bandwidth and low latency communications between storage node managers and storage nodes located within the same or different data centers. For example in an embodiment the storage node manager rack connection has a bandwidth of 10 Gigabit per second Gbps .

In some embodiments each datacenter may also include one or more storage node server racks where each server rack hosts one or more servers that collectively provide the functionalities of a number of storage nodes such as described in connection with . Configuration parameters such as number of storage nodes per rack number of storage node racks ration between storage node managers and storage nodes and the like may be determined based on factors such as cost scalability redundancy and performance requirements hardware and software resources and the like. For example in one embodiment there are 3 storage nodes per storage node server rack 30 80 racks per data center and a storage nodes storage node manager ratio of 10 to 1.

Each storage node server rack may have a storage node rack connection to an interconnection network switch used to connect to the interconnection network . In some embodiments the connection is implemented using a network switch that may include a top of rack Ethernet switch or any other type of network switch. In various embodiments the bandwidth of storage node rack connection may be configured to enable high bandwidth and low latency communications between storage node managers and storage nodes located within the same or different data centers. In some embodiments a storage node rack connection has a higher bandwidth than a storage node manager rack connection . For example in an embodiment the storage node rack connection has a bandwidth of 20 Gbps while a storage node manager rack connection has a bandwidth of 10 Gbps.

In some embodiments datacenters including storage node managers and storage nodes communicate via connection with other computing resources services such as payload data cache storage node manager job store storage node registrar storage node registrar store orphan cleanup data store metadata manager job store and the like as described in connection with .

In some embodiments one or more datacenters may be connected via inter datacenter connection . In some embodiments connections and may be configured to achieve effective operations and use of hardware resources. For example in an embodiment connection has a bandwidth of 30 100 Gbps per datacenter and inter datacenter connection has a bandwidth of 100 250 Gbps.

In some embodiments control plane services and metadata plane services as described in connection with may be hosted by one or more server racks . Such services may include job tracker metadata manager cleanup agent job request balancer and other services. In some embodiments such services include services that do not handle frequent bulk data transfers. Finally components described herein may communicate via connection with other computing resources services such as payload data cache job tracker store metadata manager job store and the like as described in connection with .

In an embodiment process includes receiving a data storage request to store archival data such as a document a video or audio file or the like. Such a data storage request may include payload data and metadata such as size and digest of the payload data user identification information e.g. user name account identifier and the like a logical data container identifier and the like. In some embodiments process may include receiving multiple storage requests each including a portion of larger payload data. In other embodiments a storage request may include multiple data objects to be uploaded. In an embodiment step of process is implemented by a service such as API request handler of front end as described in connection with .

In an embodiment process includes processing the storage request upon receiving the request. Such processing may include for example verifying the integrity of data received authenticating the customer authorizing requested access against access control policies performing meter and accounting related activities and the like. In an embodiment such processing may be performed by services of front end such as described in connection with . In an embodiment such a request may be processed in connection with other requests for example in batch mode.

In an embodiment process includes storing the data associated with the storage request in a staging data store. Such staging data store may include a transient data store such as provided by payload data cache as described in connection with . In some embodiments only payload data is stored in the staging store. In other embodiments metadata related to the payload data may also be stored in the staging store. In an embodiment data integrity is validated e.g. based on a digest before being stored at a staging data store.

In an embodiment process includes providing a data object identifier associated with the data to be stored for example in a response to the storage request. As described above a data object identifier may be used by subsequent requests to retrieve delete or otherwise reference data stored. In an embodiment a data object identifier may encode storage location information that may be used to locate the stored data object payload validation information such as size digest timestamp and the like that may be used to validate the integrity of the payload data metadata validation information such as error detection codes that may be used to validate the integrity of metadata such as the data object identifier itself and information encoded in the data object identifier and the like. In an embodiment a data object identifier may also encode information used to validate or authorize subsequent customer requests. For example a data object identifier may encode the identifier of the logical data container that the data object is stored in. In a subsequent request to retrieve this data object the logical data container identifier may be used to determine whether the requesting entity has access to the logical data container and hence the data objects contained therein. In some embodiments the data object identifier may encode information based on information supplied by a customer e.g. a global unique identifier GUID for the data object and the like and or information collected or calculated by the system performing process e.g. storage location information . In some embodiments generating a data object identifier may include encrypting some or all of the information described above using a cryptographic private key. In some embodiments the cryptographic private key may be periodically rotated. In some embodiments a data object identifier may be generated and or provided at a different time than described above. For example a data object identifier may be generated and or provided after a storage job described below is created and or completed.

In an embodiment providing a data object identifier may include determining a storage location for the before the data is actually stored there. For example such determination may be based at least in part on inventory information about existing data storage entities such as operational status e.g. active or inactive available storage space data isolation requirement and the like. In an environment such as environment illustrated by such determination may be implemented by a service such as storage node registrar as described above in connection with . In some embodiments such determination may include allocating new storage space e.g. volume on one or more physical storage devices by a service such as storage allocator as described in connection with .

In an embodiment a storage location identifier may be generated to represent the storage location determined above. Such a storage location identifier may include for example a volume reference object which comprises a volume identifier component and data object identifier component. The volume reference component may identify the volume the data is stored on and the data object identifier component may identify where in the volume the data is stored. In general the storage location identifier may comprise components that identify various levels within a logical or physical data storage topology such as a hierarchy in which data is organized. In some embodiments the storage location identifier may point to where actual payload data is stored or a chain of reference to where the data is stored.

In an embodiments a data object identifier encodes a digest e.g. a hash of at least a portion of the data to be stored such as the payload data. In some embodiments the digest may be based at least in part on a customer provided digest. In other embodiments the digest may be calculated from scratch based on the payload data.

In an embodiment process includes creating a storage job for persisting data to a long term data store and scheduling the storage job for execution. In environment as described in connection with steps and may be implemented at least in part by components of control plane for direct I O and common control plane as described above. Specifically in an embodiment job tracker creates a job record and stores the job record in job tracker store . As described above job tracker may perform batch processing to reduce the total number of transactions against job tracker store . Additionally job tracker store may be partitioned or otherwise optimized to facilitate parallel processing cleanup operations and the like. A job record as described above may include job related information such as a customer account identifier job identifier storage location identifier reference to data stored in payload data cache job status job creation and or expiration time and the like. In some embodiments a storage job may be created before a data object identifier is generated and or provided. For example a storage job identifier instead of or in addition to a data object identifier may be provided in response to a storage request at step above.

In an embodiment scheduling the storage job for execution includes performing job planning and optimization such as queue based load leveling or balancing job partitioning and the like as described in connection with common control plane of . For example in an embodiment job request balancer transfers job items from job request queue to storage node manager job store according to a scheduling algorithm so as to dampen peak to average load levels jobs coming from control plane for I O and to deliver manageable workload to data plane . As another example storage node manager job store may be partitioned to facilitate parallel processing of the jobs by multiple workers such as storage node managers . As yet another example storage node manager job store may provide querying sorting and other functionalities to facilitate batch processing and other job optimizations.

In an embodiment process includes selecting the storage job for execution for example by a storage node manager from storage node manager job stored as described in connection with . The storage job may be selected with other jobs for batch processing or otherwise selected as a result of job planning and optimization described above.

In an embodiment process includes obtaining data from a staging store such as payload data cache described above in connection with . In some embodiments the integrity of the data may be checked for example by verifying the size digest an error detection code and the like.

In an embodiment process includes obtaining one or more data encoding schemes such as an encryption scheme a redundancy encoding scheme such as erasure encoding redundant array of independent disks RAID encoding schemes replication and the like. In some embodiments such encoding schemes evolve to adapt to different requirements. For example encryption keys may be rotated periodically and stretch factor of an erasure coding scheme may be adjusted over time to different hardware configurations redundancy requirements and the like.

In an embodiment process includes encoding with the obtained encoding schemes. For example in an embodiment data is encrypted and the encrypted data is erasure encoded. In an embodiment storage node managers described in connection with may be configured to perform the data encoding described herein. In an embodiment application of such encoding schemes generates a plurality of encoded data components or shards which may be stored across different storage entities such as storage devices storage nodes datacenters and the like to provide fault tolerance. In an embodiment where data may comprise multiple parts such as in the case of a multi part upload each part may be encoded and stored as described herein.

In an embodiment process includes determining the storage entities for such encoded data components. For example in an environment illustrated by a storage node manager may determine the plurality of storage nodes to store the encoded data components by querying a storage node registrar using a volume identifier. Such a volume identifier may be part of a storage location identifier associated with the data to be stored. In response to the query with a given volume identifier in an embodiment storage node registrar returns a list of network locations including endpoints DNS names IP addresses and the like of storage nodes to store the encoded data components. As described in connection with storage node registrar may determine such a list based on self reported and dynamically provided and or updated inventory information from storage nodes themselves. In some embodiments such determination is based on data isolation fault tolerance load balancing power conservation data locality and other considerations. In some embodiments storage registrar may cause new storage space to be allocated for example by invoking storage allocator as described in connection with .

In an embodiment process includes causing storage of the encoded data component s at the determined storage entities. For example in an environment illustrated by a storage node manager may request each of the storage nodes determined above to store a data component at a given storage location. Each of the storage nodes upon receiving the storage request from storage node manager to store a data component may cause the data component to be stored in a connected storage device. In some embodiments at least a portion of the data object identifier is stored with all or some of the data components in either encoded or unencoded form. For example the data object identifier may be stored in the header of each data component and or in a volume component index stored in a volume component. In some embodiments a storage node may perform batch processing or other optimizations to process requests from storage node managers .

In an embodiment a storage node sends an acknowledgement to the requesting storage node manager indicating whether data is stored successfully. In some embodiments a storage node returns an error message when for some reason the request cannot be fulfilled. For example if a storage node receives two requests to store to the same storage location one or both requests may fail. In an embodiment a storage node performs validation checks prior to storing the data and returns an error if the validation checks fail. For example data integrity may be verified by checking an error detection code or a digest. As another example storage node may verify for example based on a volume index that the volume identified by a storage request is stored by the storage node and or that the volume has sufficient space to store the data component.

In some embodiments data storage is considered successful when storage node manager receives positive acknowledgement from at least a subset a storage quorum of requested storage nodes . In some embodiments a storage node manager may wait until the receipt of a quorum of acknowledgement before removing the state necessary to retry the job. Such state information may include encoded data components for which an acknowledgement has not been received. In other embodiments to improve the throughput a storage node manager may remove the state necessary to retry the job before receiving a quorum of acknowledgement.

In an embodiment process includes updating metadata information including for example metadata maintained by data plane such as index and storage space information for a storage device mapping information stored at storage node registrar store and the like metadata maintained by control planes and such as job related information metadata maintained by metadata plane such as a cold index and the like. In various embodiments some of such metadata information may be updated via batch processing and or on a periodic basis to reduce performance and cost impact. For example in data plane information maintained by storage node registrar store may be updated to provide additional mapping of the volume identifier of the newly stored data and the storage nodes on which the data components are stored if such a mapping is not already there. For another example volume index on storage devices may be updated to reflect newly added data components.

In common control plane job entries for completed jobs may be removed from storage node manager job store and added to job result queue as described in connection with . In control plane for direct I O statuses of job records in job tracker store may be updated for example by job tracker which monitors the job result queue . In various embodiments a job that fails to complete may be retried for a number of times. For example in an embodiment a new job may be created to store the data at a different location. As another example an existing job record e.g. in storage node manager job store job tracker store and the like may be updated to facilitate retry of the same job.

In metadata plane metadata may be updated to reflect the newly stored data. For example completed jobs may be pulled from job result queue into metadata manager job store and batch processed by metadata manager to generate an updated index such as stored in cold index store . For another example customer information may be updated to reflect changes for metering and accounting purposes.

Finally in some embodiments once a storage job is completed successfully job records payload data and other data associated with a storage job may be removed for example by a cleanup agent as described in connection with . In some embodiments such removal may be processed by batch processing parallel processing or the like.

In an embodiment process includes receiving a data retrieval request to retrieve data such as stored by process described above. Such a data retrieval request may include a data object identifier such as provided by step of process described above or any other information that may be used to identify the data to be retrieved.

In an embodiment process includes processing the data retrieval request upon receiving the request. Such processing may include for example authenticating the customer authorizing requested access against access control policies performing meter and accounting related activities and the like. In an embodiment such processing may be performed by services of front end such as described in connection with . In an embodiment such request may be processed in connection with other requests for example in batch mode.

In an embodiment processing the retrieval request may be based at least in part on the data object identifier that is included in the retrieval request. As described above data object identifier may encode storage location information payload validation information such as size creation timestamp payload digest and the like metadata validation information policy information and the like. In an embodiment processing the retrieval request includes decoding the information encoded in the data object identifier for example using a private cryptographic key and using at least some of the decoded information to validate the retrieval request. For example policy information may include access control information that may be used to validate that the requesting entity of the retrieval request has the required permission to perform the requested access. As another example metadata validation information may include an error detection code such as a cyclic redundancy check CRC that may be used to verify the integrity of data object identifier or a component of it.

In an embodiment process includes creating a data retrieval job corresponding to the data retrieval request and providing a job identifier associated with the data retrieval job for example in a response to the data retrieval request. In some embodiments creating a data retrieval job is similar to creating a data storage job as described in connection with step of process illustrated in . For example in an embodiment a job tracker may create a job record that includes at least some information encoded in the data object identifier and or additional information such as a job expiration time and the like and store the job record in job tracker store . As described above job tracker may perform batch processing to reduce the total number of transactions against job tracker store . Additionally job tracker store may be partitioned or otherwise optimized to facilitate parallel processing cleanup operations and the like.

In an embodiment process includes scheduling the data retrieval job created above. In some embodiments scheduling the data retrieval job for execution includes performing job planning and optimization such as described in connection with step of process of . For example the data retrieval job may be submitted into a job queue and scheduled for batch processing with other jobs based at least in part on costs power management schedules and the like. For another example the data retrieval job may be coalesced with other retrieval jobs based on data locality and the like.

In an embodiment process includes selecting the data retrieval job for execution for example by a storage node manager from storage node manager job stored as described in connection with . The retrieval job may be selected with other jobs for batch processing or otherwise selected as a result of job planning and optimization described above.

In an embodiment process includes determining the storage entities that store the encoded data components that are generated by a storage process such as process described above. In an embodiment a storage node manager may determine a plurality of storage nodes to retrieve the encoded data components in a manner similar to that discussed in connection with step of process above. For example such determination may be based on load balancing power conservation efficiency and other considerations.

In an embodiment process includes determining one or more data decoding schemes that may be used to decode retrieved data. Typically such decoding schemes correspond to the encoding schemes applied to the original data when the original data is previously stored. For example such decoding schemes may include decryption with a cryptographic key erasure decoding and the like.

In an embodiment process includes causing retrieval of at least some of the encoded data components from the storage entities determined in step of process . For example in an environment illustrated by a storage node manager responsible for the data retrieval job may request a subset of storage nodes determined above to retrieve their corresponding data components. In some embodiments a minimum number of encoded data components is needed to reconstruct the original data where the number may be determined based at least in part on the data redundancy scheme used to encode the data e.g. stretch factor of an erasure coding . In such embodiments the subset of storage nodes may be selected such that no less than the minimum number of encoded data components is retrieved.

Each of the subset of storage nodes upon receiving a request from storage node manager to retrieve a data component may validate the request for example by checking the integrity of a storage location identifier that is part of the data object identifier verifying that the storage node indeed holds the requested data component and the like. Upon a successful validation the storage node may locate the data component based at least in part on the storage location identifier. For example as described above the storage location identifier may include a volume reference object which comprises a volume identifier component and a data object identifier component where the volume reference component to identify the volume the data is stored and a data object identifier component may identify where in the volume the data is stored. In an embodiment the storage node reads the data component for example from a connected data storage device and sends the retrieved data component to the storage node manager that requested the retrieval. In some embodiments the data integrity is checked for example by verifying the data component identifier or a portion thereof is identical to that indicated by the data component identifier associated with the retrieval job. In some embodiments a storage node may perform batching or other job optimization in connection with retrieval of a data component.

In an embodiment process includes decoding at least the minimum number of the retrieved encoded data components with the one or more data decoding schemes determined at step of process . For example in one embodiment the retrieved data components may be erasure decoded and then decrypted. In some embodiments a data integrity check is performed on the reconstructed data for example using payload integrity validation information encoded in the data object identifier e.g. size timestamp digest . In some cases the retrieval job may fail due to a less than minimum number of retrieved data components failure of data integrity check and the like. In such cases the retrieval job may be retried in a fashion similar to that described in connection with . In some embodiments the original data comprises multiple parts of data and each part is encoded and stored. In such embodiments during retrieval the encoded data components for each part of the data may be retrieved and decoded e.g. erasure decoded and decrypted to form the original part and the decoded parts may be combined to form the original data.

In an embodiment process includes storing reconstructed data in a staging store such as payload data cache described in connection with . In some embodiments data stored in the staging store may be available for download by a customer for a period of time or indefinitely. In an embodiment data integrity may be checked e.g. using a digest before the data is stored in the staging store.

In an embodiment process includes providing a notification of the completion of the retrieval job to the requestor of the retrieval request or another entity or entities otherwise configured to receive such a notification. Such notifications may be provided individually or in batches. In other embodiments the status of the retrieval job may be provided upon a polling request for example from a customer.

In an embodiment process includes receiving a data deletion request to delete data such as stored by process described above. Such a data retrieval request may include a data object identifier such as provided by step of process described above or any other information that may be used to identify the data to be deleted.

In an embodiment process includes processing the data deletion request upon receiving the request. In some embodiments the processing is similar to that for step of process and step of process described above. For example in an embodiment the processing is based at least in part on the data object identifier that is included in the data deletion request.

In an embodiment process includes creating a data retrieval job corresponding to the data deletion request. Such a retrieval job may be created similar to the creation of storage job described in connection with step of process and the creation of the retrieval job described in connection with step of process .

In an embodiment process includes providing an acknowledgement that the data is deleted. In some embodiments such acknowledgement may be provided in response to the data deletion request so as to provide a perception that the data deletion request is handled synchronously. In other embodiments a job identifier associated with the data deletion job may be provided similar to the providing of job identifiers for data retrieval requests.

In an embodiment process includes scheduling the data deletion job for execution. In some embodiments scheduling of data deletion jobs may be implemented similar to that described in connection with step of process and in connection with step of process described above. For example data deletion jobs for closely located data may be coalesced and or batch processed. For another example data deletion jobs may be assigned a lower priority than data retrieval jobs.

In some embodiments data stored may have an associated expiration time that is specified by a customer or set by default. In such embodiments a deletion job may be created and schedule automatically on or near the expiration time of the data. In some embodiments the expiration time may be further associated with a grace period during which data is still available or recoverable. In some embodiments a notification of the pending deletion may be provided before on or after the expiration time.

In some embodiments process includes selecting the data deletion job for execution for example by a storage node manager from storage node manager job stored as described in connection with . The deletion job may be selected with other jobs for batch processing or otherwise selected as a result of job planning and optimization described above.

In some embodiments process includes determining the storage entities for data components that store the data components that are generated by a storage process such as process described above. In an embodiment a storage node manager may determine a plurality of storage nodes to retrieve the encoded data components in a manner similar to that discussed in connection with step of process described above.

In some embodiments process includes causing the deletion of at least some of the data components. For example in an environment illustrated by a storage node manager responsible for the data deletion job may identify a set of storage nodes that store the data components for the data to be deleted and requests at least a subset of those storage nodes to delete their respective data components. Each of the subset of storage node upon receiving a request from storage node manager to delete a data component may validate the request for example by checking the integrity of a storage location identifier that is part of the data object identifier verifying that the storage node indeed holds the requested data component and the like. Upon a successful validation the storage node may delete the data component from a connected storage device and sends an acknowledgement to storage node manager indicating whether the operation was successful. In an embodiment multiple data deletion jobs may be executed in a batch such that data objects located close together may be deleted as a whole. In some embodiments data deletion is considered successful when storage node manager receives positive acknowledgement from at least a subset of storage nodes . The size of the subset may be configured to ensure that data cannot be reconstructed later on from undeleted data components. Failed or incomplete data deletion jobs may be retried in a manner similar to the retrying of data storage jobs and data retrieval jobs described in connection with process and process respectively.

In an embodiment process includes updating metadata information such as that described in connection with step of process . For example storage nodes executing the deletion operation may update storage information including index free space information and the like. In an embodiment storage nodes may provide updates to storage node registrar or storage node registrar store. In various embodiments some of such metadata information may be updated via batch processing and or on a periodic basis to reduce performance and cost impact.

In an embodiment process includes receiving a request to store a data object. In some instances such as in an environment as illustrated by receiving a request may include selecting a job for execution by a storage node manager . Jobs such as storage jobs retrieval jobs deletion jobs may be submitted to a job request queue such as job request queue and provided to storage node managers such as in a storage node manager job store as described in connection with . As described above storage node manager job store may be partitioned so that multiple instances of storage node managers may process jobs in different partitions independently and or in parallel.

In some embodiments selecting a job to execute includes performing job planning and optimization such as by a storage node manager on pending jobs in a partition in the storage node manager job store. As described above in various embodiments a job may have one or more attributes relevant to the execution of the job such as the type of the job e.g. storage retrieval deletion data object identifier including storage location e.g. volume id for a data object size of a data object identifier of a storage location within a transient store where the data object is stored customer account identifier time and the like.

A storage node manager may sort the pending jobs by one or more job attributes described above and perform the jobs according to the sorted order. For example the jobs may be sorted by time and then by type of job such that storage jobs are performed before retrieval or deletion jobs. Such an approach may be desirable to reduce the time that data spends in the transient data store thereby freeing resources in the transient data store and reducing the risk of data loss. Similarly jobs may be sorted by a job priority indicator. Such an indication may be based on a customer account identifier service level agreement and the like and may be configurable by a system administrator a customer or automatically based on system parameters such as system load time of day and the like. For example a customer who pays a higher monthly fee or other rate for the archival data storage service may receive a higher priority for their jobs. As another example jobs may be sorted by data object identifiers such that in some instances operations for the same data objects may be coalesced as described below.

As used herein coalescence refers to the act of combining multiple items into fewer items. As noted a number of jobs may be coalesced into one job to reduce the number of total jobs. In some cases jobs for the same data object may be coalesced. For example two data retrieval jobs for the same data object may be combined into one retrieval job assuming the data object is not updated or removed in between. In some cases jobs for different data objects may be coalesced into one job. For example data storage jobs for multiple small data objects may be coalesced into one job of storing a collection of the small data objects.

In an embodiment jobs may be selected and or processed according to a batch processing schedule. The batch processing schedule may be determined to improve efficiency. For example a batch processing schedule may limit the batch size to N jobs at a time for jobs sorted by time where N is a positive integer. As another example a batch processing schedule may schedule all data storage jobs in a batch before data retrieval jobs in another batch and data deletion jobs in yet another batch. As another example a batch processing schedule may be configured such that all the jobs for a particular volume in a batch before jobs for another volume in another batch and so on. In various embodiments a batch process schedule may be based at least in part on costs data locality a power management schedule and the like. In various embodiments job records may be updated to reflect the job planning and optimizations described herein.

Still referring to in an embodiment process includes obtaining data object to be stored. In an illustrative embodiment obtaining data object includes retrieving the data object from a transient data store such as payload data cache as described in connection with . In other embodiments obtaining data may include receiving or getting data from other sources such as a customer a third party entity and the like.

In an embodiment process includes allocating storage space to store encoded data components. As described above data object may be encoded to provide data redundancy security and the like. For example in an embodiment a data object is encrypted and then erasure encoded. Such data encoding may be performed by storage node manager as described above or by another entity. In various embodiments storage space may be allocated to store the encoded data component on a temporary basis before they are stored in the archival data storage system. For example in an embodiment allocating storage space includes dynamically allocating memory from a memory pool. Once the storage space is allocated encoded data components may be stored in the allocated storage space.

In an embodiment process includes providing each of the encoded data components to be stored at a storage node as part of a data collection. As used herein a data collection refers to a collection of data such as encoded data components. In some embodiments a data collection may include one or more pages. As described in connection with a logical volume may comprise multiple volume components and each volume components may comprise multiple pages. Each page may include multiple encoded data components and other data. In other embodiments the size of a data collection may be lower or higher than a page. In various embodiments the size of a data collection may be fixed or a variable. In some embodiments one or more such data collection or a portion of a data collection may be stored as a whole in a storage device in a write transaction.

In some embodiments providing the encoded data component includes filling a data collection with the encoded data component and other data. In an embodiment such other data includes encoded data components that are generated based on at least another data object for example such as resulting from the processing of another storage request. In another embodiment a data collection may include unencoded data metadata and the like.

As noted encoded data components from multiple data objects may be used to form data collections. shows a diagram illustrating the formation of data collections from encoded data components in accordance with at least one embodiment. In the example data objects and are each encoded into sets of encoded data components A C A C and A C respectively. Data collections and are formed by including one encoded component from each of the above sets of encoded data components. For example data collection includes encoded data components A A and A data collection includes encoded data components B B and B and data collection includes encoded data components C C and C. In this example instead of sending nine storage requests to three storage nodes for each of nine encoded data components three requests may be sent with one of the three data collections to three respective storage node. In some embodiments an entire data collection of encoded data components or a portion thereof may be stored by a storage node as a whole in a storage device.

Referring back to in an embodiment process includes making available storage space allocated to the encoded data components regardless of whether a response is received from storage nodes indicating whether storage is successful. In some embodiments making available storage space includes deallocating memory previously allocated to store the encoded data components. As discussed above to increase throughput in some embodiments storage entities including storage node managers and storage nodes may keep a shallow buffer storage space for storing data such as encoded data components associated with outstanding storage requests because the deeper the buffer is the lesser storage space there is to store data for incoming requests. Thus in one embodiment memory allocated to store encoded data components is freed soon after the encoded data components are provided to storage nodes so that the storage space may be used to store data for a new job.

Storage nodes that receive such storage requests from a storage node manager may perform the requested storage operation and respond with an acknowledgement or response of whether the operation is successful. Typically a storage node manager waits for acknowledgement from a quorum of storage nodes before determining whether the storage job is successful. If a quorum of confirmations is received then the job may be determined to be successfully executed. Otherwise the job may be determined to have failed and attempts may be made to re execute the job. In some embodiments a storage node manager keeps in storage all the encoded data components until a quorum of confirmation is received to avoid regenerating the encoded data components in case the job needs to be retried in event of failure. In some other embodiments storage space for an encoded data component is deallocated as soon as acknowledgement for that particular encoded data component is received that is deallocation is triggered by receipt of the acknowledgment. In an embodiment illustrated by process storage space allocated to the encoded data components is deallocated soon after the encoded data components are sent to the storage nodes regardless of whether an acknowledgement is received. In such an embodiment if there is a failure a new job may be created for example with a new storage location.

Process may include coalescing at least some of the received data requests. In some embodiments coalescing of data requests may be similar to the coalescing of jobs described above in connection with process of . For example a storage node may receive two data retrieval requests from two different storage node managers to retrieve the same data component. In this case the two data retrieval requests may be coalesced into one data retrieval request. As another example write requests for small encoded data components or even data collections such as pages may be coalesced into one write request for a big chunk of data relative to the collective size of the small encoded data components that includes all the small encoded data components or data collections. As yet another example data deletion requests for data located close together may be coalesced into one data deletion request to delete a whole portion of a storage device.

In an embodiment process includes determining a batch processing schedule to process the data requests. In various embodiments batch processing may be used to improve system efficiency. In some embodiments such a batch processing schedule may be determined based at least in part on a power management schedule for one or more storage devices. For example a power management schedule may determine a time period when a storage device is scheduled be available e.g. rotating for hard disk drives to process read write requests. Based on this schedule data requests for this particular storage device may be batch processed during that time period. Other batch processing schedules may be based on data locality type of data operation e.g. read write or delete data size and the like. For example some or all data storage requests may be batch processed before data retrieval requests where the requests may be sorted by volumes storage devices and the like.

Finally in an embodiment process includes processing the data requests according to the batch processing schedule the data requests some of which may be coalesced and providing responses to the data request such as described above in connection with . In some embodiments data validation may be performed before and or after the performance of data requests. In general data integrity may be validated at various points along the data flow path. For example data integrity may be validated before the data is stored or after the data is retrieved from a transient data store. As another example data integrity may be validated before data flows through a storage entity such as storage node manager or a storage node. In various embodiments data integrity validation may use a digest a checksum or any other data validation tools.

The illustrative environment includes at least one application server and a data store . It should be understood that there can be several application servers layers or other elements processes or components which may be chained or otherwise configured which can interact to perform tasks such as obtaining data from an appropriate data store. As used herein the term data store refers to any device or combination of devices capable of storing accessing and retrieving data which may include any combination and number of data servers databases data storage devices and data storage media in any standard distributed or clustered environment. The application server can include any appropriate hardware and software for integrating with the data store as needed to execute aspects of one or more applications for the client device handling a majority of the data access and business logic for an application. The application server provides access control services in cooperation with the data store and is able to generate content such as text graphics audio and or video to be transferred to the user which may be served to the user by the Web server in the form of HTML XML or another appropriate structured language in this example. The handling of all requests and responses as well as the delivery of content between the client device and the application server can be handled by the Web server. It should be understood that the Web and application servers are not required and are merely example components as structured code discussed herein can be executed on any appropriate device or host machine as discussed elsewhere herein.

The data store can include several separate data tables databases or other data storage mechanisms and media for storing data relating to a particular aspect. For example the data store illustrated includes mechanisms for storing production data and user information which can be used to serve content for the production side. The data store also is shown to include a mechanism for storing log data which can be used for reporting analysis or other such purposes. It should be understood that there can be many other aspects that may need to be stored in the data store such as for page image information and to access right information which can be stored in any of the above listed mechanisms as appropriate or in additional mechanisms in the data store . The data store is operable through logic associated therewith to receive instructions from the application server and obtain update or otherwise process data in response thereto. In one example a user might submit a search request for a certain type of item. In this case the data store might access the user information to verify the identity of the user and can access the catalog detail information to obtain information about items of that type. The information then can be returned to the user such as in a results listing on a Web page that the user is able to view via a browser on the user device . Information for a particular item of interest can be viewed in a dedicated page or window of the browser.

Each server typically will include an operating system that provides executable program instructions for the general administration and operation of that server and typically will include a computer readable storage medium e.g. a hard disk random access memory read only memory etc. storing instructions that when executed by a processor of the server allow the server to perform its intended functions. Suitable implementations for the operating system and general functionality of the servers are known or commercially available and are readily implemented by persons having ordinary skill in the art particularly in light of the disclosure herein.

The environment in one embodiment is a distributed computing environment utilizing several computer systems and components that are interconnected via communication links using one or more computer networks or direct connections. However it will be appreciated by those of ordinary skill in the art that such a system could operate equally well in a system having fewer or a greater number of components than are illustrated in . Thus the depiction of the system in should be taken as being illustrative in nature and not limiting to the scope of the disclosure.

The various embodiments further can be implemented in a wide variety of operating environments which in some cases can include one or more user computers computing devices or processing devices which can be used to operate any of a number of applications. User or client devices can include any of a number of general purpose personal computers such as desktop or laptop computers running a standard operating system as well as cellular wireless and handheld devices running mobile software and capable of supporting a number of networking and messaging protocols. Such a system also can include a number of workstations running any of a variety of commercially available operating systems and other known applications for purposes such as development and database management. These devices also can include other electronic devices such as dummy terminals thin clients gaming systems and other devices capable of communicating via a network.

Most embodiments utilize at least one network that would be familiar to those skilled in the art for supporting communications using any of a variety of commercially available protocols such as TCP IP OSI FTP UPnP NFS CIFS and AppleTalk. The network can be for example a local area network a wide area network a virtual private network the Internet an intranet an extranet a public switched telephone network an infrared network a wireless network and any combination thereof.

In embodiments utilizing a Web server the Web server can run any of a variety of server or mid tier applications including HTTP servers FTP servers CGI servers data servers Java servers and business application servers. The server s also may be capable of executing programs or scripts in response requests from user devices such as by executing one or more Web applications that may be implemented as one or more scripts or programs written in any programming language such as Java C C or C or any scripting language such as Perl Python or TCL as well as combinations thereof. The server s may also include database servers including without limitation those commercially available from Oracle Microsoft Sybase and IBM .

The environment can include a variety of data stores and other memory and storage media as discussed above. These can reside in a variety of locations such as on a storage medium local to and or resident in one or more of the computers or remote from any or all of the computers across the network. In a particular set of embodiments the information may reside in a storage area network SAN familiar to those skilled in the art. Similarly any necessary files for performing the functions attributed to the computers servers or other network devices may be stored locally and or remotely as appropriate. Where a system includes computerized devices each such device can include hardware elements that may be electrically coupled via a bus the elements including for example at least one central processing unit CPU at least one input device e.g. a mouse keyboard controller touch screen or keypad and at least one output device e.g. a display device printer or speaker . Such a system may also include one or more storage devices such as disk drives optical storage devices and solid state storage devices such as random access memory RAM or read only memory ROM as well as removable media devices memory cards flash cards etc.

Such devices also can include a computer readable storage media reader a communications device e.g. a modem a network card wireless or wired an infrared communication device etc. and working memory as described above. The computer readable storage media reader can be connected with or configured to receive a computer readable storage medium representing remote local fixed and or removable storage devices as well as storage media for temporarily and or more permanently containing storing transmitting and retrieving computer readable information. The system and various devices also typically will include a number of software applications modules services or other elements located within at least one working memory device including an operating system and application programs such as a client application or Web browser. It should be appreciated that alternate embodiments may have numerous variations from that described above. For example customized hardware might also be used and or particular elements might be implemented in hardware software including portable software such as applets or both. Further connection to other computing devices such as network input output devices may be employed.

Storage media and computer readable media for containing code or portions of code can include any appropriate media known or used in the art including storage media and communication media such as but not limited to volatile and non volatile removable and non removable media implemented in any method or technology for storage and or transmission of information such as computer readable instructions data structures program modules or other data including RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disk DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by the a system device. Based on the disclosure and teachings provided herein a person of ordinary skill in the art will appreciate other ways and or methods to implement the various embodiments.

The specification and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense. It will however be evident that various modifications and changes may be made thereunto without departing from the broader spirit and scope of the invention as set forth in the claims.

Other variations are within the spirit of the present disclosure. Thus while the disclosed techniques are susceptible to various modifications and alternative constructions certain illustrated embodiments thereof are shown in the drawings and have been described above in detail. It should be understood however that there is no intention to limit the invention to the specific form or forms disclosed but on the contrary the intention is to cover all modifications alternative constructions and equivalents falling within the spirit and scope of the invention as defined in the appended claims.

The use of the terms a and an and the and similar referents in the context of describing the disclosed embodiments especially in the context of the following claims are to be construed to cover both the singular and the plural unless otherwise indicated herein or clearly contradicted by context. The terms comprising having including and containing are to be construed as open ended terms i.e. meaning including but not limited to unless otherwise noted. The term connected is to be construed as partly or wholly contained within attached to or joined together even if there is something intervening. Recitation of ranges of values herein are merely intended to serve as a shorthand method of referring individually to each separate value falling within the range unless otherwise indicated herein and each separate value is incorporated into the specification as if it were individually recited herein. All methods described herein can be performed in any suitable order unless otherwise indicated herein or otherwise clearly contradicted by context. The use of any and all examples or exemplary language e.g. such as provided herein is intended merely to better illuminate embodiments of the invention and does not pose a limitation on the scope of the invention unless otherwise claimed. No language in the specification should be construed as indicating any non claimed element as essential to the practice of the invention.

Preferred embodiments of this disclosure are described herein including the best mode known to the inventors for carrying out the invention. Variations of those preferred embodiments may become apparent to those of ordinary skill in the art upon reading the foregoing description. The inventors expect skilled artisans to employ such variations as appropriate and the inventors intend for the invention to be practiced otherwise than as specifically described herein. Accordingly this invention includes all modifications and equivalents of the subject matter recited in the claims appended hereto as permitted by applicable law. Moreover any combination of the above described elements in all possible variations thereof is encompassed by the invention unless otherwise indicated herein or otherwise clearly contradicted by context.

All references including publications patent applications and patents cited herein are hereby incorporated by reference to the same extent as if each reference were individually and specifically indicated to be incorporated by reference and were set forth in its entirety herein.

