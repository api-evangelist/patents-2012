---

title: Proxy queue pair for offloading
abstract: A method for offloading includes a host channel adapter (HCA) receiving a first work request identifying a queue pair (QP), making a first determination that the QP is a proxy QP, and offloading the first work request to a proxy central processing unit (CPU) based on the first determination and based on the first work request satisfying a filter criterion. The HCA further receives a second work request identifying the QP, processes the second work request without offloading based on the QP being a proxy QP and based on the first work request failing to satisfy the filter criterion. The HCA redirects a first completion for the first work request and a second completion for the second work request to the proxy CPU based on the first determination. The proxy CPU processes the first completion and the second completion in order.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09069633&OS=09069633&RS=09069633
owner: Oracle America, Inc.
number: 09069633
owner_city: Redwood Shores
owner_country: US
publication_date: 20121220
---
The present application contains subject matter that may be related to the subject matter in the following U.S. Patent Application which is assigned to a common assignee and is incorporated by reference in its entirety U.S. patent application Ser. No. 13 149 436 entitled METHOD AND SYSTEM FOR PROCESSING COMMANDS ON AN INFINIBAND HOST CHANNEL ADAPTOR. 

The Infiniband network includes nodes that communicate through a channel based switched fabric Infiniband is a registered trademark of Infiniband Trade Association located in Beaverton Oreg. . For example the nodes may be a host an input output subsystem or a router which connects to another network. The switched fabric is made of a collection of switches routers and or links that connect a set of channel adapters. The channel adapters form an interface between the switched fabric and the nodes. The channel adapter of the host is referred to as a host channel adapter. The channel adapter of an I O subsystem is referred to as a target channel adapter.

In Infiniband two processes communicate using a queue pair. A queue pair includes a send queue and a receive queue. Specifically in order for a process to send a message to another process the process posts the message to the send queue. The host channel adapter sends the message in the form of packets to the channel adapter having the receive queue. Each packet that is sent may include a packet sequence number. Logic associated with the receive queue ensures that packets are processed in a particular order using the packet sequence number.

In general in one aspect the invention relates to a method for offloading. The method includes receiving by a host channel adapter HCA a first work request identifying a queue pair QP where the QP is associated with an application executing on a host connected to the HCA making a first determination that the QP is a proxy QP and offloading by the HCA the first work request to a proxy central processing unit CPU based on the first determination and based on the first work request satisfying a filter criterion. The method further includes receiving by the HCA a second work request identifying the QP processing by the HCA the second work request without offloading based on the QP being a proxy QP and based on the first work request failing to satisfy the filter criterion. The method further includes redirecting a first completion for the first work request and a second completion for the second work request to the proxy CPU based on the first determination and processing by the proxy CPU the first completion and the second completion in order.

In general in one aspect the invention relates to a host channel adapter HCA for offloading. The HCA includes a proxy central processing unit and a receive module for receiving a first work request identifying a first queue pair QP where the first QP is associated with an application executing on a host connected to the HCA making a first determination that the first QP is a proxy QP and redirecting the first work request to the proxy CPU on the HCA based on the first determination and based on the first work request satisfying a filter criterion. The receive module is further for receiving a second work request identifying the first QP and processing the second work request without redirecting to the proxy CPU based on the first determination and based on the first work request failing to satisfy the filter criterion. The HCA further includes a completion module for generating a first completion for the first work request and generating a second completion for the second work request. The HCA additionally includes a descriptor fetch module for redirecting the first completion and the second completion to the proxy CPU based on the first determination where the proxy CPU processes the first work request the first completion and the second completion.

In general in one aspect the invention relates to A system that includes a host comprising an application and a host channel adapter HCA connected to the host. The HCA is for receiving a first work request identifying a queue pair QP where the QP is associated with the application making a first determination that the QP is a proxy QP offloading the first work request to a proxy central processing unit CPU based on the first determination and based on the first work request satisfying a filter criterion receiving a second work request identifying the QP processing the second work request without offloading based on the QP being a proxy QP and based on the first work request failing to satisfy the filter criterion redirecting a first completion for the first work request and a second completion for the second work request to the proxy CPU based on the first determination and processing by the proxy CPU the first completion and the second completion in order.

Other aspects of the invention will be apparent from the following description and the appended claims.

Specific embodiments of the invention will now be described in detail with reference to the accompanying figures. Like elements in the various figures are denoted by like reference numerals for consistency.

In the following detailed description of embodiments of the invention numerous specific details are set forth in order to provide a more thorough understanding of the invention. However it will be apparent to one of ordinary skill in the art that the invention may be practiced without these specific details. In other instances well known features have not been described in detail to avoid unnecessarily complicating the description.

In general embodiments of the invention provide a method and apparatus for managing the offloading of work requests to a proxy central processing unit CPU . Specifically embodiments of the invention provide a mechanism for maintaining information about when to offload a work request for a queue pair QP and conforming to QP ordering requirements when work requests are offloaded. Specifically in one or more embodiments of the invention a QP requires that messages are processed in accordance with sequence number. When a QP is set to be a proxy QP embodiments require that the completion queue CQ for the QP is also a proxy CQ resulting in completions for work requests to be offloaded to the proxy CQ. Thus even when some work request are offloaded and some are not the completions are issued in order thereby maintaining ordering requirements. The work request may be a request from a remote requestor from the Infiniband network or may be based on a new transmission to the Infiniband network.

By way of an overview a communication system may include a transmitting system and a receiving system which each are any type of physical computing device connected to the Infiniband network. By way of an example of the transmitting system and the receiving system the transmitting system and or a receiving system may be a host system such as the host system described in or and below. The host system may be an application server a storage server any other type of computing device. In one or more embodiments of the invention for a particular message the transmitting system is a system that sends the message and the receiving system is a system that receives the message. In other words the use of the words transmitting and receiving refer to the roles of the respective systems for a particular message. The roles may be reversed for another message such as a response sent from receiving system to transmitting system. For such a message the receiving system becomes a transmitting system and the transmitting system becomes a receiving system. Thus communication may be bi directional in one or more embodiments of the invention. In one or more embodiments of the invention one or more messages may include a work request. A work request is a request to perform an action.

The work request may be directed to an application or the HCA discussed below that is interposed between the device executing the application and the network. Specifically the transmitting system and the receiving system include a requestor application and a responder application respectively. The requestor application is a software application that sends the message and the responder application is a software application that receives the message. When the requestor application initiates a send of the message or perform a configuration of the HCA or network the requestor application issues a command to the HCA. The command is a work request. When the requestor application issues a message which may or may not be an RDMA read or RDMA write to the responder application the message is the work request. For example the message may explicitly or implicitly indicate an action for the responder application to perform. The HCA connected to the host executing the responder application may be configured to perform at least a portion of the work request. In other words the work request is offloaded to the HCA.

Requestor application and responder application communicate using QPs. Specifically a QP defines communication channels between the requestor application and the responder application. More specifically per the Infiniband protocol the message has a corresponding send queue and corresponding a receive queue. An application uses the send queue to send messages and the receive queue to receive messages. The send queue and receive queue on the same host that is used by the application to communicate with another application form a QP. Each QP may have a corresponding QP with which to communicate. For example consider the scenario where application M is communicating with application N. In such a scenario application M may have QP M with send queue M and receive queue M and application N may have QP N with send queue N and receive queue N. Messages from application M to application N are sent from send queue M to receive queue N. Messages from application N to application M are sent from send queue N to receive queue M. Logic and data structures used by the host system specify which QP on the recipient to use to send messages. Thus by the requestor application specifying the QP the requestor application is identifying the responder application to receive the message.

In one or more embodiments of the invention the host includes an HCA driver and operating system and a root complex . In one or more embodiments of the invention the HCA driver is software that provides an interface to the HCA for the operating system . Specifically when the operating system wants to send work requests to the HCA the operating system invokes a routine in the HCA driver .

Continuing with the host the host includes hardware . The hardware may include for example a central processing unit CPU memory and a root complex . In one or more embodiments of the invention the CPU is a hardware processor component for processing instructions of the host. The CPU may include multiple hardware processors. Alternatively or additionally each hardware processor may include multiple processing cores in one or more embodiments of the invention. In general the CPU is any device configured to execute instructions on the host .

In one or more embodiments of the invention the memory is any type of hardware device for storage of data. In one or more embodiments of the invention the memory may be partitioned. In one or more embodiments of the invention the memory includes functionality to store a send queue not shown . In one or more embodiments of the invention a send queue includes functionality to store an ordered list of work request identifiers for work requests for processing by the host channel adapter . In one or more embodiments of the invention the work request identifiers may be the actual work requests and or references to the work requests stored in memory.

In one or more embodiments of the invention the root complex includes functionality to connect the CPU and memory subsystem to a peripheral component interconnect PCI Express switch fabric. Specifically in one or more embodiments of the invention the root complex connects the host to the host channel adapter . Although shows the root complex as separate from the CPU the root complex may be integrated as part of the CPU.

The root complex includes an input output memory management unit IOMMU in one or more embodiments of the invention. The IOMMU includes functionality to connect a direct memory access DMA input output I O bus to the memory. In one or more embodiments of the invention the IOMMU includes functionality to translate addresses from one level of abstraction to another.

Continuing with the host is connected to the host channel adapter . In one or more embodiments of the invention the connection between the host and the host channel adapter may be a PCI express connection. Specifically the host channel adapter may connect to a PCI express fabric connector on the host.

In one or more embodiments of the invention the host channel adapter is a hardware device configured to connect the host to the Infiniband network . Specifically the host channel adapter includes functionality to receive work requests from the host and process the work requests. Processing the work requests may include performing DMA with host memory to obtain and store packet data and to obtain control information performing any validation required on the packet data generating packets from the packet data and sending and receiving packets on the Infiniband network . shows a schematic diagram of the host channel adapter from the prospective of the host . As shown in the host channel adapter includes at least one Infiniband port e.g. Infiniband port 1 Infiniband port 2 a resource pool and an embedded processor subsystem . Each of the components of the host channel adapter is discussed below.

In one or more embodiments of the invention an Infiniband port e.g. Infiniband port 1 Infiniband port 2 is a physical interface connector between the host channel adapter and the Infiniband network . Although shows two Infiniband ports a different number of ports may exist without departing from the invention.

The resource pool is a collection of resources that are required to send and receive packets on the Infiniband network. Specifically the resource pool corresponds to the collection of hardware and stored data that is accessible by the host and may be shared among virtual machines on the host . The resource pool is discussed in below.

The embedded processor subsystem includes a service processor resource manager and a proxy CPU . The service processor resource manager includes functionality to receive and process the management requests on the host channel adapter. For example the management requests may be to change the allocation of HCA resources change the configuration of the HCA and perform other management of the HCA.

Continuing with the proxy CPU includes functionality to execute various instructions on behalf of an application not shown executing on the host . Specifically the proxy CPU includes functionality to execute the various instructions in place of the application executing the instructions. Thus the application may offload certain functionality to the proxy CPU . The proxy CPU may be associated with memory having the offloaded instructions.

In one or more embodiments of the invention the host includes one or more guest virtual machines e.g. virtual machine virtual machine Y a control virtual machine a hypervisor and hardware . Each of these components is discussed below.

Broadly speaking the virtual machines e.g. virtual machine virtual machine Y control virtual machine are distinct operating environments configured to inherit underlying functionality of the host operating system via an abstraction layer. In one or more embodiments of the invention each virtual machine includes a separate instance of an operating system e.g. OS 1 OS Y Control Virtual Machine Operating System OS in . The separate instances of the operating system may be the same type of operating system or different types of operating systems.

Specifically the guest virtual machine operating system e.g. OS 1 OS Y operates as if the guest virtual machine operating system is the only operating system on the host and the resources e.g. processor cycles memory resources of the HCA allocated to the guest virtual machine are the only resources available on the host . Thus the guest virtual machine operating system e.g. OS 1 OS Y includes functionality to control the operating environment of applications executing in the guest virtual machine using resource allocated to the guest virtual machine. Each virtual machine may be allocated disjoint or non overlapping physical memory .

Many different types of virtual machines exist. For example the Xen virtualization project allows for multiple guest operating systems executing in a host operating system. Xen is a trademark overseen by the Xen Project Advisory Board. In one embodiment of the invention the host operating system supports virtual execution environments not shown . Another example is a Solaris Container. In such cases the Solaris Container may execute in the host operating system which may be a Solaris operating system. Solaris is a trademark of Oracle America Inc. In one embodiment of the invention the host operating system may include both virtual machines and virtual execution environments.

In one or more embodiments of the invention the guest virtual machine includes a virtual HCA device driver e.g. vHCA driver vHCA driver Y . The virtual HCA device driver is software program that provides an interface to HCA for the guest virtual machine operating system. Specifically when the guest virtual machine operating system wants to send work requests to the HCA the virtual machine operating system invokes a routine in the virtual HCA device driver. In response the virtual HCA device driver issues work requests to a virtualized device controller not shown presented by the hypervisor discussed below . In turn the hypervisor includes functionality to transmit the message to the HCA .

In addition to the guest virtual machine e.g. virtual machine virtual machine Y the host also includes a control virtual machine . In one or more embodiments of the invention the control virtual machine has a separate address space and operating system environment than the guest virtual machine e.g. virtual machine virtual machine Y . The control virtual machine includes a control virtual machine operating system a control virtual machine manager and a virtual machine HCA device driver . The virtual machine HCA device driver includes functionality similar to the guest virtual machine HCA device drivers e.g. vHCA driver vHCA driver Y discussed above. The host virtual machine operating system includes functionality to provide an operating environment for software executing in the control virtual machine .

In one or more embodiments of the invention the software executing in the control virtual machine includes a virtual machine manager discussed below . In one or more embodiments of the invention the virtual machine manager includes functionality to configure the hypervisor configure the HCA create remove and configure guest virtual machines and perform the management of the host . With respect to configuring the HCA the virtual machine manager includes functionality to send work requests to the HCA to adjust the number of resources allocated to each virtual machine To receive parameter values for performing the above management tasks the virtual machine manager may include a user interface and or an application programming interface for communicating with a computer administrator or another program in one or more embodiments of the invention.

Continuing with the hypervisor includes functionality to control the sharing of hardware resources on the host . Specifically the hypervisor includes functionality to virtualize the physical devices of the host so that more than one operating system may share the same physical device transparently in accordance with one or more embodiments of the invention. Further the hypervisor controls when the guest virtual machine e.g. virtual machine virtual machine Y and the control virtual machine are allowed to execute. For example the hypervisor may be a thin privileged layer of software that only manages which guest virtual machine or the host virtual machine is executing.

Continuing with the host the host includes hardware which may include for example a central processing unit memory and a root complex . The root complex may include an IOMMU . The hardware and included components may be the same as substantially the same as or similar to the hardware in and thus the corresponding descriptions from are incorporated herein. Further with virtualization the memory may include a separate send queue for each virtual machine. Alternatively or additionally multiple virtual machines may share one or more send queues.

Continuing with the host is connected to the HCA . As shown in the HCA includes at least one Infiniband port e.g. Infiniband port 1 Infiniband port 2 a resource pool and an embedded processor subsystem . The connection between the host and the HCA the HCA the Infiniband port s resource pool and an embedded processor subsystem may be the same as substantially the same as or similar to the corresponding like named components discussed above with reference to and thus the corresponding descriptions from are incorporated herein.

In one or more embodiments of the invention the sharing of the resource pool is performed using the concepts of physical function and virtual functions. A physical function exposes the actual hardware of the HCA to an operating system. Specifically by way of the physical function the control virtual machine operating system may control the HCA. Thus the physical function allows the control virtual machine to control the HCA such as to disable the HCA .

A virtual function e.g. virtual function 1 virtual function Y exposes a virtualized HCA to a virtual machine. Specifically the virtual function e.g. virtual function 1 virtual function Y exposes to the virtual machine operating system only the partition of the resource pool allocated to the virtual machine To the guest virtual machine e.g. virtual machine virtual machine Y the resources exposed by the virtual function e.g. virtual function 1 virtual function Y appear as if the resource are the only resources on the HCA . Thus the virtual function e.g. virtual function 1 virtual function Y allows the virtual machine operating system e.g. OS 1 OS Y to control the portion of resources allocated to the virtual machine. In other words a virtual function e.g. virtual function 1 virtual function Y provides the virtual machine operating system e.g. OS 1 OS Y the appearance that the virtual machine operating system e.g. OS 1 OS Y is controlling the HCA as a whole even though the actions of the virtual machine operating system e.g. OS 1 OS Y does not affect any other virtual function e.g. virtual function 1 virtual function Y .

In one or more embodiments of the invention the term underlying function UF is used to refer generally to either a physical function or a virtual function. Specifically as used herein an underlying function may be a physical function or a virtual function.

The embedded processor subsystem corresponds to an embedded processor and logic for managing the HCA . The embedded processor subsystem includes a service processor resource manager and a proxy CPU . The embedded processor subsystem service processor resource manager and proxy CPU may be the same as substantially the same as or similar to the corresponding like named components discussed above with reference to and thus the corresponding descriptions from are incorporated herein.

Although show the proxy CPU on the HCA alternatively or additionally the proxy CPU or a different proxy CPU may be on the host. For example the proxy CPU may be a dedicated core of the host. In such embodiments of the invention work requests are routed to the proxy CPU on the host.

As shown in the host channel adapter may include a collect buffer unit module a virtual kick module a QP fetch module a direct memory access DMA module an Infiniband packet builder module one or more Infiniband ports a completion module an Infiniband packet receiver module a receive module a descriptor fetch module a receive queue entry handler module and a DMA validation module . The respective modules correspond to both transmitting processing logic for sending messages on the Infiniband network and receiving processing logic for receiving messages from the Infiniband network . In one or more embodiments of the invention the collect buffer unit module virtual kick module QP fetch module direct memory access DMA module Infiniband packet builder module and completion module may be components of the transmitting processing logic. The completion module Infiniband packet receiver module receive module descriptor fetch module receive queue entry handler module and DMA validation module may be components of the receiving processing logic. As shown the completion module may be considered a component of both the transmitting processing logic and the receiving processing logic in one or more embodiments of the invention.

In one or more embodiments of the invention each module may correspond to hardware and or firmware. Each module is configured to process data units. Each data unit corresponds to a command or a received message or packet. For example a data unit may be the command an address of a location on the communication adapter storing the command a portion of a message corresponding to the command a packet an identifier of a packet or any other identifier corresponding to a command a portion of a command a message or a portion of a message. A command or received message may be considered a work request.

The dark arrows between modules show the transmission path of data units between modules as part of processing work requests and received messages in one or more embodiments of the invention. Data units may have other transmission paths not shown without departing from the invention. Further other communication channels and or additional components of the host channel adapter may exist without departing from the invention. Each of the components of the resource pool is discussed below.

The collect buffer controller module includes functionality to receive work request data from the host and store the work request data on the host channel adapter. Specifically the collect buffer controller module is connected to the host and configured to receive the work request from the host and store the work request in a buffer. When the work request is received the collect buffer controller module is configured to issue a kick that indicates that the work request is received.

In one or more embodiments of the invention the virtual kick module includes functionality to load balance work requests received from applications. Specifically the virtual kick module is configured to initiate execution of work requests through the remainder of the transmitting processing logic in accordance with a load balancing protocol.

In one or more embodiments of the invention the QP fetch module includes functionality to obtain QP status information for the QP corresponding to the data unit. Specifically per the Infiniband protocol the message has a corresponding send queue and a receive queue. The send queue and receive queue form a QP. Accordingly the QP corresponding to the message is the QP corresponding to the data unit in one or more embodiments of the invention. The QP state information may include for example sequence number address of remote receive queue send queue whether the QP is allowed to send or allowed to receive and other state information.

In one or more embodiments of the invention the DMA module includes functionality to perform DMA with host memory. The DMA module may include functionality to determine whether a work request in a data unit or referenced by a data unit identifies a location in host memory that includes payload. The DMA module may further include functionality to validate that the process sending the work request has necessary permissions to access the location and to obtain the payload from the host memory and store the payload in the DMA memory. Specifically the DMA memory corresponds to a storage unit for storing a payload obtained using DMA.

Continuing with in one or more embodiments of the invention the DMA module is connected to an Infiniband packet builder module . In one or more embodiments of the invention the Infiniband packet builder module includes functionality to generate one or more packets for each data unit and to initiate transmission of the one or more packets on the Infiniband network via the Infiniband port s . In one or more embodiments of the invention the Infiniband packet builder module may include functionality to obtain the payload from a buffer corresponding to the data unit from the host memory and from an embedded processor subsystem memory.

In one or more embodiments of the invention the completion module includes functionality to generate completions for work requests. For example the completion module may include functionality to manage packets for QPs set in reliable transmission mode. Specifically in one or more embodiments of the invention when a QP is in a reliable transmission mode then the receiving channel adapter of a new packet responds to the new packet with an acknowledgement message indicating that transmission completed or an error message indicating that transmission failed. The completion module includes functionality to manage data units corresponding to packets until an acknowledgement is received or transmission is deemed to have failed e.g. by a timeout .

In one or more embodiments of the invention the Infiniband packet receiver module includes functionality to receive packets from the Infiniband port s . In one or more embodiments of the invention the Infiniband packet receiver module includes functionality to perform a checksum to verify that the packet is correct parse the headers of the received packets and place the payload of the packet in memory. In one or more embodiments of the invention the Infiniband packet receiver module includes functionality to obtain the QP state for each packet from a QP state cache. In one or more embodiments of the invention the Infiniband packet receiver module includes functionality to transmit a data unit for each packet to the receive module for further processing.

In one or more embodiments of the invention the receive module includes functionality to validate the QP state obtained for the packet. The receive module includes functionality to determine whether the packet should be accepted for processing. In one or more embodiments of the invention if the packet corresponds to an acknowledgement or an error message for a packet sent by the host channel adapter the receive module includes functionality to update the completion module .

Additionally or alternatively the receive module includes a queue that includes functionality to store data units waiting for one or more reference to buffer location s or waiting for transmission to a next module. Specifically when a process in a virtual machine is waiting for data associated with a QP the process may create receive queue entries that reference one or more buffer locations in host memory in one or more embodiments of the invention. For each data unit in the receive module the receive module includes functionality to identify the receive queue entries from a host channel adapter cache or from host memory and associate the identifiers of the receive queue entries with the data unit.

In one or more embodiments of the invention the descriptor fetch module includes functionality to obtain descriptors for processing a data unit. For example the descriptor fetch module may include functionality to obtain descriptors for a receive queue a shared receive queue a ring buffer and the completion queue CQ .

In one or more embodiments of the invention the receive queue entry handler module includes functionality to obtain the contents of the receive queue entries. In one or more embodiments of the invention the receive queue entry handler module includes functionality to identify the location of the receive queue entry corresponding to the data unit and obtain the buffer references in the receive queue entry. In one or more embodiments of the invention the receive queue entry may be located on a cache of the host channel adapter or in host memory.

In one or more embodiments of the invention the DMA validation module includes functionality to perform DMA validation and initiate DMA between the host channel adapter and the host memory. The DMA validation module includes functionality to confirm that the remote process that sent the packet has permission to write to the buffer s referenced by the buffer references and confirm that the address and the size of the buffer s match the address and size of the memory region referenced in the packet. Further in one or more embodiments of the invention the DMA validation module includes functionality to initiate DMA with host memory when the DMA is validated.

In general in one or more embodiments of the invention a repository e.g. QP context state repository CQ repository is any type of storage unit and or device e.g. a file system database collection of tables or any other storage mechanism for storing data. Further a repository may include multiple different storage units and or devices.

In one or more embodiments of the invention a QP state repository corresponds to a repository for storing QP states e.g. QP state . Although shows a single QP state multiple QP states may be in the QP state repository without departing from the scope of the invention. Specifically in one or more embodiments of the invention the QP state repository stores a separate and unique QP state for each QP used by the host. The QP state includes information about the QP. Specifically the QP state includes QP information and a QP proxy enable setting . The QP proxy enable setting is a configuration parameter that defines whether the QP is a proxy QP. In one or more embodiments of the invention the QP proxy enable setting may be a bit or any other mechanism for indicating whether the QP is a proxy QP. If the QP proxy enable setting is set then work requests for the QP are examined to determine whether the work requests should be processed by the proxy CPU.

QP information includes other information about the QP state. For example the QP information may include status and control information. For example the QP information may include status and control information about the send queue and receive queue corresponding to the QP the number of pending requests size of the send queue a QP number for the remote node i.e. the node with which the application is communicating using the QP one or more sequence numbers e.g. send queue sequence number CQ sequence number index to a CQ descriptor discussed below acknowledgement timeout period and parameters for whether other actions are enabled for the QP.

A CQ repository is a repository that stores CQ and CQ descriptors for one or more QP. Specifically each QP may have a separate and unique corresponding CQ and CQ descriptor . The CQ is any data structure that includes functionality to store completions. In one or more embodiments of the invention the CQ may be implemented as a ring buffer. A completion is an indication to an application executing on the host that an action occurred on the HCA for the QP corresponding to the application. Specifically a completion includes a sequence number an operation code and zero or more parameters. The sequence number indicates the order of a communication with respect to other communications of a QP. The operation code indicates to the application what the application needs to perform e.g. that data is in memory that no operation i.e. no op needs to be performed or other actions . The optional parameter s provide parameters for the operation code such as location in memory in which the data is store a reference to the receive queue or other operation.

For example a receive completion indicates that a message has been received from the network and processed by the HCA for the QP. In one or more embodiments of the invention processing the message may include performing remote direct memory access RDMA to store data in the message in host memory associated with the application transmitting the message to the receive queue of the QP or performing other such processing. In such a scenario the receive completion may indicate that new data is waiting for processing by the responder application. Processing the message may include performing by the proxy CPU a work request specified by the message on behalf of the responder application and requested by a remote requestor application. In such a scenario the receive completion may be a no op and a sequence number for the message. The no op may indicate that a message was received but the responder application does not need to perform any action for the message.

By way of another example a send completion to a requester application indicates that a work request originating from the requester application successfully completed. Specifically the send completion indicates that the message is sent. Such send completion may be created when the message is transmitted and or when an acknowledgement is received from the receiving system.

The CQ descriptor corresponding to a CQ stores information about the corresponding CQ . The CQ descriptor includes a CQ proxy enable setting and CQ information in one or more embodiments of the invention. The CQ proxy enable setting is a configuration parameter that defines whether the CQ is a proxy CQ. In one or more embodiments of the invention the CQ proxy enable setting may be a bit or any other mechanism for indicating whether the CQ is a proxy CQ. If the CQ proxy enable setting is set then completions for the CQ are routed to the proxy CPU. By routing all completions to the proxy CPU regardless of whether the corresponding work request is offloaded to the proxy CPU embodiments maintain the expected ordering of completions by sequence number of the work request for the application. In other words completions for work requests that take more time because of being offloaded to the proxy CPU are not behind completions for later work requests that are not offloaded in one or more embodiments of the invention.

CQ information corresponds to other control and status information about the CQ. For example CQ information may include a consumer pointer to the CQ indicating the last entry in the CQ read a producer pointer in the CQ indicating the last entry in the CQ received a sequence number corresponding to last work request that had a completion added and or any other information about the CQ.

Continuing with the memory of an application memory includes functionality to store data for an application. For example the application memory may include packet payload data to send on the network and or data received from the network. The packet payload data for each packet of a message forms a work request. In other words when the complete message is received i.e. all the packets of the message are received the complete message is a work request which may be processed by the proxy CPU.

In one or more embodiments of the invention the QP state cache corresponds to a cache of QP states discussed above with reference to . Specifically the QP state cache includes a portion of the QP states that are stored in memory. The portion of the QP states may correspond to QPs currently being processed by the HCA. In one or more embodiments of the invention for each QP state in the QP state cache only a subset of the QP state used by the receive module is stored. In one or more embodiments of the invention the subset of the QP state includes the QP proxy enable settings for the QP.

In one or more embodiments of the invention the filter criteria includes one or more criterion for filtering which work requests are redirected to the proxy CPU. For example the one or more filter criterion may include the amount of processing to complete the work request the type of work request the size of the work request e.g. size of the payload s number of packets in the message and other information. The type of the work request specifies what operations need to be performed to complete the work request. For example the operation may be to obtain a lock on one or more memory locations for a requester application. As another example the operation may be to perform calculations on or otherwise process data in host memory.

Although shows the filter criteria as separate from the QP state cache the filter criteria may be stored in the QP state cache. Specifically the filter criteria may be a part of the QP information for each particular QP. In such a scenario the filter criteria may be specific to the QP.

Continuing with the processing module includes functionality to perform the actions of the receive module . Specifically the processing module corresponds to the hardware of the receive module that performs the functionality of the receive module in one or more embodiments of the invention. In one or more embodiments of the invention the processing module includes a header validation module and a redirection module .

The header validation module is hardware or firmware that includes functionality to validate the header field of an incoming packet from the network. Specifically the header validation module includes functionality to obtain the QP state of the QP specified in the packet and validate the header against the QP state. The validation determines whether the packet should be accepted for processing by the HCA. Further the header validation module determines the destination of the packet.

The redirection module is hardware or firmware that includes functionality to determine whether to redirect the packet to the proxy CPU. Specifically the redirection module includes functionality to access the QP state cache and determine whether the QP of the packet is a proxy QP. In one or more embodiments of the invention the redirection module further includes functionality to access the filter criteria and determine whether the packet satisfies the filter criteria. The redirection module further includes functionality to transmit packets to the proxy CPU or through normal processing channels to the application based on the results of the determinations.

In one or more embodiments of the invention the CQ descriptor cache corresponds to a cache of CQ descriptors discussed above with reference to . Specifically the CQ descriptor cache includes a portion of the CQ descriptors that are stored in memory. The portion of the CQ descriptors may correspond to QPs currently being processed by the HCA. In one or more embodiments of the invention for each CQ descriptor in the CQ descriptor cache only a subset of the CQ descriptor used by the descriptor fetch module is stored. In one or more embodiments of the invention the subset of the CQ descriptor includes the CQ proxy enable settings for the CQ corresponding to the QP.

Continuing with the processing module includes functionality to perform the actions of the descriptor fetch module . Specifically the processing module corresponds to the hardware of the descriptor fetch module that performs the functionality of the descriptor fetch module in one or more embodiments of the invention. In one or more embodiments of the invention the processing module includes a fetch module and a redirection module .

The fetch module is hardware or firmware that includes functionality to fetch from memory and cache in the corresponding locations not necessarily on the descriptor fetch module queue descriptors used to process a packet. The queue descriptors may include descriptors for one or more of a receive queue a shared receive queue a ring buffer and the CQ.

The redirection module is hardware or firmware that includes functionality to determine whether to redirect the completion to the proxy CPU. Specifically the redirection module includes functionality to access the CQ descriptor cache and determine whether the CQ is a proxy CQ. The redirection module further includes functionality to transmit the completion to the proxy CPU or through normal processing channels to the application based on the results of the determinations.

In the work request is received in one or more embodiments of the invention. In one or more embodiments of the invention the work request may be received from a network or received from the host. Receiving a work request from the host is discussed in U.S. patent application Ser. No. 12 149 436 which is incorporated herein by reference. In one or more embodiments of the invention the work request corresponds to a command as described in U.S. patent application Ser. No. 12 149 436. In one or more embodiments of the invention receiving the work request from the network includes the work request being received on the Infiniband ports. For a work request received from the network the work request may include one or more packets.

In the QP state is obtained for the work request in one or more embodiments of the invention. Obtaining the QP state may be performed by accessing the work request and identifying the QP referenced in the work request. In one or more embodiments of the invention each work request includes a sequence number and an identifier of the QP. The identifier of the QP may be an identifier of a send queue for work requests corresponding to sending on the network a receive queue for work requests originating from the network or a separate identifier. Based on the identifier a determination may be made whether the QP state is already cached on the HCA. If the QP state is already cached then the QP state is obtained from cache. If the QP state is not cached then the QP state is obtained from memory on the host. Obtaining the QP state from memory may include sending a direct memory access DMA request to obtain the QP state from the QP state repository. In one or more embodiments of the invention for work requests received from the network the receive module obtains the QP state in one or more embodiments of the invention. For work requests received from the host the QP fetch module obtains the QP state in one or more embodiments of the invention. Other modules may obtain the QP state without departing from the scope of the invention.

In a determination is made whether the work request corresponds to a proxy QP. In particular the QP proxy setting is accessed in the QP state. If the QP proxy setting is set then the work request corresponds to a proxy QP. If the QP proxy setting is not set then the work request does not correspond to a proxy QP.

If the work request does not correspond to a proxy QP then the work request is processed as normal in one or more embodiments of the invention in Step . Specifically the work request is processed by the HCA to send a message to the network or to send the work request for processing by the application. Below is a description of how the work request may be processed as normal in one or more embodiments of the invention. Additional or alternative methods may be used to process the work request without departing from the scope of the claims.

In one or more embodiments of the invention if the work request is for sending a message processing the work request may include one or more of the following actions. If the work request includes packet data then the packet data is read from the work request. If the work request specifies one or more locations in host memory then the location s in host memory to read are identified from the work request in one or more embodiments of the invention. DMA may be performed to obtain the packet data. A DMA module on the HCA may identify one or more locations in host memory that are specified in the packet data obtains the packet data from the one or more locations in host memory and stores the packet data into buffers. In one or more embodiments of the invention as part of obtaining the packet data the DMA may perform validation to ensure that the host process sending the work request has necessary permissions to obtain the packet data. Regardless of whether the DMA is required the packets are generated from the packet data. The generating of the packets may include performing various levels of identification and validation such as validating the send queue sending the packet s and the receive queue receiving the packet s . The packets are issued to the port for sending on a network in one or more embodiments of the invention. Further a completion is initiated as described below and in .

For incoming work requests processing the work requests may include performed as follows. Based on the type of work request location s in host memory to store data are identified. If the work request is a message then the locations may be locations associated with the responder application and identified as the receive queue from the QP. If the work request is for RDMA then the locations may be specified in the work request. Validation is performed to confirm that the process sending the work request has permissions to access the location s in host memory. Specifically for example the validation may be based on the QP. If validated then DMA is performed with the host memory to store or retrieve data in the specified location s of host memory. Further a completion is initiated as described below and in .

Continuing with when the work request corresponds to a proxy QP in Step a determination is made whether the work request corresponds to a proxy CQ in one or more embodiments of the invention. Specifically the CQ descriptor is accessed. Obtaining the CQ descriptor may be performed by accessing the work request and identifying the QP referenced in the work request. Based on the QP a determination may be made whether the CQ descriptor is already cached on the HCA. If the CQ descriptor is already cached then the CQ descriptor is obtained from the cache. If the CQ descriptor is not cached then the CQ descriptor is obtained from memory. Obtaining the CQ descriptor from memory may include sending a DMA request to obtain the CQ descriptor from the CQ descriptor repository. In one or more embodiments of the invention descriptor fetch module obtains the CQ descriptor. Other modules may obtain the QP state without departing from the scope of the invention. Further the CQ proxy setting is accessed in the QP state. If the CQ proxy setting is set then the work request corresponds to a proxy CQ. If the CQ proxy setting is not set then the work request does not correspond to a proxy CQ. If the work request does not correspond to a proxy CQ but does correspond to a proxy QP then an error is identified in Step . When the error is identified the work request may be discarded and or an error message may be transmitted to the host e.g. a control application or another application on the host .

Further in Step a determination is made whether the filter criteria is satisfied in one or more embodiments of the invention. Determining whether filter criteria is satisfied may be performed by comparing the work request to the one or more filter criterion in the filter criteria. For example the size of the payload in the work request may be compared to determine whether the work request complies with any size requirements specified for the proxy QP. By way of another example if a single packet message has a size which is below a programmable size threshold then the single packet message may be offloaded.

Possible filter criteria may be based on inspecting the header of the packet. Specifically a determination is made as to whether information in the header matches the filter criteria. If so the work request is offloaded. For example the work request may be offloaded when an operation code in the work request specifies a permitted operation for offloading in the filter criteria. As another example the filtering criteria may be based on specific addresses in RDMA packets or particular Remote Keys R Keys used for accessing the RDMA addresses. By way of another example for non connection based QPs Unreliable Datagram UD when the QP number or other information about the source of the packet SLID SGID or the Service Level that the packet used to cross the network matches a pre specified information the packet may be offloaded.

Other possible filter criteria may be based on inspecting the payload of the packet. Specifically a determination is made as to whether the payload or attributes thereof matches the filter criteria. If so the work request is offloaded.

For example Infiniband IB payload often carries other protocols tunneled through the IB network. Ethernet over IB and IPoIB are two examples. If the packets are Ethernet or Internet protocol IP packets a large number of layered protocols may exist as well. In support of the various protocols Ethernet and IP packet classifiers may distinguish between L2 L3 and L4 packet types. The result of the classification may be used as part of the filter criteria e.g. only TCP packets . Additionally or alternatively a number of fields may be extracted from the packet headers e.g. of Ethernet or IP packets and use them for various checks similar to a firewall. The extracted fields may be part of the filter criteria as well e.g. only transmission control protocol TCP packets using TCP port 100 .

The various possible filtering may be used with custom circuitry specific to the packet type or with more generic circuitry which provides a programmable filter on a bitwise basis such as a ternary content addressable memory TCAM . For example with a TCAM each entry in the TCAM has two fields an N bit value and an N bit mask. When a packet is received the first N bits of the packet are then presented to the TCAM. Each bit is compared against the corresponding bit in the TCAM value. If the two bits are equal then those bits match. If the two bits are not equal then they do not match. If the mask bit is set then the bit is ignored. If all non ignored bits match then the packet is determined to match the filter criteria. A policy may exist for the TCAM whereby matched packets go to the Proxy CPU and unmatched do not or matched packets do not go to the proxy CPU and unmatched do.

Another criteria may be based on the relative load of the proxy CPU. Specifically if the proxy CPU has too much work the proxy CPU might be in the critical path and slow down other applications in one or more embodiments of the invention. To avoid or alleviate the problem of the critical path a load detection logic may be used which would then transition the filter machine to a no offload mode where the HCA stops offloading packets which otherwise match the filter criteria until the proxy CPU has managed to catch up.

Above are a few example techniques and criteria for filtering. Other techniques and filtering criteria may be used without departing from the scope of the invention.

If the filter criteria is not satisfied then the work request is processed as normal in Step and as discussed above. If the filter criteria is satisfied then the work request is processed by the proxy CPU in Step . Specifically the work request is routed to the proxy CPU rather than or in addition to processing the work request on the host channel adapter. Specifically the work request may go through all or part of the normal processing on the HCA. For incoming work requests rather than notifying the application the work request is transmitted to the proxy CPU. The proxy CPU executes the offloaded instructions on the work request. Specifically the instructions specifying what actions to perform may be in memory on the HCA for execution by the proxy CPU. Accordingly the proxy CPU performs the actions. For example the proxy CPU may perform one or more of the following access host memory or other local storage disk through reads writes or atomic operations send packets to other locations in the network perform local calculations on the contents of the packet or the contents of memory or both and or send response packets to the original requester.

In Step a determination is made as to whether the processing of the work request is successful. For example while processing the work request the proxy CPU may encounter an error. As another example the proxy CPU may determine that a work request is too complicated to process. The determination may be based on rules maintained by the proxy CPU. If the processing of the work request is not successful then the work request is processed as normal in Step and as discussed above. Specifically any processing of the work request performed by the proxy CPU is reversed.

If the processing of the work request is successful or after normal processing of the work request processing of the completion is initiated in Step . For example a notification may be sent to the completion module to initiate the completion.

In Step a CQ descriptor is obtained in one or more embodiments of the invention. In Step a determination is made whether the work request corresponds to a proxy CQ. Steps and may be performed for example as discussed above with reference to Step .

In Step if the work request does not correspond to a proxy CQ then a determination is made whether the work request corresponds to a proxy QP. Step may be performed for example as discussed above with reference to Steps and .

If the work request corresponds to a proxy QP and not to a proxy CQ then an error is identified in Step . When the error is identified the work request may be discarded and or an error message may be transmitted to the host e.g. a control application or another application on the host .

If the work request does not correspond to a proxy QP or a proxy CQ then the completion is processed as normal in Step . Specifically a completion is generated and added to the CQ corresponding to the application. Adding the completion may be performed using DMA with host memory. Further adding the completion may include updating the CQ descriptor to indicate that a new completion is added.

Returning to Step if the work request corresponds to a proxy CQ then in Step the completion is redirected to the proxy CPU. In Step processing of the completion is initiated by the proxy CPU in one or more embodiments of the invention. When received by the proxy CPU the proxy CPU buffers the completion until completions for work requests having preceding sequence number are written by the proxy CPU. In Step the completion is written in order to the CQ in one or more embodiments of the invention. Writing the completion may be performed similar to the discussion above with the following exception. If the work request was processed by the proxy CPU then the proxy CPU may change the completion from one specifying an instruction to a no op which indicates to the application that something was performed for the sequence number and the application does not need to perform additional actions. Alternatively if the proxy CPU only performs a subset of the actions performed by the application then the proxy CPU may change the completion to indicate that only a remaining subset of actions needs to be performed.

In Step the completion is processed by the application in one or more embodiments of the invention. Processing the completion may be performed using techniques known in the art. For completions that include no ops the application may merely update memory to indicate that the work request having the sequence number in the completion was received but no processing on the part of the application needs to be completed.

The following example is for explanatory purposes only and not intended to limit the scope of the invention. In the following example consider the scenario in which a distributed storage system such as a distributed database is accessed by one or more requester systems . Specifically the distributed storage system includes storage system 1 and storage system X as well as other storage systems not shown . Each storage system includes a storage server and a host channel adapter . The requester system includes a requester device and a requester host channel adapter . An Infiniband network connects the host channel adapters.

Continuing with the example to access and request manipulation of data a requester application not shown on the requestor device first requests via the requester host channel adapter to obtain a lock on the particular data in the storage server having the data. Then the requester application requests data manipulation by a responder application not shown on the storage server.

In the example consider the scenario in which the requester application issues the first request to storage system 1 for the lock and then issues a second request immediately to perform the data manipulation if the lock is acquired. Further in the example the responder application offloads to a proxy CPU on storage system 1 host channel adapter to obtain locks for requester applications but not to perform any data manipulation.

In the example when the storage system 1 host channel adapter receives the first request the storage system 1 HCA identifies that the QP and the CQ referenced in the first request are proxy QP and CQ respectively. Further the storage system 1 HCA determines that because the first request is to obtain a lock the first request matches the filter criteria for transmitting the work request to the proxy CPU. Accordingly the first request is a request for processing by the proxy CPU on the storage system 1 HCA . Accordingly the request is routed to the proxy CPU.

Continuing with the example while the proxy CPU is in the process of obtaining the lock the second request is received by the storage system 1 HCA . Although the second request is for a proxy QP and proxy CQ because the second request is for data manipulation the second request does not match the filter criteria. Thus the second request is processed as normal without being routed to the proxy CPU. In other words the second request is processed by the storage system 1 HCA to be stored in the receive queue for processing by the responder application. Further a completion for the second request i.e. second completion is generated. Because the second request is after the first request the second request has a higher sequence number. Thus the second completion has the higher sequence number. Further because the CQ of the QP in the second request is a proxy CQ the second completion is routed to the proxy CPU.

When the proxy CPU receives the second completion the proxy CPU buffers the second completion based on the sequence number of the second request being greater than the first request and not having completed processing of the first request. After the proxy CPU completes processing the first request to acquire the lock the proxy CPU writes a first completion for the first request to the CQ for the responder application. The first completion includes the sequence number of the first request and a no op to indicate that the responder application does not need to perform any action. Only after the first completion is written the proxy CPU writes the second completion for the second request. The second completion includes the sequence number of the second requests and indicates that a request is waiting to be processed by the responder application in the receive queue.

Accordingly the responder application does nothing with the first request except update internal records. The responder application then performs the data manipulation requested in the second request because the lock was acquired in the first request. As shown in the example by routing completions to the proxy CPU regardless of whether the corresponding requests were offloaded embodiments ensure that completions are processed in order by the responder application.

While the invention has been described with respect to a limited number of embodiments those skilled in the art having benefit of this disclosure will appreciate that other embodiments can be devised which do not depart from the scope of the invention as disclosed herein. Accordingly the scope of the invention should be limited only by the attached claims.

