---

title: Cache management for memory operations
abstract: Embodiments of the present invention provides for the execution of threads and/or workitems on multiple processors of a heterogeneous computing system in a manner that they can share data correctly and efficiently. Disclosed method, system, and article of manufacture embodiments include, responsive to an instruction from a sequence of instructions of a work-item, determining an ordering of visibility to other work-items of one or more other data items in relation to a particular data item, and performing at least one cache operation upon at least one of the particular data item or the other data items present in any one or more cache memories in accordance with the determined ordering. The semantics of the instruction includes a memory operation upon the particular data item.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08935475&OS=08935475&RS=08935475
owner: Advanced Micro Devices, Inc.
number: 08935475
owner_city: Sunnyvale
owner_country: US
publication_date: 20120330
---
The present invention is generally directed to computing systems. More particularly the present invention is directed to memory operations executed in a heterogeneous computing system.

The desire to use a graphics processing unit GPU for general computation has become much more pronounced recently due to the GPU s exemplary performance per unit power and or cost. The computational capabilities for GPUs generally have grown at a rate exceeding that of the corresponding central processing unit CPU platforms. This growth coupled with the explosion of the mobile computing market e.g. notebooks mobile smart phones tablets etc. and its necessary supporting server enterprise systems has been used to provide a specified quality of desired user experience. Consequently the combined use of CPUs and GPUs for executing workloads with data parallel content is becoming a volume technology.

However GPUs have traditionally operated in a constrained programming environment available primarily for the acceleration of graphics. These constraints arose from the fact that GPUs did not have as rich a programming ecosystem as CPUs. Their use therefore has been mostly limited to two dimensional 2D and three dimensional 3D graphics and a few leading edge multimedia applications which are already accustomed to dealing with graphics and video application programming interfaces APIs .

With the advent of multi vendor supported OpenCL and DirectCompute standard APIs and supporting tools the limitations of the GPUs in traditional applications has been extended beyond traditional graphics. Although OpenCL and DirectCompute are a promising start there are many hurdles remaining to creating an environment and ecosystem that allows the combination of a CPU and a GPU to be used as fluidly as the CPU for most programming tasks.

Existing computing systems often include multiple processing devices. For example some computing systems include both a CPU and a GPU on separate chips e.g. the CPU might be located on a motherboard and the GPU might be located on a graphics card or in a single chip package. Both of these arrangements however still include significant challenges associated with i separate memory systems ii efficient scheduling iii providing quality of service QoS guarantees between processes iv programming model and v compiling to multiple target instruction set architectures ISAs all while minimizing power consumption.

For example the discrete chip arrangement forces system and software architects to utilize chip to chip interfaces for each processor to access memory. While these external interfaces e.g. chip to chip negatively affect memory latency and power consumption for cooperating heterogeneous processors the separate memory systems i.e. separate address spaces and driver managed shared memory create overhead that becomes unacceptable for fine grain offload.

Although GPUs accelerated processing units APUs and general purpose use of the graphics processing unit GPGPU are commonly used terms in this field the expression accelerated processing device APD is considered to be a broader expression. For example APD refers to any cooperating collection of hardware and or software that performs those functions and computations associated with accelerating graphics processing tasks data parallel tasks or nested data parallel tasks in an accelerated manner with respect to resources such as conventional CPUs conventional GPUs and or combinations thereof.

Embodiments of the present invention provide for the execution of threads and or workitems on multiple processors of a heterogeneous computing system in a manner that they can share data correctly and efficiently. Disclosed embodiments include responsive to an instruction from a sequence of instructions of a work item determining an ordering of visibility to other work items of one or more other data items in relation to a particular data item and performing at least one cache operation upon at least one of the particular data item or the other data items present in any one or more cache memories in accordance with the determined ordering. The semantics of the instruction includes a memory operation upon the particular data item.

Further features and advantages of the invention as well as the structure and operation of various embodiments of the invention are described in detail below with reference to the accompanying drawings. It is noted that the invention is not limited to the specific embodiments described herein. Such embodiments are presented herein for illustrative purposes only. Additional embodiments will be apparent to persons skilled in the relevant art s based on the teachings contained herein.

The features and advantages of the present invention will become more apparent from the detailed description set forth below when taken in conjunction with the drawings in which like reference characters identify corresponding elements throughout. In the drawings like reference numbers generally indicate identical functionally similar and or structurally similar elements. The drawing in which an element first appears is indicated by the leftmost digit s in the corresponding reference number.

In the detailed description that follows references to one embodiment an embodiment an example embodiment etc. indicate that the embodiment described may include a particular feature structure or characteristic but every embodiment may not necessarily include the particular feature structure or characteristic. Moreover such phrases are not necessarily referring to the same embodiment. Further when a particular feature structure or characteristic is described in connection with an embodiment it is submitted that it is within the knowledge of one skilled in the art to affect such feature structure or characteristic in connection with other embodiments whether or not explicitly described.

The term embodiments of the invention does not require that all embodiments of the invention include the discussed feature advantage or mode of operation. Alternate embodiments may be devised without departing from the scope of the invention and well known elements of the invention may not be described in detail or may be omitted so as not to obscure the relevant details of the invention. In addition the terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. For example as used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will be further understood that the terms comprises comprising includes and or including when used herein specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

Embodiments of the present invention may be used in any computer system computing device entertainment system media system game systems communication device personal digital assistant or any system using two or more processors. The present invention is particularly useful where the system comprises a heterogeneous computing system such as the systems illustrated in . A heterogeneous computing system as the term is used herein is a computing system in which multiple types of processors are available. The multiple types of processors can include for example CPUs and APDs such as CPUs.

Embodiments of the present invention enable threads and workitems executing on multiple processors including processors of different types to efficiently share data while ensuring the integrity of the data items accessed by the workitems. Ensuring the integrity of accessed data items includes ensuring that the value of a read returns the latest value of that data item regardless of where that data item was last updated. Embodiments perform cache operations as a side effect of a memory operation in order to enforce a determined visibility ordering of data items that are accessed by memory operations. A data item is said to be visible to a workitem when it is in a memory that is accessible to that workitem. For a data item to be visible to workitems executing on multiple processors that data item should be in a common memory that is shared by all the multiple processors. The visibility ordering also referred to as ordering of visibility of data items as described below is a partial ordering of the order in which data items are made visible i.e. available for access to workitems across multiple processors. Embodiments use the determined visibility ordering to reduce the overhead associated with cache operations associated with memory operations. The caches associated with the respective processors such as the CPUs and APDs can be managed using embodiments disclosed herein to perform cache operations e.g. cache flush cache invalidate with reduced traffic between the respective caches and the system memory.

An example heterogeneous computing system is shown in according to an embodiment of the present invention. Heterogeneous computing system can include one or more CPUs such as CPU and one or more APDs such as APD . Heterogeneous computing system can also include at least one system memory at least one persistent storage device at least one system bus memory order determiner and a cache updater .

In one embodiment of the present invention the system is formed on a single silicon die or package combining CPU and APD to provide a unified programming and execution environment. This environment enables the APD to be used as fluidly as the CPU for some programming tasks. However it is not an absolute requirement of this invention that the CPU and APD be formed on a single silicon die. In some embodiments it is possible for them to be formed separately and mounted on the same or different substrates.

CPU can include a commercially available control processor or a custom control processor. CPU can include one or more of a control processor field programmable gate array FPGA application specific integrated circuit ASIC or digital signal processor DSP . CPU according to one embodiment initiates and controls the execution of an application by for example distributing the processing associated with that application across the CPU and other processing resources such as the APD . CPU for example executes control logic that controls the operation of heterogeneous computing system . CPU can be a multi core CPU such as a multi core CPU with two CPU cores and . CPU in addition to any control circuitry can include CPU cache memory such as the cache memories and of CPU cores and respectively. CPU cache memories and can be used to temporarily hold instructions and or parameter values during the execution of an application on CPU cores and respectively. For example CPU cache memory can be used to temporarily hold one or more control logic instructions values of variables or values of constant parameters from the system memory during the execution of control logic instructions on CPU core . In some embodiments CPU can also include specialized vector instruction processing units. For example CPU core can include a Streaming SIMD Extensions SSE unit that can efficiently process vectored instructions. A person skilled in the art will understand that CPU can include more or less than the CPU cores in the example chosen and can also have either no cache memories or more complex cache memory hierarchies.

APD can execute specialized code for selected functions. For example APD can be used to execute graphics functions such as graphics pipeline computations such as geometric computations and rendering of image on a display. APD may be a GPU. In general APD may be preferred for the execution of data parallel code. APD can include its own compute units not shown such as but not limited to one or more single instruction multiple data SIMD processing cores.

Some graphics pipeline operations such as pixel processing and other parallel computation operations can require that the same command stream or compute kernel be performed on streams or collections of input data elements. Respective instantiations of the same compute kernel can be executed concurrently on multiple compute units in order to process such data elements in parallel. As referred to herein for example a compute kernel is a function containing instructions declared in a program and executed on an APD compute unit. This function is also referred to as a kernel a shader a shader program or a program.

APD can include an APD global cache memory and one or more compute units and . A graphics memory can be included in or coupled to APD . Each compute unit and are associated with an APD local memory and respectively. Each compute unit includes one or more APD processing elements PE . For example compute unit includes APD processing elements and and compute unit includes APD processing elements and . Each APD processing element and is associated with at least one private memory PM and respectively. Each APD processing element can include one or more of a scalar and vector floating point units. The APD processing elements can also include special purpose units such as inverse square root units and sine cosine units. APD global cache memory can be coupled to a system memory such as system memory and or graphics memory such as graphics memory . According to an embodiment APD global cache memory may include two or more levels of cache memories. Graphics memory may not be limited to graphics.

System memory can include at least one non persistent memory such as dynamic random access memory DRAM . System memory can hold processing logic instructions constant values and variable values during execution of portions of applications or other processing logic. For example in one embodiment the control logic and or other processing logic of memory order determiner and cache updater can reside within system memory during execution. The term processing logic as used herein refers to control flow instructions instructions for performing computations and instructions for associated access to resources.

Persistent memory includes one or more storage devices capable of storing digital data such as magnetic disk optical disk or flash memory. Persistent memory can for example store at least parts of instruction logic of memory order determiner and cache updater . For example at the startup of heterogeneous computing system the operating system and other application software can be loaded in to system memory from persistent storage .

System bus can include a Peripheral Component Interconnect PCI bus Advanced Microcontroller Bus Architecture AMBA bus Industry Standard Architecture ISA bus or such a device. System bus can also include a network such as a local area network LAN . System bus includes the functionality to couple components including components of heterogeneous computing system .

Graphics memory is coupled to system bus and to APD . Graphics memory is in general used to hold data transferred from system memory for fast access by the APD. For example the interface between APD and graphics memory can be several times faster than the system bus interface .

Memory order determiner and cache updater include processing logic respectively to determine visibility requirements of data items and to execute cache operations in accordance with embodiments. Memory order determiner may be configured to determine visibility requirements of a set of data items according to a set of visibility rules . For example in response to receiving an instruction such as an instruction with load acquire semantics or store release semantics described below memory order determiner may be configured to find a set of data items accessed in a sequence of instructions and to determine a relative ordering of visibility between the set of data items and the data item accessed by the current instruction. Cache updater may be configured to perform cache operations such that the visibility ordering of data items that is determined by memory order determined is achieved. The functionalities of memory order determiner and cache updater are described below in relation to . Memory order determiner and cache updater can be implemented using software firmware hardware or any combination thereof. When implemented in software memory order determiner can be a computer program that when compiled and executed resides in system memory . In source code form and or compiled executable form memory order determiner and cache updater can be stored in persistent memory or other computer readable storage medium. In one embodiment some or all of the functionalities of memory order determiner and cache updater are specified in a hardware description language such as Verilog RTL netlists to enable ultimately configuring a manufacturing process through the generation of maskworks photomasks to generate a hardware device embodying aspects of the invention described herein.

A person of skill in the art will understand that heterogeneous computing systems and can include more or less components than shown in .

Compute kernels may include data parallel kernels and task parallel kernels. In general CPUs are better suited to execute task parallel kernels whereas data parallel kernels are better suited for APD execution. Both CPUs and APDs can execute multiple instances of a compute kernel in parallel. Each instance of an executing compute kernel may be referred to as a workitem. In APD for example workitems may simultaneously execute on each processing element and . In CPU workitems may simultaneously execute on each core and . illustrates a workitems and executing respectively in processing elements and in APD and in cores and of CPU .

System memory may include sequence of instructions . Sequence of instructions may be a sequence of instructions from an application not shown . The application may be executing on CPU and may execute compute kernels or programs in APD and or CPU . Sequence of instructions may have the instructions in order of how the respective instructions appear in the source code or in order of how a compiler orders the respective source code instructions. Data items represent data items that are accessed by instructions in sequence of instructions . Data items can include data in system memory and or in any one or more of the cache or local memories in the respective CPU cores or APD. Memory operations and cache operations include executable code for respectively implementing memory operations and cache operations from sequence of instructions .

At process an instruction from a sequence of instructions of the executing one or more workitems is received. According to an embodiment the received instruction is from a workitem executing on an APD. The sequence of instructions can include one or more memory operations. Memory operations include instructions for performing an operation on one or more data items in memory. Memory operations can include load or read operations and store or write operations. A load operation reads one or more data items from memory to registers in order that workitems can access those data items. A store operation writes one or more data items from a register to memory. The sequence of instructions may include one or more instruction types that perform load operations. Similarly one or more instruction types in the sequence may be configured for store operations. The instruction received at process may be a memory operation such as a load operation or store operation which includes semantics to perform one or more cache operations to enforce a relative visibility ordering of data items accessed by the received instruction and the sequence of instructions.

At process a visibility ordering of data items accessed by instructions in the sequence of instructions is determined in relation to the instruction received at process . The relative ordering of the visibility of data items is determined so that one or more visibility rules are enforced. The rides are directed to ensuring the correctness of values of data items when those data items are accessed e.g. are written and or read by a single workitem or multiple workitems executing on the same or different processors or processing elements.

The ordering of instructions in the source code is not necessarily the order in which instructions are executed. The compiler and or other component may reorder the instructions in some situations to be different than the order in the sequence of instructions in the source code. In some other situations the hardware may reorder the execution of the instructions differently than the order of instruction issue. In still other situations such as in the embodiment illustrated in in which respective processors have one or more local and or cache memories the visibility of the latest value of a data item to workitems executing on the same processor or different processors may be different. The visibility of data items may be different because for example the latest values of those data items may not be propagated to the system memory e.g. the common memory accessible to separate processors in the system from respective local memories or cache memories or because the latest values are not propagated from the system memory to respective local memories or cache memories. The visibility of data items may also be affected by each processor first accessing and or writing that data item to a local memory or cache and only subsequently writing that data item to a common memory shared with other processors. The ordering of data item visibility determined in process may be directed to selectively ensuring that data items are correctly visible to the workitems that access those data items. The visibility ordering of data items and rules referred to herein as visibility rules for ordering is further described below in relation to .

At process one or more cache operations are performed in accordance with the visibility ordering determined at process . Cache operations can include one or more of a cache flush or a cache invalidate operation. The cache flush may be performed to write data items that are updated in a local or cache memory to system memory so that those data items are visible to workitems executing on the same or different processors. According to an embodiment the cache flush operation may be selectively performed in order to flush one or more selected data items rather than flushing all the values in the one or more caches. Selectively flushing data items from the cache results in less traffic between the caches and system memory when compared to flushing entire caches. Cache invalidate operation may be performed in order to mark copies of a data item invalid in one or more other caches when that data item is updated in one cache or system memory. According to embodiments the invalidate operation may be performed on selected caches rather than all caches. Selecting of data items to be flushed in flush operations and selecting of caches to be invalidated in an invalidate operation may be performed in accordance with rules to enforce an ordering of memory operations. According to an embodiment one or more of the above descried cache flush or cache invalidate operation are performed in response to an instruction that includes an associated load or store operation. The performing of cache operations is further described in relation to below.

Method determines the relative visibility ordering of data items accessed in a sequence of instructions in accordance with visibility rules. The visibility rules may include preconfigured rules. In some embodiments the rules may also include one or more dynamically generated rules. Dynamically generated rules can adapt the system to current system conditions. For example if the interface between the caches and system memory is not loaded with traffic in excess of a threshold the visibility rules may be adjusted by removing the reduction of memory traffic from goals to be considered.

At process the ordering of the sequence of instructions in the source code is determined. In general the ordering of the instructions in the source code reflects the intent of the programmer for ordering or the various operations. The ordering of the instructions in the source code may be one of the considerations in selecting applicable ordering rules.

At process it is determined whether respective memory operations and corresponding instructions are related by a synchronization operation. Synchronization operations are operations such as barrier and sync operations that explicitly cause workitems to wait for one or more other workitems to reach a corresponding point in their instruction sequences.

At process the visibility ordering of data items for the sequence of instructions is determined. The ordering of execution may be based upon the characteristics of the sequence of instructions determined in one or more of processes and on a set of rules of ordering. The rules may include preconfigured rules and dynamically configured rules. The visibility ordering may be in relation to the one or more data items accessed by the current received instruction from the sequence of instructions.

The visibility rules in accordance with an embodiment specify a sequenced before ordering between instructions. Sequenced before is an asymmetric transitive pair wise relation between instructions executed by a single workitem which induces a partial ordering among those instructions. Given any two instructions X and Y if X is sequenced before Y then the execution of X shall precede the execution of Y. If A is not sequenced before Y and Y is not sequenced before X then X and Y are not sequenced. The phrase X sb Y is used herein to denote that X is sequenced before Y.

In the description below for two different memory operations X and Y in sequenced before order for a workitem X sb Y specifies that X is before Y in sequenced before order X Y indicates that Y must be visible only after X is visible and transitivity for a single work item applies if X Y and Y Z then X Z.

The rules may specify that if X sb Y then an ordering of the sequence of instructions should be according to conditions that include the following 

The visibility rules described above provide for an execution ordering of instructions in the sequence of instructions. The relative visibility ordering of the data items accessed by the sequence of instructions can be determined based upon the rules. As described in relation to the visibility ordering of the data items may be used to optimize cache operations to minimize traffic between system memory and cache memories.

At process it is determined whether the instruction includes a store release functionality. According to an embodiment the store release functionality may be included in the semantics for a store release instruction type and an atomic instruction type with store release. The semantics for store release for example considering an instruction such as store release Y includes writing data item Y to a memory local or system and ensuring that for any data item X such that X sb Y X is visible before Y.

If at process it is determined that the instruction includes store release semantics then method proceeds to process . At process the rules applicable to the received instruction are determined. The determination of the applicable rules may be based upon the type of instruction e.g. instruction with store release semantics and the data items that are accessed by the received instruction. According to an embodiment based upon the type of instruction and the data items accessed the applicable ordering rules can be determined from the sequenced before rules described above.

At process cache operations in accordance with the determined rules are executed. The executed cache operations can include cache flush operations. According to an embodiment the flush operation selectively flushes data items that are required to be visible before the currently accessed data item. As noted above selectively flushing data items from cache may result in a reduction in traffic between system memory and cache memories. According to another embodiment the flush operation may be performed for all data items in cache. Cache operations are further described below in relation to .

At process the memory operation corresponding to the received instruction is performed. For example the data item X accessed by a received store release instruction may be written to memory. Writing of data item X to memory in executing a store release operation includes writing data item X to system memory so that X would be visible to other workitems. The writing of X may include writing of X to a local or cache memory and to system memory. The operation may or may not include a flush operation specifically for data item X. Note that in accordance with store release semantics any other data items Y that were required to be visible before the currently accessed data item X would have already been made visible by a technique such as for example selectively flushing one or more caches for those data items Y. Completion of process completes the processing of the operation with store release semantics.

If at process it was determined that the received instruction does not include store release semantics method proceeds to process . At process it is determined whether the received instruction includes load acquire semantics. According to an embodiment load acquire semantics may be included in instructions of a type load acquire and in atomic instructions that have a load acquire aspect. The semantics for load acquire of a data item Y for example includes accessing Y only after any other data items X where X sb Y have already been made visible before Y.

At process the rules applicable to the received instruction type e.g. instruction with load acquire semantics and accessed one or more data items are determined. According to an embodiment based upon the type of instruction and the data items accessed the applicable ordering rules can be determined from the sequenced before rules described above.

At process cache operations in accordance with the determined rules are executed. The executed cache operations can include cache flush operations. According to an embodiment the flush operation selectively flushes data items that are required to be visible before the currently accessed data item. As noted above selectively flushing data items from cache may result in a reduction in traffic between system memory and cache memories. According to another embodiment the flush operation may be performed for all data items in cache. Cache operations are further described below in relation to .

At process the memory operation corresponding to the received instruction is performed. For example the data item Y accessed by a received load acquire instruction may be read from memory. Reading data item Y from memory in executing a load acquire operation may include ensuring that any data items X where X sb Y are made visible to workitems before X is read. Completion of process completes the processing of the operation with load acquire semantics.

If at process it is determined that the received instruction does not include load acquire semantics then at process processing of the received instruction may be performed in accordance with corresponding instruction semantics.

At process it is determined whether a cache flush operation is required to be performed. Both types of memory operations discussed above operations with load acquire semantics and operations with store release semantics require that one or more cache flush operations are performed. For example a store release Y instruction may require that one or more caches are flushed such that any data item X where X sb Y is visible to workitems before Y is written. Likewise a load acquire Y instruction may require that one or more caches are flushed such that any data item X where X sb Y is visible to workitems before Y is read.

If a cache flush is required at process one or more cache operation are performed. According to an embodiment as described above the one or more cache flush operations may include selectively flushing data items from caches to system memory. The selective flushing of data items may be performed in order to enforce the required sequenced before ordering rules described above with reduced traffic between the one or more caches and the system memory. The selective flushing may be based upon flushing individual data items or upon areas or blocks of cache memory. According to another embodiment the entire cache may be flushed in one or more caches.

At process it is determined whether a cache invalidate operation is required. A cache invalidate operation may be required for example for a store release Y instruction in order to write the new value of data item Y and then to mark all cached instances of data item Y as invalid. A load acquire Y operation may not require a cache invalidate operation.

At process if required the cache invalidate operation is performed. The cache invalidation operation can be performed to invalidate one or more selected data items. According to an embodiment instances of data item Y in all caches are invalidated. According to another embodiment the invalidation of instances of data item X may be performed only in selected caches. For example the sequenced before rules may indicate that the instances of data item Y in one or more caches may not require invalidation in order to be compliant with the rules.

Method may be repeated for each memory operation performed in order to perform cache operations as required.

The present invention has been described above with the aid of functional building blocks illustrating the implementation of specified functions and relationships thereof. The boundaries of these functional building blocks have been arbitrarily defined herein for the convenience of the description. Alternate boundaries can be defined so long as the specified functions and relationships thereof are appropriately performed.

The foregoing description of the specific embodiments will so fully reveal the general nature of the invention that others can by applying knowledge within the skill of the art readily modify and or adapt for various applications such specific embodiments without undue experimentation without departing from the general concept of the present invention. Therefore such adaptations and modifications are intended to be within the meaning and range of equivalents of the disclosed embodiments based on the teaching and guidance presented herein. It is to be understood that the phraseology or terminology herein is for the purpose of description and not of limitation such that the terminology or phraseology of the present specification is to be interpreted by the skilled artisan in light of the teachings and guidance.

The breadth and scope of the present invention should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents.

