---

title: Method and apparatus for inferring the geographic location of captured scene depictions
abstract: A method and apparatus for determining a geographic location of a scene in a captured depiction comprising extracting a first set of features from the captured depiction by algorithmically analyzing the captured depiction, matching the extracted features of the captured depiction against a second set of extracted features associated with reference depictions with known geographic locations and when the matching is successful, identifying the geographic location of the scene in the captured depiction based on a known geographic location of a matching reference depiction from the reference depictions.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08989483&OS=08989483&RS=08989483
owner: SRI International
number: 08989483
owner_city: Menlo Park
owner_country: US
publication_date: 20120611
---
This application claims the benefit of U.S. provisional patent application Nos. 61 495 765 and 61 495 777 both filed Jun. 10 2011 the disclosures of which are incorporated herein by reference in their entirety.

Governmental Interest The invention described herein was made with Government support under contract number W91CRB 08 C 0117 awarded by the U.S. Army. This invention was also made with Government support under contract number HM1582 09 C 0017 awarded by the National Geospatial Intelligence Agency. The Government has certain rights in the invention.

Embodiments of the present invention generally relate to geolocalization and more particularly to a method and apparatus for inferring the geographic location of scenes in captured scene depictions using geo referenced data.

Determining the geographic location of scenes in captured depictions here depiction is used inclusively for electronic data that represents the contents of a scene regardless of medium including photographs and other still images video sequences drawings and or textual descriptions of the contents of a scene for example is referred to as geolocalization of content. Traditional approaches for geolocalization rely on expressly encoded location data e.g. metadata that is either embedded within the depiction itself or associated with it such as global positioning system GPS coordinates and the like. If such metadata is not available geolocalization of a depiction such as an image is a challenging problem.

The location of an aerial or satellite image is sometimes determined by comparing the image to an existing geo referenced database of satellite images and selecting a statistically matched item as the result. However such image comparisons do not account for angle discrepancies for example where the images in the database are top view or aerial imagery and the images required to be geolocalized consist of narrow field of view ground plane images such as tourist images in an urban or suburban environment. Thus with two or three multiple coordinate systems and angles of view performing image comparisons becomes computationally challenging.

Therefore there is a need in the art for geolocalizing scene depictions such as images and more particularly a method and apparatus for inferring the geographic location of captured depictions using geo referenced data captured from a different perspective.

An apparatus and or method for inferring the geographic location of scenes in captured depictions using geo referenced data substantially as shown in and or described in connection with at least one of the figures as set forth more completely in the claims.

Various advantages aspects and features of the present disclosure as well as details of an illustrated embodiment thereof are more fully understood from the following description and drawings.

Embodiments of the present invention generally relate to determining the geographic location of a captured depiction whose location is unknown using other geo referenced depiction data captured from a different perspective. According to one embodiment the captured depictions are narrow field of view NFOV ground plane and or street view SV images and the method determines the geographic location of the scene depicted in a captured image by extracting a set of features from a database of reference depictions which according to some embodiments include satellite SAT imagery three dimensional 3D model data and oblique bird s eye view BEV images i.e. oblique aerial imagery of an area of interest. In an exemplary embodiment feature extraction includes annotating those images with the objects that they are determined to contain such as trees bushes houses and the like. In some embodiments the database includes hyperspectral HS multispectral MS as well as standard imagery. The captured NFOV images are also annotated and the respective annotations for a captured depiction and the reference depictions are compared e.g. using a statistical method to determine a best match. The geographic location of the captured depiction such as an image can be identified by reference to the known geographic location of the matching referencing depiction.

The captured sensed depictions are presented to the registration module . The registration module annotates the captured depictions by extracting features of the depictions. For example in one embodiment module recognizes and extracts entities or objects present in the captured depictions adjusts for pose of the sensor in embodiments where pose is known or can be readily computed and determines relationships between the detected entities. The annotations made by registration module thus represent knowledge of the various types of objects present in e.g. a captured image. According to this embodiment the registration module similarly annotates reference depictions within database of the particular area of interest which covers the general location of scene . The registration module creates a set of extracted features which comprise objects such as describable houses buildings trees patios and the like as well as features or aspects of such objects for example corners of buildings facades and the like.

The extracted features are transmitted into the classification module which classifies the extracted features into one or more semantic classes used as an internal data representation. In one embodiment the semantic classes include but are not limited to houses trees bushes patios desks roads intersections hedges fences pools and unclassified objects. According to some embodiments the extracted features may also include a color description to aid in matching. Constraints are formed from these classes. According to one embodiment constraints comprise one or more of feature similarity constraints geometrical constraints topological constraints and or geospatial constraints and may also include other image attributes.

The constraints derived from the captured depiction are coupled to the matching module along with the extracted entities features . The matching module matches the constraints against a corresponding set of constraints derived from the reference depictions in database comprising a statistical representation of the entities and their associated classes for an area of interest. The matching module determines whether a match exists for example by assessing whether when the constraints and the constraints derived from a given depiction of the reference data are sufficiently close e.g. within some e.g. predetermined threshold value of each other. In some embodiments extracted entities features are similarly matched. Once a sufficiently close match is found a geographic location is determined for the captured depiction of scene by reference to the known geographic location of the match in database .

In various embodiments the sensor is used to sense a corresponding variety of scenes such as terrain urban imagery and other real world scenery.

According to some embodiments the geolocalization module further comprises a query module . The query module is used to construct a geographically indexed database of the captured depictions using a plurality of matched captured depictions. The query module further provides an interactive search engine responsive to a query specifying a geographic location by returning one or more of the captured depictions whose corresponding derived location matches the location specified by the query. According to some embodiments the captured depictions are displayed on an interactive map corresponding to their matched locations in the reference depiction data.

The RELM determines relationships between the entities . For example if the backyard photo contains three trees one hedge a picnic table and a brown fence the RELM determines that the trees are lined up in parallel at a ninety degree angle to the brown fence on one side and the hedge on the other side and the picnic table is at the midpoint between the fence and the hedge. These descriptors are collectively referred to as relationships and are stored in the attributes . The FEM extracts features in the captured image and stores the features in the attributes .

For example if the captured image is a photograph of the backyard as shown in there would be four groups trees hedge fence and table. The tree group contains three entries and each other group only contains one entry or similar to shown in the semantic object representation . illustrates a satellite image and its corresponding semantic map as well as bird s eye view image and its representative semantic map . In some embodiments annotation module allows a user of the geolocalization module to additionally annotate the captured image for complete semantic description. The classification module forms a set of constraints based on the grouping and annotation and transmits the constraints to the matching module shown in .

Subsequently the entities of the captured image and the satellite images are transferred to a data structure for example semantic concept graphs SCGs by the graphing module . A semantic concept graph is a graphical representation of the hierarchical relationships between the entities in a particular captured image satellite image or bird s eye view image. SCGs may have hard or soft edges indicating the strength of the relationships and are enhanced using existing geographic information system GIS data. The comparison module compares the extracted features of the captured depiction with the extracted features of the reference depictions stored in database and determines a first set of candidate matches based on a first matching score. If there is a set of matches found a second matching score is calculated between the extracted features for the captured depiction and the set of candidate matches respectively. If the second matching score for a best one of the candidate matches satisfies a threshold value where according to one embodiment the threshold value is configured by a user of the geolocalization module the captured depiction is determined to have successfully matched with the best candidate from the reference depictions. The matching module returns the known geographical location of the successfully matching reference depiction from the satellite and bird s eye view images of database .

The memory or computer readable medium stores non transient processor executable instructions and or data that may be executed by and or used by the processor . These processor executable instructions may comprise firmware software and the like or some combination thereof. Modules having processor executable instructions that are stored in the memory comprise a registration module a matching module and a classification module . As described below in an exemplary embodiment the registration module comprises a semantic extraction module a relationship module and a feature extraction module . The matching module comprises a scoring module a graphing module a comparison module and a terrain matching module . The classification module comprises in an exemplary embodiment a grouping module and an annotation module . The memory also stores a database . The computer may be programmed with one or more operating systems generally referred to as operating system OS which may include OS 2 Java Virtual Machine Linux Solaris Unix HPUX AIX Windows Windows95 Windows98 Windows NT and Windows2000 Windows ME Windows XP Windows Server among other known platforms. At least a portion of the operating system may be disposed in the memory . The memory may include one or more of the following random access memory read only memory magneto resistive read write memory optical read write memory cache memory magnetic read write memory and the like as well as signal bearing media as described below.

The transition module is coupled to the database and the pruning module . The transition module receives the terrain features and the 3D terrain data from database and converts the terrain data into a plurality of depth images at each of a plurality of points in the 3D terrain. The depth images provide a basis on which to match the pruned features and the depth images in the approximation and matching module AMM . According to one embodiment the AMM uses polyline approximation to establish a skyline feature of the captured image terrain and the 3D terrain from the depth image as discussed with regard to in detail below. The result of the terrain matching module is matched terrain assisting in geolocalizing a particular photograph.

At step features are extracted from a captured depiction by the registration module . In one embodiment the depiction is transmitted to the method via a network. In another embodiment the depiction is transmitted to the method via sensor coupled to the geolocalization module as shown in . In other embodiments a plurality of images forming a video stream is transmitted to the geolocalization module and the method operates on the streaming video. In some embodiments relationships among the extracted features are also extracted. For example the geospatial or topological relationship among various trees hedges fences and the like are extracted from the captured image. Features of the image such as long edges landmarks and the like are also extracted. Collectively these extracted features are attributes of the image or other depiction that will next be used to classify the depiction and form constraints.

At step the extracted features are classified into semantic classes by the classification module . In one embodiment the semantic classes include but are not limited to houses trees bushes patios desks roads intersections hedges fences pools and unclassified objects. According to some embodiments the extracted features may also include a color description to aid in matching.

At step constraints are formed based on the classes of extracted features and the attributes of the captured depiction by the classification module . The constraints are used in removing comparisons which are clear mismatches with the captured image. For example if a particular sector of the AOI contains three trees next to a road and the captured image contains two trees next to a road the sector in question will be removed from the matching step.

At step the constraints are used by the matching module to match against constraints from a database containing constraints extracted from the reference depiction data. If the captured depiction is of terrain the terrain matching module performs matching against the database which may also contain terrain satellite and bird s eye view images in some embodiments. The matching module takes into account the relationships among the extracted features such as the geo spatial relationship the topological relationship the geometry size and shape of the objects as well as a subset of image features such as long edges large homogeneous regions and land marks. According to one embodiment dynamic graph matching using data structures such as semantic concept graphs is applied to the captured images and the database reference depictions. GIS data is further applied to improve accuracy of the results.

At step a determination is made as to whether a match exists for the captured depiction and the geographic location of the matching reference depiction is returned. The determination is based on a comparison between the first set of constraints from the captured depiction and the second set of constraints from the reference depictions stored in database . If the constraints match within a predefined threshold the two sets of constraints are determined to be matches. At step the method ends.

At step captured depictions are transferred to a data structure such as a graph using the graphing module . According to one embodiment the graph is a semantic concept graph where semantic classes include buildings roads trees pools ponds structure sculptures grass lawns and parking lots amongst others. Each node of the graph corresponds to a depicted feature from the captured image. The attribute of the node is the class that the depicted feature belongs to. If two extracted features are adjacent there corresponding graph nodes are linked by an edge link. The link attribute is the relative position of the two nodes i.e. left upper left up upper right right bottom right bottom bottom left. In some embodiments the links may also be fuzzy links which represent confidence of the links or connections that may not necessarily be important. illustrates an example of a tourist image and the corresponding graph with soft and hard links.

At step maximum matching scores are computed for each building in the satellite images from the database in all directions. The matching score of two graphs is the number of nodes that both have the same attributes class labels and the same kind of links to the building number and types of links . A building in the reference depiction is said to be a semantic match of the building in the captured depiction if the building s matching score divided by the average matching score of all buildings is greater than a preconfigured threshold. Initially a coarse matching is performed where the extracted features of the captured depiction are matched with the extracted features of the reference depictions in database resulting in an initial set of candidate matches. A fine matching is then performed between the respective extracted features of the captured image and of the set of candidate matches.

At step a database of SAT BEV and oblique aerial imagery OAI images is created by the matching module . The database may contain several areas of interest that are generally known to be local to a captured image. A specific location of the captured depiction is not known. The SAT BEV and OAI imagery contains several buildings from an urban environment. According to one embodiment a possible source of the imagery is Microsoft s Bing Web service. The OAI images are warped to align with the SAT image coordinate system thereby aligning the ground plane for the OAI images. In one embodiment the dominant city block direction in the SAT imagery is determined and the BEV and OAI imagery is rotated before performing the warping.

At step facades of the buildings in the SAT imagery are extracted. To ensure least distortion in one embodiment only the facade planes which face the heading direction of the particular BEV and OAI image are considered. In some embodiments methods such as vertical vanishing point estimation are performed for grouping building edges into line segments and those corresponding to city block axes are removed. Then image rectification is performed by mapping the vanishing point to a point at infinity causing the building fa ade edges in the rectified BEV and OAI images to become parallel to the image scan lines.

According to one embodiment building edges and facades SAT Edge Extraction are extracted from BEV and OAI images by detecting building contours in the overheat SAT imagery as chains of line segments each corresponding to one face of a building. The chaining is achieved by linking the edges into edge chains based on proximity and then fitting the line segments to the edge chains. The line segments are split wherever the deviation of the edges from the fitted line segment becomes greater than a predefined threshold value. Consistent line segments are merged into longer line segments and the overall process is iterated a few times.

From the extracted line segments only those along the dominant fa ade direction in the BEV OAI are kept. The kept segments are warped into the rectified BEV OAI image coordinate system and are then mapped to the bottom of the buildings. Tops of the buildings are determined by sliding the mapped line segments horizontally. In some embodiments building tops are determined using a Graph Cut optimization of an objective function. Then four corners of each fa ade are determinable and mapped back to the unrectified BEV OAI imagery for texture retrieval.

The method then proceeds to step where a building corresponding to the captured image is found using the extracted facades in the BEV images. For a given pixel q in the captured image the local self similarity descriptor dq is computed by defining a patch centered at q and correlating it with a larger surrounding image region Rq to form a local correlation surface which is then transformed into a binned log polar representation to account for local spatial affine deformations. In one embodiment the matching is performed as disclosed in the paper entitled Matching local self similarities across E. Schechtman and M. Irani CVPR 2007 hereby incorporated by reference in its entirety.

At step patches of the fa ade are extracted by constructed a vocabulary tree of the features. The layout of local patches within each facade of buildings is used to create a statistical description of the facade pattern. Such statistical descriptions do not get affected by the appearance and viewpoint changes. A uniform grid of points on each extracted fa ade is sampled and a self similarity descriptor at each point is obtained. In one embodiment an adaptive Vocabulary Tree ADT structure is used where each feature from each fa ade populates the ADT based on the frequency of the fa ade IDs.

According to some embodiments pose estimation is further performed to facilitate in localizing the captured image for more precise geolocalization. Six degrees of freedom 6DOF pose is established for the sensor or capturing camera. In one embodiment seven point correspondences are established between the street view and BEV OAI imagery in a structure surrounding the matched fa ade. The correspondences are used to estimate a fundamental matrix F between the street view and BEV OAI images and thus the epipole of the BEV OAI images corresponds to the street view camera location in the BEV OAI coordinate system.

The sensor location in the BEV OAI image is mapped to absolute lat long coordinates using the ground plane correspondence with the SAT imagery. Finally the metric cms pixel information in the SAT image is used to estimate the sensor focal length which can be used in conjunction with any knowledge about the CCD array dimensions of the sensor to establish the field of view as well. The look at direction is also estimated using the metric information available from the SAT imagery by a simple trigonometric calculation known to those of ordinary skill in the art.

At step the method receives user input regarding three dimensional terrain data in the database . Through a user interface provided by the computer the user enters annotations of the 3D data in the database i.e. adds onto the existing annotations to improve matching functionality. At step features are extracted from the terrain data using a captured depiction and the annotations of the user. The method ends at step .

At step three dimensional terrain data stored in database is converted into height maps. In some embodiments the 3D data is LIDAR or DEM data. The method then proceeds to step where the method determines locations of ridges and basis using according to one embodiment a watershed algorithm. Gradient discontinuities such as basins are found in the height map where water accumulates. At step thresholds such as ridges for example are found as borders of each gradient discontinuity. When the method proceeds to step the ridges and basis are pruned by removing shallow thresholds based on their characteristics. According to one embodiment the characteristics comprise length and curvature of the thresholds.

At step the method generates a depth image as shown in at each point in a plurality of points in the 3D terrain data. From the depth image information the method will compute a skyline by searching for transition between the depths at step . Since the sky region has infinite depth and the non sky region has finite depth skylines can be efficiently computed by searching for the transition between infinite depth to finite depth in the vertical direction as shown in image of . According to one embodiment the result is a dense polyline of the extracted skyline.

At step the module approximates a skyline in a captured image by applying a polyline approximation technique. According to one embodiment the polyline segment is approximated by starting one line segment connecting two ends of the skyline. A progressive approximation procedure improves the approximation by splitting the line segments in the previous approximation until the error between the polyline and the original skyline is less than a preconfigured threshold.

At step feature points are extracted from the polyline. The feature points are key feature points which are locations on the polyline where the variance of the neighbor pixels exceeds a given threshold. The method then proceeds to step where match scores are computed between feature points and model skylines in database . For each key feature point in the skyline a key feature point from the model skyline is found that best matches the key feature point. The matching score between two key feature points is in one embodiment the Chamfer distance between the two local regions centered on those key feature points.

Given best matches for each key feature points random sample consensus RANSAC algorithms are applied to find the best transformation between the two skylines that result in the maximum number of inlier matching pairs.

With less discriminative skylines ridge matching and verification is further applied. Each ridge is represented by polyline approximation and for each line segment of the ridges in a given image the distance to and angle difference between the closed ridge segment of the model image is computed. If the distance and angle segment are below a predefined threshold value then the segment is deemed an inlier segment. The percentage of inlier segments over the total number of ridge segments is used as a similarity measure.

At step the method determines whether a match exists based on the match scores computed in step . The method then ends at step .

Various elements devices modules and circuits are described above in association with their respective functions. These elements devices modules and circuits are considered means for performing their respective functions as described herein. While the foregoing is directed to embodiments of the present invention other and further embodiments of the invention may be devised without departing from the basic scope thereof and the scope thereof is determined by the claims that follow.

