---

title: Managing I/O operations in a virtualized environment
abstract: A set of techniques is described for performing input/output (I/O) between a guest domain and a host domain in a virtualized environment. A pool of memory buffers is reserved for performing virtualized I/O operations. The reserved pool of memory buffers has static mappings that grant access to both the guest domain and the host domain. When a request to perform an I/O operation is received, the system can determine whether the memory buffers allocated to the I/O operation belong to the reserved pool. If the buffers are in the reserved pool, the host domain executes the I/O operation using the buffers without the need to map/unmap the buffers and perform TLB flushes. If the buffers are not in the reserved pool, the system can either copy the data into the reserved pool or perform the mapping and unmapping of the memory buffers to the address space of the host domain.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08938571&OS=08938571&RS=08938571
owner: Amazon Technologies, Inc.
number: 08938571
owner_city: Reno
owner_country: US
publication_date: 20120613
---
As an increasing number of applications and services are being made available over networks such as the Internet an increasing number of content application and or service providers are turning to technologies such as cloud computing. Cloud computing in general is an approach to providing access to electronic resources through services such as Web services where the hardware and or software used to support those services is dynamically scalable to meet the needs of the services at any given time. A user or customer typically will rent lease or otherwise pay for access to resources through the cloud and thus does not have to purchase and maintain the hardware and or software needed.

In this context many cloud computing providers utilize virtualization to allow multiple users to share the underlying hardware and or software resources. Virtualization can allow computing servers storage device or other resources to be partitioned into multiple isolated instances that are associated with e.g. owned by a particular user. In many cases the underlying technology used to enable virtualization utilizes memory sharing mechanisms or other tools to communicate information between these isolated instances or virtual machines. For example input output I O virtualization provides a way to abstract the guest operating systems and other upper layer protocols from the physical connections with the hardware. However while virtualized I O is useful in shared and secured environments it often tends to suffer from performance problems such as low throughput high latency and high latency jitter.

In the following description various embodiments will be illustrated by way of example and not by way of limitation in the figures of the accompanying drawings. References to various embodiments in this disclosure are not necessarily to the same embodiment and such references mean at least one. While specific implementations and other details are discussed it is to be understood that this is done for illustrative purposes only. A person skilled in the relevant art will recognize that other components and configurations may be used without departing from the scope and spirit of the claimed subject matter.

Systems and methods in accordance with various embodiments of the present disclosure may overcome one or more of the foregoing or other deficiencies experienced in conventional approaches for performing operations in a virtualized computing environment. In particular various embodiments provide approaches for reserving a pool of memory e.g. statically mapped pool of memory buffers to perform virtualized input output I O operations between a guest domain and a host domain in the virtualized environment.

In accordance with various embodiments one such approach involves a virtualized computing system that includes a hypervisor a host domain and one or more guest domains that share underlying hardware resources. In accordance with an embodiment upon initialization of a particular device driver in the guest domain the guest driver requests the hypervisor to allocate a pool of memory buffers for performing I O operations on that hardware device. Once the pool of memory is allocated it can be locked into memory to prevent swap and access to the pool can be provided to the host domain. The host domain can map the pool of memory into its own memory address space such that the host domain is enabled to access the same pool of memory buffers.

In various embodiments when the guest kernel I O stack requests buffers for an I O operation it may indicate to the memory management layer various information about which device the particular I O operation will be destined to. For example the guest kernel may indicate the device type device number device object address and the like. In accordance with an embodiment the memory management system can then determine whether to allocate memory buffers to the I O operation from the specifically reserved pool or from regular memory pools.

In accordance with an embodiment when the guest driver is requested to perform the I O operation the guest driver can check whether the memory buffers allocated belong to the reserved memory pool. In an alternative embodiment the memory management system can indicate to the guest driver that the memory buffers are from the reserved pool.

In accordance with an embodiment if the memory buffers are determined to be from the reserved memory pool the guest driver can skip the hypercall to grant memory access to the host domain and can instead indicate to the host domain that the buffers allocated for this I O operation are in the reserved memory pool. This indication may be provided by passing I O metadata over the ring buffer between the guest driver and the host driver. If the allocated memory buffers are not from the reserved memory pool the system can either a copy the allocated memory buffers into the reserved pool of memory buffers or b proceed to request the hypervisor to create new mappings for the allocated buffers in order to enable the host domain access to the allocated buffers.

In accordance with an embodiment when the host driver receives the I O operation request the host driver determines that the memory buffer belongs to the specifically reserved memory pool and the host driver uses the existing static mapping to perform the I O instead of initiating new mappings for the memory buffers. Once the I O operation is completed the guest driver may release the memory buffers to replenish the reserved memory pool.

In accordance with the illustrated embodiment the virtualized environment includes a set of hardware a hypervisor a host domain and one or more guest domains . The hypervisor manages the execution of the one or more guest operating systems and allows multiple instances of different operating systems to share the underlying hardware resources . Conventionally hypervisors are installed on server hardware with the function of running guest operating systems where the guest operating systems themselves act as servers. In various embodiments there can be at least two types of hypervisor a type 1 bare metal hypervisor and a type 2 hosted hypervisor. A type 1 hypervisor runs directly on the hardware resources and manages and controls one or more guest operating systems which run on top of the hypervisor. A type 2 hypervisor is executed within the operating system and hosts the one or more guest operating conceptually at a third level above the hardware resources .

In accordance with an embodiment the hypervisor can host a number of domains e.g. virtual machines such as the host domain and one or more guest domains . In one embodiment the host domain e.g. Dom 0 is the first domain created and manages all of the other domains running on the hypervisor . For example the host domain can manage the creating destroying migrating saving or restoring the one or more guest domains e.g. Dom U . In accordance with various embodiments the hypervisor controls access to the hardware resources such as the CPU input output I O memory and hypervisor memory.

In accordance with the illustrated embodiment the guest domain includes one or more virtualized or paravirtualized drivers and the host domain includes one or more physical device drivers . When the guest domain wants to invoke an I O operation the virtualized driver may perform the operation by way of communicating with the physical device driver . When the guest driver wants to initiate an I O operation e.g. sending out a network packet a guest kernel component will identify which physical memory buffer contains the packet or other data and the guest driver will either copy the memory buffer to a temporary storage location in the kernel for performing I O or obtain a set of pointers to the memory pages that contain the packet s . In one embodiment the guest driver will place the pointers into a data structure that is part of the ring buffer .

In various embodiments the ring buffer is a set of statically mapped memory pages that are shared between the guest domain and the host domain . These virtual pages can be linked to the same physical addresses and they can be used as the primary channel to pass metadata and pointers associated with network packets or I O operations. The ring buffer serves as the core of front to back communication of a paravirtualized driver such as driver . Conceptually the ring buffer can be thought of as containing a number of slots wherein each slot is a data structure that corresponds to an I O operation. Each I O operation may be associated with data from multiple memory pages. Accordingly each data structure in the ring buffer contains a set of pointers to the memory pages for the I O operation as well as any metadata associated with the I O operation.

In accordance with an embodiment when the guest driver submits an I O operation one of the data structures is filled out and placed into the ring buffer and the host driver is then notified that the I O operation has been submitted. At this point either the guest driver or the host driver or both may need to initiate one or more hypervisor calls to set up the grant table mechanism so that the host driver can obtain access to the data in the memory pages associated with the I O operation that has been submitted. For example the guest domain may indicate to the hypervisor which physical memory pages contain the data for the I O operation and requests the hypervisor to grant the host domain access to those memory pages . The host domain then retrieves the pointers to the memory pages from the ring buffer and request access to those memory pages from the hypervisor. The access can be provided by either copying the data from the shared buffer into the host domain specific buffer or by performing a virtual to physical remapping of the memory buffers where the host domain gives one or more virtual addresses from its own virtual space and requests the hypervisor to perform the remapping of the physical memory pages to the virtual addresses of the host domain. The hypervisor then performs the remapping of the physical memory from the virtual address space of the guest domain into the virtual address space of the host domain. Once this remapping is performed the host domain can have access to the memory pages that contain the data for the I O operation. The host domain driver can then perform the I O operation and once the operation is complete the memory pages can be unmapped.

In accordance with an embodiment one reason for low performance of virtualized I O stack is that guest memory buffers which are part of an I O operation may need to be mapped into host domain s memory during the I O operation and then unmapped as previously described. The mapping and unmapping operation can be fairly expensive as it involves multiple hypercalls hypervisor memory management activities host domain memory management activities and or Transaction Lookaside Buffer TLB flush activities. On the other hand fully mapping the guest memory into the host domain is usually not a feasible solution. If the guest memory were to be fully mapped into host domain then hypervisor security and isolation paradigms would be compromised since it may be much easier for the host domain to corrupt guest memory inadvertently or maliciously.

In accordance with various embodiments a TLB is a cache containing memory addresses which memory management hardware uses to improve the speed of translation of memory addresses. The TLB contains the mappings of various physical memory locations e.g. addresses to virtual memory locations. In accordance with an embodiment each CPU in the system is associated with its own TLB . In various embodiments when the system needs to determine the physical address of a virtual memory location it first inspects the TLB. If there is a TLB hit e.g. the address is present in the TLB the system can retrieve the physical address quickly and access the appropriate memory. If the requested address is not found in the TLB the system will proceed to look up the address in the page tables data structure by performing a page walk. Once the memory address is determined from the page tables the virtual to physical address mapping can be entered into the TLB for future access.

In conventional virtual environments when the hypervisor performs the remapping of the physical memory from the virtual address space of the guest domain into the virtual address space of the host domain the mapping change is often made visible to all of the CPUs in the system. As a result whenever the page table entries are changed in this manner the TLBs on all host domain CPUs are usually flushed in order to prevent any CPU from having invalid memory address mappings. This operation can be quite expensive and adds to the overall latency and lower throughput caused by the various mapping unmapping operations in virtualized environments.

In accordance with the illustrated embodiment instead of mapping all of guest domain memory to host domain e.g. Dom 0 permanently the system can map a small portion of the guest memory pool into the host domain and use the memory pool for I O operations. This statically mapped pool of memory will be reserved for the purpose of performing paravirtualized I O.

In accordance with an embodiment when the paravirtualized driver initializes itself inside the guest domain the paravirtualized driver allocates a set of memory pages that the paravirtualized driver can share with the host domain as the ring buffer . The ring buffer is used as the primary channel to pass metadata about network packets or I O operations as previously described. In addition the paravirtualized driver calls a function in operating system OS memory management layer requesting the memory management layer to allocate a semi permanent pool of memory buffers that can be used for I O operations via the paravirtualized interface that is being initialized. In one embodiment the function call includes parameters such as maximum queue depth maximum I O size type of I O e.g. network block etc. as well as other hints such as expected latency to complete an I O operation from memory buffer standpoint. The expected latency to complete an I O operation is the amount of time the driver expects the buffers associated with an I O operation to be held before the I O is completed and buffers are released. In one embodiment the reserved memory pool of buffers is specific to a particular device. In other embodiments the pool is specific to the guest domain. In yet other embodiments the pool may be shared by multiple device instances.

In accordance with an embodiment once the memory management layer allocates the pool of memory the pool is then locked into memory to prevent swap full access is granted to the host domain via hypervisor s memory sharing mechanism e.g. Grant table mechanism in Xen and passed to the host domain backend driver . The host domain driver requests a full mapping of the memory pool into its own memory mapping and potentially creates its own data structure to help determine if a particular memory buffer belongs to the special memory pool or not.

In accordance with an embodiment when the guest kernel I O stack requests buffers for an I O operation it can provide hints on what device the I O operation might be heading to. For example the hint may indicate the type of device major device number minor device number and device object address. In one embodiment device numbers are OS specific indices that identify devices and device object address is an OS specific handler that uniquely identifies a device. In some embodiments not all the entries may be used as a hint. In other embodiments I O stack may not provide any hint at all.

In accordance with an embodiment the memory management subsystem leverages the hints internal rules as well as current state of memory buffers and allocates buffers from one of the special memory pools or from regular memory pool of the other memory buffers. A set of internal rules can be implemented to help the memory management make better decisions about which memory pool to allocate the memory from particularly when the hints are not sufficient. For example if the I O stack declares that the buffers are for a network packet but do not specify which network device the internal rule might indicate that the memory buffers should come from eth0 s e.g. Ethernet port s memory pool.

In accordance with an embodiment when guest driver is requested to perform an I O operation the guest driver checks if the memory buffers allocated belong to the special memory pool allocated to it by the memory management layer . Alternatively the memory management could indicate to the I O stack that the buffers are from a special pool allocated to a particular device which is then passed on to the device driver via I O metadata e.g. skb flags .

In accordance with an embodiment if the guest driver detects that the memory buffers come from the special pool allocated to it then it skips the hypercall to grant memory access to the host domain and flags this in the I O metadata which is passed on as part of the I O ring slots between frontend and backend paravirtualized drivers.

In accordance with an embodiment when the backend driver host device driver receives an I O operation request it checks the metadata flags to see if the buffer belongs to the special memory pool . Alternatively it can check its own data structure to see if the memory buffer belongs to the special pool or not. If the memory buffer is from special memory pool the backend driver uses the existing mapping instead of initiating new memory mappings for the memory buffers.

In accordance with an embodiment once the I O operation is completed by the host device driver the host driver indicates the completion to the frontend guest driver . If the buffers associated with the I O were part of the special buffer pool then no unmapping operation is performed. The frontend driver receives the I O completion notification and releases the buffer to the memory management layer which replenishes the special buffer pool .

In various embodiments if there are multiple device instances that are handled by the same frontend and backend drivers the memory pools could potentially be shared across all of the devices.

In some embodiments special memory management APIs can be used inside the guest domain for applications that would like to use Direct I O. For example the application could request that a particular set of pages be allocated from the special pool for an I O device. The application can then use these special buffers for Direct I O operations. Alternatively the I O layer in the guest OS and or the device driver could copy the buffers sent from the application to buffers allocated from the special memory pool before using the buffers for paravirtualized I O operations. Similar techniques can also be used for other OS layers such as Page cache.

In accordance with some embodiments the reserved pool of memory buffers may be periodically remapped into different physical address space in order to reduce the likelihood that any malicious agents learn the location of the shared memory between the guest domain and the host domain. The timing of the remapping may be performed according to one or more security policies. In some embodiments the remapping of the addresses may be randomized in order to further confuse such agents. For example any potentially malicious actors that may be monitoring communications between a device driver and a virtual address and gathering information about those communications would have a more difficult time learning the shared memory patterns and performing other undesirable actions.

In operation the system reserves a pool of memory buffers specifically for performing virtualized I O operations. In accordance with an embodiment the reserved pool is mapped to both the address space of the host domain as well as the guest domain so that both domains can share access to the memory buffers in the pool. For example the mapping may be implemented by mapping the virtual addresses owned by the guest domain and the host domain to the same physical memory addresses. Once allocated the reserved pool of memory buffers will be used to communicate data for performing I O operations between the guest drivers and the host drivers. In at least one embodiment the size of the memory pool for most types of devices can be associated with the amount of data that can be in flight for the particular device. In some embodiments the size of the memory pool can be modified. For example as new devices attach themselves to the guest or detach themselves from the guest the size of the memory pool may be increased or decreased accordingly.

In operation the system receives a request to perform an I O operation. For example the I O request may be received from an application to a paravirtualized guest driver in the guest domain. As previously mentioned the guest driver can work with the backend host driver in order to carry out the I O operation by using the reserved pool of memory buffers.

In operation the system copies the data for the I O operation into the reserved pool of memory buffers. In one embodiment the system can first check if the data is stored in the reserved memory pool and if the data is already there no copying is required. For example if the memory management system specifically allocated the memory buffers from the reserved pool for the I O operation then the data for the I O operation will already reside in the statically mapped pool. In other embodiments the system may automatically copy the data into the reserved pool without any checking of where the data resides.

In operation the system performs the I O operation using the data from the reserved pool of memory buffers. In one embodiment the host driver reads the data from the pool and performs the I O operation e.g. sending a network packet . Once the I O operation is completed the host driver may notify the guest driver to release the memory buffers in the reserved pool.

In operation the guest domain reserves a pool of memory for performing I O operations. In accordance with an embodiment this is performed by the guest driver upon initialization invoking the hypervisor and requesting the hypervisor to make the pool of memory available to the host device driver. In one embodiment the reserved pool is associated with a particular device. Alternatively the pool may be associated with the guest domain or multiple device instances.

In operation the guest domain instructs or otherwise causes the host domain to map the reserved pool of buffers into its own memory address space. In one embodiment upon initializing the host domain driver the host domain may invoke the hypervisor and request the hypervisor to create static mappings of the physical memory associated with the pool into the virtual address space of the host domain. Once this mapping is performed the host domain will have static mapping of the pool to its own address space and therefore will not need to perform any mapping unmapping or TLB flush operations to gain access to the memory buffers of the reserved pool.

In operation the guest kernel requests the memory management system to allocate a set of memory buffers for the I O operation. For example the guest kernel I O stack can request buffers that will be allocated to the I O operation. At that time the guest kernel can also provide hints regarding which device type the I O operation is destined to e.g. device type major device number minor device number device object address etc. .

In operation the memory management system can determine whether to allocate the memory buffers from the reserved pool or from regular memory. This can be implemented by providing hooks into the memory management activities where the hooks can be used to determine the type of device or other information for the I O operation. In one embodiment the memory management system uses the hints as well as its own internal rules to determine which pool to allocate the buffers from. For example if the guest I O stack indicates that the buffers are for a network packet the internal rule may indicate that the memory buffers should come from the reserved pool associated with the Ethernet port device. Once the system makes the determination it allocates the memory buffers from the pool to be used with the I O operations.

In operation an I O request is submitted to the guest driver e.g. paravirtualized driver . In various embodiments the paravirtualized driver communicates with the backend host driver in order to perform the I O operation.

In operation when the guest driver receives the request it can first check whether the memory buffers allocated to the I O belong to the reserved pool of memory buffers allocated by the memory management layer. This check can either be performed by the guest driver actively checking information associated with the request or by the memory system indicating to the guest kernel that the buffers are from the reserved memory pool. If the data for the I O resides in the memory buffers from the reserved pool the process continues to operation . Otherwise if the memory buffers are not from the reserved memory pool the process can continue to operation .

In operation if the memory buffers allocated to the I O are from the reserved pool the guest driver can instruct the host device driver to perform the I O operation using the statically mapped buffers in the reserved pool. In one embodiment the host driver can skip all the usual memory mapping unmapping and TLB flush operations that would usually be needed to gain access to the memory buffers that contain the data for the I O operation.

In operation if the memory buffers are not part of the reserved pool the guest driver can either a copy the data into the reserved pool or b carry out the usual mapping and unmapping operations to grant the host domain access to the memory buffers by invoking the appropriate hypervisor calls. In some embodiments the copying operation may be faster than the memory management activities and it may be preferable to copy the data into the reserved pool rather than mapping and unmapping operations. In other embodiments the system can weigh the potential trade off in estimated latency that would be introduced by the copy versus the memory management operations to determine which option to select. Once the data has been copied or mapped the system can continue to step where the I O operation is performed.

In operation the guest driver receives a notification from the host driver indicating that the I O operation was successfully completed. In step the guest driver can then release the memory buffers to replenish the reserved memory pool for other I O operations.

In operation during initial allocation the backend host driver creates a static mapping of the memory pages that will be part of the reserved pool. Thereafter the host driver may receive a request to perform an I O operation from the guest driver as shown in operation . When the host driver receives the request the host driver first checks whether the buffers are part of the statically mapped pool in operation . If the memory buffers are part of the statically mapped pool the host driver does not need to perform any further functions to gain access to the memory buffers and it can simply execute the I O operation using the existing static mappings as illustrated in operation . If on the other hand the memory buffers containing the data for the I O are not part of the statically mapped pool then the host driver invokes a regular hypercall to create the new mappings for the memory buffers as shown in operation . Thereafter the I O operation can be performed using the new mappings operation . Once the I O is completed the host driver can notify the guest driver indicating that the I O was successful as shown in operation .

As discussed different approaches can be implemented in various environments in accordance with the described embodiments. For example illustrates an example of an environment for implementing aspects in accordance with various embodiments. As will be appreciated although a Web based environment is used for purposes of explanation different environments may be used as appropriate to implement various embodiments. The system includes an electronic client device which can include any appropriate device operable to send and receive requests messages or information over an appropriate network and convey information back to a user of the device. Examples of such client devices include personal computers cell phones handheld messaging devices laptop computers set top boxes personal data assistants electronic book readers and the like. The network can include any appropriate network including an intranet the Internet a cellular network a local area network or any other such network or combination thereof. Components used for such a system can depend at least in part upon the type of network and or environment selected. Protocols and components for communicating via such a network are well known and will not be discussed herein in detail. Communication over the network can be enabled via wired or wireless connections and combinations thereof. In this example the network includes the Internet as the environment includes a Web server for receiving requests and serving content in response thereto although for other networks an alternative device serving a similar purpose could be used as would be apparent to one of ordinary skill in the art.

The illustrative environment includes at least one application server and a data store . It should be understood that there can be several application servers layers or other elements processes or components which may be chained or otherwise configured which can interact to perform tasks such as obtaining data from an appropriate data store. As used herein the term data store refers to any device or combination of devices capable of storing accessing and retrieving data which may include any combination and number of data servers databases data storage devices and data storage media in any standard distributed or clustered environment. The application server can include any appropriate hardware and software for integrating with the data store as needed to execute aspects of one or more applications for the client device and handling a majority of the data access and business logic for an application. The application server provides access control services in cooperation with the data store and is able to generate content such as text graphics audio and or video to be transferred to the user which may be served to the user by the Web server in the form of HTML XML or another appropriate structured language in this example. The handling of all requests and responses as well as the delivery of content between the client device and the application server can be handled by the Web server . It should be understood that the Web and application servers are not required and are merely example components as structured code discussed herein can be executed on any appropriate device or host machine as discussed elsewhere herein.

The data store can include several separate data tables databases or other data storage mechanisms and media for storing data relating to a particular aspect. For example the data store illustrated includes mechanisms for storing production data and user information which can be used to serve content for the production side. The data store also is shown to include a mechanism for storing log or session data . It should be understood that there can be many other aspects that may need to be stored in the data store such as page image information and access rights information which can be stored in any of the above listed mechanisms as appropriate or in additional mechanisms in the data store . The data store is operable through logic associated therewith to receive instructions from the application server and obtain update or otherwise process data in response thereto. In one example a user might submit a search request for a certain type of item. In this case the data store might access the user information to verify the identity of the user and can access the catalog detail information to obtain information about items of that type. The information can then be returned to the user such as in a results listing on a Web page that the user is able to view via a browser on the user device . Information for a particular item of interest can be viewed in a dedicated page or window of the browser.

Each server typically will include an operating system that provides executable program instructions for the general administration and operation of that server and typically will include computer readable medium storing instructions that when executed by a processor of the server allow the server to perform its intended functions. Suitable implementations for the operating system and general functionality of the servers are known or commercially available and are readily implemented by persons having ordinary skill in the art particularly in light of the disclosure herein.

The environment in one embodiment is a distributed computing environment utilizing several computer systems and components that are interconnected via communication links using one or more computer networks or direct connections. However it will be appreciated by those of ordinary skill in the art that such a system could operate equally well in a system having fewer or a greater number of components than are illustrated in . Thus the depiction of the system in should be taken as being illustrative in nature and not limiting to the scope of the disclosure.

Various embodiments discussed or suggested herein can be implemented in a wide variety of operating environments which in some cases can include one or more user computers computing devices or processing devices which can be used to operate any of a number of applications. User or client devices can include any of a number of general purpose personal computers such as desktop or laptop computers running a standard operating system as well as cellular wireless and handheld devices running mobile software and capable of supporting a number of networking and messaging protocols. Such a system also can include a number of workstations running any of a variety of commercially available operating systems and other known applications for purposes such as development and database management. These devices also can include other electronic devices such as dummy terminals thin clients gaming systems and other devices capable of communicating via a network.

Most embodiments utilize at least one network that would be familiar to those skilled in the art for supporting communications using any of a variety of commercially available protocols such as TCP IP OSI FTP UPnP NFS CIFS and AppleTalk. The network can be for example a local area network a wide area network a virtual private network the Internet an intranet an extranet a public switched telephone network an infrared network a wireless network and any combination thereof.

In embodiments utilizing a Web server the Web server can run any of a variety of server or mid tier applications including HTTP servers FTP servers CGI servers data servers Java servers and business application servers. The server s also may be capable of executing programs or scripts in response requests from user devices such as by executing one or more Web applications that may be implemented as one or more scripts or programs written in any programming language such as Java C C or C or any scripting language such as Perl Python or TCL as well as combinations thereof. The server s may also include database servers including without limitation those commercially available from Oracle Microsoft Sybase and IBM .

The environment can include a variety of data stores and other memory and storage media as discussed above. These can reside in a variety of locations such as on a storage medium local to and or resident in one or more of the computers or remote from any or all of the computers across the network. In a particular set of embodiments the information may reside in a storage area network SAN familiar to those skilled in the art. Similarly any necessary files for performing the functions attributed to the computers servers or other network devices may be stored locally and or remotely as appropriate. Where a system includes computerized devices each such device can include hardware elements that may be electrically coupled via a bus the elements including for example at least one central processing unit CPU at least one input device e.g. a mouse keyboard controller touch screen or keypad and at least one output device e.g. a display device printer or speaker . Such a system may also include one or more storage devices such as disk drives optical storage devices and solid state storage devices such as random access memory RAM or read only memory ROM as well as removable media devices memory cards flash cards etc.

Such devices also can include a computer readable storage media reader a communications device e.g. a modem a network card wireless or wired an infrared communication device etc. and working memory as described above. The computer readable storage media reader can be connected with or configured to receive a computer readable storage medium representing remote local fixed and or removable storage devices as well as storage media for temporarily and or more permanently containing storing transmitting and retrieving computer readable information. The system and various devices also typically will include a number of software applications modules services or other elements located within at least one working memory device including an operating system and application programs such as a client application or Web browser. It should be appreciated that alternate embodiments may have numerous variations from that described above. For example customized hardware might also be used and or particular elements might be implemented in hardware software including portable software such as applets or both. Further connection to other computing devices such as network input output devices may be employed.

Storage media and computer readable media for containing code or portions of code can include any appropriate media known or used in the art including storage media and communication media such as but not limited to volatile and non volatile removable and non removable media implemented in any method or technology for storage and or transmission of information such as computer readable instructions data structures program modules or other data including RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disk DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by a system device. Based on the disclosure and teachings provided herein a person of ordinary skill in the art will appreciate other ways and or methods to implement the various embodiments.

The specification and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense. It will however be evident that various modifications and changes may be made thereunto without departing from the broader spirit and scope of the invention as set forth in the claims.

