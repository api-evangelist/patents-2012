---

title: Performing setup operations for receiving different amounts of data while processors are performing message passing interface tasks
abstract: A system and method are provided for performing setup operations for receiving a different amount of data while processors are performing message passing interface (MPI) tasks. Mechanisms for adjusting the balance of processing workloads of the processors are provided so as to minimize wait periods for waiting for all of the processors to call a synchronization operation. An MPI load balancing controller maintains a history that provides a profile of the tasks with regard to their calls to synchronization operations. From this information, it can be determined which processors should have their processing loads lightened and which processors are able to handle additional processing loads without significantly negatively affecting the overall operation of the parallel execution system. As a result, setup operations may be performed while processors are performing MPI tasks to prepare for receiving different sized portions of data in a subsequent computation cycle based on the history.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08893148&OS=08893148&RS=08893148
owner: International Business Machines Corporation
number: 08893148
owner_city: Armonk
owner_country: US
publication_date: 20120615
---
This application is a continuation of application Ser. No. 11 846 154 filed Aug. 28 2007 now U.S. Pat. No. 8 234 652.

This invention was made with Government support under DARPA HR0011 07 9 0002. THE GOVERNMENT HAS CERTAIN RIGHTS IN THIS INVENTION.

The present application relates generally to an improved data processing system and method. More specifically the present application is directed to a system and method for performing setup operations for receiving a different amount of data while processors are performing message passing interface tasks.

A parallel computing system is a computing system with more than one processor for parallel processing of tasks. A parallel program is a program that may consist of one or more jobs that may be separated into tasks that may be executed in parallel by a plurality of processors. Parallel programs allow the tasks to be simultaneously executed on multiple processors with some coordination between the processors in order to obtain results faster.

There are many different approaches to providing parallel computing systems. Examples of some types of parallel computing systems include multiprocessing systems computer cluster systems parallel supercomputer systems distributed computing systems grid computing systems and the like. These parallel computing systems are typically distinguished from one another by the type of interconnection between the processors and memory. One of the most accepted taxonomies of parallel computing systems classifies parallel computing systems according to whether all of the processors execute the same instructions i.e. single instruction multiple data SIMD or each processor executes different instructions i.e. multiple instruction multiple data MIMD .

Another way by which parallel computing systems are classified is based on their memory architectures. Shared memory parallel computing systems have multiple processors accessing all available memory as a global address space. These shared memory parallel computing systems may be further classified into uniform memory access UMA systems in which access times to all parts of memory are equal or non uniform memory access NUMA systems in which access times to all parts of memory are not equal. Yet another classification distributed memory parallel computing systems also provides a parallel computing system in which multiple processors are utilized but each of the processors can only access its own local memory i.e. no global memory address space exists across them. Still another type of parallel computing system and the most prevalent in use today is a combination of the above systems in which nodes of the system have some amount of shared memory for a small number of processors but many of these nodes are connected together in a distributed memory parallel system.

The Message Passing Interface MPI is a language independent computer communications descriptive application programming interface API for message passing on shared memory or distributed memory parallel computing systems. With MPI typically a parallel application is provided as one or more jobs which are then separated into tasks which can be processed in a parallel manner on a plurality of processors. MPI provides a communication API for the processors to communicate with one another regarding the processing of these tasks.

There are currently two versions of the MPI standard that are in use. Version 1.2 of the MPI standard emphasizes message passing and has a static runtime environment. Version 2.1 of the MPI standard includes new features such as scalable file I O dynamic process management and collective communication of groups of processes. These MPI standards are available from www.mpi forum.org docs docs.html. It is assumed for purposes of this description that the reader has an understanding of the MPI standards.

Of particular note the MPI standard provides for collective communication of processes or tasks i.e. communications that involve a group of processes or tasks. A collective operation is executed using MPI by having all the tasks or processes in the group call a collective communication routine with matching arguments. Such collective communication routine calls may but are not required to return as soon as their participation in the collective communication is complete. The completion of a call indicates that the caller is now free to access locations in a communication buffer but does not indicate that other processes or tasks in the group have completed or even have started the operation. Thus a collective communication call may or may not have the effect of synchronizing all calling processes.

One way in which MPI enforces synchronization of the processes or tasks is to provide a synchronization operation referred to as the MPI BARRIER call. The MPI BARRIER call blocks the caller until all tasks or processes in the group have called MPI BARRIER . Thus the MPI BARRIER call is used with a group of tasks which must wait for the other tasks in the group to complete before proceeding to the next tasks i.e. each task must call MPI BARRIER before any of the processors are able to execute additional tasks. Essentially the barrier operation enforces synchronization of the tasks of a job and enforces temporal dependence.

While such synchronization operations aid programmers in generating parallel programs that ensure that dependent tasks are accommodated without errors the synchronization results in inefficient use of the processor resources. For example if a processor executes a task in parallel with one or more other processors and finishes its task before the other processors then it must wait for each of the other processors to complete their tasks and call the synchronization operation before it can proceed. As a result there are a number of wasted processor cycles while the fast processors wait for the slower processors to complete. During this time period the faster processors are still consuming power but are not providing any useful work.

The illustrative embodiments provide a system and method for performing setup operations for receiving a different amount of data while processors are performing message passing interface tasks. Moreover the illustrative embodiments provide a system and method for providing hardware based dynamic load balancing of message passing interface MPI tasks. In particular the illustrative embodiments provide mechanisms for adjusting the balance of processing workloads of the processors executing tasks of an MPI job so as to minimize the wait periods and hence the wasted processor cycles associated with waiting for all of the processors to call a synchronization operation. With the mechanisms of the illustrative embodiments each processor has an associated MPI load balancing controller which may be implemented as a hardware device in or coupled to the processors. The hardware implemented MPI load balancing controller maintains a history data structure that provides a history profile of the tasks with regard to their calls to synchronization operations. For example the history data structure may maintain a listing of the tasks and timestamps of when the tasks call the synchronization operation. In this way a determination of the relative completion of computation phases by the processors may be made to identify which processors completed their computation phases first second third etc.

From this information it can be determined which processors should have their processing loads lightened and which processors are able to handle additional processing loads without significantly negatively affecting the overall operation of the parallel execution system. As a result operations may be performed to shift workloads from the slowest processor to one or more of the faster processors. Of course thresholds may be utilized to determine if the wait periods are sufficient for performing such load balancing operations since there is some overhead in actually performing the load balancing operations and a tradeoff may need to be considered.

In addition the mechanisms of the illustrative embodiments provide for the overlap of the computation phases of the processors with setup operations for performing redistribution of workloads amongst the processors. That is mechanisms are provided in the illustrative embodiments for faster processors to begin based on a knowledge from the history data structure that they have completed the computation phase before other processors in their group operating on the MPI job setup operations to prepare for accepting a larger workload in a subsequent computation phase. Such setup operations may include for example adjusting the allocation of resources so as to provide additional resources for the faster processors to accommodate larger workloads.

In another illustrative embodiment the mechanisms of the illustrative embodiments may provide mechanisms for dynamically modifying the number of MPI tasks sent to each of the processors. Such mechanisms may be used in cases where the MPI tasks are data dependent. The modification of the number of MPI tasks may involve the adding or removing of tasks from a queue of tasks for the processors i.e. converging tasks. Thresholds may again be used to determine if the wait periods are degrading performance in a significant enough manner to warrant performing such convergence to obtain better performance. The converging of tasks in this manner involves the MPI load balancing mechanisms associated with the processors communicating with a load leveler of the parallel program dispatcher to cause the load leveler to adjust the number of tasks dispatched to each of the processors in accordance with their historical profile.

In one illustrative embodiment a method for balancing a Message Passing Interface MPI workload across a plurality of processors is provided. The method may comprise receiving one or more MPI synchronization operation calls from one or more processors of the plurality of processors and identifying a first processor in the plurality of processors having a fastest time of completion of a computation phase of an associated MPI task during a computation cycle based on the received one or more MPI synchronization operation calls wherein the computation phase of the first associated MPI task involves executing the MPI task on a first data set. The method may further comprise performing a first setup operation in the first processor for preparing to receive a second data set that is larger than the first data set in response to identifying the first processor as having a fastest time of completion of the computation phase. The first setup operation may modify an allocation of resources in the multiple processor system for use by the first processor in receiving the second data set.

The first setup operation may be performed while at least one other processor in the plurality of processors is still in a computation phase of its associated MPI task during the same computation cycle. The first setup operation may comprise at least one of allocating a larger portion of cache memory for use by the first processor setting up buffer space to receive an additional amount of data for processing by the first processor or acquiring a host fabric interface window or windows for communication by the first processor.

The method may further comprise identifying a second processor in the plurality of processors having a slowest time of completion of a computation phase of a second associated MPI task during the computation cycle based on the received one or more MPI synchronization operation calls. The computation phase of the first associated MPI task may involve executing the MPI task on a third data set. The method may also comprise performing a second setup operation in the second processor for preparing to receive a fourth data set that is smaller than the third data set in response to identifying the second processor as having a slowest time of completion of the computation phase. The second setup operation may modify an allocation of resources in the multiple processor system for use by the second processor in receiving the fourth data set. The second setup operation may comprise at least one of allocating a smaller portion of cache memory for use by the second processor setting up buffer space to receive a smaller amount of data for processing by the second processor or acquiring a host fabric interface window or windows for communication by the second processor.

The method may further comprise determining if a difference in the fastest time of completion and the slowest time of completion exceeds a threshold. The first setup operation and the second setup operation may be performed in response to the difference exceeding the threshold.

Each processor of the plurality of processors may comprise a MPI load balancing controller. Each MPI load balancing controller may implement the receiving and identifying operations. An MPI load balancing controller associated with the first processor may implement performing the first setup operation. An MPI load balancing controller associated with the second processor may implement performing the second setup operation.

The MPI job may be a set of tasks to be performed in parallel on the plurality of processors. Each processor of the plurality of processors may execute a corresponding task of the MPI job in parallel on a corresponding set of data allocated to the processor from a superset of data.

In other illustrative embodiments a computer program product comprising a computer useable medium having a computer readable program is provided. The computer readable program when executed on a computing device causes the computing device to perform various ones and combinations of the operations outlined above with regard to the method illustrative embodiment.

In yet another illustrative embodiment a system is provided. The system may comprise a plurality of processors and at least one load balancing controller associated with the plurality of processors. The at least one load balancing controller may perform various ones and combinations of the operations outlined above with regard to the method illustrative embodiment.

These and other features and advantages of the present invention will be described in or will become apparent to those of ordinary skill in the art in view of the following detailed description of the exemplary embodiments of the present invention.

The illustrative embodiments provide a system and method for providing hardware based dynamic load balancing of message passing interface tasks. As such the illustrative embodiments are especially well suited for use with a distributed data processing system in which a plurality of processors are used for the distribution of parallel program message passing interface tasks for parallel processing. Thus are provided hereafter as examples of data processing systems and devices in which the illustrative embodiments of the present invention may be implemented. It should be appreciated that are only exemplary and are not intended to assert or imply any limitation with regard to the environments in which aspects or embodiments of the present invention may be implemented. Many modifications to the depicted environments may be made without departing from the spirit and scope of the present invention.

With reference now to the figures depicts a pictorial representation of an exemplary distributed data processing system in which aspects of the illustrative embodiments may be implemented. Distributed data processing system may include a network of computers in which aspects of the illustrative embodiments may be implemented. The distributed data processing system contains at least one network which is the medium used to provide communication links between various devices and computers connected together within distributed data processing system . The network may include connections such as wire wireless communication links or fiber optic cables.

In the depicted example server and server are connected to network along with storage unit . In addition clients and are also connected to network . These clients and may be for example personal computers network computers or the like. In the depicted example server provides data such as boot files operating system images and applications to the clients and . Clients and are clients to server in the depicted example. Distributed data processing system may include additional servers clients and other devices not shown.

In the depicted example distributed data processing system may be the Internet with network representing a worldwide collection of networks and gateways that use the Transmission Control Protocol Internet Protocol TCP IP suite of protocols to communicate with one another. At the heart of the Internet is a backbone of high speed data communication lines between major nodes or host computers consisting of thousands of commercial governmental educational and other computer systems that route data and messages. Of course the distributed data processing system may also be implemented to include a number of different types of networks such as for example an intranet a local area network LAN a wide area network WAN or the like. As stated above is intended as an example not as an architectural limitation for different embodiments of the present invention and therefore the particular elements shown in should not be considered limiting with regard to the environments in which the illustrative embodiments of the present invention may be implemented.

It should be appreciated that the servers and and additional servers if any not depicted may be provided as part of a server cluster over which a parallel program having one or more jobs which in turn have one or more tasks may be distributed for processing. Alternatively a parallel program in accordance with the mechanisms of the illustrative embodiments may be provided to a single server e.g. server which may be a supercomputer or the like having multiple processors upon which the parallel program may be distributed. The parallel program may be of the type as is generally known in the art where similar tasks are performed on each of a plurality of processors but on different sets of data. That is a superset of data may be partitioned into portions to be provided to a plurality of tasks which each perform similar processing of the data portions assigned to them. The results of such processing may be passed to other processors in the cluster or group for use in further processing portions of data from the superset of data. Moreover in addition to communicating results data from one processor to another various communications are supported for communicating state for synchronization and the like via the use of a Message Passing Interface MPI .

With reference now to there is illustrated a high level block diagram of a multiprocessor MP data processing system in accordance with one embodiment of the present invention. The MP data processing system may be a single computing device such as server or in . Alternatively the processors shown in may actually be distributed in a plurality of computing devices such as in a cluster architecture but which have a communication mechanism such as wired or wireless links through which the processors may communicate with each other and with management hardware.

As depicted data processing system includes a number of processing units 1 4 referred to collectively by the processing unit group coupled for communication by a system interconnect . Only one processing unit group is shown in for simplicity but it should be appreciated that more than one processing unit group may be included in the data processing system . In one illustrative embodiment for example the processing units 1 4 and or processing unit group may be implemented as a POWER processing chip or chips available from International Business Machines Corporation of Armonk N.Y.

As depicted in the embodiment of processing unit contains four processor units 1 4 however the illustrative embodiments are not limited by any number of processor units and the invention will support any number or type of processor units. For example the illustrative embodiments may utilize a data processing system having any number of processor units e.g. 2 4 8 16 32 etc. in the multi processor system. Each processing unit group may be provided as one or more integrated circuits including the one or more processor units 1 4 which comprise associated processor cores . In addition to registers instruction flow logic and execution units utilized to execute program instructions each of processor cores includes associated level one L1 instruction and data caches and which temporarily buffer instructions and operand data respectively that are likely to be accessed by the associated processor core .

As further illustrated in the memory hierarchy of data processing system also includes the physical memory comprising one or more memory modules shown as memory modules and which form the lowest level of volatile data storage in the memory hierarchy and one or more lower levels of cache memory such as on chip level two L2 caches which are utilized to stage instructions and operand data from physical memory to processor cores . As understood by those skilled in the art each succeeding lower level of the memory hierarchy is typically capable of storing a larger amount of data than higher levels but at higher access latency.

As shown physical memory which is interfaced to interconnect by memory controllers and may store operand data and portions of one or more operating systems and one or more application programs. Memory controllers and are coupled to and control corresponding memory modules and respectively.

Also shown is input output connector which operates in a similar manner as the processing units 1 4 of the processing unit group when performing direct memory access operations to the memory system. As will be appreciated the system may have additional input output connectors equal to input output connector connected to interconnect . As various input output devices such as disk drives and video monitors are added and removed on PCI bus or other similar attached buses input output connector operates to transfer data between PCI bus and interconnect through bridge .

Those skilled in the art will appreciate that data processing system can include many additional un illustrated components such as I O adapters interconnect bridges non volatile storage ports for connection to networks or attached devices etc. Because such additional components are not necessary for an understanding of the present invention they are not illustrated in or discussed further herein. It should also be understood however that the enhancements provided by the present invention are applicable to data processing systems of any architecture and are in no way limited to the generalized MP architecture illustrated in .

In accordance with the illustrative embodiments a plurality of processors are utilized to perform parallel processing of tasks of a job of a parallel program. With such parallel processing a superset of data is partitioned into portions of data that may individually be provided to each of the tasks of the job. The tasks may operate on the data to generate results data that may be communicated to neighboring processors in the plurality of processors if the tasks of the neighboring processors require the results for their own processing in a subsequent cycle. The processors may be provided in the same computing device in a plurality of computing devices that are distributed and in communication with each other via one or more data networks in a plurality of computing devices of a cluster or the like. The processors may be part of a multiprocessor MP system such as a symmetric multiprocessor SMP system or the like. Any multiple processor architecture may be used to provide the plurality of processors for executing in parallel tasks of jobs corresponding to a parallel program without departing from the spirit and scope of the present invention.

As mentioned above in the illustrative embodiments the plurality of processors support the use of a Message Passing Interface MPI through the calling of MPI functions provided in one or more MPI Application Program Interfaces APIs . The mechanisms of the illustrative embodiments provide additional functionality for affecting the execution of the parallel program in a parallel and distributed manner based on a history of the way in which the parallel program is executed in the multiple processor system. This history may be maintained and dynamically updated in a history data structure whose entries identify which processors in the multiple processor system should perform which operations to achieve a desired execution of the parallel program.

The affecting of the execution of the parallel program may comprise for example providing a load balancing functionality for reducing the number of wasted processor cycles due to waiting for all tasks of a group i.e. a job to return a synchronization operation call. Various load balancing operations may be performed to balance the processing load of the tasks on the various processors so that each of the processors completes its computations in approximately the same time period thereby reducing the number of wasted processor cycles.

In addition to load balancing for a primary parallel program that is being run the mechanisms of the illustrative embodiments may affect the execution of the primary parallel program by selecting to run another program during any relatively idle time or within any dead space during execution of the primary parallel program. For example if it is known that there will be a long delay between the first and last process to arrive at a barrier operation another program may be run on processors that would otherwise be idle waiting for the last process to arrive at the barrier operation.

Moreover the operating system or other applications may perform housekeeping tasks such as memory management and garbage collection operations during these times of relative idleness. As is generally known in the art memory management operations are operations for allocating portions of memory to programs at their request and freeing memory for reuse when no longer needed. Garbage collection is a type of memory management in which a garbage collector attempts to reclaim garbage i.e. memory used by objects that will never be accessed again by an application. It should be appreciated that other types of housekeeping tasks may also be performed by the operating system or applications when an associated processor is in a relatively idle state without departing from the spirit and scope of the illustrative embodiments.

Furthermore the mechanisms of the illustrative embodiments may select to place the faster processors nodes i.e. the ones reaching the barrier operation first in a lower power state during their idle periods. Knowing ahead of time based on the history information mentioned above that an idle period may exist is helpful in determining whether to enter a lower power state since doing so requires some time to perform. Any functionality for modifying the execution of the primary parallel program such as by changing the state of the processors nodes that have already reached a barrier operation may be performed without departing from the spirit and scope of the present invention.

The MPI job is essentially a group of tasks that are to be executed in parallel on the plurality of processors . As is generally known in the art parallel programs are typically programmed for separation into jobs which in turn are designed for separation into tasks to be performed in parallel. Similarly the data upon which the parallel program is to execute may be partitioned into sets to be processed in parallel by the tasks . In some illustrative embodiments the tasks may be substantially the same but may be executed on different sets of data from a superset of data stored in the data storage . For example the tasks may be clones or replicated instances of the same original task. In other illustrative embodiments the tasks may be different from one another and may operate on the same or a different set of data .

As shown in each processor executes its corresponding task on a portion of data . The portion of data that a particular processor receives is communicated to the processor by the parallel program dispatcher by sending for example an ending address and possibly a starting address if necessary within the data superset in the data storage for each processor. Each processor may then read that data such as via a Remote Direct Memory Access RDMA operation from the data storage and process it according to the associated task . Preferably the ending addresses and starting addresses of portions of data are communicated to the processors such as via MPI communications. The starting and ending addresses may be distributed in such a manner as to provide a same size data set to each of the processors . It should be appreciated that in some exemplary implementations it may not be necessary to communicate both the starting address and the ending address of a portion of data and only one of the starting address or the ending address is needed to define the size of the data set.

The period of time in which the processors execute the instructions of their tasks on their portion of data is referred to herein as the computation phase. For example parallel programs are often developed to perform the same task on different sets of data in parallel using a plurality of processors. As one example parallel programs have been developed to analyze DNA codes such that the same computations are performed by each of a plurality of tasks of an MPI job but on different sets of DNA coding data. Thus in the computation phase each of the processors may execute instructions on different sets of data . Alternatively the data sets may be the same set of data but with the tasks that are performed being different.

In either case since there is some measure of difference in the computations being performed in the computation phase between processors there is the possibility that the computation phase may require a different amount of processor cycles or time to complete in each of the processors . Many different factors may affect how long the computation phase is for each of the processors . For example one processor may perform a computation in which the data used as part of the computation is consistently found in the processor s L1 data cache while another processor may have to access main memory to complete its computations resulting in a greater latency.

Typically in MPI jobs the tasks must be synchronized at some point in order to ensure proper operation and execution of the MPI job and the parallel program. One way in which MPI tasks are synchronized is to make a call to a synchronization operation when the computation phase of the task is completed. In the current MPI standard this synchronization operation is referred to as the MPI barrier operation i.e. the MPI BARRIER function call. With this synchronization mechanism the processors are not permitted to continue execution of tasks until all of the processors communicate via point to point communications facilitated by the MPI APIs that their computation phases are complete by calling the synchronization operation i.e. the barrier operation. When the MPI barrier operation is called this call is communicated to each of the other processors executing tasks in the MPI job . Once each of the processors perform a call to the MPI barrier operation results data obtained as a result of the computations performed during the computation phases may be communicated between the processors e.g. from a processor to each of its neighboring processors in a cluster of processors and the processors are permitted to continue execution of tasks based on this results data and other data sets if any provided to the tasks.

It can be seen from that because the processors may complete the computation phase at various times relative to the other processors executing tasks of the MPI job some processors e.g. processors and may experience wait periods where they wait for one or more of the other processors e.g. processors to complete their computation phase and perform a call to the synchronization operation i.e. the barrier operation. This wait period is essentially wasted processor cycles in which the processor is powered on and in an idle state not producing any useful work. Thus the processors in the wait period consume power but provide no useful work in return. This results in a loss of possible computations within the same time period i.e. because of the wasted processor cycles a smaller number of computations may be performed using the MPI mechanism shown in than if the wait periods were able to be avoided. From empirical data it has been determined that in some architectures this loss of computation may be on the order of a 20 loss in computation ability.

The illustrative embodiments provide mechanisms for adjusting the balance of processing workloads of the processors executing tasks of an MPI job so as to minimize the wait periods and hence the wasted processor cycles associated with waiting for all of the processors to call a synchronization operation. Alternatively as mentioned above rather than performing load balancing operations or in addition to performing load balancing operations the mechanisms of the illustrative embodiments may perform other operations such as executing other programs performing housekeeping operations such as memory management or garbage collection operations or placing idle processors in a low power state in order to minimize the waste associated with processors waiting for all of the processors performing MPI job tasks to call an MPI synchronization operation.

With regard to load balancing each processor has an associated MPI load balancing controller which may be implemented as a hardware device in or coupled to the processors. The hardware implemented MPI load balancing controller maintains a history data structure that provides a history profile of the tasks with regard to their calls to synchronization operations. For example the history data structure may maintain a listing of the tasks and timestamps of when the tasks call the synchronization operation. In this way a measure of the relative completion of computation phases by the processors may be made such as based on the timestamp information to identify which processors completed their computation phases first second third etc. For example this measure may represent a difference between the completion time for a processor and the completion time of a fastest slowest processor s call to a MPI synchronization operation. Such measures may then be used to determine if and how much of a workload to shift within the processors .

That is from this history data structure information it can be determined which processors should have their processing loads lightened and which processors are able to handle additional processing loads without significantly negatively affecting the overall operation of the parallel execution system. As a result operations may be performed to shift workloads from the slowest processor to one or more of the faster processors. Of course thresholds may be utilized to determine if the wait periods are sufficient for performing such load balancing operations since there is some overhead in actually performing the load balancing operations and a tradeoff may need to be considered.

In addition the mechanisms of the illustrative embodiments provide for the overlap of the computation phases of the processors with setup operations for performing redistribution of workloads amongst the processors. That is mechanisms are provided in the illustrative embodiments for faster processors to begin based on a knowledge from the history data structure that they have completed the computation phase before other processors in their group operating on the MPI job setup operations to prepare for accepting a larger workload in a subsequent computation phase. Such setup operations may include for example adjusting the allocation of resources so as to provide additional resources for the faster processors to accommodate larger workloads.

In another illustrative embodiment the mechanisms of the illustrative embodiments may provide mechanisms for dynamically modifying the number of MPI tasks sent to each of the processors. Such mechanisms may be used in cases where the MPI tasks are data dependent. The modification of the number of MPI tasks may involve the adding or removing of tasks from a queue of tasks for the processors i.e. converging tasks. Thresholds may again be used to determine if the wait periods are degrading performance in a significant enough manner to warrant performing such convergence to obtain better performance. The converging of tasks in this manner involves the MPI load balancing mechanisms associated with the processors communicating with a load leveler of the parallel program dispatcher to cause the load leveler to adjust the number of tasks dispatched to each of the processors in accordance with their historical profile.

A non data dependent MPI job is defined as an MPI job in which the amount of computation performed during the computation phase is not dependent on the type of data that is received for processing. In other words if all of the processors receive the same amount of data and there are no other factors to cause an increase in the computations performed during the computation phase all of the processors should return a MPI barrier operation call at the same time. In the data dependent case the amount of computation performed during the computation phase is dependent upon the type of data received for processing. Thus even if the same amount of data is received by each processor and there are no other factors to increase the amount of computation the type of data itself may cause the computation phase to be extended or shortened for various processors.

As shown in when each processor finishes its computations during the computation phase of the MPI task that it is executing on a corresponding data set the processor calls a synchronization operation i.e. the MPI barrier operation. The call of this MPI barrier operation is communicated to each of the other processors . For simplicity the communication of the MPI barrier operation to each of the other processors and is shown only for processor but it should be appreciated that each processor would communicate their own MPI barrier operation call to each of the other processors when it occurs. In addition to simply informing the other processors that the processor called the MPI barrier operation an identifier of the task such as a task id or thread id and a timestamp of the MPI barrier operation call are provided to each of the processors . This information may be communicated by each of the processors to their associated MPI load balancing controllers .

The MPI load balancing controllers maintain corresponding history data structures that comprise entries having the task or thread id and a corresponding timestamp for each MPI barrier operation call of each of the processors . Thus each of the history data structures should be identical such that each of the MPI load balancing controllers have the same picture of the historical operation of the parallel execution system with regard to the MPI tasks . The MPI load balancing controllers further contain logic for performing analysis of the entries in the corresponding history data structures to determine how to redistribute workloads amongst the processors so as to minimize wait periods of the processors .

The logic in the MPI load balancing controllers analyzes the entries in the history data structures to determine if the processor associated with the MPI load balancing controller should expect to receive a larger workload smaller workload or the same workload in a subsequent computation phase as a previous computation phase. This determination may be made for example in response to each MPI barrier operation call notification received by the processor either from its own call to the MPI barrier operation or a call from another processor. For example in response to an MPI barrier call notification the thread id and timestamp of the MPI barrier call may be provided to the MPI load balancing controller which stores an entry in the history data structure .

In addition the MPI load balancing controller may determine if any other processor has performed a MPI barrier operation call prior to this MPI barrier operation call notification for the same computation cycle i.e. each processor begins its computation phase at the same time due to the synchronization performed and thus each start of computation phases for the group of processors is referred to as a computation cycle. If no other processor has performed a MPI barrier operation call prior to this MPI barrier operation call then the processor that sent the current MPI barrier operation call e.g. processor is the fastest processor in the group of processors executing tasks of the MPI job . As a result the processor may begin operations to prepare for receiving a larger workload for the next computation cycle. For example the processor may perform operations for obtaining a larger allocation of cache memory for use by the processor setting up buffer space to receive an additional amount of data acquire a host fabric interface HFI window or windows for communication and or the like.

In addition to determining if the processor is the fastest processor the MPI load balancing controller may determine if there are any additional processors that are still in the computation phase and have not returned a MPI barrier operation call. This determination may be made for example based on the task identifiers or thread identifiers returned with the MPI barrier operation calls from the processors that have already made MPI barrier operation calls. Based on a knowledge of the task or thread identifiers that make up the MPI job the MPI load balancing controller may determine which if any task or thread identifiers have not been returned in an MPI barrier call.

If there are still processors engaged in the computation phase then the MPI load balancing controller may determine if its own processor e.g. processor is the last processor in the computation phase. That is if there is only one task or thread identifier that has not been returned in a MPI barrier operation call and that task or thread identifier matches the task or thread identifier of the MPI load balancing controller s associated processor s task or thread then it can be determined that the processor associated with the MPI load balancing controller is the slowest processor of the group. If its own processor is the last one in the computation phase the MPI load balancing controller may begin operations to prepare for receiving a smaller workload i.e. smaller amount of data to process in the next computation cycle. Such operations may include for example reducing the allocated cache for the processor reducing the allocated buffers for receiving data for the next computation cycle determining an ending address offset for the data to be sent to the fastest processor by computing an amount of data to be shifted to the fastest processor releasing HFI windows for communication and or the like.

Once the last of the processors in the group has returned a MPI barrier operation call the next cycle of computation may begin. Results data may be communicated between neighboring processors prior to the start of the next cycle of computation if necessary. One common scenario in which processors must communicate data values occurs with data that lies along the border between two processors i.e. data computed by one processor that is to be used by a neighboring processor. This data will need to be used by each of the two processors on either side of the border. Thus the values of this data are communicated between the two neighboring processors. This does not increase the amount of data that is processed by processors but only changes the values of the data that the processors are utilizing.

Typically each processor obtains their initial chunk of data to work on from the initial distribution of data discussed above. As the computation cycle progresses the data that lies along the border between two processors i.e. data computed by one processor that is to be used by a neighboring processor will need to be used by each of the two processors on either side of the border. Thus the values of this data are communicated between the two neighboring processors. This does not increase the amount of data that is processed by processors but only changes the values of the data that the processors are utilizing.

Prior to starting the next cycle of computation the slowest processor which has been preparing to receive less data in the next computation cycle transmits to the previously determined fastest processor via an MPI communication an ending address offset for the set of data to be processed by the fastest processor . This ending address offset is added to the ending address provided to the processor from the parallel program dispatcher as the next data set for the processor to process. In this way the size of the data set processed by the processor may be increased by changing the ending address in the data superset at which the processor is to stop processing in the next computation cycle. Similarly an address offset for the slowest processor may also be computed and used in a similar manner to reduce the data set processed by the slowest processor in the next computation cycle by a similar amount as the address offset increases the size of the data set for the fastest processor . In effect this causes a shift of data from the slowest processor to the fastest processor . The processors may then read their data sets such as by performing an RDMA read operation from the data storage based on their starting and ending addresses as modified by the address offsets if applicable.

It should be appreciated that the manner by which the ending address offset is calculated may be performed in any desirable manner. As one example in a parallel program or application that is regularly partitioned the ending address offset may be generated based on a linear relationship between an amount of data and a time difference between the fastest and slowest processors. This linear relationship may be provided via a mathematical function lookup table data structure or the like that correlates an amount of time difference with an amount of data to be shifted from one processor to another. Thus based on a determined time difference between the fastest and slowest processors a corresponding amount of data to be shifted may be identified. The ending addresses and hence the starting addresses of the portions of data allocated to the affected processors may then be modified by this amount of data by applying a corresponding offset that is equivalent to the amount of data. In other words a simple ratio of the amount of data per unit time may be utilized and the ending address offset may be set such that a corresponding amount of data may be moved between processors.

It should further be appreciated that one benefit of the illustrative embodiments is that by shifting workloads such as by changing the size of data sets processed by the various processors any heterogeneity between processors may be compensated for. That is an MPI job may be run on a cluster of devices having processors with differing capabilities e.g. faster or slower processing cores more or less memory and the like. By using the mechanisms of the illustrative embodiments these differences in computational ability can be automatically adjusted for without the programmer user having to know there is any heterogeneity in the cluster. The processors having fewer resources will be slower in general and thus work will be shifted to the processors having a larger amount of resources automatically.

As a result the data set in the next computation cycle is reduced for the fourth processor and increased for the first processor. Thus in the second computation cycle the first processor completes its computation phase in 1200 processor cycles or time units the second and third processors again complete their computation phases in 1000 processor cycles time units and the fourth processor completes its computation phase in 900 processor cycles time units. As a result rather than each of the faster processors having to wait 500 processor cycles time units before continuing to the next computation cycle the faster processors only have to wait a maximum of 300 processor cycles time units. This process may be repeated until a sufficiently low wait period is achieved. A processor may continue to receive its increased or decreased size data set until a rebalancing of the workloads calls for the data sets to again be adjusted. Thus a more balanced computation phase is achieved with a reduction in wait time or wasted processor cycles.

It should be appreciated that while the above description of the illustrative embodiments refers to the shifting of a workload from a slowest processor to only a single fastest processor the present invention is not limited to such. Rather the slowest processor may shift workloads to more than one faster processor without departing from the spirit and scope of the present invention. In such an illustrative embodiment the timestamps associated with the MPI barrier operation calls performed by the processors in for example may be used to determine which processors are the fastest processors i.e. the processors that completed their computation phase in the shortest amount of time. A predetermined or dynamically determined number of processors may be selected to which workload from the slowest processor i.e. the processor that completed the computation phase in the longest amount of time may be shifted. The number of processors may be dynamically determined in any suitable manner based on dynamic characteristics of the multiple processor system in which the MPI job is executing. For example the number of processors may be determined based on a difference between the timestamp of the MPI barrier operation call of the fastest processor and the timestamp of the MPI barrier operation call of the slowest processor. This timestamp may be compared to one or more threshold values to determine a number of fastest processors to select for shifting the workload from the slowest processor.

The amount of workload shifted may be distributed evenly over the selected number of fastest processors or may be distributed according to a weighting scheme based on characteristics of the fastest processors. For example relative weights may be determined based on the ranking fastest second fastest third fastest etc. and relative time difference between the MPI barrier operation calls and these weights may be used to determine how much of the workload that is being shifted should be apportioned to each of the selected fastest processors. Other schemes for distributing a workload to be shifted over a plurality of processors may be used without departing from the spirit and scope of the present invention.

The above description of the illustrative embodiment assumes that the identification of the fastest and slowest processors is performed on the fly as MPI barrier operation calls are received in the MPI load balancing controllers in . However rather than performing the identification operations on the fly such identification operations may be performed after all of the MPI barrier operation calls from all of the processors have been received in each of the MPI load balancing controllers . At that time the fastest and slowest processors may be identified by performing a simple comparison of the timestamps of the MPI barrier operation calls. The other operations described above for performing load balancing between the slowest and fastest processors may then be performed based on this identification.

Moreover the above description assumes that the MPI load balancing controllers shift workloads from the slowest processor to the fastest processor s any time there is a difference in the time it takes to complete the computation phase. However in reality some measure of difference in computation phase is acceptable so as to avoid the overhead of performing the load balancing operations. Thus a threshold may be used for determining when the discrepancy between the timestamps of the MPI barrier operation calls of the fastest and slowest processors is sufficient to warrant a load balancing operation to be performed. In this way the overhead of the load balancing operations may be weighed against the amount of wait period and thus wasted processing cycles experienced by the fastest processor s .

The operations described above may be repeated with each new computation cycle such that the excess workload from the slowest processor is shifted to one or more faster processors if necessary. A processor continues to use the end address offset that has been assigned to it until the situation with the multiple processor system dictates that another shift of the workload from the slowest processor to one or more faster processors is necessary. Thus once the fastest processor receives a larger data set it will continue to receive that larger data set in future computation cycles until load balancing reduces that data set.

It should further be appreciated that while the above illustrative embodiments have been described in terms of using address offsets to adjust the ending address of the data sets the present invention is not limited to such. Rather any mechanism for shifting the workload of one processor to one or more other processors may be used without departing from the spirit and scope of the present invention.

As mentioned above the illustrative embodiments previously described in are directed to the case where the tasks performed by the processors are not data dependent. Such workload shifting cannot be performed when the tasks are data dependent since shifting data from one processor to another may corrupt the results obtained from the computations performed by virtue of the fact that the tasks are dependent upon the particular type of data that is received for processing. In the data dependent case it becomes necessary for the workloads to be balanced at a higher level than at the MPI load balancing controller level.

One way in which the load leveler may distribute the load across the processors is to generate additional tasks for the faster processors while keeping the number of tasks assigned to the slower processor the same or even possibly converging the tasks. That is typically an MPI program is executed such that there is exactly one MPI task per processor in the system of processors. Therefore moving a task from a slowest processor to a fastest processor will only have the effect of slowing down the faster processor by providing it twice as much work to do. Meanwhile the slowest processor will have nothing to do and will remain idle thereby wasting resources. Rather than taking this approach however in one illustrative embodiment the load leveler views the MPI job or program as having a predefined amount of work A that may be distributed over N processors with each processor usually being allocated a portion A N of the work to do in the form of N tasks. Work in this context means a portion of a matrix or some other type of data structure that the processor must process in accordance with the instructions in the MPI program.

If one processor is able to perform its A N portion of work faster than the other processors such as because of a dependence of the time it takes to complete the work on the data itself a load imbalance occurs. Thus in order to rebalance the system it is important to be able to provide more work to the faster processor thereby essentially slowing it down and optionally less work to the slower processor s so that they are able to perform their MPI task faster. One way to do this is to give the MPI task assigned to the faster processor more work by increasing the size of the portion of data that it must process and reducing the size of the portion of data the slower processor s must process. This option has been described in detail above.

Another option is to allocate additional tasks to the faster processor for it to run concurrently. This essentially increases the amount of useful work being done by the faster processor slows the faster processor down because of the additional load on its resources and thereby makes the faster processor complete its MPI tasks at a timeframe closer to that of the other processors. Essentially in order to provide additional tasks to the faster processor rather than dividing the work A by the number of processors N such that each processor is allocated 1 Nth of the work the work is divided by N delta where delta is some additional amount determined by the load leveler for example based on a determined difference in completion times of the MPI tasks of the fastest and slowest processors. Thus if the difference in completion times is large the delta value may be large and if the difference is small the delta value may be small. Various relationships between the delta value and the difference in completion times may be utilized. These relationships may be reflected in a function executed by the load leveler a lookup table data structure or the like.

Having divided the work into N delta portions more portions of the work may be assigned to the fastest processor to process concurrently as separate MPI tasks. Thus the fastest processor may spawn additional threads for executing a plurality of MPI tasks which may all be the same MPI program executing on different portions of data for example or different MPI tasks executing on the same or different portions of data concurrently. In a non data dependent implementation of this illustrative embodiment each MPI task may operate on a 1 N delta portion of data. However in a data dependent implementation the amount of data processed by each MPI task may be different but additional MPI tasks may be processed by the same processor concurrently.

Thus if X number of MPI tasks are running concurrently on the fastest processor in the timeframe of a single MPI task the fastest processor would process X N delta work not necessarily data but an amount of work while the slowest processor would process 1 N delta work. The amount of work being processed by the slowest processor is thus reduced and hence the slowest processor should complete faster while the amount of work being processed by the faster processor is increased and thus the fastest processor should complete slower. In this way the completion times of the faster and slower processors are brought closer together and wasted cycles waiting on other processors to complete are thereby reduced.

To illustrate this option with an example it is best to first assume a non data dependent implementation. Thus as one example assume there are four available processors for performing a MPI job on a matrix of data values. Rather than dividing the matrix into four portions and giving one portion to each of the processors as would typically be done the mechanisms of the illustrative embodiment e.g. the load leveler may divide the matrix into five portions and assign two tasks to the fastest processor and one portion to each of the slower processors. The fastest processor runs its two tasks concurrently such that they both complete at approximately the same time. As a result the slower processors have instead of of the matrix i.e. the work to process and the faster processor has instead of of the matrix to process thereby evening out the workload across the plurality of processors.

It should be appreciated that there are other instances where load balancing of data dependent MPI jobs may be advantageous other than when wait periods exceed a predetermined threshold. In these other instances resources of the processors which were previously allocated to other jobs may be freed such that they may be used with the current MPI job. Thus the load leveler may monitor the available resources of the processors in the system to determine if additional resources have been freed and may be used with the current MPI job. For example a resource pool on each of the processors may be monitored by the load leveler to determine if additional resources are freed a number of jobs running on each of the processors may be monitored or the like to determine when load balancing should be performed to make use of freed resources.

For example the processors in the system performing the MPI job may be shared by multiple users executing different jobs. If a first user s job completes far ahead of a second user s job then the resources consumed by the first user s job will be freed and returned to the resource pool. In this case the second user s job may be able to potentially create more MPI tasks and run them on the resources freed up by the completion of the first user s job. Thus in response to detecting the freeing of resources on one or more of the processors the load leveler may initiate the MPI job splitting operation described above to generate additional MPI tasks to take advantage of the additional freed resources.

As another example individual processors in the system performing the MPI job may execute jobs of different priorities. Thus a processor may execute a job having a high priority and a job having a lower priority at the same time. In such a case the job with the higher priority may temporarily steal resources from the job with the lower priority based on the priority policy utilized by the processor. Thus additional resources may be freed for use by the higher priority job. The load leveler may thus again initiate the operation described above in order to generate additional MPI tasks to take advantage of these stolen resources.

Thus the illustrative embodiments provide mechanisms for balancing the load of MPI tasks across a plurality of processors in a multiple processor system executing an MPI job. In so doing the size of the wait periods of faster processors is minimized and thus the available computation time is increased. This provides a great advantage over known MPI based systems which do not provide any load leveling or balancing functionality and instead rely solely on the partitioning of data into equal sized sets to attempt to distribute the workload across processors. As noted above the known approach results in large wait periods for faster processors as they wait for slower processors to complete their computation phases.

Accordingly blocks of the flowchart illustrations support combinations of means for performing the specified functions combinations of steps for performing the specified functions and program instruction means for performing the specified functions. It will also be understood that each block of the flowchart illustrations and combinations of blocks in the flowchart illustrations can be implemented by special purpose hardware based computer systems which perform the specified functions or steps or by combinations of special purpose hardware and computer instructions.

Furthermore the flowcharts are provided to demonstrate the operations performed within the illustrative embodiments. The flowcharts are not meant to state or imply limitations with regard to the specific operations or more particularly the order of the operations. The operations of the flowcharts may be modified to suit a particular implementation without departing from the spirit and scope of the present invention.

As shown in the operation may start with receiving one or more synchronization operation calls from one or more processors of the multiple processor system step . These synchronization calls may include an identifier of a task performing the synchronization operation call and a timestamp of the synchronization operation call for example. Based on the one or more synchronization operation calls one or more entries in a history data structure identifying the one or more synchronization operation calls and their associated task identifier and timestamp are generated step .

A measure of the relative completion of computation phases of tasks of the job on the plurality of processors in the multiprocessor system is determined based on the history data structure entries step . For example this measure of relative completion of computation phases may comprise determining a difference in timestamps to two or more entries of the history data structure. A determination is made as to whether the measure of the relative completion of computation phases meets or exceeds a threshold value step . If not the operation terminates. If the measure of relative completion of computation phases does meet or exceed the threshold value an operation of the plurality of processors for executing the job is modified step . The operation then terminates.

Preferably the operation is modified to reduce wasted processor cycles of one or more of the processors in the plurality of processors. In one illustrative embodiment the modification of the operation comprises modifying workloads of the processors in the plurality of processors in order to bring time periods for completion of MPI tasks executing on the processors within a tolerance of each other. In another illustrative embodiment the modification of the operation comprises selecting another program to execute on at least one of the processors in the plurality of processor while that processor is idle with regard to the MPI job.

In yet another illustrative embodiment the modification of the operation may comprise executing one or more housekeeping operations e.g. a memory management or garbage collection operation in at least one of the processors in the plurality of processors while that processor is idle with regard to the MPI job. In still another illustrative embodiment the modification of the operation may comprise placing at least one of the processors in the plurality of processors into a low power consumption state while that processor is idle with regard to the MPI job. Moreover modifying an operation of the plurality of processors may comprise performing a load balancing function for shifting workload amongst at least two processors in the plurality of processors.

As shown in the operation starts with the MPI load balancing controller receiving a MPI barrier operation call from a processor step . The MPI load balancing controller determines if this MPI barrier operation call is associated with a fastest processor of the plurality of processors executing the MPI job step . If this MPI barrier operation call is associated with a fastest processor the MPI load balancing controller determines if the fastest processor is associated with the MPI load balancing controller step . If not the operation terminates. If so the MPI load balancing controller performs one or more operations to prepare for a larger data set to be distributed to the fastest processor in the next computation cycle step . As discussed above these operations may include for example allocating additional cache resources buffers and or the like for handling an increased size data set.

The MPI load balancing controller then waits to receive a new ending address offset from the MPI load balancing controller of a slowest processor step which is then applied to the ending address provided by the parallel program dispatcher to adjust the size of the data set for the next computation cycle step . The fastest processor then retrieves the data corresponding to the modified data set based on the modified ending address step .

Thereafter or if the MPI barrier operation call is not associated with a fastest processor step the MPI load balancing controller determines if there is only one processor that has not performed an MPI barrier operation call step . If there is only one processor i.e. a slowest processor that has not performed an MPI barrier operation call the MPI load balancing controller determines if the processor that has not performed an MPI barrier operation call is associated with the MPI load balancing controller step . If so the MPI load balancing controller performs one or more operations to prepare to receive a smaller sized data set to be processed in the next computation cycle step . As discussed above this may involve reducing an allocated amount of cache number of buffers etc. as well as computing an amount of the reduction in the size of the data set. The MPI load balancing controller then determines an ending address offset which it communicates to a fastest processor step . Thereafter if there is more than one processor that has not performed an MPI barrier operation call step or if the processor that has not performed the MPI barrier operation call is not associated with the MPI load balancing controller the operation terminates. It should be appreciated that this operation may be repeated for each MPI barrier operation call that is received by the MPI load balancing controller.

If the current state of the multiple processor system does not meet the criteria for load re balancing then the operation terminates. If the current state of the multiple processor system does meet the criteria for load re balancing then a number of MPI tasks to be generated is determined step . As discussed above this determination may involve for example determining a difference between a fastest processor s and a slowest processor s timestamps of their respective barrier operation calls from a previous MPI job cycle and determining a delta value for the number of MPI tasks to be generated. Again such information may be maintained in a history data structure as discussed above and thus may be retrieved when determining the delta value for re balancing MPI tasks in data dependent implementations. The delta value may be determined based on a pre established function a lookup table data structure or the like. The number of MPI tasks may then be generated based on the number of processors and this determined delta value as previously described above.

The load leveler may then generate the additional tasks by splitting the original MPI tasks for the processors into subtasks based on the determined number of MPI tasks to be generated step . As a result the original N MPI tasks for N processors may be split into N delta subtasks which may then be apportioned out to the processors. The load leveler may then determine how many of the MPI subtasks to assign to each of the processors with the fastest processor s receiving a larger number of the MPI subtasks and the slower processor s receiving a smaller number of the MPI subtasks step . The load leveler may then allocate the MPI subtasks to the processors in accordance with the determination in step step with the operation terminating thereafter. Although shows the operation terminating it should be appreciated that this operation may be repeated in a periodic or continuous manner or in response to an event occurring such as a user input or the like.

Thus in one example the fastest processor may receive two MPI subtasks which are now simply MPI tasks to perform concurrently while the rest of the processors may receive one MPI subtask. The MPI subtasks provided to the slower processors will be smaller than the original MPI tasks and thus these processors should complete the MPI subtasks more quickly while the fastest processor will complete its concurrent MPI subtasks more slowly thereby converging the completion times of all the processors. It should be appreciated that the illustrative embodiments may take the form of an entirely hardware embodiment an entirely software embodiment or an embodiment containing both hardware and software elements. In one exemplary embodiment the mechanisms of the illustrative embodiments are implemented in software which includes but is not limited to firmware resident software microcode etc.

Furthermore the illustrative embodiments may take the form of a computer program product accessible from a computer usable or computer readable medium providing program code for use by or in connection with a computer or any instruction execution system. For the purposes of this description a computer usable or computer readable medium can be any apparatus that can contain store communicate propagate or transport the program for use by or in connection with the instruction execution system apparatus or device.

The medium may be an electronic magnetic optical electromagnetic infrared or semiconductor system or apparatus or device or a propagation medium. Examples of a computer readable medium include a semiconductor or solid state memory magnetic tape a removable computer diskette a random access memory RAM a read only memory ROM a rigid magnetic disk and an optical disk. Current examples of optical disks include compact disk read only memory CD ROM compact disk read write CD R W and DVD.

A data processing system suitable for storing and or executing program code will include at least one processor coupled directly or indirectly to memory elements through a system bus. The memory elements can include local memory employed during actual execution of the program code bulk storage and cache memories which provide temporary storage of at least some program code in order to reduce the number of times code must be retrieved from bulk storage during execution.

Input output or I O devices including but not limited to keyboards displays pointing devices etc. can be coupled to the system either directly or through intervening I O controllers. Network adapters may also be coupled to the system to enable the data processing system to become coupled to other data processing systems or remote printers or storage devices through intervening private or public networks. Modems cable modems and Ethernet cards are just a few of the currently available types of network adapters.

The description of the present invention has been presented for purposes of illustration and description and is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art. The embodiment was chosen and described in order to best explain the principles of the invention the practical application and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.

