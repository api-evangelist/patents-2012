---

title: Managing incremental cache backup and restore
abstract: A method is used for managing incremental cache backup and restore. I/O operations are quiesced at a cache module. A first snapshot of a storage object and a second snapshot of an SSD cache object are taken. The I/O operations at the cache module are unquiesced. A single backup image comprising the first snapshot and the second snapshot is created.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09021222&OS=09021222&RS=09021222
owner: Lenovoemc Limited
number: 09021222
owner_city: Hong Kong
owner_country: HK
publication_date: 20120328
---
This application is related to U.S. patent application Ser. No. 13 342 008 entitled MANAGING CACHE BACKUP AND RESTORE filed Dec. 31 2011 which is incorporated herein by reference for all purposes.

Storage devices are employed to store data that is accessed by computer systems. Examples of basic storage devices include volatile and non volatile memory floppy drives hard disk drives tape drives optical drives etc. A storage device may be locally attached to an input output I O channel of a computer. For example a hard disk drive may be connected to a computer s disk controller.

As is known in the art a disk drive contains at least one magnetic disk which rotates relative to a read write head and which stores data nonvolatilely. Data to be stored on a magnetic disk is generally divided into a plurality of equal length data sectors. A typical data sector for example may contain 512 bytes of data. A disk drive is capable of performing a write operation and a read operation. During a write operation the disk drive receives data from a host computer along with instructions to store the data to a specific location or set of locations on the magnetic disk. The disk drive then moves the read write head to that location or set of locations and writes the received data. During a read operation the disk drive receives instructions from a host computer to access data stored at a specific location or set of locations and to transfer that data to the host computer. The disk drive then moves the read write head to that location or set of locations senses the data stored there and transfers that data to the host.

Advances in semiconductor technology have lead to an increase in the use of a semiconductor solid state drive also known as a solid state disk or SSD which uses a flash memory as a storage device in areas such as computer systems. Thus in at least some cases there seems to be a trend towards the use of an SSD as a storage device instead of a magnetic disk. In spite of having features such as for example a relatively small storage capacity and a relatively high price the SSD has some other features that can make it more attractive as a storage device than the conventional magnetic disk in at least some cases.

Features that can make SSDs preferable as storage devices are for example a fast access rate high throughput a high integration density and stability against an external impact. SSDs can move much larger amounts of data and process far more I O requests per time period than conventional magnetic disks. This allows users to complete data transactions much more quickly.

Furthermore advances in manufacturing technologies for SSDs may reduce the production costs of SSDs and also increase the storage capacities of SSDs. These developments may provide further incentive to use SSDs in place of magnetic disks in at least some cases.

Solid state disk systems may also comprise communication controllers such as Fibre Channel FC controllers Ethernet mechanisms ATA or serial ATA interfaces or SCSI controllers for managing data communication with external computing devices.

A storage device may also be accessible over a network. Examples of such a storage device include network attached storage NAS and storage area network SAN devices. A storage device may be a single stand alone component or be comprised of a system of storage devices such as in the case of Redundant Array of Inexpensive Disks RAID groups.

Virtually all computer application programs rely on such storage devices which may be used to store computer code and data manipulated by the computer code. A typical computer system includes one or more host computers that execute such application programs and one or more storage systems that provide storage.

The host computers may access data by sending access requests to the one or more storage systems. Some storage systems require that the access requests identify units of data to be accessed using logical volume LUN and block addresses that define where the units of data are stored on the storage system. Such storage systems are known as block I O storage systems. In some block I O storage systems the logical volumes presented by the storage system to the host correspond directly to physical storage devices e.g. disk drives on the storage system so that the specification of a logical volume and block address specifies where the data is physically stored within the storage system. In other block I O storage systems referred to as intelligent storage systems internal mapping technology may be employed so that the logical volumes presented by the storage system do not necessarily map in a one to one manner to physical storage devices within the storage system. Nevertheless the specification of a logical volume and a block address used with an intelligent storage system specifies where associated content is logically stored within the storage system and from the perspective of devices outside of the storage system e.g. a host is perceived as specifying where the data is physically stored.

In contrast to block I O storage systems some storage systems receive and process access requests that identify a data unit or other content unit also referenced to as an object using an object identifier rather than an address that specifies where the data unit is physically or logically stored in the storage system. Such storage systems are referred to as object addressable storage OAS systems. In object addressable storage a content unit may be identified e.g. by host computers requesting access to the content unit using its object identifier and the object identifier may be independent of both the physical and logical location s at which the content unit is stored although it is not required to be because in some embodiments the storage system may use the object identifier to inform where a content unit is stored in a storage system . From the perspective of the host computer or user accessing a content unit on an OAS system the object identifier does not control where the content unit is logically or physically stored. Thus in an OAS system if the physical or logical location at which the unit of content is stored changes the identifier by which host computer s access the unit of content may remain the same. In contrast in a block I O storage system if the location at which the unit of content is stored changes in a manner that impacts the logical volume and block address used to access it any host computer accessing the unit of content must be made aware of the location change and then use the new location of the unit of content for future accesses.

One example of an OAS system is a content addressable storage CAS system. In a CAS system the object identifiers that identify content units are content addresses. A content address is an identifier that is computed at least in part from at least a portion of the content which can be data and or metadata of its corresponding unit of content. For example a content address for a unit of content may be computed by hashing the unit of content and using the resulting hash value as the content address. Storage systems that identify content by a content address are referred to as content addressable storage CAS systems.

Some storage systems receive and process access requests that identify data organized by file system. A file system is a logical construct that translates physical blocks of storage on a storage device into logical files and directories. In this way the file system aids in organizing content stored on a disk. For example an application program having ten logically related blocks of content to store on disk may store the content in a single file in the file system. Thus the application program may simply track the name and or location of the file rather than tracking the block addresses of each of the ten blocks on disk that store the content.

File systems maintain metadata for each file that inter alia indicates the physical disk locations of the content logically stored in the file. For example in UNIX file systems an inode is associated with each file and stores metadata about the file. The metadata includes information such as access permissions time of last access of the file time of last modification of the file and which blocks on the physical storage devices store its content. The file system may also maintain a map referred to as a free map in UNIX file systems of all the blocks on the physical storage system at which the file system may store content. The file system tracks which blocks in the map are currently in use to store file content and which are available to store file content.

When an application program requests that the file system store content in a file the file system may use the map to select available blocks and send a request to the physical storage devices to store the file content at the selected blocks. The file system may then store metadata e.g. in an inode that associates the filename for the file with the physical location of the content on the storage device s . When the file system receives a subsequent request to access the file the file system may access the metadata use it to determine the blocks on the physical storage device at which the file s content is physically stored request the content from the physical storage device s and return the content in response to the request.

In general since file systems provide computer application programs with access to data stored on storage devices in a logical coherent way file systems hide the details of how data is stored on storage devices from application programs. For instance storage devices are generally block addressable in that data is addressed with the smallest granularity of one block multiple contiguous blocks form an extent. The size of the particular block typically 512 bytes in length depends upon the actual devices involved. Application programs generally request data from file systems byte by byte. Consequently file systems are responsible for seamlessly mapping between application program address space and storage device address space.

File systems store volumes of data on storage devices i.e. collections of data blocks each for one complete file system instance. These storage devices may be partitions of single physical devices or logical collections of several physical devices. Computers may have access to multiple file system volumes stored on one or more storage devices.

File systems maintain several different types of files including regular files and directory files. Application programs store and retrieve data from regular files as contiguous randomly accessible segments of bytes. With a byte addressable address space applications may read and write data at any byte offset within a file. Applications can grow files by writing data to the end of a file the size of the file increases by the amount of data written. Conversely applications can truncate files by reducing the file size to any particular length. Applications are solely responsible for organizing data stored within regular files since file systems are not aware of the content of each regular file.

Files are presented to application programs through directory files that form a tree like hierarchy of files and subdirectories containing more files. Filenames are unique to directories but not to file system volumes. Application programs identify files by pathnames comprised of the filename and the names of all encompassing directories. The complete directory structure is called the file system namespace. For each file file systems maintain attributes such as ownership information access privileges access times and modification times.

File systems often utilize the services of operating system memory caches known as buffer caches and page caches. These caches generally consist of system memory buffers stored in volatile solid state memory of the computer. In this context caching is a technique to speed up data requests from application programs by saving frequently accessed data in memory for quick recall by the file system without having to physically retrieve the data from the storage devices. Caching is also useful during file writes the file system may write data to the memory cache and return control to the application before the data is actually written to non volatile storage. Eventually the cached data is written to the storage devices.

The state of the cache depends upon the consistency between the cache and the storage devices. A cache is clean when its contents are exactly the same as the data stored on the underlying storage devices. A cache is dirty when its data is newer than the data stored on storage devices a cache becomes dirty when the file system has written to the cache but the data has not yet been written to the storage devices. A cache is stale when its contents are older than data stored on the storage devices a cache becomes stale when it has not been updated to reflect changes to the data stored on the storage devices.

In order to maintain consistency between the caches and the storage devices file systems perform flush and invalidate operations on cached data. A flush operation writes dirty cached data to the storage devices before returning control to the caller. An invalidation operation removes stale data from the cache without invoking calls to the storage devices. File systems may flush or invalidate caches for specific byte ranges of the cached files.

Many file systems utilize data structures mentioned above called inodes to store information specific to each file. Copies of these data structures are maintained in memory and within the storage devices. Inodes contain attribute information such as file type ownership information access permissions access times modification times and file size. Inodes also contain lists of pointers that address data blocks. These pointers may address single data blocks or address an extent of several consecutive blocks. The addressed data blocks contain either actual data stored by the application programs or lists of pointers to other data blocks. With the information specified by these pointers the contents of a file can be read or written by application programs. When application programs write to files data blocks may be allocated by the file system. Such allocation modifies the inodes.

Additionally file systems maintain information called allocation tables that indicate which data blocks are assigned to files and which are available for allocation to files. File systems modify these allocation tables during file allocation and de allocation. Most modem file systems store allocation tables within the file system volume as bitmap fields. File systems set bits to signify blocks that are presently allocated to files and clear bits to signify blocks available for future allocation.

A method is used for managing incremental cache backup and restore. I O operations are quiesced at a cache module. A first snapshot of a storage object and a second snapshot of an SSD cache object are taken. The I O operations at the cache module are unquiesced. A single backup image comprising the first snapshot and the second snapshot is created.

High end data storage devices provide enhanced I O performance through multi spindle RAID configurations. Most general RAID configurations provide higher levels of sequential performance over single drive configurations but the performance of highly random I O workloads can suffer on any storage configuration built from rotating media devices.

Solid state drives have become a cost effective power saving performance solution for consumers and small businesses but can trail traditional hard drives in terms of cost of capacity. Having a flash cache feature in data storage device allows users to combine a single SSD with a traditional RAID configuration and realize the performance benefits of the SSD without the added cost of moving to an entirely SSD based solution.

Referring to shown is an example of an embodiment of a system that may be used in connection with performing one or more implementations of the current techniques described herein. The system includes a data storage system connected to host systems and secondary storage system s through communication medium . In this embodiment of the computer system the n hosts may access the data storage system and or secondary storage system for example in performing input output I O operations or data requests. The communication medium may be any one or more of a variety of networks or other type of communication connections as known to those skilled in the art. The communication medium may be a network connection bus and or other type of data link such as a hardwire or other connections known in the art. For example the communication medium may be the Internet an intranet network or other wireless or other hardwired connection s by which the host systems may access and communicate with the data storage system and or secondary storage system and may also communicate with other components included in the system .

Each of the host systems and the data storage system included in the system may be connected to the communication medium by any one of a variety of connections as may be provided and supported in accordance with the type of communication medium . The processors included in the host computer systems may be any one of a variety of proprietary or commercially available single or multi processor system such as an Intel based processor or other type of commercially available processor able to support traffic in accordance with each particular embodiment and application. In alternative example embodiments the secondary storage system may be connected directly to the hosts or other hosts not shown .

It should be noted that the particular examples of the hardware and software that may be included in the data storage systems are described herein in more detail and may vary with each particular embodiment. Each of the host computers and data storage systems may all be located at the same physical site or alternatively may also be located in different physical locations. The communication medium that may be used to provide the different types of connections between the host computer systems and the data storage systems of the system may use any of a variety of different communication protocols. Some or all of the connections by which the hosts and data storage system may be connected to the communication medium may pass through other communication devices such as switching equipment and or a phone line a repeater a multiplexer or a satellite.

Each of the host computer systems may perform different types of data operations in accordance with different types of tasks. In the embodiment of any one of the host computers may issue a data request to the data storage system to perform a data operation. For example an application executing on one of the host computers may perform a read or write operation resulting in one or more data requests to the data storage system and or the secondary storage system .

It should be noted that although elements and are illustrated as single data storage systems such as single data storage arrays elements and may also represent for example multiple data storage arrays alone or in combination with other data storage devices systems appliances and or components having suitable connectivity in an embodiment using the techniques herein. It should also be noted that an embodiment may include data storage arrays or other components from one or more vendors. In subsequent examples illustrating the techniques herein reference may be made to a single data storage array by a vendor such as by EMC Corporation of Hopkinton Mass. However as will be appreciated by those skilled in the art the techniques herein are applicable for use with other data storage arrays by other vendors and with other components than as described herein for purposes of example.

The data storage systems may be a data storage array including a plurality of data storage devices . The data storage devices may include one or more types of data storage devices such as for example one or more disk drives and or one or more solid state drives SSDs . An SSD is a data storage device that uses solid state memory to store persistent data. An SSD using SRAM or DRAM rather than flash memory may also be referred to as a RAM drive. SSD may refer to solid state electronics devices as distinguished from electromechanical devices such as hard drives having moving parts. Flash devices or flash memory based SSDs are one type of SSD that contains no moving parts.

The particular data storage system as described in this embodiment or a particular device thereof such as a disk or particular aspects of a flash device should not be construed as a limitation. Other types of commercially available data storage systems as well as processors and hardware controlling access to these particular devices may also be included in an embodiment.

Host systems provide data through channels to the storage systems and the storage systems may also provide data to the host systems also through the channels. Data in a single data storage system such as a single data storage array may be accessed by multiple hosts allowing the hosts to share the data residing therein.

The data storage system in the embodiment of further comprises a host adaptor HA e.g. network interface card NIC or other network interface coupled to the hosts via the communication medium that may operate in accordance with the technique described herein. Device interfaces may comprise device adaptors and interfaces e.g. a flash drive interface . Secondary storage system may be similarly configured and include similar components not shown .

As described above the data storage systems may be data storage arrays including a plurality of data storage devices respectively in which one or more of the devices are flash memory devices employing one or more different flash memory technologies. In one embodiment the data storage system and or secondary storage array may be an Iomega StorCenter storage system by EMC Corporation of Hopkinton Mass. In the foregoing data storage arrays the data storage devices may include a combination of disk devices and flash devices in which the flash devices may appear as standard drives to the various software tools used in connection with the data storage array. The disk devices may be any one or more different types of disk devices such as for example an ATA disk drive FC disk drive and the like. The flash devices may be constructed using different types of memory technologies such as nonvolatile semiconductor NAND flash memory forming one or more SLC devices and or MLC devices. Additionally flash memory devices and disk devices are two exemplary types of devices that may be included in a data storage system used in connection with the techniques described herein.

Thus the storage systems may include a variety of storage devices with different physical and performance characteristics e.g. types of storage devices disk speed such as in RPMs RAID levels and configurations different replication services such as particular software used in the data storage system providing data replication allocation of cache and processors used to service an I O request.

Referring to shown is a more detailed representation of components that may be included in an embodiment using the techniques herein. In the example a data storage system may include disks as described above. The data storage system may also include storage processing circuitry having a controller a flash based cache and optionally a volatile memory cache e.g. DRAM as well. As described in more detail below one or more of the disks may be include or be included in a flash based disk and cache may include such flash based disk.

The flash based cache is a flash based memory or solid state drive SSD which is non volatile to store data persistently. During normal operations data may be stored to a portion of the component and data may be read from disk into a portion of component .

The controller may be configured to perform data storage operations e.g. file system operations on behalf of the hosts of during normal operation using the flash based cache and the disks . During normal operation in the case of a write request when a host sends a request to write data to the data storage system the controller may store the data of the write request to a portion of the component . During normal operation in the case of a read request when a host sends a request to read data from the data storage system the controller reads the data of the read request from component if such data is available there. Also depending on the implementation the controller may be configured to perform for example as a background process copying of data from disks to a portion of the component and or from component to disks .

As will be appreciated by those skilled in the art the data storage may also include other components than as described for purposes of illustrating the techniques herein.

In step I O operations flow from the application to the file system . In step the I O operation may flow from the file system to the storage object . In step I O operation may flow from the storage object to the SSD cache object . In step the I O operation flows from the SSD cache object back storage object . In step the I O operation flows from the storage object back to the file system . From there in step the I O operation is transmitted from the file system back to the application and processed accordingly. It should be noted that although the example employs a file system and therefore supports files this should not be construed as a limitation. Other known data protocols are supported such as for example blocks.

I O operations flow between the application and objects and as shown in the numbered steps. For example in step I O operations received from hosts at or generated by an application flow from the application to the raw storage object . In step I O operations may flow from the raw storage object to the SSD cache object . In step I O operations flow from the SSD cache object back to the storage object via for example an I O callback. In step an I O operation e.g. buffer is returned to the application .

The techniques described herein advantageously enable communication between the backup module and the cache module . Accordingly in step the backup module may issue a request to freeze or quiesce I O operations e.g. I O communications to the cache module . Freezing or quiescing I O operations may be performed to prepare the storage system for creating a consistent backup state. For example updates are not allowed to an object but reads are allowed. In some embodiments quiescing may be associated with performing one or more operations to ensure object consistency. In some embodiments quiescing a storage object includes performing one or more following flushing one or more buffers associated with the object disabling updates to the object and logging transactions associated with the object. In various embodiments the type of operations associated with performing quiescing is configurable. For example configuration parameters may be included with the request to specify the type of operations and or configuration associated with the operations performed during quiescing. Quiescing I O operations associated with applications running on a host may be coordinated by the host.

In step a response is sent from the cache module back to the backup module . In step a snapshot request is generated and forwarded to the storage object . Similarly in step another snapshot request is generated and forwarded to the SSD cache object . In step based on the received request a snapshot image of the storage object is created. In step based on the received request a snapshot image of the SSD cache object is created. In step I O operations are resumed i.e. unfrozen or unquiesced . In some embodiments unquiescing a component includes performing one or more of the following re enabling updates ending hot backup transaction logging and implementing changes requested to the object while quiesced. In step a response is sent from the cache module to the backup module . In step a backup request is forwarded from the backup module to the storage object snap . In various embodiments the type of backup method used is configurable and or specified by configuration preference included with the request. In some embodiments backup is initiated within a prescribed or indicated amount of time from unquiescing I O operations.

A complete backup of the storage object snap may be forwarded to the single backup image in step . In step a backup request is forwarded from the backup module to the SSD cache object snap. In step a backup of SSD cache dirty block and metadata dump of clean block is performed. Thus a single backup image of both the storage object snap and the SSD cache object snap may be created. The control module may communicate with the storage object and or the SSD cache object via control data signals which communications are represented via the dotted communication paths.

In operation at step a restore operation is initiated. At step the restore module reads restore data from the single backup image . At step the storage object is restored using the storage object portion of the single backup image and at step a storage object restore status is forwarded to the restore module . At step the SSD cache object is restored using the SSD cache object portion of the single backup image using a cache mapping technique described below with reference to . At step an SSD cache object restore status is forwarded to the restore module . At step cache mapping between restored SSD cache object and restored storage object is enabled. Upon completion a restore status value is forwarded to the restore module and at step the application is granted access to the restored storage object . Control and or data signals may be communicated between the cache module and the restored storage object as shown via the dotted communication paths. Thus the restored cache configured storage is exposed to application thereby resulting in an immediate SSD cache I O performance benefit. By contrast conventional systems do not have a cache module that communicates with a backup module and restore module in the manner described herein. Consequently when conventional systems restore a backup because the cache was not restored the cache must be rebuilt anew and as a result suffers performance degradation.

Various embodiments utilizing techniques described herein may employ a cache algorithms such as that described in the following example. The algorithm may configure an SSD cache object of a size X to a storage object of size Y with block size B. Upon successful cache configuration a superblock having the following format may be maintained in the data storage system memory and stored persistently at SSD Cache Object Sector Z. An example on disk superblock format may include the following 

The cache dirty region logging DRL may be an on disk structure comprising a set of cache block entries where each entry contains cached block information state information i.e. dirty or clean and DRL entry size. Block mapping between an SSD cache object and storage object may be done by using known hash based algorithms so that the storage object block data will be cached in a corresponding SSD cache block. An example cache mapping table of an SSD cache and storage object is shown below.

The above table may be build using a linear probing algorithm and hash formula such as for example Storage Object LBA Total Number of SSD Cache Blocks 1 . A cache search module can be implemented using hashing and or various dynamic data structures. The cache search module maintains the locality of reference information of the cached data blocks.

The method begins at step S by reading I O for a block. At Step S a cache logical block address LBA search is executed. If at step S the LBA is found in the cache database the method proceeds to step S where the method reads from cache and then updates flags for the block according to the cache replacement algorithm and then proceeds to step S where a buffer is returned to the requestor. However if the LBA is not found at step S the method proceeds to step S and reads the storage object. At step a determination is made as to whether the storage object read was successful or not. If the read was not successful the method proceeds to step S where the read error is processed. If the read was successful the method proceeds to step S where the buffer is returned to the requestor. At step S a determination is made to see if the cache is full and if not at step S the method finds a free cache block to be updated with the storage object. However if at step S the cache is determined to be full the method proceeds to step S where based on the cache replenish mechanism the method locates the cache block to be updated with storage object data. In either case the method proceeds to step S where data is written to the identified cache block. At step S a determination is made as to whether the write was successful or not. If the read was successful the method proceeds to step S where various updates are performed including updating the cache search database with the new block updating flags and updating system memory and on disk cache superblock with the block information as clean. However if at step S the write was not successful the method proceeds to step S where the write error is processed.

However if the LBA is not found in the cache database at step S the method proceeds to step S and a determination to see if the cache is full. If the cache is full based on the cache replenish mechanism the method locates the cache block to be updated with write I O data and continues with step S. If the cache is not full at step S the method locates a free cache block to be updated with write I O data. At step S the data is written to the identified cache block.

At step S if the write was not successful the method proceeds to step S where the write error is processed. If the write was successful the method proceeds to step S where various updates are performed including updating the cache search database with the new block updating flags and updating system memory and on disk cache superblock with the required information. The method then proceeds to step where it returns to the requestor and a dirty threshold value is calculated and a cache flush daemon is invoked accordingly.

It should again be emphasized that the technique implementations described above are provided by way of illustration and should not be construed as limiting the present invention to any specific embodiment or group of embodiments. For example the invention can be implemented in other types of systems using different arrangements of processing devices and processing operations. Also message formats and communication protocols utilized may be varied in alternative embodiments. Moreover various simplifying assumptions made above in the course of describing the illustrative embodiments should also be viewed as exemplary rather than as requirements or limitations of the invention. Numerous alternative embodiments within the scope of the appended claims will be readily apparent to those skilled in the art.

In this embodiment incremental backup is supported through the use of an in memory and on disk bitmap to track all write operations of the storage object at the cache module. The size of the bitmap maintained by the cache module may be as follows 

The in memory and on disk i.e. on cache dedicated external volume storage object DRL bitmap of above size is maintained and the same is mutually exclusive of the cache DRL bitmap. That is bitmaps are maintained for both the disk and the cache.

The DRL bitmap tracks all writes going to the disk as well as the cache and are maintained in the cache module. These bitmaps are used when a backup is performed for the disk and cache. Thus they are maintained by the cache module because the cache module is aware of all writes going to the disk and cache. Conventionally only one bitmap i.e. only metadata of dirty blocks in the superblock of storage object is maintained as part of a standard backup procedure i.e. not maintained by the cache module and as a result conventional methods cannot avail point in time SSD cache benefit to a restored storage object.

A full backup of a cache configured storage object may be performed using backup techniques such as those described in the related U.S. patent application Ser. No. 13 342 008 entitled MANAGING CACHE BACKUP AND RESTORE. Consecutive incremental backups may follow the backup method described below.

More detailed examples of backup algorithms with reference to the general backup algorithm discussed above in are presented below. In one example embodiment the algorithm below referred to herein as backup method 1 uses the backup image format shown in and proceeds as follows 

Step perform a full backup of storage object data blocks from the storage object in the backup image starting from 2nd block i.e. after the backup header start marker 

It should be noted that the backup header cache superblock and DRL metadata blocks need not be written at the locations specified in above algorithm and the locations that consume less seek time during the backup restore can be standardized accordingly. The backup format shown in is but one example embodiment that can be implemented to achieve the functionality described herein.

An alternative example embodiment employing the backup image depicted in is described below. In this example the algorithm illustrated may use considerably less storage space than backup method 1. As described above the backup image of includes storage object data blocks SSD cache dirty data blocks backup header DRL metadata blocks cache superblock and storage object SO DRL metadata . The backup header DRL metadata block and cache superblocks include a number of fields and are of sufficient size to hold respective fields . The space efficient algorithm below referred to herein as backup method 2 uses the backup image format shown in 

A corresponding example restore algorithm with reference to the general restore algorithm discussed above in is presented below. The restore algorithm may utilize the backup image format shown in and may be used in conjunction with backup method 1 and backup method 2 as described above. The restore algorithm proceeds as follows 

The subject matter described herein for managing data backup and recovery in a data storage system may be implemented in hardware software firmware or any combination thereof. As such the terms technique or logic as used herein may refer to software in combination with hardware and or firmware for implementing the feature being described. In one example implementation the subject matter described herein may be implemented using a non transitory computer readable medium having stored thereon computer executable instructions that when executed by the processor of a computer perform steps.

Exemplary non transitory computer readable media suitable for implementing the subject matter described herein include disk memory devices chip memory devices programmable logic devices and application specific integrated circuits. In addition a computer program product that implements the subject matter described herein may be located on a single device or computing platform or may be distributed across multiple devices or computing platforms.

While the invention has been disclosed in connection with preferred embodiments shown and described in detail their modifications and improvements thereon will become readily apparent to those skilled in the art. Accordingly the spirit and scope of the present invention should be limited only by the following claims.

