---

title: Architecture of networks with middleboxes
abstract: Some embodiments provide a system for implementing a logical network that includes a set of end machines, a first logical middlebox, and a second logical middlebox connected by a set of logical forwarding elements. The system includes a set of nodes. Each of several nodes includes (i) a virtual machine for implementing an end machine of the logical network, (ii) a managed switching element for implementing the set of logical forwarding elements of the logical network, and (iii) a middlebox element for implementing the first logical middlebox of the logical network. The system includes a physical middlebox appliance for implementing the second logical middlebox.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08966024&OS=08966024&RS=08966024
owner: Nicira, Inc.
number: 08966024
owner_city: Palo Alto
owner_country: US
publication_date: 20121115
---
This application claims the benefit of U.S. Provisional Application 61 560 279 entitled Virtual Middlebox Services filed Nov. 15 2011. U.S. Application 61 560 279 is incorporated herein by reference.

Many current enterprises have large and sophisticated networks comprising switches hubs routers middleboxes e.g. firewalls load balancers source network address translation etc. servers workstations and other networked devices which support a variety of connections applications and systems. The increased sophistication of computer networking including virtual machine migration dynamic workloads multi tenancy and customer specific quality of service and security configurations require a better paradigm for network control. Networks have traditionally been managed through low level configuration of individual network components. Network configurations often depend on the underlying network for example blocking a user s access with an access control list ACL entry requires knowing the user s current IP address. More complicated tasks require more extensive network knowledge forcing guest users port 80 traffic to traverse an HTTP proxy requires knowing the current network topology and the location of each guest. This process is of increased difficulty where the network switching elements are shared across multiple users.

In response there is a growing movement towards a new network control paradigm called Software Defined Networking SDN . In the SDN paradigm a network controller running on one or more servers in a network controls maintains and implements control logic that governs the forwarding behavior of shared network switching elements on a per user basis. Making network management decisions often requires knowledge of the network state. To facilitate management decision making the network controller creates and maintains a view of the network state and provides an application programming interface upon which management applications may access a view of the network state.

Some of the primary goals of maintaining large networks including both datacenters and enterprise networks are scalability mobility and multi tenancy. Many approaches taken to address one of these goals results in hampering at least one of the others. For instance one can easily provide network mobility for virtual machines within an L2 domain but L2 domains cannot scale to large sizes. Furthermore retaining user isolation greatly complicates mobility. As such improved solutions that can satisfy the scalability mobility and multi tenancy goals are needed.

Some embodiments provide a system that allows a user to specify a logical network that includes one or more middleboxes e.g. firewalls load balancers network address translators intrusion detection systems IDS wide area network WAN optimizers etc. . The system implements the logical network by distributing logical forwarding elements e.g. logical switches logical routers etc. across numerous managed switching elements operating on numerous physical machines that also host virtual machines of the logical network. In implementing such a logical network the system of some embodiments implements different middleboxes in different manners. For instance the system may implement a first middlebox in a distributed manner e.g. with the middlebox implemented across numerous managed middlebox elements that also operate on the physical machines alongside the managed switching elements and a second middlebox in a centralized manner e.g. as a single appliance or virtual machine as a cluster . In some embodiments the determination as to whether to implement a particular middlebox in a distributed or centralized matter is based on the state sharing requirements between different middlebox elements when the middlebox is distributed.

In some embodiments the spectrum for possible implementation of logical middleboxes into a physical network ranges from a fully distributed middlebox to a fully centralized middlebox with different middleboxes implemented at different points along such a spectrum. In addition a single type of middlebox may be implemented in both a centralized or a distributed fashion including within the same managed logical network. For example a user might want a first firewall for filtering all traffic incoming from external networks and a second firewall for filtering traffic between different subnets of the logical network. In some cases the best solution may be to implement the first firewall as a single appliance to which all external incoming traffic is forwarded while implementing the second firewall in a distributed fashion across all of the physical machines on which virtual machines of the logical network are hosted.

At one end of the spectrum is a fully distributed middlebox architecture. In this case the middlebox is implemented across numerous nodes physical host machines . Each of the physical host machines in some embodiments hosts at least one virtual machine in the logical network containing the logical middlebox. In addition a managed switching element runs on each of the host machines in order implement the logical forwarding elements of the logical network. As a particular physical host machine may host virtual machines in more than one logical network e.g. belonging to different tenants both the distributed middlebox and the managed switching element running on the host may be virtualized in order to implement middleboxes and logical forwarding elements from different logical networks.

In some embodiments a middlebox may be implemented in such a distributed fashion when minimal sharing of state or none at all is required between the middlebox instances. At least some types of middleboxes are stateful in that they establish states for connections between machines e.g. between two virtual machines in the network between a virtual machine in the network and an external machine etc. . In some embodiments the middlebox establishes a state for each transport layer connection e.g. TCP connection UDP connection . In the distributed case of some embodiments a middlebox element operating at a particular host machine creates states for the transport connections passing through it but does not need to share these states with the other middlebox elements operating on the other host machines. When the states only apply to the virtual machines hosted on the particular host machine and the middlebox does not need to perform any analysis using state information established for other virtual machines then the middlebox may be distributed. Examples of such middleboxes include source network address translation S NAT destination network address translation D NAT and firewalls.

In addition some embodiments allow distribution of middleboxes that have a minimal level of state sharing. For example load balancers may query the machines across which they balance traffic to determine the current level of traffic sent to each of the machines then distribute this to the other load balancers. However each load balancing element can run a load balancing algorithm on its own and perform the queries at regular intervals rather than sharing state information with every other load balancing element every time a packet is routed to one of the virtual machines or every time a transport e.g. TCP UDP etc. connection is established with one of the virtual machines.

On the other side of the spectrum is the fully centralized middlebox implementation. In such a centralized implementation the managed switching elements in the hosts send all traffic for the middlebox to process to the same middlebox appliance. This single middlebox may be a separate physical machine or a separate virtual machine operating within the physical network on its own host machine or in the same host as one of the virtual machines in the network . When a managed switching element identifies that a packet should be sent to the middlebox the switching element sends the packet through the physical network to the middlebox e.g. via a tunnel . The middlebox processes the packet then sends the packet actually a new packet to another managed switching element for processing e.g. a pool node .

Some embodiments use such a centralized middlebox when a distributed middlebox would require packet sharing at data plane speeds. That is for each traffic packet processed by a middlebox element the element would have to update all of the other middlebox instances with the state change resulting from the packet processing. Thus each traffic packet passing through a middlebox would result in an explosion of additional traffic in order to update all of the other middlebox instances. Examples of such middleboxes include IDSs and WAN optimizers. For instance in order to properly monitor for intrusions IDS processing needs to know about all connections within the network. As such if the IDS were distributed new state updates would have to be sent for every packet processed by a distributed IDS element.

As a third option some embodiments use a cluster architecture for some middleboxes that is similar to the fully centralized architecture except that the cluster acts as a centralized resource pool rather than a single physical machine. A middlebox cluster e.g. a cluster of IDS boxes may be beneficial in some embodiments when the network or networks using the middlebox is larger and a single appliance may not have enough resources e.g. memory processing power etc. to handle the larger deployment. However when the cluster is a middlebox that requires knowledge of all of the state information then this state information will be shared between the various machines in the cluster. In some embodiments the middlebox cluster may be a better option than a single appliance when the analysis does not require state updates on a per packet basis but rather a per transport connection or several updates per connection while less often than per packet basis. In order to perform the high speed state sharing required some embodiments link the middlebox machines in the cluster via a separate dedicated high speed connection for sharing the state information.

The preceding Summary is intended to serve as a brief introduction to some embodiments of the invention. It is not meant to be an introduction or overview of all inventive subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly to understand all the embodiments described by this document a full review of the Summary Detailed Description and the Drawings is needed. Moreover the claimed subject matters are not to be limited by the illustrative details in the Summary Detailed Description and the Drawing but rather are to be defined by the appended claims because the claimed subject matters can be embodied in other specific forms without departing from the spirit of the subject matters.

In the following detailed description of the invention numerous details examples and embodiments of the invention are set forth and described. However it will be clear and apparent to one skilled in the art that the invention is not limited to the embodiments set forth and that the invention may be practiced without some of the specific details and examples discussed.

Some embodiments provide a system that allows a user to specify a logical network that includes one or more middleboxes e.g. firewalls load balancers network address translators intrusion detection systems IDS wide area network WAN optimizers etc. . The system implements the logical network by distributing logical forwarding elements e.g. logical switches logical routers etc. across numerous managed switching elements operating on numerous physical machines that also host virtual machines of the logical network.

In implementing such a logical network the system of some embodiments implement different middleboxes in different manners. For instance the system may implement a first middlebox in a distributed manner e.g. with the middlebox implemented across numerous managed middlebox elements that also operate on the physical machines alongside the managed switching elements and a second middlebox in a centralized manner e.g. as a single appliance or virtual machine as a cluster . In some embodiments the determination as to whether to implement a particular middlebox in a distributed or centralized matter is based on the state sharing requirements between different middlebox elements when the middlebox is distributed.

At the left end of spectrum is a fully distributed middlebox architecture of some embodiments. As shown in the diagram the middlebox is implemented across numerous nodes physical host machines . Each of the physical host machines in some embodiments hosts at least one virtual machine in the logical network containing the logical middlebox. In addition a managed switching element runs on each of the host machines in order implement the logical forwarding elements e.g. logical routers logical switches of the logical network. As a particular physical host machine may host virtual machines in more than one logical network e.g. belonging to different tenants both the distributed middlebox and the managed switching element running on the host may be virtualized in order to implement middleboxes and logical forwarding elements from different logical networks. In some embodiments the middlebox is implemented as a module or set of modules that runs within the hypervisor of the host machine.

As indicated in the figure a middlebox may be implemented in such a distributed fashion when minimal sharing of state or none at all is required between the middlebox instances. At least some types of middleboxes are stateful in that they establish states for connections between machines e.g. between two virtual machines in the network between a virtual machine in the network and an external machine etc. . In some embodiments the middlebox establishes a state for each transport layer connection e.g. TCP connection UDP connection . In the distributed case of some embodiments a middlebox element operating at a particular host machine creates states for the transport connections passing through it but does not need to share these states with the other middlebox elements operating on the other host machines. When the states only apply to the virtual machines hosted on the particular host machine and the middlebox does not need to perform any analysis using state information established for other virtual machines then the middlebox may be distributed. Examples of such middleboxes include source network address translation S NAT destination network address translation D NAT and firewalls.

In addition some embodiments allow distribution of middleboxes that have a minimal level of state sharing. For example load balancers may query the machines across which they balance traffic to determine the current level of traffic sent to each of the machines then distribute this to the other load balancers. However each load balancing element can run a load balancing algorithm on its own and perform the queries at regular intervals rather than sharing state information with every other load balancing element every time a packet is routed to one of the virtual machines or every time a transport e.g. TCP UDP etc. connection is established with one of the virtual machines.

On the other side of the spectrum is the fully centralized middlebox implementation . In this implementation the managed switching elements in the hosts send all traffic for the middlebox to process to the same middlebox appliance. This single middlebox may be a separate physical machine or a separate virtual machine operating within the physical network on its own host machine or in the same host as one of the virtual machines in the network . When a managed switching element identifies that a packet should be sent to the middlebox the switching element sends the packet through the physical network to the middlebox e.g. via a tunnel . The middlebox processes the packet then sends the packet actually a new packet to another managed switching element for processing.

In some embodiments the managed switching element to which the new packet is sent is a pool node. A pool node in some embodiments is a particular type of managed switching element located at the network interior i.e. not directly connected to any of the virtual machines at the network edges for handling traffic that the edge switching elements i.e. those located on the host machines directly connected to the virtual machines are not able to process. In other embodiments rather than send the packet to a pool node the middlebox sends the traffic to a managed switching element built directly into the middlebox appliance.

Some embodiments use such a centralized middlebox when a distributed middlebox would require packet sharing at data plane speeds. That is for each traffic packet processed by a middlebox element the element would have to update all of the other middlebox instances with the state change resulting from the packet processing. Thus each traffic packet passing through a middlebox would result in an explosion of additional traffic in order to update all of the other middlebox instances. Examples of such middleboxes include IDSs and WAN optimizers. For instance in order to properly monitor for intrusions IDS processing needs to know about all connections within the network. As such if the IDS were distributed new state updates would have to be sent for every packet processed by a distributed IDS element.

The middlebox cluster architecture is similar to the fully centralized architecture except that the cluster acts as a centralized resource pool rather than a single physical machine. As shown in the figure a middlebox cluster e.g. a cluster of IDS boxes may be beneficial when the network or networks using the middlebox is larger and a single appliance may not have enough resources e.g. memory processing power etc. to handle the larger deployment. However when the cluster is a middlebox that requires knowledge of all of the state information then this state information will be shared between the various machines in the cluster. In some embodiments the middlebox cluster may be a better option than a single appliance when the analysis does not require state updates on a per packet basis but rather a per transport connection or several updates per connection while less often than per packet basis. In order to perform the high speed state sharing required some embodiments link the middlebox machines in the cluster via a separate dedicated high speed connection for sharing the state information.

The above illustrates examples of different implementation of logical middleboxes in a network of some embodiments. Several more detailed embodiments are described below. Section I describes the different middlebox architectures of some embodiments. Section II describes the distributed middlebox implementation of some embodiments. Next Section III describes the network control system of some embodiments for configuring a network in order to implement a logical network that includes one or more middleboxes. Section IV then describes an illustrative example of a network with numerous different middleboxes. Finally Section V describes an electronic system with which some embodiments of the invention are implemented.

As described above some embodiments implement different middleboxes using different architectures within a managed network. Even for the same logical network topology some middleboxes are implemented in a distributed fashion with a middlebox element operating on each host on which a virtual machine of the network operates while other middleboxes are implemented in a centralized fashion with a single appliance or cluster to which the managed switching elements on the hosts connect .

In addition a middlebox attaches to the logical router . One of ordinary skill in the art will recognize that the network topology represents just one particular logical network topology into which a middlebox may be incorporated. In various embodiments the middlebox may be located directly between two other components e.g. directly between the gateway and logical router e.g. in order to monitor and process all traffic entering or exiting the logical network or in other locations in a more complex network.

In the architecture shown in the middlebox is not located within the direct traffic flow either from one domain to the other or between the external world and the domain. Accordingly packets will not be sent to the middlebox unless routing policies are specified e.g. by a user such as a network administrator for the logical router that determine which packets should be sent to the middlebox for processing. Some embodiments enable the use of policy routing rules that forward packets based on data beyond the destination address e.g. destination IP or MAC address . For example a user might specify e.g. through a network controller application programming interface API that all packets with a source IP address in the logical subnet switched by logical switch or all packets that enter the network from the external network destined for the logical subnet switched by the logical switch should be directed to the middlebox for processing.

Different middleboxes may perform different functionalities within the network. For instance a firewall analyzes data packets to determine whether or not the packets should be allowed through i.e. similar to ACL flow entries . The firewall stores a set of rules e.g. entered by a user that determine whether or not the firewall drops i.e. discards or allows the packet through or in some cases rejects the packet by dropping the packet and sending an error response back to the sender . In some embodiments the firewall is a stateful firewall that keeps track of transport e.g. TCP and or UDP connections and uses the stored state information to make faster packet processing decisions.

Source network address translation S NAT modifies the source IP address of packets in the packet headers. For instance S NAT may be used so that the IP addresses of numerous different machines with different IP addresses can be hidden from destination machines by changing the source of packets from the different machines to a single IP address. Destination network address translation D NAT similarly modifies the destination IP address of packets in order to hide the real IP addresses from the source machines. Load balancing is a form of D NAT that uses various algorithms e.g. round robin random assignment etc. to balance traffic across numerous destination machines. A load balancer receives a packet for a specific IP address that is exposed to the source machine and modifies the destination IP address of the packet to match up with a particular one of the destination machines selected by the load balancing algorithm.

An intrusion detection system IDS is a passive middlebox in some embodiments that monitors the logical network for malicious activities or policy violations. The IDS may examine transport connections e.g. TCP connections UDP connections etc. to determine whether an attack on the network is occurring.

A WAN optimizer is a middlebox device for increasing the efficiency of data transfers across a WAN e.g. accelerating the flow of data across the WAN . Examples of WAN optimization techniques include data deduplication data compression latency optimization caching and or proxying forward error correction protocol spoofing traffic shaping equalizing connection limiting simple rate limiting etc. While the above is a list of some of the several different middleboxes one of ordinary skill in the art will recognize that some embodiments may include various different middleboxes that may be implemented in either a distributed or centralized manner.

Depending on the type of middlebox and in some cases the type of implementation requested by a user a middlebox such as that shown in will be implemented in either a centralized fashion or a distributed fashion. conceptually illustrates such a distributed implementation of some embodiments. Specifically illustrates several nodes including a first host machine a second host machine a third host machine and an Nth host machine . Each of the first three nodes hosts several virtual machines of the network with virtual machine hosted on the first host machine virtual machines and hosted on the second host machine and virtual machine hosted on the third host machine .

In addition each of the host machines includes a managed switching element MSE . The managed switching elements of some embodiments are software forwarding elements that implement logical forwarding elements for one or more logical networks. For instance the MSEs in the hosts include flow entries in forwarding tables that implement the logical forwarding elements of network . Specifically the MSEs on the host machines implement logical switches and as well as the logical router . On the other hand some embodiments only implement logical switches at a particular node when at least one virtual machine connected to the logical switch is located at the node i.e. only implementing logical switch and logical router in the MSE at host . The Nth host does not include any virtual machines from the network and therefore the MSE residing on that host does not implement any logical forwarding elements from the network .

The implementation of some embodiments also includes a pool node that connects to the host machines. In some embodiments the MSEs residing on the host perform first hop processing. That is these MSEs are the first forwarding elements a packet reaches after being sent from a virtual machine and attempt to perform all of the logical switching and routing at this first hop. However in some cases a particular MSE may not store flow entries containing all of the logical forwarding information for a network and therefore may not know what to do with a particular packet. In some such embodiments the MSE sends the packet to a pool node for further processing. These pool nodes are interior managed switching elements which in some embodiments store flow entries that encompass a larger portion of the logical network than the edge software switching elements.

Similar to the distribution of the logical switching elements across the hosts on which the virtual machines of network reside the middlebox is distributed across middlebox elements on these hosts . In some embodiments a middlebox module or set of modules resides on the host machines e.g. operating in the hypervisor of the host . When the user sets up the logical network e.g. network the input includes a configuration from the middlebox. For instance for a firewall the user would input a set of rules for packet filtering e.g. based on IP address TCP connection etc. . In some embodiments a network control system that is used to provision the managed switching elements to implement the logical forwarding elements may also be used to provision the various middlebox elements operating on the host machines. When the user inputs the middlebox configuration into a controller of the network control system the controller identifies the particular nodes over which the middlebox configuration should be implemented and distributes the configuration to these nodes e.g. through a set of controllers .

When one of the virtual machines sends a packet e.g. to another one of the virtual machines to an external address etc. the packet initially goes to the local managed switching element for processing. The MSE may use its stored flow entries stored to make a forwarding decision to send the packet to the middlebox in which case some embodiments send the packet to the local middlebox element on the same host. In some embodiments the middlebox element and the MSE negotiate a software port through which to transfer packets with minimal delay. After the middlebox processes the packet some embodiments then send the packet back to the MSE through this same port. In some embodiments this packet is sent from the middlebox to the MSE as a new packet and therefore requiring new processing by the MSE. In some situations however no packets are sent back. For instance if the middlebox is a firewall the middlebox may block or drop the packet. In addition some embodiments of the middlebox are passive and duplicates of the packets are sent to the middlebox in order for the middlebox to keep track of statistics but are not sent back to the switching element.

While illustrates only one logical network implemented across the hosts some embodiments implement numerous logical networks e.g. for different tenants across the set of hosts. As such a middlebox element on a particular host might actually store configurations for several different middleboxes belonging to several different logical networks. For example a firewall element may be virtualized to implement two or more different firewalls. These will effectively operate as two separate middlebox instances such that the middlebox element is sliced into several virtual middleboxes of the same type . In addition when the MSE on the host sends packets to the middlebox some embodiments append e.g. prepend a slice identifier or tag on the packet to identify to which of the several virtual middleboxes the packet is being sent. When multiple middleboxes are implemented on the same middlebox element for a single logical network e.g. two different load balancers the slice identifier will need to identify the particular middlebox slice rather than just the logical network to which the packet belongs. Different embodiments may use different slice identifiers for the middleboxes.

Examples of middleboxes that may be distributed in some embodiments include firewalls S NATs and load balancers. In each of these cases the middlebox plays an active role in the packet processing i.e. S NATs and load balancers modify source and destination addresses of the packets respectively while firewalls make decisions as to whether to allow or drop packets . However each of these middlebox elements on a particular node can function on its own without requiring information from the corresponding middlebox elements on the other nodes. Even distributed load balancer elements can each separately load balance incoming traffic across different virtual machines with the assumption that none of the virtual machines are likely to become overloaded so long as the other load balancer elements use the same algorithm. Nevertheless in some embodiments the load balancer elements will share state e.g. after querying the destination virtual machines for usage and health statistics at some level.

In this example however the middlebox is not distributed across the hosts . Instead the middlebox is implemented as a single machine external to the hosts. In different embodiments and for different types of middleboxes this single box may be a single physical appliance e.g. a separate physical device or a single virtual machine which may in fact operate on one of the host machines or on a different host machine . For instance some embodiments may provide a first type of middlebox e.g. a WAN optimizer as a virtual machine while providing a second type of middlebox e.g. an IDS as a single appliance. In addition some embodiments may provide both options for a single type of middlebox.

As with the distributed middlebox in some embodiments the network control system is used to provision a centralized middlebox appliance or virtual machine. Rather than the controller receiving configuration information and identifying numerous nodes to which the configuration information should be distributed the controller of some embodiments identifies an appliance on which to implement the middlebox and distributes the configuration to the appliance e.g. through an intermediary controller that manages the appliance. In some embodiments several physical appliances may exist within the physical network and the controller chooses one of these appliances to implement the middlebox. When the middlebox is implemented as a virtual machine some embodiments select a host node for the virtual machine and then distribute the configuration to the node. In either case the network control system also specifies a connection or attachment between the middlebox and the various managed switching elements on the host. In some embodiments the middlebox appliance supports one or more types of tunneling and the flow entries distributed to the managed switching elements include entries specifying the tunnel encapsulation to use in order to send packets to the middlebox.

When a flow entry in a managed switching element specifies to send traffic to the middlebox the managed switching element also encapsulates the packet using this tunnel information and sends the packet out of its host through the tunnel to the middlebox. As with the distributed middleboxes some centralized middleboxes are active middleboxes. That is the middlebox sends the packet back to the network after performing its middlebox processing. In some embodiments such middleboxes are configured to always send the packet as a new packet to a pool node e.g. always the same pool node one of several pool nodes . In the centralized middlebox sends all of its outgoing traffic to the pool node . The pool node which also implements the logical forwarding elements then forwards the packet to the appropriate destination machine.

Just as the distributed middlebox elements may be virtualized to perform middlebox instances for several different logical networks so may the centralized middlebox . The same physical appliance or virtual machine may be used by numerous different logical networks. In some embodiments a similar slicing technique to that used in the distributed architecture is used. That is the managed switching element adds a tag to indicate the logical network or particular logical middlebox in the logical network to which the packet is being sent and the middlebox uses this tag to identify which of the middlebox instances it implements should be used to process the packet. In some embodiments the centralized middlebox appliance includes numerous ports each of which maps to a different virtual middlebox instances. In such embodiments the slicing technique may not be used and instead the incoming port is used to identify the correct virtual middlebox.

Whereas the middlebox is a single resource some embodiments implement a middlebox as a centralized cluster of resources as shown in the implementation of . This example is the same as that shown in except that rather than a single middlebox device the network includes a middlebox cluster with three middlebox resources . In some embodiments each of the middlebox resources is a separate device or virtual machine i.e. the equivalent of middlebox .

Different embodiments may use different architectures within the middlebox cluster. Some embodiments include an entry point e.g. a single physical device to the cluster that load balances packets across the resource pool. Other embodiments have different host machines connected directly to different resources within the cluster. For instance the network might be set up with the first host machine connected to resource the second host machine connected to resource and the third host machine connected to resource . Other embodiments use a master backup setup in which the cluster has two devices. The host machines all connect to the master which performs the middlebox processing while sharing state data with the backup resource.

As described above by reference to some embodiments use the centralized middlebox implementation when state sharing is required on a per packet basis. That is for certain middleboxes the middlebox processing requires knowledge of all of the packets processed by the middlebox. For a distributed middlebox this would require an explosion of state updates being sent out over the network connecting the middlebox elements. However when the middlebox is a single appliance that single appliance will store all of the state at all times.

For a middlebox cluster such as the cluster this requirement means that state must be shared between the middlebox resources in the cluster at high speeds. Some embodiments use dedicated connections between the middlebox resources to share this state information. That is a particular port on each middlebox device is dedicated only to state sharing between the several devices in the cluster. Often a middlebox cluster will only be two machines or a few machines operating in close proximity making such dedicated connections more feasible. For situations with more than two middleboxes some embodiments use a mesh network in which each middlebox resource broadcasts state updates over the network to all other middlebox resources. Other embodiments use a star network in which the middlebox resources transmit their state updates to a central resource that amalgamates the updates and sends them to the other resources. While middlebox clusters require this additional infrastructure as compared to the centralized case the clusters have the benefit of being able to handle larger deployments in which a larger number of packets are processed.

As mentioned both WAN optimizers and intrusion detection systems are examples of middleboxes that some embodiments implement as centralized middleboxes because of the state sharing requirements. The WAN optimizer for example increases the efficiency of data transfers across a WAN using various optimization techniques. To perform these optimization techniques requires access to all of the traffic being sent over the WAN and thus a centralized implementation is more optimal. Furthermore the WAN optimizer may be used to cache content sent over the WAN and the caching only serves its purpose if the cache is stored together rather than distributed over numerous hosts.

An intrusion detection system is a passive system i.e. does not drop or modify packets that monitors total numbers of connections the addresses on those connections the number of packets for each connection etc. In order to detect intrusions the IDS looks for patterns in the connections heuristics etc. for which the IDS processing must be aware of all of the traffic monitored. If one distributed element has information about a first connection and a second distributed element has information about a second connection neither element has enough information to properly evaluate the network for an intrusion.

Unlike the previous figures shows connections along with arrows to show direction of packet transfer between machines in the system. Thus for example all of the hosts can send packets to each other in both directions. That is virtual machines at host send packets to virtual machines at via the managed switching element at host and then the managed switching element at host and the virtual machines at host also send packets to the virtual machines at host . In addition all of the MSEs at the hosts may use the pool node to process packets for which the edge MSEs cannot make a forwarding decision.

Each of the host machines and that host virtual machines from the particular logical network send packets to the IDS. In some embodiments all traffic on the logical network is sent to the IDS. However these arrows are unidirectional as the intrusion detection system of some embodiments is a passive middlebox. Some embodiments rather than forwarding traffic through the middlebox send a duplicate packet to the IDS box . The IDS receives these duplicate packets i.e. a packet for each one sent through the network between hosts and or an external network and performs its intrusion detection analysis. Because the intrusion detection system does not output any traffic packets there is no connection needed between the IDS and the pool node in this figure.

As described above some embodiments implement one or more different middleboxes in a distributed fashion with middlebox elements operating in some or all of the host machines on which the virtual machines and managed switching elements of a logical network are located as compared to the centralized middlebox implementation of some embodiments. This section describes the distributed middlebox implementation of some embodiments within a host machine.

In this example the middlebox element includes three components on the host machine a middlebox daemon that runs in the user space of the host machine and a middlebox kernel module that runs in the kernel of the host machine . While this figure illustrates the distributed middlebox element as two components for the purpose of explanation the middlebox daemon and the middlebox kernel module collectively form the middlebox element running on the host machine . The software switching element an open virtual switch OVS in this example includes three components an OVS kernel module that runs in the kernel of the host machine and an OVS daemon and an OVS database DB daemon which both run in the user space of the host machine.

As illustrated in the host includes hardware kernel user space and VMs . The hardware may include typical computer hardware such as processing units volatile memory e.g. random access memory RAM non volatile memory e.g. hard disk drives flash memory optical discs etc. network adapters video adapters or any other type of computer hardware. As shown the hardware includes NICs and which in some embodiments are typical network interface controllers for connecting a computing device to a network.

As shown in the host machine includes a kernel and a user space . In some embodiments the kernel is the most basic component of an operating system that runs on a separate memory space and is responsible for managing system resources e.g. communication between hardware and software resources . In contrast the user space is a memory space where all user mode applications may run.

The kernel of some embodiments is a software abstraction layer that runs on top of the hardware and runs below any operating system. In some embodiments the kernel performs virtualization functionalities e.g. to virtualize the hardware for several virtual machines operating on the host machine . The kernel is then part of a hypervisor in some embodiments. The kernel handles various management tasks such as memory management processor scheduling or any other operations for controlling the execution of the VMs and operating on the host machine.

As shown the kernel includes device drivers and for the NICs and respectively. The device drivers and allow an operating system e.g. of a virtual machine to interact with the hardware of the host . In this example the device driver allows interaction with the NIC while the driver allows interaction with the NIC . The kernel may include other device drivers not shown for allowing the virtual machines to interact with other hardware not shown in the host .

The virtual machines and are independent virtual machines running on the host machine e.g. user virtual machines such as those shown in using resources virtualized by the kernel . As such the VMs run any number of different operating systems. Examples of such operations systems include Solaris FreeBSD or any other type of Unix based operating system. Other examples include Windows based operating systems as well.

As shown the user space of the host machine includes the middlebox daemon the OVS daemon and the OVS DB daemon . Other applications not shown may be included in the user space as well including daemons for additional distributed middleboxes e.g. firewalls load balancers network address translators etc. . The OVS daemon is an application that runs in the user space . Some embodiments of the OVS daemon communicate with a network controller in order to receive instructions as described in further detail below for processing and forwarding packets sent to and from the virtual machines and . The OVS daemon of some embodiments communicates with the network controller through the OpenFlow protocol while other embodiments use different communication protocols for transferring the physical control plane data. Additionally in some embodiments the OVS daemon retrieves configuration information from the OVS DB daemon after the network controller transmits the configuration information to the OVS DB daemon.

In some embodiments the OVS DB daemon also runs in the user space . The OVS DB daemon of some embodiments communicates with the network controller in order to configure the OVS switching element e.g. the OVS daemon and or the OVS kernel module . For instance the OVS DB daemon receives configuration information from the network controller and stores the configuration information in a set of databases. In some embodiments the OVS DB daemon communicates with the network controller through a database communication protocol. In some cases the OVS DB daemon may receive requests for configuration information from the OVS daemon . The OVS DB daemon in these cases retrieves the requested configuration information e.g. from a set of databases and sends the configuration information to the OVS daemon .

The OVS daemon includes an OpenFlow protocol module and a flow processor . The OpenFlow protocol module communicates with the network controller to receive configuration information e.g. flow entries from the network controller for configuring the software switching element. When the module receives configuration information from the network controller it translates the configuration information into information understandable by the flow processor .

The flow processor manages the rules for processing and routing packets. For instance the flow processor stores rules e.g. in a storage medium such as a disk drive received from the OpenFlow protocol module . In some embodiments the rules are stored as a set of flow tables that each includes a set of flow entries. The flow processor handles packets for which integration bridge described below does not have a matching rule. In such cases the flow processor matches the packets against its stored rules. When a packet matches a rule the flow processor sends the matched rule and the packet to the integration bridge for the integration bridge to process. This way when the integration bridge receives a similar packet that matches the generated rule the packet will be matched against the generated exact match rule in the integration bridge and the flow processor will not have to process the packet.

In some embodiments the flow processor may not have a rule to which the packet matches. In such cases some embodiments of the flow processor send the packet to another managed switching element e.g. a pool node for handling packets that cannot be processed by an edge switching element. However in other cases the flow processor may have received from the network controller a catchall rule that drops the packet when a rule to which the packet matches does not exist in the flow processor .

As illustrated in the kernel includes a hypervisor network stack and an OVS kernel module . The hypervisor network stack is an Internet Protocol IP network stack in some embodiments. The hypervisor network stack processes and routes IP packets that are received from the OVS kernel module and the PIF bridges and . When processing a packet that is destined for a network host external to the host the hypervisor network stack determines to which of the physical interface PIF bridges and the packet should be sent.

The OVS kernel module processes and routes network data e.g. packets between VMs running on the host and network hosts external to the host e.g. network data received through the NICs and . In some embodiments the OVS kernel module implements the forwarding tables of the physical control plane for one or more logical networks. To facilitate the processing and routing of network data the OVS kernel module communicates with OVS daemon e.g. to receive flow entries from the OVS daemon . In some embodiments the OVS kernel module includes a bridge interface not shown that allows the hypervisor network stack to send packets to and receive packets from the OVS kernel module .

The integration bridge processes and routes packets received from the hypervisor network stack the VMs and e.g. through VIFs and the PIF bridges and . In some embodiments the integration bridge stores a subset of the rules stored in the flow processor and or rules derived from rules stored in the flow processor that the integration bridge is currently using or was recently using to process and forward packets.

In some embodiments the flow processor of some embodiments is responsible for managing rules in the integration bridge . In some embodiments the integration bridge stores only active rules. The flow processor monitors the rules stored in the integration bridge and removes the active rules that have not been access for a defined amount of time e.g. 1 second 3 seconds 5 seconds 10 seconds etc. . In this manner the flow processor manages the integration bridge so that the integration bridge stores rules that are being used or have recently been used.

Although illustrates one integration bridge the OVS kernel module may include multiple integration bridges. For instance in some embodiments the OVS kernel module includes a separate integration bridge for each logical switching element that is implemented across a managed network to which the software switching element belongs. That is the OVS kernel module has a corresponding integration bridge for each logical switching element that is implemented across the managed network.

The above description relates to the forwarding functions of the managed software switching element of some embodiments. Just as the software switching element includes a user space component that implements the control plane the OVS daemon and a kernel component that implements the data plane the OVS kernel module the distributed middlebox element of some embodiments includes a control plane component operating in the user space the middlebox daemon and a data plane component operating in the kernel the middlebox kernel module .

As shown the middlebox daemon includes a middlebox configuration receiver and a middlebox configuration compiler . The middlebox configuration receiver communicates with the network controller in order to receive the configuration for the middlebox as well as slicing information. The middlebox configuration in some embodiments is a set of records e.g. in the same form as flow entry records received by the OVS daemon describing the middlebox packet processing rules. For example a firewall configuration includes a set of packet processing rules describing when to drop packets allow packets etc. similar to ACL entries but also including TCP connection state as a factor in the decisions . A source network address translation configuration includes a set of hidden IP addresses of virtual machines that should be mapped into an exposed IP address by the translator. A load balancer configuration in some embodiments includes the network address translation mapping of an exposed IP address into several different hidden virtual machine addresses as well as a load balancing scheduling algorithm for determining to which of several machines new TCP connections should be sent.

As described above the slicing information assigns an identifier to a particular middlebox instance to be performed by the distributed middlebox element. In some embodiments the identifier is bound to a particular logical middlebox in a particular tenant s logical network. That is when a particular logical network includes several different middleboxes with different processing rules the middlebox daemon will create several middlebox instances. Each of these instances is identified with a different slice identifier on packets sent to the middlebox. In addition in some embodiments the middlebox daemon assigns a particular internal identifier for each of these instances which the middlebox uses in its internal processing e.g. in order to keep track of active TCP connections .

The middlebox daemon also includes a middlebox configuration compiler . In some embodiments the middlebox configuration compiler receives the middlebox configuration e.g. the packet processing modification or analysis rules for a particular middlebox instance in a first language and compiles these into a set of rules in a second language more optimized for the internal processing of the middlebox. The middlebox configuration compiler sends the compiled packet processing rules to the middlebox processor of the middlebox kernel module .

The middlebox kernel module processes packets sent from and or to VMs running on the host in order to determine whether to allow the packets through drop the packets etc. As shown the middlebox kernel module includes a middlebox processor to perform these functions. The middlebox processor receives translated middlebox rules for a particular middlebox instance from the middlebox configuration compiler . In some embodiments these translated middlebox rules specify a packet processing pipeline within the middlebox.

In order to receive packets from the managed switching element the middlebox processor of some embodiments connects to a software port abstraction on the integration bridge of the OVS kernel module. Through this port on the integration bridge the managed switching element sends packets to the middlebox and receives packets from the middlebox after processing by the middlebox unless the middlebox drops the packet . As described these packets include a slice identifier tag used by the middlebox processor to determine which set of compiled packet processing rules to apply to the packet.

The architectural diagram of the distributed middlebox and software switching element illustrated in is one exemplary configuration. One of ordinary skill in the art will recognize that other configurations are possible. For instance in some embodiments the middlebox processor that applies the compiled packet processing rules is located in the user space rather than the kernel . In such embodiments the kernel exposes the network interfaces and for full control by the user space so that the middlebox processor can perform its functions in the user space without a loss of speed as compared to the kernel.

Section I above described the different middlebox implementation architectures from fully distributed to fully centralized. As mentioned in some embodiments these middleboxes may be provisioned through a network control system that is also used to provision the managed switching elements that implement the logical forwarding elements of the network. In some embodiments the network control system is a hierarchical set of network controllers.

In some embodiments each of the controllers in a network control system has the capability to function as an input translation controller logical controller and or physical controller. Alternatively in some embodiments a given controller may only have the functionality to operate as a particular one of the types of controller e.g. as a physical controller . In addition different combinations of controllers may run in the same physical machine. For instance the input translation controller and the logical controller may run in the same computing device with which a user interacts.

Furthermore each of the controllers illustrated in and subsequent is shown as a single controller. However each of these controllers may actually be a controller cluster that operates in a distributed fashion to perform the processing of a logical controller physical controller or input translation controller.

The input translation controller of some embodiments includes an input translation application that translates network configuration information received from a user. For example a user may specify a network topology such as that shown in which includes a specification as to which machines belong in which logical domain. This effectively specifies a logical data path set or a set of logical forwarding elements. For each of the logical switches the user specifies the machines that connect to the logical switch i.e. to which logical ports are assigned for the logical switch . In some embodiments the user also specifies IP addresses for the machines. The input translation controller translates the entered network topology into logical control plane data that describes the network topology. For example an entry might state that a particular MAC address A is located at a particular logical port X of a particular logical switch.

In some embodiments each logical network is governed by a particular logical controller e.g. logical controller . The logical controller of some embodiments translates the logical control plane data into logical forwarding plane data and the logical forwarding plane data into universal control plane data. Logical forwarding plane data in some embodiments consists of flow entries described at a logical level. For the MAC address A at logical port X logical forwarding plane data might include a flow entry specifying that if the destination of a packet matches MAC A forward the packet to port X.

The universal physical control plane data of some embodiments is a data plane that enables the control system of some embodiments to scale even when it contains a large number of managed switching elements e.g. thousands to implement a logical data path set. The universal physical control plane abstracts common characteristics of different managed switching elements in order to express physical control plane data without considering differences in the managed switching elements and or location specifics of the managed switching elements.

As stated the logical controller of some embodiments translates logical control plane data into logical forwarding plane data e.g. logical flow entries then translates the logical forwarding plane data into universal control plane data. In some embodiments the logical controller application stack includes a control application for performing the first translation and a virtualization application for performing the second translation. Both of these applications in some embodiments use a rules engine for mapping a first set of tables into a second set of tables. That is the different data planes are represented as tables e.g. nLog tables and the controller applications use a table mapping engine to translate between the data planes.

Each of the physical controllers and is a master of one or more managed switching elements e.g. located within host machines . In this example each of the two physical controllers is a master of two managed switching elements. Furthermore the physical controller is the master of the centralized middlebox . In some embodiments a physical controller receives the universal physical control plane information for a logical network and translates this data into customized physical control plane information for the particular managed switches that the physical controller manages. In other embodiments the physical controller passes the appropriate universal physical control plane data to the managed switch which includes the ability e.g. in the form of a chassis controller running on the host machine to perform the conversion itself.

The universal physical control plane to customized physical control plane translation involves a customization of various data in the flow entries. For the example noted above the universal physical control plane would involve several flow entries. The first entry states that if a packet matches the particular logical data path set e.g. based on the packet being received at a particular logical ingress port and the destination address matches MAC A then forward the packet to logical port X. This flow entry will be the same in the universal and customized physical control planes in some embodiments. Additional flows are generated to match a physical ingress port e.g. a virtual interface of the host machine to the logical ingress port X for packets received from MAC A as well as to match logical port X to the particular egress port of the physical managed switch. However these physical ingress and egress ports are specific to the host machine containing the managed switching element. As such the universal physical control plane entries include abstract physical ports while the customized physical control plane entries include the actual physical ports involved.

In some embodiments the network control system also disseminates data relating to the middleboxes of a logical network. The network control system may disseminate middlebox configuration data as well as data relating to the sending and receiving of packets to from the middleboxes at the managed switches and to from the managed switches at the middleboxes.

As shown in the same network control system distributes data to both distributed and centralized middleboxes in some embodiments. Several physical controllers are used to disseminate the configuration of a distributed middlebox whereas some embodiments assign a specific physical controller to a centralized middlebox appliance. In this case the physical controller is assigned to disseminate the configuration of the centralized middlebox while the configuration for the distributed middlebox is disseminated through both of the physical controllers and .

In order to incorporate the middleboxes the flow entries propagated through the network control system to the managed switches will include entries for sending the appropriate packets to the appropriate middleboxes e.g. flow entries that specify for packets having a source IP address in a particular subnet to be forwarded to a particular middlebox . In addition the flow entries for the managed switch will need to specify how to send such packets to the middleboxes. That is once a first entry specifies a logical egress port of the logical router to which a particular middlebox is bound additional entries are required to attach the logical egress port to the middlebox.

For the centralized middlebox these additional entries will match the logical egress port of the logical router to a particular physical port of the host machine e.g. a physical network interface through which the host machine connects to the middlebox. In addition the entries include encapsulation information for sending the packet to the centralized middlebox appliance via a tunnel between the host machine and the middlebox.

For the distributed middlebox the packet does not have to actually leave the host machine in order to reach the middlebox. However the managed switching element nevertheless needs to include flow entries for sending the packet to the middlebox element on the host machine. These flow entries again include an entry to map the logical egress port of the logical router to the port through which the managed switching element connects to the middlebox. However in this case the middlebox attaches to a software abstraction of a port in the managed switching element rather than a physical or virtual interface of the host machine. That is a port is created within the managed switching element to which the middlebox element attaches. The flow entries in the managed switching element send packets to this port in order for the packets to be routed within the host machine to the middlebox.

For both the distributed and centralized middleboxes in some embodiments the managed switching element adds slicing information to the packet. Essentially this slicing information is a tag that indicates to which of the potentially several instances being run by the middlebox the packet should be sent. Thus when the middlebox receives the packet the tag enables the middlebox to use the appropriate set of packet processing analysis modification etc. rules in order to perform its operations on the packet. Some embodiments rather than adding slicing information to the packet either define different ports of the managed switching element for each middlebox instance and essentially use the ports to slice the traffic destined for the firewall in the distributed case or connect to different ports of the centralized appliance to differentiate between the instances in the centralized case .

The above describes the propagation of the forwarding data to the managed switching elements. In addition some embodiments use the network control system to propagate configuration data to the middleboxes. conceptually illustrates the propagation of data through the network control system of some embodiments. On the left side of the figure is the data flow to the managed switching elements that implement a logical network while the right side of the figure shows the propagation of both middlebox configuration data as well as network attachment and slicing data to the middleboxes.

On the left side the input translation controller receives a network configuration through an API which is converted into logical control plane data. This network configuration data includes a logical topology such as that shown in . In addition the network configuration data of some embodiments includes routing policies that specify which packets are sent to the middlebox. When the middlebox is located on a logical wire between two logical forwarding elements e.g. between a logical router and a logical switch then all packets sent over that logical wire will automatically be forwarded to the middlebox. However for an out of band middlebox such as that in network architecture the logical router will only send packets to the middlebox when particular policies are specified by the user.

Whereas routers and switches will normally forward packets according to the destination address e.g. MAC address or IP address of the packet policy routing allows forwarding decisions to be made based on other information stored by the packet e.g. source addresses a combination of source and destination addresses etc. . For example the user might specify that all packets with source IP addresses in a particular subnet or that have destination IP addresses not matching a particular set of subnets should be forwarded to the middlebox.

As shown the logical control plane data is converted by the logical controller specifically by the control application of the logical controller to logical forwarding plane data and then subsequently by the virtualization application of the logical controller to universal physical control plane data. In some embodiments these conversions generate a flow entry at the logical forwarding plane then adds a match over the logical data path set at the universal physical control plane . The universal physical control plane also includes additional flow entries for mapping generic physical ingress ports i.e. a generic abstraction of a port not specific to any particular physical host machine to logical ingress ports as well as for mapping logical egress ports to generic physical egress ports. For instance for the mapping to a centralized middlebox the flow entries at the universal physical control plane would include a forwarding decision to send a packet to the logical port to which the middlebox connects when a routing policy is matched as well as a mapping of the logical port to a generic physical port of a host machine that connects to the middlebox.

The physical controller one of the several physical controllers as shown translates the universal physical control plane data into customized physical control plane data for the particular managed switching elements that it manages. This conversion involves substituting specific data e.g. specific physical ports for the generic abstractions in the universal physical control plane data. For instance in the example of the above paragraph the port integration entries are configured to specify the physical layer port appropriate for the particular middlebox configuration. This port might be a virtual NIC if the firewall runs as a virtual machine on the host machine or the previously described software port abstraction within the managed switching element when the firewall runs as a process e.g. daemon within the hypervisor on the virtual machine. In some embodiments for the latter situation the port is an IPC channel or TUN TAP device like interface. In some embodiments the managed switching element includes one specific port abstraction for the firewall module and sends this information to the physical controller in order for the physical controller to customize the physical control plane flows. For the flow entries to send packets to the centralized middlebox on the other hand the inserted port will be an actual physical port of the particular host machine on which the managed switching element operates.

In addition in some embodiments the physical controller adds flow entries specifying slicing information particular to the middlebox. For instance for a particular managed switching element the flow entry may specify to add a particular tag e.g. a VLAN tag or similar tag to a packet before sending the packet to the particular firewall. This slicing information enables the middlebox to receive the packet and identify which of its several independent instances should process the packet.

The managed switching element one of several MSEs managed by the physical controller performs a translation of the customized physical control plane data into physical forwarding plane data. The physical forwarding plane data in some embodiments are the flow entries stored within a switching element either a physical router or switch or a software switching element against which the switching element actually matches received packets.

The right side of illustrates two sets of data propagated to a middleboxes either the centralized or distributed middlebox rather than the managed switching elements. The first of these sets of data is the actual middlebox configuration data that includes various rules specifying the operation of the particular logical middlebox. This data may be received at the input translation controller or a different input interface through an API particular to the middlebox implementation. In some embodiments different middlebox implementations will have different interfaces presented to the user i.e. the user will have to enter information in different formats for different particular middleboxes . As shown the user enters a middlebox configuration which is translated by the middlebox API into middlebox configuration data.

In some embodiments the middlebox configuration data is a set of records with each record specifying a particular rule. These records in some embodiments are in a similar format to the flow entries propagated to the managed switching elements. In fact some embodiments use the same applications on the controllers to propagate the firewall configuration records as for the flow entries and the same table mapping language e.g. nLog for the records.

The middlebox configuration data in some embodiments is not translated by the logical or physical controller while in other embodiments the logical and or physical controller perform at least a minimal translation of the middlebox configuration data records. As many middlebox packet processing modification and analysis rules operate on the IP address or TCP connection state of the packets and the packets sent to the middlebox will have this information exposed i.e. not encapsulated within the logical port information the middlebox configuration does not require translation from logical to physical data planes. Thus the same middlebox configuration data is passed from the input translation controller or other interface to the logical controller to the physical controller .

In some embodiments the logical controller stores a description of the logical network and of the physical implementation of that physical network. The logical controller receives the one or more middlebox configuration records for a distributed middlebox and identifies which of the various nodes i.e. host machines will need to receive the configuration information. In some embodiments the entire middlebox configuration is distributed to middlebox elements at all of the host machines so the logical controller identifies all of the machines on which at least one virtual machine resides whose packets require use of the middlebox. This may be all of the virtual machines in a network e.g. as for the middlebox shown in or a subset of the virtual machines in the network e.g. when a firewall is only applied to traffic of a particular domain within the network . Some embodiments make decisions about which host machines to send the configuration data to on a per record basis. That is each particular rule may apply only to a subset of the virtual machines and only hosts running these virtual machines need to receive the record.

Once the logical controller identifies the particular nodes to receive the records the logical controller identifies the particular physical controllers that manage these particular nodes. As mentioned each host machine has an assigned master physical controller. Thus if the logical controller identifies only first and second hosts as destinations for the configuration data the physical controllers for these hosts will be identified to receive the data from the logical controller and other physical controllers will not receive this data . For a centralized middlebox the logical controller needs only to identify the single physical controller that manages the appliance implementing the middlebox.

In order to supply the middlebox configuration data to the hosts the logical controller of some embodiments pushes the data using an export module that accesses the output of the table mapping engine in the logical controller to the physical controllers. In other embodiments the physical controllers request configuration data e.g. in response to a signal that the configuration data is available from the export module of the logical controller.

The physical controllers pass the data to the middlebox elements on the host machines that they manage much as they pass the physical control plane data. In some embodiments the middlebox configuration and the physical control plane data are sent to the same database running on the host machine and the managed switching element and middlebox module retrieve the appropriate information from the database. Similarly for a centralized middlebox the physical controller passes the middlebox configuration data to the middlebox appliance e.g. to a database for storing configuration data .

In some embodiments the middlebox translates the configuration data. The middlebox configuration data will be received in a particular language to express the packet processing analysis modification etc. rules. The middlebox distributed and or centralized of some embodiments compiles these rules into more optimized packet classification rules. In some embodiments this transformation is similar to the physical control plane to physical forwarding plane data translation. When a packet is received by the middlebox it applies the compiled optimized rules in order to efficiently and quickly perform its operations on the packet.

In addition to the middlebox configuration rules the middlebox modules receive slicing and or attachment information in order to receive packets from and send packets to the managed switching elements. This information corresponds to the information sent to the managed switching elements. As shown in some embodiments the physical controller generates the slicing and or attachment information for the middlebox i.e. this information is not generated at the input or logical controller level of the network control system .

For distributed middleboxes the physical controllers in some embodiments receive information about the software port of the managed switching element to which the middlebox connects from the managed switching element itself then passes this information down to the middlebox. In other embodiments however the use of this port is contracted directly between the middlebox module and the managed switching element within the host machine so that the middlebox does not need to receive the attachment information from the physical controller. In some such embodiments the managed switching element nevertheless transmits this information to the physical controller in order for the physical controller to customize the universal physical control plane flow entries for receiving packets from and sending packets to the middlebox.

For centralized middleboxes some embodiments provide tunneling attachment data to the middlebox. The middlebox in some embodiments will need to know the type of tunnel encapsulation various host machines will use to send packets to the middlebox. In some embodiments the middlebox has a list of accepted tunneling protocols e.g. STT GRE etc. and the chosen protocol is coordinated between the managed switching element s and the middlebox. The tunneling protocol may be entered by the user as part of the middlebox configuration or may be automatically determined by the network control system in different embodiments. In addition to the connections to the host machines a tunnel will be set up between the centralized middlebox and the pool node to which it sends packets after processing as described by reference to above.

The slicing information generated by the physical controller in some embodiments consists of an identifier for the middlebox instance to be used for the particular logical network. In some embodiments as described the middlebox whether operating on the host machine or as a centralized appliance is virtualized for use by multiple logical networks. When the middlebox receives a packet from the managed switching element in some embodiments the packet includes a prepended tag e.g. similar to a VLAN tag that identifies a particular one of the middlebox instances i.e. a particular configured set of rules to use in processing the packet.

As shown in the middlebox translates this slicing information into an internal slice binding. In some embodiments the middlebox uses its own internal identifiers different from the tags prepended to the packets in order to identify states e.g. active TCP connections statistics about various IP addresses etc. within the middlebox. Upon receiving an instruction to create a new middlebox instance and an external identifier that used on the packets for the new instance some embodiments automatically create the new middlebox instance and assign the instance an internal identifier. In addition the middlebox stores a binding for the instance that maps the external slice identifier to the internal slice identifier.

The above figures illustrate various physical and logical network controllers. illustrates example architecture of a network controller e.g. a logical controller or a physical controller . The network controller of some embodiments uses a table mapping engine to map data from an input set of tables to data in an output set of tables. The input set of tables in a controller include logical control plane LCP data to be mapped to logical forwarding plane LFP data LFP data to be mapped to universal physical control plane UPCP data and or UPCP data to be mapped to customized physical control plane CPCP data. The input set of tables may also include middlebox configuration data to be sent to another controller and or a distributed middlebox instance. The network controller as shown includes input tables a rules engine output tables an importer an exporter a translator and a persistent data storage PTD .

In some embodiments the input tables include tables with different types of data depending on the role of the controller in the network control system. For instance when the controller functions as a logical controller for a user s logical forwarding elements the input tables include LCP data and LFP data for the logical forwarding elements. When the controller functions as a physical controller the input tables include LFP data. The input tables also include middlebox configuration data received from the user or another controller. The middlebox configuration data is associated with a logical datapath set parameter that identifies the logical switching elements to which the middlebox to be is integrated.

In addition to the input tables the control application includes other miscellaneous tables not shown that the rules engine uses to gather inputs for its table mapping operations. These miscellaneous tables include constant tables that store defined values for constants that the rules engine needs to perform its table mapping operations e.g. the value 0 a dispatch port number for resubmits etc. . The miscellaneous tables further include function tables that store functions that the rules engine uses to calculate values to populate the output tables .

The rules engine performs table mapping operations that specifies one manner for converting input data to output data. Whenever one of the input tables is modified referred to as an input table event the rules engine performs a set of table mapping operations that may result in the modification of one or more data tuples in one or more output tables.

In some embodiments the rules engine includes an event processor not shown several query plans not shown and a table processor not shown . Each query plan is a set of rules that specifies a set of join operations that are to be performed upon the occurrence of an input table event. The event processor of the rules engine detects the occurrence of each such event. In some embodiments the event processor registers for callbacks with the input tables for notification of changes to the records in the input tables and detects an input table event by receiving a notification from an input table when one of its records has changed.

In response to a detected input table event the event processor 1 selects an appropriate query plan for the detected table event and 2 directs the table processor to execute the query plan. To execute the query plan the table processor in some embodiments performs the join operations specified by the query plan to produce one or more records that represent one or more sets of data values from one or more input and miscellaneous tables. The table processor of some embodiments then 1 performs a select operation to select a subset of the data values from the record s produced by the join operations and 2 writes the selected subset of data values in one or more output tables .

Some embodiments use a variation of the datalog database language to allow application developers to create the rules engine for the controller and thereby to specify the manner by which the controller maps logical datapath sets to the controlled physical switching infrastructure. This variation of the datalog database language is referred to herein as nLog. Like datalog nLog provides a few declaratory rules and operators that allow a developer to specify different operations that are to be performed upon the occurrence of different events. In some embodiments nLog provides a limited subset of the operators that are provided by datalog in order to increase the operational speed of nLog. For instance in some embodiments nLog only allows the AND operator to be used in any of the declaratory rules.

The declaratory rules and operations that are specified through nLog are then compiled into a much larger set of rules by an nLog compiler. In some embodiments this compiler translates each rule that is meant to address an event into several sets of database join operations. Collectively the larger set of rules forms the table mapping rules engine that is referred to as the nLog engine.

Some embodiments designate the first join operation that is performed by the rules engine for an input event to be based on the logical datapath set parameter. This designation ensures that the rules engine s join operations fail and terminate immediately when the rules engine has started a set of join operations that relate to a logical datapath set i.e. to a logical network that is not managed by the controller.

Like the input tables the output tables include tables with different types of data depending on the role of the controller . When the controller functions as a logical controller the output tables include LFP data and UPCP data for the logical switching elements. When the controller functions as a physical controller the output tables include CPCP data. Like the input tables the output tables may also include the middlebox configuration data. Furthermore the output tables may include a slice identifier when the controller functions as a physical controller.

In some embodiments the output tables can be grouped into several different categories. For instance in some embodiments the output tables can be rules engine RE input tables and or RE output tables. An output table is a RE input table when a change in the output table causes the rules engine to detect an input event that requires the execution of a query plan. An output table can also be an RE input table that generates an event that causes the rules engine to perform another query plan. An output table is a RE output table when a change in the output table causes the exporter to export the change to another controller or a MSE. An output table can be an RE input table a RE output table or both an RE input table and a RE output table.

The exporter detects changes to the RE output tables of the output tables . In some embodiments the exporter registers for callbacks with the RE output tables for notification of changes to the records of the RE output tables. In such embodiments the exporter detects an output table event when it receives notification from a RE output table that one of its records has changed.

In response to a detected output table event the exporter takes each modified data tuple in the modified RE output tables and propagates this modified data tuple to one or more other controllers or to one or more MSEs. When sending the output table records to another controller the exporter in some embodiments uses a single channel of communication e.g. a RPC channel to send the data contained in the records. When sending the RE output table records to MSEs the exporter in some embodiments uses two channels. One channel is established using a switch control protocol e.g. OpenFlow for writing flow entries in the control plane of the MSE. The other channel is established using a database communication protocol e.g. JSON to send configuration data e.g. port configuration tunnel information .

In some embodiments the controller does not keep in the output tables the data for logical datapath sets that the controller is not responsible for managing i.e. for logical networks managed by other logical controllers . However such data is translated by the translator into a format that can be stored in the PTD and is then stored in the PTD. The PTD propagates this data to PTDs of one or more other controllers so that those other controllers that are responsible for managing the logical datapath sets can process the data.

In some embodiments the controller also brings the data stored in the output tables to the PTD for resiliency of the data. Therefore in these embodiments a PTD of a controller has all the configuration data for all logical datapath sets managed by the network control system. That is each PTD contains the global view of the configuration of the logical networks of all users.

The importer interfaces with a number of different sources of input data and uses the input data to modify or create the input tables . The importer of some embodiments receives the input data from another controller. The importer also interfaces with the PTD so that data received through the PTD from other controller instances can be translated and used as input data to modify or create the input tables . Moreover the importer also detects changes with the RE input tables in the output tables .

The above describes various principles for implementing and configuring both distributed and centralized middleboxes. The example network shown in above is a simplified example with only a single middlebox. on the other hand conceptually illustrates a more complex logical network topology involving numerous middleboxes.

The logical network includes three logical L2 domains web servers connected to a first logical L2 switch application servers and connected to a second logical L2 switch and data servers and connected to a third logical switch . Each of these logical switches connects to a logical router through various middleboxes .

Between each logical switch and the logical router is a load balancer in order for the load balancer to schedule incoming traffic to the particular logical L2 domain. That is the first load balancer performs destination network address translation D NAT to balance traffic e.g. on a per transport connection basis between the three web servers the second load balancer performs D NAT to balance traffic between the two application servers and and the third load balancer performs D NAT to balance traffic between the two data servers and . In addition between the logical router and the second logical switch is a firewall .

The logical router also connects the three logical L2 domains to an external network from which client requests may come into the network. In addition three middleboxes hang off of the L3 router for processing traffic between the managed network and the external network. These middleboxes include a firewall for processing incoming traffic and a source NAT for converting the real IP addresses of outgoing traffic into one or more virtual IP addresses. These middleboxes are effectively located between the managed network and the external network however because the physical implementation involves sending a packet to the middlebox and then receiving a packet back from the middlebox either to send to the external network or the appropriate host machine the logical topology illustrates these middleboxes as out of band middleboxes hanging off the router. Finally an IDS also hangs off of the logical router . In the network the logical router forwards a duplicate of all processed packets to the IDS for analysis.

As shown each host machine as well as the pool node and the gateway includes a managed switching element. All of the host machines are configured to include the flow entries for the logical router as well as the logical switches . Thus the second host machine that includes an application server as well as a web server and the fifth host machine that only includes a data server implement the same managed switching elements. The managed switching element in the gateway implements the logical router as well as all three of the logical switches of the logical network . The pool node in some embodiments is also a managed switching element that implements the L3 router and all three logical switches .

In the implementation some of the logical middleboxes are distributed while others are centralized. For instance the intrusion detection service is implemented as a centralized IDS appliance . Each of the host machines connects directly to the IDS appliance as does the gateway . As shown these machines only send packets to the IDS appliance and do not receive packets back. This is because the IDS only receives duplicate packets incoming packets from the gateway and outgoing packets from the host machines and performs analysis to detect threats but does not send the packets anywhere after analyzing them.

The S NAT middlebox is distributed among each of the host machines e.g. as a daemon running within the hypervisor as all of the virtual machines may send packets to the external network that require an IP address translation to hide the real IPs behind a virtual IP address. The firewall is distributed as well but only implemented on the host machines and because these are the nodes that host the application server virtual machines and that are behind this firewall.

The three load balancers are implemented across the various host machines as well as the gateway . As shown the load balancers are implemented within a load balancer element in each of the host machines such that the load balancer element is virtualized i.e. sliced to implement several different load balancers. Whereas the firewall is located at the host machines where the application servers are each implemented each particular logical load balancer is located on each node that hosts a machine for which the particular load balancer is not responsible. This is because for example the load balancer receives any packet destined for the virtual IP address representing the data servers and determines to which of the two data servers to forward the packet then modifies the destination IP address to reflect the selected data server. As the processing is performed at the first hop packet source whenever possible this functionality is not needed at the nodes hosting the data servers unless other virtual machines are also hosted but rather at the nodes hosting the other virtual machines that may send packets to the data servers and . Accordingly the load balancer element in the gateway is sliced to implement all three load balancers as incoming packets from the external network may be destined for any of the three logical L2 domains.

In addition as shown some embodiments implement any distributed middleboxes for the logical network within the pool node and the gateway as well. Because it may be difficult to determine at the outset which middleboxes will be needed at which locations and a user may subsequently modify routing policies middlebox configuration or network architecture some embodiments do not presume that certain physical machines will not require a particular middlebox. Along the same lines some embodiments do not distribute different middleboxes to different subsets of the hosts. Instead all middleboxes for the logical network are implemented at each host at which the logical network is present.

The firewall for processing incoming packets between the external network and the managed network is implemented in a centralized fashion in a virtual machine rather than a module running in a hypervisor in the gateway. When the gateway receives incoming packets it automatically routes the packets to the firewall VM in some embodiments. While in this example the firewall VM is located in the gateway some embodiments implement the firewall as a virtual machine in a host machine e.g. a different host machine than those hosting the user VMs or using a firewall appliance.

In order to configure the network in some embodiments a user enters the network topology into a logical controller e.g. via an input translation controller as shown above by reference to . In some embodiments the user enters the connections between the switches routers middleboxes and virtual machines. Based on the locations of the various network components the input translation controller or logical controller generates logical control plane data and converts this into flow entries in the logical forwarding plane. However for middleboxes such as the firewall S NAT and IDS the user also has to enter policy routing rules indicating when to send packets to these components. For instance the routing policy for the firewall would be to send all packets with a source IP outside of the logical network while the routing policy for the S NAT would be to send all packets with a source IP in the logical network. The logical controller after converting the flow entries to the universal physical control plane identifies which physical controllers should receive which flow entries and then distributes these flow entries. The physical controllers add the specific port information unless the host machines include chassis controllers that perform the universal to customized physical control plane translation and other customizations to the flow entries and distribute them to the managed switching elements.

In addition the user enters the middlebox configurations for the various load balancers firewalls etc. For example this information would include the scheduling algorithms to use for each of the different load balancers the virtual IP to real IP mappings for the S NAT packet processing rules for the firewalls etc. The users enter this information through APIs for the various middleboxes and some embodiments convert this information to records having the same format e.g. nLog as the flow entries. The logical controllers identify which middlebox configurations need to be sent to which host machines or centralized middlebox appliances e.g. the records for the firewall only need to go to the host machines and while the S NAT records go to all five host machines . The logical controller distributes the records to the appropriate physical controllers which add slicing information and in some cases tunneling information and distribute the information to the middleboxes as described above.

The operation of the network will be described by reference to packets incoming from the external network packets outgoing to the external network and packets sent from one logical L2 domain to the other. When a packet is sent from a host machine e.g. a web server it first arrives at the MSE running on the host. The packet will first enter logical L2 processing by the logical switch which sends the packet to the logical router because the packet is outgoing it does not need to be sent to the local load balancer . The logical router also handled by the MSE at the host machine sends a duplicate of the packet to the IDS appliance in addition to sending the packet to the S NAT processing on the host. The S NAT processing modifies the source IP address and returns a new packet to the MSE. In some embodiments if the packet is part of an active TCP session the S NAT may have already sent flow entries to the MSE enabling the MSE to perform the modification without the S NAT processing being involved. The logical router implemented by the MSE then identifies the logical egress port as the port facing the external network which maps to the physical port to send the packet to the pool node . The pool node forwards the packet to the gateway which sends the packet out to the external network .

When a packet is received at the gateway from the external network the switching element processing in the gateway first sends the packet to the firewall virtual machine. If the firewall does not drop the packet then the packet is returned to the switching element processing which identifies the correct slice of the load balancer tags the packet with this slice information and sends the packet to the load balancer processing. The load balancer selects a real destination IP and sends a new packet with the IP address modified to reflect the selected destination machine. At this point the MSE in the gateway sends the packet to the correct host. If the destination VM is one of the application servers or the logical router in the MSE of the gateway first sends the packet to the firewall for processing and then sends the packet to the correct host after receiving it back from the firewall element. The MSE then delivers the packet to the destination machine.

Packets traveling from one logical domain to the other will not need to travel through the gateway . The packet is initially received at the MSE which performs L2 switching and then the L3 routing. The logical router identifies the destination domain and tags the packet with the correct load balancer slicing information then sends the tagged packet to the load balancer. The load balancer modifies the destination IP and returns the packet to the MSE which then routes the packet to the correct host machine for delivery to the VM.

Many of the above described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium also referred to as computer readable medium . When these instructions are executed by one or more computational or processing unit s e.g. one or more processors cores of processors or other processing units they cause the processing unit s to perform the actions indicated in the instructions. Examples of computer readable media include but are not limited to CD ROMs flash drives random access memory RAM chips hard drives erasable programmable read only memories EPROMs electrically erasable programmable read only memories EEPROMs etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.

In this specification the term software is meant to include firmware residing in read only memory or applications stored in magnetic storage which can be read into memory for processing by a processor. Also in some embodiments multiple software inventions can be implemented as sub parts of a larger program while remaining distinct software inventions. In some embodiments multiple software inventions can also be implemented as separate programs. Finally any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments the software programs when installed to operate on one or more electronic systems define one or more specific machine implementations that execute and perform the operations of the software programs.

The bus collectively represents all system peripheral and chipset buses that communicatively connect the numerous internal devices of the electronic system . For instance the bus communicatively connects the processing unit s with the read only memory the system memory and the permanent storage device .

From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of the invention. The processing unit s may be a single processor or a multi core processor in different embodiments.

The read only memory ROM stores static data and instructions that are needed by the processing unit s and other modules of the electronic system. The permanent storage device on the other hand is a read and write memory device. This device is a non volatile memory unit that stores instructions and data even when the electronic system is off. Some embodiments of the invention use a mass storage device such as a magnetic or optical disk and its corresponding disk drive as the permanent storage device .

Other embodiments use a removable storage device such as a floppy disk flash memory device etc. and its corresponding drive as the permanent storage device. Like the permanent storage device the system memory is a read and write memory device. However unlike storage device the system memory is a volatile read and write memory such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some embodiments the invention s processes are stored in the system memory the permanent storage device and or the read only memory . From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of some embodiments.

The bus also connects to the input and output devices and . The input devices enable the user to communicate information and select commands to the electronic system. The input devices include alphanumeric keyboards and pointing devices also called cursor control devices cameras e.g. webcams microphones or similar devices for receiving voice commands etc. The output devices display images generated by the electronic system or otherwise output data. The output devices include printers and display devices such as cathode ray tubes CRT or liquid crystal displays LCD as well as speakers or similar audio output devices. Some embodiments include devices such as a touchscreen that function as both input and output devices.

Finally as shown in bus also couples electronic system to a network through a network adapter not shown . In this manner the computer can be a part of a network of computers such as a local area network LAN a wide area network WAN or an Intranet or a network of networks such as the Internet. Any or all components of electronic system may be used in conjunction with the invention.

Some embodiments include electronic components such as microprocessors storage and memory that store computer program instructions in a machine readable or computer readable medium alternatively referred to as computer readable storage media machine readable media or machine readable storage media . Some examples of such computer readable media include RAM ROM read only compact discs CD ROM recordable compact discs CD R rewritable compact discs CD RW read only digital versatile discs e.g. DVD ROM dual layer DVD ROM a variety of recordable rewritable DVDs e.g. DVD RAM DVD RW DVD RW etc. flash memory e.g. SD cards mini SD cards micro SD cards etc. magnetic and or solid state hard drives read only and recordable Blu Ray discs ultra density optical discs any other optical or magnetic media and floppy disks. The computer readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code such as is produced by a compiler and files including higher level code that are executed by a computer an electronic component or a microprocessor using an interpreter.

While the above discussion primarily refers to microprocessor or multi core processors that execute software some embodiments are performed by one or more integrated circuits such as application specific integrated circuits ASICs or field programmable gate arrays FPGAs . In some embodiments such integrated circuits execute instructions that are stored on the circuit itself. In addition some embodiments execute software stored in programmable logic devices PLDs ROM or RAM devices.

As used in this specification and any claims of this application the terms computer server processor and memory all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification the terms display or displaying means displaying on an electronic device. As used in this specification and any claims of this application the terms computer readable medium computer readable media and machine readable medium are entirely restricted to tangible physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals wired download signals and any other ephemeral signals.

While the invention has been described with reference to numerous specific details one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. Thus one of ordinary skill in the art would understand that the invention is not to be limited by the foregoing illustrative details but rather is to be defined by the appended claims.

