---

title: Pull data transfer method in request-response models
abstract: Systems, methods, and products for pull data transfer in a request-response model are provided herein. One aspect provides for generating output data utilizing at least one data generation station; and communicating via the at least one data generation station output data related to at least one data request received from at least one data requesting station responsive to at least one criterion, the at least one criterion comprising one of expiration of a time period or generation of a threshold amount of output data. Other embodiments and aspects are also described herein.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09264506&OS=09264506&RS=09264506
owner: International Business Machines Corporation
number: 09264506
owner_city: Armonk
owner_country: US
publication_date: 20120511
---
This invention was made with Government support under Contract No. H98230 07 C 0383 awarded by Intelligence Agencys. The Government has certain rights to this invention.

The subject matter presented herein generally relates to data transfer methods and push and pull data transfer methods in particular.

Push and pull data transfer methods are two major strategies utilized by data communication systems. In a pull system data transmission is initiated by a data receiver or client essentially forming a request response transmission model. To the contrary a push system data transmission requests are initiated by the data publisher or server wherein the data publisher sends notifications and or data based on data availability. Exemplary applications include an operating system OS where the push model may be characterized as an interrupt driven system and the pull model as a polled input output I O system and a client server system where the pull model may be configured as an HTTP protocol and the push model as an HTTP push protocol.

One aspect provides a system comprising at least one data generation station configured to generate output data wherein the at least one data generation station is configured to communicate output data related to at least one data request received from at least one data requesting station responsive to at least one criterion the at least one criterion comprising one of expiration of a time period or generation of a threshold amount of output data.

Another aspect provides a method comprising generating output data utilizing at least one data generation station and communicating via the at least one data generation station output data related to at least one data request received from at least one data requesting station responsive to at least one criterion the at least one criterion comprising one of expiration of a time period or generation of a threshold amount of output data.

A further aspect provides a computer program product comprising a computer readable storage medium having computer readable program code embodied therewith the computer readable program code comprising computer readable program code configured to generate output data utilizing at least one data generation station and computer readable program code configured to communicate via the at least one data generation station output data related to at least one data request received from at least one data requesting station responsive to at least one criterion the at least one criterion comprising one of expiration of a time period or generation of a threshold amount of output data.

The foregoing is a summary and thus may contain simplifications generalizations and omissions of detail consequently those skilled in the art will appreciate that the summary is illustrative only and is not intended to be in any way limiting.

For a better understanding of the embodiments together with other and further features and advantages thereof reference is made to the following description taken in conjunction with the accompanying drawings. The scope of the invention will be pointed out in the appended claims.

It will be readily understood that the components of the embodiments as generally described and illustrated in the figures herein may be arranged and designed in a wide variety of different configurations in addition to the described example embodiments. Thus the following more detailed description of the example embodiments as represented in the figures is not intended to limit the scope of the claims but is merely representative of those embodiments.

Reference throughout this specification to embodiment s or the like means that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment. Thus appearances of the phrases according to embodiments or an embodiment or the like in various places throughout this specification are not necessarily all referring to the same embodiment.

Furthermore the described features structures or characteristics may be combined in any suitable manner in different embodiments. In the following description numerous specific details are provided to give a thorough understanding of example embodiments. One skilled in the relevant art will recognize however that aspects can be practiced without certain specific details or with other methods components materials et cetera. In other instances well known structures materials or operations are not shown or described in detail to avoid obfuscation.

Most data communication systems utilizing a push or pull strategy may be configured to use the alternative transmission strategy e.g. a pull system may be configured as a push system and vice versa . The pull strategy has been widely accepted as the primary method in most software systems and computer networks because of certain technical difficulties associated with the push strategy such as complications in design and coding additional logistic requirements and security concerns. However the pull strategy inherently suffers from long latency low throughput and high overhead. Therefore in systems requiring high performance it is desirable to have a pull strategy that can achieve similar performance as the push strategy.

The distinction between push and pull strategies originally came from supply chain management. These two strategies and their hybrids have been widely used in many other areas such as marketing and education systems and most prominently computer science and information technology. In computer and network systems these strategies are often used to describe the communication mechanism between a pair of components including a data or service provider and a data or service receiver. With the pull strategy the provider sends data to the receiver only after a request from the receiver arrives at the provider whereas with the push strategy such a request is not required. provides example push and pull strategies involving actual applications in computer and network systems.

The pull strategy is the dominating method in the world of information technology due to certain issues with the push strategy in these operating environments. For example the push strategy appears to be more complicated than the pull method for developers. Modern programmers are accustomed to think in procedure flows more suitable for the pull strategy than in state transition diagrams more suitable for the push strategy . Historically programming languages are often designed to better represent procedure flows and system providing services such as operating systems and web servers are designed to passively accept requests. Consequently using the push strategy often results in longer code than the pulling method for example because developers have to define many call back functions and or use large switch statements for the push strategy. Further in practice with the main stream programming model the push strategy may interrupt the normal processing flow and cause indeterminism and difficult race conditions.

Data providers often require additional resources and can easily overload receivers. With the push strategy both a provider and a receiver have to maintain queues to store data backlogs. In contrast with the pull strategy a single buffer suffices in most cases. With the push strategy if the provider is not clear about its receivers capacity backlogs may accumulate indefinitely. Therefore a rate control mechanism may be necessary to adapt to capacity changes. Such a mechanism may require two way communication between the provider and the receiver significantly complicating the implementation. Even a well designed rate control algorithm can fail in certain unexpected scenarios. The same problem also occurs in supply chains wherein the push strategy responds poorly to a change of demand and such a change sometimes causes a bullwhip effect. 

The push strategy may also not be viable due to logistic or security reasons. Usually a single service is used by a large number of users for example many applications may run on a single operating system and many browsers may connect to a single web server. In order to improve efficiency a service provider does not usually track receiver information details such as data pertaining to user identification location or particular requests. Service providers are often publicly available whereas service receivers tend to maintain anonymity and make requests only when needed. Moreover service receivers may have additional protections such as firewalls making the push strategy difficult or even seemingly impossible to secure.

In addition in many practical scenarios the push strategy may be implemented by or partially depend on the pull strategy in which case the push strategy may not achieve its typical performance advantages. For example Resource Description Framework RDF Site Summary RSS updates and desktop email notifications are generally implemented by periodically polling servers. In graphical user interface GUI programs a main loop repeatedly retrieves events from a queue and dispatches them to callback functions. In an operating system OS interrupt handling code usually has to retrieve actual data from hardware devices by sending I O signals to the devices although the availability of the data may generally be guaranteed. In many client server architectures a client has to register with the server or make a subscription in order to receive pushed or published data.

The main problem with the pull strategy is poor performance including high overhead and long latency. These two aspects of performance frequently conflict with each other. In the case of periodical polling in order to reduce latency smaller intervals may be utilized resulting in increased system overhead. In the extreme cases a busy waiting loop can provide immediate response at the expense of 100 wasted CPU time. Therefore an improved pull strategy may be realized by determining a better trade off between low overhead and short latency for the pull strategy.

Accordingly embodiments provide for improved pull strategy performance facilitated through optimizing overhead and latency trade offs for a dynamically changing workload. Processes for using pull to achieve the efficiency associated with push according to embodiments may be used for data transfers between any two processes for example processes acting as a client and a server. For example embodiments may provide one or more tunable parameters configured to tune the system according to practical performance requirements. According to embodiments a scheduling program may use an interface to dynamically change the one or more tunable parameters on the fly according to statistics collected in real time. As such data providers may ultimately adjust the interval between consecutive pulls according to the actual workload within a pair ranges configured in view of the one or more tunable parameters. Although a server client environment is used prominently herein embodiments are not so limited as embodiments may be generalized for transferring any data or goods between any two stations e.g. a generating station and a receiving station .

In a typical implementation a data provider transfers data continuously to a data receiver. In reality for the provider data arrives in pieces or is generated at discrete times. The elapsed time between two consecutive data arrivals may be referred to as the inter arrival time. In other words the amount of available data may be a monotonic right continuous function of time increasing only at discrete times such as the exemplary data availability function t provided in . However if the data pieces are small enough and inter arrival time is short enough continuous data availability functions may serve as approximations e.g. t and t in . In therein is provided data availability functions and response delay schemes using a combination of buffer size and a timeout. The responses indicate when a response was sent and how much data was in the response for the two functions respectively.

Data providers may differentially respond to data requests for example a provider does not have to respond to the receiver immediately after a request but instead may choose to delay the response allowing more data to be accumulated. The longer the delay the more data the provider can provide each time in the response. The amount of available data that the provider may transfer after a delay of time t may be determined by the value of the data availability function t illustrated in .

At least two common mechanisms may be used to improve performance. In a first mechanism i.e. as fast as possible a provider may transfer data in batches by responding only after a buffer is filled. Processing data in batches may reduce the cost due to the number of invocations of processing a component and therefore may ultimately reduce the overhead. In a second mechanism i.e. as complete as possible the provider may respond when a fixed delay time has been reached. In this second mechanism it may be guaranteed that the latency is bounded by the delay time plus the additional communication and processing delay if any . These two mechanisms may be mixed together the response is transferred either when the buffer is filled t in or when a timeout is reached t in . For t the provider may choose to delay the response even though it has some initial data available when the request is received.

In certain data transfer environments processing overhead may be reduced by utilizing a large buffer size. If the data arrival rate is very high the buffer may be filled in a short time and as a result latency may be low. If the data arrival rate is very low a large timeout may be required comparable to the inter arrival time otherwise the provider is going to transfer empty responses or responses containing very little data wasting computer and or network resources. In the case where the data arrival rate is very low although latency may be high it is usually acceptable because it is at the same magnitude as the inter arrival time. In a system with data rates varying over time a long latency at the time of low data rates may contribute much less to the average latency than the latency at the time of high data rates for example because the latency is usually weighed by the amount of data and the amount of data in a unit time during low rate periods is much lower. Therefore the simple double bound method shown in may apply for both of these extreme cases.

However moderate data rates may likely produce different data transfer environments wherein the methods shown in are ineffective. In a moderate data rate environment it may be desirable to response either before the buffer is filled up or a timeout has been reached otherwise the latency may be much longer compared to the inter arrival time severely effecting performance.

As depicted in when both overhead and delay are considered in general the optimal batch size may be an increasing function of the data rate the optimal response delay t may be a decreasing function of the data rate and the optimal batch size may be an decreasing function of the optimal response delay t . With unknown or variable data rate the optimal batch size and response delay may be the cross points of data availability function and .

Referring to therein is provided an example data availability function configured according to an embodiment. As shown in an approximation line may be used to approximate the optimal batch function t . A double range process may be provided according to embodiments that utilizes the following two ranges for batch size and t t for the response delay. Embodiments provide that a response may be made when any of the following are true 1 t t wherein the response delay reaches the maximum delay 2 wherein the data size reaches the maximum batch size and 3 t t and wherein the response delay reaches the minimum delay and the data size reaches the minimum batch size.

The double range process arranged according to embodiments may be applied to both pull and push data transfer environments. For push environments the process may be used for push mechanism alone for better approximations of optimal batch size. In pull environments the double range process may result in even better improvement for continuous pull mechanism since the overhead of each request response cycle is high. The double range process may be configured to be a special case of the as fast as possible and the as complete as possible mechanisms described hereinabove. For example the as fast as possible mechanism may be configured as the case where 1 buffer size and t 0 and t timeout. In another example the as complete as possible mechanism may be configured as the case where buffer size and t t timeout.

Referring to therein is provided an example double range process configured according to an embodiment. The double range process may be configured to include a buffer size i.e. the maximum amount of data allowed in each response a timeout i.e. the maximum delay time a minimum amount of data and a minimum delay. Accordingly the double range method may operates to provide the following 

As shown in t has a high rate t has a low rate and t and t have moderate rates. For moderate rates the buffer may not be filled within t and the timeout may not be reached before at least amount of data is available. If the data rate is moderate in general users only need to know the fact that the actual delay and amount of data are within the range t t and range respectively as shown by bounded area in . According to embodiments the double range process may provide some degree of freedom regarding choosing a scheme for the actual delay time t and the amount of data when a response is sent. Embodiments may provide for a decreasing function t depicted as t in wherein t and t and the response may be sent whenever t crosses t . The t function may be referred to as the scheme function of the double range process.

Embodiments provide for a double range process that may be differentially configured to handle different data rates. The following is a list of possible process configurations for moderate data rates 

In practice most applications come with their own natural ranges for amount of data and delay. Embodiments provide that the maximum amount of data may be given by the amount of allocated memory provided by a service provider and the minimum amount of data may be given by the size of an atomic datum for example a single data point or even a single byte. The minimum delay tmay be given by the time used by the service provider to process a request and prepare a response and the maximum delay tmay be given by a specified system or application timeout e.g. HTTP request timeout .

According to embodiments the optimal t may be approximated utilizing one or more functions. In a first non limiting example the optimal t may be approximated with t wherein the provider sends a response as soon as the following condition is met or t tand or t t. provides another non limiting example of an optimal t approximation according to an embodiment. In the optimal t may be approximated utilizing t and t . Ranges utilized in the optimal approximation depicted in may involve larger and tand smaller and t inter alia to achieve a more precise approximation. As shown in t and t may be used to approximate optimal t functions .

The double range method provided according to embodiments described herein may be utilized within various operating environments including software programming information technology and any process involving transferring any data or goods between any two stations. An illustrative and non restrictive first use case example involves an optimized pull programming model for input and output I O operations. In particular the first use case example involves I O operations in a LINUX environment described via input operations but which may be equally applied to output operations. LINUX is a registered trademark of Linus Torvalds.

On a Linux system an input stream may be read with the following system call ssize t read int fd void buf size t length where fd is a UNIX descriptor for either a file or a stream source such as a TCP socket. UNIX is a registered trademark of The Open Group. The parameter buf is a pre allocated buffer with a size of length. There are two modes under which this system call may behave differently the blocking mode and the non blocking mode. Under the blocking mode i.e. synchronous I O this system call may be blocked until at least one byte is read into the buffer until end of file is reached or until some I O error is detected. Under the non blocking mode e.g. asynchronous I O this function may return immediately and if no data is available in the buffer this function returns a code of EAGAIN or EWOULDBLOCK to ask the caller to invoke it again at a later time.

The blocking and non blocking modes may be unified with the double range method configured according to embodiments. The first use case may utilize the following function int dr read int fd void buf size t min len size t max len uint64 t min delay uint64 t max delay where min len and max len are the minimum and maximum numbers of bytes to read and min delay and max delay are the minimum and maximum of delay. The blocking and non blocking read system calls may be implemented according to the following dr read fd buf 1 length 0 UINT64 MAX and dr read fd buf 0 length 0 0 respectively where UINT64 MAX equals 2 1. This configuration of dr read may be utilized in other scenarios. For example to fill an entire buffer with a size of 1024 dr read may be invoked with dr read fd buf 1024 1024 0 UINT64 MAX . If the standard read system call is used instead a call read would have to loop repeatedly until the entire buffer is filled. In another example to read at least one byte into a buffer in blocking mode while performing another function wherein no data is available for a certain period of time e.g. 5 seconds dr read fd buf 1 length 0 5000 may be used with an assumed delay time represented in milliseconds. Without utilizing this configuration of the dr read function system calls to select within LINUX or multiple threads would be required resulting in much more complicated code in both cases.

Referring to therein is provided an example implementation wherein the function dr read is configured to handle multiple input streams utilizing existing non blocking read and select LINUX system calls. In the implementation shown in it is assumed that file descriptor fd is in non blocking mode. In addition the function dr read may be implemented within the kernel of an operating system and provided as a system call. The dr read function as described herein may operate to simplify the code for many scenarios for example a Java implementation of dr read may function replace Java new I O NIO application programming interfaces APIs . The dr read multiple function provided in may return if the maximum length of any of the input streams has been reached if any of the maximum lengths has been reached or if both the minimum delay and at least one of the minimum lengths has been reached.

An illustrative and non restrictive second use case example involves Hypertext Transfer Protocol HTTP a pull based request response protocol. A web based visualization frame work provides a way to visualize real time streaming data in web browsers for example web browsers based on Asynchronous JavaScript and XML Ajax . After web pages are loaded in browsers sequential XMLHttpRequest calls are issued to retrieve streaming data from a server. In the framework configured for the second user case data are sent incrementally in order to save bandwidth such that data appearing in previous responses may not be received again. Instead of sending responses immediately after receiving an HTTP request the server may wait for a certain period of time e.g. a few seconds depending on the availability of new data. This approach may be referred to as Ajax push or Ajax long polling. Client code may then issue the next XMLHttpRequest calls right after receiving and processing the response of the previous call and the frequency of the XMLHttpRequest calls may be entirely controlled by the server.

To determine the time to wait the double range method configured according to embodiments may be utilized with the scheme x by default. In addition a fairly large max delay 10 seconds but a small min delay less than 1 second may be utilized. The max delay value may be set even larger but it must be less than any timeout that a web browser may use. The values of min len and max len may depend on the application. For plotting a time series of data points an example default setting may be configured to utilize one data point for min len and setting max len to be the same as the number of points that may show simultaneously in the plot. However according to embodiments both ranges may be configurable in our framework. With the embodiment described in the second use case the latency of data may be reduced by the small min delay whereas the responsiveness of the browser may be significantly improved by the lower overhead associated with communication and processing functions.

In brief recapitulation embodiments described herein are directed toward processes for using pull to achieve the efficiency of push for data transfers between two processes acting as a client and a server. However embodiments are not limited to client and server processes as they may be generalized for transferring any manufactured goods or information products between two stations. In a certain embodiment data is transferred from a web application deployed in a web server to a web browser client for example as a web browser based visualization of a continuous stream of time series data such as moving stock price plots. In addition a server configured according to embodiments may support multiple clients transferring the same or different data streams thereto.

Referring now to therein is provided a server client pull framework configured according to embodiments. A server generates output over time for example in a manner wherein the size of the output increases over time. A request for new data is submitted to the server by one or more clients . The server determines whether to communicate data related to the new data request based on active criteria including whether to delay the response until more data arrives e.g. based on a specified threshold a specific wait time has been reached or some combination thereof. When the active criteria have been met the server may communicate the data to the one or more requesting clients . The response delay decision may be based on one or more parameters including but not limited to minimum delay maximum delay minimum data size and maximum data size.

According to embodiments the server may decide not to wait if one or more of the following conditions are met at the time when the request is being evaluated the delay already experienced by the client exceeded the maximum delay the data size to be sent to the client i.e. server output size exceeds the maximum data size both of the following conditions are met the data size to be sent is equal to the minimum data size and the delay experienced by the client is equal or higher than the minimum delay or some combination thereof. If the server decides to delay the response it may decide to not respond to the client and re evaluate the decision at a later time based on a method for determining a time for re evaluation. For example in one embodiment the server may wait an affixed amount of time before re evaluating the decision.

Referring to it will be readily understood that certain embodiments can be implemented using any of a wide variety of devices or combinations of devices. An example device that may be used in implementing embodiments includes a computing device in the form of a computer which may take the form of a server a workstation computer a mobile device and the like. In this regard the computer may execute program instructions configured to provide predictive placement of content through network analysis and perform other functionality of the embodiments as described herein.

Components of computer may include but are not limited to at least one processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit s . The computer may include or have access to a variety of computer readable media. The system memory may include computer readable storage media in the form of volatile and or nonvolatile memory such as read only memory ROM and or random access memory RAM . By way of example and not limitation system memory may also include an operating system application programs other program modules and program data.

A user can interface with for example enter commands and information the computer through input devices . A monitor or other type of device can also be connected to the system bus via an interface such as an output interface . In addition to a monitor computers may also include other peripheral output devices. The computer may operate in a networked or distributed environment using logical connections network interface to other remote computers or databases remote device s . The logical connections may include a network such local area network LAN a wide area network WAN a cellular network but may also include other networks.

Those skilled in the art will recognize that aspects may be embodied as a system method or computer program product. Accordingly aspects of the present invention may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may all generally be referred to herein as a circuit module or system. Furthermore aspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium s having computer readable program code embodied thereon.

Any combination of one or more computer readable medium s may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the computer readable storage medium would include the following an electrical connection having one or more wires a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device or any suitable combination of the foregoing. In the context of this document a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system apparatus or device.

A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein for example in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms including but not limited to electro magnetic optical or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate propagate or transport a program for use by or in connection with an instruction execution system apparatus or device.

Program code embodied on a computer readable medium may be transmitted using any appropriate medium including but not limited to wireless wireline optical fiber cable RF etc. or any suitable combination of the foregoing.

Computer program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The program code may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider .

Aspects of the present invention are described below with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable medium that can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function act specified in the flowchart and or block diagram block or blocks. The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

This disclosure has been presented for purposes of illustration and description but is not intended to be exhaustive or limiting. Many modifications and variations will be apparent to those of ordinary skill in the art. The example embodiments were chosen and described in order to explain principles and practical application and to enable others of ordinary skill in the art to understand the disclosure for various embodiments with various modifications as are suited to the particular use contemplated.

Although illustrated example embodiments have been described herein with reference to the accompanying drawings it is to be understood that embodiments are not limited to those precise example embodiments and that various other changes and modifications may be affected therein by one skilled in the art without departing from the scope or spirit of the disclosure.

