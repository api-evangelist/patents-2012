---

title: Scalable distributed processing of RDF data
abstract: In general, techniques are described for an RDF (Resource Description Framework) database system which can scale to huge size for realistic data sets of practical interest. In some examples, a database system includes a Resource Description Framework (RDF) database that stores a plurality of data chunks to one or more storage drives, wherein each of the plurality of data chunks includes a plurality of triples of the RDF database. The database system also includes a working memory, a query interface that receives a query for the RDF database, a SPARQL engine that identifies a subset of the data chunks relevant to the query, and an index interface that includes one or more bulk loaders that load the subset of the data chunks to the working memory. The SPARQL engine executes the query only against triples included within the loaded subset of the data chunks to obtain a query result.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08756237&OS=08756237&RS=08756237
owner: Architecture Technology Corporation
number: 08756237
owner_city: Minneapolis
owner_country: US
publication_date: 20121012
---
The Resource Description Framework RDF is a data model that is one of the core technologies of the Semantic Web. RDF data represents a labeled directed graph with the labels applied to both nodes and arcs. RDF data is currently stored in triple stores where the unit of storage is a single subject predicate object triple. An index of triples is used to satisfy queries in the SPARQL Protocol and RDF Query Language SPARQL language by a series of joins in which triples that match each part of the query are assembled into an overall answer.

A SPARQL query in its most basic form is a graph pattern a small labeled directed graph with some of the nodes and arcs labeled with variables. The query seeks one some or all subgraphs of the RDF graph that match the pattern. Matching means that the subgraph is isomorphic to the pattern as a directed graph and that the pattern labels other than variables match the corresponding graph labels. For each match the pattern variables take on the value of the corresponding RDF graph label thus providing one answer.

Each arc in RDF is labeled with a Uniform Resource Identifier URI . Each node is labeled with either a URI or a value from one of a set of standard types. Nodes can also be blank i.e. unlabeled. URIs are used to unambiguously name resources which can be almost anything. Each arc in the RDF graph is uniquely specified by its three labels at the tail on the arc and at the head. These are called the subject the predicate and the object respectively sometimes abbreviated S P and O and always given in that order . These three labels together are a triple. An RDF graph may be uniquely represented by the set of its triples. Each triple S P O is a proposition stating that S stands in relation P to O.

Two RDF graphs may be merged to form a new graph. The merged graph consists of roughly the union of the triples from the two constituent graphs. If each of the constituent graphs contains a node with the same URI label then that node will appear just once in the merged graph. This makes sense since the URI is supposed to have the same unambiguous meaning wherever it appears.

RDF is associated with a theory of types subtypes and containers called RDF Schema RDFS . Using RDFS nodes can be assigned a type and types can be given subtypes. Predicates the labels of arcs are also typed by assigning a domain and a range each of which are types for the subject and object respectively. Sometimes when only some of the types of nodes are given it is possible to infer the missing types. This is called RDFS inference. Many SPARQL query engines can be set to automatically perform RDFS inference in effect behaving as if there are some additional type asserting triples in the RDF graph.

OWL Web Ontology Language is a more elaborate language for describing RDF in which a much richer set of things may be expressed. Again many SPARQL engines can be set to perform OWL inference by behaving as if the missing but inferred OWL triples are actually present in the RDF graph.

Conventional RDF database systems often called triple stores treat the RDF graph data as a large set of triples. These triples are individually indexed. The index is divided in to three parts with each triple represented by an entry in each part of the index. In each of the three parts the subject predicate and object labels as a triple are sorted lexicographically and stored in a B tree or a similar data structure that supports range queries. The difference between the three parts is that the order in which the three parts of the triple are considered for lexicographic sorting is different. Although there are six possible permutations of S P and O only three of them are needed for full functionality.

To look up a pattern for a single triple such as a x b where a and b are labels and x is a variable the system will consult the part of the index in which the lexicographic ordering is S O P or O S P only one of which will be present. Suppose the S O P lexicographic ordering is present. The system will consult that index to find the first entry beginning with a b . All of the entries beginning with a b will be adjacent in the index and can be rapidly returned. Each of those represents a triple of the form a x b . To find matches for patterns with two variables such as a x y the same S O P part of the index would be used but now searching for the first entry with prefix a and following entries.

More complex patterns called Basic Graph Patterns or BGPs in the SPARQL literature consist of multiple triples. To satisfy these the SPARQL query engine begins by looking up one of the triples in the pattern. Each match induces a set of values labels for the variables occurring in that triple. The SPARQL engine then processes a second pattern triple applying each valuation to that triple and then searching for the resulting pattern. Here is an example pattern with two triples 

The typical RDF database system repeats the process using the second valuation possibly finding some additional matches each of which yields an answer to the overall query. In this small example the RDF database system consults the index three times Once to find matches for the first triple and then once for each of the two matches found.

The typical RDF database system effectively computes a join between the sets of matches for each of the triples in the pattern. A typical RDF database system uses here a strategy for this join that is known as indexed nested loop. For more complex patterns the loop nesting will be deeper and the number of total iterations can be quite large. The number of iterations and thus the number of times the index must be consulted can be quite sensitive to the order in which the pattern triples are nested. For complex queries these joins are prohibitively expensive especially as the size of the database grows.

In some cases the index is divided into parts each of which is assigned to a different host. The results from each outer loop index lookup are passed in one at a time to affect the iterations of the inner loop at each level of nesting. Even if the outer loop results are all adjacent to one another in the index and thus likely on the same host each iteration of the inner loop may require consulting the index on a distinct host. Thus for N outer loop results roughly N 1 index hosts will be involved with N inter host communication events. The cost of such communication between distributed hosts particularly for deeply nested queries can dwarf the other costs in this query handling strategy.

Enterprises are facing a deluge of data as the rate of production from sensors and other sources increases exponentially. Timely processing and analysis of this data will be essential for the success of future enterprise operations. Much of this data is heterogeneous semi structured and incomplete or non standard making its storage and handling awkward and inefficient. The Resource Description Framework RDF the primary technology of the Semantic Web is an ideal tool for representing such data in a uniform tractable way with the elegant SPARQL query language providing a powerful means of retrieving and processing the data. Unfortunately current RDF technology does not provide the necessary performance for storage and query at very large scales.

In general techniques are described for an RDF Resource Description Framework database system which can scale significantly in size for realistic data sets of practical interest i.e. enabling the persistent storage of very large RDF graphs and the rapid satisfaction of queries. The RDF database system may leverage a variety of platform architectures including IaaS Infrastructure as a Service clouds and heterogeneous distributed computing systems. Further techniques are described herein for reproducing the capabilities of SPARQL queries to the extent practical but for very large scale RDF data and with an architecture that is friendly to distributed processing.

The scale of data from sensors and other sources is increasing exponentially. The anticipated scale of such data threatens to overwhelm processing capabilities including crucially storage and retrieval thereby preventing timely exploitation of the data. This problem is exacerbated by the heterogeneous semi structured nature of the data which makes conventional Relational Database Management System RDBMS storage awkward and inefficient. However such data can be seamlessly expressed in RDF one of the core technologies of the Semantic Web and thus RDF storage and SPARQL querying is a natural way to handle this difficult data. The RDF database system described herein may enable data storage and querying in RDF form at the anticipated scale with sufficient query performance versus conventional RDF databases.

In some examples the RDF database system stores RDF in overlapping chunks much larger than one triple and indexes them with multiple indexes that are tailored to the domain and query load. Chunks that overlap have at least one triple in common i.e. some of the triples may be stored in multiple chunks. The RDF database system queries by retrieving selected chunks using the indexes and then applying conventional SPARQL querying to the resulting modestly sized RDF graph in memory. Each chunk of RDF may be stored as a binary data block of variable size that is reconstituted as an in memory RDF graph and merged with other such graphs before being queried. The binary data block is in a format that concisely captures the RDF content and can rapidly be reconstituted. In addition the techniques may include extensions to SPARQL that makes use efficient use of the RDF database system described herein.

The techniques may further include application of a scripting language referred to hereinafter as Mnemosyne Query Language MQL that specifies an ordering of parameterized steps by which the RDF database system retrieves chunks from storage for placement to working memory performs SPARQL queries on relevant in memory RDF sub graphs and optionally iteratively retrieves additional chunks for additional SPARQL queries according to the results of one or more prior SPARQL queries. MQL scripts may additionally after each of these partial queries specify the partial or whole clearing of working memory to provide working memory space for additional chunks for subsequent steps.

In one example a method comprises receiving with a database system a query for a Resource Description Framework RDF database that stores a plurality of data chunks to one or more storage drives wherein each of the plurality of data chunks includes a plurality of triples of the RDF database. The method further comprises identifying a subset of the data chunks relevant to the query loading the subset of the data chunks to a main memory associated with the database system and executing the query only against triples included within the subset of the data chunks loaded to the main memory to obtain a query result.

In another example a database system includes a Resource Description Framework RDF database that stores a plurality of data chunks to one or more storage drives wherein each of the plurality of data chunks includes a plurality of triples of the RDF database. The database system also includes a working memory a query interface that receives a query for the RDF database a SPARQL engine that identifies a subset of the data chunks relevant to the query and an index interface that includes one or more bulk loaders that load the subset of the data chunks to the working memory. The SPARQL engine executes the query only against triples included within the loaded subset of the data chunks to obtain a query result.

The details of one or more examples of the disclosure are set forth in the accompanying drawings and the description below. Other features objects and advantages will be apparent from the description and drawings and from the claims.

The RDF database is conceptually illustrated in as RDF graph . The totality of the RDF data again illustrated as conceptual RDF graph describes a large directed labeled graph. RDF graph may include one or more connected components that each consists of all nodes and arcs that can be reached starting from a single node of the connected component by an undirected path in the graph thus forming a partition of RDF graph .

RDF database system stores the RDF data as one or more chunks A M collectively chunks each of which is encoded as a binary data block that is accessible from index . Each of chunks represents a subgraph of the collection of RDF triples of an RDF database that constitute RDF graph i.e. the large directed labeled graph described by the totality of the RDF data.

Chunks each include at least two RDF triples of RDF graph and thus represent fragments of RDF data that correspond to functional units of information. Each of chunks includes RDF triples that represent a small subgraph of RDF graph and contains those arcs and nodes that pertain to a particular topic. The union of all of chunks appropriately merged will be the original RDF graph . RDF database system may be stored in a concise binary form. Each of chunks may be associated with a locally unique identifier.

Index represents an associative data structure having one or more index entries A K collectively index entries . Each of index entries includes a key value pair that maps the value to the key. The value of each of index entries provides access to a binary data block that encodes one of chunks . The value may correspond to a chunk identifier a file handle a URI URL or other reference by which RDF database system may load the corresponding one of chunks to in memory RDF data of RDF database system . Chunk storage may be a separate function and index may refer to the chunk through an extra level of indirection. As a result multiple index entries from the same or multiple indexes may all refer to the same chunk.

For example under the direction of script engine executing a script RDF database system retrieves one of more of chunks using index and reconstitutes the retrieved chunks as in memory RDF data in working memory of SPARQL engine of RDF database system . SPARQL engine may then execute SQARQL queries against in memory RDF data to obtain and return SPARQL query results.

In the illustrated example script engine receives a script by an external interface and executes script . Elements of script execution include identifying index entries B K as including keys respectively Token 1 and Token 2 relevant to an overall query represented by script . Script engine retrieves chunks A B accessible by values of index entries B K and loads chunks A B to in memory RDF data . Script engine provides a SPARQL query to SPARQL engine which executes the SPARQL query against in memory RDF data to obtain a query result . Script engine returns the query result responsive to script via the external interface.

Query processing in accordance with techniques described herein consists of a combination of gather sift and clear steps controlled by a script. A gather step retrieves RDF chunks into in memory RDF data . A sift step is the application by SPARQL engine of a SPARQL query on the in memory RDF data . A clear step directs RDF database system to selectively remove the triples from in memory RDF data . To gather the appropriate chunks RDF database system queries in index to find the chunks of interest. In contrast to conventional RDF database systems in which the RDF data is stored and indexed in small units called triples the techniques as applied by RDF database system as described herein may decrease the granularity with which RDF data is indexed and loaded to main memory for processing. Again in order to match a complex pattern conventional RDF database systems consult the index many times and join the result sets. This is typically done with deeply nested loops with the innermost loop experiencing many iterations. The large number of joins needed will be particularly expensive for distributed databases because of the cost of communications.

As described herein RDF database system in conjunction with MQL replaces a very general one size fits all index with a topic or domain specific customization that may provide scalable performance. The RDF database system provides a framework in which domain specific indexes query strategies and even chunk boundaries can be deployed. The RDF database system may be agnostic about the technology behind the indexes and other components incorporated into the system. This flexibility will enable the RDF database system to take advantage of the native capabilities of the platform it is running on such as an IaaS cloud or an asymmetric computing cluster.

Once the basic infrastructure is in place and proven researchers can develop new kinds of indexes new idioms for organizing such databases and new strategies for query satisfaction. There will be an incentive for performance models to be refined and query planning and optimization to be automated all potentially hard research problems. The RDF database system and MQL are opens up this new realm of possibilities by breaking the logjam that has prevented large scale applications of RDF.

Each of indexes includes one or more index entries that store key value pairs. In other words multiple configurable indexes associate key data to each chunk. Thus a chunk may be said to be about related to and or relevant to an associated key. Each of the configurable indexes will record some different significant aspect of the chunks subject matter or other metadata. Indexes thus enable RDF database system to rapidly and efficiently retrieve chunks that are about any given topic or combination of topics expressed by the keys. To avoid storing each chunk multiple times indexes may associate keys to chunk identifiers. Then multiple different indexes can refer to the same stored chunk via its identifier. In addition for instance a single one of indexes may include multiple index entries having different keys that each refer to the same chunk.

Indexes may be implemented using different types of indexing technologies so long as indexes are accessible by index API . Different types of index including off the shelf components may be used simultaneously for different indexes. For example conventional relational database RDBMS tables and indexes may be represented by index A. Alternatively one of the NoSQL indexes which may be tailored for use in cloud computing and have relaxed consistency guarantees e.g. Cassandra may be represented by index B. As another example index N may represent a Dynamo index.

RDF database system updates indexes incrementally as chunks are added to chunk storage system or updated. New indexes can be added to indexes by using a corresponding bulk load capability that examines all chunks or all that are relevant . The same bulk load function could also be used to reconfigure the chunks the RDF subgraphs themselves in order to make the chunk divisions more convenient for the query load. Just as indexes may be plugged into the RDF database system in the component model described herein corresponding bulk loaders which are typically specific to a type of index may also be plugged in.

RDF database system communicates with chunk storage system by a chunk storage API that provides methods for loading chunks e.g. chunks of from persistent storage e.g. a disk local to RDF database system a host server or other storage location to in memory RDF data . Chunk storage API may provide additional methods for storing RDF chunks as binary data blocks of variable size that can be reconstituted as an in memory RDF graph and merged with other such graphs before being queried. In this way chunk storage functionality of RDF database system is in this example designed as a plug in component in order to enable systems to be configured with alternatives that offer different guarantees or that take advantage of native storage mechanisms of a cloud platform.

In accordance with the techniques described herein query processing by RDF database system is a sequence of gather sift and clear steps orchestrated by a script. This script which must be supplied by the requestor is written in what is referred to hereinafter as Mnemosyne Query Language MQL . As described in further detail below the three steps may be parameterized and an MQL script may include many instances of each step. The three step types executed in MQL scripts are described in the context of RDF database system as follows 

RDF database system queries indexes by index API and retrieves from chunk storage system by chunk storage API relevant chunks of RDF data in binary form and reconstitutes the relevant chunks in in memory RDF data for local SPARQL engine . This causes a merge with any previously gathered RDF In effect a kind of database join is thereby computed. . Chunk retrieval is specified in terms of the one of indexes to use and the corresponding key value or key range to look up. The key value range may be an expression native to the specified index. MQL provides the capability to specify these expressions as templates strings with variables that will be replaced with values derived from the results of earlier steps or with values supplied by the MQL script. The MQL script specifies e.g. as computer code the computation that converts the results of earlier sift operations into values to be substituted into gather templates. A gather step may be indicated in an MQL script by a GATHER token.

In one example syntax gather returns an . In the above declaration IndexID names a particular one of indexes to search. Selector is a string specifying the keys to search for in a format that is specific to the index. The iterator with each iteration returns the key found and also loads the corresponding RDF chunk into working memory. As a result code in this style may sift from each matching chunk 

SPARQL engine applies a conventional SPARQL query to evaluate the RDF in in memory RDF data . The result may be part of the final answer or it may be an intermediate result. As with gather the SPARQL expression embedded within MQL may be a template into which values from earlier steps will be substituted. A sift step may be indicated in an MQL script by a SIFT token.

In one example syntax sift returns an . The SPARQL query in this operation is restricted to a SELECT or SELECT DISTINCT which returns an n tuple of values for each matching subgraph in the working memory. The iterator with each iteration returns one of these n tuples.

RDF database system clears some of the RDF triples in in memory RDF data to make space for additional gathering. The design of this feature may depend on the particular SPARQL engine incorporated into RDF database system . SPARQL engine may in some instances be based on the ARQ Engine for Apache Jena . In some instances RDF database system uses Named Graphs or some other native method of labeling sets of triples to permit later deletion. Alternatively RDF database system may incorporate a mechanism for specifying a SPARQL query whose answers are to be deleted. A clear step may be indicated in an MQL script by a CLEAR token.

In one example syntax clear returns . The SPARQL query in this operation is restricted as with the sift operation. The effect is to delete from working memory the entire matching subgraph for each match. The returned status indicates the number of matches removed or an error condition.

MQL links gather sift and clear steps together in various patterns with a scripting language. The scripting language may in some examples be a data flow language for the Mnemosyne evaluation will to a large extent have sift steps whose output is used to control subsequent steps of all kinds leading naturally to a pipelined implementation.

MQL queries e.g. MQL query arrive via query API to be processed by MQL Parser Evaluator . MQL Parser Evaluator may represent an interpreter for the MQL language. MQL Parser Evaluator may be adapted from an existing interpreter for e.g. the Python scripting language VBScript or other scripting language that is a basis for MQL.

A database metrics component maintains running totals of the number of triples of various kinds that are in the database. RDF database system uses these metrics to estimate the number of results a query will have. Such estimates together with a performance model component can in some cases provide inputs to a query plan optimizer. In some instances performance model may implement a mathematical performance model based on measured performance of RDF database system .

Performance model for RDF database system may also provide an estimate of the time and resources to process a specified query given the sizes of all of the intermediate results. A database metrics subsystem for RDF database system may provide a rough estimate of the size of results and thus of intermediate results . An invoker of RDF database system could receive an estimate of the time or processing resources needed to satisfy their query and may therefore be provided by query API with the opportunity to skip or alter a query that is too expensive.

Because MQL queries are more like query plans in that they specify how the answer is to be obtained in some instances RDF database system incorporates an automated query planner and optimizer. Queries specified in some more declarative form may be compiled by MQL Parser Evaluation into MQL for evaluation. Such compilers may use heuristics to propose a set of correct query plans and then select the one whose predicted performance is the best. Thus database metrics and performance model may be a natural starting point for the development of such a compiler.

As described thus far RDF database system communicates with indexes and chunk storage system that may be implemented in a distributed manner. For example chunk storage system or one of indexes may be implemented using the Cassandra distributed database. In some examples remote API includes methods by which fragments of MQL may be sent to a separate remote instance of RDF database system with the results being streamed back. In the illustrated example RDF database system issues MQL fragment a query fragment by remote API and receives MQL result a query result fragment in return. Each instance of RDF database system may have distinct indexes but one or more instances may share the chunk storage. These techniques may support database strategies such as replication federation parallel evaluation and query load sharing.

As a result one of the more advanced adaptations for distributed processing will be to specify an MQL fragment to be evaluated remotely either to spread the processing load to additional hosts or to move the computation closer to the data to reduce communication. MQL may include syntax to describe such excursions and to specify how they are to be carried out. In some examples MQL includes syntax directing the results be streamed back as they are generated. In some examples MQL includes syntax directing the results be sent back in a batch when they are all ready. Remote evaluation may in some examples proceed in parallel with other parts of computation.

RDF database system may provide for cloud computing and distributed computing. RDF database system may provide for remote invocation via query API with a particular focus on its use in systems with federated and replicated databases. RDF database system may support fault tolerance and elasticity. With regard to fault tolerance MQL scripts may contain checkpointing of intermediate results explicitly check for the failure of steps and be restarted. Thus the strategy for fault tolerance may be different for each query implementing a different tradeoff between cost performance and safety.

RDF database system may provide for various methods of inference. Using forward chaining inference RDF database system infers additional RDF triples implied by the data and add these to the RDF chunks either augmenting existing chunks or creating fresh chunks that contain just the inferred triples. Most SPARQL engines including Jena implement backward chaining inference transparently during querying. SPARQL engine may apply inference capabilities of the underlying SPARQL engine during sift steps of MQL scripts.

Chunks and are indexed by indexes . For example index A may associates key a name with each of chunks related to a person. As another example index B may associate an age with each of chunks related to a person. A chunk relating to a purchase would be associated by two indexes of indexes with a person e.g. a customer and one or more products. One or more of indexes can also record metadata about the chunks. For instance for each of chunks and the date on which it was the chunk is created may be recorded in one of indexes .

In one example the Mnemosyne capable RDF database is deployed on Amazon Web Services AWS . Individual chunks of chunks and may be stored as AWS Simple Storage Service S3 objects with the name of the object being the identifier for the chunk. Some of indexes e.g. the larger ones may be implemented as AWS DynamoDB databases. Others of indexes may be implemented as conventional RDBMS tables on a single cloud host. RDF database system of ties indexes together via its plug in architecture.

As described above with respect to one of the core components of RDF database system a Mnemosyne query processor is a conventional SPARQL query processor represented by SPARQL engine . However RDF database system uses SPARQL engine to process in memory RDF graphs which must be of modest size so that they can fit in main memory. The binary form of stored RDF chunks is engineered so that a chunk can be rapidly and efficiently merged into the in memory RDF graph of the SPARQL processor.

A Mnemosyne query is a recipe for retrieving some of chunks and or using indexes merging the chunks into the SPARQL engine working memory then running a conventional SPARQL query on the accumulated RDF graph in memory RDF data in working memory and then finally clearing out some of the working memory to make some space. The recipe consists of the cycle possibly repeated many times and is orchestrated by a script that describes how the results of each cycle will be used to invoke and control the subsequent cycles.

A Mnemosyne query is written in a scripting language called Mnemosyne Query Language MQL . MQL may include a convention scripting language extended to include the three new primitive operations gather sift and clear described above.

Gather steps find selected RDF chunks using the indexes retrieve them from storage and place them in the working memory of SPARQL engine . Sift steps direct SPARQL engine to run a conventional SPARQL query on the RDF that has accumulated in working memory in memory RDF data . Finally clear steps remove some or all of the RDF from working memory to make space for more gathering. For each of these primitive operations parameters control its function. In the case of gather parameters determine which of indexes will be searched and which keys will be searched for. The parameters of sift include a SPARQL query expression. The parameters of clear specifies which RDF triples should be cleared from working memory. Because these parameters are supplied by the script i.e. they are computed they provide a means for the results of earlier operations to influence later operations.

For example using the data described above for an enterprise a query may request employees of company C who have purchased products from their own company. This query may be scripted in MQL to be performed by RDF database system in the following manner 

If additional steps remain NO branch of and the next step is a gather step YES branch of then MQL parser evaluator uses one of indexes via index API to retrieve and load via chunk storage API one or more relevant chunks from chunk storage system to in memory RDF data . The relevant chunks are a subset of all of the data chunks of an RDF database as described by indexes .

If the next step is a sift step YES branch of or following step SPARQL engine applies a SPARQL query specified in the script to in memory RDF data which includes the loaded subset of data chunks . If the next step is a clear step YES branch of MQL parser evaluator removes RDF data as specified by MQL script from in memory RDF data . Otherwise NO branch of script execution continues .

The techniques described herein may be implemented in hardware software firmware or any combination thereof. Various features described as modules units or components may be implemented together in an integrated logic device or separately as discrete but interoperable logic devices or other hardware devices. In some cases various features of electronic circuitry may be implemented as one or more integrated circuit devices such as an integrated circuit chip or chipset.

If implemented in hardware this disclosure may be directed to an apparatus such a processor or an integrated circuit device such as an integrated circuit chip or chipset. Alternatively or additionally if implemented in software or firmware the techniques may be realized at least in part by a computer readable data storage medium comprising instructions that when executed cause a processor to perform one or more of the methods described above. For example the computer readable data storage medium may store such instructions for execution by a processor.

A computer readable medium may form part of a computer program product which may include packaging materials. A computer readable medium may comprise a computer data storage medium such as random access memory RAM read only memory ROM non volatile random access memory NVRAM electrically erasable programmable read only memory EEPROM Flash memory magnetic or optical data storage media and the like. In some examples an article of manufacture may comprise one or more computer readable storage media.

In some examples the computer readable storage media may comprise non transitory media. The term non transitory may indicate that the storage medium is not embodied in a carrier wave or a propagated signal. In certain examples a non transitory storage medium may store data that can over time change e.g. in RAM or cache .

The code or instructions may be software and or firmware executed by processing circuitry including one or more processors such as one or more digital signal processors DSPs general purpose microprocessors application specific integrated circuits ASICs field programmable gate arrays FPGAs or other equivalent integrated or discrete logic circuitry. Accordingly the term processor as used herein may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein. In addition in some aspects functionality described in this disclosure may be provided within software modules or hardware modules.

Various embodiments have been described. These and other embodiments are within the scope of the following examples.

