---

title: Auto-commit memory synchronization
abstract: Apparatuses, systems, methods and computer program products are disclosed for auto-commit memory management. A method includes receiving a memory request from a client, such as a barrier request or a checkpoint request. The memory request is associated with a volatile memory buffer of a non-volatile recording device. The memory buffer may be configured to preserve data in the non-volatile recording device in response to a trigger. A method includes issuing a serializing instruction that flushes data from a processor complex to the memory buffer. A method includes determining completion of the serializing instruction flushing the data to the memory buffer.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09047178&OS=09047178&RS=09047178
owner: SanDisk Technologies, Inc.
number: 09047178
owner_city: Plano
owner_country: US
publication_date: 20121204
---
This application claims the benefit of U.S. Provisional Patent Application No. 61 583 133 entitled APPARATUS SYSTEM AND METHOD FOR AUTO COMMIT MEMORY and filed on Jan. 4 2012 for David Flynn et al. of U.S. Provisional Patent Application No. 61 637 257 entitled APPARATUS SYSTEM AND METHOD FOR AUTO COMMIT MEMORY and filed on Apr. 23 2012 for David Flynn et al. of U.S. Provisional Patent Application No. 61 661 742 entitled APPARATUS SYSTEM AND METHOD FOR AUTO COMMIT MEMORY and filed on Jun. 19 2012 for Nisha Talagala et al. of U.S. Provisional Patent Application No. 61 691 221 entitled APPARATUS SYSTEM AND METHOD FOR AUTO COMMIT MEMORY and filed on Aug. 20 2012 for Nisha Talagala et al. of U.S. Provisional Patent Application No. 61 705 058 entitled APPARATUS SYSTEM AND METHOD FOR SNAPSHOTS IN A STORAGE DEVICE and filed on Sep. 24 2012 for Nisha Talagala et al. and is a continuation in part application of and claims priority to U.S. patent application Ser. No. 13 324 942 entitled APPARATUS SYSTEM AND METHOD FOR AUTO COMMIT MEMORY and filed on Dec. 13 2011 for David Flynn et al. which claims the benefit of U.S. Provisional Patent Application No. 61 422 635 entitled APPARATUS SYSTEM AND METHOD FOR AUTO COMMIT MEMORY and filed on Dec. 13 2010 for David Flynn et al. each of which are incorporated herein by reference.

This disclosure relates to auto commit memory and more particularly to exposing auto commit memory synchronization operations.

Processor caches are designed to cache data primarily for volatile memory using a volatile memory namespace. Because the data and the namespace are not generally persistent processor caches typically destage or flush data to the underlying memory lazily at an arbitrary time and in an arbitrary order.

In these weakly ordered systems data can trickle down from a processor cache to the underlying memory with no guarantee of operation order. Without strict ordering of data in operation order it can be difficult for a memory mapped device to provide data persistence especially if a host device experiences a power failure or other restart event.

A method for auto commit memory management is presented. In one embodiment the method includes receiving a memory request from a client. In a further embodiment the memory request is associated with a volatile memory buffer of a non volatile recording device. The memory buffer in certain embodiments is configured to preserve data in the non volatile recording device in response to a trigger. In one embodiment the method includes issuing a synchronization request that flushes data from a processor complex to the memory buffer. In one embodiment the method includes determining completion of the synchronization request flushing the data to the memory buffer.

An apparatus for auto commit memory management is presented. The apparatus in one embodiment includes a volatile memory buffer within an isolation zone of a non volatile memory device. In one embodiment the memory buffer is configured to receive data destaged from a processor complex of a host computing device in response to a serializing instruction. In one embodiment the apparatus includes a barrier completion module configured to determine completion of the serializing instruction destaging the data to the memory buffer. In a further embodiment the apparatus includes a checkpoint module configured to create a snapshot copy of one or more pages of the memory buffer in response to completion of the serializing instruction.

A system for auto commit memory management is also presented. In one embodiment the system includes a device driver for a non volatile storage device. In certain embodiments the device driver is configured to execute on a processor of a host computing device. In one embodiment the device driver includes an auto commit request module configured to receive an auto commit request for an auto commit buffer within an isolation zone of the non volatile storage device. In one embodiment the device driver includes a flush module configured to execute a serializing instruction to flush data from a cache of the processor to the auto commit buffer in response to the auto commit request. In a further embodiment the system includes a storage controller disposed on the non volatile storage device. The storage controller in one embodiment includes a barrier completion module configured to determine completion of the serializing instruction flushing the data to the auto commit buffer in response to receiving a completion identifier flushed from the cache of the processor to the non volatile storage device.

A computer program product comprising a computer readable storage medium storing computer usable program code executable to perform operations for auto commit memory management is also presented. In one embodiment the operations include issuing a first synchronization instruction in response to an auto commit request for an auto commit buffer of a non volatile memory device. In a further embodiment the first synchronization instruction is to flush data from a processor cache to the auto commit buffer. In one embodiment the operations include placing a completion identifier in the processor cache. In a further embodiment the completion identifier is associated with the non volatile memory device. In one embodiment the operations include issuing a second synchronization instruction to flush the completion identifier from the processor cache to the non volatile memory device such that receiving the completion identifier in the non volatile memory device indicates completion of the first synchronization instruction.

Another apparatus for auto commit memory management is presented. The apparatus in one embodiment includes means for receiving an auto commit request for an auto commit buffer of a non volatile recording device. In a further embodiment a memory address range of a host is mapped to the auto commit buffer. In one embodiment the apparatus includes means for issuing a serializing instruction in response to the auto commit request. In a further embodiment the serializing instruction is to flush data of the memory address range from a processor cache of the host to the auto commit buffer. In one embodiment the apparatus includes means for determining completion of the serializing instruction flushing the data of the memory address range to the auto commit buffer.

Reference throughout this specification to features advantages or similar language does not imply that all of the features and advantages that may be realized with the present disclosure should be or are in any single embodiment of the disclosure. Rather language referring to the features and advantages is understood to mean that a specific feature advantage or characteristic described in connection with an embodiment is included in at least one embodiment of the present disclosure. Thus discussion of the features and advantages and similar language throughout this specification may but do not necessarily refer to the same embodiment.

Furthermore the described features advantages and characteristics of the disclosure may be combined in any suitable manner in one or more embodiments. One skilled in the relevant art will recognize that the disclosure may be practiced without one or more of the specific features or advantages of a particular embodiment. In other instances additional features and advantages may be recognized in certain embodiments that may not be present in all embodiments of the disclosure. These features and advantages of the present invention will become more fully apparent from the following description and appended claims or may be learned by the practice of the disclosure as set forth hereinafter.

Many of the functional units described in this specification have been labeled as modules in order to more particularly emphasize their implementation independence. For example a module may be implemented as a hardware circuit comprising custom VLSI circuits or gate arrays off the shelf semiconductors such as logic chips transistors or other discrete components. A module may also be implemented in programmable hardware devices such as field programmable gate arrays programmable array logic programmable logic devices or the like.

Modules may also be implemented in software for execution by various types of processors. An identified module of executable code may for instance comprise one or more physical or logical blocks of computer instructions which may for instance be organized as an object procedure or function. Nevertheless the executables of an identified module need not be physically located together but may comprise disparate instructions stored in different locations which when joined logically together comprise the module and achieve the stated purpose for the module.

Indeed a module of executable code may be a single instruction or many instructions and may even be distributed over several different code segments among different programs and across several memory devices. Similarly operational data may be identified and illustrated herein within modules and may be embodied in any suitable form and organized within any suitable type of data structure. The operational data may be collected as a single data set or may be distributed over different locations including over different storage devices and may exist at least partially merely as electronic signals on a system or network. Where a module or portions of a module are implemented in software the software portions are stored on one or more computer readable media.

Reference throughout this specification to one embodiment an embodiment or similar language means that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the present disclosure. Thus appearances of the phrases in one embodiment in an embodiment and similar language throughout this specification may but do not necessarily all refer to the same embodiment.

Reference to a computer readable medium may take any form capable of storing machine readable instructions on a digital processing apparatus. A computer readable medium may be embodied by a compact disk digital video disk a magnetic tape a Bernoulli drive a magnetic disk a punch card flash memory integrated circuits or other digital processing apparatus memory device.

Furthermore the described features structures or characteristics of the disclosure may be combined in any suitable manner in one or more embodiments. In the following description numerous specific details are provided such as examples of programming software modules user selections network transactions database queries database structures hardware modules hardware circuits hardware chips etc. to provide a thorough understanding of embodiments of the disclosure. One skilled in the relevant art will recognize however that the disclosure may be practiced without one or more of the specific details or with other methods components materials and so forth. In other instances well known structures materials or operations are not shown or described in detail to avoid obscuring aspects of the disclosure.

The schematic flow chart diagrams included herein are generally set forth as logical flow chart diagrams. As such the depicted order and labeled steps are indicative of one embodiment of the presented method. Other steps and methods may be conceived that are equivalent in function logic or effect to one or more steps or portions thereof of the illustrated method. Additionally the format and symbols employed are provided to explain the logical steps of the method and are understood not to limit the scope of the method. Although various arrow types and line types may be employed in the flow chart diagrams they are understood not to limit the scope of the corresponding method. Indeed some arrows or other connectors may be used to indicate only the logical flow of the method. For instance an arrow may indicate a waiting or monitoring period of unspecified duration between enumerated steps of the depicted method. Additionally the order in which a particular method occurs may or may not strictly adhere to the order of the corresponding steps shown.

The host stores data in the storage device and communicates data with the storage device via a communications connection not shown . The storage device may be internal to the host or external to the host . The communications connection may be a bus a network or other manner of connection allowing the transfer of data between the host and the storage device . In one embodiment the storage device is connected to the host by a PCI connection such as PCI express PCI e . The storage device may be a card that plugs into a PCI e connection on the host .

The storage device also has a primary power connection that connects the storage device with a primary power source that provides the storage device with the power that it needs to perform data storage operations such as reads writes erases etc. The storage device under normal operating conditions receives the necessary power from the primary power source over the primary power connection . In certain embodiments such as the embodiment shown in the primary power connection connects the storage device to the host and the host acts as the primary power source that supplies the storage device with power. In certain embodiments the primary power connection and the communications connection discussed above are part of the same physical connection between the host and the storage device . For example the storage device may receive power over a PCI connection.

In other embodiments the storage device may connect to an external power supply via the primary power connection . For example the primary power connection may connect the storage device with a primary power source that is a power converter often called a power brick . Those in the art will appreciate that there are various ways by which a storage device may receive power and the variety of devices that can act as the primary power source for the storage device .

The storage device provides nonvolatile storage memory and or recording media for the host . shows the storage device comprising a write data pipeline a read data pipeline nonvolatile memory a storage controller a power management apparatus and a secondary power supply . The storage device may contain additional components that are not shown in order to provide a simpler view of the storage device .

The nonvolatile memory stores data such that the data is retained even when the storage device is not powered. Examples of nonvolatile memory include flash memory nano random access memory nano RAM or NRAM nanocrystal wire based memory silicon oxide based sub 10 nanometer process memory graphene memory Silicon Oxide Nitride Oxide Silicon SONOS Resistive random access memory RRAM programmable metallization cell PMC conductive bridging RAM CBRAM magneto resistive RAM MRAM dynamic RAM DRAM phase change RAM PRAM or other non volatile solid state storage media. In other embodiments the non volatile memory may comprise magnetic media optical media or other types of non volatile storage media. For example in those embodiments the non volatile storage device may comprise a hard disk drive an optical storage drive or the like.

While the non volatile memory is referred to herein as memory media in various embodiments the non volatile memory may more generally comprise a non volatile recording media capable of recording data the non volatile recording media may be referred to as a non volatile memory media a non volatile storage media or the like. Further the non volatile storage device in various embodiments may comprise a non volatile recording device a non volatile memory device a non volatile storage device or the like.

The storage device also includes a storage controller that coordinates the storage and retrieval of data in the nonvolatile memory . The storage controller may use one or more indexes to locate and retrieve data and perform other operations on data stored in the storage device . For example the storage controller may include a groomer for performing data grooming operations such as garbage collection.

As shown the storage device in certain embodiments implements a write data pipeline and a read data pipeline an example of which is described in greater detail below with regard to . The write data pipeline may perform certain operations on data as the data is transferred from the host into the nonvolatile memory . These operations may include for example error correction code ECC generation encryption compression and others. The read data pipeline may perform similar and potentially inverse operations on data that is being read out of nonvolatile memory and sent to the host .

The storage device also includes a secondary power supply that provides power in the event of a complete or partial power disruption resulting in the storage device not receiving enough electrical power over the primary power connection . A power disruption is any event that unexpectedly causes the storage device to stop receiving power over the primary power connection or causes a significant reduction in the power received by the storage device over the primary power connection . A significant reduction in power in one embodiment includes the power falling below a predefined threshold. The predefined threshold in a further embodiment is selected to allow for normal fluctuations in the level of power from the primary power connection . For example the power to a building where the host and the storage device may go out. A user action such as improperly shutting down the host providing power to the storage device a failure in the primary power connection or a failure in the primary power supply may cause the storage device to stop receiving power. Numerous varied power disruptions may cause unexpected power loss for the storage device .

The secondary power supply may include one or more batteries one or more capacitors a bank of capacitors a separate connection to a power supply or the like. In one embodiment the secondary power supply provides power to the storage device for at least a power hold up time during a power disruption or other reduction in power from the primary power connection . The secondary power supply in a further embodiment provides a power hold up time long enough to enable the storage device to flush data that is not in nonvolatile memory into the nonvolatile memory . As a result the storage device can preserve the data that is not permanently stored in the storage device before the lack of power causes the storage device to stop functioning. In certain implementations the secondary power supply may comprise the smallest capacitors possible that are capable of providing a predefined power hold up time to preserve space reduce cost and simplify the storage device . In one embodiment one or more banks of capacitors are used to implement the secondary power supply as capacitors are generally more reliable require less maintenance and have a longer life than other options for providing secondary power.

In one embodiment the secondary power supply is part of an electrical circuit that automatically provides power to the storage device upon a partial or complete loss of power from the primary power connection . Similarly the system may be configured to automatically accept or receive electric power from the secondary power supply during a partial or complete power loss. For example in one embodiment the secondary power supply may be electrically coupled to the storage device in parallel with the primary power connection so that the primary power connection charges the secondary power supply during normal operation and the secondary power supply automatically provides power to the storage device in response to a power loss. In one embodiment the system further includes a diode or other reverse current protection between the secondary power supply and the primary power connection to prevent current from the secondary power supply from reaching the primary power connection . In another embodiment the power management apparatus may enable or connect the secondary power supply to the storage device using a switch or the like in response to reduced power from the primary power connection .

An example of data that is not yet in the nonvolatile memory may include data that may be held in volatile memory as the data moves through the write data pipeline . If data in the write data pipeline is lost during a power outage i.e. not written to nonvolatile memory or otherwise permanently stored corruption and data loss may result.

In certain embodiments the storage device sends an acknowledgement to the host at some point after the storage device receives data to be stored in the nonvolatile memory . The write data pipeline or a sub component thereof may generate the acknowledgement. It is advantageous for the storage device to send the acknowledgement as soon as possible after receiving the data.

In certain embodiments the write data pipeline sends the acknowledgement before data is actually stored in the nonvolatile memory . For example the write data pipeline may send the acknowledgement while the data is still in transit through the write data pipeline to the nonvolatile memory . In such embodiments it is highly desirable that the storage device flush all data for which the storage controller has sent an acknowledgement to the nonvolatile memory before the secondary power supply loses sufficient power in order to prevent data corruption and maintain the integrity of the acknowledgement sent.

In addition in certain embodiments some data within the write data pipeline may be corrupted as a result of the power disruption. A power disruption may include a power failure as well as unexpected changes in power levels supplied. The unexpected changes in power levels may place data that is in the storage device but not yet in nonvolatile memory at risk. Data corruption may begin to occur before the power management apparatus is even aware or notified that there has been a disruption in power.

For example the PCI e specification indicates that in the event that a power disruption is signaled data should be assumed corrupted and not stored in certain circumstances. Similar potential corruption may occur for storage devices connected to hosts using other connection types such as PCI serial advanced technology attachment serial ATA or SATA parallel ATA PATA small computer system interface SCSI IEEE 1394 FireWire Fiber Channel universal serial bus USB PCIe AS or the like. A complication may arise when a power disruption occurs meaning that data received from that point to the present time may be presumed corrupt a period of time passes the disruption is sensed and signaled and the power management apparatus receives the signal and becomes aware of the power disruption. The lag between the power disruption occurring and the power management apparatus discovering the power disruption can allow corrupt data to enter the write data pipeline . In certain embodiments this corrupt data should be identified and not stored to the nonvolatile memory . Alternately this corrupt data can be stored in the nonvolatile memory and marked as corrupt as described below. For simplicity of description identifying corrupt data and not storing the data to the nonvolatile memory will be primarily used to describe the functions and features herein. Furthermore the host should be aware that this data was not stored or alternatively data for which integrity is a question is not acknowledged until data integrity can be verified. As a result corrupt data should not be acknowledged.

The storage device also includes a power management apparatus . In certain embodiments the power management apparatus is implemented as part of the storage controller . The power management apparatus may be for instance a software driver or be implemented in firmware for the storage device . In other embodiments the power management apparatus may be implemented partially in a software driver and partially in the storage controller or the like. In one embodiment at least a portion of the power management apparatus is implemented on the storage device as part of the storage controller or the like so that the power management apparatus continues to function during a partial or complete power loss using power from the secondary power supply even if the host is no longer functioning.

In one embodiment the power management apparatus initiates a power loss mode in the storage device in response to a reduction in power from the primary power connection . During the power loss mode the power management apparatus in one embodiment flushes data that is in the storage device that is not yet stored in nonvolatile memory into the nonvolatile memory . In particular embodiments the power management apparatus flushes the data that has been acknowledged and is in the storage device that is not yet stored in nonvolatile memory into the nonvolatile memory . In certain embodiments described below the power management apparatus may adjust execution of data operations on the storage device to ensure that essential operations complete before the secondary power supply loses sufficient power to complete the essential operations i.e. during the power hold up time that the secondary power supply provides.

In certain embodiments the essential operations comprise those operations for data that has been acknowledged as having been stored such as acknowledged write operations. In other embodiments the essential operations comprise those operations for data that has been acknowledged as having been stored and erased. In other embodiments the essential operations comprise those operations for data that have been acknowledged as having been stored read and erased. The power management apparatus may also terminate non essential operations to ensure that those non essential operations do not consume power unnecessarily and or do not block essential operations from executing for example the power management apparatus may terminate erase operations read operations unacknowledged write operations and the like.

In one embodiment terminating non essential operations preserves power from the secondary power supply allowing the secondary power supply to provide the power hold up time. In a further embodiment the power management apparatus quiesces or otherwise shuts down operation of one or more subcomponents of the storage device during the power loss mode to conserve power from the secondary power supply . For example in various embodiments the power management apparatus may quiesce operation of the read data pipeline a read direct memory access DMA engine and or other subcomponents of the storage device that are associated with non essential operations.

The power management apparatus may also be responsible for determining what data was corrupted by the power disruption preventing the corrupt data from being stored in nonvolatile memory and ensuring that the host is aware that the corrupted data was never actually stored on the storage device . This prevents corruption of data in the storage device resulting from the power disruption.

In one embodiment the system includes a plurality of storage devices . The power management apparatus in one embodiment manages power loss modes for each storage device in the plurality of storage devices providing a system wide power loss mode for the plurality of storage devices . In a further embodiment each storage device in the plurality of storage devices includes a separate power management apparatus that manages a separate power loss mode for each individual storage device . The power management apparatus in one embodiment may quiesce or otherwise shut down one or more storage devices of the plurality of storage devices to conserve power from the secondary power supply for executing essential operations on one or more other storage devices .

In one embodiment the system includes one or more adapters for providing electrical connections between the host and the plurality of storage devices . An adapter in various embodiments may include a slot or port that receives a single storage device an expansion card or daughter card that receives two or more storage devices or the like. For example in one embodiment the plurality of storage devices may each be coupled to separate ports or slots of the host . In another example embodiment one or more adapters such as daughter cards or the like may be electrically coupled to the host i.e. connected to one or more slots or ports of the host and the one or more adapters may each provide connections for two or more storage devices .

In one embodiment the system includes a circuit board such as a motherboard or the like that receives two or more adapters such as daughter cards or the like and each adapter receives two or more storage devices . In a further embodiment the adapters are coupled to the circuit board using PCI e slots of the circuit board and the storage devices are coupled to the adapters using PCI e slots of the adapters. In another embodiment the storage devices each comprise a dual in line memory module DIMM of non volatile solid state storage such as Flash memory or the like. In one embodiment the circuit board the adapters and the storage devices may be external to the host and may include a separate primary power connection . For example the circuit board the adapters and the storage devices may be housed in an external enclosure with a power supply unit PSU and may be in communication with the host using an external bus such as eSATA eSATAp SCSI FireWire Fiber Channel USB PCIe AS or the like. In another embodiment the circuit board may be a motherboard of the host and the adapters and the storage devices may be internal storage of the host .

In view of this disclosure one of skill in the art will recognize many configurations of adapters and storage devices for use in the system . For example each adapter may receive two storage devices four storage devices or any number of storage devices. Similarly the system may include one adapter two adapters three adapters four adapters or any supported number of adapters. In one example embodiment the system includes two adapters and each adapter receives four storage devices for a total of eight storage devices .

In one embodiment the secondary power supply provides electric power to each of a plurality of storage devices . For example the secondary power supply may be disposed in a circuit on a main circuit board or motherboard and may provide power to several adapters. In a further embodiment the system includes a plurality of secondary power supplies that each provide electric power to a subset of a plurality of storage devices . For example in one embodiment each adapter may include a secondary power supply for storage devices of the adapter. In a further embodiment each storage device may include a secondary power supply for the storage device . In view of this disclosure one of skill in the art will recognize different arrangements of secondary power supplies for providing power to a plurality of storage devices .

In one embodiment at least one solid state controller is a field programmable gate array FPGA and controller functions are programmed into the FPGA. In a particular embodiment the FPGA is a Xilinx FPGA. In another embodiment the solid state storage controller comprises components specifically designed as a solid state storage controller such as an application specific integrated circuit ASIC or custom logic solution. Each solid state storage controller typically includes a write data pipeline and a read data pipeline which are describe further in relation to . In another embodiment at least one solid state storage controller is made up of a combination FPGA ASIC and custom logic components.

The solid state storage media is an array of non volatile solid state storage elements arranged in banks and accessed in parallel through a bi directional storage input output I O bus . The storage I O bus in one embodiment is capable of unidirectional communication at any one time. For example when data is being written to the solid state storage media data cannot be read from the solid state storage media . In another embodiment data can flow both directions simultaneously. However bi directional as used herein with respect to a data bus refers to a data pathway that can have data flowing in only one direction at a time but when data flowing one direction on the bi directional data bus is stopped data can flow in the opposite direction on the bi directional data bus.

A solid state storage element e.g. SSS 0.0 is typically configured as a chip a package of one or more dies or a die on a circuit board. As depicted a solid state storage element e.g. operates independently or semi independently of other solid state storage elements e.g. even if these several elements are packaged together in a chip package a stack of chip packages or some other package element. As depicted a row of solid state storage elements is designated as a bank . As depicted there may be n banks and m solid state storage elements per bank in an array of n m solid state storage elements in a solid state storage media . Of course different embodiments may include different values for n and m. In one embodiment a solid state storage media includes twenty solid state storage elements per bank with eight banks . In one embodiment the solid state storage media includes twenty four solid state storage elements per bank with eight banks . In addition to the n m storage elements one or more additional columns P may also be addressed and operated in parallel with other solid state storage elements for one or more rows. The added P columns in one embodiment store parity data for the portions of an ECC chunk i.e. an ECC codeword that span m storage elements for a particular bank. In one embodiment each solid state storage element is comprised of single level cell SLC devices. In another embodiment each solid state storage element is comprised of multi level cell MLC devices.

In one embodiment solid state storage elements that share a common line on the storage I O bus e.g. are packaged together. In one embodiment a solid state storage element may have one or more dies per package with one or more packages stacked vertically and each die may be accessed independently. In another embodiment a solid state storage element e.g. SSS 0.0 may have one or more virtual dies per die and one or more dies per package and one or more packages stacked vertically and each virtual die may be accessed independently. In another embodiment a solid state storage element SSS 0.0 may have one or more virtual dies per die and one or more dies per package with some or all of the one or more dies stacked vertically and each virtual die may be accessed independently.

In one embodiment two dies are stacked vertically with four stacks per group to form eight storage elements e.g. SSS 0.0 SSS 8.0 . . . each in a separate bank . . . . In another embodiment 24 storage elements e.g. SSS 0.0 SSS 0.24 . . . form a logical bank so that each of the eight logical banks has 24 storage elements e.g. SSS 0.0 SSS 8.24 . Data is sent to the solid state storage media over the storage I O bus to all storage elements of a particular group of storage elements SSS 0.0 SSS 8.0 . The storage control bus is used to select a particular bank e.g. Bank 0 so that the data received over the storage I O bus connected to all banks is written just to the selected bank

In one embodiment the storage I O bus is comprised of one or more independent I O buses IIOBa m comprising . . . wherein the solid state storage elements within each column share one of the independent I O buses that are connected to each solid state storage element in parallel. For example one independent I O bus of the storage I O bus may be physically connected to a first solid state storage element of each bank . A second independent I O bus of the storage I O bus may be physically connected to a second solid state storage element of each bank . Each solid state storage element in a bank a row of solid state storage elements as illustrated in may be accessed simultaneously and or in parallel. In one embodiment where solid state storage elements comprise stacked packages of dies all packages in a particular stack are physically connected to the same independent I O bus. As used herein simultaneously also includes near simultaneous access where devices are accessed at slightly different intervals to avoid switching noise. Simultaneously is used in this context to be distinguished from a sequential or serial access wherein commands and or data are sent individually one after the other.

Typically banks are independently selected using the storage control bus . In one embodiment a bank is selected using a chip enable or chip select. Where both chip select and chip enable are available the storage control bus may select one package within a stack of packages. In other embodiments other commands are used by the storage control bus to individually select one package within a stack of packages. Solid state storage elements may also be selected through a combination of control signals and address information transmitted on storage I O bus and the storage control bus .

In one embodiment each solid state storage element is partitioned into erase blocks and each erase block is partitioned into pages. An erase block on a solid state storage element may be called a physical erase block or PEB. A typical page is 2048 bytes 2 kB . In one example a solid state storage element e.g. SSS 0.0 includes two registers and can program two pages so that a two register solid state storage element has a capacity of 4 kB. A bank of 20 solid state storage elements would then have an 80 kB capacity of pages accessed with the same address going out the independent I O buses of the storage I O bus .

This group of pages in a bank of solid state storage elements . . . of 80 kB may be called a logical page or virtual page. Similarly an erase block of each storage element . . . of a bank may be grouped to form a logical erase block which may also be called a virtual erase block . In one embodiment an erase block of pages within a solid state storage element is erased when an erase command is received within the solid state storage element. Whereas the size and number of erase blocks pages planes or other logical and physical divisions within a solid state storage element are expected to change over time with advancements in technology it is to be expected that many embodiments consistent with new configurations are possible and are consistent with the general description herein.

Typically when a packet is written to a particular location within a solid state storage element wherein the packet is intended to be written to a location within a particular page which is specific to a particular physical erase block of a particular storage element of a particular bank a physical address is sent on the storage I O bus and is followed by the packet. The physical address contains enough information for the solid state storage element to direct the packet to the designated location within the page. Since all storage elements in a column of storage elements e.g. SSS 0.0 SSS N.0 . . . are connected to the same independent I 0 bus e.g. . of the storage I O bus to reach the proper page and to avoid writing the data packet to similarly addressed pages in the column of storage elements SSS 0.0 SSS N.0 . . . the bank that includes the solid state storage element SSS 0.0 with the correct page where the data packet is to be written is selected by the storage control bus and other banks . . . of the solid state storage are deselected.

Similarly satisfying a read command on the storage I O bus requires a signal on the storage control bus to select a single bank and the appropriate page within that bank . In one embodiment a read command reads an entire page and because there are multiple solid state storage elements . . . in parallel in a bank an entire logical page is read with a read command. However the read command may be broken into subcommands as will be explained below with respect to bank interleave. Similarly an entire logical page may be written to the solid state storage elements . . . of a bank in a write operation.

An erase block erase command may be sent out to erase an erase block over the storage I O bus with a particular erase block address to erase a particular erase block. Typically storage controller may send an erase block erase command over the parallel paths independent I O buses of the storage I O bus to erase a logical erase block each with a particular erase block address to erase a particular erase block. Simultaneously a particular bank e.g. Bank 0 is selected over the storage control bus to prevent erasure of similarly addressed erase blocks in non selected banks e.g. Banks 1 N . Alternatively no particular bank e.g. Bank 0 is selected over the storage control bus or all of the banks are selected to enable erasure of similarly addressed erase blocks in all of the banks Banks 1 N in parallel. Other commands may also be sent to a particular location using a combination of the storage I O bus and the storage control bus . One of skill in the art will recognize other ways to select a particular storage location using the bi directional storage I O bus and the storage control bus .

In one embodiment packets are written sequentially to the solid state storage media . For example storage controller streams packets to storage write buffers of a bank of storage elements and when the buffers are full the packets are programmed to a designated logical page. Storage controller then refills the storage write buffers with packets and when full the packets are written to the next logical page. The next logical page may be in the same bank or another bank e.g. . This process continues logical page after logical page typically until a logical erase block is filled. In another embodiment the streaming may continue across logical erase block boundaries with the process continuing logical erase block after logical erase block.

In a read modify write operation data packets associated with requested data are located and read in a read operation. Data segments of the modified requested data that have been modified are not written to the location from which they are read. Instead the modified data segments are again converted to data packets and then written sequentially to the next available location in the logical page currently being written. The index entries for the respective data packets are modified to point to the packets that contain the modified data segments. The entry or entries in the index for data packets associated with the same requested data that have not been modified will include pointers to original location of the unmodified data packets. Thus if the original requested data is maintained for example to maintain a previous version of the requested data the original requested data will have pointers in the index to all data packets as originally written. The new requested data will have pointers in the index to some of the original data packets and pointers to the modified data packets in the logical page that is currently being written.

In a copy operation the index includes an entry for the original requested data mapped to a number of packets stored in the solid state storage media . When a copy is made a new copy of the requested data is created and a new entry is created in the index mapping the new copy of the requested data to the original packets. The new copy of the requested data is also written to the solid state storage media with its location mapped to the new entry in the index. The new copy of the requested data packets may be used to identify the packets within the original requested data that are referenced in case changes have been made in the original requested data that have not been propagated to the copy of the requested data and the index is lost or corrupted.

Beneficially sequentially writing packets facilitates a more even use of the solid state storage media and allows the solid storage device controller to monitor storage hot spots and level usage of the various logical pages in the solid state storage media . Sequentially writing packets also facilitates a powerful efficient garbage collection system which is described in detail below. One of skill in the art will recognize other benefits of sequential storage of data packets.

In various embodiments the solid state storage device controller also includes a data bus a local bus a buffer controller buffers 0 N a master controller a direct memory access DMA controller a memory controller a dynamic memory array a static random memory array a management controller a management bus a bridge to a system bus and miscellaneous logic which are described below. In other embodiments the system bus is coupled to one or more network interface cards NICs some of which may include remote DMA RDMA controllers one or more central processing unit CPU one or more external memory controllers and associated external memory arrays one or more storage controllers peer controllers and application specific processors which are described below. The components connected to the system bus may be located in the host computing system or may be other devices.

Typically the solid state storage controller s communicate data to the solid state storage media over a storage I O bus . In a typical embodiment where the solid state storage is arranged in banks and each bank includes multiple storage elements accessed in parallel the storage I O bus is an array of busses one for each column of storage elements spanning the banks . As used herein the term storage I O bus may refer to one storage I O bus or an array of independent data busses wherein individual data busses of the array independently communicate different data relative to one another. In one embodiment each storage I O bus accessing a column of storage elements e.g. may include a logical to physical mapping for storage divisions e.g. erase blocks accessed in a column of storage elements . This mapping or bad block remapping allows a logical address mapped to a physical address of a storage division to be remapped to a different storage division if the first storage division fails partially fails is inaccessible or has some other problem.

Data may also be communicated to the solid state storage controller s from a requesting device through the system bus bridge local bus buffer s and finally over a data bus . The data bus typically is connected to one or more buffers controlled with a buffer controller . The buffer controller typically controls transfer of data from the local bus to the buffers and through the data bus to the pipeline input buffer and output buffer . The buffer controller typically controls how data arriving from a requesting device can be temporarily stored in a buffer and then transferred onto a data bus or vice versa to account for different clock domains to prevent data collisions etc. The buffer controller typically works in conjunction with the master controller to coordinate data flow. As data arrives the data will arrive on the system bus be transferred to the local bus through a bridge .

Typically the data is transferred from the local bus to one or more data buffers as directed by the master controller and the buffer controller . The data then flows out of the buffer s to the data bus through a solid state controller and on to the solid state storage media such as NAND flash or other storage media. In one embodiment data and associated out of band metadata metadata arriving with the data is communicated using one or more data channels comprising one or more solid state storage controllers 1 and associated solid state storage media 1 while at least one channel solid state storage controller solid state storage media is dedicated to in band metadata such as index information and other metadata generated internally to the solid state storage device .

The local bus is typically a bidirectional bus or set of busses that allows for communication of data and commands between devices internal to the solid state storage device controller and between devices internal to the solid state storage device and devices connected to the system bus . The bridge facilitates communication between the local bus and system bus . One of skill in the art will recognize other embodiments such as ring structures or switched star configurations and functions of buses and bridges .

The system bus is typically a bus of a host computing system or other device in which the solid state storage device is installed or connected. In one embodiment the system bus may be a PCI e bus a Serial Advanced Technology Attachment serial ATA bus parallel ATA or the like. In another embodiment the system bus is an external bus such as small computer system interface SCSI FireWire Fiber Channel USB PCIe AS or the like. The solid state storage device may be packaged to fit internally to a device or as an externally connected device.

The solid state storage device controller includes a master controller that controls higher level functions within the solid state storage device . The master controller in various embodiments controls data flow by interpreting object requests and other requests directs creation of indexes to map object identifiers associated with data to physical locations of associated data coordinating DMA requests etc. Many of the functions described herein are controlled wholly or in part by the master controller .

In one embodiment the master controller uses embedded controller s . In another embodiment the master controller uses local memory such as a dynamic memory array dynamic random access memory DRAM a static memory array static random access memory SRAM etc. In one embodiment the local memory is controlled using the master controller . In another embodiment the master controller accesses the local memory via a memory controller . In another embodiment the master controller runs a Linux server and may support various common server interfaces such as the World Wide Web hyper text markup language HTML etc. In another embodiment the master controller uses a nano processor. The master controller may be constructed using programmable or standard logic or any combination of controller types listed above. One skilled in the art will recognize many embodiments for the master controller .

In one embodiment where the storage device solid state storage device controller manages multiple data storage devices solid state storage media the master controller divides the work load among internal controllers such as the solid state storage controllers . For example the master controller may divide an object to be written to the data storage devices e.g. solid state storage media so that a portion of the object is stored on each of the attached data storage devices. This feature is a performance enhancement allowing quicker storage and access to an object. In one embodiment the master controller is implemented using an FPGA. In another embodiment the firmware within the master controller may be updated through the management bus the system bus over a network connected to a NIC or other device connected to the system bus .

In one embodiment the master controller which manages objects emulates block storage such that a host computing system or other device connected to the storage device solid state storage device views the storage device solid state storage device as a block storage device and sends data to specific physical addresses in the storage device solid state storage device . The master controller then divides up the blocks and stores the data blocks as it would objects. The master controller then maps the blocks and physical address sent with the block to the actual locations determined by the master controller . The mapping is stored in the object index. Typically for block emulation a block device application program interface API is provided in a driver in a computer such as the host computing system or other device wishing to use the storage device solid state storage device as a block storage device.

In another embodiment the master controller coordinates with NIC controllers and embedded RDMA controllers to deliver just in time RDMA transfers of data and command sets. NIC controller may be hidden behind a non transparent port to enable the use of custom drivers. Also a driver on a host computing system may have access to the computer network through an I O memory driver using a standard stack API and operating in conjunction with NICs .

In one embodiment the master controller is also a redundant array of independent drive RAID controller. Where the data storage device solid state storage device is networked with one or more other data storage devices solid state storage devices the master controller may be a RAID controller for single tier RAID multi tier RAID progressive RAID etc. The master controller also allows some objects to be stored in a RAID array and other objects to be stored without RAID. In another embodiment the master controller may be a distributed RAID controller element. In another embodiment the master controller may comprise many RAID distributed RAID and other functions as described elsewhere. In one embodiment the master controller controls storage of data in a RAID like structure where parity information is stored in one or more storage elements of a logical page where the parity information protects data stored in the other storage elements of the same logical page.

In one embodiment the master controller coordinates with single or redundant network managers e.g. switches to establish routing to balance bandwidth utilization failover etc. In another embodiment the master controller coordinates with integrated application specific logic via local bus and associated driver software. In another embodiment the master controller coordinates with attached application specific processors or logic via the external system bus and associated driver software. In another embodiment the master controller coordinates with remote application specific logic via the computer network and associated driver software. In another embodiment the master controller coordinates with the local bus or external bus attached hard disk drive HDD storage controller.

In one embodiment the master controller communicates with one or more storage controllers where the storage device solid state storage device may appear as a storage device connected through a SCSI bus Internet SCSI iSCSI fiber channel etc. Meanwhile the storage device solid state storage device may autonomously manage objects and may appear as an object file system or distributed object file system. The master controller may also be accessed by peer controllers and or application specific processors .

In another embodiment the master controller coordinates with an autonomous integrated management controller to periodically validate FPGA code and or controller software validate FPGA code while running reset and or validate controller software during power on reset support external reset requests support reset requests due to watchdog timeouts and support voltage current power temperature and other environmental measurements and setting of threshold interrupts. In another embodiment the master controller manages garbage collection to free erase blocks for reuse. In another embodiment the master controller manages wear leveling. In another embodiment the master controller allows the data storage device solid state storage device to be partitioned into multiple logical devices and allows partition based media encryption. In yet another embodiment the master controller supports a solid state storage controller with advanced multi bit ECC correction. One of skill in the art will recognize other features and functions of a master controller in a storage controller or more specifically in a solid state storage device .

In one embodiment the solid state storage device controller includes a memory controller which controls a dynamic random memory array and or a static random memory array . As stated above the memory controller may be independent or integrated with the master controller . The memory controller typically controls volatile memory of some type such as DRAM dynamic random memory array and SRAM static random memory array . In other examples the memory controller also controls other memory types such as electrically erasable programmable read only memory EEPROM etc. In other embodiments the memory controller controls two or more memory types and the memory controller may include more than one controller. Typically the memory controller controls as much SRAM as is feasible and by DRAM to supplement the SRAM .

In one embodiment the object index is stored in memory and then periodically off loaded to a channel of the solid state storage media or other non volatile memory. One of skill in the art will recognize other uses and configurations of the memory controller dynamic memory array and static memory array .

In one embodiment the solid state storage device controller includes a DMA controller that controls DMA operations between the storage device solid state storage device and one or more external memory controllers and associated external memory arrays and CPUs . Note that the external memory controllers and external memory arrays are called external because they are external to the storage device solid state storage device . In addition the DMA controller may also control RDMA operations with requesting devices through a NIC and associated RDMA controller .

In one embodiment the solid state storage device controller includes a management controller connected to a management bus . Typically the management controller manages environmental metrics and status of the storage device solid state storage device . The management controller may monitor device temperature fan speed power supply settings etc. over the management bus . The management controller may support the reading and programming of erasable programmable read only memory EEPROM for storage of FPGA code and controller software. Typically the management bus is connected to the various components within the storage device solid state storage device . The management controller may communicate alerts interrupts etc. over the local bus or may include a separate connection to a system bus or other bus. In one embodiment the management bus is an Inter Integrated Circuit I2C bus. One of skill in the art will recognize other related functions and uses of a management controller connected to components of the storage device solid state storage device by a management bus .

In one embodiment the solid state storage device controller includes miscellaneous logic that may be customized for a specific application. Typically where the solid state device controller or master controller is are configured using a FPGA or other configurable controller custom logic may be included based on a particular application customer requirement storage requirement etc.

The write data pipeline includes a packetizer that receives a data or metadata segment to be written to the solid state storage either directly or indirectly through another write data pipeline stage and creates one or more packets sized for the solid state storage media . The data or metadata segment is typically part of a data structure such as an object but may also include an entire data structure. In another embodiment the data segment is part of a block of data but may also include an entire block of data. Typically a set of data such as a data structure is received from a computer such as the host or other computer or device and is transmitted to the solid state storage device in data segments streamed to the solid state storage device . A data segment may also be known by another name such as data parcel but as referenced herein includes all or a portion of a data structure or data block.

Each data structure is stored as one or more packets. Each data structure may have one or more container packets. Each packet contains a header. The header may include a header type field. Type fields may include data attribute metadata data segment delimiters multi packet data structures data linkages and the like. The header may also include information regarding the size of the packet such as the number of bytes of data included in the packet. The length of the packet may be established by the packet type. The header may include information that establishes the relationship of the packet to a data structure. An example might be the use of an offset in a data packet header to identify the location of the data segment within the data structure. One of skill in the art will recognize other information that may be included in a header added to data by a packetizer and other information that may be added to a data packet.

Each packet includes a header and possibly data from the data or metadata segment. The header of each packet includes pertinent information to relate the packet to the data structure to which the packet belongs. For example the header may include an object identifier or other data structure identifier and offset that indicates the data segment object data structure or data block from which the data packet was formed. The header may also include a logical address used by the storage bus controller to store the packet. The header may also include information regarding the size of the packet such as the number of bytes included in the packet. The header may also include a sequence number that identifies where the data segment belongs with respect to other packets within the data structure when reconstructing the data segment or data structure. The header may include a header type field. Type fields may include data data structure attributes metadata data segment delimiters multi packet data structure types data structure linkages and the like. One of skill in the art will recognize other information that may be included in a header added to data or metadata by a packetizer and other information that may be added to a packet.

The write data pipeline includes an ECC generator that that generates one or more error correcting codes ECC for the one or more packets received from the packetizer . The ECC generator typically uses an error correcting algorithm to generate ECC check bits which are stored with the one or more data packets. The ECC codes generated by the ECC generator together with the one or more data packets associated with the ECC codes comprise an ECC chunk. The ECC data stored with the one or more data packets is used to detect and to correct errors introduced into the data through transmission and storage. In one embodiment packets are streamed into the ECC generator as un encoded blocks of length N. A syndrome of length S is calculated appended and output as an encoded block of length N S. The value of N and S are dependent upon the characteristics of the ECC algorithm which is selected to achieve specific performance efficiency and robustness metrics. In one embodiment there is no fixed relationship between the ECC blocks and the packets the packet may comprise more than one ECC block the ECC block may comprise more than one packet and a first packet may end anywhere within the ECC block and a second packet may begin after the end of the first packet within the same ECC block. In one embodiment ECC algorithms are not dynamically modified. In one embodiment the ECC data stored with the data packets is robust enough to correct errors in more than two bits.

Beneficially using a robust ECC algorithm allowing more than single bit correction or even double bit correction allows the life of the solid state storage media to be extended. For example if flash memory is used as the storage medium in the solid state storage media the flash memory may be written approximately 100 000 times without error per erase cycle. This usage limit may be extended using a robust ECC algorithm. Having the ECC generator and corresponding ECC correction module onboard the solid state storage device the solid state storage device can internally correct errors and has a longer useful life than if a less robust ECC algorithm is used such as single bit correction. However in other embodiments the ECC generator may use a less robust algorithm and may correct single bit or double bit errors. In another embodiment the solid state storage device may comprise less reliable storage such as multi level cell MLC flash in order to increase capacity which storage may not be sufficiently reliable without more robust ECC algorithms.

In one embodiment the write pipeline includes an input buffer that receives a data segment to be written to the solid state storage media and stores the incoming data segments until the next stage of the write data pipeline such as the packetizer or other stage for a more complex write data pipeline is ready to process the next data segment. The input buffer typically allows for discrepancies between the rate data segments are received and processed by the write data pipeline using an appropriately sized data buffer. The input buffer also allows the data bus to transfer data to the write data pipeline at rates greater than can be sustained by the write data pipeline in order to improve efficiency of operation of the data bus . Typically when the write data pipeline does not include an input buffer a buffering function is performed elsewhere such as in the solid state storage device but outside the write data pipeline in the host such as within a network interface card NIC or at another device for example when using remote direct memory access RDMA .

In another embodiment the write data pipeline also includes a write synchronization buffer that buffers packets received from the ECC generator prior to writing the packets to the solid state storage media . The write synchronization buffer is located at a boundary between a local clock domain and a solid state storage clock domain and provides buffering to account for the clock domain differences. In other embodiments synchronous solid state storage media may be used and synchronization buffers may be eliminated.

In one embodiment the write data pipeline also includes a media encryption module that receives the one or more packets from the packetizer either directly or indirectly and encrypts the one or more packets using an encryption key unique to the solid state storage device prior to sending the packets to the ECC generator . Typically the entire packet is encrypted including the headers. In another embodiment headers are not encrypted. In this document encryption key is understood to mean a secret encryption key that is managed externally from a solid state storage controller .

The media encryption module and corresponding media decryption module provide a level of security for data stored in the solid state storage media . For example where data is encrypted with the media encryption module if the solid state storage media is connected to a different solid state storage controller solid state storage device or server the contents of the solid state storage media typically could not be read without use of the same encryption key used during the write of the data to the solid state storage media without significant effort.

In a typical embodiment the solid state storage device does not store the encryption key in non volatile storage and allows no external access to the encryption key. The encryption key is provided to the solid state storage controller during initialization. The solid state storage device may use and store a non secret cryptographic nonce that is used in conjunction with an encryption key. A different nonce may be stored with every packet. Data segments may be split between multiple packets with unique nonces for the purpose of improving protection by the encryption algorithm.

The encryption key may be received from a host a server key manager or other device that manages the encryption key to be used by the solid state storage controller . In another embodiment the solid state storage media may have two or more partitions and the solid state storage controller behaves as though it was two or more solid state storage controllers each operating on a single partition within the solid state storage media . In this embodiment a unique media encryption key may be used with each partition.

In another embodiment the write data pipeline also includes an encryption module that encrypts a data or metadata segment received from the input buffer either directly or indirectly prior sending the data segment to the packetizer the data segment encrypted using an encryption key received in conjunction with the data segment. The encryption keys used by the encryption module to encrypt data may not be common to all data stored within the solid state storage device but may vary on an per data structure basis and received in conjunction with receiving data segments as described below. For example an encryption key for a data segment to be encrypted by the encryption module may be received with the data segment or may be received as part of a command to write a data structure to which the data segment belongs. The solid sate storage device may use and store a non secret cryptographic nonce in each data structure packet that is used in conjunction with the encryption key. A different nonce may be stored with every packet. Data segments may be split between multiple packets with unique nonces for the purpose of improving protection by the encryption algorithm.

The encryption key may be received from a host another computer key manager or other device that holds the encryption key to be used to encrypt the data segment. In one embodiment encryption keys are transferred to the solid state storage controller from one of a solid state storage device host computer or other external agent which has the ability to execute industry standard methods to securely transfer and protect private and public keys.

In one embodiment the encryption module encrypts a first packet with a first encryption key received in conjunction with the packet and encrypts a second packet with a second encryption key received in conjunction with the second packet. In another embodiment the encryption module encrypts a first packet with a first encryption key received in conjunction with the packet and passes a second data packet on to the next stage without encryption. Beneficially the encryption module included in the write data pipeline of the solid state storage device allows data structure by data structure or segment by segment data encryption without a single file system or other external system to keep track of the different encryption keys used to store corresponding data structures or data segments. Each requesting device or related key manager independently manages encryption keys used to encrypt only the data structures or data segments sent by the requesting device .

In one embodiment the encryption module may encrypt the one or more packets using an encryption key unique to the solid state storage device . The encryption module may perform this media encryption independently or in addition to the encryption described above. Typically the entire packet is encrypted including the headers. In another embodiment headers are not encrypted. The media encryption by the encryption module provides a level of security for data stored in the solid state storage media . For example where data is encrypted with media encryption unique to the specific solid state storage device if the solid state storage media is connected to a different solid state storage controller solid state storage device or host the contents of the solid state storage media typically could not be read without use of the same encryption key used during the write of the data to the solid state storage media without significant effort.

In another embodiment the write data pipeline includes a compression module that compresses the data for metadata segment prior to sending the data segment to the packetizer . The compression module typically compresses a data or metadata segment using a compression routine known to those of skill in the art to reduce the storage size of the segment. For example if a data segment includes a string of 512 zeros the compression module may replace the 512 zeros with code or token indicating the 512 zeros where the code is much more compact than the space taken by the 512 zeros.

In one embodiment the compression module compresses a first segment with a first compression routine and passes along a second segment without compression. In another embodiment the compression module compresses a first segment with a first compression routine and compresses the second segment with a second compression routine. Having this flexibility within the solid state storage device is beneficial so that hosts or other devices writing data to the solid state storage device may each specify a compression routine or so that one can specify a compression routine while another specifies no compression. Selection of compression routines may also be selected according to default settings on a per data structure type or data structure class basis. For example a first data structure of a specific data structure may be able to override default compression routine settings and a second data structure of the same data structure class and data structure type may use the default compression routine and a third data structure of the same data structure class and data structure type may use no compression.

In one embodiment the write data pipeline includes a garbage collector bypass that receives data segments from the read data pipeline as part of a data bypass in a garbage collection system. A garbage collection system typically marks packets that are no longer valid typically because the packet is marked for deletion or has been modified and the modified data is stored in a different location. At some point the garbage collection system determines that a particular section of storage may be recovered. This determination may be due to a lack of available storage capacity the percentage of data marked as invalid reaching a threshold a consolidation of valid data an error detection rate for that section of storage reaching a threshold or improving performance based on data distribution etc. Numerous factors may be considered by a garbage collection algorithm to determine when a section of storage is to be recovered.

Once a section of storage has been marked for recovery valid packets in the section typically must be relocated. The garbage collector bypass allows packets to be read into the read data pipeline and then transferred directly to the write data pipeline without being routed out of the solid state storage controller . In one embodiment the garbage collector bypass is part of an autonomous garbage collector system that operates within the solid state storage device . This allows the solid state storage device to manage data so that data is systematically spread throughout the solid state storage media to improve performance data reliability and to avoid overuse and underuse of any one location or area of the solid state storage media and to lengthen the useful life of the solid state storage media .

The garbage collector bypass coordinates insertion of segments into the write data pipeline with other segments being written by hosts or other devices. In the depicted embodiment the garbage collector bypass is before the packetizer in the write data pipeline and after the depacketizer in the read data pipeline but may also be located elsewhere in the read and write data pipelines . The garbage collector bypass may be used during a flush of the write pipeline to fill the remainder of the logical page in order to improve the efficiency of storage within the solid state storage media and thereby reduce the frequency of garbage collection.

In one embodiment the write data pipeline includes a write buffer that buffers data for efficient write operations. Typically the write buffer includes enough capacity for packets to fill at least one logical page in the solid state storage media . This allows a write operation to send an entire page of data to the solid state storage media without interruption. By sizing the write buffer of the write data pipeline and buffers within the read data pipeline to be the same capacity or larger than a storage write buffer within the solid state storage media writing and reading data is more efficient since a single write command may be crafted to send a full logical page of data to the solid state storage media instead of multiple commands.

While the write buffer is being filled the solid state storage media may be used for other read operations. This is advantageous because other solid state devices with a smaller write buffer or no write buffer may tie up the solid state storage when data is written to a storage write buffer and data flowing into the storage write buffer stalls. Read operations will be blocked until the entire storage write buffer is filled and programmed. Another approach for systems without a write buffer or a small write buffer is to flush the storage write buffer that is not full in order to enable reads. Again this is inefficient because multiple write program cycles are required to fill a page.

For depicted embodiment with a write buffer sized larger than a logical page a single write command which includes numerous subcommands can then be followed by a single program command to transfer the page of data from the storage write buffer in each solid state storage element to the designated page within each solid state storage element . This technique has the benefits of eliminating partial page programming which is known to reduce data reliability and durability and freeing up the destination bank for reads and other commands while the buffer fills.

In one embodiment the write buffer is a ping pong buffer where one side of the buffer is filled and then designated for transfer at an appropriate time while the other side of the ping pong buffer is being filled. In another embodiment the write buffer includes a first in first out FIFO register with a capacity of more than a logical page of data segments. One of skill in the art will recognize other write buffer configurations that allow a logical page of data to be stored prior to writing the data to the solid state storage media .

In another embodiment the write buffer is sized smaller than a logical page so that less than a page of information could be written to a storage write buffer in the solid state storage media . In the embodiment to prevent a stall in the write data pipeline from holding up read operations data is queued using the garbage collection system that needs to be moved from one location to another as part of the garbage collection process. In case of a data stall in the write data pipeline the data can be fed through the garbage collector bypass to the write buffer and then on to the storage write buffer in the solid state storage media to fill the pages of a logical page prior to programming the data. In this way a data stall in the write data pipeline would not stall reading from the solid state storage device .

In another embodiment the write data pipeline includes a write program module with one or more user definable functions within the write data pipeline . The write program module allows a user to customize the write data pipeline . A user may customize the write data pipeline based on a particular data requirement or application. Where the solid state storage controller is an FPGA the user may program the write data pipeline with custom commands and functions relatively easily. A user may also use the write program module to include custom functions with an ASIC however customizing an ASIC may be more difficult than with an FPGA. The write program module may include buffers and bypass mechanisms to allow a first data segment to execute in the write program module while a second data segment may continue through the write data pipeline . In another embodiment the write program module may include a processor core that can be programmed through software.

Note that the write program module is shown between the input buffer and the compression module however the write program module could be anywhere in the write data pipeline and may be distributed among the various stages . In addition there may be multiple write program modules distributed among the various states that are programmed and operate independently. In addition the order of the stages may be altered. One of skill in the art will recognize workable alterations to the order of the stages based on particular user requirements.

The read data pipeline includes an ECC correction module that determines if a data error exists in ECC blocks a requested packet received from the solid state storage media by using ECC stored with each ECC block of the requested packet. The ECC correction module then corrects any errors in the requested packet if any error exists and the errors are correctable using the ECC. For example if the ECC can detect an error in six bits but can only correct three bit errors the ECC correction module corrects ECC blocks of the requested packet with up to three bits in error. The ECC correction module corrects the bits in error by changing the bits in error to the correct one or zero state so that the requested data packet is identical to when it was written to the solid state storage media and the ECC was generated for the packet.

If the ECC correction module determines that the requested packets contains more bits in error than the ECC can correct the ECC correction module cannot correct the errors in the corrupted ECC blocks of the requested packet and sends an interrupt. In one embodiment the ECC correction module sends an interrupt with a message indicating that the requested packet is in error. The message may include information that the ECC correction module cannot correct the errors or the inability of the ECC correction module to correct the errors may be implied. In another embodiment the ECC correction module sends the corrupted ECC blocks of the requested packet with the interrupt and or the message.

In one embodiment a corrupted ECC block or portion of a corrupted ECC block of the requested packet that cannot be corrected by the ECC correction module is read by the master controller corrected and returned to the ECC correction module for further processing by the read data pipeline . In one embodiment a corrupted ECC block or portion of a corrupted ECC block of the requested packet is sent to the device requesting the data. The requesting device may correct the ECC block or replace the data using another copy such as a backup or mirror copy and then may use the replacement data of the requested data packet or return it to the read data pipeline . The requesting device may use header information in the requested packet in error to identify data required to replace the corrupted requested packet or to replace the data structure to which the packet belongs. In another embodiment the solid state storage controller stores data using some type of RAID and is able to recover the corrupted data. In another embodiment the ECC correction module sends an interrupt and or message and the receiving device fails the read operation associated with the requested data packet. One of skill in the art will recognize other options and actions to be taken as a result of the ECC correction module determining that one or more ECC blocks of the requested packet are corrupted and that the ECC correction module cannot correct the errors.

The read data pipeline includes a depacketizer that receives ECC blocks of the requested packet from the ECC correction module directly or indirectly and checks and removes one or more packet headers. The depacketizer may validate the packet headers by checking packet identifiers data length data location etc. within the headers. In one embodiment the header includes a hash code that can be used to validate that the packet delivered to the read data pipeline is the requested packet. The depacketizer also removes the headers from the requested packet added by the packetizer . The depacketizer may directed to not operate on certain packets but pass these forward without modification. An example might be a container label that is requested during the course of a rebuild process where the header information is required for index reconstruction. Further examples include the transfer of packets of various types destined for use within the solid state storage device . In another embodiment the depacketizer operation may be packet type dependent.

The read data pipeline includes an alignment module that receives data from the depacketizer and removes unwanted data. In one embodiment a read command sent to the solid state storage media retrieves a packet of data. A device requesting the data may not require all data within the retrieved packet and the alignment module removes the unwanted data. If all data within a retrieved page is requested data the alignment module does not remove any data.

The alignment module re formats the data as data segments of a data structure in a form compatible with a device requesting the data segment prior to forwarding the data segment to the next stage. Typically as data is processed by the read data pipeline the size of data segments or packets changes at various stages. The alignment module uses received data to format the data into data segments suitable to be sent to the requesting device and joined to form a response. For example data from a portion of a first data packet may be combined with data from a portion of a second data packet. If a data segment is larger than a data requested by the requesting device the alignment module may discard the unwanted data.

In one embodiment the read data pipeline includes a read synchronization buffer that buffers one or more requested packets read from the solid state storage media prior to processing by the read data pipeline . The read synchronization buffer is at the boundary between the solid state storage clock domain and the local bus clock domain and provides buffering to account for the clock domain differences.

In another embodiment the read data pipeline includes an output buffer that receives requested packets from the alignment module and stores the packets prior to transmission to the requesting device . The output buffer accounts for differences between when data segments are received from stages of the read data pipeline and when the data segments are transmitted to other parts of the solid state storage controller or to the requesting device . The output buffer also allows the data bus to receive data from the read data pipeline at rates greater than can be sustained by the read data pipeline in order to improve efficiency of operation of the data bus .

In one embodiment the read data pipeline includes a media decryption module that receives one or more encrypted requested packets from the ECC correction module and decrypts the one or more requested packets using the encryption key unique to the solid state storage device prior to sending the one or more requested packets to the depacketizer . Typically the encryption key used to decrypt data by the media decryption module is identical to the encryption key used by the media encryption module . In another embodiment the solid state storage media may have two or more partitions and the solid state storage controller behaves as though it was two or more solid state storage controllers each operating on a single partition within the solid state storage media . In this embodiment a unique media encryption key may be used with each partition.

In another embodiment the read data pipeline includes a decryption module that decrypts a data segment formatted by the depacketizer prior to sending the data segment to the output buffer . The data segment may be decrypted using an encryption key received in conjunction with the read request that initiates retrieval of the requested packet received by the read synchronization buffer . The decryption module may decrypt a first packet with an encryption key received in conjunction with the read request for the first packet and then may decrypt a second packet with a different encryption key or may pass the second packet on to the next stage of the read data pipeline without decryption. When the packet was stored with a non secret cryptographic nonce the nonce is used in conjunction with an encryption key to decrypt the data packet. The encryption key may be received from a host a computer key manager or other device that manages the encryption key to be used by the solid state storage controller .

In another embodiment the read data pipeline includes a decompression module that decompresses a data segment formatted by the depacketizer . In one embodiment the decompression module uses compression information stored in one or both of the packet header and the container label to select a complementary routine to that used to compress the data by the compression module . In another embodiment the decompression routine used by the decompression module is dictated by the device requesting the data segment being decompressed. In another embodiment the decompression module selects a decompression routine according to default settings on a per data structure type or data structure class basis. A first packet of a first object may be able to override a default decompression routine and a second packet of a second data structure of the same data structure class and data structure type may use the default decompression routine and a third packet of a third data structure of the same data structure class and data structure type may use no decompression.

In another embodiment the read data pipeline includes a read program module that includes one or more user definable functions within the read data pipeline . The read program module has similar characteristics to the write program module and allows a user to provide custom functions to the read data pipeline . The read program module may be located as shown in may be located in another position within the read data pipeline or may include multiple parts in multiple locations within the read data pipeline . Additionally there may be multiple read program modules within multiple locations within the read data pipeline that operate independently. One of skill in the art will recognize other forms of a read program module within a read data pipeline . As with the write data pipeline the stages of the read data pipeline may be rearranged and one of skill in the art will recognize other orders of stages within the read data pipeline .

The solid state storage controller includes control and status registers and corresponding control queues . The control and status registers and control queues facilitate control and sequencing commands and subcommands associated with data processed in the write and read data pipelines . For example a data segment in the packetizer may have one or more corresponding control commands or instructions in a control queue associated with the ECC generator . As the data segment is packetized some of the instructions or commands may be executed within the packetizer . Other commands or instructions may be passed to the next control queue through the control and status registers as the newly formed data packet created from the data segment is passed to the next stage.

Commands or instructions may be simultaneously loaded into the control queues for a packet being forwarded to the write data pipeline with each pipeline stage pulling the appropriate command or instruction as the respective packet is executed by that stage. Similarly commands or instructions may be simultaneously loaded into the control queues for a packet being requested from the read data pipeline with each pipeline stage pulling the appropriate command or instruction as the respective packet is executed by that stage. One of skill in the art will recognize other features and functions of control and status registers and control queues .

The solid state storage controller and or solid state storage device may also include a bank interleave controller a synchronization buffer a storage bus controller and a multiplexer MUX which are described in relation to .

The bank interleave controller directs one or more commands to two or more queues in the bank interleave controller and coordinates among the banks of the solid state storage media execution of the commands stored in the queues such that a command of a first type executes on one bank while a command of a second type executes on a second bank . The one or more commands are separated by command type into the queues. Each bank of the solid state storage media has a corresponding set of queues within the bank interleave controller and each set of queues includes a queue for each command type.

The bank interleave controller coordinates among the banks of the solid state storage media execution of the commands stored in the queues. For example a command of a first type executes on one bank while a command of a second type executes on a second bank . Typically the command types and queue types include read and write commands and queues but may also include other commands and queues that are storage media specific. For example in the embodiment depicted in erase and management queues are included and would be appropriate for flash memory NRAM MRAM DRAM PRAM etc.

For other types of solid state storage media other types of commands and corresponding queues may be included without straying from the scope of the disclosure. The flexible nature of an FPGA solid state storage controller allows flexibility in storage media. If flash memory were changed to another solid state storage type the bank interleave controller storage bus controller and MUX could be altered to accommodate the media type without significantly affecting the data pipelines and other solid state storage controller functions.

In the embodiment depicted in the bank interleave controller includes for each bank a read queue for reading data from the solid state storage media a write queue for write commands to the solid state storage media an erase queue for erasing an erase block in the solid state storage an a management queue for management commands. The bank interleave controller also includes corresponding read write erase and management agents . In another embodiment the control and status registers and control queues or similar components queue commands for data sent to the banks of the solid state storage media without a bank interleave controller .

The agents in one embodiment direct commands of the appropriate type destined for a particular bank to the correct queue for the bank . For example the read agent may receive a read command for bank 1 and directs the read command to the bank 1 read queue . The write agent may receive a write command to write data to a location in bank 0 of the solid state storage media and will then send the write command to the bank 0 write queue . Similarly the erase agent may receive an erase command to erase an erase block in bank 1 and will then pass the erase command to the bank 1 erase queue . The management agent typically receives management commands status requests and the like such as a reset command or a request to read a configuration register of a bank such as bank 0 . The management agent sends the management command to the bank 0 management queue

The agents typically also monitor status of the queues and send status interrupt or other messages when the queues are full nearly full non functional etc. In one embodiment the agents receive commands and generate corresponding sub commands. In one embodiment the agents receive commands through the control status registers and generate corresponding sub commands which are forwarded to the queues . One of skill in the art will recognize other functions of the agents .

The queues typically receive commands and store the commands until required to be sent to the solid state storage banks . In a typical embodiment the queues are first in first out FIFO registers or a similar component that operates as a FIFO. In another embodiment the queues store commands in an order that matches data order of importance or other criteria.

The bank controllers typically receive commands from the queues and generate appropriate subcommands. For example the bank 0 write queue may receive a command to write a page of data packets to bank 0 . The bank 0 controller may receive the write command at an appropriate time and may generate one or more write subcommands for each data packet stored in the write buffer to be written to the page in bank 0 . For example bank 0 controller may generate commands to validate the status of bank 0 and the solid state storage array select the appropriate location for writing one or more data packets clear the input buffers within the solid state storage memory array transfer the one or more data packets to the input buffers program the input buffers into the selected location verify that the data was correctly programmed and if program failures occur do one or more of interrupting the master controller retrying the write to the same physical location and retrying the write to a different physical location. Additionally in conjunction with example write command the storage bus controller will cause the one or more commands to multiplied to each of the each of the storage I O buses with the logical address of the command mapped to a first physical addresses for storage I O bus and mapped to a second physical address for storage I O bus and so forth as further described below.

Typically bus arbiter selects from among the bank controllers and pulls subcommands from output queues within the bank controllers and forwards these to the Storage Bus Controller in a sequence that optimizes the performance of the banks . In another embodiment the bus arbiter may respond to a high level interrupt and modify the normal selection criteria. In another embodiment the master controller can control the bus arbiter through the control and status registers . One of skill in the art will recognize other means by which the bus arbiter may control and interleave the sequence of commands from the bank controllers to the solid state storage media .

The bus arbiter typically coordinates selection of appropriate commands and corresponding data when required for the command type from the bank controllers and sends the commands and data to the storage bus controller . The bus arbiter typically also sends commands to the storage control bus to select the appropriate bank . For the case of flash memory or other solid state storage media with an asynchronous bi directional serial storage I O bus only one command control information or set of data can be transmitted at a time. For example when write commands or data are being transmitted to the solid state storage media on the storage I O bus read commands data being read erase commands management commands or other status commands cannot be transmitted on the storage I O bus . For example when data is being read from the storage I O bus data cannot be written to the solid state storage media .

For example during a write operation on bank 0 the bus arbiter selects the bank 0 controller which may have a write command or a series of write sub commands on the top of its queue which cause the storage bus controller to execute the following sequence. The bus arbiter forwards the write command to the storage bus controller which sets up a write command by selecting bank 0 through the storage control bus sending a command to clear the input buffers of the solid state storage elements associated with the bank 0 and sending a command to validate the status of the solid state storage elements associated with the bank 0 . The storage bus controller then transmits a write subcommand on the storage I O bus which contains the physical addresses including the address of the logical erase block for each individual physical erase solid stage storage element as mapped from the logical erase block address. The storage bus controller then muxes the write buffer through the write synchronization buffer to the storage I O bus through the MUX and streams write data to the appropriate page. When the page is full then storage bus controller causes the solid state storage elements associated with the bank 0 to program the input buffer to the memory cells within the solid state storage elements . Finally the storage bus controller validates the status to ensure that page was correctly programmed.

A read operation is similar to the write example above. During a read operation typically the bus arbiter or other component of the bank interleave controller receives data and corresponding status information and sends the data to the read data pipeline while sending the status information on to the control and status registers . Typically a read data command forwarded from bus arbiter to the storage bus controller will cause the MUX to gate the read data on storage I O bus to the read data pipeline and send status information to the appropriate control and status registers through the status MUX .

The bus arbiter coordinates the various command types and data access modes so that only an appropriate command type or corresponding data is on the bus at any given time. If the bus arbiter has selected a write command and write subcommands and corresponding data are being written to the solid state storage media the bus arbiter will not allow other command types on the storage I O bus . Beneficially the bus arbiter uses timing information such as predicted command execution times along with status information received concerning bank status to coordinate execution of the various commands on the bus with the goal of minimizing or eliminating idle time of the busses.

The master controller through the bus arbiter typically uses expected completion times of the commands stored in the queues along with status information so that when the subcommands associated with a command are executing on one bank other subcommands of other commands are executing on other banks . When one command is fully executed on a bank the bus arbiter directs another command to the bank . The bus arbiter may also coordinate commands stored in the queues with other commands that are not stored in the queues .

For example an erase command may be sent out to erase a group of erase blocks within the solid state storage media . An erase command may take 10 to 1000 times more time to execute than a write or a read command or 10 to 100 times more time to execute than a program command. For N banks the bank interleave controller may split the erase command into N commands each to erase a virtual erase block of a bank . While Bank 0 is executing an erase command the bus arbiter may select other commands for execution on the other banks . The bus arbiter may also work with other components such as the storage bus controller the master controller etc. to coordinate command execution among the buses. Coordinating execution of commands using the bus arbiter bank controllers queues and agents of the bank interleave controller can dramatically increase performance over other solid state storage systems without a bank interleave function.

In one embodiment the solid state controller includes one bank interleave controller that serves all of the storage elements of the solid state storage media . In another embodiment the solid state controller includes a bank interleave controller for each column of storage elements . For example one bank interleave controller serves one column of storage elements SSS 0.0 SSS N.0 . . . a second bank interleave controller serves a second column of storage elements SSS 0.1 SSS N.1 . . . etc.

The solid state storage controller includes a synchronization buffer that buffers commands and status messages sent and received from the solid state storage media . The synchronization buffer is located at the boundary between the solid state storage clock domain and the local bus clock domain and provides buffering to account for the clock domain differences. The synchronization buffer write synchronization buffer and read synchronization buffer may be independent or may act together to buffer data commands status messages etc. In one embodiment the synchronization buffer is located where there are the fewest number of signals crossing the clock domains. One skilled in the art will recognize that synchronization between clock domains may be arbitrarily moved to other locations within the solid state storage device in order to optimize some aspect of design implementation.

The solid state storage controller includes a storage bus controller that interprets and translates commands for data sent to and read from the solid state storage media and status messages received from the solid state storage media based on the type of solid state storage media . For example the storage bus controller may have different timing requirements for different types of storage storage with different performance characteristics storage from different manufacturers etc. The storage bus controller also sends control commands to the storage control bus .

In one embodiment the solid state storage controller includes a MUX that comprises an array of multiplexers where each multiplexer is dedicated to a row in the solid state storage array . For example multiplexer is associated with solid state storage elements . MUX routes the data from the write data pipeline and commands from the storage bus controller to the solid state storage media via the storage I O bus and routes data and status messages from the solid state storage media via the storage I O bus to the read data pipeline and the control and status registers through the storage bus controller synchronization buffer and bank interleave controller .

In one embodiment the solid state storage controller includes a MUX for each column of solid state storage elements e.g. SSS 0.0 SSS 1.0 SSS N.0 . A MUX combines data from the write data pipeline and commands sent to the solid state storage media via the storage I O bus and separates data to be processed by the read data pipeline from commands. Packets stored in the write buffer are directed on busses out of the write buffer through a write synchronization buffer for each column of solid state storage elements SSS 0.x to SSS N.x to the MUX for each column of solid state storage elements SSS 0.x to SSS N.x . The commands and read data are received by the MUXes from the storage I O bus . The MUXes also direct status messages to the storage bus controller .

The storage bus controller includes a mapping module . The mapping module maps a logical address of an erase block to one or more physical addresses of an erase block. For example a solid state storage media with an array of twenty storage elements e.g. SSS 0.0 to SSS 0.M per bank may have a logical address for a particular erase block mapped to twenty physical addresses of the erase block one physical address per storage element. Because the storage elements are accessed in parallel erase blocks at the same position in each storage element in a column of storage elements will share a physical address. To select one erase block e.g. in storage element SSS 0.0 instead of all erase blocks in the row e.g. in storage elements SSS 0.0 1.0 . . . N.0 one bank in this case Bank 0 is selected.

This logical to physical mapping for erase blocks is beneficial because if one erase block becomes damaged or inaccessible the mapping can be changed to map to another erase block. This mitigates the loss of losing an entire virtual erase block when one element s erase block is faulty. The remapping module changes a mapping of a logical address of an erase block to one or more physical addresses of a virtual erase block spread over the array of storage elements . For example virtual erase block 1 may be mapped to erase block 1 of storage element SSS 0.0 to erase block 1 of storage element SSS 0.1 . . . and to storage element 0.M virtual erase block 2 may be mapped to erase block 2 of storage element SSS 1.0 to erase block 2 of storage element SSS 1.1 . . . and to storage element 1.M etc. Alternatively virtual erase block 1 may be mapped to one erase block from each storage element in an array such that virtual erase block 1 includes erase block 1 of storage element SSS 0.0 to erase block 1 of storage element SSS 0.1 to storage element 0.M and erase block 1 of storage element SSS 1.0 to erase block 1 of storage element SSS 1.1 . . . and to storage element 1.M for each storage element in the array up to erase block 1 of storage element N.M

If erase block 1 of a storage element SSS 0.0 is damaged experiencing errors due to wear etc. or cannot be used for some reason the remapping module could change the logical to physical mapping for the logical address that pointed to erase block 1 of virtual erase block 1. If a spare erase block call it erase block of storage element SSS 0.0 is available and currently not mapped the remapping module could change the mapping of virtual erase block 1 to point to erase block of storage element SSS 0.0 while continuing to point to erase block 1 of storage element SSS 0.1 erase block 1 of storage element SSS 0.2 not shown . . . and to storage element 0.M . The mapping module or remapping module could map erase blocks in a prescribed order virtual erase block 1 to erase block 1 of the storage elements virtual erase block 2 to erase block 2 of the storage elements etc. or may map erase blocks of the storage elements in another order based on some other criteria.

In one embodiment the erase blocks could be grouped by access time. Grouping by access time meaning time to execute a command such as programming writing data into pages of specific erase blocks can level command completion so that a command executed across the erase blocks of a virtual erase block is not limited by the slowest erase block. In other embodiments the erase blocks may be grouped by wear level health etc. One of skill in the art will recognize other factors to consider when mapping or remapping erase blocks.

In one embodiment the storage bus controller includes a status capture module that receives status messages from the solid state storage media and sends the status messages to the status MUX . In another embodiment when the solid state storage media is flash memory the storage bus controller includes a NAND bus controller . The NAND bus controller directs commands from the read and write data pipelines to the correct location in the solid state storage media coordinates timing of command execution based on characteristics of the flash memory etc. If the solid state storage media is another solid state storage type the NAND bus controller would be replaced by a bus controller specific to the storage type. One of skill in the art will recognize other functions of a NAND bus controller .

The monitor module in one embodiment initiates a power loss mode in the nonvolatile storage device in response to a primary power source failing to supply electric power above a predefined threshold through the primary power connection . The power loss mode in one embodiment is a mode of operation in which the power management apparatus prepares the storage device for shutting down within a power hold up time provided by the secondary power supply . The power loss module in one embodiment adjusts execution of in process operations on the nonvolatile storage device during the power loss mode to allow essential in process operations to execute.

In one embodiment power above the predefined threshold is sufficient for the storage device . Sufficient power in one embodiment is power that meets the requirements for the storage device to operate properly. The predefined threshold in a further embodiment is set at or above an insufficient power level for the storage device . Insufficient power is power that does not meet the requirements for the storage device . Power with a high AC or harmonic component when DC is expected and a voltage or current level that is too low are examples of insufficient power. As described above in one embodiment the storage device is configured to automatically accept or otherwise draw power from the secondary power supply when power from the primary power source falls below the predefined threshold. The predefined threshold in one embodiment is an engineered threshold determined by characteristics of the secondary power supply and corresponding circuits.

The primary power source in one embodiment is a source of power that the nonvolatile storage device uses during normal operation and which provides a substantially continuous supply of power that is not unexpectedly interrupted during normal operation. For example in typical embodiments the computer system i.e. the host or the like to which the storage device is attached is the primary power source and provides power through the motherboard such as through a bus or slot connection such as PCI PCI e AGP or the like or through an external port such as a USB port a FireWire port an eSATAp port or the like. In another embodiment the primary power source is a standard electrical outlet.

In one embodiment the monitor module monitors the primary power connection directly to determine when electric power from the primary power source falls below the predefined threshold. For example the monitor module may include a power sensor a current sensor and or another appropriate sensor to use to determine whether the nonvolatile storage device is receiving sufficient external power. In other embodiments the monitor module may be notified by another component in the nonvolatile storage device in the event the nonvolatile storage device loses external power.

In one embodiment the monitor module includes an analog circuit that responds to a loss of power from the primary power connection . For example the primary power connection and the secondary power supply may be placed in parallel such that the primary power connection keeps the secondary power supply fully charged for example when the secondary power supply is made up of capacitors and also supplies power to the storage device . In the parallel configuration the secondary power supply naturally begins providing power in the event of a failure of the primary power connection and the storage device naturally accepts the power from the secondary power supply . The monitor module circuit may also provide proper isolation to ensure that power from the secondary power supply is sent to the storage device for example a diode may be used to ensure that in the event of a failure in the primary power supply power flows from the secondary power supply to the storage device and not to the failed primary power supply. Approaches to proper isolation will be appreciated by those of skill in the art in light of this disclosure.

The monitor module in such an embodiment may still include detection components such as current sensors voltage sensors or the like to sense the power disruption and to initiate the power loss mode to trigger the operations of other modules in the power management apparatus in response. In another embodiment monitor module may sense a power disruption signal and activate a switch that changes the power draw for the storage device from the primary power connection to the secondary power supply or the like.

The monitor module in one embodiment may initiate the power loss mode by directly or indirectly communicating to the power loss module and or another module that the storage device has entered the power loss mode. For example in various embodiments the monitor module may set a status register send a power loss mode command send a power loss signal send a power loss interrupt initiate a power loss mode function or procedure place the storage device in a power loss state and or otherwise notify the power loss module of the power loss mode.

The power loss module in one embodiment adjusts execution of in process operations on the storage device during the power loss mode to ensure that essential operations such as operations acknowledged to the host or the like are executed during the power hold up time. In process operations in one embodiment include operations that the storage device is currently executing. In a further embodiment in process operations include operations that are queued for execution on the storage device that are in flight in the write data pipeline and or the read data pipeline or the like. In the depicted embodiment the power loss module includes the identification module the terminate module and the corruption module .

The identification module in one embodiment identifies one or more non essential operations on the nonvolatile storage device in response to the monitor module determining that external power has been lost is below the predefined threshold or is otherwise insufficient and entering the power loss mode. Non essential operations are those operations that can be terminated stopped or paused without causing data corruption or data loss on the storage device . Essential operations are those operations that must be executed in order to avoid data corruption data loss on the storage device or inconsistent communications between the storage device and the host i.e. sending an acknowledgement to the host for data that later is not properly handled consistent with the acknowledgement . The identification module may further determine whether the non essential operations are executing or whether they are queued and awaiting execution.

The terminate module in one embodiment terminates the non essential operations identified by the identification module . The terminate module in various embodiments may terminate non essential operations by erasing the non essential operations commands and instructions that are queued and or by interrupting non essential operations that are currently executing on the storage device . In one embodiment the terminate module allows the storage device to power off i.e. once the power hold up time has expired and the secondary power supply is depleted without executing the non essential operations. In a further embodiment the terminate module terminates the non essential operations in a way that the non essential operations are not executed or resumed once the storage device is again powered on after a power loss. For example in one embodiment the terminate module terminates the non essential operations without leaving a record of the terminated non essential operations so that the storage device powers on without executing or resuming the terminated non essential operations.

In one embodiment the identification module also manages a power budget for the storage device while the storage device is operating on the secondary power supply . The identification module may determine for example how much power is available how much power all pending operations on the storage device will require and prioritize the pending operations. The operations may thus be reordered and executed in order of priority to execute at least the essential in process operations within the power hold up time. In one embodiment if the identification module determines that there is insufficient power to execute all write operations i.e. program operations on a nonvolatile solid state storage device possibly due to an error or failure the identification module may log this information to provide notification possibly after power is restored to a user or system that some or all of the write operations have been lost.

In one embodiment the non essential operations include erase operations that are erasing nonvolatile memory on the nonvolatile storage device and or read operations that are reading data on the nonvolatile storage device . The erase operations may have been generated for example as part of a garbage collection operation that is reclaiming space on a solid state storage device such as a Flash memory device. Non essential operations may also include operations such as generating a hash key for data in the nonvolatile storage device decompressing data read from storage or other operations. Non essential operations in a further embodiment may include write or program operations for which the nonvolatile storage device has not sent an acknowledgement to the host . In one embodiment a user or system designer specifies which operations are essential and which operations are non essential.

In certain embodiments the terminate module terminates the non essential operations based on how much power they require. For example erase operations in solid state storage devices tend to consume considerable amounts of power. The terminate module may quickly terminate the erase operations in order to conserve power. In contrast read operations require relatively little power. The terminate module may begin terminating read operations only after the erase operations are terminated or the like.

In one embodiment the identification module prioritizes operations with the priorities based on the importance of executing the operation. For example program operations for data that has been acknowledged may be given the highest priority while an erase operation is given the lowest priority. The terminate module may begin terminating the lowest priority operations and move up a prioritized list of operations and not terminate any essential operations. Thus the terminate module beginning with the lowest priority operation determines if the operation is essential. If not that operation is terminated. If the operation is essential the operation is not terminated and the terminate module moves to the next operation for consideration.

In certain embodiments the identification module may also prioritize non essential operations that are in the process of executing based on the amount of energy required to complete the non essential operation. For example an erase operation that is 90 complete may be given a lower priority for termination than an erase operation that is 5 complete thus the erase operation that is 90 may be allowed to complete while the erase operation that is 5 complete when the power disruption is detected is stopped. In one embodiment the amount of energy required for an operation may vary over the time during which the operation is executed.

The terminate module in one embodiment terminates non essential operations identified by the identification module . As noted above the terminate module may terminate certain classes of operations such as power intensive erase operations or autonomous grooming operations as prioritized by the identification module for termination before other operations. In one embodiment the terminate module terminates the non essential operation by identifying the memory area or component on which the operation is working executing and resetting the memory area or component as discussed in greater detail in connection with . As used herein a memory area or component refers to a physical section of the nonvolatile memory for which operations executing on that physical section can be reset terminated halted suspended or paused with a command or signal.

By terminating the non essential operations the power management apparatus can ensure that power is used for essential write operations and other essential operations so that the essential operations can execute within the power hold up time. In addition the power management apparatus can thus reduce the total amount of power that the secondary power supply needs to provide. Thus a designer is permitted to choose for example to use smaller capacitors to provide power which may save space in the storage device reduce cost and improve reliability while maintaining the ability to ensure that all received and acknowledged data is preserved and protected from unexpected power disruptions.

In one embodiment the terminate module determines whether the particular non essential operation is either queued or executing. The terminate module may delete queued non essential operations by removing them from the queue to ensure that they do not execute. Alternatively or in addition the terminate module may cancel operations that are executing to prevent the executing operations from consuming additional power. In certain embodiments as mentioned above the terminate module terminates some non essential operations that are in process while allowing others to complete.

The corruption module in one embodiment identifies data received by the storage device that is to be written to the nonvolatile memory that is presumed to be corrupt or must be presumed to be corrupt. Such data may for example be data in the write data pipeline . The corruption module ensures that the data that is presumed to be corrupt is not stored to the nonvolatile memory and also ensures that the host is either made aware that the data was not stored or ensures that the host is not told that the corrupt data was successfully stored.

In certain embodiments the corruption module and the terminate module log the actions taken once the monitor module detects the power disruption. For example the terminate module may log which non essential operations were canceled before they began execution and which non essential operations were terminated during execution. The corruption module may log information concerning what data it determined to be corrupt. Other modules in the power management apparatus may similarly log their activity or a subset thereof to help the storage device the host or other interested entity determine what occurred during the unexpected shutdown.

In one embodiment the corruption module expects that all data received by the storage device beginning at some specified time in the past for example 5 microseconds before the power disruption signal was received by the monitor module is corrupt and should not be stored in the nonvolatile storage . This specification may be dictated by a standard such as PCI PCI e or the like or by the host storage device vendor manufacturer etc. In a further embodiment the corruption module regards data that is in flight in the write data pipeline before a predefined stage as corrupted.

The corruption period is a result of the time necessary to detect the power disturbance shown occurring at 5 microseconds generate a signal indicating that there has been a power disturbance shown occurring at 3 microseconds and the monitor module receiving the power disturbance signal shown occurring at 5 microseconds . Generally the corruption module prevents new data from entering the write data pipeline once it is determined that there has been a power disturbance as this new data is presumed corrupt. However corrupt data may have moved into the write data pipeline during the corruption period.

Thus all data received after the corruption time is presumed to be corrupt and should not be stored. For example the corruption module may determine that the monitor module received a power disruption signal at time t and the corruption module may always set the corruption time to t 5 microseconds. The corruption module may therefore conclude that all data received after the corruption time of t 5 microseconds is corrupt. In such an embodiment the corruption module identifies all write operations i.e. program operations for Flash memory and the like received after t 5 microseconds determines where they are in the write data pipeline and skips the write operations. The corruption module in various embodiments may skip the write operations by canceling them skipping them clearing them interrupting them or otherwise failing to execute them.

In one embodiment the power management apparatus also includes a completion module . In some implementations certain operations associated with stages in a write data pipeline will not execute or permit continued flow of data through the pipeline until a buffer associated with that stage is filled. For example an ECC stage such as the ECC generator of may require a full buffer before generating the ECC value. Similarly an output buffer such as the write buffer the write synchronization buffer or the like may have to be filled before the data is moved out of the output buffer and onto the nonvolatile storage . In one embodiment if a buffer is partially filled under normal conditions the stage associated with the buffer will wait until the buffer is filled before operations associated with that buffer are executed. The buffers referred to herein may be physical buffers or may simply be temporary storage locations such as registers DRAM locations or others. In a further embodiment the packetizer may not pass a packet to a further stage in the write data pipeline until the packet is complete until a group of packets are complete or the like. Similarly the write buffer in certain embodiments may not send data to the storage device until a page a logical page a group of pages or logical pages or the like is complete.

In the event of a power disruption it may be useful to move data through the write data pipeline even if a buffer packet or page at one or more stages is not filled to flush the data to the nonvolatile memory or the like. The completion module flushes data in a partially filled data buffer through the write data pipeline and onto the nonvolatile memory . In one embodiment the completion module identifies the partially filled buffers packets and or pages that will not fill and pads the buffers with pad data such that the data is moved out of the buffers and through the write data pipeline .

The completion module in one embodiment ensures that the padding is identifiable as pad data to ensure that the storage device and or the host can identify the padding and know that the pad data is not part of the actual data. In one embodiment the completion module uses a unique header token marker pattern or other identifier to identify the padding data. In a further embodiment the completion module flushes a buffer packet and or page without adding padding data using existing data in the unfilled space in the buffer to complete the partially filled buffer packet and or page. For example a buffer in an unfilled or empty state may store all binary ones all binary zeroes junk or garbage data data from a previous transaction or the like. The completion module in one embodiment identifies the existing data in the unfilled area of the buffer as padding data. The completion module may use a unique pattern a flag or other indicator or other approaches known to those in the art in light of this disclosure.

The completion module in one embodiment uses a unique header footer token marker pattern or other identifier to identify that the power management apparatus has successfully completed the essential operations in the power loss mode. In one embodiment successfully completing the essential operations means that the completion module successfully flushed write data from write operations through the write data pipeline and to the nonvolatile memory or the like. The indicator in one embodiment is the same indicator described above to identify the padding data. In a further embodiment the completion module uses a separate indicator to identify successful execution of essential operations during the power loss mode.

Those of skill in the art will appreciate that the embodiment shown in is simply one example of an architecture for nonvolatile memory such as flash and that numerous other architectures are also possible. shows a simplified version of nonvolatile memory in order to focus on features of the nonvolatile memory in a manner helpful to understanding the present disclosure. Greater detail on a nonvolatile memory implementation may be found in U.S. patent application Ser. No. 11 952 095 to David Flynn et al. filed Dec. 6 2007 entitled Apparatus System and Method for Managing Commands of Solid State Storage Using Bank Interleave which is incorporated herein by reference referred to hereinafter as The Bank Interleave Application .

As noted above the terminate module may terminate a non essential operation identified by the identification module by determining the memory area or component on which the operation is executing and resetting the memory area or component. As used herein a memory area or component refers to a physical section of the nonvolatile memory that can be reset with a reset command. A reset command is a command that causes all operations that are executing for the memory area such as write erase and read to terminate. In one embodiment each die and can be independently reset such that each individual die and constitutes a unique memory area or component. The reset operation causes the operation on the particular die that is the subject of the reset operation to terminate the process.

In certain embodiments as described herein the operations occur on a bank basis. For example an erase operation in one embodiment is executed on a logical erase block that spans multiple die that make up a bank. In such embodiments the memory area or component may be the bank and the reset operation is sent to all die in the bank at substantially the same time. The reset operation itself may be one command or multiple commands in such embodiments each die in the bank is reset which stops the erase operations for each of the physical erase blocks in each die of the logical erase block.

In another embodiment the terminate module may reset substantially all of the nonvolatile memory at the same time. For example in one embodiment the storage device may schedule erase operations on each bank simultaneously and the terminate module may send reset commands to each bank in the nonvolatile memory to terminate those scheduled erase operations.

In such an embodiment the terminate module may send a reset command over a bus to a specific die or . This allows the terminate module to reset the memory areas that are performing non essential operations such as an erase while allowing programming operations i.e. data storage write operations on other memory areas to continue. In one embodiment the terminate module terminates executing operations by issuing a reset signal and terminates pending operations i.e. those operations in a command queue that have not yet started by removing the operation from the command queue or otherwise skipping the operations.

Certain non essential operations may be terminated without the use of a reset command. For example as noted above non essential operations that are in a command queue may simply be skipped by deleting clearing marking to prevent execution or removing the non essential operations without ever starting them. Since these operations have never started no die or needs to be reset to terminate the operation. Other non essential operations that are not executed on the die and may similarly be terminated without a reset command even when they are executing for example if a stage in the write data pipeline is generating a hash key for the data when the power disruption is detected the hash generation operation may be terminated without a reset operation being sent to the die and . In certain embodiments only program write read and erase operations that are in the process of executing on a particular die and are terminated with the reset command.

In certain embodiments the terminate module may quiesce or otherwise shut down particular areas sections modules subcomponents of the storage device . For example the terminate module may shut down all physical devices components and or logical modules that implement the read data pipeline . In a further embodiment the terminate module may quiesce or otherwise shut down a read DMA engine or other subcomponents associated with non essential operations. The terminate module may also shut down one or more CPUs operating on the storage device for example the storage device may have a multi core CPU. In such an embodiment the terminate module may shut down one or more cores on the CPU that the power management apparatus is not using.

The terminate module may also monitor and ensure that no activity unrelated to the operations of the power management apparatus is occurring on the core that is supporting the power management apparatus . In certain embodiments the power management apparatus may be implemented in hardware separate from the CPU such that the terminate module may simply shut down the CPU or CPUs to preserve power. The terminate module may shut down the read data pipeline and the CPU by stopping the respective clocks. Those of skill in the art will appreciate other approaches to shutting down the read data pipeline the read DMA engine the CPU and or other subcomponents of the storage device .

In certain embodiments as described in the Bank Interleave Application certain operations may occur on a bank level for example data is programmed i.e. written or stored to the die and during a program operation that affects the bank . The banks and may be organized such that they provide logical erase blocks made up of n number of physical erase blocks when there are n die in the banks logical pages made up of N number of physical erase blocks when there are N die in the banks and so on. Thus in the bank may present a logical erase block that is made up of three physical erase blocks from die and and logical pages of data made up of three physical pages from die and

In such an embodiment the terminate module may send the reset command over the bus to the die such as die and that are running in parallel in the bank . In such an embodiment the group of die and would be reset simultaneously effectively halting the operations occurring on each of the die and . Thus since an erase operation occurs on a logical erase block that includes physical erase blocks on the three physical die and the reset operation may be physically sent to the three physical erase blocks on the die and simultaneously to halt the erase operation that is in process for the logical erase block. Similarly in a further embodiment the terminate module may send the reset operation to all the dies to reset the entire nonvolatile memory simultaneously.

In one possible example the monitor module may determine that the storage device has lost power. The identification module determines that there is an erase operation occurring on the nonvolatile memory against a logical erase block on bank . The terminate module sends a reset command to the bank which causes the die and to reset and thus terminates the erase operation. A similar pattern may occur for other erase operations and read operations pending for the nonvolatile memory after the storage device loses power. In addition the banks may be independent of one another such that operations occurring on one bank can be terminated or paused without affecting the operations on the other banks in the storage device .

In certain embodiments the program erase and read operations do not occur on a bank level as described above in certain architectures the program erase and read operations occur individually on each die and . In such embodiments the reset operation may be sent to the affected die for example an erase of a physical erase block on die may be terminated by the terminate module sending a reset command to the die

Other approaches may be taken to terminate non essential operations that are executing as identified by the identification module . In one embodiment the terminate module terminates the non essential operations that are executing or are queued to execute by pausing the non essential operation. Certain nonvolatile memory devices may allow executing operations to be paused. In such embodiments the terminate module may send a command to pause the non essential operations without sending a subsequent command to resume the non essential operations effectively causing the operation to cancel. In other embodiments the terminate module may send a command to pause the non essential operations wait until all essential program operations are complete and then send one or more resume commands to the various paused operations.

As shown in the write data pipeline may be implemented as part of a solid state storage SSS controller . The power management apparatus in one embodiment may also be implemented as part of the SSS controller . In one embodiment the power management apparatus may be implemented separately but be in communication with the SSS controller . The power management apparatus in a further embodiment may be integrated with the SSS controller .

As discussed above the corruption module in certain embodiments identifies data received over the PCI e connection or other connection depending on the implementation that was received after the power disruption and that is presumed corrupted generally referred to hereafter as corrupt data . The corruption module in one embodiment also ensures that the host can or should know that the data presumed corrupted was not saved in the storage device . In one embodiment the corruption module determines the location of the oldest piece of corrupt data in the write data pipeline . The oldest piece of corrupt data is at the start of the data received after the corruption period begins. All data from the oldest piece of corrupt data back to the beginning of the write data pipeline for example the input buffer is presumed to be corrupt and is removed from the write data pipeline .

In one embodiment the corruption module may cause the storage device to delay sending the acknowledgment back to the host until after the period of time used to calculate the corruption time has passed. As discussed above in certain embodiments depending on the architecture of the storage device and of the write data pipeline the corruption module may assume that all data received 5 microseconds or later after the monitor module detects the power disruption is corrupt. Thus the 5 microseconds is the period of time used to calculate the corruption time. Thus the corruption module may specify that the acknowledgement is not to be sent to the host until 5 microseconds after the data was received by the storage device . As a result in certain embodiments data is never acknowledged as having been stored until the storage device can guarantee that the data was not corrupted by a power disruption that has not yet been detected and or communicated to the storage device .

In one embodiment the corruption module sends the acknowledgement once data leaves a buffer that is managed by the buffer controller but prior to the data entering the write data pipeline . For example data may be transferred by a direct memory access DMA engine into buffers on the storage device and that data is then moved by one or more buffer controllers into the write data pipeline .

In one embodiment the buffer controller allows the buffer receiving the data from the DMA engine to fill waits for expiration of the corruption time and then sends an acknowledgement to the host . Once the period of time passes after the buffer is filled it is known whether or not a power disruption has corrupted all or part of the data in the buffer and the data may be safely acknowledged. If a power disruption has occurred the data can be removed from the buffer without being sent to the write data pipeline . In addition no acknowledgement may be sent to the host acknowledging that the data was stored if a power disruption has occurred. According to best practices the host should therefore assume that the data was not stored. In another embodiment the potential risk of data corruption in the write data pipeline is acceptable and so the buffer controller allows the buffer to fill no delay is imposed for the corruption time and then the storage device sends an acknowledgement to the host . In certain embodiments the storage device inserts the corruption avoidance delay by default and is configurable to allow for not inserting the corruption avoidance delay.

As a result in such an embodiment the corruption module can prevent data corrupted by a power disruption from entering the write data pipeline and further prevent the storage device from sending an acknowledgement until after the storage device can assure that the data was not corrupted during a power disruption.

In another embodiment the corruption module stops corrupted data within the write data pipeline at a choke point. The choke point is the location in the write data pipeline where in the event a power disruption is detected any data above the choke point i.e. between the choke point and the input buffer including data in the input buffer is presumed to be corrupted. The location of the choke point may be determined by the rate at which data travels through the write data pipeline and also on the period of time used to determine the corruption time. For example the corruption module may assume that in the 5 microseconds since the corruption time the farthest data may have moved into the write data pipeline is to the ECC generator . Thus the ECC generator in the example embodiment is the choke point in the write data pipeline . In the event that a power disruption is detected the corruption module may prevent data within the ECC generator and any data farther up the write data pipeline i.e. in the media encryption module the packetizer and so on up the write data pipeline from moving through the write data pipeline and into the nonvolatile memory . In certain embodiments the corruption module aborts the operations occurring in the write data pipeline above the choke point.

In a further embodiment the location of the choke point may be determined by the location at which the write data pipeline has enough information to write data to the nonvolatile memory . For example in one embodiment once the packetizer has added header metadata to a complete packet the write data pipeline has enough information to further process the packet i.e. pass the packet to the ECC generator etc. and to write the packet to the nonvolatile memory . A packet in one embodiment is the smallest writable unit of data in the write data pipeline . In this example embodiment the packetizer is the choke point. In a further embodiment an ECC chunk or codeword is the smallest writable unit of data in the write data pipeline and the ECC generator may be the choke point. In one embodiment characteristics of the secondary power supply are selected to provide a power hold up time sufficiently long enough for data to pass through the write data pipeline from the choke point on and to be written to the nonvolatile memory .

In certain embodiments the corruption module sends an acknowledgement for the data once the data has moved completely through the choke point in the write data pipeline . Thus the corrupt data may be stopped and the operations working on the corrupt data aborted before the acknowledgement is sent. As a result the host is not given an acknowledgement until the data that is stored or in the pipeline to be stored is good uncorrupt data.

In certain embodiments the data may be organized into atomic data units. For example the atomic data unit may be a packet a page a logical page a logical packet a block a logical block a set of data associated with one or more logical block addresses the logical block addresses may be contiguous or noncontiguous a file a document or other grouping of related data. In such embodiments the corruption module may delay sending the acknowledgement until the entire atomic data unit has passed through the choke point. For example part of a file may have passed through the choke point and is thus known to be uncorrupt data however the last half of the file has not yet passed through the choke point and thus may include corrupt data. The corruption module may wait until the entire atomic data unit has passed through the choke point before sending the acknowledgement as opposed to sending an acknowledgment when only a portion of the atomic data unit has moved through. In one embodiment the corruption module discards partially corrupted atomic data units. In a further embodiment the corruption module allows an uncorrupted portion of an atomic data unit or both an uncorrupted portion and a corrupted portion of an atomic data unit to pass through the write data pipeline and to be written to the nonvolatile memory . In certain embodiments where an atomic data unit may include partial data or data that is corrupted the power management apparatus may include an indicator with the stored data to indicate the proper state of the atomic data unit.

The corruption module may further be responsible for halting the flow of data into the write data pipeline after a power disruption is detected. Thus regardless of whether the corrupted data is handled outside the write data pipeline or within the write data pipeline the corruption module may prevent any data from entering the write data pipeline after the power disruption is detected.

The completion module may also work in conjunction with the write data pipeline to ensure that data that is not corrupt and has been acknowledged is moved through the write data pipeline and stored in the nonvolatile memory . The modules stages in the write data pipeline may use buffers to support their operations. In certain embodiments the modules such as modules only perform the operations once the relevant buffer is filled. For example the ECC generator may wait until the buffer is full and then generate an ECC value for the entire buffer. In one embodiment the buffer controller manages the flow of data through buffers in the write data pipeline . Similarly the write data pipeline may include one or more control queues for stages in the write data pipeline as described above with regard to .

During normal operation the write data pipeline continually streams data through the write data pipeline such that the buffers will always be filled. However in the event of a power disruption data flow into the write data pipeline may be stopped when one or more buffers in the write data pipeline are only partially full. For example as noted above the corruption module may remove corrupt data from the write data pipeline and prevent new data from flowing into the storage device . As a result one or more buffers in the write data pipeline may be left partially full. If the data is not moved through the write data pipeline the data will be lost at the end of the power hold up time once the secondary power supply is exhausted.

In certain embodiments the completion module flushes data through partially filled buffers in the write data pipeline during the power loss mode. The completion module in one embodiment fills the partially filled buffers with padding data. In other embodiments as described above the completion module may flush data without adding padding data by using existing values stored in the unfilled portion of the buffer as padding data or the like. As a result the data and the padding are operated on moved out of the buffer and moved through the write data pipeline . The buffers used in the write data pipeline may not all be the same size in such embodiments the completion module may monitor the data as the data moves through the write data pipeline and flush additional buffers at any point where a buffer is partially filled.

In certain embodiments the completion module uses a unique marker indicator or header to identify the padding data to prevent the padding data from being mistaken for actual data in the future. In certain embodiments the pad sequence is made up of 1 values as the value 1 is the state the nonvolatile memory cells are in prior to the program of the cells occurring. For example in Flash memory the program operations convert is to 0s. By using a pad sequence made up of 1s the power necessary to convert is to 0s may be conserved. In a related embodiment the is making up the pad data do not need to be transferred prior to initiating a program operation as the cells will already be in the 1 state.

In certain embodiments as data is moved out of the write data pipeline over the storage I O bus and into nonvolatile memory an indicator is inserted in the packet indicating whether or not the data was properly written. In certain embodiments the indicator is inserted in the header of a packet for the data and indicates whether the data in the packet that preceded the packet with the indicator was properly written. Thus if a packet is successfully programmed the header of the subsequent packet is programmed with an indicator stating that the last packet programmed was successfully programmed.

In other embodiments the indicator is placed at the end of the packet in a footer and indicates whether the packet in which the indicator is contained was properly written. In one embodiment this is done by shifting the data forward one bit such that the data encroaches into the header space. Thus if the header is a 64 bit header the shift reduces the header space to 63 bits and adds one bit to the footer. This leaves one bit at the end of the packet which may be used as the indicator. This approach allows each packet to indicate its own status while maintaining proper alignment in embodiments that may be sensitive to boundary alignment.

The indicator may be used to identify that the packet includes padding and that the data is therefore incomplete and may not be usable by the system. In certain embodiments when the storage device is powered on again after the failure the indicator is used to aid in reconstruction of the indexes and the validity map for the nonvolatile memory .

In certain embodiments one indicator is inserted for each atomic data unit. As noted above the indicator may be placed as a footer at the end of the last packet in the atomic data unit. The indicator may thus indicate whether the data for the entire atomic data unit was properly written. If for example the power disruption causes only a portion of the atomic data unit to be written and the last packet was padded as described above the indicator would indicate that the entire atomic data unit was not properly written. In addition as discussed above in certain embodiments no acknowledgement would have been sent to the host in certain embodiments.

In one embodiment corrupt data is allowed to progress through the write data pipeline in order to flush acknowledged good data in progress to the nonvolatile memory . The corrupt data may be identified by setting the indicator as described above which indicator flags the data as invalid corrupt. In related embodiments other forms of indicators such as specialized packets headers unique character streams markers and similar methods known to those skilled in the art may be substituted for the indicator described above to invalidate the corrupt data stored in the nonvolatile memory . In all such cases the corrupt data should never be acknowledged to the host .

As described above with regard to the completion module in one embodiment uses a unique header footer token marker pattern or other identifier to identify that the power management apparatus has successfully completed the essential operations in the power loss mode such as successfully flushing write data through the write data pipeline or the like and successfully storing the data on the nonvolatile memory during the power hold up time. The indicator in one embodiment is the same indicator described above to identify corrupt data padding data or the like. In a further embodiment the completion module uses a separate indicator to identify successful execution of essential operations during the power loss mode.

In one embodiment an atomic data unit is associated with a plurality of noncontiguous and or out of order logical block addresses or other identifiers that the write data pipeline handles as a single atomic data unit. As used herein writing noncontiguous and or out of order logical blocks in a single write operation is referred to as an atomic write. In one embodiment a hardware controller processes operations in the order received and a software driver of the host sends the operations to the hardware controller for a single atomic write together so that the write data pipeline can process the atomic write operation as normal. Because the hardware processes operations in order this guarantees that the different logical block addresses or other identifiers for a given atomic write travel through the write data pipeline together to the nonvolatile memory . In one embodiment because the terminate module does not terminate acknowledged write operations acknowledged atomic writes are successfully stored in the nonvolatile memory and the host can detect that an atomic write has failed due to a power loss or the like if the host does not receive an acknowledgment. The host in one embodiment can back out reprocess or otherwise handle failed atomic writes and or other failed or terminated operations upon recovery once power has been restored.

In one embodiment a software driver on the host may mark blocks of an atomic write with a metadata flag indicating whether a particular block is part of an atomic write. One example metadata marking is to rely on the log write append only protocol of the nonvolatile memory together with a metadata flag or the like. The use of an append only log for storing data and prevention of any interleaving blocks enables the atomic write membership metadata to be a single bit. In one embodiment the flag bit may be a 0 unless the block is a member of an atomic write and then the bit may be a 1 or vice versa. If the block is a member of an atomic write and is the last block of the atomic write in one embodiment the metadata flag may be a 0 to indicate that the block is the last block of the atomic write. In another embodiment different hardware commands may be sent to mark different headers for an atomic write such as first block in an atomic write middle member blocks of an atomic write tail of an atomic write or the like.

On recovery from a power loss or other failure of the host or of the storage device in one embodiment the storage controller the power management apparatus or the like scans the log on the nonvolatile memory in a deterministic direction for example in one embodiment the start of the log is the tail and the end of the log is the head and data is always added at the head . In one embodiment the power management apparatus scans from the head of the log toward the tail of the log. In other embodiments the power management apparatus may scan from the tail of the log toward the head of the log scan once from tail to head and once from head to tail or otherwise scan the log for recovery purposes. For atomic write recovery in one embodiment when scanning head to tail if the metadata flag bit is a 0 then the block is either a single block atomic write or a non atomic write block. In one embodiment once the metadata flag bit changes from 0 to 1 the previous block scanned and potentially the current block scanned are members of an atomic write. The power management apparatus in one embodiment continues scanning the log until the metadata flag changes back to a 0 at that point in the log the previous block scanned is the last member of the atomic write and the first block stored for the atomic write.

In one embodiment the nonvolatile memory uses a log based append only write structured writing system where new writes go on the front of the log i.e. at the head of the log . In a further embodiment the storage controller reclaims deleted stale and or invalid blocks of the log using a garbage collection system a groomer a cleaner agent or the like. The storage controller in a further embodiment uses a forward map to map logical block addresses to physical addresses to facilitate use of the append only write structure and garbage collection.

The storage controller in a further embodiment tracks write operations in process during normal operation of the storage device using a data structure such as an in flight tree or the like. An inflight tree in one embodiment is a data structure that maintains a record of block storage requests in particular write requests that have been received by the storage device but have not yet been completed. The power management apparatus in one embodiment ensures that for a single block write the write is guaranteed to complete even if power is lost.

In the depicted embodiment the packetizer includes an incomplete packet and a complete packet . In one embodiment if the incomplete packet is at the end of an atomic data unit the corruption module may send an acknowledgment for the data in the incomplete packet and the complete packet to the host . During power loss mode in one embodiment the completion module flushes the incomplete packet from the packetizer . As described above in certain embodiments the completion module may add a marker indicating the end of valid data in the incomplete packet add padding data to the packet and or otherwise flush the incomplete packet from the packetizer .

In another embodiment if the complete packet is at the end of an atomic data unit and the incomplete packet is from an incomplete different atomic data unit the corruption module sends an acknowledgment to the host for the data in the complete packet but does not acknowledge the data of the incomplete packet to the host . During power loss mode in one embodiment the terminate module may discard the incomplete packet as unacknowledged data skip one or more operations relating to the incomplete packet as non essential operations or the like.

In the depicted embodiment the write buffer includes one incomplete page and two complete pages . In one embodiment the pages comprise logical pages as described above. The completion module in one embodiment flushes one or both of the packets from the packetizer through the ECC generator and to the write buffer during the power loss mode.

In one embodiment the write buffer writes the complete pages to the nonvolatile memory substantially as normal even during the power loss mode. In a further embodiment the terminate module may terminate and or reset one or more non essential operations on the nonvolatile memory so that the write buffer can write the complete pages to the nonvolatile memory . The completion module in one embodiment flushes the incomplete page from the write buffer to the nonvolatile memory so that the nonvolatile memory stores the incomplete page within the power hold up time. As described above in various embodiments the completion module may add a marker indicating the end of valid data in the incomplete page add padding data to the incomplete page and or otherwise flush the incomplete page from the write buffer .

In the depicted embodiment if the monitor module determines that power from the primary power connection is below the predefined threshold the monitor module initiates a power loss mode in the storage device . The storage device accepts power from the secondary power source for at least a power hold up time during the power loss mode. The power loss module in the depicted embodiment adjusts execution of in process operations on the storage device during the power loss mode so that essential in process operations execute within the power hold up time and the method ends.

If the power to the storage device has not been interrupted the monitor module continues monitoring the power to the storage device for interruptions. In the event of an interruption the method includes identifying the uncompleted operations on the storage device . In one embodiment the identification module identifies the uncompleted operations. In certain embodiments the identification module deals with only erase operations read operations and program operations. In certain embodiments other types of operations are also identified.

In the embodiment shown if the uncompleted operations are read or erase operations the identification module may determine which read operations and erase operations are currently being executed i.e. those that are currently occurring on the nonvolatile memory and those that are pending. For those read and erase operations that are currently being executed in one embodiment the terminate module sends a reset command to reset the affected memory area and cancel the relevant operation. As discussed above the terminate module may perform these actions according to a priority system and may also alternatively choose to allow certain operations that are near completion to complete.

If the uncompleted read erase operations are not currently being executed the terminate module may simply cause the operations to be canceled or otherwise skipped. For example the operations may be queued in one or more command queues and awaiting execution. The terminate module may remove read and erase operations from the queue such that they are not executed. The terminate module may alternatively cause the operations to be ignored or skipped that is the operations may be left in the queue but not selected for execution. In a further embodiment the terminate module may ignore one or more non essential command queues that hold non essential operations and select operations for execution from one or more essential command queues that hold essential operations or the like.

If the uncompleted operation is a program operation the identification module may determine whether or not an acknowledgement has been sent to the host . If the acknowledgement has not been sent the terminate module may choose to cancel the queued operation or reset the affect memory area as described above. In other embodiments program operations may be allowed to complete if they are in the storage device regardless of whether or not an acknowledgement has been sent.

If an acknowledgement has been sent the program operation is allowed to complete. As a result the data associated with the program operation is moved into nonvolatile memory as reported to the host . As discussed above the corruption module may purge corrupt data from the data write pipeline as part of the method . Similarly the completion module may flush partially filled buffers to ensure that data to be programmed is moved through the data write pipeline . As discussed above the corruption module and or the completion module may cause an indicator to be set which identifies the corrupt data to the storage device .

By reducing the number of operations to be executed by a nonvolatile storage device during a power failure the size cost and complexity of the secondary power supply can be reduced. In certain embodiments the focus is placed on particularly power hungry expensive operations such as erases that are less critical but consume considerable power. The system may further distinguish between essential programs those for which an acknowledgement has been sent to the host and non essential programs those for which no acknowledgement has been sent .

The systems methods and apparatus for power reduction management described above may be leveraged to implement an auto commit memory capable of implementing memory semantic write operations e.g. persistent writes at CPU memory write granularity and speed. By guaranteeing that certain commit actions for the write operations will occur even in the case of a power failure or other restart event in certain embodiments volatile memory such as DRAM SRAM BRAM or the like may be used as considered or represented as non volatile.

As used herein the term memory semantic operations or more generally memory operations refers to operations having a granularity synchronicity and access semantics of volatile memory accesses using manipulatable memory pointers or the like. Memory semantic operations may include but are not limited to load store peek poke write read set clear and so on. Memory semantic operations may operate at a CPU level of granularity e.g. single bytes words cache lines or the like and may be synchronous e.g. the CPU waits for the operation to complete . In certain embodiments providing access at a larger sized granularity such as cache lines may increase access rates provide more efficient write combining or the like than smaller sized granularity access.

The ACM may be available to computing devices and or applications both local and remote using one or more of a variety of memory mapping technologies including but not limited to memory mapped I O MMIO port I O port mapped IO PMIO Memory mapped file I O and the like. For example the ACM may be available to computing devices and or applications both local and remote using a PCI e Base Address Register BAR or other suitable mechanism. ACM may also be directly accessible via a memory bus of a CPU using an interface such as a double data rate DDR memory interface HyperTransport QuickPath Interconnect QPI or the like. Accordingly the ACM may be accessible using memory access semantics such as CPU load store DMA 3party DMA RDMA atomic test and set and so on. The direct memory semantic access to the ACM disclosed herein allows many of the system and or virtualization layer calls typically required to implement committed operations to be bypassed e.g. call backs via asynchronous Input Output interfaces may be bypassed . In some embodiments an ACM may be mapped to one or more virtual ranges e.g. virtual BAR ranges virtual memory addresses or the like . The virtual mapping may allow multiple computing devices and or applications to share a single ACM address range e.g. access the same ACM simultaneously within different virtual address ranges . An ACM may be mapped into an address range of a physical memory address space addressable by a CPU so that the CPU may use load store instructions to read and write data directly to the ACM using memory semantic accesses. A CPU in a further embodiment may map the physically mapped ACM into a virtual memory address space making the ACM available to user space processes or the like as virtual memory.

The ACM may be pre configured to commit its contents upon detection of a restart condition or other pre determined triggering event and as such operations performed on the ACM may be viewed as being instantly committed. For example an application may perform a write commit operation on the ACM using memory semantic writes that operate at CPU memory granularity and speed without the need for separate corresponding commit commands which may significantly increase the performance of applications affected by write commit latencies. As used herein a write commit operation is an operation in which an application writes data to a memory location e.g. using a memory semantic access and then issues a subsequent commit command to commit the operation e.g. to persistent storage or other commit mechanism .

Applications whose performance is based on write commit latency the time delay between the initial memory write and the subsequent persistent commit operation typically attempt to reduce this latency by leveraging a virtual memory system e.g. using a memory backed file . In this case the application performs high performance memory semantic write operations in system RAM but in order to commit the operations must perform subsequent commit commands to persist each write operation to the backing file or other persistent storage . Accordingly each write commit operation may comprise its own separate commit command. For example in a database logging application each log transaction must be written and committed before a next transaction is logged. Similarly messaging systems e.g. store and forward systems must write and commit each incoming message before receipt of the message can be acknowledged. The write commit latency therefore comprises a relatively fast memory semantic write followed by a much slower operation to commit the data to persistent storage. Write commit latency may include several factors including access times to persistent storage system call overhead e.g. translations between RAM addresses backing store LBA etc. and so on. Examples of applications that may benefit from reduced write commit latency include but are not limited to database logging applications filesystem logging messaging applications e.g. store and forward semaphore primitives and so on.

The systems apparatus and methods for auto commit memory disclosed herein may be used to significantly increase the performance of write commit latency bound applications by providing direct access to a memory region at any suitable level of addressing granularity including byte level page level cache line level or other memory region level that is guaranteed to be committed in the event of a system failure or other restart event without the application issuing a commit command. Accordingly the write commit latency of an application may be reduced to the latency of a memory semantic access a single write over a system bus .

Accordingly when data is written to the ACM it may not initially be committed per se is not necessarily stored on a persistent memory media and or state rather a pre configured process is setup to preserve the ACM data and its state if a restart event occurs while the ACM data is stored in the ACM . The pre configuring of this restart survival process is referred to herein as arming. The ACM may be capable of performing the pre configured commit action autonomously and with a high degree of assurance despite the system experiencing failure conditions or another restart event. As such an entity that stores data on the ACM may consider the data to be instantaneously committed or safe from loss or corruption at least as safe as if the data were stored in a non volatile storage device such as a hard disk drive tape storage media or the like.

In embodiments where the ACM comprises a volatile memory media the ACM may make the volatile memory media appear as a non volatile memory may present the volatile memory as a non volatile medium or the like because the ACM preserves data such as ACM data and or ACM metadata across system restart events. The ACM may allow a volatile memory media to be used as a non volatile memory media by determining that a trigger event such as a restart or failure condition has occurred copying the contents of the volatile memory media to a non volatile memory media during a hold up time after the trigger event and copying the contents back into the volatile memory media from the non volatile memory media after the trigger event is over power has been restored the restart event has completed or the like.

In one embodiment the ACM is at least byte addressable. A memory media of the ACM in certain embodiments may be natively byte addressable directly providing the ACM with byte addressability. In another embodiment a memory media of the ACM is not natively byte addressable but a volatile memory media of the ACM is natively byte addressable and the ACM writes or commits the contents of the byte addressable volatile memory media to the non byte addressable memory media of the ACM in response to a trigger event so that the volatile memory media renders the ACM byte addressable.

The ACM may be accessible to one or more computing devices such as the host . As used herein a computing device such as the host refers to a computing device capable of accessing an ACM. The host may be a computing device that houses the ACM as a peripheral the ACM may be attached to a system bus of the host the ACM may be in communication with the host over a data network and or the ACM may otherwise be in communication with the host . The host in certain embodiments may access the ACM hosted by another computing device. The access may be implemented using any suitable communication mechanism including but not limited to CPU programmed IO CPIO port mapped IO PMIO memory mapped IO MMIO a Block interface a PCI e bus Infiniband RDMA or the like. The host may comprise one or more ACM users . As used herein an ACM user refers to any operating system OS virtual operating platform e.g. an OS with a hypervisor a guest OS application process thread entity utility or the like that is configured to access the ACM .

The ACM may be physically located at one or more levels of the host . In one embodiment the ACM may be connected to a PCI e bus and may be accessible to the host with MMIO. In another embodiment the ACM may be directly accessible to a CPU of the host via a memory controller. For example the ACM may be directly attached to and or directly e.g. Quick Path Interconnect QPI in communication with a CPU of the host or the like. Volatile media of the ACM and non volatile backing media of the ACM in certain embodiments may not be physically co located within the same apparatus but may be in communication over a communications bus a data network or the like. In other embodiments as described below hardware components of the ACM may be tightly coupled and integrated in a single physical hardware apparatus. Volatile memory media and or non volatile memory media of the ACM in one embodiment may be integrated with or may otherwise cooperate with a CPU cache hierarchy of the host to take advantage of CPU caching technologies such as write combining or the like.

One or more ACM buffers in certain embodiments may be mapped into an address range of a physical memory address space addressable by a CPU a kernel or the like of the host device such as the memory system described below. For example one or more ACM buffers may be mapped as directly attached physical memory as MMIO addressable physical memory over a PCI e bus or otherwise mapped as one or more pages of physical memory. At least a portion of the physically mapped ACM buffers in a further embodiment may be mapped into a virtual memory address space accessible to user space processes or the like as virtual memory.

Allowing ACM users to directly address the ACM buffers in certain embodiments bypasses one or more layers of the traditional operating system memory stack of the host device providing direct load store operation access to kernel space and or user space applications. An operating system using a kernel module an application programming interface the storage management layer SML described below or the like in one embodiment maps and unmaps ACM buffers to and from the memory system for one or more ACM users and the ACM users may directly access an ACM buffer once the operating system maps the ACM buffer into the memory system . In a further embodiment the operating system may also service system flush calls for the ACM buffers or the like.

The SML and or the SML API described below in certain embodiments provide an interface for ACM users an operating system and or other entities to request certain ACM functions such as a map function an unmap function a flush function and or other ACM functions. To perform a flush operation in response to a flush request the ACM may perform a commit action for each ACM buffer associated with the flush request. Each ACM buffer is committed as indicated by the ACM metadata of the associated ACM buffer . A flush function in various embodiments may be specific to one or more ACM buffers system wide for all ACM buffers or the like. In one embodiment a CPU an operating system or the like for the host may request an ACM flush operation in response to or as part of a CPU cache flush a system wide data flush for the host or another general flush operation.

An ACM user an operating system or the like may request a flush operation to maintain data consistency prior to performing a maintenance operation such as a data snapshot or a backup to commit ACM data prior to reallocating an ACM buffer to prepare for a scheduled restart event or for other circumstances where flushing data from an ACM buffer may be beneficial. An ACM user an operating system or the like in certain embodiments may request that the ACM map and or unmap one or more ACM buffers to perform memory management for the ACM buffers to reallocate the ACM buffers between applications or processes to allocate ACM buffers for new data applications or processes to transfer use of the ACM buffers to a different host in shared ACM embodiments or to otherwise manipulate the memory mapping of the ACM buffers . In another embodiment the SML may dynamically allocate map and or unmap ACM buffers using a resource management agent as described below.

Since the ACM is guaranteed to auto commit the data stored thereon in the event of a trigger event the host or ACM user may view data written to the ACM as being instantaneously committed or non volatile as the host or ACM user may access the data both before and after the trigger event. Advantageously while the restart event may cause the ACM user to be re started or re initialized the data stored in the ACM is in the same state condition after the restart event as it was before the restart event. The host may therefore write to the ACM using memory write semantics and at CPU speeds and granularity without the need for explicit commit commands by relying on the pre configured trigger of the ACM to commit the data in the event of a restart or other trigger event .

The ACM may comprise a plurality of auto commit buffers each comprising respective ACM metadata . As discussed below the ACM metadata may include data to facilitate committing of ACM data in response to a triggering event for the auto commit buffer such as a logical identifier for data in the ACM buffer an identifier of a commit agent instructions for a commit process or other processing procedure security data or the like. The auto commit buffers may be of any suitable size from a single sector page byte or the like to a virtual or logical page size e.g. 80 to 400 kb . The size of the auto commit buffers may be adapted according to the storage capacity of the underlying non volatile storage media and or hold up time available from the secondary power supply .

In one embodiment the ACM may advertise or present to the host to ACM users or the like a storage capacity of the ACM buffers that is larger than an actual storage capacity of memory of the ACM buffers . To provide the larger storage capacity the ACM may dynamically map and unmap ACM buffers to the memory system and to the non volatile backing memory of the ACM such as the non volatile memory described above. For example the ACM may provide virtual address ranges for the ACM buffers and demand page data and or ACM buffers to the non volatile memory as ACM buffer accesses necessitate. In another embodiment for ACM buffers that are armed to commit to one or more predefined LBAs of the non volatile memory the ACM may dynamically move the ACM data and ACM metadata from the ACM buffers to the associated LBAs of the non volatile memory freeing storage capacity of the ACM buffers to provide a larger storage capacity. The ACM may further return the ACM data and ACM metadata back to one or more ACM buffers as ACM buffers become available certain addresses outside the data of currently loaded ACM buffers is requested or the like managing storage capacity of the ACM buffers .

The ACM is pre configured or armed to implement one or more triggered commit actions in response to a restart condition or other pre determined condition . As used herein a restart condition or event may include but is not limited to a software or hardware shutdown restart of a host a failure in a host computing device a failure of a component of the host e.g. failure of the bus a software fault e.g. an fault in software running on the host or other computing device a loss of the primary power connection an invalid shutdown or another event that may cause the loss of data stored in a volatile memory.

In one embodiment a restart event comprises the act of the host commencing processing after an event that can cause the loss of data stored within a volatile memory of the host or a component in the host . The host may commence resume processing once the restart condition or event has finished a primary power source is available and the like.

The ACM is configured to detect that a restart event condition has occurred and or respond to a restart event by initiating a recovery stage. During a recovery stage the ACM may restore the data of the ACM to the state prior to the restart event. Alternatively or in addition during the recovery stage the ACM may complete processing of ACM data or ACM metadata needed to satisfy a guarantee that data in the ACM is available to ACM users after the restart event. Alternatively or in addition during the recovery stage the ACM may complete processing of ACM data or ACM metadata needed to satisfy a guarantee that data in the ACM is committed after the restart event. As used herein commit means data in the ACM is protected from loss or corruption even after the restart event and is persisted as required per the arming information associated with the data. In certain embodiments the recovery stage includes processing ACM data and ACM metadata such that the ACM data is persisted even though the restart event occurred.

As used herein a triggered commit action is a pre configured commit action that is armed to be performed by the ACM in response to a triggering event e.g. a restart event a flush command or other pre determined event . In certain embodiments the triggered commit action persists at least enough ACM data and or ACM metadata to make data of the ACM available after a system restart to satisfy a guarantee of the ACM that the data will be accessible to an ACM user after a restart event in certain embodiments this guarantee is satisfied at least in part by committing and or persisting data of the ACM to non volatile memory media. A triggered commit action may be completed before during and or after a restart event. For example the ACM may write ACM data and ACM metadata to a predefined temporary location in the nonvolatile memory during a hold up time after a restart event and may copy the ACM data back into the ACM buffers to an intended location in the nonvolatile memory or perform other processing once the restart event is complete.

A triggered commit action may be armed when the ACM is requested and or a particular ACM buffer is allocated for use by a host . In some embodiments an ACM may be configured to implement a triggered commit action in response to other non restart conditions. For example an operation directed to a particular logical address e.g. a poke may trigger the ACM a flush operation may trigger the ACM or the like. This type of triggering may be used to commit the data of the ACM during normal operation e.g. non restart or non failure conditions .

The arming may occur when an auto commit buffer is mapped into the memory system of the host . Alternatively arming may occur as a separate operation. As used herein arming an auto commit buffer comprises performing the necessary configuration steps needed to complete the triggered action when the action is triggered. Arming may include for example providing the ACM metadata to the ACM or the like. In certain embodiments arming further includes performing the necessary configuration steps needed to complete a minimal set of steps for the triggered action such that the triggered action is capable of completing after a trigger event. In certain embodiments arming further includes verifying the arming data e.g. verifying that the contents of the auto commit buffer or portion thereof can be committed as specified in the ACM metadata and verifying that the ACM is capable and configured to properly perform the triggered action without error or interruption.

The verification may ensure that once armed the ACM can implement the triggered commit action when required. If the ACM metadata cannot be verified e.g. the logical identifier or other ACM metadata is invalid corrupt unavailable or the like the arming operation may fail memory semantic operations on the auto commit buffer may not be allowed unit the auto commit buffer is successfully armed with valid ACM metadata . For example an auto commit buffer that is backed by a hard disk having a one to one mapping between LBA and physical address may fail to arm if the LBA provided for the arming operation does not map to a valid and operational physical address on the disk. Verification in this case may comprise querying the disk to determine whether the LBA has a valid corresponding physical address and or using the physical address as the ACM metadata of the auto commit buffer .

The armed triggered commit actions are implemented in response to the ACM or other entity detecting and or receiving notification of a triggering event such as a restart condition. In some embodiments an armed commit action is a commit action that can be performed by the ACM and that requires no further communication with the host or other devices external to the isolation zone of the ACM discussed below . Accordingly the ACM may be configured to implement triggered commit actions autonomously of the host and or other components thereof. The ACM may guarantee that triggered commit actions can be committed without errors and or despite external error conditions. Accordingly in some embodiments the triggered commit actions of the ACM do not comprise and or require potentially error introducing logic computations and or calculations. In some embodiments a triggered commit action comprises committing data stored on the volatile ACM to a persistent storage location. In other embodiments a triggered commit action may comprise additional processing of committed data before during and or after a triggering event as described below. The ACM may implement pre configured triggered commit actions autonomously the ACM may be capable of implementing triggered commit actions despite failure or restart conditions in the host loss of primary power or the like. The ACM can implement triggered commit actions independently due to arming the ACM as described above.

The ACM metadata for an ACM buffer in certain embodiments identifies the data of the ACM buffer . For example the ACM metadata may identify an owner of the data may describe the data itself or the like. In one embodiment an ACM buffer may have multiple levels of ACM metadata for processing by multiple entities or the like. The ACM metadata may include multiple nested headers that may be unpackaged upon restart and used by various entities or commit agents to determine how to process the associated ACM data to fulfill the triggered commit action as described above. For example the ACM metadata may include block metadata file metadata application level metadata process execution point or callback metadata and or other levels of metadata. Each level of metadata may be associated with a different commit agent or the like. In certain embodiments the ACM metadata may include security data such as a signature for an owner of the associated ACM data a pre shared key a nonce or the like which the ACM may use during recovery to verify that a commit agent an ACM user or the like is authorized to access committed ACM metadata and or associated ACM data. In this manner the ACM may prevent ownership spoofing or other unauthorized access. In one embodiment the ACM does not release ACM metadata and or associated ACM data until a requesting commit agent ACM user or the like provides valid authentication such as a matching signature or the like.

One or more commit agents such as the commit management apparatus described below with regard to in certain embodiments process ACM data based on the associated ACM metadata to execute a triggered commit action. A commit agent in various embodiments may comprise software such as a device driver a kernel module the SML a thread a user space application or the like and or hardware such as the controller described below that is configured to interpret ACM metadata and to process the associated ACM data according to the ACM metadata . In embodiments with multiple commit agents the ACM metadata may identify one or more commit agents to process the associated ACM data. The ACM metadata may identify a commit agent in various embodiments by identifying a program function of the commit agent to invoke e.g. a file path of the program by including computer executable code of the commit agent e.g. binary code or scripts by including a unique identifier indicating which of a set of registered commit agents to use and or by otherwise indicating a commit agent associated with committed ACM metadata . The ACM metadata in certain embodiments may be a functor or envelope which contains the information such as function pointer and bound parameters for a commit agent to commit the ACM data upon restart recovery.

In one embodiment a primary commit agent processes ACM metadata and hands off or transfers ACM metadata and or ACM data to one or more secondary commit agents identified by the ACM metadata . A primary commit agent in one embodiment may be integrated with the ACM the controller or the like. An ACM user or other third party in certain embodiments may provide a secondary commit agent for ACM data that the ACM user or other third party owns and the primary commit agent may cooperate with the provided secondary commit agent to process the ACM data. The one or more commit agents for ACM data in one embodiment ensure and or guarantee that the ACM data remains accessible to an owner of the ACM data after a restart event. As described above with regard to triggered commit actions a commit agent may process ACM metadata and associated ACM data to perform one or more triggered commit actions before during and or after a trigger event such as a failure or other restart event.

In one embodiment a commit agent in cooperation with the ACM or the like may store the ACM metadata in a persistent or non volatile location in response to a restart or other trigger event. The commit agent may store the ACM metadata at a known location may store pointers to the ACM metadata at a known location may provide the ACM metadata to an external agent or data store or the like so that the commit agent may process the ACM metadata and associated ACM data once the restart or other trigger event has completed. The known location may include one or more predefined logical block addresses or physical addresses of the non volatile memory a predefined file or the like. In certain embodiments hardware of the ACM is configured to cooperate to write the ACM metadata and or pointers to the ACM metadata at a known location. In one embodiment the known location may be a temporary location that stores the ACM data and ACM metadata until the host has recovered from a restart event and the commit agent may continue to process the ACM data and ACM metadata . In another embodiment the location may be a persistent location associated with the ACM metadata .

In response to completion of a restart event or other trigger event during recovery in one embodiment a commit agent may locate and retrieve the ACM metadata from the non volatile memory from a predefined location or the like. The commit agent in response to locating and retrieving the ACM metadata locates the ACM data associated with the retrieved ACM metadata . The commit agent in certain embodiments may locate the ACM data in a substantially similar manner as the commit agent locates the ACM metadata retrieving ACM data from a predefined location retrieving pointers to the ACM data from a predefined location receiving the ACM data from an external agent or data store or the like. In one embodiment the ACM metadata identifies the associated ACM data and the commit agent uses the ACM metadata to locate and retrieve the associated ACM data. For example the commit agent may use a predefined mapping to associate ACM data with ACM metadata e.g the Nth piece of ACM data may be associated with the Nth piece of ACM metadata or the like the ACM metadata may include a pointer or index for the associated ACM data or another predefined relationship may exist between committed ACM metadata and associated ACM data. In another embodiment an external agent may indicate to the commit agent where associated ACM data is located.

In response to locating and retrieving the ACM metadata and associated ACM data the commit agent interprets the ACM metadata and processes the associated ACM data based on the ACM metadata . For example in one embodiment the ACM metadata may identify a block storage volume and LBA s where the commit agent is to write the ACM data upon recovery. In another embodiment the ACM metadata may identify an offset within a file within a file system where the commit agent is to write the ACM data upon recovery. In a further embodiment the ACM metadata may identify an application specific persistent object where the commit agent is to place the ACM data upon recovery such as a database record or the like. The ACM metadata in an additional embodiment may indicate a procedure for the commit agent to call to process the ACM data such as a delayed procedure call or the like. In an embodiment where the ACM advertises or presents volatile ACM buffers as nonvolatile memory the ACM metadata may identify an ACM buffer where the commit agent is to write the ACM data upon recovery.

In certain embodiments the ACM metadata may identify one or more secondary commit agents to further process the ACM metadata and or associated ACM data. A secondary commit agent may process ACM metadata and associated ACM data in a substantially similar manner to the commit agent described above. Each commit agent may process ACM data in accordance with a different level or subset of the ACM metadata or the like. The ACM metadata may identify a secondary commit agent in various embodiments by identifying a program function of the secondary commit agent to invoke e.g. a file path of the program by including computer executable code of the secondary commit agent by including a unique identifier indicating which of a set of registered secondary commit agents to use and or by otherwise indicating a secondary commit agent associated with committed ACM metadata .

In one embodiment a secondary commit agent processes a remaining portion of the ACM metadata and or of the ACM data after a previous commit agent has processed the ACM metadata and or the ACM data. In a further embodiment the ACM metadata may identify another non volatile medium separate from the ACM for the secondary commit agent to persist the ACM data even after a host experiences a restart event. By committing the ACM metadata and the associated ACM data from the ACM buffers in response to a trigger event such as a failure or other restart condition and processing the ACM metadata and the associated ACM data once the trigger event has completed or recovered the ACM may guarantee persistence of the ACM data and or performance of the triggered commit action s defined by the ACM metadata .

The ACM is communicatively coupled to a host which like the host described above may comprise operating systems virtual machines applications a processor complex a central processing unit CPU and the like. In the example these entities are referred to generally as ACM users . Accordingly as used herein an ACM user may refer to an operating system a virtual machine operating system e.g. hypervisor an application a library a CPU fetch execute algorithm or other program or process. The ACM may be communicatively coupled to the host as well as the ACM users via a bus such as a system bus a processor s memory exchange bus or the like e.g. HyperTransport QuickPath Interconnect QPI PCI bus PCI e bus or the like . In some embodiments the bus comprises the primary power connection e.g. the solid state storage device may be powered through the bus . Although some embodiments described herein comprise solid state storage devices such as solid state storage device the disclosure is not limited in this regard and could be adapted to use any suitable storage device and or storage media.

The ACM may be tightly coupled to the device used to perform the triggered commit actions. For example the ACM may be implemented on the same device peripheral card or within the same isolation zone as the controller and or secondary power source . The tight coupling of the ACM to the components used to implement the triggered commit actions defines an isolation zone which may provide an acceptable level of assurance based on industry standards or other metric that the ACM is capable of implementing the triggered auto commit actions in the event of a restart condition. In the example the isolation zone of the ACM is provided by the tight coupling of the ACM with the autonomous controller and secondary power supply discussed below .

The controller may comprise an I O controller such as a network controller e.g. a network interface controller storage controller dedicated restart condition controller or the like. The controller may comprise firmware hardware a combination of firmware and hardware or the like. In the example the controller comprises a storage controller such as the storage controller and or solid state storage device controller described above. The controller may be configured to operate independently of the host . As such the controller may be used to implement the triggered commit action s of the ACM despite the restart conditions discussed above such as failures in the host and or ACM users and or loss of the primary power connection .

The ACM is powered by a primary power connection which like the primary power connection described above may be provided by a system bus bus external power supply the host or the like. In certain embodiments the ACM also includes and or is coupled to a secondary power source . The secondary power source may power the ACM in the event of a failure to the primary power connection . The secondary power source may be capable of providing at least enough power to enable the ACM and or controller to autonomously implement at least a portion of a pre configured triggered commit action s when the primary power connection has failed. The ACM in one embodiment commits or persists at least enough data e.g. ACM data and ACM metadata while receiving power from the secondary power source to allow access to the data once the primary power connection has been restored. In certain embodiments as described above the ACM may perform at least a portion of the pre configured triggered commit action s after the primary power connection has been restored using one or more commit agents or the like.

The ACM may comprise volatile memory storage. In the example the ACM includes one or more auto commit buffers . The auto commit buffers may be implemented using a volatile Random Access Memory RAM . In some embodiments the auto commit buffers may be embodied as independent components of the ACM e.g. in separate RAM modules . Alternatively the auto commit buffers may be implemented on embedded volatile memory e.g. BRAM available within the controller a processor complex an FPGA or other component of the ACM .

Each of the auto commit buffers may be pre configured armed with a respective triggered commit action. In some embodiments each auto commit buffer may comprise its own respective ACM metadata . The ACM metadata in some embodiments identifies how and or where the data stored on the auto commit buffer is to be committed. In some examples the ACM metadata may comprise a logical identifier e.g. an object identifier logical block address file name or the like associated with the data in the auto commit buffer . The logical identifier may be predefined. In one embodiment when an auto commit buffer is committed the data therein may be committed with the ACM metadata e.g. the data may be stored at a physical storage location corresponding to the logical identifier and or in association with the logical identifier . To facilitate committing of ACM data during a hold up time after a restart event the ACM may write ACM data and ACM metadata in a single atomic operation such as a single page write or the like. To permit writing of ACM and ACM metadata in a single atomic operation the ACM buffers may be sized to correspond to a single write unit for a non volatile storage media that is used by the ACM . In some embodiments the ACM metadata may comprise a network address an LBA or another identifier of a commit location for the data.

In a further embodiment a logical identifier may associate data of an auto commit buffer with an owner of the data so that the data and the owner maintain the ownership relationship after a restart event. For example the logical identifier may identify an application an application type a process ID an ACM user or another entity of a host device so that the ACM data is persistently associated with the identified entity. In one embodiment a logical identifier may be a member of an existing namespace such as a file system namespace a user namespace a process namespace or the like. In other embodiments a logical identifier may be a member of a new or separate namespace such as an ACM namespace. For example a globally unique identifier namespace as is typically used in distributed systems for identifying communicating entities may be used as an ACM namespace for logical identifiers. The ACM may process committed ACM data according to a logical identifier for the data once a restart event has completed. For example the ACM may commit the ACM data to a logical identifier associated with a temporary location in response to a restart event and may write the ACM data to a persistent location identified by another logical identifier during recovery after the restart event.

As described above the ACM may be tightly coupled with the components used to implement the triggered commit actions e.g. the ACM is implemented within an isolation zone which ensures that the data on the ACM will be committed in the event of a restart condition. As used herein a tight coupling refers to a configuration wherein the components used to implement the triggered commit actions of the ACM are within the same isolation zone or two or more distinct trusted isolation zones and are configured to operate despite external failure or restart conditions such as the loss of power invalid shutdown host failures or the like.

The ACM may be accessible by the host and or ACM users running thereon. Access to the ACM may be provided using memory access semantics such as CPU load store commands DMA commands 3rd party DMA commands RDMA commands atomic test and set commands manipulatable memory pointers and so on. In some embodiments memory semantic access to the ACM is implemented over the bus e.g. using a PCI e BAR as described below .

In a memory semantic paradigm ACM users running on the host may access the ACM via a memory system of the host . The memory system may comprise a memory management unit virtual memory system virtual memory manager virtual memory subsystem or similar memory address space implemented by an operating system a virtualization system e.g. hypervisor an application or the like. A portion of the ACM e.g. one or more auto commit buffers may be mapped into the memory system such that memory semantic operations implemented within the mapped memory address range ACM address range are performed on the ACM .

The SML in certain embodiments allocates and or arbitrates the storage capacity of the ACM between multiple ACM users using a resource management agent or the like. The resource management agent of the SML may comprise a kernel module provided to an operating system of the host device a device driver a thread a user space application or the like. In one embodiment the resource management agent determines how much storage capacity of the ACM buffers to allocate to an ACM user and how long the allocation is to last. Because in certain embodiments the ACM commits or persists data across restart events the resource management agent may allocate storage capacity of ACM buffers across restart events.

The resource management agent may assign different ACM buffers to different ACM users such as different kernel and or user space applications. The resource management agent may allocate ACM buffers to different usage types may map ACM buffers to different non volatile memory locations for destaging or the like. In one embodiment the resource management agent may allocate the ACM buffers based on commit agents associated with the ACM buffers by the ACM metadata or the like. For example a master commit agent may maintain an allocation map in ACM metadata identifying allocation information for ACM buffers of the ACM and identifying in one embodiment one or more secondary commit agents and the master commit agent may allocate a portion of the ACM buffers to each of the secondary commit agents . In another embodiment commit agents may register with the resource management agent may request resources such as ACM buffers from the resource management agent or the like. The resource management agent may use a predefined memory management policy such as a memory pressure policy or the like to allocate and arbitrate ACM buffer storage capacity between ACM users .

In some embodiments establishing an association between an ACM address range within the memory system and the ACM may comprise pre configuring arming the corresponding auto commit buffer s with a triggered commit action. As described above this pre configuration may comprise associating the auto commit buffer with a logical identifier or other metadata which may be stored in the ACM metadata of the buffer . As described above the ACM may be configured to commit the buffer data to the specified logical identifier in the event of a restart condition or to perform other processing in accordance with the ACM metadata .

Memory semantic access to the ACM may be implemented using any suitable address and or device association mechanism. In some embodiments memory semantic access is implemented by mapping one or more auto commit buffers of the ACM into the memory system of the host . In some embodiments this mapping may be implemented using the bus . For example the bus may comprise a PCI e or similar communication bus and the mapping may comprise associating a Base Address Register BAR of an auto commit buffer of the ACM on the bus with the ACM address range in the memory system e.g. the host mapping a BAR into the memory system .

The association may be implemented by an ACM user e.g. by a virtual memory system of an operating system or the like through an API of a storage layer such as the storage management layer SML . The SML may be configured to provide access to the auto commit memory to ACM users . The storage management layer may comprise a driver kernel level application user level application library or the like. One example of an SML is the Virtual Storage Layer of Fusion io Inc. of Salt Lake City Utah. The SML may provide a SML API comprising inter alia an API for mapping portions of the auto commit memory into the memory system of the host for unmapping portions of the auto commit memory from the memory system of the host for flushing the ACM buffers and the like. The SML may be configured to maintain metadata which may include a forward index comprising associations between logical identifiers of a logical address space and physical storage locations on the auto commit memory and or persistent storage media. In some embodiments ACM may be associated with one or more virtual ranges that map to different address ranges of a BAR or other addressing mechanism . The virtual ranges may be accessed e.g. mapped by different ACM users . Mapping or exposing a PCI e ACM BAR to the host memory may be enabled on demand by way of a SML API call.

The SML API may comprise interfaces for mapping an auto commit buffer into the memory system . In some embodiments the SML API may extend existing memory management interfaces such as malloc calloc or the like to map auto commit buffers into the virtual memory range of ACM user applications e.g. a malloc call through the SML API may map one or more auto commit buffers into the memory system . Alternatively or in addition the SML API may comprise one or more explicit auto commit mapping functions such as ACM alloc ACM free or the like. Mapping an auto commit buffer may further comprise configuring a memory system of the host to ensure that memory operations are implemented directly on the auto commit buffer e.g. prevent caching memory operations within a mapped ACM address range .

The association between the ACM address range within the host memory system and the ACM may be such that memory semantic operations performed within a mapped ACM address range are implemented directly on the ACM without intervening system RAM or other intermediate memory in a typical write commit operation additional layers of system calls or the like . For example a memory semantic write operation implemented within the ACM address range may cause data to be written to the ACM on one or more of the auto commit buffers . Accordingly in some embodiments mapping the ACM address range may comprise disabling caching of memory operations within the ACM address range such that memory operations are performed on an ACM and are not cached by the host e.g. cached in a CPU cache in host volatile memory or the like . Disabling caching within the ACM address range may comprise setting a non cacheable flag attribute associated with the ACM range when the ACM range is defined.

As discussed above establishing an association between the host memory system and the ACM may comprise arming the ACM to implement a pre determined triggered commit action. The arming may comprise providing the ACM with a logical identifier e.g. a logical block address a file name a network address a stripe or mirroring pattern or the like . The ACM may use the logical identifier to arm the triggered commit action. For example the ACM may be triggered to commit data to a persistent storage medium using the logical identifier e.g. the data may be stored at a physical address corresponding to the logical identifier and or the logical identifier may be stored with the data in a log based data structure . Arming the ACM allows the host to view subsequent operations performed within the ACM address range and on the ACM as being instantly committed enabling memory semantic write granularity e.g. byte level operations and speed with instant commit semantics.

Memory semantic writes such as a store operation for a CPU are typically synchronous operations such that the CPU completes the operation before handling a subsequent operation. Accordingly memory semantic write operations performed in the ACM memory range can be viewed as instantly committed obviating the need for a corresponding commit operation in the write commit operation which may significantly increase the performance of ACM users affected by write commit latency. The memory semantic operations performed within the ACM memory range may be synchronous. Accordingly ACM may be configured to prevent the memory semantic operations from blocking e.g. waiting for an acknowledgement from other layers such as the bus or the like . Moreover the association between ACM address range and the ACM allow memory semantic operations to bypass system calls e.g. separate write and commit commands and their corresponding system calls that are typically included in write commit operations.

Data transfer between the host and the ACM may be implemented using any suitable data transfer mechanism including but not limited to the host performing processor IO operations PIO with the ACM via the bus the ACM or other device providing one or more DMA engines or agents data movers to transfer data between the host and the ACM the host performing processor cache write flush operations or the like.

As discussed above an ACM may be configured to automatically perform a pre configured triggered commit action in response to detecting certain conditions e.g. restart or failure conditions . In some embodiments the triggered commit action may comprise committing data stored on the ACM to a persistent storage media. Accordingly in some embodiments an ACM such as the ACM described above may be comprise persistent storage media. is a block diagram of a system depicting an embodiment of an ACM configured to implement triggered commit actions which may include committing data to a persistent solid state storage.

The ACM of the example may be tightly coupled to the solid state storage device which comprises a controller . The controller may comprise a write data pipeline and a read data pipeline which may operate as described above. The solid state storage device may be capable of persisting data on a non volatile memory such as solid state storage media.

A commit management apparatus is used to commit data to the non volatile memory in response to a trigger event such as loss of primary power connection or other pre determined trigger event. Accordingly the commit management apparatus may comprise and or be configured to perform the functions of the power management apparatus described above. The commit management apparatus may be further configured to commit data on the ACM e.g. the contents of the auto commit buffers to the non volatile memory in response to a restart condition or on request from the host and or ACM users and in accordance with the ACM metadata . The commit management apparatus is one embodiment of a commit agent .

The data on the ACM may be committed to the persistent storage in accordance with the ACM metadata such as a logical identifier or the like. The ACM may commit the data to a temporary location for further processing after a restart event may commit the data to a final intended location or the like as described above. If the non volatile memory is sequential storage device committing the data may comprise storing the logical identifier or other ACM metadata with the contents of the auto commit buffer e.g. in a packet or container header . If the non volatile memory comprises a hard disk having a 1 1 mapping between logical identifier and physical address the contents of the auto commit buffer may be committed to the storage location to which the logical identifier maps. Since the logical identifier or other ACM metadata associated with the data is pre configured e.g. armed the ACM implements the triggered commit action independently of the host . The secondary power supply supplies power to the volatile auto commit buffers of the ACM until the triggered commit actions are completed and or confirmed to be completed or until the triggered commit actions are performed to a point at which the ACM may complete the triggered commit actions during recovery after a restart event.

In some embodiments the ACM commits data in a way that maintains an association between the data and its corresponding logical identifier per the ACM metadata . If the non volatile memory comprises a hard disk the data may be committed to a storage location corresponding to the logical identifier which may be outside of the isolation zone e.g. using a logical identifier to physical address conversion . In other embodiments in which the non volatile memory comprises a sequential media such as solid state storage media the data may be stored sequentially and or in a log based format as described in above and or in U.S. Provisional Patent Application Publication No. 61 373 271 entitled APPARATUS SYSTEM AND METHOD FOR CACHING DATA and filed 12 Aug. 2010 which is hereby incorporated by reference in its entirety. The sequential storage operation may comprise storing the contents of an auto commit buffer with a corresponding logical identifier as indicated by the ACM metadata . In one embodiment the data of the auto commit buffer and the corresponding logical identifier are stored together on the media according to a predetermined pattern. In certain embodiments the logical identifier is stored before the contents of the auto commit buffer . The logical identifier may be included in a header of a packet comprising the data or in another sequential and or log based format. The association between the data and logical identifier may allow a data index to be reconstructed as described above.

As described above the auto commit buffers of the ACM may be mapped into the memory system of the host enabling the ACM users of access these buffers using memory access semantics. In some embodiments the mappings between logical identifiers and auto commit buffers may leverage a virtual memory system of the host .

For example an address range within the memory system may be associated with a memory mapped file. As discussed above a memory mapped file is a virtual memory abstraction in which a file portion of a file or block device is mapped into the memory system address space for more efficient memory semantic operations on data of the solid state storage device . An auto commit buffer may be mapped into the host memory system using a similar abstraction. The ACM memory range may therefore be represented by a memory mapped file. The backing file must be stored on the non volatile memory within the isolation zone See below or another network attached solid state storage device also protected by an isolation zone . The auto commit buffers may correspond to only a portion of the file the file itself may be very large exceeding the capacity of the auto commit buffers and or the non volatile memory . When a portion of a file is mapped to an auto commit buffer the ACM user or other entity may identify a desired offset within the file and the range of blocks in the file that will operate with ACM characteristics e.g. have ACM semantics . This offset will have a predefined logical identifier and the logical identifier and range may be used to trigger committing the auto commit buffer s mapped within the file. Alternatively a separate offset for a block or range of blocks into the file may serve as a trigger for committing the auto commit buffer s mapped to the file. For example anytime a memory operation load store poke etc. is performed on data in the separate offset or range of blocks may result in a trigger event that causes the auto commit buffer s mapped to the file to be committed.

The underlying logical identifier may change however e.g. due to changes to other portions of the file file size changes etc. . When a change occurs the SML via the SML API an ACM user or other entity may update the ACM metadata of the corresponding auto commit buffers . In some embodiments the SML may be configured to query the host operating system hypervisor or other application for updates to the logical identifier of files associated with auto commit buffers . The queries may be initiated by the SML API and or may be provided as a hook callback mechanism into the host . When the ACM user no longer needs the auto commit buffer the SML may de allocate the buffer as described above. De allocation may further comprise informing the host that updates to the logical identifier are no longer needed.

In some embodiments a file may be mapped across multiple storage devices e.g. the storage devices may be formed into a RAID group may comprise a virtual storage device or the like . Associations between auto commit buffers and the file may be updated to reflect the file mapping. This allows the auto commit buffers to commit the data to the proper storage device. The ACM metadata of the auto commit buffers may be updated in response to changes to the underlying file mapping and or partitioning as described above. Alternatively the file may be locked to a particular mapping or partition while the auto commit buffers are in use. For example if a remapping repartitioning of a file is required the corresponding auto commit buffers may commit data to the file and then be re associated with the file under the new mapping partitioning scheme. The SML API may comprise interfaces and or commands for using the SML to lock a file release a file and or update ACM metadata in accordance with changes to a file.

Committing the data to solid state non volatile storage may comprise the storage controller accessing data from the ACM auto commit buffers associating the data with the corresponding logical identifier e.g. labeling the data and injecting the labeled data into the write data pipeline as described above. In some embodiments to ensure there is a page program command capable of persisting the ACM data the storage controller maintains two or more pending page programs during operation. The ACM data may be committed to the solid state non volatile memory before writing the power loss identifier power cut fill pattern described above.

In some embodiments the ACMs A and B may implement a striping scheme e.g. a RAID scheme . In this case different portions of the host data may be sent to different ACMs A and or B. Driver level software such as a volume manager implemented by the SML and or operating system may map host data to the proper ACM per the striping pattern.

In some configurations the memory access semantics provided by the ACMs may be adapted according to a particular storage striping pattern. For example if host data is mirrored from the ACM A to the ACM B a memory semantic write may not complete and or an acknowledgement may not be returned until the ACM A verifies that the data was sent to the ACM B under the instant commit semantic . Similar adaptations may be implemented when ACMs are used in a striping pattern e.g. a memory semantic write may be not return and or be acknowledged until the striping pattern for a particular operation is complete . For example in a copy on write operation the ACM A may store the data of an auto commit buffer and then cause the data to be copied to the ACM B. The ACM A may not return an acknowledgment for the write operation or allow the data to be read until the data is copied to the ACM B.

The use of mirrored ACM devices A and B may be used in a high availability configuration. For example the ACM devices A and B may be implemented in separate host computing devices. Memory semantic accesses to the devices A and B are mirrored between the devices as described above e.g. using PCI e access . The devices may be configured to operate in high availability mode such that device proxying may not be required. Accordingly trigger operations as well as other memory semantic accesses may be mirrored across both devices A and B but the devices A and B may not have to wait for a acknowledge from the other before proceeding which removes the other device from the write commit latency path.

The commit management apparatus includes a monitor module which may be configured to detect restart conditions such as power loss or the like. The monitor module may be configured to sense triggering events such as restart conditions e.g. shutdown restart power failures communication failures host or application failures and so on and in response to initiate the commit module to initiate the commit loss mode of the apparatus failure loss mode and or to trigger the operations of other modules such as modules and or . The commit module includes an identification module terminate module corruption module and completion module which may operate as described above.

The identification module may be further configured to identify triggered commit actions to be performed for each ACM buffer of the ACM . As discussed above the identification module may prioritize operations based on relative importance with acknowledged operations being given a higher priority than non acknowledged operations. The contents of auto commit buffers that are armed to be committed may be assigned a high priority due to the instant commit semantics supported thereby. In some embodiments the ACM triggered commit actions may be given a higher priority than the acknowledged contents of the write data pipeline . Alternatively the contents of armed auto commit buffers may be assigned the next highest priority. The priority assignment may be user configurable via an API IO control IOCTRL or the like .

The termination module terminates non essential operations to allow essential to continue as described above. The termination module may be configured to hold up portions of the ACM that are armed to be committed e.g. armed auto commit buffers and may terminate power to non armed unused portions of the auto commit memory . The termination module may be further configured to terminate power to portions of the ACM individual auto commit buffers as the contents of those buffers are committed.

The corruption module identifies corrupt or potentially corrupt data in the write data pipeline as described above. The module may be further configured to identify corrupt ACM data data that was written to the ACM during a power disturbance or other restart condition per above . The corruption module may be configured to prevent corrupt data on the ACM from being committed in a triggered commit action.

An ACM module is configured to access armed auto commit buffers in the auto commit memory identify the ACM metadata associated therewith e.g. label the data with the corresponding logical identifier per the ACM metadata and inject the data and metadata into the write data pipeline of the solid state storage controller . In some embodiments the logical identifier or other ACM metadata of the auto commit buffer may be stored in the buffer itself. In this case the contents of the auto commit buffer may be streamed directly into a sequential and or log based storage device without first identifying and or labeling the data. The ACM module may inject data before or after data currently in the write data pipeline . In some embodiments data committed from the ACM is used to fill out the remainder of a write buffer of the write data pipeline after removing potentially corrupt data . If the remaining capacity of the write buffer is insufficient the write buffer is written to the solid state storage and a next write buffer is filled with the remaining ACM data.

As discussed above in some embodiments the solid state storage controller may maintain an armed write operation logical page write to store the contents of the write data pipeline in the event of power loss. When used with an ACM two or more armed write operations logical page writes may be maintained to ensure the contents of both the write data pipeline and all the armed buffers of the ACM can be committed in the event of a restart condition. Because a logical page in the write buffer may be partially filled when a trigger event occurs the write buffer is sized to hold at least one more logical page of data than the total of all the data stored in all ACM buffers of the ACM and the capacity of data in the write data pipeline that has been acknowledged as persisted. In this manner there will be sufficient capacity in the write buffer to complete the persistence of the ACM in response to a trigger event. Accordingly the auto commit buffers may be sized according to the amount of data the ACM is capable of committing. Once this threshold is met the SML may reject requests to use ACM buffers until more become available.

In the depicted embodiment of the write buffer includes one incomplete page and two complete pages . In one embodiment the pages comprise logical pages as described above. The armed write operations may each correspond to one of the pages . The two full pages may correspond to two separate auto commit buffers. The armed write operations commands are configured to write the pages and to the non volatile storage media autonomously without any further need for addressing information or calculations . In some embodiments the pages and may be written using only components within the isolation zone of the ACM . The partially filled page may comprise data in the pipeline at the time the restart condition occurred.

The completion module is configured to flush the write data pipeline regardless of whether the certain buffers packets and or pages are completely filled. The completion module is configured to perform the flush and insert the related padding data after data on the ACM if any has been injected into the write data pipeline . The completion module may be further configured to inject completion indicator into the write data pipeline which may be used to indicate that a restart condition occurred e.g. a restart condition fill pattern . This fill pattern may be included in the write data pipeline after injecting the triggered data from the ACM .

As discussed above the secondary power supply may be configured to provide sufficient power to store the contents of the ACM as well as data in the write data pipeline . Storing this data may comprise one or more write operations e.g. page program operations in which data is persistently stored on the solid state storage media . In the event a write operation fails another write operation on a different storage location may be attempted. The attempts may continue until the data is successfully persisted on the solid state storage media . The secondary power supply may be configured to provide sufficient power for each of a plurality of such page program operations to complete. Accordingly the secondary power supply may be configured to provide sufficient power to complete double or more page program write operations as required to store the data of the ACM and or write data pipeline .

The ACM module identifies triggered auto commit buffers of the ACM labels the data with a corresponding logical identifier per ACM metadata and injects the labeled data into the write data pipeline as described above. The data of the armed ACM buffers may be injected according to a relative priority determined by the identification module .

As discussed above the completion module flushes data through partially filled buffers in the write data pipeline during a failure mode. The completion module may fill out the partially filled buffers with padding data which may include a failure mode indicator. The padding data and or indicator may be included after triggered data in the ACM has been streamed into the write data pipeline . The ACM data may be used to fill out incomplete packets e.g. in the write data pipeline.

If the remaining capacity of the write buffer is insufficient to store the triggered ACM data the write buffer is committed to the solid state storage media and a next write buffer is filled with the ACM data. Accordingly whereas the solid state storage controller of the write data pipeline may comprise a continuously armed write operation sufficient to commit the write buffer when used with the ACM two or more write operations may be continuously armed. The number of continuously armed write operations may be adapted according to the capacity of the ACM e.g. if the ACM could include enough triggered data to fill two 2 additional write operations three or more write operations may be continuously armed . As discussed above the secondary power supply may be configured to provide sufficient power to complete the required write operations. In some embodiments the secondary power supply may be capable of supplying power for additional write operations to allow for one or more retries of failed write operations to the solid state storage media . In some embodiments two or more write operations may occur simultaneously e.g. concurrently on separate banks of the solid state storage media as described above . The simultaneous write operations may reduce the hold up time required to commit data of the ACM to the solid state storage media . Alternatively or in addition certain write operations may occur in parallel. For example while a first write operation is taking place a second write operation may be in progress e.g. data of a second write operation may be streaming to write or programming buffers of the solid state storage media while a first write operation is in progress which may similarly reduce hold up time requirements. As depicted in the ACM commit management apparatus write data pipeline solid state storage media and secondary power supply are tightly coupled in a isolation zone . The tight coupling within isolation zone may isolate the ACM and related components from external failure conditions which may ensure that the triggered data on the ACM buffers is committed to the solid state storage media in the event of a failure or other restart. This assurance may be relied upon by the host and or applications implemented thereon to treat memory semantic operations performed on the ACM buffers as instantly committed. 

The host may be communicatively coupled to the ACM via a bus which may comprise a PCI e bus or the like. Portions of the ACM are made accessible to the host may mapping in auto commit buffers into the host . In some embodiments mapping comprises associating an address range within the host memory system with an auto commit buffer of the ACM . These associations may be enabled using the SML API and or SML available on the host .

The SML may comprise libraries and or provide interfaces e.g. SML API to implement the memory access semantics described above. The API may be used to access the ACM using memory access semantics via a memory semantic access module . Other types of access such as access to the solid state storage may be provided via a block device interface .

The SML may be configured to memory map auto commit buffers of the ACM into the memory system via the SML API . The memory map may use a virtual memory abstraction of the memory system . For example a memory map may be implemented using a memory mapped file abstraction. In this example the operating system or application designates a file to be mapped into the memory system . The file is associated with a logical identifier LID e.g. logical block address which may be maintained by a file system an operating system or the like.

The memory mapped file may be associated with an auto commit buffer of the ACM . The association may be implemented by the SML using the bus . The SML associates the address range of the memory mapped file in the memory system with a device address of an auto commit buffer on the ACM . The association may comprise mapping a PCI e BAR into the memory system . In the example the ACM address range in the memory system is associated with the auto commit buffer .

As discussed above providing memory access semantics to the ACM may comprise arming the ACM to commit data stored thereon in the event of failure or other restart. The pre configured arming ensures that in the event of a restart data stored on the ACM will be committed to the proper logical identifier. The pre configuration of the trigger condition enables applications to access the auto commit buffer using instant commit memory access semantics. The logical identifier used to arm the auto commit buffer may be obtained from an operating system the memory system e.g. virtual memory system or the like.

The SML may be configured to arm the auto commit buffers with a logical identifier e.g. automatically by callback and or via the SML API . Each auto commit buffer may be armed to commit data to a different logical identifier different LBA persistent identifier or the like which may allow the ACM to provide memory semantic access to a number of different concurrent ACM users . In some embodiments arming an auto commit buffer comprises setting the ACM metadata with a logical identifier. In the example the ACM address range is associated with the logical identifier and the ACM metadata of the associated auto commit buffer is armed with the corresponding logical identifier .

The SML may arm an auto commit buffer using an I O control IOCTL command comprising the ACM address range the logical identifier and or an indicator of which auto commit buffer is to be armed. The SML through the SML API may provide an interface to disarm or detach the auto commit buffer . The disarm command may cause the contents of the auto commit buffer to be committed as described above e.g. committed to the solid state storage device . The detach may further comprise disarming the auto commit buffer e.g. clearing the ACM metadata . The SML may be configured to track mappings between address ranges in the memory system and auto commit buffers so that a detach command is performed automatically.

Alternatively or in addition the SML may be integrated into the operating system or virtual operating system e.g. hypervisor of the host . This may allow the auto commit buffers to be used by a virtual memory demand paging system. The operating system may through the SML API or other integration technique map arm auto commit buffers for use by ACM users . The operating system may issue commit commands when requested by an ACM user and or its internal demand paging system. Accordingly the operating system may use the ACM as another generally available virtual memory resource.

Once an ACM user has mapped the ACM address range to an auto commit buffer and has armed the buffer the ACM user may access the resource using memory access semantics and may consider the memory accesses to be logically committed as soon as the memory access has completed. The ACM user may view the memory semantic accesses to the ACM address range to be instantly committed because the ACM is configured to commit the contents of the auto commit buffer to the logical identifier regardless of experiencing restart conditions. Accordingly the ACM user may not be required to perform separate write and commit commands e.g. a single memory semantic write is sufficient to implement a write commit . Moreover the mapping between the auto commit buffer and the ACM disclosed herein removes overhead due to function calls system calls and even a hypervisor if the ACM user is running in a virtual machine that typically introduce latency into the write commit path. The write commit latency time of the ACM user may therefore be reduced to the time required to access the ACM itself.

As described above in certain embodiments the host may map one or more ACM buffers into an address range of a physical memory address space addressable by a CPU a kernel or the like of the host device such as the memory system as directly attached physical memory as MMIO addressable physical memory over a PCI e bus or otherwise mapped as one or more pages of physical memory. The host may further map at least a portion of the physically mapped ACM buffers into a virtual memory address space accessible to user space processes or the like as virtual memory. The host may map the entire capacity of the physically mapped ACM buffers into a virtual memory address space a portion of the physically mapped ACM buffers into a virtual memory address space or the like.

In a similar manner the host may include a virtual machine hypervisor host operating system or the like that maps the physically mapped ACM buffers into an address space for a virtual machine or guest operating system. The physically mapped ACM buffers may appear to the virtual machine or guest operating system as physically mapped memory pages with the virtual machine hypervisor or host operating system spoofing physical memory using the ACM buffers . A resource management agent as described above may allocate arbitrate storage capacity of the ACM buffers among multiple virtual machines guest operating systems or the like.

Because in certain embodiments virtual machines guest operating systems or the like detect the physically mapped ACM buffers as if they were simply physically mapped memory the virtual machines can sub allocate arbitrate the ACM buffers into one or more virtual address spaces for guest processes or the like. This allows processes within guest operating systems in one embodiment to change ACM data and or ACM metadata directly without making guest operating system calls without making requests to the hypervisor or host operating system or the like.

In another embodiment instead of spoofing physical memory for a virtual machine and or guest operating system a virtual machine hypervisor a host operating system or the like of the host device may use para virtualization techniques. For example a virtual machine and or guest operating system may be aware of the virtual machine hypervisor or host operating system and may work directly with it to allocate arbitrate the ACM buffers or the like. When the ACM is used in a virtual machine environment in which one or more ACM users operate within a virtual machine maintained by a hypervisor the hypervisor may be configured to provide ACM users operating within the virtual machine with access to the SML API and or SML .

The hypervisor may access the SML API to associate logical identifiers with auto commit buffers of the ACM as described above. The hypervisor may then provide one or more armed auto commit buffers to the ACM users e.g. by mapping an ACM address range within the virtual machine memory system to the one or more auto commit buffers . The ACM user may then access the ACM using memory access semantics e.g. efficient write commit operations without incurring overheads due to inter alia hypervisor and other system calls. The hypervisor may be further configured to maintain the ACM address range in association with the auto commit buffers until explicitly released by the ACM user e.g. the keep the mapping from changing during use . Para virtualization and cooperation in certain embodiments may increase the efficiency of the ACM in a virtual machine environment.

In some embodiments the ACM user may be adapted to operate with the instant commit memory access semantics provided by the ACM . For example since the armed auto commit buffers are triggered to commit in the event of a restart without an explicit commit command the order in which the ACM user performs memory access to the ACM may become a consideration. The ACM user may employ memory barriers complier flags and the like to ensure the proper ordering of memory access operations.

For example read before write hazards may occur where an ACM user attempts to read data through the block device interface that is stored on the ACM via the memory semantic interface . In some embodiments the SML may maintain metadata tracking the associations between logical identifiers and or address ranges in the memory system and auto commit buffers . When an ACM user or other entity attempts to access a logical identifier that is mapped to an auto commit buffer e.g. through the block device interface the SML directs the request to the ACM via the memory semantic interface preventing a read before write hazard.

The SML may be configured to provide a consistency mechanism for obtaining a consistent state of the ACM e.g. a barrier snapshot or logical copy . The consistency mechanism may be implemented using metadata maintained by the SML which as described above may track the triggered auto commit buffers in the ACM . A consistency mechanism may comprise the SML committing the contents of all triggered auto commit buffers such that the state of the persistent storage is maintained e.g. store the contents of the auto commit buffers on the solid state storage or other persistent storage .

As described above ACM users may access the ACM using memory access semantics at RAM granularity with the assurance that the operations will be committed if necessary in the event of restart failure power loss or the like . This is enabled by inter alia a mapping between the memory system of the host and corresponding auto commit buffers memory semantic operations implemented within an ACM memory range mapped to an auto commit buffer are implemented directly on the buffer . As discussed above data transfer between the host and the ACM may be implemented using any suitable data transfer mechanism including but not limited to the host performing processor IO operations PIO with the ACM via the bus e.g. MMIO PMIO and the like the ACM or other device providing one or more DMA engines or agents data movers to transfer data between the host and the ACM the host performing processor cache write flush operations or the like. Transferring data on the bus may comprise issuing a bus write operation followed by a read. The subsequent read may be required where the bus e.g. PCI bus does not provide an explicit write acknowledgement.

In some embodiments an ACM user may wish to transfer data to the ACM in bulk as opposed to a plurality of small transactions. Bulk transfers may be implemented using any suitable bulk transfer mechanism. The bulk transfer mechanism may be predicated on the features of the bus . For example in embodiments comprising a PCI e bus bulk transfer operations may be implemented using bulk register store CPU instructions.

Similarly certain data intended for the ACM may be cached in processor cache of the processor complex . Data that is cached in a processor cache may be explicitly flushed to the ACM to particular auto commit buffers using a CPU cache flush instruction or the like such as the serializing instruction described below.

The DMA engines described above may also be used to perform bulk data transfers between an ACM user and the ACM . In some embodiments the ACM may implement one or more of the DMA engines which may be allocated and or accessed by ACM users using the SML through the SML API . The DMA engines may comprise local DMA transfer engines for transferring data on a local system bus as well as RDMA transfer engines for transferring data using a network bus network interface or the like.

In some embodiments the ACM may be used in caching applications. For example the solid state storage device may be used as cache for other backing store such as a hard disk network attached storage or the like not shown . One or more of the ACM auto commit buffers may be used as a front end to the solid state storage cache a write back cache by configuring one or more of the auto commit buffers of the ACM to commit data to the appropriate logical identifiers in the solid state storage . The triggered buffers are accessible to ACM users as described above e.g. by mapping the buffers into the memory system of the host . A restart condition causes the contents of the buffers to be committed to the solid state storage cache. When the restart condition is cleared the cached data in the solid state storage committed by the auto commit buffers on the restart condition will be viewed as dirty in the write cache and available for use and or migration to the backing store. The use of the ACM as a cache front end may increase performance and or reduce wear on the cache device.

In some embodiments auto commit buffers of the ACM may be leveraged as a memory write back cache by an operating system virtual memory system and or one or more CPUs of the host . Data cached in the auto commit buffers as part of a CPU write back cache may be armed to commit as a group. When committed the auto commit buffers may commit both data and the associated cache tags. In some embodiments the write back cache auto commit buffers may be armed with an ACM address or armed with a predetermined write back cache address . When the data is restored logical identifier information such as LBA and the like may be determined from a log or other data.

In some embodiments the SML may comprise libraries and or publish APIs adapted to a particular set of ACM users . For example the SML may provide an Instant Committed Log Library ICL adapted for applications whose performance is tied to write commit latency such as transaction logs database file system and other transaction logs store and forward messaging systems persistent object caching storage device metadata and the like.

The ICL provides mechanisms for mapping auto commit buffers of the ACM into the memory system of an ACM user as described above. ACM users or the ICL itself may implement an efficient supplier consumer paradigm for auto commit buffer allocation arming and access. For example a supplier thread or process in the application space of the ACM users may be used to allocate and or arm auto commit buffers for the ACM user e.g. map auto commit buffers to address ranges within the memory system of the host arm the auto commit buffers with a logical identifier and so on . A consumer thread or process of the ACM user may then accesses the pre allocated auto commit buffers . In this approach allocation and or arming steps are taken out of the write commit latency path of the consumer thread. The consumer thread of the ACM user may consider memory semantic accesses to the memory range mapped to the triggered auto commit buffers the ACM memory range as being instantly committed as described above.

Performance of the consumer thread s of the ACM user may be enhanced by configuring the supplier threads of an Instant Committed Log Library ICL or ACM user to allocate and or arm auto commit buffers in advance. When a next auto commit buffer is needed the ACM user have access a pre allocated armed buffer from a pool maintained by the supplier. The supplier may also perform cleanup and or commit operations when needed. For example if data written to an auto commit buffer is to be committed to persistent storage a supplier thread or another thread outside of the write commit path may cause the data to be committed using the SML API . Committing the data may comprise re allocating and or re arming the auto commit buffer for a consumer thread of the ACM user as described above.

The supplier consumer approach described above may be used to implement a rolling buffer. An ACM user may implement an application that uses a pre determined amount of rolling data. For example an ACM user may implement a message queue that stores the last 20 inbound messages and or the ACM user may manage directives for a solid state storage device e.g. persistent trim directives or the like . A supplier thread may allocate auto commit buffers having at least enough capacity to hold the rolling data needed by the ACM user e.g. enough capacity to hold the last 20 inbound messages . A consumer thread may access the buffers using memory access semantics load and store calls as described above. The SML API or supplier thread of the ACM user may monitor the use of the auto commit buffers . When the consumer thread nears the end of its auto commit buffers the supplier thread may re initialize the head of the buffers by causing the data to be committed if necessary mapping the data to another range within the memory system and arming the auto commit buffer with a corresponding logical identifier. As the consumer continues to access the buffers the consumer stores new data at a new location that rolls over to the auto commit buffer that was re initialized by the supplier thread and continues to operate. In some cases data written to the rolling buffers described above may never be committed to persistent storage unless a restart condition or other triggering condition occurs . Moreover if the capacity of the auto commit buffers is sufficient to hold the rolling data of the ACM user the supplier threads may not have to perform re initialize re arming described above. Instead the supplier threads may simply re map auto commit buffers that comprise data that has rolled over and or discard the rolled over data therein .

In its simplest form a rolling buffer may comprise two ACM buffers and the SML may write to one ACM buffer for an ACM user while destaging previously written data from the other ACM buffer to a storage location such as the non volatile memory or the like. In response to filling one ACM buffer and completing a destaging process of the other ACM buffer the SML may transparently switch the two ACM buffers such that the ACM user writes to the other ACM buffer during destaging of the one ACM buffer in a ping pong fashion. The SML may implement a similar rolling process with more than two ACM buffers . The ICL in certain embodiments includes and or supports one or more transactional log API functions. An ACM user may use the ICL in these embodiments to declare or initialize a transactional log data structure.

As a parameter to a transactional log API command to create a transactional log data structure in one embodiment the ICL receives a storage location such as a location in a namespace and or address space of the solid state storage or the like to which the SML may commit empty and or destage data of the transactional log from two or more ACM buffers in a rolling or circular manner as described above. Once an ACM user has initialized or declared a transactional log data structure in one embodiment the use of two or more ACM buffers to implement the transactional log data structure is substantially transparent to the ACM user with the performance and benefits of the ACM . The use of two or more ACM buffers in certain embodiments is transparent when the destage rate for the two or more ACM buffers is greater than or equal to the rate at which the ACM user writes to the two or more ACM buffers . The ICL in one embodiment provides byte level writes to a transactional log data structure using two or more ACM buffers .

In another example a supplier thread may maintain four 4 or more ACM buffers . A first ACM buffer may be armed and ready to accept data from the consumer as described above. A second ACM buffer may be actively accessed e.g. filled by a consumer thread as described above. A third ACM buffer may be in a pre arming process e.g. re initializing as described above and a fourth ACM buffer may be emptying or destaging e.g. committing to persistent storage as described above .

In some embodiments the ICL and or rolling log mechanisms described above may be used to implement an Intent Log for Synchronous Writes for a filesystem e.g. the ZFS file system . The log data ZIL may be fairly small 1 to 4 gigabytes and is typically write only. Reads may only be performed for file system recovery. One or more auto commit buffers may be used to store filesystem data using a rolling log and or demand paging mechanism as described above.

The ICL library may be configured to operate in a high availability mode as described above in conjunction with . In a high availability mode the SML and or bus sends commands pertaining to memory semantic accesses to two or more ACM each of which may implement the requested operations and or be triggered to commit data in the event of a restart condition.

The ACM disclosed herein may be used to enable other types of applications such as durable synchronization primitives. A synchronization primitive may include but is not limited to a semaphore mutex atomic counter test and set or the like.

A synchronization primitive may be implemented on an auto commit buffer . ACM users or other entities that wish to access the synchronization primitive may map the auto commit buffer into the memory system . In some embodiments each ACM user may map the synchronization primitive auto commit buffer into its own respective address range in the memory system . Since the different address ranges are all mapped to the same auto commit buffer all will show the same state of the synchronization primitive. ACM users on remote computing devices may map the synchronization primitive auto commit buffer into their memory system using an RDMA network or other remote access mechanism e.g. Infiniband remote PCI etc. .

In some embodiments the SML may comprise a Durable Synchronization Primitive Library DSL to facilitate the creation of and or access to synchronization primitives on the ACM . The DSL may be configured to facilitate one to many mappings as described above one auto commit buffer many address ranges in the memory system .

The ACM users accessing the semaphore primitive may consider their accesses to be durable since if a restart condition occurs while the synchronization primitive is in use the state of the synchronization primitive will be persisted as described above the auto commit buffer of the synchronization primitive will be committed to the solid state storage or other persistent storage .

As described above the SML may be used to map a file into the memory system virtual address space of the host . The file may be mapped in an Instant Committed Memory ICM mode. In this mode all changes made to the memory mapped file are guaranteed to be reflected in the file even if a restart condition occurs. This guarantee may be made by configuring the demand paging system to use an auto commit buffer of the ACM for all dirty pages of the ICM file. Accordingly when a restart condition occurs the dirty page will be committed to the file and no data will be lost.

In some embodiments the SML may comprise an ICM Library ICML to implement these features. The ICML may be integrated with an operating system and or virtual memory system of the host . When a page of an ICM memory mapped file is to become dirty the ICML prepares an auto commit buffer to hold the dirty page. The auto commit buffer is mapped into the memory system of the host and is triggered to commit to a logical identifier associated with the memory mapped file. As described above changes to the pages in the memory system are implemented on the auto commit buffer via the memory semantic access module .

The ICML may be configured to commit the auto commit buffers of the memory mapped file when restart conditions occur and or when the demand paging system of the host needs to use the auto commit buffer for another purpose. The determination of whether to detach the auto commit buffer from a dirty page may be made by the demand paging system by the SML e.g. using a least recently used LRU metric or the like or by some other entity e.g. an ACM user . When the auto commit buffer is detached the SML may cause its contents to be committed. Alternatively the contents of the auto commit buffer may be transferred to system RAM at which point the virtual memory mapping of the file may transition to use a RAM mapping mechanisms.

In some embodiments the SML or ICML may be configured to provide a mechanism to notify the operating system virtual memory system or the like that a page of a memory mapped file is about to become dirty in advance of an ACM user writing the data. This notification may allow the operating system to prepare an auto commit buffer for the dirty page in advance and prevent stalling when the write actually occurs while the auto commit buffer is mapped and armed . The notification and preparation of the auto commit buffer may implemented in a separate thread e.g. a supplier thread as described above .

The SML and or ICML may provide an API to notify the operating system that a particular page that is about to be written has no useful contents and should be zero filled. This notification may help the operating system to avoid unnecessary read operations.

The mechanisms for memory mapping a file to the ACM may be used in log type applications. For example the ICL library may be implemented to memory map a log file to one or more auto commit buffers as described above. A supplier thread may provide notifications to the operating system regarding which pages are about to become dirty and or to identify pages that do not comprise valid data.

Alternatively or in addition the ICML may be implemented without integration into an operating system of the host . In these embodiments the ICML may be configured to monitor and or trap system signals such as mprotect mmap and manual segment fault signals to emulate the demand paging operations typically performed by an operating system.

At step an auto commit buffer of the ACM may be mapped into the memory system of a computing device e.g. the host . The mapping may comprise associating a BAR address of the auto commit buffer with an address range in the memory system.

At step the auto commit buffer may be armed with ACM metadata configured to cause the auto commit buffer to be committed to a particular persistent storage and or at a particular location in the persistent storage in the event of a restart condition. In some embodiments the ACM metadata may comprise a logical identifier such as a LBA object identifier or the like. Step may comprise verifying that the ACM metadata is valid and or can be used to commit the contents of the auto commit buffer.

At step an ACM user such as an operating system application or the like may access the armed auto commit buffer using memory access semantics. The ACM user may consider the accesses to be instantly committed due to the arming of step . Accordingly the ACM user may implement instant committed writes that omit a separate and or explicit commit command. Moreover since the memory semantic accesses are directly mapped to the auto commit buffer via the mapping of step the memory semantic accesses may bypass systems calls typically required in virtual memory systems.

At step an auto commit buffer of an ACM is mapped into the memory system of a computing device e.g. the host and is armed as described above.

At step an ACM user accesses the auto commit buffer using memory access semantics e.g. by implementing memory semantic operations within the memory range mapped to the auto commit buffer at step .

At step a restart condition is detected. As described above the restart condition may be a system shutdown a system restart a loss of power a loss of communication between the ACM and the host computing device a software fault or any other restart condition that precludes continued operation of the ACM and or the host computing device.

At step the ACM implements the armed triggered commit actions on the auto commit buffer. The triggered commit action may comprise committing the contents of the auto commit buffer to persistent storage such as a solid state storage or the like.

At step the method ends until a next auto commit buffer is mapped and or armed or a restart condition is detected.

At step the method accesses armed auto commit buffers on the ACM if any . Accessing the armed auto commit buffer may comprise the method determining whether an auto commit buffer has been armed by inspecting the triggered ACM metadata thereof. If no triggered ACM metadata exists or the ACM metadata is invalid the method may determine that the auto commit buffer is not armed. If valid triggered ACM metadata does exist for a particular auto commit buffer the method identifies the auto commit buffer as an armed buffer and continues to step .

At step the triggered commit action for the armed auto commit buffers is performed. Performing the triggered commit action may comprise persisting the contents of the auto commit buffer to a sequential and or log based storage media such as a solid state storage media. Accordingly the triggered commit action may comprise accessing a logical identifier of the auto commit buffer labeling the data with the logical identifier and injecting the labeled data into a write data pipeline. Alternatively the triggered commit action may comprise storing the data on a persistent storage having a one to one mapping between logical identifier and physical storage address e.g. a hard disk . The triggered commit action may comprise storing the contents of the armed auto commit buffer to the specified physical address.

Performing the triggered commit action at step may comprise using a secondary power supply to power the ACM solid state storage medium and or other persistent storage medium until the triggered commit actions are completed.

In certain embodiments instead of or in addition to using a volatile memory namespace such as a physical memory namespace a virtual memory namespace or the like and or instead of or in addition to using a storage namespace such as a file system namespace a logical unit number LUN namespace or the like one or more commit agents as described above may implement an independent persistent memory namespace for the ACM . For example a volatile memory namespace which is typically accessed using an offset in physical and or virtual memory is not persistent or available after a restart event such as a reboot failure event or the like and a process that owned the data in physical and or virtual memory prior to the restart event typically no longer exists after the restart event. Alternatively a storage namespace is typically accessed using a file name and an offset a LUN ID and an offset or the like. While a storage namespace may be available after a restart event a storage namespace may have too much overhead for use with the ACM . For example saving a process for each executing process using a file system storage namespace may result in a separate file for each executing process which may not be an efficient use of the ACM .

The one or more commit agents and or the controller in certain embodiments provide ACM users with a new type of persistent memory namespace for the ACM that is persistent through restart events without the overhead of a storage namespace. One or more processes such as the ACM user in one embodiment may access the persistent memory namespace using a unique identifier such as a globally unique identifier GUID universal unique identifier UUID or the like so that data stored by a first process for an ACM user prior to a restart event is accessible to a second process for the ACM user after the restart event using a unique identifier without the overhead of a storage namespace a file system or the like.

The unique identifier in one embodiment may be assigned to an ACM user by a commit agent the controller or the like. In another embodiment an ACM user may determine its own unique identifier. In certain embodiments the persistent memory namespace is sufficiently large and or ACM users determine a unique identifier in a predefined known manner e.g. based on a sufficiently unique seed value nonce or the like to reduce limit and or eliminate collisions between unique identifiers. In one embodiment the ACM metadata includes a persistent memory namespace unique identifier associated with an owner of an ACM buffer an owner of one or more pages of an ACM buffer or the like.

In one embodiment the one or more commit agents and or the controller provide a persistent memory namespace API to ACM users over which the ACM users may access the ACM using the persistent memory namespace. In various embodiments the one or more commit agents and or the controller may provide a persistent memory namespace API function to transition convert map and or copy data from an existing namespace such as a volatile memory namespace or a storage namespace to a persistent memory namespace a persistent memory namespace API function to transition convert map and or copy data from a persistent memory namespace to an existing namespace such as a volatile memory namespace or a storage namespace a persistent memory namespace API function to assign a unique identifier such as a GUID a UUID or the like a persistent memory namespace API function to list or enumerate ACM buffers associated with a unique identifier a persistent memory namespace API function to export or migrate data associated with a unique identifier so that an ACM user such as an application and or process may take its ACM data to a different host to a different ACM or the like and or other persistent memory namespace API functions for the ACM . For example an ACM user in one embodiment may use a persistent memory namespace API function to map one or more ACM buffers of a persistent memory namespace into virtual memory of an operating system of the host or the like and the mapping into the virtual memory may end in response to a restart event while the ACM user may continue to access the one or more ACM buffers after the restart event using the persistent memory namespace. In certain embodiments the SML may provide the persistent memory namespace API in cooperation with the one or more commit agents and or the controller .

The persistent memory namespace in certain embodiments is a flat non hierarchical namespace of ACM buffers and or associated ACM pages indexed by the ACM metadata . The one or more commit agents and or the controller in one embodiment allow the ACM buffers to be queried by ACM metadata . In embodiments where the ACM metadata includes a unique identifier in certain embodiments an ACM user may query or search the ACM buffers by unique identifier to locate ACM buffers and or stored data associated with a unique identifier. In a further embodiment the one or more commit agents and or the controller may provide one or more generic metadata fields in the ACM metadata such that an ACM user may define its own ACM metadata in the generic metadata field or the like. The one or more commit agents and or the controller in one embodiment may provide access control for the ACM based on unique identifier or the like.

In one embodiment an ACM buffer may be a member of a persistent memory namespace and one or more additional namespaces such as a volatile namespace a storage namespace or the like. In a further embodiment the one or more commit agents and or the controller may provide multiple ACM users with simultaneous access to the same ACM buffers . For example multiple ACM users of the same type and or with the same unique identifier multiple instances of a single type of ACM user multiple processes of a single ACM user or the like may share one or more ACM buffers . Multiple ACM users accessing the same ACM buffers in one embodiment may provide their own access control for the shared ACM buffers such as a locking control turn based control moderator based control or the like. In a further embodiment using a unique identifier a new ACM user an updated ACM user or the like on the host may access

In certain embodiments the ACM may comprise a plurality of independent access channels buses and or ports being at least dual ported e.g. dual ported triple ported quadruple ported . In embodiments where the ACM is at least dual ported the ACM is accessible over a plurality of independent buses . For example the ACM may be accessible over redundant bus connections with a single host may be accessible to a plurality of hosts over separate buses with the different hosts or the like. In embodiments where the ACM is at least dual ported if one node and or access channel fails e.g. a host a bus one or more additional nodes and or access channels to the ACM remain functional obviating the need for redundancy replication or the like between multiple hosts .

In one embodiment the ACM comprises a PCI e attached dual port device and the ACM may be connected to and in communication with two hosts over independent PCI e buses . For example the ACM may comprise a plurality of PCI e edge connectors for connecting to a plurality of PCI e slot connectors or the like. In a further embodiment the power connection may also be redundant with one power connection per bus or the like. At least one of the plurality of connections in certain embodiments may comprise a data network connection such as a NIC or the like. For example the ACM may comprise one or more PCI e connections and one or more data network connections.

In one embodiment the controller may arbitrate between a plurality of hosts to which the ACM is coupled such that one host may access the ACM buffers at a time. The controller in another embodiment may accept a reservation request from a host and may provide the requesting host with access to the ACM buffers in response to receiving the reservation request. The ACM may natively support a reservation request as an atomic operation of the ACM . In other embodiments the ACM may divide ACM buffers between hosts may divide ACM buffers between hosts but share backing non volatile memory between hosts or may otherwise divide the ACM buffers the non volatile memory and or associated address spaces between hosts .

In one embodiment the controller the one or more commit agents and or other elements of the ACM may be dual headed split brained or the like each head or brain being configured to communicate with a host and with each other to provide redundant functions for the ACM . By being at least dual ported in certain embodiments the ACM may be redundantly accessible without the overhead of replication duplication or the like which would otherwise reduce I O speeds of the ACM especially if such replication duplication were performed over a data network or the like.

In general the ACM module services auto commit requests from an ACM user or other client for the ACM . As described above with regard to the ACM users as used herein a client may comprise one or more of an operating system OS virtual operating platform e.g. an OS with a hypervisor guest OS application process thread entity utility or the like that is configured to access or use the ACM . In certain embodiments the ACM module supports one or more auto commit memory synchronization operations such as a barrier operation a checkpoint operation or the like. In the depicted embodiment the ACM module includes an auto commit request module a flush module and a barrier completion module . The ACM module in various embodiments may support a barrier operation a checkpoint operation both a barrier operation and a checkpoint operation and or other auto commit memory synchronization operations. By supporting a barrier and or checkpoint operation in certain embodiments the ACM module provides an interface whereby an ACM user or other client may manage or ensure persistence and consistency for the byte addressable ACM buffers whether the ACM buffers are natively volatile or non volatile regardless of the type of media used for the ACM buffers .

As described above in certain embodiments the ACM module and or the ACM enable clients such as the ACM users to access fast byte addres sable persistent memory combining benefits of volatile memory and non volatile storage. Auto commit logic inside the hardware of the storage device such as the power management apparatus described above in certain embodiments provides power cut protection for data written to the auto commit buffers of the ACM .

The ACM may be accessible to applications operating system components and or other ACM users as byte addressable memory mapped to a virtual address space of a memory system of a processor complex . Updates to data of the ACM by ACM users may be stored in one or more processor caches of the processor complex and or the memory system from which the data may be written back lazily to the underlying ACM buffers . A processor cache may include a write combine buffer an L1 processor cache an L2 processor cache an L3 processor cache a processor cache hierarchy and or another type of processor cache.

Caching data of the ACM buffers in a processor cache may improve performance e.g. decrease an amount of time it takes for the processor complex to access data of the ACM buffers . However in certain embodiments caching data of the ACM buffers may increase a risk of losing updates to the data in response to a restart event such as a power failure of the host . For example a processor cache may be weakly ordered not guaranteeing or ensuring that the processor cache will maintain an order of operations for cached data but instead trickling data down to the auto commit buffers arbitrarily without a guaranteed order or timeframe.

In certain embodiments the ACM module in cooperation with the SML or the like makes the ACM available to one or more ACM users using an API such as the SML API described above. The ACM module and or the SML may provide the SML API and or another ACM API to ACM users at a user level and or a kernel level.

To make the ACM usable for ACM users even across restart events the ACM module may provide persistence and or consistency for data of the auto commit buffers despite the weak ordering of a processor cache of the processor complex . The ACM module in certain embodiments may guarantee or ensure that application data residing in processor caches of the processor complex has been flushed or destaged to the ACM and will be persisted across restart events as described above.

The ACM module may provide consistency of data of the auto commit buffers to ensure that the data is meaningful to the ACM user after a restart event e.g. the data may be accessed recovered interpreted by an ACM user to recover application data and or state . As described above the memory system may flush destage or otherwise move data from the one or more processor caches of the processor complex to the auto commit buffers at arbitrary times without strict ordering.

Further ACM users may perceive consistency across multiple updates to data of the auto commit buffers . For example a transaction of an ACM user may change multiple attributes within a data structure and the ACM module may atomically update each change to preserve application consistency for the ACM user . By managing consistency for data of the auto commit buffers for the ACM users in certain embodiments the ACM module may obviate the need for ACM users to manage consistency themselves thereby simplifying application development use of the ACM and the like.

The ACM module may provide guarantee or otherwise manage persistence and or consistency using one or more synchronization operations such as a barrier operation a checkpoint operation or the like. As used herein a barrier operation comprises an ACM operation that synchronizes flushes and or destages dirty data from one or more processor caches to one or more auto commit buffers of the ACM . A checkpoint operation as used herein comprises an ACM operation that creates a copy or snapshot of the data contents of one or more pages of the auto commit buffers . In one embodiment a checkpoint operation synchronizes flushes and or destages dirty data from one or more processor caches to the auto commit buffers e.g. performs a barrier operation prior to copying the data contents of the pages of the auto commit buffers to create a snapshot. The ACM module in various embodiments may use the auto commit request module the flush module and or the barrier completion module to execute barrier operations checkpoint operations and or other synchronization operations for the ACM .

In one embodiment the auto commit request module is configured to monitor detect or otherwise receive auto commit requests from clients such as the ACM users described above another module a host computing device or the like. The auto commit request module in one embodiment receives auto commit requests from clients over an ACM API such as the SML API described above.

An auto commit request in certain embodiments may comprise a barrier request a checkpoint request or another synchronization request. The auto commit request module in certain embodiments determines a request type for a received auto commit request for example determining whether a received auto commit request is a barrier request or a checkpoint request so that the ACM module can service the received auto commit request.

In one embodiment the flush module to service a barrier request a checkpoint request or the like is configured to issue perform or otherwise execute a serializing instruction for a processor cache in response to the auto commit request module receiving an auto commit request. A serializing instruction flushes destages or otherwise synchronizes data from a processor cache of the processor complex and or the memory system to underlying memory such as an auto commit buffer . One or more auto commit buffers receive data that the flush module flushes or destages from the processor complex . An auto commit buffer or other underlying memory device to which data is flushed or destaged from the processor complex in response to a serializing instruction in one embodiment is selected by a memory manager for the processor complex or the like based on which underlying memory device is mapped into a logical address range for the data in virtual memory of the processor complex .

Examples of serializing instructions include an MFENCE instruction an SFENCE instruction an xchg instruction e.g. compare and swap compare and swap double CMPXCHG CMPXCHNG8B CMPXCHNG16B and or CMP8XCHG16 or the like. In certain embodiments a serializing instruction ensures and or guarantees that operations such as memory semantic load and store operations that precede the serializing instruction are flushed destaged or otherwise synchronized prior to operations that follow the serializing instruction.

In one embodiment the flush module issues performs or otherwise executes a serializing instruction for an entire processor cache or set of processor caches in response to an auto commit request. In another embodiment the flush module may issue perform or otherwise execute a serializing instruction just for data of one or more auto commit buffers stored in a processor cache or set of processor caches so that data associated with other memory devices is not necessarily flushed destaged and or synchronized. For example the flush module may include a memory address range for pages of one or more auto commit buffers in the serializing instruction so that the processor cache or set of caches flushes destages or otherwise synchronizes just the indicated memory address range .

In one embodiment the barrier completion module is configured to determine completion of the serializing instruction flushing destaging or otherwise synchronizing data to one or more auto commit buffers . The barrier completion module in certain embodiments determines that a serializing instruction has completed by placing a predefined completion identifier in the processor cache after the flush module issues the serializing instruction issuing a second serializing instruction and determining that the serializing instruction is complete once the completion identifier is received at the non volatile storage device of the ACM . Because a serializing instruction ensures or guarantees that operations performed prior to the serializing instruction are synchronized prior to operations performed after the serializing instruction the synchronization of the completion identifier to the ACM in response to the second serializing instruction indicates that the first serializing instruction has completed. In other embodiments the barrier completion module may determine completion of the serializing instruction without issuing a second serializing instruction. For example the processor complex may notify the barrier completion module of completion of the serializing instruction by sending an interrupt writing a completion identifier to a control status register which the barrier completion module may poll or the like.

The barrier completion module in certain embodiments may place a completion identifier in the processor cache by writing or storing the completion identifier to a virtual memory address of the host device mapped to a control status register of the ACM . In another embodiment the barrier completion module may place a completion identifier in the processor cache by writing or storing the completion identifier to a virtual memory address mapped to a page of an auto commit buffer or the like. The barrier completion module in various embodiments may detect arrival of a completion identifier in the ACM by polling a control status register of the ACM by polling a predefined location in the auto commit buffers by receiving an interrupt from the ACM or the like.

The barrier completion module in certain embodiments may use different completion identifiers depending on the type of the auto commit request. For example the barrier completion module may write a BARRIER COMPLETE completion identifier for a barrier request may write a CHECKPOINT BEGIN completion identifier for a checkpoint request or the like. A completion identifier in one embodiment comprises a predefined sequence string pattern flag or other indicator that the barrier completion module may write or store to a processor cache to the ACM or the like.

For checkpoint requests as described below with regard to the checkpoint module in response to a CHECKPOINT BEGIN completion identifier or another checkpoint indicator the checkpoint module may create a snapshot copy of one or more pages of the auto commit buffers . The barrier completion module and or the checkpoint module in certain embodiments may write a CHECKPOINT COMPLETE completion identifier to a control status register of the ACM in response to the checkpoint module completing the snapshot copy.

In one embodiment the barrier completion module indicates to a requesting client completion of an auto commit request. The barrier completion module in various embodiments may indicate or notify a client of completion of an auto commit request by returning execution control to the client by sending a return value to the client by sending an interrupt to the client by writing a completion identifier to a control status register polled or poked by the client or the like.

The ACM module in certain embodiments may guarantee or ensure persistence of data flushed to one or more auto commit buffers in response to the barrier completion module determining that a serializing instruction for the flushed data has completed. In a further embodiment the ACM module may guarantee or ensure persistence of operations received for one or more auto commit buffers prior to a received auto commit request in response to the barrier completion module determining completion of a serializing instruction for the auto commit request. As described above once data has been synchronized or stored in the auto commit buffers the ACM preserves or persists the data in non volatile memory media and provides the data from the non volatile memory media to clients such as ACM users after recovery from the restart event. In this manner in certain embodiments the ACM module can provide persistence and consistency of data for ACM users even if a processor cache does not guarantee an order of data an order of operations or the like.

As described above in certain embodiments the ACM is coupled to the host device using a communications bus such as a PCI e bus or the like. In one embodiment the communications bus supports strong operation ordering at least for transactions within a similar traffic class or the like so that the communications bus maintains an order in which data is flushed from the processor cache to one or more auto commit buffers . For example PCI e 2.0 PCI e 3.0 and the like support strong ordering semantics for transactions within the same traffic class. By flushing or destaging data from a processor cache to an auto commit buffer over a communications bus that supports strong operation ordering in certain embodiments the barrier completion module may ensure that a serializing instruction has actually completed in response to receiving a completion identifier at the ACM because the data of the serializing instruction and the completion identifier are received in operation order. In embodiments where the communications bus does not support operation ordering the flush module may act as an intermediary between the processor complex and the communications bus coordinating with the controller to provide strong operation ordering over the communications bus or the like. For example the flush module may queue commands in a FIFO queue and manage and confirm the exchange of each command with the controller or the like to enforce strong or strict operation ordering. The communications bus may be in communication with the processor complex through a northbridge device a root complex or the like of the processor complex .

In one embodiment the auto commit request module is substantially similar to the auto commit request module described above with regard to receiving detecting or otherwise monitoring auto commit requests and the like. The barrier module in certain embodiments services barrier auto commit requests using the flush module and the barrier completion module substantially as described above with regard to the flush module and the barrier completion module of .

In one embodiment the checkpoint module cooperates with the barrier module to service checkpoint requests for one or more pages of the auto commit buffers . A checkpoint operation in certain embodiments comprises a barrier operation to ensure that pending writes residing in processor caches for the auto commit buffers are written back to the auto commit buffers followed by a snapshot operation to create a copy of one or more pages of data.

In one embodiment the checkpoint module sends to the ACM a checkpoint indicator identifying one or more pages of the auto commit buffers to be checkpointed e.g. the one or more pages that are to comprise the requested snapshot copy . The auto commit request module may receive a checkpoint indicator with a checkpoint request from a client or the like and may provide the checkpoint indicator to the checkpoint module . A checkpoint indicator in various embodiments may comprise a set a list a bitmap an array an address range a page index or another data structure indicating which pages of the one or more auto commit buffers the checkpoint module is to checkpoint.

The checkpoint module in certain embodiments sends a checkpoint indicator to the ACM by writing storing or otherwise placing the checkpoint indicator in a processor cache of the processor complex prior to the flush module issuing a first serializing instruction so that the first serializing instruction flushes or destages the checkpoint indicator to the ACM . For example the checkpoint module may store a checkpoint indicator to a virtual memory address that is mapped to a predefined control status register of the ACM that is mapped to a predefined location within an auto commit buffer or the like. In certain embodiments the checkpoint module comprises a driver checkpoint module disposed in a device driver on the host computing device such as the SML which places a checkpoint indicator in a processor cache of the processor complex and a cooperating controller checkpoint module disposed in a storage controller to receive directly or indirectly the checkpoint indicator at the ACM . By placing a checkpoint indicator in a processor cache prior to a first serializing instruction in certain embodiments the checkpoint module may ensure that the checkpoint indicator reaches the ACM prior to a completion identifier from the barrier completion module .

In response to the barrier completion module determining that the first serializing instruction has completed in one embodiment the checkpoint module copies one or more pages of the auto commit buffers creating a snapshot of the one or more pages or the like. For example the checkpoint module may copy the one or more pages in response to a completion identifier from the barrier completion module comprising a checkpoint indicator such as CHECKPOINT BEGIN or the like. In one embodiment the checkpoint module copies the one or more pages to a second location within the auto commit buffers so that the snapshot copy is separately preserved in response to a restart event. In another embodiment the checkpoint module copies the one or more pages directly to non volatile memory media of the ACM creating the snapshot in the non volatile memory media . In a further embodiment the checkpoint module may copy the one or more pages to a separate non volatile memory device to a separate ACM B or the like that is independent from the ACM copying the one or more pages over a communications bus over a data network or the like.

A commit agent of the ACM in certain embodiments makes the snapshot copy of the one or more pages available to an ACM user after recovery from a restart event. In one embodiment the ACM only makes snapshot copies of checkpointed data available to ACM users after recovery from a restart event to provide the ACM users with a consistent known state or the like. Writes or other changes to ACM data that occur between a checkpoint operation and a restart event in certain embodiments may be lost and unavailable after the restart event. In another embodiment a commit agent of the ACM may make a raw un checkpointed version of data available to ACM users in addition to one or more snapshot copies or checkpointed versions of the data after a restart event.

In one embodiment the lock module is configured to stall operations for one or more auto commit buffers during execution of an auto commit request so that the ACM module can guarantee or ensure persistence and or consistency of data after the auto commit request or the like. For example the lock module may stall operations between the flush module issuing a serializing instruction and the barrier completion module determining completion of the serializing instruction. The lock module in certain embodiments may stall operations by unmapping one or more auto commit buffers from the virtual address space of an ACM user in response to an auto commit request and remapping the one or more auto commit buffers into the virtual address space in response to completion of the auto commit request.

In certain embodiments the checkpoint module may track the progress of a checkpoint operation and the lock module may allow operations on portions of auto commit buffers that have already been copied. The checkpoint module in a further embodiment may cooperate with the lock module to copy or move existing data from an auto commit buffer to a snapshot so that the lock module can allow a pending operation for the auto commit buffer to execute.

The ACM module and its various sub modules as described above may be disposed in a device driver for the ACM executing on a processor of the host device such as the SML may be disposed in a storage controller for the ACM and or may comprise portions in each of a device driver and a storage controller or the like. In one embodiment a device driver for a non volatile storage device comprises the auto commit request module and the flush module and a storage controller of the non volatile storage device comprises at least a portion of the barrier completion module and or the checkpoint module or the like.

For example in one embodiment the barrier completion module comprises two portions a driver barrier completion module disposed in a device driver on the host device and a controller barrier completion module disposed in a storage controller . The driver barrier completion module may store a completion identifier to a processor cache and issue a second serializing instruction from the host while the controller barrier completion module may determine completion of the serializing instruction in response to receiving the completion identifier flushed or destaged from the processor cache to the non volatile storage device or the like in response to a second serializing instruction.

The lock module stalls operations for one or more auto commit buffers during execution of the received auto commit request. The flush module issues a serializing instruction to flush destage and or synchronize data from a processor cache to the one or more auto commit buffers . If the barrier completion module determines that the serializing instruction is not complete the barrier completion module continues to determine whether the serializing instruction is complete. If the barrier completion module determines that the serializing instruction is complete the barrier completion module writes a completion identifier to a control status register on the non volatile storage device of the ACM . The barrier completion module issues a second serializing instruction to flush the completion identifier to the non volatile storage device .

If the auto commit request module determines that the auto commit request is a checkpoint request the checkpoint module copies one or more pages identified by the checkpoint indicator to create a snapshot copy within the non volatile storage device of the ACM . The method continues with step . If the auto commit request module determines that the auto commit request is a barrier request the checkpoint module may not copy pages of the auto commit buffers. The lock module allows operations for the one or more auto commit buffers and the auto commit request module and or the barrier completion module indicates completion of the auto commit request to a client. The method continues and the auto commit request module monitors for additional auto commit requests.

A means for receiving an auto commit request for an auto commit buffer in various embodiments may include a storage management layer a device driver a storage controller an ACM API such as a SML API an auto commit request module a processor complex other logic hardware and or other executable code stored on a computer readable storage medium. Other embodiments may include similar or equivalent means for receiving an auto commit request for an auto commit buffer .

A means for issuing a serializing instruction in response to an auto commit request in various embodiments may include a storage management layer a device driver a storage controller a flush module a processor complex other logic hardware and or other executable code stored on a computer readable storage medium. Other embodiments may include similar or equivalent means for issuing a serializing instruction in response to an auto commit request.

A means for determining completion of a serializing instruction in various embodiments may include a storage management layer a device driver a storage controller a barrier completion module a processor complex other logic hardware and or other executable code stored on a computer readable storage medium. Other embodiments may include similar or equivalent means for determining completion of a serializing instruction.

A means for creating a snapshot copy of one or more pages of an auto commit buffer in various embodiments may include a storage management layer a device driver a storage controller a checkpoint module a processor complex other logic hardware and or other executable code stored on a computer readable storage medium. Other embodiments may include similar or equivalent means for creating a snapshot copy of one or more pages of an auto commit buffer .

A means for preserving data of a memory address range in a non volatile storage device in response to a restart event in various embodiments may include an ACM a storage controller a power management apparatus a non volatile memory a write data pipeline a commit agent a commit management apparatus other logic hardware and or other executable code stored on a computer readable storage medium. Other embodiments may include similar or equivalent means for preserving data of a memory address range in a non volatile storage device .

A means for providing data of the memory address range from a non volatile storage device after a restart event in various embodiments may include an ACM a storage management layer a device driver a storage controller a power management apparatus a non volatile memory a read data pipeline a commit agent a commit management apparatus other logic hardware and or other executable code stored on a computer readable storage medium. Other embodiments may include similar or equivalent means for providing data of the memory address range from a non volatile storage device after a restart event.

The present disclosure may be embodied in other specific forms without departing from its spirit or essential characteristics. The described embodiments are to be considered in all respects only as illustrative and not restrictive. The scope of the disclosure is therefore indicated by the appended claims rather than by the foregoing description. All changes which come within the meaning and range of equivalency of the claims are to be embraced within their scope.

