---

title: System and method for displaying a user interface across multiple electronic devices
abstract: A system and method are provided for displaying a user interface using multiple electronic devices. The method comprises providing a first user interface framework on a first electronic device having a first display screen; the first user interface framework determining that a second display screen on a second electronic device is available via a connection between the first and second electronic devices; the first user interface framework using application logic from an application on the first mobile device to determine a first user interface portion to be displayed on the first display screen and a second user interface portion to be displayed on the second display screen; and the first user interface framework providing data associated with the second user interface portion to the second electronic device.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08786517&OS=08786517&RS=08786517
owner: Blackberry Limited
number: 08786517
owner_city: Waterloo
owner_country: CA
publication_date: 20120221
---
The following relates to systems and methods for displaying user interfaces across multiple electronic devices.

The incorporation of near field or short range communication technologies into portable handheld or otherwise mobile electronic devices has made sharing data between such devices more convenient. Devices may now routinely pair or tether with each other for enabling both interactions between users and to facilitate the use of multiple devices by the same user.

Interactions between paired devices that extend beyond basic capabilities such as file transfer often require custom low level programming by an application developer and thus the quality and extent of functionality made available to the user is dictated by the application developer. When such low level programming is significant application developers may be discouraged with incorporating cross device functionality.

It will be appreciated that for simplicity and clarity of illustration where considered appropriate reference numerals may be repeated among the figures to indicate corresponding or analogous elements. In addition numerous specific details are set forth in order to provide a thorough understanding of the examples described herein. However it will be understood by those of ordinary skill in the art that the examples described herein may be practiced without these specific details. In other instances well known methods procedures and components have not been described in detail so as not to obscure the examples described herein. Also the description is not to be considered as limiting the scope of the examples described herein.

It will be appreciated that the examples and corresponding diagrams used herein are for illustrative purposes only. Different configurations and terminology can be used without departing from the principles expressed herein. For instance components and modules can be added deleted modified or arranged with differing connections without departing from these principles.

It has been found that the low level programming required to distribute a UI over multiple display screens on multiple devices can be minimized by allowing a UI framework to abstract the management and handling of combined virtual and physical screen spaces from code written by an application developer. By abstracting the management and handling of the combined virtual and physical screen spaces application developers can automatically benefit from screen space provided by an interconnected device when available without having to expend a significant amount of development resources in low level programming. In this way an application can be developed without having to account for different screen sizes form factors and device functionality. Instead the UI framework enables application logic as written by the application developer to be rendered according to the total screen space available at that time whether utilizing a single screen or multiple screens.

Moreover the UI framework described herein handles the virtual combined screen space transparently to one or both of the application developer and application programming interface API such that from the application s perspective there appears to be no difference between the distribution of UI elements when utilizing a single mobile device screen or utilizing multiple mobile device screens thus enabling an application to be dynamically adapted to changing environments and corresponding changes in the virtual screen space available to the application.

The application displaying the first and second UI elements may also benefit from an enlarged screen space afforded by the utilization of a second display screen on a paired or otherwise nearby mobile device as shown in . Once the two mobile devices are paired near each other or paired and near each other the first and second UI elements may be rearranged to enable an enlarged first UI element to occupy the first screen and an enlarged second UI element to occupy the second screen . In this way a rearrangement similar to what is shown in can be distributed over multiple screens and the increase in virtual screen space allows for further enhancements namely the enlargement of the first and second UI elements to take advantage of the different form factors. By abstracting the management and handling of the combined virtual screen space using a UI framework as explained below the application developer would not need to account for various different form factors and orientations of the connected devices thus reducing the low level programming required. Instead the UI framework transparently determines the screen space available to the application and can distribute UI elements according to application logic that is device agnostic. For example the application developer may specify that the first and second UI elements are to be rearranged in a side by side configuration when a single device is rotated and distributed in a side by side arrangement across multiple screens when a second screen is available wherein each element is resized to maximize its appearance within the screen on which it is displayed.

It can be appreciated that although the examples shown in and those examples provided below illustrate virtual combined screens distributed over pairs of portable and handheld type mobile devices such as smart phones tablet computers portable gaming systems etc. the principles discussed herein equally apply to configurations wherein at least one of the paired devices is non portable stationary incorporated into a larger system etc. For example one of the paired devices may include an in vehicle infotainment screen a public kiosk or digital signage screen home television etc. As such the principles discussed herein may be applied to any electronic devices having respective display screens and being communicable with each other.

It can also be appreciated that although the examples described herein include a short range communication connection between the devices providing the shared screen space various other pairing methods may be used. For example pairing could also be accomplished with mechanisms that incorporate computer vision acoustics other sensors etc. Moreover any such pairing methods may operate in a standalone configuration or combined with communication channels such as wlan wi fi mobile networks etc.

The mobile devices each include a short range communication subsystem for enabling near field or short range communication channels or connections to be established between the first mobile device and the second mobile device . The short range communication subsystem may be operable to communicate via any suitable short range communication or near field communication protocol such as Bluetooth infrared etc. As noted above pairing between the devices may also be accomplished using other methods and via the short range communication subsystem is only one illustrative example. The first and second mobile devices also include a network interface for enabling the first and second mobile devices to communicate via a network e.g. over Ethernet Wi Fi cellular etc. A display module is also included which may represent any interface programming code module or component that enables UI elements to be rendered on the respective display screens . A UI framework is provided for handling UI operations and decisions on behalf of at least one application . As shown in the UI framework may support multiple applications . The UI framework operates according to application logic to obtain or otherwise handle UI elements for the applications and render those UI elements on the display screen via the display module . Although not shown in the UI frameworks may also communicate via the respective network interfaces e.g. when pairing over a mobile network.

Further detail regarding a configuration for the UI framework will now be described making reference to .

UIs may be generally visualized as a graphical scene comprising elements or objects also referred to as entities . Data structures known as scene graphs may be used to define the logical or spatial representation or both the logical and spatial representation of a graphical scene. A scene graph is a collection of nodes in a graph or tree structure. The elements or objects of a UI may be represented as nodes in the scene graph. A node in a scene graph may have many children. The parent node of a scene graph that does not itself have a parent node corresponds to the overall UI.

Consequently an effect applied to a parent is applied to all its child nodes i.e. an operation performed on the parent of a group related by a common parent automatically propagates to all of its child nodes. For example related objects entities may be grouped into a compound object also known as a layout which may by moved transformed selected etc. as a single group. In general a layout can be any grouping of UI elements or objects. The term container as used herein refers to layouts that group UI elements in a particular ordered manner. A parent node can have one or more child nodes that can be for example any type of layout including a container. Each container can in turn have its own child nodes which may be for example other container nodes basic UI elements or special effect nodes. The basic UI elements correspond to discrete components of the UI such as for example a button or a slider. A leaf node in a scene graph corresponds to a basic UI element. A leaf node does not have any child nodes.

As mentioned above containers are layouts that group interface elements in a particular ordered manner. Containers can be of various types including but not limited to docking containers stacking containers grid based containers and scrolling containers.

The UI framework shown in differs from conventional UIs that are developed for individual applications by the application developers with limited or no consistency between the UIs for different applications. For example in conventional systems an application is responsible for driving its UI. The application creates the UI elements composites them into a complete UI screen and is responsible for displaying them. The actual rendering is often handled by the UI framework e.g. calling the draw function for all widgets on the screen but most of the code related to the UI is within the application. It is the responsibility of the application to collect the requisite data for each UI and to populate the UI. The data flow in the system is therefore driven by the applications leading to a large amount of UI related code in the application that is both difficult to maintain and customize.

The UI framework herein described is independent of device platform e.g. independent of mobile device architecture and operating system as well as application framework e.g. independent of application programming language . The UI framework described herein provides scalability improved graphical capabilities and ease of customization and results in enhanced user experiences. The UI framework is used by applications to render their UIs. The UI framework is itself not an application framework i.e. is not used for developing applications and does not impose any rules on application structuring or application management. The UI framework does not provide application functionality. The applications themselves implement the functionality or business logic behind the UI. However using the UI framework removes all UI call functionalities from the application code and instead lets the UI control data call functions. Thus a the UI can interact with multiple applications for data requests in a seamless manner. The single UI framework described herein enforces a clear separation between UI visualization UI logic and UI data thereby allowing the creation of a seamless and truly rich UI. The applications are reduced to simple services responsible for performing business logic and provide the data that the UI requests. An advantage of the single UI framework is that it allows that UI designer to create any user scenario without having to account for the applications that are currently running on the mobile device or whether or not multiple display screens are available for displaying UI elements . That is the UI is driving the data flow. If there is a list on the screen displaying contacts there will be requests for data to a Contacts List application. The UI designer can readily use any application available on the mobile device for its UI without having to specifically create or implement UI elements and populate the lists. Consequently the architecture of the UI framework described herein enables seamless cross application scenarios such as the example shown in .

The UI framework shown in comprises multiple modules or engines typically a single UI rendering engine for a device or a display and separate UI client engines . . . associated with separate applications and respectively. Each of these modules is described in further detail below with reference to .

Each UI client engine is responsible for providing UI data from its associated application to the UI rendering engine . The UI client engine is responsible for setting up UI component trees and informing the UI rendering engine of the tree structure . In the example shown in the UI component tree includes an item as a parent node with two data items as child nodes. The UI client engine gets this information from the application . For example the application code could specify the creation of elements such as buttons and containers programmatically in a language such as C or the application could describe the tree in a declarative language such as XML and have the UI client engine load it. The UI rendering engine mirrors the tree set up by UI client engine to create a mirrored tree . The UI rendering engine sets up visual node trees and for each UI element of the UI component tree . To set up the visual node trees the UI rendering engine has predefined visual node trees for each UI component that the UI client engine provides. For example if the UI client engine sets up a Button the UI rendering engine will have a predefined visual node tree for Button which it will use. Typically this predefined visual node tree will be described in a mark up language such as XML but it could also be described in programmatic code such as an API. The visual node trees are used for rendering the elements for example the background foreground and highlight images of a button is represented in the visual node tree . The UI client engine is not aware of the visual node trees.

The UI rendering engine handles the logic and event handling associated with the UI elements that composite the UI e.g. lists menus softkeys etc. . The UI rendering engine receives data from the UI client engine in an asynchronous manner and binds the data to its visual nodes in the visual tree . As used herein asynchronous means that the transmission of data from the UI client engine to the UI rendering engine is independent of processing of data or inputs by the application . All data that can be presented in the UI for processing as a single thread is made available to the UI rendering engine as it is available to the UI client engine . The underlying application processing and data sources behind the UI client engine are hidden from the UI rendering engine . The UI client engine and UI rendering engine can execute separate threads without waiting for responses from each other. In this manner the UI rendering engine can render the UI tree using the visual node tree without being blocked or stalled by UI client engine .

Since the UI client engine sends data to the UI rendering engine as it becomes available the UI client engine should also indicate to the UI rendering engine whether the data is complete or to await further data prior to rendering. In an example implementation the data items necessary for rendering the UI form a transaction. Rather than waiting until all required data items are available the UI client engine can send data items relating to a single transaction in several communications or messages as they become available and the messages will be received asynchronously by the UI rendering engine . The UI rendering engine does not start processing the received data items until it has received all messages that at are part of the transaction.

For example the UI client engine can inform the UI rendering engine that one container with two child buttons has been created as one transaction. The UI rendering engine does not process this transaction until it has received all data items related to the particular transaction. In other words the UI rendering engine will not create the container and buttons before it has all the information.

The UI client engine and the UI rendering engine are as decoupled from each other as possible. The UI client engine is not aware of where in the UI its data is used i.e. it does not hold a UI state. The elements are the building blocks of the UI. The elements of the UI component tree represent the basic UI elements lists menus tab lists soft keys etc. Elements are typically specified in a declarative language such as XML or JSON currently QML which is JSON based and given different attributes to make them behave as desired. Examples of attributes include rendered attributes response attributes and decoding attributes. Rendered attributes refer to any attribute that specifies how a UI element is rendered. Examples of rendered attributes can include color opacity transparency the position on the display orientation shape and size. In various embodiments the position on the display can be described with any suitable coordinate system including x y coordinates or x y z coordinates. It can be appreciated however that the position or size of a UI element relative to the virtual screen space may be specified based on a relative dimension such as length etc.

Examples of response attributes can include any attribute that specifies how the user interface element responds to commands or inputs such as for example a single tap double tap or swipe. For example a response attribute can specify a speed of a double tap for the UI element. Decoding attributes can include image decoding priority. A complete UI is a set of elements composited in a visual tree. The elements interpret their associated data for example a menu component will interpret the data differently from a list component. The elements react upon events for example when a key is pressed or other event is posted to the UI the elements in the UI will react e.g. move up and down in a list or opening a sub menu. The elements also bind data to their respective visual tree nodes. The elements have built in UI logic such as highlight when pressed scroll when flicked navigate to tab when tab icon is clicked but the application logic such as start new application find shortest route to bus station etc. is in the application code and typically is triggered by high level events from the elements e.g. a Button Click event detected by the UI rendering engine and passed to the UI client engine may trigger the application to find shortest route .

Visuals define the appearance of elements and are specified in the visual node trees . In an example the visuals may be defined in XML. The XML code could be generated independently or using a suitable visuals generation application. A visual could for example be a generic list that can be used by several different lists or a highly specialized visualization of a media player with a number of graphical effects and animations. Using different visual representations of elements is an effective way to change the look and feel of the UI. For example skin changes can readily be done simply by changing the visuals of components in the UI. If the visuals have a reference to a specific data element the UI client engine retrieves the data from the application and transmits such data to the UI rendering engine . The UI client engine also initiates animations on visuals. For example UI client engine can create and start animations on properties of UI elements position opacity etc. .

The UI client engine is unaware of the actual composition and structure of its visuals. For example when a list item receives focus the list element will assume that there is animation for focusing in the list item visuals. The UI rendering engine executes started animations. Animations run without involvement from the UI client engine . In other words the UI client engine cannot block the rendering of animations. The UI rendering engine is a rendering engine that may be specifically optimized for the electronic device. The rendering engine is capable of rendering a tree of visual elements and effects and performing real time animations. The UI rendering engine renders the pixels that eventually will be copied on to the physical screen of the mobile device for example. All elements active on the display have a graphical representation in the visual tree . The UI rendering engine processes touch key input without UI client engine involvement to ensure responsiveness for example list scrolling changing of slider values component animations etc. run without UI client engine involvement . The UI rendering engine notifies UI client engine that a button has been pressed slider has been dragged etc. The UI client engine can then react on the event for example change the brightness if the slider has been dragged but as already mentioned above the UI client engine does not need to be involved in updating the actual UI only in responding to events from the UI. The advantages of the UI driven architecture described herein is readily apparent during runtime. Runtime behaviour is defined by what is visible on the display screen of the mobile device .

The UI rendering engine may operate in a single client single server configuration similar to the configuration shown in . In such a configuration the UI rendering engine receive a UI component tree for an application from a UI client engine associated with the application . Based on the component tree the UI rendering engine then determines a visual node tree for each element and assembles the visual node trees into an overall visual node tree corresponding to the UI component tree . The UI rendering engine then asynchronously receives from the UI client engine UI data items related to elements of the UI component tree . The UI rendering engine populates the visual node tree with the UI elements and renders them to the UI in accordance with the visual node tree independently of further input from the UI client engine . Since the UI client thread which depends on interaction with the application is separate and independent from the UI rendering thread the rendering thread is not blocked by the application processing.

When the UI rendering engine detects a user input in the UI it communicates the user input to the UI client engine for further processing. In addition if necessary the UI rendering engine re renders the UI in response to the user input independently of further input from the UI client engine . For example if the user input is a button press the UI rendering engine re renders to animate a button associated with the button press. If the UI client engine determines that the user input received from the UI rendering engine requires new data i.e. a modification to the UI the UI client engine sends further data items invoking the modification to the UI rendering engine which then re renders UI in accordance with the further data items and their associated visual node tree independently of further input from the client UI engine . For example as described above the UI client engine could initiate an animation effect.

According to another aspect the UI framework can operate in a configuration wherein a single UI rendering engine can support multiple UI client engines etc e.g. as shown in . Thus multiple applications can coexist on the single UI rendering engine . The UI client engines etc. each associated with an application etc. or an instance of an application while the UI rendering engine is associated with a display . Each UI client engine determines a corresponding UI component tree for its respective application. Each UI client engine also receives inputs from its respective application related to elements of its UI component tree and determines UI data items related to the inputs.

In operation the UI rendering engine receives the UI component trees from the UI client engines etc. The UI rendering engine then joins the plurality of UI component trees into a single tree structure. To specify the parameters for joining the trees the UI client engines etc. can for example define or indicate where in their trees other trees can be inserted. Subject to the logic implemented in the UI rendering engine the UI client engines etc. can indicate the location of possible tree insertions in a generic way such as here it is ok to insert a background effect . The UI client engines etc. can also suggest define or indicate where their tree should be inserted. This indication can also be performed in a quite general way such as I want to insert a particle effect in the background . The UI rendering engine can then determine an appropriate location to insert the tree within the UI tree structure . Once in possession of a the single tree structure the UI rendering engine determines a visual node tree for the single tree structure and then populates the visual node tree with UI data items received from at least one of the plurality of UI client engines and renders the UI in accordance with the visual node tree independently of further input from UI client engines as described above.

Different UI client engines etc. with different language bindings can coexist in same node render tree no matter what runtime limitations the language has e.g. Python threads . Since the individual UI component trees of the applications are combined to a single joint UI tree on the UI rendering engine the UI that is rendered by the server i.e. the UI rendering engine will for end users appear as if all the application UIs are part of the same application .

According to yet another aspect a single UI rendering engine can support multiple UI client engines and their associated applications running on different devices or different platforms such as a local device and an application running on a remote device such as in the cloud or on networked server. As above since the UI client engines for each application inject their trees and data items into the same tree on the UI rendering engine all scene graph UI advantages apply. The UI rendering engine does not need to know anything about a new application so for example the UI client engine for a new car radio application can be transparently injected into a common UI for an in vehicle navigation system for example.

According to another aspect and as shown in multiple UI rendering engines can support a single UI client engine and its associated application . Such a configuration enables an application on the first mobile device to utilize the screen space of the second mobile device by having the UI framework on the first mobile device communicate with a second UI rendering engine on the second mobile device .

In this way the single UI client engine can inject its tree and provide data items to multiple devices such as a desktop computer and a portable electronic device or a pair of mobile devices as shown in . Each device can have a separate UI rendering engines optimized for its particular form factor and display capabilities. Since the UI rendering engines do their own rendering it is possible to make a distributed UI that is responsive regardless of transport layer performance. According to this aspect the UI client engine determines a UI component tree for the application receives inputs from the application related to elements of the UI component tree and determines UI data items related to the inputs as described above. The UI client engine then interfaces with two or more UI rendering engines each of which can be associated with a separate display or be designed and optimized for different performance as described below.

In operation the UI rendering engines each receive the UI component tree from the client UI engine and individually determine a visual node tree for the UI component tree . The separate UI rendering engines asynchronously receive from the UI client engine the UI data items related to elements of the UI component tree and populate the visual node tree with the UI data items. Each UI rendering engine then renders the UI in accordance with the visual node tree independently of further input from the client UI engine . If a user input such as a touch event or gesture is detected by one of the UI rendering engines the input is communicated back to the UI client engine and to the other UI rendering engine . Both UI rendering engines can then re render the UI if appropriate while the UI client engine can provide the input to the application or otherwise act upon it.

As a further example not shown the single UI client engine can use several UI rendering engines on a same device. For example UI rendering engine could include an OpenGL renderer while UI rendering engine could include a software rendering backend rasterizer. The different UI rendering engines could for example be different versions of the rendering engine on the same device. For example UI rendering engines could be designed to render at different frame rates to serve different displays on a multi display device. The UI rendering engines could provide different power management capabilities. For example using wallpaper as example UI rendering engine could render wallpaper or background with less fidelity lower resolution to meet power management requirements. The UI rendering engines could form a dynamic cluster distributing different UI elements of a client application between rendering engines to meet metrics like expected FPS power management and resource management. The UI rendering engines can for example selectively render different elements or parts of the UI as defined by the UI client engine . The division of rendering tasks can be for example defined in an appropriate mark up language such as XML or programmatically such as in an API. Generally the UI rendering engines work independently to render their element s of the UI. However in a standalone mode the UI rendering engines could exchange data to improve rendering efficiency.

Referring again to it can be appreciated that the UI frameworks of the first and second mobile devices enable a client server configuration to be arranged such that the UI client engine can have UI elements rendered on both displays by communicating with the corresponding UI rendering engines . Since the UI client engine removes low level programming burden from the application the coordination of the UI being rendered across multiple screens can be performed by the UI client engine to take advantage of the additional screen space when available without the application requiring custom programming for each device type form factor screen size etc.

The application logic also defines what UI elements are to be included in the UI and if applicable from where the UI elements can be obtained. As described above the UI elements may be stored in a memory on the mobile device and may be obtained by the UI framework in order to have them rendered on the display for the application . When distributing a UI across multiple devices e.g. as shown in data for a first UI portion may be rendered on the display of the first mobile device and data for a second UI portion may be rendered on the display of the second mobile device . As a result of the connection between the first and second mobile devices interactions with the application and the UI being displayed on the screens are coordinated between the UI frameworks . As shown by way of example in interactions detected on either mobile device including interactions detected via the display itself e.g. gestures touches taps etc. are handled by the UI framework on the corresponding mobile device in order for such interactions if applicable to be communicated back to the application e.g. to perform an operation such as a database call message send operation etc.

An example of an interaction is shown in . In the first mobile device displays a first UI element including zoom and rotate operates while at the same time the second mobile device displays a second UI element including an image that can be zoomed into or out of or rotated using the functions displayed on the first mobile device . At stage the screen settings are provided by the second mobile device to the first mobile device and a second UI portion is sent by the first mobile device at stage . Stages and include displaying first and second UI portions respectively as discussed above thus distributing the UI experience across the first and second mobile devices . At stage a user selects the rotate function displayed on the first screen . To apply this function to the image being displayed on the second screen an instruction is generated and sent by the first mobile device to the second mobile device at stage . A rotation of the image may then be rendered by the UI framework on the second mobile device at stage .

Another example of an interaction is shown in . In the first mobile device displays a first portion A of a UI element and the second mobile device displays a second portion B of the UI element. At stage the screen settings are provided by the second mobile device to the first mobile device and a second UI portion is sent by the first mobile device at stage . It can be appreciated that the second UI portion sent at stage may include an already modified version of the complete image or the original image with instructions for modifying the image on the second mobile device when rendering the second UI portion . Stages and include displaying first and second UI portions respectively as discussed above thus distributing the UI experience across the first and second mobile devices . Between stages and a cursor is moved across the first screen causing an instruction to be sent from the first mobile device to the second mobile device at stage to have the cursor appear on the second screen at stage . At stage in this example an operation is performed on the second mobile device namely the selection of a button to close the image. Selection of the close button causes a second instruction to be sent from the second mobile device to the first mobile device at stage to have the UI framework on the first mobile device coordinate with the application and the UI rendering engines to update the respective displays .

Once the first and second mobile devices are near each other and paired they are capable of communicating with each other over a short range communications channel thus established and may exchange data. At and the first mobile device and second mobile device may then establish a virtual screen space. The operations performed at and may vary depending on which device initiates the pairing. For example one of the devices sharing data may have an application running that intends on sharing data and therefore initiates the establishment of the virtual screen that incorporates the display screens and . In the example shown in it is assumed that the first mobile device is sharing data or otherwise utilizing the combined virtual screen and may be designated as the master device while the second mobile device is designated the slave device. It can be appreciated that the master slave relationship is only one example and various other configurations may be applicable including masterless configurations.

By establishing the first mobile device as the master device the first mobile device is capable of taking over the display screen of the second mobile device and sends one or more UI elements to the second mobile device at e.g. the 2UI portion as shown in . The UI framework of the second mobile device receives the one or more UI elements at and the first mobile device displays the first UI portion at and the second mobile device displays the second UI portion at . At and the first and second mobile devices are operating with a shared UI controlled by an application on the first mobile device and interactions are tracked by the respective UI frameworks . The first and second UI portions remain displayed until an interaction is detected at or at which time the first and second devices coordinate changes to the combined display screen at and respectively. The operations performed at and vary depending on which device detects the interaction and the nature of the interaction. For example some interactions may cause a change to the currently displayed UI elements whereas other interactions may cause the current UI elements to be replaced by new UI elements. The UI elements are rendered by the UI rendering engines according to the interactions e.g. as discussed above.

It can therefore be seen that by having the UI frameworks on the paired devices handle the UI decisions such as how to update the respective UI spaces based on detected interactions the application developer does not need to expend significant programming resources on low level programming that would require knowledge of the size form factor OS version etc. of the device being utilized to create the combined space.

At the application development environment e.g. an application toolkit provides the tools for developing the application . The tools in this example include the ability to include a multi screen UI in at least a portion of the application . The development toolkit determines at whether or not the application developer wishes to include a multi screen UI. If so the toolkit provides one or more tools that enable the application developer to generate application logic for distributing UI elements across the multiple screens at . For example the toolkit may enable the application developer to incorporate a display portion on one screen and a tools portions e.g. picker on the other screen with the ability to specify which portion is displayed on the larger screen. The toolkit also enables the generation of application logic for a single screen mode at whether or not the application being developed intends on utilizing a second screen when available. The application is then generated for use e.g. compiled tested distributed etc. at .

Accordingly there is provided a method of displaying a user interface using multiple electronic devices the method comprising providing a first user interface framework on a first electronic device having a first display screen the first user interface framework determining that a second display screen on a second electronic device is available via a connection between the first and second electronic devices the first user interface framework using application logic from an application on the first electronic device to determine a first user interface portion to be displayed on the first display screen and a second user interface portion to be displayed on the second display screen and the first user interface framework providing data associated with the second user interface portion to the second electronic device.

There is also provided a computer readable storage medium comprising computer executable instructions for displaying a user interface using multiple electronic devices the computer executable instructions comprising instructions for providing a first user interface framework on a first electronic device having a first display screen the first user interface framework determining that a second display screen on a second electronic device is available via a connection between the first and second electronic devices the first user interface framework using application logic from an application on the first mobile device to determine a first user interface portion to be displayed on the first display screen and a second user interface portion to be displayed on the second display screen and the first user interface framework providing data associated with the second user interface portion to the second electronic device.

There is also provided a first electronic device comprising a processor memory and a display the memory comprising computer executable instructions for displaying a user interface using multiple electronic devices the computer executable instructions comprising instructions for providing a first user interface framework on the first electronic device the first user interface framework determining that a second display screen on a second electronic device is available via a connection between the first and second electronic devices the first user interface framework using application logic from an application on the first electronic device to determine a first user interface portion to be displayed on the first display screen and a second user interface portion to be displayed on the second display screen and the first user interface framework providing data associated with the second user interface portion to the second electronic device.

Referring to to further aid in the understanding of the example mobile devices described above shown therein is a block diagram of an example configuration of the first mobile device . The mobile device includes a number of components such as a main processor that controls the overall operation of the mobile device . Communication functions including data and voice communications are performed through a network interface . The network interface receives messages from and sends messages to a wireless network . In this example of the mobile device the network interface is configured in accordance with the Global System for Mobile Communication GSM and General Packet Radio Services GPRS standards which is used worldwide. Other communication configurations that are equally applicable are the 3G and 4G networks such as Enhanced Data rates for Global Evolution EDGE Universal Mobile Telecommunications System UMTS and High Speed Downlink Packet Access HSDPA Long Term Evolution LTE Worldwide Interoperability for Microwave Access Wi Max etc. New standards are still being defined but it is believed that they will have similarities to the network behavior described herein and it will also be understood by persons skilled in the art that the examples described herein are intended to use any other suitable standards that are developed in the future. The wireless link connecting the network interface with the wireless network represents one or more different Radio Frequency RF channels operating according to defined protocols specified for GSM GPRS communications.

The main processor also interacts with additional subsystems such as a Random Access Memory RAM a flash memory a touch sensitive display an auxiliary input output I O subsystem a data port a keyboard physical virtual or both a speaker a microphone a GPS receiver short range communications subsystem and other device subsystems . Some of the subsystems of the mobile device perform communication related functions whereas other subsystems may provide resident or on device functions. By way of example the display and the keyboard may be used for both communication related functions such as entering a text message for transmission over the wireless network and device resident functions such as a calculator or task list. In one example the mobile device can include a non touch sensitive display in place of or in addition to the touch sensitive display . For example the touch sensitive display can be replaced by a display that may not have touch sensitive capabilities.

The mobile device can send and receive communication signals over the wireless network after required network registration or activation procedures have been completed. Network access is associated with a subscriber or user of the mobile device . To identify a subscriber the mobile device may use a subscriber module component or smart card such as a Subscriber Identity Module SIM a Removable User Identity Module RUIM and a Universal Subscriber Identity Module USIM . In the example shown a SIM RUIM USIM is to be inserted into a SIM RUIM USIM interface in order to communicate with a network.

The mobile device is typically a battery powered device and includes a battery interface for receiving one or more rechargeable batteries . In at least some examples the battery can be a smart battery with an embedded microprocessor. The battery interface is coupled to a regulator not shown which assists the battery in providing power to the mobile device . Although current technology makes use of a battery future technologies such as micro fuel cells may provide the power to the mobile device .

The mobile device also includes an operating system and software components to and . The operating system and the software components to and that are executed by the main processor are typically stored in a persistent store such as the flash memory which may alternatively be a read only memory ROM or similar storage element not shown . Those skilled in the art will appreciate that portions of the operating system and the software components to and such as specific device applications or parts thereof may be temporarily loaded into a volatile store such as the RAM . Other software components can also be included as is well known to those skilled in the art.

The subset of software applications that control basic device operations including data and voice communication applications may be installed on the mobile device during its manufacture. Software applications may include a message application a device state module a Personal Information Manager PIM an application and a UI framework . A message application can be any suitable software program that allows a user of the mobile device to send and receive electronic messages wherein messages are typically stored in the flash memory of the mobile device . A device state module provides persistence i.e. the device state module ensures that important device data is stored in persistent memory such as the flash memory so that the data is not lost when the mobile device is turned off or loses power. A PIM includes functionality for organizing and managing data items of interest to the user such as but not limited to e mail contacts calendar events and voice mails and may interact with the wireless network .

Other types of software applications or components can also be installed on the mobile device . These software applications can be pre installed applications i.e. other than message application or third party applications which are added after the manufacture of the mobile device . Examples of third party applications include games calculators utilities etc.

The additional applications can be loaded onto the mobile device through at least one of the wireless network the auxiliary I O subsystem the data port the short range communications subsystem or any other suitable device subsystem .

The data port can be any suitable port that enables data communication between the mobile device and another computing device. The data port can be a serial or a parallel port. In some instances the data port can be a USB port that includes data lines for data transfer and a supply line that can provide a charging current to charge the battery of the mobile device .

For voice communications received signals are output to the speaker and signals for transmission are generated by the microphone . Although voice or audio signal output is accomplished primarily through the speaker the display can also be used to provide additional information such as the identity of a calling party duration of a voice call or other voice call related information.

The touch sensitive display may be any suitable touch sensitive display such as a capacitive resistive infrared surface acoustic wave SAW touch sensitive display strain gauge optical imaging dispersive signal technology acoustic pulse recognition and so forth as known in the art. In the presently described example the touch sensitive display is a capacitive touch sensitive display which includes a capacitive touch sensitive overlay . The overlay may be an assembly of multiple layers in a stack which may include for example a substrate a ground shield layer a barrier layer one or more capacitive touch sensor layers separated by a substrate or other barrier and a cover. The capacitive touch sensor layers may be any suitable material such as patterned indium tin oxide ITO .

The display of the touch sensitive display may include a display area in which information may be displayed and a non display area extending around the periphery of the display area. Information is not displayed in the non display area which is utilized to accommodate for example one or more of electronic traces or electrical connections adhesives or other sealants and protective coatings around the edges of the display area.

One or more touches also known as touch contacts or touch events may be detected by the touch sensitive display . The processor may determine attributes of the touch including a location of a touch. Touch location data may include an area of contact or a single point of contact such as a point at or near a center of the area of contact known as the centroid. A signal is provided to the controller in response to detection of a touch. A touch may be detected from any suitable object such as a finger thumb appendage or other items for example a stylus pen or other pointer depending on the nature of the touch sensitive display . The location of the touch moves as the detected object moves during a touch. One or both of the controller and the processor may detect a touch by any suitable contact member on the touch sensitive display . Similarly multiple simultaneous touches are detected.

One or more gestures are also detected by the touch sensitive display . A gesture is a particular type of touch on a touch sensitive display that begins at an origin point and continues to an end point. A gesture may be identified by attributes of the gesture including the origin point the end point the distance traveled the duration the velocity and the direction for example. A gesture may be long or short in distance and long or short in duration. Two points of the gesture may be utilized to determine a direction of the gesture.

An example of a gesture is a swipe also known as a flick . A swipe has a single direction. The touch sensitive overlay may evaluate swipes with respect to the origin point at which contact is initially made with the touch sensitive overlay and the end point at which contact with the touch sensitive overlay ends rather than using each of location or point of contact over the duration of the gesture to resolve a direction.

Examples of swipes include a horizontal swipe a vertical swipe and a diagonal swipe. A horizontal swipe typically comprises an origin point towards the left or right side of the touch sensitive overlay to initialize the gesture a horizontal movement of the detected object from the origin point to an end point towards the right or left side of the touch sensitive overlay while maintaining continuous contact with the touch sensitive overlay and a breaking of contact with the touch sensitive overlay . Similarly a vertical swipe typically comprises an origin point towards the top or bottom of the touch sensitive overlay to initialize the gesture a horizontal movement of the detected object from the origin point to an end point towards the bottom or top of the touch sensitive overlay while maintaining continuous contact with the touch sensitive overlay and a breaking of contact with the touch sensitive overlay .

Swipes can be of various lengths can be initiated in various places on the touch sensitive overlay and need not span the full dimension of the touch sensitive overlay . In addition breaking contact of a swipe can be gradual in that contact with the touch sensitive overlay is gradually reduced while the swipe is still underway.

Meta navigation gestures may also be detected by the touch sensitive overlay . A meta navigation gesture is a gesture that has an origin point that is outside the display area of the touch sensitive overlay and that moves to a position on the display area of the touch sensitive display. Other attributes of the gesture may be detected and be utilized to detect the meta navigation gesture. Meta navigation gestures may also include multi touch gestures in which gestures are simultaneous or overlap in time and at least one of the touches has an origin point that is outside the display area and moves to a position on the display area of the touch sensitive overlay . Thus two fingers may be utilized for meta navigation gestures. Further multi touch meta navigation gestures may be distinguished from single touch meta navigation gestures and may provide additional or further functionality.

In some examples an optional force sensor or force sensors is disposed in any suitable location for example between the touch sensitive display and a back of the mobile device to detect a force imparted by a touch on the touch sensitive display . The force sensor may be a force sensitive resistor strain gauge piezoelectric or piezoresistive device pressure sensor or other suitable device. Force as utilized throughout the specification refers to one or more of force measurements estimates and calculations such as pressure deformation stress strain force density force area relationships thrust torque and other effects that include force or related quantities.

Force information related to a detected touch may be utilized to select information such as information associated with a location of a touch. For example a touch that does not meet a force threshold may highlight a selection option whereas a touch that meets a force threshold may select or input that selection option. Selection options include for example displayed or virtual keys of a keyboard selection boxes or windows e.g. cancel delete or unlock function buttons such as play or stop on a music player and so forth. Different magnitudes of force may be associated with different functions or input. For example a lesser force may result in panning and a higher force may result in zooming.

It will be appreciated that any module or component exemplified herein that executes instructions may include or otherwise have access to computer readable media such as storage media computer storage media or data storage devices removable or non removable or both removable and non removable such as for example magnetic disks optical disks or tape. Computer storage media may include volatile and non volatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Examples of computer storage media include RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by an application module or both. Any such computer storage media may be part of the mobile device or accessible or connectable thereto. Any application or module herein described may be implemented using computer readable executable instructions that may be stored or otherwise held by such computer readable media.

The steps or operations in the flow charts and diagrams described herein are just for example. There may be many variations to these steps or operations without departing from the principles discussed above. For instance the steps may be performed in a differing order or steps may be added deleted or modified.

Although the above principles have been described with reference to certain specific examples various modifications thereof will be apparent to those skilled in the art as outlined in the appended claims.

