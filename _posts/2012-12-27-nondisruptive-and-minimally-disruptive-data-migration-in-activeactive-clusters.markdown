---

title: Non-disruptive and minimally disruptive data migration in active-active clusters
abstract: Data migration is performed in a cluster of host computers each using a mechanism associating data with a source LUN. During a synchronization operation the contents of the source LUN are copied to the target LUN while ongoing normal source LUN writes are cloned to the target LUN. A datapath component of an agent coordinates the writes at the target LUN to maintain data consistency. Upon completion of synchronization, each host stops the write cloning and disables access to the source LUN, in conjunction with a modification of the mechanism to newly associate the data with the target LUN. Depending on the type of mechanism and system, the modification may be done either disruptively or non-disruptively, i.e., with or without stopping normal operation of software of the host computers.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09460028&OS=09460028&RS=09460028
owner: EMC Corporation
number: 09460028
owner_city: Hopkinton
owner_country: US
publication_date: 20121227
---
The present invention relates to migration of data from a source data storage device to a target data storage device in a data processing system.

Data migration techniques are used to move or migrate data from one storage device or logical unit to another for any of a variety of purposes such as upgrading storage hardware or information lifecycle management. Generally migration involves synchronizing the target device to the source device i.e. achieving an operating state in which the target device stores the same data as the source device and then switching operation so that subsequent accesses of the data are directed to the target device instead of the source device. Once the switching is successfully accomplished the source device can be taken out of service or put to some other use.

Non disruptive migration is performed while there is ongoing application level access to the data stored on the source storage device. In non disruptive migration there are two parts to achieving synchronization existing data on the source device is copied to the target device and ongoing application writes of new data are cloned i.e. sent to both the source and target devices. Non disruptive migration also requires a non disruptive mechanism for switching operation to the target device. Example descriptions of non disruptive migration can be found in the following US patents whose entire contents are incorporated by reference herein 

Clustering is a technique used in computer systems to provide certain desirable functionality and characteristics from the perspective of external users. Advantages include increased performance and availability over non clustered systems. Two general types of clusters are failover and parallel or active active clusters. In a failover cluster all cluster nodes may be aware of a given storage device accessible in the cluster but in general a given storage device is accessed by only one node during operation. In the event of node failure a failover mechanism causes ownership of the storage device to be transferred to a new node that has assumed responsibility for the workload of the failed node. Due to the single node access there is no need for synchronizing accesses among the hosts. In active active clusters storage devices may be actively accessed from all nodes in the cluster and the operating software e.g. application software of the nodes is responsible for synchronizing access to shared storage resources.

It is desirable to support data migration in a cluster environment but providing such support can present certain challenges. Non disruptive migration involves several sensitive operations where input output I O is temporarily suspended and from which it is necessary to recover in a non obtrusive manner. The fine control over I O and the possibility of aborting and restarting a migration at different steps of the process would require significant communication and coordination among the nodes of the cluster most of it needed only for the unlikely event of a failure and therefore constituting inefficient use of system resources.

Methods and apparatus are disclosed for migrating data from a source LUN to a target LUN in a data processing system having a cluster of multiple host computers where the data is accessed by software of the host computers using a mechanism initially associating the data with the source LUN as the location of the data in the system. An example of such a mechanism is a pseudoname as described in the above referenced U.S. Pat. No. 7 904 681 and other examples are described herein.

A method includes commanding a kernel level migration component of each host computer to begin cloning source LUN writes to the target LUN each write to be cloned by writing a duplicate of each source LUN write to the target LUN. Subsequently a LUN copying operation and target LUN write coordination by an agent in the system are initiated. The LUN copying operation reads data from the source LUN and writes the data to the target LUN so as to transfer all existing data. The target LUN write coordination coordinates target LUN writes from the LUN copying operation with target LUN writes from the cloning of source LUN writes so as to ensure consistency between the source LUN and target LUN.

Upon completion of the LUN copying operation the kernel level component of each host computer is commanded to stop the cloning of source LUN writes and to disable access to the source LUN by the host computer software in conjunction with a modification of the mechanism so as to newly associate the data with the target LUN as the location of the data in the system. Depending on the type of mechanism and system the modification may be done either disruptively or non disruptively i.e. with or without stopping normal operation of the data processing system.

The interconnect includes one or more storage oriented networks providing pathways for data transfer among the hosts and devices . An example of the interconnect is a FibreChannel storage area network SAN either by itself or in conjunction with Ethernet or other network components. The devices are logical units of storage allocated for uses such as storing databases file systems etc. used by application programs executing on the hosts . Generally the devices are visible to the hosts as block oriented storage devices.

The LUNs include a source LUN S and a target LUN T participating in a migration operation by which the target LUN T functionally replaces the source LUN S in the system. It is assumed that prior to migration the source LUN S stores a data resource that is accessed by operating software e.g. application software executing on the hosts using a mechanism that specifically associates the application visible data resource with the source LUN S. Specific examples of such mechanisms are described below. A migration operation moves the data resource to the target LUN T and changes the mechanism so that future application accesses of the data resource are directed to the target LUN T rather than to the source LUN S. Reasons for such migration of storage resources include a desire for additional storage capacity or improved performance or to upgrade to more current and well supported hardware for example. In some cases the source LUN S is to be removed from the system although in other cases it may be maintained and reused for other purposes.

In the active active cluster there may be applications executing simultaneously on different hosts having access to the source LUN S. One aspect of the migration operation is to coordinate certain operations of the hosts to ensure that there is no data loss or data incoherency created which could have any of several deleterious effects as generally known in the art. These aspects of the migration operation are described below.

For migration operation the system includes an actor referred to as a migration controller MIG CTRLLR as well as an off host agent . As indicated by lines during a migration the migration controller communicates with the agent as well as with the drivers . In some embodiments the migration controller may be implemented by software executing on one of the hosts whereas in other embodiments it may be realized by software executing on a separate physical or virtual device in the system referred to below as a migration appliance . The agent generally includes a control component CTL C and a datapath component DP D. The datapath component D may be realized by software firmware executing on a storage controller included in a storage housing or array that houses the target LUN T. The control component C may be formed by software executing on a separate controller device or agent appliance functionally interconnected to the hosts and devices . This connection may be via the interconnect or in other embodiments via other connections apart from the datapath. In one embodiment the agent may be realized using the RecoverPoint product of EMC Corporation. In the RecoverPoint architecture the datapath component D is referred to as a write splitter or splitter and the control component C resides on a device referred to as the RecoverPoint appliance or RPA. In some embodiments the migration appliance and agent appliance may be the same physical or virtual device.

The agent plays two important roles in the migration operation described herein. First the control component C has initiator access to the source and target LUNs S T and it includes functionality for bulk copying of data from the source LUN S to the target LUN T. Second the datapath component D serves as a single point via which the hosts and the control portion C access the target LUN T. As described in more detail below during migration the datapath component D is responsible for coordinating the cloned writes from the hosts with the writes of the copying operation from the control component C to ensure data integrity.

Once the target LUN T is synchronized to the source LUN S at a change is made to the mechanism to newly associate the application visible data with the target LUN T. Future application level writes to the data are automatically directed to the target LUN T rather than to the source LUN S. After any necessary cleanup the source LUN S can be retired or put to other use. Details of the change process are also described below.

It is assumed that prior to the process both the source and target LUNs S T have become configured for use by the hosts . The contents data of the source LUN S constitute a data resource for operating software of the hosts and there is some mechanism that associates this data resource with the source LUN S as the location of the resource. Different mechanisms have different properties and may require different handling during a migration operation. One aspect of a mechanism is whether it is location dependent or location independent i.e. whether the mechanism includes physical device identifying information. Another aspect of a mechanism is whether it involves a name used by applications to identify the location of application data to lower level components such as the drivers or rather a signature stored with application data on a device used by operating software for purposes such as detection of duplication or other integrity consistency checking. In the process of it is assumed that whatever mechanism is used there is a way to non disruptively make a change to the mechanism as required to complete migration. Different specific mechanisms and the manner in which they are handled in non disruptive as well as disruptive migrations are discussed below.

Referring to in response to the Setup command the migration controller verifies that the target LUN T is a suitable target such as checking that its size capacity is at least the size of the source LUN S. It then communicates with the agent to instruct it to establish a replication relationship between the source LUN S and the target LUN T. The agent may return an explicit response indicating success or failure in establishing the replication relationship. Further operation as described below assumes successful completion of this task.

In response to the Synchronize command the migration controller commands certain kernel level components of the driver of each host to prevent application access to the target LUN T and to begin cloning writes of the source LUN S to the target LUN T. Thus when a given host writes to the source LUN S the driver of that host performs the write operation on the source LUN S and performs the same write operation on the target LUN T. In one embodiment it is required that the writes occur in that order i.e. first to the source LUN S and then to the target LUN T and that the write to the target LUN T only occur if the write to the source LUN S completes successfully. In this case the agent may operate in a particular advantageous fashion as described below. It should be noted that each host may need to temporarily suspend input output to the source and target LUNs S T to transition into this operating mode target access disabled and write cloning active but in general it is not required that such suspension occur across all hosts at the same time. However it is necessary that each host has transitioned into this operating mode before migration continues.

The migration controller then communicates with the control component C of the agent to instruct it to begin the copying operation to copy all existing contents of the source LUN S to the target LUN T. The control component C initiates the copying. At this point the migration advances to the Synchronizing state .

During the Synchronizing state the contents of the target LUN T are made identical to the contents of the source LUN S. This is achieved by duplicating or cloning any host writes performed on the source LUN S to the target LUN T as well as copying all the existing previously written data from the source LUN S to the target LUN T. Synchronization is complete when all existing data has been copied. However all application writes continue to be duplicated.

The migration controller may monitor the progress of the copying operation or at least become notified of its completion. For monitoring the migration controller may periodically or upon user command send a query message to the controller component C to which the agent responds with some type of progress or completion indicator. In one embodiment the controller component C returns a value representing the number of blocks of the source LUN S that have been copied. The migration controller knows the size of the source LUN S and can compute a percentage or fraction of completion.

Once synchronization is complete the migration progresses to a Source Selected state in which reads to the data resource continue to be directed to the source LUN S while writes to the resource continue to be duplicated to both the source LUN S and target LUN T. Generally this state may last only as long as deemed necessary by the human or machine user e.g. a storage administrator user .

When the user is ready the user issues a Commit command . This causes the following operations by the driver of each host under control of the migration controller 

The above operations require proper sequencing. In particular they are performed in three sequential phases each of which must be complete for every host of the cluster before the step s of the next phase can be initiated 

The migration controller may take action to initiate each phase in sequence and leave any intra phase sequencing to each host . Thus in phase 2 steps 2 and 3 may be performed in any order. Features may be incorporated that protect the integrity of the process in the event of a host reboot during phase 2. In some embodiments I O suspension does not survive a reboot and therefore without such protective features there is a possibility of inconsistency between the source LUN S and the target LUN T at the point of commitment. The protective features are preferably instituted at the end of Phase 1 and removed at the beginning of Phase 3. An example of such protective features is given in U.S. patent application Ser. No. 13 575 740 filed Jun. 28 2012. In an embodiment in which I O suspension survives a reboot such protective mechanisms may not be necessary.

Once the above operations have been performed the migration enters the Committed state . In this state application I O is automatically directed to the target LUN T and not to the source LUN S by virtue of the change to the association mechanism. The target LUN T will store all newly written data and no synchronization is maintained with the source LUN S. It is generally not possible to revert to operating with the source LUN S.

A cleanup command initiates a cleanup operation to remove any remaining metadata associating the source LUN S with the storage resource that has been migrated. At that point the source LUN S may be removed from the system or it may be re configured for another use in the system. One important task performed during cleanup is to erase any information on the source device S that might cause it to be identified mistakenly as the storage resource that has been migrated to the target device T. Earlier in the migration access control prevents this mistaken identity . Also either at this time or earlier during processing of the Commit command the source target relationship previously established in the agent is removed. If done during the Commit command it can be done after I O is resumed at each host as it does not require IO suspension. In connection with potential re use of the source LUN S the contents of the source LUN S may be erased perhaps with replacement by a known pattern such as all zeros for security or other operational reasons.

The process of includes an abort path leading from the Synchronizing state and Source Selected state back to the Setup state . Aborting may occur by user command or by automatic operation when problems are encountered during the process. For example if either the source LUN S or target LUN T fails or otherwise becomes unavailable during the process such failure may be detected either manually or automatically and lead to aborting the migration.

More particularly with respect to failures a device fault is a write failure to either the source or target LUNs S T. Since all writes are duplicated migration can only proceed if both writes succeed. If one succeeds and the other fails migration must be aborted. In this case the migration will go into a target device faulted state at this point and the user will have to execute the abort and start over perhaps first curing whatever problem caused the fault. The copy operation could also fail due to a read failure on the source LUN S or a write failure on the target LUN T. This is not a device fault but it will cause the synchronization to stop. An explanation of the handling of device faults in a non cluster environment can be found in the above referenced U.S. Pat. No. 7 770 053.

Device fault handling in the cluster environment may be generally similar to that described in the 053 patent but there are specific differences. For example in the non cluster environment as described in the 053 patent there is a target selected state in addition to a source selected state and in the target selected state reads are redirected to the target device T while writes are still being cloned and synchronization maintained. If the system should be shut down unexpectedly upon restart the non selected side is faulted because there s no guarantee that all writes made it to both sides. Thus the source device is faulted if this occurs during operation in the target selected state. Also if a fault happens during normal I O then the side that fails the write will be faulted so the source side is faulted when operating in the source selected state and a write to the source device fails. In contrast in the cluster environment as described herein operation proceeds directly from the Source Selected state to the Committed state or Committed and Redirected state there is no target selected state. Only the target LUN T is ever faulted when a write fails no matter which side the write fails on.

In the Setup state the hosts continue to access the data resource on source LUN S and the agent is inactive. The agent has previously set up the replication relationship between the source LUN S and the target LUN T and is awaiting further instruction as described above with reference to .

During the Synchronizing state each host prevents application access to the target LUN T and is cloning the writes for the source LUN S to the target LUN T. The controller component C of agent is performing the copying operation and reports progress to the migration controller . The datapath component D of agent coordinates the writing to the target LUN T. An example of this coordinating is described below.

During the Source Selected state the hosts continue to prevent access to the target LUN T and to clone writes. The agent applies the cloned writes to the target LUN T.

During the Committed state the hosts prevent access to the source LUN S and direct I O for the data resource to the target LUN T. No write cloning is performed. The agent is inactive assuming termination of the source target relationship in agent at the end of the Commit step .

During the Committed state read and write accesses by the hosts to the data resource formerly stored on the source LUN S are directed to the target LUN T. The target LUN T continues to have the correct current data as seen by the rest of the system and the source LUN S is no longer synchronized with the target LUN T because writes are no longer directed to the source LUN S.

Referring to the control component C performs the copying operation by stepping linearly through the contents of the source LUN S such as in order of increasing block addresses. The direction of copying to the target LUN T is indicated by the arrow DIRECTION. Additionally copying occurs in multi block chunks such as chunks 1 MB in size for example. The control component C and datapath component D maintain information such as pointers that divide the target LUN T into three regions as shown a synchronized region of chunks whose contents have already been copied to the target T an in progress region for a chunk currently being synchronized i.e. for which the data is being copied from the source LUN S to the target LUN T and a to be synchronized region of chunks whose contents have not yet been copied to the target LUN T. The pointers defining these regions are advanced as the copying of each chunk is completed.

To maintain data coherency the datapath component D handles the cloned writes and the writes from the copying operation differently depending on which region of the target LUN T they are directed to. For those to the synchronized region cloned writes are serviced normally. The copying for this region is already complete so there is no danger that a write from the copying operation will overwrite the newer data from a cloned write. Writes to the to be synchronized region are discarded but with an indication of success being returned to the host that initiated the write. This operation is possible because writes are being cloned by first writing to the source LUN S and then to the target LUN T. A cloned write arriving at the datapath component D has already been applied to the source LUN S and therefore it will automatically be copied to the target LUN T later as part of the copying operation.

For writes directed to the in progress region certain writes may be serviced normally and other writes either delayed or discarded for consistency. In one embodiment all cloned writes to this region are delayed until the copying is complete for this region then the delayed writes are processed normally as for region . In another embodiment the datapath component D performs finer grained processing to remove delays. For example the datapath component D might service the cloned writes and maintain a bitmap or other record identifying those blocks of the in progress region that have been the object of a cloned write. For these blocks the corresponding writes of the copying operation are discarded i.e. not performed . This discarding prevents the problem of overwriting newly written data as described above.

The purpose of the Committed and Redirected state is to support a separate task of changing the mechanism that associates the data resource with a source LUN S so that the data resource can be newly associated with the target LUN T. This is the point at which disruption occurs where for example one or more applications of the hosts may be stopped reconfigured as necessary to create the new association and then restarted. Once the change is made the system can operate correctly using the new association with the target LUN T so that redirection is no longer required.

Once the application is stopped redirection and changing the association mechanism can generally be done in either order as redirection has no effect when the application is stopped. In some cases the mechanism may be changed while redirection is still in effect. With some association mechanisms it may be necessary to stop redirection prior to updating the mechanism. In either case prior to restarting normal operation e.g. restarting any applications that are being reconfigured the Undo Redirect command is used to advance the migration state to Committed . Normal operation is then resumed. It should be noted that at least some control communications such as SCSI Inquiry commands are not redirected so that each LUN remains directly accessible for purposes of receiving or providing corresponding control related information.

As mentioned above there are several specific cases of mechanisms that associate a data resource with the location LUN of the data resource. Specific cases may differ along one or more of the following dimensions 

The application s executed by the user VMs may be conventional user level applications such as a web server database application simulation tool etc. These access data of so called virtual disks that are presented by the hypervisor . The hypervisor itself employs the FS LVM and devices of as the underlying real physical file system. The MP DM plugin is a component working in conjunction with other storage device driver code not shown of the hypervisor to carry out the reading and writing of data to from the devices in operation of the FS LVM . The MP DM plugin may provide specialized and or enhanced input output functionality with respect to the devices . For example it may include multipathing functionality and an ability to access individual LUNs via multiple paths using the paths for increased performance and or availability. An example of such multipathing functionality is that provided by the PowerPath VE product sold by EMC Corporation.

The migration application is a specialized application providing the functionality of the migration controller described above. In particular the migration application carries out higher level logic and user facing functionality of migration. For example it may provide the above mentioned command line interface or application programming interface API for interacting with a human or machine user that exercises control over a migration process. In operation it communicates with the MP DM plugin of each host to cause the MP DM plugin of each host to perform lower level operations pertaining to migration. One example is the above discussed duplication or cloning of writes used to maintain synchronization between the source LUN S and the target LUN T. Another is to prevent access to the target LUN T prior to the migration becoming committed as well as preventing access to the source LUN S once the migration has become committed. The migration VM of a given host may call directly to the other hosts through a so called common information model object manager or CIMOM. More generically a call may be made through a listener employing a migration specific component that handles function invocations used to provide commands to the MP DM plugin at each migration step. Overall communication is done by the migration VM invoking each kernel action on each host as needed through each individual host s listener.

The hypervisor in a host such as shown in may employ a construct known as a datastore in its use of storage devices . This construct is used by ESX for example. A datastore is an organized container usable by the FS LVM to store files which implement virtual disks presented to the user VMs . Each datastore is assigned a unique signature which includes information identifying the device on which the datastore resides. A signature is a set of information metadata written on the device itself. For ESX it includes a device identifier associated with the device by the physical storage system array and which is returned in response to a SCSI extended inquiry command for vital product data VPD page 0x83 for example. ESX writes this SCSI property onto the device on which the datastore resides. When ESX accesses the datastore it checks the signature included in the datastore against the device identifier as returned by the SCSI extended inquiry command for VPD page 0x83 for the device on which the datastore resides. The values must match in order for the access to be handled normally. If there is a mismatch ESX treats it as an error condition and does not complete the access. This is done to prevent a replica of the datastore being used in place of the actual datastore.

It will be appreciated that the datastore signature as described above is a location dependent mechanism associating the datastore with the device where the datastore is located. When a datastore is copied from a source LUN S to a target LUN T the existing signature is also copied so that the signature for the datastore as residing on the target LUN T identifies the source LUN S instead of the target LUN T. If the datastore were accessed from the target LUN T in this condition the signature checking would fail because the signature having the device identifier for the source LUN S does not match the device identifier for the target LUN T on which the datastore now resides. In order for the migration to be fully completed the signature must be changed to include the device identifier of the target LUN T instead of the device identifier of the source LUN S.

In one embodiment the disruptive process of may be used along with a separate resignaturing process that is performed while the migration is in the Committed and Redirected state . More comprehensively the following sequence of actions may be performed 

In other embodiments another process may be used to effect resignaturing without requiring the stopping and starting of the pertinent VMs . In such a case the non disruptive process of may be used. It should also be noted that in other types of systems notably the Windows operating system from Microsoft signatures are also employed but they do not include physical device identifying information and thus are location independent. In this case also a non disruptive process such as that of may be used because the signature is copied to the target LUN T as part of the migration.

While the above description focuses on use of signatures in particular in the system of in alternative embodiments other associating mechanisms may be used and are handled in a corresponding manner. For example the system may use device names such as described below and in that case operations the same or similar to those described below may be used.

A filter driver is a component working in conjunction with a standard device driver not shown as part of an operating system that implements the system calls reading and writing data from to the user devices as requested by the applications . The filter driver may provide specialized and or enhanced input output functionality with respect to the user devices . For example in one embodiment the filter driver may be a multipathing driver having an ability to access individual LUNs via multiple paths and it manages the use of the paths for increased performance and or availability. An example of a multipathing driver is the above mentioned PowerPath driver.

The migration tool contains functionality for data migration operations. The user level part carries out higher level logic under control of the migration controller . A kernel level part of the migration tool performs lower level operation such as that provided by MP DM plugin . The kernel level part may be realized in one embodiment as an extension component having a defined interface to a basic or core set of components of the filter driver .

It should be noted that the applications may be virtual machines that contain user applications. Also referring back to although not shown there is a kernel space and there may be a user space.

As previously indicated the migration controller is a single point of control for migrations. It can reside in an off host computerized device or appliance or in one of the hosts in the cluster .

When the migration controller is realized in an off host migration appliance then a migration is initiated at the appliance and the commands executed on each host are subordinate commands. The above described flows may be augmented as follows. During setup a cluster flag may be added to the Setup command that lists the hosts in the cluster . This command is executed on the appliance. The appliance in turn calls Setup cluster on each host with another flag sub subordinate . This is a system level command for communications between the appliance and hosts not available to users. Alternatively the API on the host may be invoked with the same information. The sub flag indicates to the host receiving the command that the host is only doing host specific setup actions and not setting up the overall migration which happens only once from the appliance. For the Synchronize command the user runs it on the appliance. The appliance in turn invokes the Synchronize command or API on each host . Each host sets up the host specific state for the synchronization. Once all hosts have successfully performed these tasks the appliance code invokes the technology specific code to start the copy operation. The commit operation may require multiple staged operations at the hosts i.e. commit start then commit middle and finally commit end the three phases discussed above with reference to . As for the other commands during the Commit command the appliance is the only place that technology specific code is called. Cleanup follows the same pattern as synchronize but there s no technology specific code. Undo Redirect if used also works like synchronize because it is assumed that the application using the target LUN T is not running when that command executes.

The above referenced U.S. Pat. No. 7 904 681 provides two examples of mechanisms that associate application visible data with a particular LUN. In one case an application including a file system or logical volume manager is configured with a native name of the source LUN S and uses this name in all I O commands for the associated data e.g. database records . In this case the minimally disruptive process of may be used for migration and during the Committed and Redirected state all applications using such a native name are stopped reconfigured with the native name of the target LUN T and then restarted. In another case applications are configured with a so called pseudoname that is mapped by lower level driver components to a particular LUN. In the present context this mapping will initially associate the pseudoname with an identifier of the source LUN S. The non disruptive process of can be used and the mapping changed as part of the transition from the Source Selected state to the Committed state as described above. While these mechanisms and operations are described with reference to a host similar to host they may also be used in a host of the type shown in .

While various embodiments of the invention have been particularly shown and described it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the spirit and scope of the invention as defined by the appended claims.

