---

title: Apparatus and system for interfacing with computers and other electronic devices through gestures by using depth sensing and methods of use
abstract: Disclosed herein are systems and methods for gesture capturing, detection, recognition, and mapping them into commands which allow one or many users to interact with electronic games or any electronic device interfaces. Gesture recognition methods, apparatus and system are disclosed from which application developers can incorporate gesture-to-character inputs into their gaming, learning or the like applications. Also herein are systems and methods for receiving 3D data reflecting hand, fingers or other body parts movements of a user, and determining from that data whether the user has performed gesture commands for controlling electronic devices, or computer applications such as games or others.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09041775&OS=09041775&RS=09041775
owner: Mgestyk Technologies Inc.
number: 09041775
owner_city: Ottawa, ON
owner_country: CA
publication_date: 20120323
---
This application claims priority from U.S. provisional patent application U.S. 61 466 624 filed on Mar. 23 2011 entitled Methods Apparatus and System for Depth Measurement and Gesture Control of Electronic Devices and Computer Applications which is incorporated by reference herein in its entirety.

The invention relates to the apparatus method system and applications of interfacing computers various electronic devices and or other systems with people using gesture or body or body parts movements based controls which in some embodiments can be structured as a language and which are interpreted by the interface which in a particular case can contain a depth map generator device which may be a specialized 3D three dimensional image generating camera and a processing device to detect identify and understand the gesture command.

The field of 3D objects reconstruction has been investigated thoroughly by the academia and industry and results of the efforts put so far in this field are abundant. Hereafter a brief overview is given. Various methods are known in the art for 3D object reconstructions and or depth measurement of points and surfaces of real objects using image capturing devices controlled light environment or structured light generators and computers. Named also optical 3D mapping the generation of a 3D profile of the surface of an object by processing the optical image of that object has been for a long time the subject of numerous scientific works and patents. There is a vast literature on the subject of depth mapping methods. The most straight forward scheme combines a distance measuring device with a scanning method and system. In this category are 3D laser micro vision for micro objects and laser scanners for macro objects.

Another approach that has been studied extensively is depth mapping based on triangulation. Triangulation methods require mathematical manipulation in order to obtain the depth information. Another interesting scheme for depth mapping involves the projection of structured patterns. A structured pattern is projected over an object and from the distortion of the projected pattern the surface shape of the object is determined. Various kinds of projected patterns have been used including stripe sinusoidal coded spatially intensity modulated pseudo random grid and Moir patterns.

All solid state imaging range cameras were proposed in technical publications or patents like U.S. Pat. Nos. 4 687 325 or 5 682 229 and others. The Time of Flight method was used to produce a depth map of the object by sending invisible optical radiation on the object and capturing with an image capturing device the reflected radiation on which a processing device calculated the time needed by the optical radiation to travel to the object and for the reflected image to be captured by the image capturing device the optical radiation emitting device and the capturing device being practically mounted together on the same location on the apparatus. The following patents were filled in U.S. Pat. Nos. 7 224 384 B1 7 671 391 and others. Some technologies were produced and patented for various types of image capturing devices such as the U.S. Pat. No. 7 688 944 B2.

Some methods are based on generating a precisely timed beam of energy emitted from a single source with the beam illuminating the scene while the reflected energy from the beam is detected with a camera whose sensitivity is precisely timed and synchronized with the emitted beam. The reflected energy is separated and segregated according to the time of arrival at the camera as for example in the U.S. Pat. No. 5 081 530. One or more cycles of energy are emitted the camera segregates reflected energy during each cycle by separately storing the energy detected before and after a given time. The ratio of two consecutive separately detected energies conveys depth or third dimension information which is processed according to a special algorithm. Signals can be displayed to a viewer to create a stereoscopic image or used by a machine for automatic response to a three dimensional environment.

Some methods and system were based on projecting a grid of a randomly organized speckle of dots called a Pseudo Random Binary Array PRBA which is generated by a light source traversing a transparent plastic sheet on which the PRBA was imprinted. The depth of the points on the object surface is calculated by processing the information contained in the dots present on the object image acquired by an image acquisition device and a reference image of a calibrated PRBA pattern imprinted on the transparent plastic sheet. A coarse disparity map is generated by the image processing algorithm which is called a coarse 3D depth map. Using the epipolar geometry the camera model and the Fundamental Matrix a stereo fusion algorithm based on Dynamic Programming technique combined with autoregressive modeling allows to reconstruct a 3D epipolarized image of the object with high precision in a range of tens of tau . A de epipolarization method is finally providing the real 3D image of the object. The method is repetitively applied until all 3D views of the object are reconstructed. This technique is prior art for the U.S. Pat. No. 6 751 344 Patent Applications US 2008 0240602 U.S. Provisional Patent Application 61 016 832 U.S. patent application Ser. No. 11 899 542 U.S. Provisional Patent Application 60 909 487 and International Publication WO 2007 043036.

Some other methods and system also published were based on colored PRBA grid of randomly organized and differently colored lines which were projected on a real object the grid being generated by a light source traversing a transparent plastic sheet on which the colored PRBA grid was imprinted. An image acquisition unit detects the light response of the illuminated region of the object and generates image data. Deformations of the colored grid pattern as projected on the object are compared with a reference image of the pattern as projected on the real object in real time and from the above comparison a set of control points is obtained which then were used for the calculation of 3D surfaces which displayed on the visualization unit represent the 3D map of the object in three dimension and displayed graphically on the computer screen.

Some other methods and system also published and which were using pulsed laser light triggered by a function generating device and a gain modulated camera. Objects are illuminated by the infrared pulsed laser. The gain of the camera that captures an image of the objects is linearly increased with respect to time. Thus the light reflected from farther distances is amplified with a larger camera gain than that from nearer distances. The output image intensity of far objects is higher than that of near objects. The image intensity represents the distance to the object. With an increase in the speed of the camera gain modulation the sensitivity of the depth measurement is increased.

Some methods are based on projecting a laser speckle pattern onto the object and then analyzing the image of the pattern as projected on the object. For example PCT International Publication WO 2007 043036 describes a system and method for object reconstruction in which a coherent light source and a generator of a random speckle pattern projects onto the object a coherent random speckle pattern. An imaging unit detects the light response of the illuminated region and generates image data. Deformations of the pattern as projected on the object are compared with a reference image of the pattern in real time and from the above comparison a 3D map of the object is calculated leading eventually to a 3D image of the object.

Variations of the above principles are used as in the PCT International Publication WO 93 03579 which describes a three dimensional vision system in which one or two projectors establish structured light comprising two sets of parallel stripes having different periodicities and angles while the U.S. Pat. No. 6 751 344 describes a method for optically scanning a subject in which the subject is illuminated with a matrix of discrete two dimensional image objects such as a grid of dots. Other methods involve projection of a grating pattern as described for example in U.S. Pat. No. 4 802 759. Noticeable for all grid pattern based methods is that the precision of the reconstructed object suffers due to the discrete information extracted from the intersection points of the object and grid pattern.

Many computing applications such as electronic games learning multimedia office applications or the like or consumer electronic devices use various control methods either through computer commands or specific radiations for allowing users to manipulate game characters or other aspects of computer applications or to control consumer electronic devices. Typically such controls are applied to the above computer applications or electronic devices using for example remotes controllers keyboards mice joysticks or the like. Unfortunately such control devices do not map directly to the spirit of the actual game or other application actions or the spirit of naturally using electronic devices for which the controls are used. For example a game control that causes a game character to either drive a car fight in a fencing game swing a baseball bat may not correspond to an actual motion of driving the car fighting in a duel or swinging the baseball bat. A new paradigm for human computer interface is therefore necessary.

The following summary is made by way of example and not by way of limitation. Disclosed herein are apparatus systems and methods for building a 3D map of one or more objects which in a particular case can be humans who are present in the field of view FOV of an optical device whose images are projected onto an image sensing device which provides the electronic image to a reconfigurable architecture device which comprises an image capturing device and in some embodiments can also process the 3D map or the RGB image of one or more objects present in the FOV using image processing methods for example to detect and identify the user faces and then provides the 3D map together or no with the results of the image processing to an embedded processor which processes the image using image processing methods for the detection recognition and interpretation of the gesture made and produces a command corresponding to the position and movement of the object or objects in the FOV to an electronic device which has an interface to receive the commands. At the same time the novel apparatus methods devices and systems that are used in these embodiments make it possible to use them in combination with the devices which produce movement generated signals such as accelerometers gyroscopes magnetometers and alike as well as in combination with a voice capturing and recognitions device by applying data fusion methods specific to the methods used in this invention in order to produce more complex commands. The commands are further used for controlling consumer electronic devices such as TV or set top boxes sets PC and PC applications or gaming consoles or computer applications such as electronic games or alike.

There is therefore provided in accordance with an embodiment of the present invention apparatus methods and system for generating the 3D map of an object placed in the field of view of the image capturing device for processing the 3D map to detect identify and recognize the movements of the objects present in the FOV for mapping the detected movement onto a command which is sent by the device to another electronic device including 

a set of radiation generating devices. Alternately the controlled modulated illumination assembly may include lasers or light emitting diodes LEDs 

an electronic circuit to control the width of the period that the radiation generating devices are lit and its intensity 

a reconfigurable architecture hardware which controls the parameters of the image sensing device and also the radiation generating device using an algorithm based on a monotonic increasing law captures the image generated by the image sensing device stores the images in the information storage device and builds the 3D map of the object illuminated by the radiation generated by the radiation generating device by calculating consecutive sliced views of the object based on the reflected radiation for each of the radiation intensities and sends the 3D map of the object to the processor or in another embodiment the reconfigurable architecture hardware can include image processing methods applied to the 3D map obtained 

a processor connected to the reconfigurable architecture hardware device processor which processes the 3D image by applying various image processing algorithms on a three dimensional 3D map of the object or objects in order to analyze and identify the gestures produced by the user s and synthesize an appropriate command to control consumer electronic devices connected to the apparatus. The processor uses an information storage device in the process of applying various image processing algorithms for detecting and recognizing the movement present in the FOV.

a communication assembly where the communication with the external devices such as PCs gaming consoles TV sets set top boxes and the like is done via various standard communication ports such as Ethernet USB WiFi WiMax Bluetooth Infrared blaster and the like.

In a disclosed embodiment the reconfigurable architecture hardware is arranged to acquire images from the image capturing device store them in the information storage device and process a succession of images captured while the object is illuminated by the controlled modulated illuminating device in a spatial succession imposed by a specific distance sampling arrangement so as to create a 3D map of the object.

In a disclosed embodiment the processor is arranged together with its information storage device to process a succession of 3D images produced by the reconfigurable architecture hardware while the object is moving so as to map a 3D movement of the object wherein the object is a part of a human body and the 3D movement includes a gesture made by the part of the human body and wherein the processor is coupled to provide an input to a consumer electronic device such as a game console or a TV or a set top box or a mobile device or a computer application responsive to the gesture movement of the object.

There is also provided in accordance with an embodiment of the present invention apparatus and system for mapping an object including 

a controlled modulated device which produces in a controlled way a succession of radiation pulses whose duration increases as the object is sampled by various pulses of constant intensities along a frame but monotonically increasing with every frame up to the prescribed sampling density and 

a radiation source which is configured to illuminate the object according to the optical radiation prescribed by the controlled modulated device so as to project the depth sampling pulses onto the object 

an image sensing assembly which is configured to provide a series of images of the object or objects present in the FOV of the optical device and whose parameters are controlled by the reconfigurable architecture hardware using an algorithm which takes into account the distance to the object and the quality of the image obtained by capturing the reflected radiation from the object.

a reconfigurable architecture hardware which controls the parameters of the image sensing device and also the radiation generating device using hardware based functions generating a monotonic increasing law for the pulses produced by the radiation generating device in correspondence with the distance of the object captures the image generated by the image sensing device stores the images generated by the image capturing device in an information storage device and builds the 3D map of the object illuminated by the radiation generated by the radiation generating device by processing consecutive sliced views of the object based on the reflected radiation for each of the radiation intensities corrects the lens edge effects and by calculating the depth of each point in the above images generates a 3D map of the visible by the image sensing device of the apparatus and system part of the object and sends the resulting 3D map of the visible part of the object to the processor of the device.

In addition to the above embodiment of the device and system described above at point 0025 in another embodiment of the invention an embedded image processing device can be added to the device described in this patent and system above as the item of including 

a processor which is coupled with the reconfigurable architecture hardware and which processes the 3D image produced by the latter by applying various image processing algorithms on a three dimensional 3D map of the object in order to analyze identify and interpret the gestures produced by the user s and to synthesize an appropriate command to control consumer electronic devices such as game consoles. TV sets or set top boxes or mobile computer of phones or any computer application. The processor uses an information storage device in order to apply various image processing algorithms and move the RGB and depth map images inside the device and to send them out to monitors or other processing units such as processors PCs consoles or the like.

an information storage device connected to and used by the processor in order to store various images needed to produce the result of the gesture interpretation algorithm.

In one embodiment the radiation produced by the radiation generating device includes an algorithm based on a monotonic increasing law for the pulses produced by the radiation generating device which is reflecting the way the object is sampled in the z axis direction. The sampling rate can be either s 4 or s 8.

Typically the pulses are such as a duty cycle that is less than 1 e is accomplished. Alternatively or additionally the radiation pulses have a duty cycle that varies within frames however is constant during a frame.

In some embodiments the reconfigurable architecture hardware is configured to derive the 3D map by finding respective discrete depth maps of the object based on the reflected radiation by the object and by reconstructing based on the above a complete depth map of the object. In one embodiment the reflected radiation follows a duty cycle pattern as imposed by the reconfigurable architecture hardware that varies monotonically from a discrete radiation and frame to the other and the reconfigurable architecture hardware is configured such as to determine local gray levels of the discrete frame responsively to the radiation pulse duty cycle and to calculate the depth map in a discrete way based on the local gray levels of the respective captured frame by the image sensing device.

There is additionally provided in accordance with an embodiment of the present invention a method for building a 3D mapping of an object including 

a controlled modulated device containing an algorithm for modulating the radiation pulses following a discrete monotonic law concerning the width and duty cycle of the radiation devices such as illuminate the object with variable radiation intensities which are maintained constant along a frame which captures the reflected radiation by the object.

an image sensing device which captures the discrete image of the sampled object image along the z axis 

an information storage device connected to the reconfigurable architecture hardware which stores discrete images of the object as sent to the information storage device by the reconfigurable architecture hardware.

a reconfigurable architecture hardware which controls the emitting radiation device the image sensing device stores in the information storage device the discrete images representing the discrete image of the object as produced by a single train of radiation pulses modulated such that the pulses width is kept constant along a frame and eventually processes the captured images so as to reconstruct a three dimensional 3D map of the object and conveys the result in form of a 3D map to the processing device which can be any CPU endowed device such a PC or a laptop or even and embedded processor which applies specific image processing algorithms as described in this invention such that the gestures made by the user are properly identified and mapped into commands which can control either consumer electronic devices such as TV and set top boxes sets or computer applications.

There is additionally provided in accordance with an embodiment of the present invention a method and apparatus for building a command to control the external electronic devices such as game consoles TV sets set top boxes PCs mobile phones or alike or computer applications such as electronic games digital signage or alike based on processing the 3D mapping of an object including 

a processor or any CPU based board which is coupled with the reconfigurable architecture hardware via USB Wi Fi or even directly and which processes the 3D image produced by the latter so as to process by applying various image processing methods on a three dimensional 3D map of the object in order to detect analyze and identify the gestures produced by the user s and synthesize an appropriate command to control consumer electronic devices such as TV or set top boxes sets or any computer application. The processor CPU or alike uses an information storage device in order to apply various image processing methods.

an information storage device connected to and used by the processor in order to store various images needed to produce the result of the gesture interpretation algorithm.

There is further provided in accordance with an embodiment of the present invention a method for 3D mapping an object which is further used by the apparatus and system for the control of PCs or a gaming console including 

an illuminating control algorithm implemented onto the reconfigurable architecture hardware through which it controls the radiation generating devices such that they produce reflected radiation by the object with a specific width of the radiation pulses in correspondence with the specific discrete part of the object whose 3D image has to be reconstructed.

an image sensing device which produces an image produced by the reflected radiation off the object and which is produced by the radiation that is projected onto the object 

processing the captured image by the reconfigurable architecture hardware so as to reconstruct a three dimensional 3D map of the object.

an information storage device connected to and used by the processor in order to store various images needed to produce the result of the gesture interpretation algorithm.

a processor which is connected to the reconfigurable architecture hardware and which processes the 3D image produced by the latter so as to process by applying various image processing algorithms on a three dimensional 3D map of the object in order to analyze and identify the gestures produced by the user s and synthesize an appropriate command to control consumer electronic devices such as TV or set top boxes sets or any computer application. The processor uses an information storage device in order to apply various image processing algorithms.

An application programming interface is shipped out by the processor and provides the PC laptop mobile phone or any CPU of gaming consoles or other electronic devices the command used to control consumer electronic devices such as TV sets set top boxes laptops mobile phones or the CPU of gaming consoles robots and any other devices controlled by a CPU or Wi Fi IR blasters and alike or to any computer applications such as games digital signage learning and alike.

The present invention will be more fully understood from the following detailed description of the embodiments thereof taken together with the drawings.

In some embodiments an illumination device projects a controlled modulated radiation onto the object which can be in or hypostasis. In the context of the present patent application and in the claims the term controlled modulated radiation refers to a train of modulated pulses of a specific radiation whose frequency duty cycle and intensity are controlled in correlation with the parameters of the image sensing device . The radiation pulses are controlled in the sense that the control of the radiation function follows a monotonic increasing function in correlation with the electronic gain and timing parameters of the image sensing device . The illumination device parameters are calculated by the processor applying iteratively image processing methods on the 3D map produced by the reconfigurable architecture hardware until an optimal image is obtained. Another image sensing device provides an RGB image which is further used to provide images for the head and face identification method as presented in and .

In some embodiments a reconfigurable hardware architecture device of contained also on the schematic of as may be implemented physically by an FPGA or a SoC or an ASIC or alike placed on the Processing Board processes the images produced by the image sensing device building the depth map of object or following the process shown in . The reconfigurable hardware architecture is programmed in software such that it produces dynamically adjustable hardware depending on the application domain for the device and systems in accordance with the embodiment of the present invention. The term 3D map refers to a set of 3D coordinates representing the visible surface of the object or . The derivation of such a map based on image data is referred to herein as 3D mapping or equivalently 3D object reconstruction. . The 3D coordinates of the object or are of points on the surface of one of the objects above by applying the method represented in illustrating the process of defining a depth width around the object and its position in the FOV of the device and system. The depth is calculated taking a configurable number of images of the object either or produced by the reflected radiation of the surface of that object and retaining two images one which is the image of the closest points of the visible object surface and one which corresponds to the farther points of the visible surface of the object. These methods may be implemented mutatis mutandis using other two images produced by the image sensor of .

In one embodiment the functionality of the reconfigurable architecture hardware of is controlled by the state machine of whose state space contains two states the ILLUMINATOR STATE and the DEPTH IMAGE STATE . The ILLUMINATOR STATE controls the functionality of the Illuminator device of or A. The ILLUMINATOR state is triggered by the DEPTH IMAGE calculator by raising the ZREADY event to 1. When ZREADY takes the value zero the ILLUMINATOR is reset. For control purposes there is a RESET EVENT provided which also resets the ILLUMINATOR from a console or PC. Once triggered by ZREADY 1 the ILLUMINATOR triggers the radiation device to illuminate the scene with a frequency and duty cycle which are established by the processor and memorized in the appropriate registers of the reconfigurable architecture hardware of . The DEPTH IMAGE CALCULATION automaton programmed in hardware inside the reconfigurable architecture hardware of is presented in the flow diagram of and described hereafter.

In the same embodiment the ILLUMINATOR functionality is described by the automaton which part of the reconfigurable architecture hardware of and shown in more details in . According to the present embodiment the ILLUMINATOR is triggered by the RESET EVENT generated either by the DEPTH IMAGE CALCULATOR automaton of or by the user. The RESET EVENT brings the DEPTH IMAGE CALCULATOR into the INITIAL STATE of and after 10 time cycles into the WAIT FOR COMMAND state . As long as none of the following signals are true PARAMETERS UPDATE EVENT PARAMETERS COMMIT EVENT SWITCH ON OFF ILLUMINATION EVENT NEW NINE CYCLE FRAME BEGINS EVENT the ILLUMINATOR automaton remains in the same WAIT FOR COMMAND state. When the event PARAMETERS COMMIT EVENT AND 9 CYCLE FRAME BEGINS EVENT is true the ILLUMINATOR automaton enters in the COMMIT PARAMETER state . When all parameters are committed then the automaton passes automatically in the WAIT FOR COMMAND state. From the WAIT FOR COMMAND state the ILLUMINATOR automaton goes to the TOGGLE ILLUMINATION state as a result of the SWITCH ON OFF ILLUMINATION AND 9 CYCLE FRAME EVENT being both true. When the TOGGLE ILLUMINATION state is done the ILLUMINATOR automaton regains the WAIT FOR COMMAND state. Also from this state the ILLUMINATOR automaton allows the passage to the UPDATE PARAMETER state by the PARAMETERS UPDATE EVENT being true.

In one embodiment the reconfigurable hardware architecture implements the method shown in the flow diagram of . According to this flow diagram the image produced by the image acquisition device of is acquired by the reconfigurable architecture hardware in series of eight or four images as in by the sequence of instructions IMAGE ACQUISITION and for example for a number of times depending on the configuration of the device and system which is based on a configurable precision for depth measurement. Initially the reconfigurable architecture hardware acquires a background image which is used by the apparatus and system to process a BACKGROUND SUBTRACTION . and operation on all subsequent images produced by the image acquisition device of under different and monotonically increasing intensity of the radiation producing device of . Every image obtained by the background subtraction operation is then thresholded by the THRESHOLD operation and for example with a fixed threshold which is configurable then submitted to an XOR and operation for example applied on the thresholded image pixels and on the image produced in the previous iteration step. A mask calculated through the MASK operation and for example which are the image binary pixels obtained after the threshold operation is used to eliminate the pixels of the image obtained at the precedent step of the calculation of the depth image. After the mask step is completed the image obtained is a union of all previous mask operation output. This image is called a Primary i image and indexed with the natural number of the image acquisition step. At the end of the eight step the depth image is calculated from the Primary8 image and from the Primary5 or Primary4 renamed as the Secondary image. The depth image is obtained by the DEPTH CALCULATION FORMULA operation . The formula used for depth calculation is as follows 

The processor may comprise a special purpose and programmable processor which is programmed in software to carry out the functions described herein below. The software may be downloaded to processor in electronic form over a network for example using the network connections or of the board of or it may alternatively be provided on tangible media such as an electronic information storage device. Some or all of the processing functions of processor may be performed by suitable dedicated circuitry within the housing of the apparatus and system or otherwise associated with the apparatus and system.

The 3D map generated by reconfigurable architecture hardware may be used for a wide range of different purposes. In some embodiments the 3D map is sent to the processor which can either send it to a display device which shows a pseudo 3D image of the object or or may be further processed by the reconfigurable architecture hardware or the processor for controlling either external electronic devices as in or B or interactive computer applications such as a game or a game console or a digital signage application or a learning application by translating the gesture language of the user in commands for controlling all the above. Alternatively apparatus and system whose electronic schematic is shown in may be used to create 3D maps of objects of any types which can be seen in the FOV for immersing them in any application in which 3D coordinates are needed for interacting with the electronic world.

Light source of or in is controlled by the LED Control Circuit driven at its turn by the reconfigurable architecture hardware in order to produce and project optical radiation onto object . The terms light and optical radiation in the context of the present patent application refer to any band of optical radiation including infrared and ultraviolet. In some applications however near infrared light is preferred on account of the availability of suitable sources and detectors and the fact that the controlled modulated radiation is invisible to human viewers and can be contained in a proper way such that it is not harmful to human eyes or skin. In the configuration shown in and the elements of the light source are point sources meaning that the rays of radiation emitted by the light source emanate from a locus small enough so that a homogeneous field of radiation is generated and the radiation projected onto object or is uniform such that the reflected radiation is also as uniform as possible. For this purpose light source may comprise for example a coherent source with large angular divergence such as a laser diode. When such a radiation point source is used the optics of the apparatus comprising the lenses and radiation filters has to be chosen such that the object is all visible in the FOV.

An image sensing assembly produces a series of images of the object or with a variable speed which can be as high as 240 frames per second fps . Assembly comprises objective optics for the chosen radiation and which focus the image onto an image sensor . Typically sensors and comprises a rectilinear array of detector elements such as a CCD or CMOS based image sensor array. Assembly may also comprise a band pass filter calculated for the wavelength of the radiation chosen and positioned so that sensor receives only radiation in the emission band of radiation source . The band pass filter is calculated such that the other radiations produced by the ambient light or other radiation sources are filtered out.

In the embodiment shown in illumination assembly and image capture assembly and are held in a fixed spatial relation. This configuration and the processing techniques used by the reconfigurable architecture hardware of make it possible to perform 3D mapping using the single image capture assembly without relative movement between the illumination and image capture assemblies and without moving parts. Alternatively the techniques of illumination and mapping that are described herein below may be used in conjunction with other sorts of image capture assemblies in various different configurations such as those described in the Background section above.

The 3D map of object or is generated by the reconfigurable architecture hardware of by processing a number of pairs of images following the method illustrated in images which were collected by the sensor such that at the final stage of the method implemented in the firmware of the reconfigurable architecture hardware the 3D map of the visible surface of one of the objects or is obtained. The 3D map is obtained by calculating arithmetic sum and difference and then calculating the ratio of the result of the difference operation over the result of the sum operation applied to two intermediate images i.e. the brightest one called the primary image and a medium intensity one called the secondary image. In some embodiment the secondary image can be even the last image obtained by the reconfigurable architecture hardware as a result of processing the last pair of images.

As described herein a user may control an electronic device such as a TV set a set top box a PC game console a mobile phone a laptop or alike or a computer application such as a PC game a digital signage a medical information system a learning tool and alike executing on a computing environment such as a game console a computer or the like by performing one or more gestures which might be assembled in a gesture language or considered independently by the gesture recognition application. According to one embodiment the data representative of a gesture such as depth image of a scene may be received by for example an image acquisition device. In one embodiment the device making the object of the patent and the image capturing device of the system or the reconfigurable architecture hardware or the processor coupled to the image acquisition device may determine whether one or more targets or objects in the scene corresponds to a human target such as the user or if many the users of the device and the system. The object in the scene is then evaluated in order to decide whether or not that object is a human target by comparing the results of applying image processing procedures on the depth image in order to obtain a stable representation through a measure of that image such as the skeleton or alike with the skeleton pattern of the image of a human user which is stored in the information storage device of the processor. As will be described herein a user may control an application executing on a computing environment such as a game console a computer or the like by performing one or more gestures. According to one embodiment the data representative of a gesture such as one or a series of depth images of a scene may be received by the processor. In other embodiment the image can be received by a computing system coupled to the device and the system and the computer system may determine whether one or more targets or objects in the scene corresponds to a human target such as the user. To determine whether a target or object in the scene corresponds to a human target each of the targets may be thresholded and compared to a pattern of a human body model as described above. Each target or object that matches the human body model may then be processed at run time to dynamically generate a model in form of a skeleton or alike as described in and associated therewith the user or with the users. The user s or users model may then be tracked dynamically in real time and an avatar associated with the model is animated corresponding to the pose gesture or movements made by the user to control a game or to execute an action in any virtual reality environments or electronic devices such as TV sets set top boxes laptops mobile phones or devices or alike.

It will thus be appreciated that the embodiments described above are cited by way of example and that the present invention is not limited to what has been particularly shown and described hereinabove. Rather the scope of the present invention includes both combinations of the various features or components of the features described hereinabove as well as variations and modifications thereof which would occur to persons skilled in the art upon reading the foregoing description and which are not disclosed in the prior art.

