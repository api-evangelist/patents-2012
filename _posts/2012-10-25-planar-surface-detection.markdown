---

title: Planar surface detection
abstract: A planar surface within a physical environment is detected enabling presentation of a graphical user interface overlaying the planar surface. Detection of planar surfaces may be performed, in one example, by obtaining a collection of three-dimensional surface points of a physical environment imaged via an optical sensor subsystem. A plurality of polygon sets of points are sampled within the collection. Each polygon set of points includes three or more localized points of the collection that defines a polygon. Each polygon is classified into one or more groups of polygons having a shared planar characteristic with each other polygon of that group. One or more planar surfaces within the collection are identified such that each planar surface is at least partially defined by a group of polygons containing at least a threshold number of polygons.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09070194&OS=09070194&RS=09070194
owner: MICROSOFT TECHNOLOGY LICENSING, LLC
number: 09070194
owner_city: Redmond
owner_country: US
publication_date: 20121025
---
Televisions computer displays movie screens and other video monitors provide visual information to users. This visual information may take the form of a graphical user interface that enables a user to interact with a computing system. Conventional video monitors may be limited by a variety of different physical constraints including the physical size of the monitor and the physical locations at which the monitor may be positioned.

A planar surface within a physical environment is detected enabling presentation of a graphical user interface overlaying the planar surface. Detection of planar surfaces may be performed in one example by obtaining a collection of three dimensional surface points of a physical environment imaged via an optical sensor subsystem. A plurality of polygon sets of points are sampled within the collection. Each polygon set of points includes three or more localized points of the collection that defines a polygon. Each polygon is classified into one or more groups of polygons having a shared planar characteristic with each other polygon of that group. One or more planar surfaces within the collection are identified such that each planar surface is at least partially defined by a group of polygons containing at least a threshold number of polygons.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter. Furthermore the claimed subject matter is not limited to implementations that solve any or all disadvantages noted in any part of this disclosure.

Detecting and tracking physical objects within a physical environment using optical sensors such as depth infrared or visible light cameras enable a compelling user experience. A computing system is able to understand the physical environment through imaging detection and tracking of physical objects. In turn the computing system is enabled to transform any physical surface into a virtual touch screen interface that provides user interaction with virtual content while also enabling the user to experience real world tactile feedback from the physical surface.

In the field of computer vision the process of detecting and tracking physical objects may be computationally involved occupying substantial processing resources or time making it difficult to achieve acceptable performance in a real time use setting. The planar surface detection and tracking disclosed herein may achieve improved detection and tracking performance while consuming fewer processing resources. Using optical sensor data planar surfaces are initially detected within an observed scene the planar surfaces are refined for greater accuracy the boundaries of the planar surfaces are resolved and the planar surfaces are tracked them between frames of optical sensor data.

A GUI may be presented overlaying one or more of the planar surfaces. Foreground objects moving in front of the planar surfaces may be segmented and user input in the form of touch events are detected based on proximity to the planar surfaces. Detected user input may be used to control various aspects of a computing system.

A variety of techniques may be used to present the GUI including 1 projecting visible light representing the GUI onto the planar surface 2 displaying the GUI via a graphical display of a mobile computing device overlaying a camera view of the physical environment or 3 displaying the GUI via a see through graphical display of a head mounted display system. Before addressing the techniques for detecting planar surfaces within the physical environment an example head mounted display system for displaying a GUI overlaying a physical environment will be described.

Head mounted display system also includes a virtual reality engine . The virtual reality engine is configured to cause the see through graphical display to visually present one or more graphical elements. The one or more graphical elements may form a GUI. The GUI can simulate the appearance of a real world television computer display movie screen and or other monitor. To a user viewing the physical environment through the see through graphical display the GUI appears to be integrated with the physical environment. In this way the user is able to view a GUI that is not actually present in the physical environment. Virtual reality engine may include software hardware firmware or any combination thereof.

As one example a GUI may be playing a video stream of moving or static images. A video stream of moving images may be played at a relatively high frame rate so as to create the illusion of live action. A video stream of static images may present the same image on the virtual monitor for a relatively longer period of time. As a nonlimiting example a video stream of a photo slideshow may only change images every five seconds. It is to be understood that virtually any frame rate may be used without departing from the scope of this disclosure.

As another example a GUI may be opaque or partially transparent. An opaque GUI may be rendered so as to occlude real world objects that appear to be behind the GUI. A partially transparent GUI may be rendered so that real world objects or other graphical elements can be viewed through the GUI.

As another example a GUI may be frameless or framed. A frameless GUI may be rendered with an edge to edge viewable region that can play a video stream without any other structure rendered around the screen portion. In contrast a framed GUI may be rendered to include a frame around the viewable region. Such a frame may be rendered so as to resemble the appearance of a conventional television frame computer display frame movie screen frame or the like.

Both frameless and framed GUIs may be rendered without any depth. For example when viewed from an angle a depthless GUI will not appear to have any structure behind the surface of the viewable region. Furthermore both frameless and framed GUIs may be rendered with a depth such that when viewed from an angle the GUI will appear to occupy space behind the surface of the GUI.

As another example a GUI may include a rectangular or nonrectangular viewable region. Furthermore the viewable region may be planar or non planar. In some embodiments the viewable region of a GUI may be shaped and or sized to match the planar or non planar surface of a real world object in a physical environment or to match the planar or non planar surface of another graphical element.

Even when a planar viewable region of a GUI is rendered the video stream rendered on the planar viewable region may be configured to display three dimensional virtual objects e.g. to create the illusion of watching a three dimension television . Three dimensional virtual objects may be accomplished via simulated stereoscopic 3D content e.g. watching 3D content from a 3D recording so that content appears in 2D and on the plane of the display but the user s left and right eyes see slightly different views of the video producing a 3D effect. In some implementations playback of content may cause virtual 3D objects to actually leave the plane of the display. For example a movie where the menus actually pop out of the TV into the user s living room. Further a frameless GUI may be used to visually present three dimensional virtual objects from the video stream thus creating the illusion that the contents of the video stream are playing out in the physical environment.

As another example the GUI may be rendered in a stationary location relative to real world objects in the physical environment or the GUI may be rendered so as to move relative to real world objects. A stationary GUI may appear to be fixed to a wall table or other surface for example. A stationary GUI may also appear to be floating apart from any real world objects.

A moving GUI may appear to move in a constrained or unconstrained fashion. For example a GUI may be constrained to a physical wall but the GUI may move along the wall as a user walks by the wall. As another example a GUI may be constrained to a moving object. As yet another example a GUI may not be constrained to any physical objects and may appear to float directly in front of a user regardless of where the user looks.

In some embodiments the virtual reality engine may be configured to map a virtual coordinate system to the physical environment such that the GUI appears to be at a particular physical space location. Furthermore the virtual coordinate system may be a shared coordinate system useable by one or more other head mounted display systems. In such a case each separate head mounted display systems may recognize the same physical environment location where the GUI is to appear. Each head mounted display system may then render the GUI at that physical environment location so that two or more users viewing the physical environment location through different see through graphical displays will see the same GUI in the same place and with the same orientation. In other words the particular physical space location at which one head mounted display system renders a GUI will be the same physical space location that another head mounted display systems renders the GUI.

Turning back to head mounted display system may optionally include one or more audio speakers e.g. two audio speakers to enable stereo sound effects such as positional audio hints. In other embodiments the head mounted display system may be communicatively coupled to an off board or external audio speaker. In either case the virtual reality engine may be configured to cause these audio speakers to play an audio stream that is synchronized with a video stream presented at a GUI. For example while the GUI of plays a video stream in the form of a television program an audio speaker may play an audio stream that constitutes the audio component of the television program.

Referring again to head mounted display system includes a sensor subsystem . The sensor subsystem may include a variety of different sensors in different embodiments. As nonlimiting examples a sensor subsystem may include a microphone one or more forward facing away from user infrared and or visible light cameras one or more rearward facing towards user infrared and or visible light cameras . The forward facing camera s may include one or more depth cameras and or the rearward facing cameras may include one or more eye tracking cameras. In some embodiments an onboard sensor subsystem may communicate with one or more off board or external sensors that send observation information to the onboard sensor subsystem. For example a depth camera used by a gaming console may send depth maps and or modeled virtual skeletons to the sensor subsystem of the head mounted display.

The virtual reality engine may be configured to control one or more GUIs responsive to commands recognized via the sensor subsystem. As nonlimiting examples commands recognized via the sensor subsystem may be used to control GUI creation GUI positioning e.g. where and how large GUIs appear playback controls e.g. which content is visually presented fast forward rewind pause etc. volume of audio associated with a GUI privacy settings e.g. who is allowed to see similar instances of the GUI what such people are allowed to see screen capture sending printing and saving and or virtually any other aspect of a GUI.

As introduced above a sensor subsystem may include or be configured to communicate with one or more different types of sensors and each different type of sensor may be used to recognize commands for controlling a GUI. As nonlimiting examples the virtual reality engine may be configured to control the GUI responsive to audible commands recognized via a microphone hand gesture commands recognized via a camera and or eye gesture commands recognized via a camera.

The types of commands and the way that such commands control the GUIs may vary without departing from the scope of this disclosure. As one example the location and size of a GUI may be established or changed by recognizing a user tapping on a surface within the physical environment. For example a user may tap on a planar surface to indicate that the planar surface is the target planar surface for purposes of presenting a GUI and or receiving user input. As yet another example a user may speak the command new monitor and a GUI may be rendered on a surface towards which eye tracking cameras determine the user is looking.

Referring again to head mounted display system may include one or more features that allow the head mounted display to be worn on a user s head. In the illustrated example head mounted display system takes the form of eye glasses and includes a nose rest and ear rests and . In other embodiments a head mounted display may include a hat or helmet with an in front of the face see through visor. Furthermore while described in the context of a head mounted see through graphical display the concepts described herein may be applied to see through graphical displays that are not head mounted e.g. a windshield and to displays that are not see through e.g. an opaque display that renders real objects observed by a camera with virtual objects not within the camera s field of view .

Head mounted display system may also include a communication subsystem . Communication subsystem may be configured to communicate with one or more off board or external computing devices. As an example the communication subsystem may be configured to wirelessly receive a video stream audio stream coordinate information graphical element descriptions and or other information to render a GUI.

At the method includes obtaining a collection of three dimensional surface points of a physical environment via an optical sensor subsystem. The collection of three dimensional surface points may be obtained from one or more optical sensors of the optical sensor subsystem and may be represented as optical sensor information.

Optical sensor information from two or more optical sensors may be combined to obtain the collection of three dimensional points using any suitable technique. Non limiting examples include dense depth reconstructions from stereo triangulation of sparse appearance based stereo features three dimensional surface reconstructions integrated over time etc.

As one example an optical sensor subsystem may include a depth camera that images a physical environment to obtain a depth image that includes the collection of three dimensional surface points. As another example an optical sensor subsystem may include two or more depth cameras each imaging a physical environment to obtain respective depth images which may collectively include or otherwise define the collection of three dimensional surface points.

In some examples a collection of three dimensional surface points may be obtained from an optical sensor subsystem having one or more optical sensors that do not include a depth camera. In this case where the optical sensor information is not a depth image or depth map the local neighboring structure may be inferred from the optical sensor information for an additional computational cost.

A collection of three dimensional surface points may refer to a structured or unstructured collection. A structured collection may take the form of three dimensional surface points having an ordered two dimensional position within a plane of the depth image e.g. x y directions in coordinate space and a depth e.g. a z direction in coordinate space . Ordered surface point positions within a depth image may follow a grid structure polar structure or other structure. In the context of a depth image each three dimensional surface point may take the form of a depth pixel defining a three dimensional value in three dimensional physical space. An unstructured collection may include a surface point cloud of three dimensional surface points. Surface point density may be constant or may vary across a field of view of an optical sensor and may have a pre defined density or surface points may be randomly sampled to obtain a random density. At the method includes sampling a plurality of polygon sets of points within the collection. Each polygon set of points includes three or more localized points of the collection defining a polygon. Polygons defined by at least three points are used to identify the depth and or orientation of physical surfaces within the physical environment at least in part because three points represent the minimum quantity of points needed to define a plane within three dimensional space.

As a nonlimiting example each polygon set of points includes three localized points of the collection defining a triangle. However in other examples each polygon set of points may include four five six or more points forming other types of polygons. depicts a nonlimiting example of a plurality of polygon sets of points sampled from a collection.

In at least some implementations each point may correspond to an individual depth pixel within the collection. In this implementation each polygon set of points defines a polygon having depth pixels contained within the boundaries of that polygon. Sampled depth pixels and or polygon sets of points may be spatially partition for faster access using any suitable approach including use of binary space partitioning oct trees quad trees or grids to name just a few examples.

For each polygon set of points sampling performed at may include randomly sampling a depth value for an initial point of the collection and sampling two additional depth values for two additional points of the collection. In at least some implementations a minimum and or a maximum number of initial points for each polygon set of points may be sampled within defined regions of the collection of three dimensional surface points to provide a suitable density and uniformity of sampling over the entire collection. However in some examples the density of sampling may be greater in defined regions of the collection. For example a center portion of the collection of three dimensional surface points may be defined to have a greater density of sampled points than an edge region of the collection.

The two additional points beyond the initial point may each be located within a maximum proximity to the initial point and or each other. Use of a maximum proximity between two or more points of a polygon set of points may be used to ensure that the points are sufficiently localized. Additionally or alternatively the two additional points may each be located further than a minimum proximity from the initial point and or each other. Use of a minimum proximity between two or more points of a polygon set of points may be used to ensure that the points define a polygon spanning a sufficient area or region of the collection. In at least some implementations minimum and or maximum proximity may be defined by a human user or a human software developer in the form of a sampling resolution setting may be defined automatically by an artificial intelligence AI algorithm or may be defined programmatically by an operating system or an application program via an application programming interface API command.

At the method includes classifying each polygon into one or more groups of polygons. Each group of polygons may be defined as having a shared planar characteristic with each other polygon of that group and or a threshold proximity to one or more other polygons of that group.

For each group of polygons the shared planar characteristic of that group includes a depth and or an orientation of the polygons of that group defining a common plane within three dimensional space. As one example a polygon may be classified as being a member of a group of polygons if the depth and or orientation of that polygon is within a threshold proximity to the depth and or orientation of other polygons of that group. Such depths and or orientations may be analyzed using the combination of a raw Euclidian distance of 4D vector representation of a 3D plane 3D distance between planes when they are parallel and or an angle between normal vectors of respective planes. This threshold may be defined by a human user or a human software developer in the form of a sampling resolution setting may be defined automatically by an AI algorithm or other suitable algorithm or may be defined programmatically by an operating system or an application program via an application programming interface API command. depicts a histogram representing a quantity of polygon sets of points classified into specific groups based on the sampling of .

At the method includes identifying one or more planar surfaces within the collection of three dimensional surface points such that each planar surface is at least partially defined by a group of polygons containing at least a threshold number of polygons. depicts an example where groups of polygons containing at least the threshold number of polygons represented in have been filtered from the sampling of to enable identification of planar surfaces defined by the groups of polygons

At the method includes outputting an indication of the one or more planar surfaces. As one example the indication output at may include a planar surface definition and or a planar surface identifier for each planar surface identified at . As one example a planar surface definition may include a list or range of pixels of the collection that are contained within a planar surface. As another example a planar surface definition may include a position depth and or orientation of the planar surface within the collection. A planar surface identifier may include any suitable identifier for distinguishing two or more planar surfaces from each other.

In at least some implementations the method at may include associating a planar surface definition and or a planar surface identifier with each planar surface identified at . Planar surface definitions and or identifiers may be output to an application program operating system or other suitable functional entity via a programming interface such as for example an API. Planar surface definitions and or identifiers may be stored at a storage device where they may be later retrieved by an application program operating system or other suitable functional entity.

Within the example collection of three dimensional surface points of each polygon has been classified into a number of groups of polygons in which each group of polygons has a shared planar characteristic with each other polygon of that group. Example groups of polygons are depicted at and . Polygons of a particular group e.g. may have a different planar characteristic than polygons of other groups e.g. and indicating that the groups of polygons define portions of different planar surfaces within the collection.

Polygons may be classified into any suitable number of groups depending on the geometry of the physical environment that is imaged. Polygons of each group have been depicted in with a common surface pattern for the purpose of explanation. As may be observed from groups of polygons and indicated by their respective surface patterns each have more member polygons than group .

It will be appreciated that any suitable form of thresholding may be applied to filter polygons polygon groups or planar surfaces present within a collection of three dimensional surface points. The threshold may be varied on an individual collection basis to maintain a minimum and or a maximum number of planar surfaces within the collection. This threshold may be user defined or may be programmatically defined by an application program an algorithm e.g. AI algorithm operating system or other suitable functional entity.

At the method includes identifying one or more planar surfaces within a physical environment from a collection of three dimensional surface points. As one example previously described method of may be used to perform initial plane detection for one or more planar surfaces of the physical environment.

Once initial planar surface candidates have been identified the initial planar surfaces may be optionally refined. A variety of techniques may be used to refine the initial planar surfaces. Such refining may be implemented based on or responsive to the computational budget and or accuracy needs of a user application program operating system or other functional entity.

At the method includes refining the one or more planar surfaces through additional sampling of the collection of three dimensional surface points. The additional sampling of the collection may be primarily or entirely focused within the regions of the collection containing the one or more planar surfaces identified at . Additionally or alternatively plane closeness thresholds for classifying groups of polygons may be reduced to further reduce the quantity of planar surfaces and reduce polygon grouping membership. These refinement techniques may provide a more accurate definition of the planar surfaces present within the physical environment. However use of these refinement techniques may be limited to maintain computational budget detection accuracy and or to ensure that at least one planar surface remains after refinement.

As one example one or more planar surfaces may be refined within the collection by resampling a second plurality of polygon sets of points within that collection. Each polygon set of points of the second plurality of polygon sets of points may include three or more localized points of the collection defining a polygon with at least one point of that polygon being located within the one or more planar surfaces or within a threshold proximity to the one or more planar surfaces. Hence resampling of the collection may be limited to regions containing the one or more planar surfaces.

The resampled collection of three dimensional surface points may be processed using similar techniques applied at and as previously described with reference to method of . For example each polygon of the second plurality of polygon sets of points may be classified into the one or more groups of polygons identified from a previous sampling of the collection. Furthermore each planar surface within the collection may be further defined by a group of polygons having at least a threshold number of polygons from both the plurality of polygon sets of points and the second plurality of polygon sets of points. An indication of the one or more planar surfaces that satisfy the threshold number of polygons criteria after resampling may be output for use in the presentation of a GUI for example.

Refining of planar surfaces within a collection of three dimensional surface points may be performed by resampling the collection one two three or other suitable number of times to further improve planar surface resolution or understanding. Through such resampling the minimum and or maximum proximity of points resampled for a given polygon set of points may be increased or decreased relative to a previous sampling of the collection. For example the minimum and maximum proximity may be reduced during resampling to obtain a finer resolution of the collection in the region of the most prominent planar surfaces initially identified at .

In at least some implementations refining of planar surfaces within a collection of three dimensional surface points may be performed without resampling. For example proximity of initially sampled points or polygon groups of points to their associated planar surface may be reduced to exclude one or more of such points. As one example a covariance matrix of the set of points that are within a threshold to the initial plane estimation may be calculated. A singular value decomposition of the covariance matrix may be performed in which case the normal to the plane is given by the eigenvector corresponding to the lowest eigenvalue.

At the method includes applying edge detection to the one or more planar surfaces to resolve the boundaries of the one or more planar surfaces. Edge detection may be performed iteratively in some examples. Edge detection may be applied to all sampled points within a threshold of a planar surface.

As described by way of example with reference to edge detection may be performed by first getting a depth sample set that is within a threshold of a detected plane. Then one or more iterations of hole filling erosion and or dilation may be performed. In hole filling one or more holes within an outer boundary of a detected plane may be filled to more closely match surrounding neighboring pixels. In erosion the outer edge of the detected plane may be contracted so as to smooth the edge. In dilation the outer edge of the detected plane may be expanded so as to recover any plane that may have been lost during erosion. Next connected component analysis may be used to find candidate planes having at least a threshold size. Each of the candidates that satisfies this threshold is identified as a separate plane and the edge of that candidate is identified as the edge of the plane.

Returning to at the method includes tracking the one or more planar surfaces over a time series of three dimensional surface point collections. A planar surface may appear to be moving within a time series of collections due to relative movement of the optical sensor subsystem relative to the physical environment. For example the collection processed through application of method of or at and of may be one of a plurality of three dimensional surface point collections of the physical environment imaged via an optical sensor subsystem in a time series. Movement within the time series may include translation and or rotation of the planar surfaces in one or more directions within three dimensional space. Changes to the planar surface definitions including transformations e.g. rotation and or translation and or planar surface identities between time series collections may be tracked so that GUIs overlaying these planar surfaces are maintained in proper alignment and registration throughout the time series.

In the context of method the term target planar surface may be used to refer to a subset of planar surfaces within the physical environment that are the subject of tracking presentation of a GUI or detection of a user input through application of processes and or . It will be understood that a single target planar surface or multiple target planar surfaces may be identified from a given set of planar surfaces that have been detected within a physical environment. As one example a target planar surface may include the greatest number of polygons having the shared planar characteristic of the set of planar surfaces.

Identification of one or more target planar surfaces may be used to reduce computational load by reducing the number of planar surfaces that are actively tracked between frames of optical sensor information. In at least some implementations an indication of a target planar surface may be received from an application program or from an operating system via an API. An indication of a target planar surface may also be received from a user selection of the target planar surface from a set of planar surfaces detected within a physical environment. The indication of the target planar surface including a planar surface definition and or a planar surface identity may be stored in a storage subsystem where it may be retrieved to implement aspects of method .

A target planar surface that has moved through translation or rotation between a plurality of three dimensional surface point collections may be tracked to enable a reduction in sampling resolution processing time and or processing resources of one or more collections following a previously sampled collection. As one example tracking may be performed by identifying a distance between one or more points of the target planar surface in a first collection and a second collection. The one or more points in the first collection may be matched to the one or more points in the second collection if the distance between the one or more points in the first collection and second collection is less than a threshold distance.

Tracking may be achieved in some implementations by computing a match score for the target planar surface between two or more frames of optical sensor information. For example a match score may be computed as a bipartite matching for the planar surface between the two or more frames. A match score between processed three dimensional surface point collections for a target planar surface may be calculated by weighing and or combining one or more of the following 1 a distance between the center of the edges of the planar surface between two frames 2 an edge shape match as identified by an Iterative Closest Point ICP of the edges of the target planar surface between two frames 3 the average distance of one or more edge pixels of the target planar surface of a previous frame to the closest edge pixel of a subsequent frame 4 surface features present within infrared and or visible light images that are aligned in time and or space with the collections.

Depending on performance requirements and computational budget not all points may be matched between time series depth images. In some examples defined number of points within minimum and maximum bounds may be matched to limit or otherwise manage the computational load. Matching may be performed with more iterations of ICP in which an average edge distance score can be calculated between time series frames for a given set of points of a target planar surface. During this process a delta transformation value may be calculated between the planar surface from a previous frame and the planar surface of the subsequent frame of the optical sensor information for the given set of points.

Additionally or alternatively tracking may be performed by identifying surface features in the target planar surface from first and second visible light and or infrared images that are registered in time and space with the first and second collections respectively. For example an optical sensor subsystem may image the physical environment to obtain visible light and or infrared images while obtaining three dimensional surface point collections of the physical environment. Surface features in the target planar surface may enable or improve matching of points within a target planar surface between two or more visible light and or infrared images.

A planar surface that has been tracked across two or more time series three dimensional surface point collections may be associated with a common planar surface identifier to consistently represent that planar surface to other functional entities. For example a planar surface present within each of a first collection and a second collection or other subsequent collection may be associated with a common planar surface identifier if the surface features in the first and second visible light and or infrared images have a shared surface feature characteristic and or if a distance between one or more points in the first collection and second collection are less than a threshold distance.

At the method includes presenting a GUI overlaying a target planar surface of the one or more planar surfaces. In at least some implementations the GUI overlaying a target planar surface may be contained within the boundaries of the target planar surface. In at least some implementations the boundaries of the GUI may be sized and or shaped to substantially match the boundaries of the target planar surface. In this implementation the GUI may simulate a virtual tablet computer in which the target planar surface is capable of providing tactile feedback to the user such as provided by a traditional tablet computer.

As one example a GUI overlaying at least a portion of a target planar surface may be presented via a display subsystem. The display subsystem may take the form of or may include a see through graphical display of a head mounted display system or a graphical display of a mobile computing device that presents a camera view of the physical environment.

As another example a GUI overlaying at least a portion of the target planar surface may be presented by projecting visible light onto the target planar via a display subsystem. In this implementation the display subsystem includes a projector capable of projecting visible light representing the GUI onto the target planar surface within the physical environment.

At the method includes detecting a user input directed at the GUI overlaying the target planar surface. A user input directed at a graphical selector of the GUI may be detected from one or more collections of three dimensional surface points of the target planar surface. The user input may be detected based on at least partial occlusion of the target planar surface by an object e.g. a body portion of the user such as a finger hand or other suitable pointing device and or a depth of the object relative to the target planar surface within the one or more collections. One or more of these collections may be obtained in a time series subsequent to a previous collection that was used to detect a planar surface upon which the GUI was presented.

A user input may be defined by the occurrence of a touch event at the planar surface by an object. The touch event may be detected using a variety of techniques. As one example centroids of finger or hand body portions may be detected segmented from the background collection of three dimensional surface points and measured in relation to the depth of the planar surface.

As one example an object may be sampled within a collection to obtain a depth of the object relative to the target planar surface. An indication of a touch condition representing the user input may be output responsive to the depth of the object relative to a depth of the target planar surface being less than a threshold. However other suitable techniques may be used to determine whether a user input has been directed at the GUI.

In at least some embodiments the above described methods and processes may be tied to a computing system including one or more computers. In particular the methods and processes described herein may be implemented as a computer application computer service computer API computer library and or other computer program product.

Computing system includes a logic subsystem and a storage subsystem . Computing system may optionally include a display subsystem audio subsystem sensor subsystem communication subsystem and or other components not shown in . Computing system may also optionally include user input devices such as keyboards mice game controllers cameras microphones and or touch screens for example.

Logic subsystem may include one or more physical devices configured to execute one or more instructions. For example the logic subsystem may be configured to execute one or more instructions that are part of one or more applications services programs routines libraries objects components data structures or other logical constructs. Such instructions may be implemented to perform a task implement a data type transform the state of one or more devices or otherwise arrive at a desired result.

The logic subsystem may include one or more processors that are configured to execute software instructions. Additionally or alternatively the logic subsystem may include one or more hardware or firmware logic machines configured to execute hardware or firmware instructions. Processors of the logic subsystem may be single core or multicore and the programs executed thereon may be configured for parallel or distributed processing. The logic subsystem may optionally include individual components that are distributed throughout two or more devices which may be remotely located and or configured for coordinated processing. One or more aspects of the logic subsystem may be virtualized and executed by remotely accessible networked computing devices configured in a cloud computing configuration.

Storage subsystem includes one or more physical non transitory devices configured to hold data and or instructions executable by the logic subsystem to implement the herein described methods and processes. When such methods and processes are implemented the state of storage subsystem may be transformed e.g. to hold different data .

Storage subsystem may include removable media and or built in devices. Storage subsystem may include optical memory devices e.g. CD DVD HD DVD Blu Ray Disc etc. semiconductor memory devices e.g. RAM EPROM EEPROM etc. and or magnetic memory devices e.g. hard disk drive floppy disk drive tape drive MRAM etc. among others. Storage subsystem may include devices with one or more of the following characteristics volatile nonvolatile dynamic static read write read only random access sequential access location addressable file addressable and content addressable. In some embodiments logic subsystem and storage subsystem may be integrated into one or more common devices such as an application specific integrated circuit or a system on a chip.

It is to be appreciated that storage subsystem includes one or more physical non transitory devices. In contrast in some embodiments aspects of the instructions described herein may be propagated in a transitory fashion by a pure signal e.g. an electromagnetic signal an optical signal etc. that is not held by a physical device for at least a finite duration. Furthermore data and or other forms of information pertaining to the present disclosure may be propagated by a pure signal.

The terms module program and engine may be used to describe an aspect of computing system that is implemented to perform one or more particular functions. In some cases such a module program or engine may be instantiated via logic subsystem executing instructions held by storage subsystem . It is to be understood that different modules programs and or engines may be instantiated from the same application service code block object library routine API function etc. Likewise the same module program and or engine may be instantiated by different applications services code blocks objects routines APIs functions etc. The terms module program and engine are meant to encompass individual or groups of executable files data files libraries drivers scripts database records etc.

It is to be appreciated that a service as used herein may be an application program executable across multiple user sessions and available to one or more system components programs and or other services. In some implementations a service may run on a server responsive to a request from a client.

When included display subsystem may be used to present a visual representation of data held by storage subsystem . As the herein described methods and processes change the data held by the data holding subsystem and thus transform the state of the data holding subsystem the state of display subsystem may likewise be transformed to visually represent changes in the underlying data. Display subsystem may include one or more display devices utilizing virtually any type of technology. Such display devices may be combined with logic subsystem and or storage subsystem in a shared enclosure or such display devices may be peripheral display devices.

As one example display subsystem may include or take the form of a projector that is configured to project visible light onto a physical surface. As another example display subsystem may include or take the form of a graphical display such as a see through graphical display of a head mounted display system or a graphical display of a mobile computing device. It will be understood that display subsystem may take other forms beyond these examples.

Sensor subsystem may include one or more optical sensor subsystems including one or depth cameras infrared and or visible light cameras. Sensor subsystem may include other suitable sensors including microphones inertial sensors etc. In some embodiments sensor subsystem may communicate with one or more external sensors that send observation information to the sensor subsystem. External sensors may include the previously described optical sensor subsystem for example.

When included communication subsystem may be configured to communicatively couple computing system with one or more other computing devices. Communication subsystem may include wired and or wireless communication devices compatible with one or more different communication protocols. As nonlimiting examples the communication subsystem may be configured for communication via a wireless telephone network a wireless local area network a wired local area network a wireless wide area network a wired wide area network etc. In some embodiments the communication subsystem may allow computing system to send and or receive messages to and or from other devices via a network such as the Internet.

It is to be understood that the configurations and or approaches described herein are exemplary in nature and that these specific embodiments or examples are not to be considered in a limiting sense because numerous variations are possible. The specific routines or methods described herein may represent one or more of any number of processing strategies. As such various acts illustrated may be performed in the sequence illustrated in other sequences in parallel or in some cases omitted. Likewise the order of the above described processes may be changed.

The subject matter of the present disclosure includes all novel and nonobvious combinations and subcombinations of the various processes systems and configurations and other features functions acts and or properties disclosed herein as well as any and all equivalents thereof.

