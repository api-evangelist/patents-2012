---

title: Inertial sensor aided instant autofocus
abstract: The disclosure is directed to creating an inertial sensor aided depth map of a scene. An embodiment of the disclosure captures at least a first image and a second image during movement of a device caused by a user while framing or recording the scene, compensates for rotation between the first image and the second image, calculates an amount of translation of the device between the first image and the second image, calculates a pixel shift of a plurality of key points of the first image and the second image, and estimates a depth to one or more of the plurality of key points of the first image and the second image.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09025859&OS=09025859&RS=09025859
owner: Qualcomm Incorporated
number: 09025859
owner_city: San Diego
owner_country: US
publication_date: 20120730
---
The disclosure relates to image processing and more particularly to creating an inertial sensor aided depth map of a scene.

When taking a picture or recording a video cameras need to determine the focal distance that renders the subject of the image in sharpest focus. Current autofocus solutions require a search of different focal depths and measuring contrast or performing phase detection using dedicated sensors. Another solution known as dual camera depth estimation performs depth estimation using two cameras. Another conventional solution is focus stacking which requires a camera to take a number of pictures at different focal depths and combine or stack them into one picture.

Many camera users also have a desire to form three dimensional views of a given scene. While this can be done using stereoscopic cameras it is difficult to do so with a single camera. While a stereoscopic camera can be approximated using a single camera by instructing the user to move the camera and take two photos at two horizontally displaced locations this requires user intervention.

The disclosure is directed to creating an inertial sensor aided depth map of a scene. An embodiment of the disclosure captures at least a first image and a second image during movement of a device caused by a user while framing or recording the scene compensates for rotation between the first image and the second image calculates an amount of translation of the device between the first image and the second image calculates a pixel shift of a plurality of key points of the first image and the second image and estimates a depth to one or more of the plurality of key points of the first image and the second image.

Aspects of the invention are disclosed in the following description and related drawings directed to specific embodiments of the invention. Alternate embodiments may be devised without departing from the scope of the invention. Additionally well known elements of the various embodiments of invention will not be described in detail or will be omitted so as not to obscure the relevant details of the various embodiments invention.

The word exemplary is used herein to mean serving as an example instance or illustration. Any embodiment described herein as exemplary is not necessarily to be construed as preferred or advantageous over other embodiments. Likewise the term embodiments of the invention does not require that all embodiments of the invention include the discussed feature advantage or mode of operation.

The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of embodiments of the invention. As used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will be further understood that the terms comprises comprising includes and or including when used herein specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

Further many embodiments are described in terms of sequences of actions to be performed by for example elements of a computing device. It will be recognized that various actions described herein can be performed by specific circuits e.g. application specific integrated circuits ASICs by program instructions being executed by one or more processors or by a combination of both. Additionally these sequence of actions described herein can be considered to be embodied entirely within any form of computer readable storage medium having stored therein a corresponding set of computer instructions that upon execution would cause an associated processor to perform the functionality described herein. Thus the various aspects of the invention may be embodied in a number of different forms all of which have been contemplated to be within the scope of the claimed subject matter. In addition for each of the embodiments described herein the corresponding form of any such embodiments may be described herein as for example logic configured to perform the described action.

When taking a picture or recording a video a camera may attempt to automatically determine the focal distance that renders the subject of the image in sharpest detail. This is called autofocus. Current autofocus solutions require searching different focal depths and measuring contrast or performing phase detection. Alternately different sensing modalities such as infrared and ultrasound are used to estimate the distance of the subject from the camera. Another solution known as dual camera depth estimation performs depth estimation using two cameras. Another conventional solution is focus stacking which requires a camera to take a number of pictures at different focal depths and combine or stack them into one picture.

Many camera users also have a desire to form three dimensional 3D views of a given scene. While this can be done using stereoscopic cameras it is difficult to do so with a single camera. A stereoscopic camera is a camera either still or video which has two or more lenses each with a separate image sensor. While a stereoscopic camera can be approximated using a single camera and instructing the user to move the camera and take two photos at two horizontally displaced locations this requires user intervention. It would be preferable to form a 3D representation of a scene in the absence of such user involvement.

Often a user moves the camera to a small extent while framing a photo or taking a video. Embodiments of the invention take advantage of these unintentional or random motions to form a 3D representation of a scene or perform autofocus. During this movement the camera captures at least two images of the scene a.k.a. frames before the user snaps the picture or while the user records the scene. The movement may consist of both rotation and translation. By cancelling out the effects of rotation multiple frames separated only by camera translation can be obtained providing an approximation of a stereoscopic camera. This permits the camera to quickly autofocus on the area of interest in a given scene and or create a 3D depth map of the scene that can be used to form a 3D image of the scene.

Integrated inertial micro electro mechanical systems MEMS sensors such as accelerometers and gyroscopes have recently made their way into low cost mobile devices such as consumer cameras and cellular phones with camera capability. Gyroscopes measure device rotation along all three axes by measuring the angular velocity of the device along those axes. Assuming that a user starts moving the camera after the first frame is captured the total rotation between the first frame and the last frame can be computed by integrating the rotation matrix derived from the angular velocity measurements. The last frame can then be transformed using a projective transform derived from this rotation matrix to cancel out the effects of the rotation and closely match the first frame.

The user movement will likely also consist of some translation. The camera translation between the first and last frames can be computed from the image itself or by integrating the accelerometer data to arrive at the linear separation between the two images. Moving subjects in the frame can be identified by looking for local motion that does not match the motion predicted by the inertial sensors. Slowly moving subjects can be considered stationary if they do not move significantly for the duration of the camera motion. Once the effect of rotation has been cancelled out the remaining difference in stationary parts of the scene between the first and last frames is due to translation. This translation causes the image of objects near the camera to move more than objects far away from the camera. This fact can be used to infer the relative or absolute distance of objects from the camera and form a depth map of the scene. The same technique can also be used to estimate the size of objects in the scene.

Once the depth map is formed it can be used to create a 3D model of the scene and create a 3D photo or video. It can also be used to aid in continuous autofocus by providing the depth of the object of interest. This allows the autofocus algorithm to directly focus at the depth of the object of interest as opposed to searching different depths as is currently done.

This inertial sensor aided autofocus can be used in both camera and video applications. If the user pans the device before taking a photo the depth of the scene can be estimated using the method described above and the lens can be moved directly to the desired focus position at the end of the pan instead of searching for an optimal focus point at the end of the pan. In the case of a video the lens position at the end of the pan would be gradually changed to ensure that no abrupt change is seen in the video.

UE has a platform that can receive and execute software applications data and or commands transmitted from a radio access network RAN that may ultimately come from a core network the Internet and or other remote servers and networks. The platform can include a transceiver operably coupled to an application specific integrated circuit ASIC or other processor microprocessor logic circuit or other data processing device. The ASIC or other processor executes the application programming interface API layer that interfaces with any resident programs in the memory of UE . The memory can be comprised of read only memory ROM or random access memory RAM electrically erasable programmable ROM EEPROM flash cards or any memory common to computer platforms. The platform also can include a local database that can hold applications not actively used in memory . The local database is typically a flash memory cell but can be any secondary storage device as known in the art such as magnetic media EEPROM optical media tape soft or hard disk or the like. The internal platform components can also be operably coupled to external devices such as antenna display and keypad among other components as is known in the art.

UE also includes an image sensor which may be one or more charge coupled devices CCDs a complementary metal oxide semiconductor CMOS or any other image sensor. Image sensor is operatively coupled to camera lens . Lens is illustrated as facing the user but it is apparent that it could be positioned on any face of UE .

Accordingly an embodiment of the invention can include a UE including the ability to perform the functions described herein. As will be appreciated by those skilled in the art the various logic elements can be embodied in discrete elements software modules executed on a processor or any combination of software and hardware to achieve the functionality disclosed herein. For example ASIC memory API and local database may all be used cooperatively to load store and execute the various functions disclosed herein and thus the logic to perform these functions may be distributed over various elements. Alternatively the functionality could be incorporated into one discrete component. Therefore the features of the UE in are to be considered merely illustrative and the invention is not limited to the illustrated features or arrangement.

At UE uses the natural movements of the user while framing the photo or recording the video to estimate the distance to various objects in the scene. At UE captures at least two frames of the scene while the user is framing the photo or recording the video. The time difference between the first and second frames is chosen based on the speed at which the user is moving the camera. For example for slow movements a longer time is chosen to lead to a sufficiently large translation. A particular value of camera translation is required to resolve object depths at a given distance as discussed below with reference to and .

While illustrates and as occurring consecutively it is apparent that they may occur simultaneously or in reverse order. That is while UE is capturing multiple frames the key points tracker may be identifying and tracking key points within those frames. Alternatively after UE captures the at least two frames the key points tracker may then identify and track the key points within those frames.

At UE uses its internal gyroscope to measure the rotation of the camera in three dimensions during the time interval between the two frames. If UE is not equipped with a gyroscope it can estimate the rotation between two frames using the key points extracted from the first and second frame alone. To do this UE estimates the best scaling factor and roll yaw pitch angle that minimizes the difference between the images. That is given a set of key points x y x y . . . xn yn for the first frame and a set of key points x y x y . . . xn yn for the second frame UE performs scaling and 3D projection to the key points on the second frame so that the sum of the absolute difference of yi yi for i 1 to n can be minimized. Compensating for the rotation of UE between the first and second frames leaves only pure translation between the two frames. illustrates an example of how UE may be rotated and translated between two frames while taking a picture of scene .

Objects at different distances from the camera move to different extents due to camera translation. This disparity in the movement of different key points is used to estimate object depth. illustrates an example of the movement between objects at different depths from frame to frame . Specifically key point in the foreground shifts noticeably while key point in the background moves a smaller distance.

Referring to at UE determines the amount of translation between the first and second frames. A sufficiently large value of camera translation is required to resolve object depths at a given distance. Translation can be obtained by integrating the accelerometer data twice to get position. This is inherently a very noisy process and requires excellent accelerometer and gyroscope calibration. Embodiments of the invention may continuously calibrate the accelerometer and gyroscope by comparing consecutive frames to detect when UE is stationary and using those instants to calibrate both sensors.

Referring to at UE estimates the depth of the key points identified at . Often the subject of a photograph or video is a person so it is desirable to detect a face and precisely estimate its depth. Embodiments of the invention use face detection algorithms to detect one or more faces and track the features that belong to this face to estimate its depth. Other techniques can be used to infer the principal subject of the frame if no faces are present. For example a user interface can be used whereby the user chooses the subject of interest using a touch screen.

Assuming a camera focal length of 3.546 mm and a pixel pitch of 0.0014 mm and assuming that UE can detect a one pixel motion of a key point one pixel shift on the sensor plane translates to an angular shift of del theta pixel pitch focal length 3.94 10radian. For an object at depth d the camera shift that causes a shift of one pixel is d del theta. This suggests that the depth of objects meters away from the camera can be resolved using mm scale camera translation.

Objects that move during depth estimation need to be identified since they might lead to spurious depth estimates. This can be accommodated by calculating the movement of the different features or key points in the scene compared to a global motion vector. Features that belong to moving objects are not included in the depth estimation algorithm.

At once the depth of the subject is estimated UE automatically focuses on the accurate depth or performs a search in a narrow window around the optimum depth. This provides a significant speedup in autofocus when the user takes a photograph or video. Alternatively once this depth map is formed it can be used to create a 3D model of the scene and create a 3D photo or video.

Further the orientation of UE with respect to the vertical can be obtained using the accelerometer. This orientation helps to estimate the lens movement driving current since pointing up takes a larger current to move the lens to the desired position due to gravity. Less current is needed when pointing down.

Those of skill in the art will appreciate that information and signals may be represented using any of a variety of different technologies and techniques. For example data instructions commands information signals bits symbols and chips that may be referenced throughout the above description may be represented by voltages currents electromagnetic waves magnetic fields or particles optical fields or particles or any combination thereof.

Further those of skill in the art will appreciate that the various illustrative logical blocks modules circuits and algorithm steps described in connection with the embodiments disclosed herein may be implemented as electronic hardware computer software or combinations of both. To clearly illustrate this interchangeability of hardware and software various illustrative components blocks modules circuits and steps have been described above generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans may implement the described functionality in varying ways for each particular application but such implementation decisions should not be interpreted as causing a departure from the scope of the various embodiments of the invention.

The methods sequences and or algorithms described in connection with the embodiments disclosed herein may be embodied directly in hardware in a software module executed by a processor or in a combination of the two. A software module may reside in RAM memory flash memory ROM memory EPROM memory EEPROM memory registers hard disk a removable disk a CD ROM or any other form of storage medium known in the art. An exemplary storage medium is coupled to the processor such that the processor can read information from and write information to the storage medium. In the alternative the storage medium may be integral to the processor.

Accordingly an embodiment of the invention can include a computer readable media embodying a method for creating an inertial sensor aided depth map of a given scene. Accordingly the invention is not limited to illustrated examples and any means for performing the functionality described herein are included in embodiments of the invention.

While the foregoing disclosure shows illustrative embodiments of the invention it should be noted that various changes and modifications could be made herein without departing from the scope of the invention as defined by the appended claims. The functions steps and or actions of the method claims in accordance with the embodiments of the invention described herein need not be performed in any particular order. Furthermore although elements of the invention may be described or claimed in the singular the plural is contemplated unless limitation to the singular is explicitly stated.

