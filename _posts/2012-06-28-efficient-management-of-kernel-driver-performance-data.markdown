---

title: Efficient management of kernel driver performance data
abstract: An improved technique for managing access to performance data of a kernel driver includes acquiring performance data in the kernel driver over identified intervals of time marked by the kernel driver itself and transferring the performance data to a persistence manager outside the kernel, where client applications can access the performance data and/or post-processed versions thereof without disturbing the operation of the kernel driver.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09003432&OS=09003432&RS=09003432
owner: EMC Corporation
number: 09003432
owner_city: Hopkinton
owner_country: US
publication_date: 20120628
---
Host computers in data storage systems access storage arrays such as disk arrays at a high rate of speed and with high reliability. To meet the demanding requirements of such systems host computers are typically equipped with kernel drivers for managing critical I O activities. The kernel drivers run within the kernels of their host computers operating systems and efficiently direct data READs and WRITEs to their respective storage arrays. Often host computers are connected to their arrays through multiple paths and the kernel drivers are specially configured to select paths dynamically for conveying I O requests to the arrays so as to provide load balancing and failover. An example of a kernel driver that supports multiple paths is the PowerPath driver from EMC Corporation of Hopkinton Mass.

In a typical scenario a host computer runs an application on behalf of a user. The application executes commands including instructions for reading from and writing to an array. The kernel driver receives these instructions in the form of I O requests which the kernel driver processes and directs to the array.

Some kernel drivers for managing I O requests collect performance data. In one scenario a kernel driver collects performance data and various clients running on the host computer outside the kernel access the kernel driver to extract the performance data and perform calculations.

The kernel driver for handling I O requests e.g. from an application or other program running on the host computer is typically positioned directly in the critical I O stack of the host computer i.e. between the application program and adapters connected to the storage array. Any demands on the kernel driver to perform tasks other than to process I O requests thus have the potential to burden the kernel driver and impair throughput of the host computer in reading from or writing to the array.

Unfortunately the actions of clients running on the host computer or on other computers can impair the kernel driver s performance when such clients call upon the kernel driver to access performance data. The kernel driver upon receiving a request for performance data typically freezes all I O activity when it services the request. It has been observed that repeatedly freezing I O activity servicing requests and unfreezing I O activity can impair the efficiency of the kernel driver and impact throughput. Allowing multiple clients to access the kernel driver multiplies the negative impact on performance.

In contrast with the conventional approach where clients frequently access a kernel driver to obtain performance data and thereby impair performance an improved technique for managing access to performance data of a kernel driver includes acquiring performance data in the kernel driver over identified intervals of time marked by the kernel driver itself and transferring the performance data to a persistence manager outside the kernel where client applications can access the performance data and or post processed versions thereof without disturbing the operation of the kernel driver. Communications with the kernel driver and burdensome interruptions to I O processing that follow are thereby reduced e.g. to once per sampling interval allowing the kernel driver to perform its primary functions more efficiently. Additionally collecting performance data in this manner promotes consistency among different clients as all such clients operate based on a common set of performance data collected and processed once per interval for all such clients.

Some embodiments are directed to a method of managing access to performance data of a driver running in an operating system kernel. The method includes recording driver performance data within the kernel over a predetermined interval of time marked by a timer running within the kernel. The method further includes after expiration of the timer performing a collecting operation within the kernel to collect the recorded performance data and transferring the collected performance data to a persistence manager outside the kernel.

Other embodiments are directed to computerized apparatus and computer program products. Some embodiments involve activity that is performed at a single location while other embodiments involve activity that is distributed over a computerized environment e.g. over a network .

Embodiments of the invention will now be described. It is understood that such embodiments are provided by way of example to illustrate various features and principles of the invention and that the invention hereof is broader than the specific example embodiments disclosed.

An improved technique for managing access to performance data of a kernel driver includes acquiring performance data in the kernel driver over identified intervals of time marked by the kernel driver itself and transferring the performance data to a persistence manager outside the kernel where client applications can access the performance data and or post processed versions thereof without disturbing the operation of the kernel driver Using this technique client applications can access a consistent set of performance data collected and processed once per interval for all such client applications.

The host computing machine i.e. the host includes a user interface such as a keyboard pointer and display for interacting with a user and a network interface such as one or more network interface cards for connecting to a computer network. The host also includes a set of processors i.e. one or more processing chips and or assemblies memory and a set of host bus adapters . The set of processors and the memory together form a specialized circuit constructed and arranged to perform various functions as described herein.

The memory stores code for applications an API application programming interface and an operating system kernel . The applications include an active client a passive client and a user application for example. The API provides access to a userspace persistence manager and the operating system kernel includes a multipathing driver . The multipathing driver is thus a kernel driver which may be provided as a kernel extension of the host s operating system. In some examples the multipathing driver is implemented as an I O filter driver. The multipathing driver includes a kernel sampling agent . It is understood that the memory may include a myriad of other constructs however such constructs are omitted from for the sake of simplicity.

In some examples the host bus adapters are SCSI initiators and the array includes SCSI targets . SCSI Initiator Targets I T s are provided within the medium to connect the host bus adapters to respective SCSI targets . The I T s are also referred to herein as buses. 

Also shown in are a number of SCSI I T L s Initiator Target LUNs which are referred to herein as paths . In this example a total of nine paths are shown. Each of the paths extends from one of the host bus adapters through one of the respective buses to a respective one of the targets and to a respective one of the LUNs . Providing different paths to each respective LUN of the array affords the electronic environment with redundancy load balancing and failover.

Although the host is typically a server level computer this is not required. The host may be any type of computer or computing device. Also while the applications are typically of a type that are accessible to users via a graphical user interface GUI or command line interface CLI it is understood that the applications or a subset of the applications can be run independently of user interaction or in the background. For example the applications can be run as daemons processes or other software constructs. In some examples the active client and or the passive client are run off host i.e. on computing machines other than the host which are connected to the host via a computer network and the network interface . A variety of operating systems can be run on the host with non limiting examples including Microsoft Windows Unix Red Hat Linux or vSphere from VMware for example.

In operation the host executes the user application . The user application typically includes high level instructions for writing to and or reading from the array . The host translates these high level instructions into I O requests which are passed along an I O stack of the host to the multipathing driver . The multipathing driver processes the I O requests and forwards the I O requests along the I O stack e.g. to HBAs . The I O requests are then sent via designated paths to designated LUNs on the array . Responses to I O requests such as data for READs and acknowledgements for WRITEs are sent back from the array to the multipathing driver .

As the multipathing driver processes I O requests the multipathing driver may collect performance data such as numbers of I O requests received I O sizes of READs and WRITEs and latencies of READs and WRITEs e.g. the amount of time required for an I O request to propagate from the multipathing driver to the array and back . In some examples the multipathing driver stores certain types of performance data in the form of data bins or buckets. For instance rather than storing values of I O size as individual numbers which can consume valuable kernel space data buckets are provided wherein each bucket covers a particular range of values of I O size. The ranges are contiguous and may be arranged linearly or logarithmically for example. When an I O request is processed a counter is incremented for the bucket that covers the range of I O sizes that includes that of the received I O request. Data buckets can also be provided for I O request latency. Different sets of buckets can be provided for READs and WRITEs as well as for different paths and or for different LUNs . Considerable space is conserved as each value of I O size and or latency can be recorded simply as a count in a data bucket. A technique of this kind for collecting performance metrics using data buckets is described in U.S. patent application Ser. No. 13 341 348 filed Dec. 30 2011 and entitled AQUISITION AND KERNEL MEMORY STORAGE OF I O METRICS which is hereby incorporated by reference in its entirety.

As the user application runs and the multipathing driver continues to acquire performance data the kernel sampling agent performs collecting operations of the accumulated performance data according to a predetermined schedule. For example the kernel sampling agent takes periodic snapshots of the bucket counters at regular intervals and stores the snapshots in the kernel . In some examples multiple snapshots of data buckets as well as other performance data are stored in the multipathing driver or elsewhere in the kernel in connection with different time intervals.

Preferably the active client obtains snapshots of the performance data from the multipathing driver e.g. in an event driven manner whereas the passive client obtains performance data from the API . Once snapshots are obtained the active client post processes the snapshots and stores the results in the userspace persistence manager where they are available for access by one or more passive clients . Clients both on and off the host are thus able to access driver performance data from the userspace persistence manager . As the userspace persistence manager resides outside the driver the userspace persistence manager can be accessed frequently by numerous clients without burdening the multipathing driver or causing the multipathing driver to be diverted from its primary function of processing I O requests.

In the kernel the multipathing driver is seen to include I O monitors and a performance data manager and a performance data structure including data buckets . The multipathing driver is also seen to include the kernel sampling agent a kernel persistence manager options storage a command event processor and a timer . Although the performance data manager the performance data structure the kernel sampling agent the kernel persistence manager the options storage the command event processor and a timer are shown as being included within the multipathing driver it is understood that some or all of these constructs may be located elsewhere in the kernel but run in coordination with the multipathing driver .

Constructs in userspace include the active client one or more passive clients the user application and the API . The API provides access to a performance monitor a post processor a wait for event handler a retrieving handler and the userspace persistence manager . The API is arranged to allow communication between the active client and the multipathing driver for setting up performance monitoring obtaining performance data from the multipathing driver post processing the obtained performance data and making the post processed performance data available for consumption by the passive client s .

I O monitors and measure performance data pertaining to outgoing I O requests and returning I O requests . I O requests and are respectively shown as originating from and being provided to the application program . It is understood that additional processing layers may be placed between the application program and the driver however such layers are omitted here for the sake of simplicity.

In operation the performance data manager obtains the measured performance data and stores the data in the performance data structure which includes the above described data buckets . The kernel sampling agent captures performance data from the performance data structure at intervals marked by the timer . The timer is configured to expire or wake up at predetermined intervals. Each time the timer wakes up the kernel sampling agent obtains another snapshot of the performance data structure or a portion thereof . Snapshots of sampled data from the performance data structure are stored in the kernel persistence manager .

Preferably each snapshot stored in the kernel persistence manager is associated with an index which is incremented each time a new snapshot is stored. Only a limited number of snapshots may be stored beyond which oldest snapshots are typically discarded. The options storage stores a value that designates the maximum number of snapshots that may be stored in the kernel persistence manager . The options storage may also store the interval of the timer . The maximum number of snapshots and the sampling interval can preferably be adjusted by an administrative management program not shown .

The command event processor is the primary means by which communication takes place between the multipathing driver and software constructs in userspace . In some examples the command event processor communicates with the API using IOCTL I O Control statements. The API translates instructions from the active client into IOCTL statements and returns results of IOCTL statements back to the active client . The command event processor also raises events to indicate when new snapshots of collected performance data are available.

The API provides a means through which the applications can control the multipathing driver and manage performance data obtained from the multipathing driver . The API includes pointers to software libraries in userspace for performing various functions. Examples of constructs for performing these functions include the following 

Although the API is illustrated as a single construct those skilled in the art will realize that the API may include multiple APIs such as one for communicating with applications and one for communicating with the multipathing driver for example.

In some examples a single active client and multiple passive clients are provided. The active client and passive client s may be installed on the host or off the host . Some clients may be on the host while others may be off the host . Similarly the API or portions thereof may be provided on or off the host .

The active client can obtain performance data from the multipathing driver by waiting for events from the multipathing driver that indicate when new snapshots are ready. In an example the driver is configured to raise an event each time a new performance data snapshot is available. The active client can issue a command via the wait for event handler that alerts the active client when a next such event occurs. Upon being alerted of the event the active client can retrieve the new snapshot from the kernel persistence manager of the driver e.g. using the retrieving handler .

In some examples a subscription service is provided whereby passive clients can issue subscriptions to the active client for new snapshots. When the active client receives an event raised by the driver in response to a wait for event command the active client can retrieve the new snapshot from the driver using the retrieving handler post process the new snapshot using the post processor and store the newly post processed data in the userspace persistence manager . The active client can then send messages to the various passive clients to inform those clients that the new post processed data are available. Using this subscription service only the active client disturbs the driver for new snapshots and this typically happens at most once per sampling interval. The passive clients can access post processed performance data from the userspace persistence manager but these data are accessed entirely in userspace without the need to interfere with the multipathing driver and thereby impair its performance in processing I O requests.

Also the userspace persistence manager stores the same dataset for all clients. Thus not only does this approach avoid needless duplication of steps and consequent inefficiencies but it also avoids confusion that might otherwise occur if different clients were allowed to report results of different datasets acquired at different times and or post processed in different ways.

Details of collecting and processing performance data by the driver will now be described with further reference to . Referring again to the driver the I O monitors and operate by collecting raw data including the I O size of I O requests and timestamps indicating when such requests are encountered and whether the request is a read or a write. Performance data may be acquired for an I O request as follows. An outgoing I O request is received by the I O monitor . The I O monitor reads the size of the corresponding I O from the I O request and obtains a timestamp to indicate when the I O monitor encounters the I O request . The I O request then propagates through the driver along a pathway to the I O monitor . The I O monitor obtains a timestamp to indicate the time that the I O monitor encounters the I O request. An outgoing I O request is then forwarded from the driver to lower levels of the I O stack of the host and eventually to the array . The I O request is processed by the array and returned. A returning I O request is received by the I O monitor which obtains a timestamp to indicate the time of receipt and propagates through the driver along a pathway to the I O monitor which also obtains a timestamp. A processed returning I O request is then returned to higher levels of the I O stack of the host .

The performance data manager receives the I O size of the I O request from the I O monitor and receives timestamp information from all of the I O monitors and . The performance data manager then calculates differences between timestamps to produce measures of I O latency. These may include for example latency through the driver e.g. between I O monitors and or between I O monitors and external round trip latency e.g. between I O monitors and and total round trip latency e.g. between I O monitors and which includes driver latency in both directions as well as external latency to the array and back.

The performance data obtained and or calculated by the performance data manager is stored in the performance data structure . The performance data structure is preferably organized by LUN and by path as well as by READs and WRITEs. The performance data structure preferably stores performance data in the form of data buckets .

In some examples the data buckets store additional performance data besides counts of I O requests. For instance buckets may also store sums of performance data values. Each bucket for I O size can be configured to store a count of I O requests having I O sizes that fall within the range of the bucket as described above a sum of all I O sizes counted in and a sum of all total round trip latencies associated with I O requests counted in . Additional values can be stored for each bucket such as sums for other latency measurements.

Similar sums may be computed and stored for latency buckets. For instance each bucket for total round trip latency can be configured to store a count of I O requests having round trip latencies that fall within the range of the bucket as described above a sum of all total round trip latencies counted in and a sum of all I O sizes for each I O request counted in . Similar sums can be computed and stored other latency measurements.

In some examples bucket counts and sums can be stored together for example in an array data structure. The array data structure has an index that spans the number of ranges specified for the type of performance data to be counted e.g. . For each index range the data array can store the count of metric values for the respective range as well as any sums computed. In some examples sums of metric values can be computed and stored as indicated above even when counts of metric values are not obtained or stored.

Storing sums of metric values in addition to bucket counts provides a rich set of accurate data from which to calculate aggregate I O size latency as a function of I O size average response times sum of round trip latency divided by I O count and throughput sum of bytes transferred divided by the elapsed time between successive reads of the stored sums . Sums are calculated from raw performance data values and thus preserve accuracy that may be compromised if bucket counts alone are used. Although storing sums does consume additional kernel memory it significantly improves the accuracy of metric reporting and may be preferred in some implementations.

In some examples the performance data structure also stores maximum and minimum values of performance data obtained over designated sampling intervals e.g. intervals of the timer . Thus the performance data structure may store both the greatest and least I O size of any I O request processed during a particular sampling interval as well as the greatest and least value of latency or multiple latencies where more than one latency value is computed . Multiple sets of maximum minimum values may be saved e.g. for each LUN for each path for READs for WRITEs and so forth according to the perceived importance of such data and the amount of excess kernel memory that storing such values requires.

The kernel sampling agent samples any or all of the performance data of the performance data structure to produce a snapshot of such data for different intervals of the timer . As indicated snapshots may be retrieved by the active client post processed and stored in the userspace persistence manager for access by any passive client s in userspace .

Operation of the post processor will now be described with continued reference to . The post processor performs multiple operations on the snapshots of performance data retrieved from the multipathing driver . These include calculating differences between data captured in successive snapshots calculating averages calculating totals normalizing data and reducing the volume of data presented to clients. Post processed data are stored in the userspace persistence manager .

In an example the post processor calculates differences in bucket counts accumulated across successive snapshots. For example changes in counts stored for each bucket reveal the number of new I O requests processed during the sampling interval having I O sizes or latencies that fall within the ranges of the respective buckets.

The post processor can also calculate summary data such as averages and totals. For example if snapshots include sums of performance data in addition to counts the sums for different buckets can be added together to produce totals across all buckets for a particular measurement. Also sums of I O size values can be divided by the sampling interval to produce an indication of throughput over the sampling interval.

The post processor can also normalize performance data such as by computing measures of throughput e.g. total number of bytes written or read over the sampling interval expressed in bytes per second . Expressing these measurements per unit time allows direct comparisons of snapshots even when sampling intervals are non uniform.

In addition the post processor can consolidate data. For example in some implementations sixteen buckets are provided for each measurement. Sixteen may be a greater number of buckets than is desired by some customers. In these cases the post processor can consolidate counts from the sixteen buckets into some other number of buckets such as four buckets by merging four groups of four contiguous ranges and adding together the bucket counts for each group. In some examples data consolidation is done non uniformly. In one example where sixteen buckets are consolidated into four the first five buckets could be consolidated into one bucket the next three buckets could be consolidated into one bucket the next two buckets could be consolidated into one bucket and the remaining six buckets could be consolidated into one bucket.

In general the post processed data produced by the post processor and stored in the userspace persistence manager is sufficiently relevant and comprehensive that it suits the needs of multiple clients for multiple purposes. Consequently clients are able to obtain all the information they typically require about performance data of the multipathing driver from the userspace persistence manager .

At step the timer is started and begins counting down the specified interval. As I O requests are processed data accumulates in the performance data structure including in the buckets . Preferably the timer is run in a thread of the driver which includes multiple threads running in effect simultaneously.

At step the multipathing driver waits for the timer to expire while performance data continues to accumulate in the performance data structure . When the timer expires control proceeds to step .

At step the timer is reset and begins counting down again. Using the timer in a repetitive manner allows the driver effectively to collect performance data at a sampling rate substantially equal to the inverse of the interval of the timer .

At step the processing of I O requests is frozen. I O requests for a particular LUN or for all LUNs is temporarily halted.

At step the kernel sampling agent takes a snapshot of the accumulated performance data. For example values of all bucket counters sums and max min values are captured.

At step the processing of I O requests is unfrozen and the driver resumes processing I O requests as they arrive.

At step the captured snapshot is stored in the kernel persistence manager . A snapshot index is incremented and stored in the kernel persistence manager along with the snapshot. The kernel persistence manager may store multiple snapshots and associated indices corresponding to different sampling intervals.

At step the command event processor raises an event to indicate that a new snapshot of performance data is ready to be retrieved. Control then returns to step where the thread of the multipathing driver once again waits for the timer to expire.

The thread of the multipathing driver remains in the loop described by steps until performance monitoring is turned off.

Asynchronously to the above described process of steps the multipathing driver may receive a wait for event command at step from the active client via the wait for event handler of the API . After receiving this command the process of steps continues as usual but the next time an event is raised i.e. after the next iteration of step the wait for event handler may receive the raised event and inform the active client that a new snapshot is ready.

At step a retrieving command is received from the active client via the retrieving handler of the API . At step the command event processor returns the new snapshot to the active client .

Although the various activities of the process are described as being conducted by the multipathing driver it is understood that the constructs that perform the various steps may alternatively be situated elsewhere in the operating system kernel but in close coordination with the multipathing driver .

At step performance monitoring is turned on and the sampling interval is set for the timer . Step is illustrated with dashed lines to indicate that it may be performed at any time and is only optionally performed by the active client . For example performance monitoring may be turned on in response to a user manually entering a command in an administrative management tool.

At step the active client issues a wait for event command to the multipathing driver . The wait for event command is processed by the wait for event handler of the API . The wait for event handler listens for an event to be raised by the driver that indicates that a new snapshot is ready for pickup. A time out may be specified with the wait for event command to direct the wait for event handler to stop listening after a specified period of time expires. The time out period should be longer than the interval of the timer to ensure proper operation.

At step the active client waits to be informed that the event has occurred that indicates that a new snapshot is ready. When the active client receives such indication control proceeds to step .

At step the active client retrieves the new snapshot from the driver e.g. using the retrieving construct of the API . The active client also receives the current index of the snapshot read back from the driver .

At step the active client invokes the post processor to post process the snapshot. As indicated in connection with post processing may involve multiple operations including identifying differences in performance data between successive snapshots computing totals averages and other aggregate statistics and consolidating data arriving in buckets.

At step the active client stores the post processed snapshot and snapshot index in the userspace persistence manager . Post processed metrics are then available for access by other clients.

At step the active client alerts any subscriber clients e.g. passive clients and any other clients that have subscribed to performance data from the active client that new post processed data is ready for consumption by sending messages to such clients. In some examples the active client also sends the actual snapshot or the post processed data.

Control then returns to step where the active client again issues a wait for event command to request the next snapshot. The active client remains in the loop including steps indefinitely e.g. until performance monitoring is turned off. Because the active client only accesses the driver after the driver has raised an event indicating that a new snapshot is ready retrieving performance data from the driver is typically limited to once per sampling interval. With sampling intervals set on the order of once per minute the effect of the active client on the driver is negligible. Other clients requiring access to performance data from the driver may obtain such data from the userspace persistence manager without causing the driver to experience any loss of performance.

At step the passive client sends a subscription request to the active client to request that the passive client be alerted when new post processed snapshots are available.

At step after some time has passed the passive client receives a message from the active client that new post processed data is ready.

At step the passive client contacts the userspace persistence manager to request the new post processed data. At step the passive client receives the new post processed data from the userspace persistence manager .

It is understood that the process of can be conducted by multiple passive clients . Such passive clients may operate independently from one another or in coordination and may be located on the host or off the host .

As indicated the kernel persistence manager of the driver may store multiple snapshots corresponding to multiple respective time intervals where each snapshot is associated with an index. In some examples the active client can request snapshots by index number. The snapshot returned is then the requested snapshot rather than necessarily the newest snapshot. Multiple snapshots may be requested at once. For example the active client can specify the indices of multiple snapshots and the retrieve construct will return all of the requested snapshots to the active client . In some examples the active client may query the driver to obtain the number of snapshots stored and the time until the next snapshot is available. Indices are also used to identify sets of post processed data. In an example sets of post processed data are associated with the same indices used to identify the respective snapshots from which they are processed.

An improved technique has been described for managing access to performance data of a multipathing kernel driver . The technique includes acquiring snapshots of performance data in the driver at identified instants of time marked by the driver itself and transferring the performance data to a userspace persistence manager outside the operating system kernel where client applications such as the active client and any number of passive clients can access the performance data and or post processed versions thereof without disturbing the operation of the driver . Communications with the driver and burdensome interruptions of I O processing that follow are thereby reduced allowing the driver to perform its primary functions of processing I O requests more efficiently.

As used throughout this document the words comprising including and having are intended to set forth certain items steps elements or aspects of something in an open ended fashion. Although certain embodiments are disclosed herein it is understood that these are provided by way of example only and the invention is not limited to these particular embodiments.

Having described certain embodiments numerous alternative embodiments or variations can be made. For example it is shown and described that the driver stores performance data in the form of data buckets. However this is merely an example. Alternatively performance data may be stored by the driver in other forms such as raw measurements. The invention is therefore not limited to any particular form of performance data.

Along similar lines the performance data is described as including I O size I O latency and numbers of I O requests. Other types of performance data may be collected and stored however. The invention is therefore not limited to any particular type of performance data.

Also the driver has been shown and described as a multipathing driver. However this again is merely an example. The driver can be any type of driver that runs within the kernel of a host operating system.

Also computing activities are shown and described as taking place on a host computing system which is connected to a storage array . Although the techniques described herein are particularly advantageous in data processing systems the invention is not limited to these types of systems and may be used in any type of computing environment.

Further it has been shown and described that the active client is alerted of the availability of new snapshots of performance data in an event driven manner i.e. using the wait for event command. The active client is not limited to this command however. Alternatively the active client or any client may poll the driver e.g. it can operate in a loop where it repeatedly requests that the driver respond indicating whether a new snapshot is ready. The client once it receives an affirmative response can retrieve the snapshot as described above.

Further still the improvement or portions thereof may be embodied as one or more non transient computer readable storage media such as a magnetic disk magnetic tape compact disk DVD optical disk flash memory Application Specific Integrated Circuit ASIC Field Programmable Gate Array FPGA and the like shown by way of example as media and in and . The media may be encoded with instructions which when executed on one or more computers or other processors perform methods that implement the various processes described herein. Such media may be considered articles of manufacture or machines and may be transportable from one machine to another.

Those skilled in the art will therefore understand that various changes in form and detail may be made to the embodiments disclosed herein without departing from the scope of the invention.

