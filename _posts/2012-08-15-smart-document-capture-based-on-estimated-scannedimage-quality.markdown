---

title: Smart document capture based on estimated scanned-image quality
abstract: A method for providing real-time feedback of an estimated quality of a captured final image, the method including obtaining a preliminary image, calculating a quality score of the preliminary image, and in response to the quality score of the preliminary image exceeding a threshold quality value, taking a first action.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09208550&OS=09208550&RS=09208550
owner: FUJI XEROX CO., LTD.
number: 09208550
owner_city: Tokyo
owner_country: JP
publication_date: 20120815
---
Aspects of the example embodiments are directed to document capture and more specifically smart document capture by estimating scanned image quality before scanning.

People are increasingly using cameras of mobile devices such as cellular phones and personal digital music players to attempt to scan physical documents. In these situations the user captures an image of the physical document the usual way the user must look at the screen of the mobile device. However camera quality uneven lighting conditions and other contextual issues may result in the quality of document images captured with mobile devices being much lower than expected.

Thus using a mobile device to capture a high quality image of a physical document can be difficult. For example even if the user is careful to position the physical document in a preview screen of the mobile device the quality of the final image may not appear as expected based on the preview screen. Furthermore the act of capturing an image of the physical document may affect the quality of the image as explained below.

Some of these issues may be handled using post processing techniques i.e. image sharpening techniques image rotating techniques image cropping techniques etc. . However image information may not be fully captured due to above noted issues and not be recoverable by post processing or post processing may introduce artifacts. Thus post processing techniques cannot fully compensate for the above noted issues.

Thus low camera quality uneven lighting conditions and other contextual issues may affect the quality of a captured image with respect to a preliminary image displayed on the screen of the mobile device even though the cameras on such mobile devices have increasingly higher resolutions. Thus existing techniques and apparatuses may not be able to provide accurate scans due to poor quality of captured images.

This application relates to a method for providing real time feedback to users about the likely quality of a captured image. The quality measures such as the sharpness and or framing of a page or of one or more columns from a page may be captured allowing users to adjust the camera position and the time or place to capture an image a picture of a physical document to produce a high quality result may be determined automatically or provided to a user who can determine when to take the picture based on the feedback. The feedback mechanisms may avoid a need to rely on using the preview screen to judge an imaging being captured and may permit a user to lean away from the document so as to avoid interference with the captured image.

Aspects of certain example embodiments include a method for providing real time feedback of an estimated quality of a captured final image the method including obtaining a preliminary image calculating a quality score of the preliminary image and in response to the quality score of the preliminary image exceeding a threshold quality value taking a first action.

Aspects of certain example embodiments include a computer readable medium having stored therein executable instructions for providing real time feedback of an estimated quality of a captured final image the instructions including obtaining a preliminary image calculating a quality score of the preliminary image and in response to the quality score of the preliminary image exceeding a threshold quality value taking a first action.

Aspects of certain example embodiments include an apparatus for providing real time feedback of an estimated quality of a captured final image the apparatus including an image capture device that captures a preliminary image a quality analyzer that calculates at least one quality score of the preliminary image and a controller that takes a first action in response to the one or more quality score s of the preliminary image exceeding a threshold quality value.

In the following detailed description reference will be made to the accompanying drawing s in which identical functional elements are designated with like numerals. The aforementioned accompanying drawings show by way of illustration and not by way of limitation specific embodiments and implementations consistent with principles of an example embodiment. These implementations are described in sufficient detail to enable those skilled in the art to practice an example embodiment and it is to be understood that other implementations may be utilized and that structural changes and or substitutions of various elements may be made without departing from the scope and spirit of an example embodiment. The following detailed description is therefore not to be construed in a limited sense. Additionally the various embodiments of the invention as described may be implemented in the form of software running on a general purpose computer in the form of a specialized hardware or combination of software and hardware.

In this application an example method and an example apparatus for providing real time feedback to users about estimated quality of a to be captured image i.e. an image that has not yet been captured but may be captured in the future is described. The quality of the to be captured image is estimated based on quality measures of a preliminary image that was previously captured. Examples of the quality measures of the preliminary image may include but are not limited to the sharpness and the framing of a page of text or of one or more columns from a page. By providing real time feedback of quality measures of the preliminary image the camera position may be adjusted in a manner that may improve quality measures.

In some example embodiments a secondary or final image may be automatically captured when it is determined that the position and or timing of the camera is sufficient to produce an image of sufficient quality for a scanning and or image recognition operation. Alternatively in some example embodiments feedback such as a visual indicator or an audible sound or tone may be provided to a user to indicate that the quality measures are sufficient that the to be captured image is estimated to have a sufficient quality and the user can determine when to capture the image picture based on the feedback.

An example embodiment may include an application running on the mobile device which may provide instruction to capture images of a document with the camera of the mobile device. In such an example embodiment the application may be running on the mobile device with no external support the application may only need access to the camera of the mobile device. As the camera views the document the application may analyze the preview frame using two distinct quality scores described below . As the quality scores change the mobile device translates the quality scores into feedback to the user.

Feedback can include audio tones e.g. sounds and may also include visual indicators such as the color or hue of an LED of the mobile device being changed or a change in the displayed preview image such as highlighting edges of the preview image to indicate proper framing . For example lower scores may correspond to low frequency tones and higher scores may correspond to high frequency tones on a continuous or discrete scale or different tones can be used as would be apparent to a person of ordinary skill in the art.

Feedback can also be provided by controlling an LED of the mobile device to vary in color or hue based on the scores. For example lower scores may correspond to the LED being red and higher scores may correspond to the LED being green. The LED may vary on a continuous or discrete scale based on scores or alternative color schemes may be used as would be apparent to a person of ordinary skill in the art.

When the mobile device is capturing an image depending on the current feedback mode a distinct audio tone may play. Further the LED may flash a color outside of the score range such as blue . Furthermore the application in some example embodiments may capture the image of the physical document automatically based on the quality scores. Alternatively the application in some example embodiments may capture the image of the physical document only based on a manual input by a user such as by pushing a button or touching an icon on a touch screen display .

As illustrated in in a method according to an example embodiment a preliminary image is captured in S. The method of capture of the preliminary image is not particularly limited and may include capturing an image with a charge couple device CCD capturing an image with an active pixel sensor APS or any other image capture method as would be apparent to a person of ordinary skill in the art. Then one or more image quality measures e.g. framing scores and or sharpness scores of the preliminary image are determined e.g. obtained and or calculated in S. As discussed below regarding S and S feedback on the quality measures may be provided including whether the quality measures are sufficient for an image to be captured. The obtained and or calculated quality measures are discussed in more detail below.

At S the determined image quality measures are evaluated to determine whether the image quality measures are sufficient for image capture e.g. do the quality measures exceed thresholds for image capture . As discussed in more detail below one or more quality measures e.g. sharpness scores and or framing scores may be used independently or in combination to identify or address some or all of the document image issues outlined in the background section above. For example the document sharpness scores may be used to identify focus issues and document framing scores may be used to identify framing issues. Further document sharpness scores and document framing scores may be used in combination to identify rotation shadow lighting and depth variation issues.

If the quality measures are not high enough feedback is provided to a user indicating that image quality is not sufficient at S. As mentioned above feedback may include audio tones or sounds and or visual indicators. Based on the feedback that the quality measures are not sufficient a user may reposition the mobile device and a new preliminary image may be captured at S.

Alternatively if the quality measures are sufficiently high feedback is provided to a user indicating that image quality is sufficient at S. If the image quality measures are sufficient a secondary image i.e. a final image may be captured at S by receiving a user input e.g. by pushing a button or touching an icon on a touch screen display on a mobile device of or alternatively a secondary image i.e. a final image may be captured automatically without requiring user input if the quality measures are sufficient.

As discussed above one or more quality measures may be used to evaluate the quality of the preliminary image. In an example embodiment both a sharpness score and a framing score such as a quiet margin size as discussed below may be used. Additionally in some example embodiments an alignment or orientation measure may also be a quality measure used to estimate alignment of text with a camera edge.

However calculating an alignment or orientation measure may be computationally expensive and thus be difficult for real time capture by a mobile device. Thus in example embodiments image sharpness measures and framing measures such as quiet margin size can be used to indirectly provide an indication of text line alignment without needing to calculating alignment or orientation measurements. Thus combining image sharpness measures and framing measures text alignment can be indirectly measured.

Another example quality measure may be movement or more specifically lack of movement. For example movement can be estimated from accelerometer readings available in mobile devices. Checking accelerometer readings may be faster than checking computing sharpness and thus may be a backstop against taking a photo right when the user decides to point the camera someplace else. However lack of movement alone does not guarantee a sharp image since blur can be due to other causes such as the image being out of focus. Thus multiple quality measures may be combined.

When capturing an image of a page of a physical document a user captures an image of the entire page or may capture an image of a portion of a column of text on the page of the physical document. When the user is capturing an image of the entire page the edges of the page may be used to frame a whole page. To frame only a portion of a column a text the size of the quiet margin may instead be estimated. Quiet margins correspond to the gutters between columns of text or the margins between text columns and the edge of the page i.e. areas of blank space which may surround the text areas on a page .

In an example embodiment a threshold may be set for minimum and maximum quiet margin size. Use of a maximum quiet margin size may ensure that a region of interest fills most of captured image to use more of the available resolution. An example of an allowable range for the inside edge of quiet margins for a page is illustrated are shown by in . In an example page is shown with eight zones highlighting eight areas where quiet margins could be measured on the example page . These areas include top left and top right zones upper right side and lower right side zones bottom left and bottom right zones and upper left side and lower left side zones. However quiet margins need not be divided into eight zones and may be divided into more than 8 zones or may be divided into less than 8 zones as would be apparent to a person of ordinary skill in the art. Further the arrangement and orientation of the quiet margins are not limited to shapes or sizes shown in and may be any shape or size to divide the regions near the edges of a capture image into zones for further analysis as discussed below.

In an example embodiment a page captured in portrait orientation a minimum left and right quiet margin size may be set at about 2 of the image width and a maximum left and right quiet margin size may be set at about 8 of the image width. The minimum and maximum top and bottom quiet margin sizes may be set to be within similar ranges. These ranges are merely provided for exemplary purposes and the minimum and maximum quiet margin sizes are not particularly limited to being between about 2 and about 8 .

In an example embodiment the framing score may be determined to be sufficient under one or more of the following circumstances 

If all four margins top bottom left and right all meet the quiet margin size constraints ranges discussed above a framing score for the image may be determined to be sufficient. This circumstance is shown in .

There are a number of features that may be used to identify quiet margin size including pixel values text character locations and feature point locations for example Scale Invariant Feature Transform Features SIFT features may be used . Further a combination of methods may be used instead of a single method.

Although pages of a physical document to be captured may have a white background magazines slides and other documents may have colored backgrounds. Unlike scanned images images captured using a mobile device may have been taken with the camera positioned so close to the page that no margins or columns are viewed in the captured image or so far from the page that other irrelevant objects are in view. Furthermore the lighting may not be carefully controlled such that different lighting conditions can alter the captured color over different parts of the image.

Thus in an example embodiment discussed below a combination of two complementary methods may be used to identify. In this example embodiment the complementary methods may include 1 a pixel based method which may work on uniform backgrounds and may handle photos and 2 a character location based method which may work on text and may handle non uniform backgrounds better than pixel based but may not handle photos with gradual color changes as well as a pixel based method. Each of these example methods are discussed in more detail below.

Block diagrams illustrating an example method of finding quiet margin sizes using pixel values as features is shown in . illustrates the method for processing the whole image and provides example embodiments of S S S and S of the process of . illustrates a method of processing an individual zone making up a portion of the image in the method of .

To check quiet margins using pixel values the image is first binarized i.e. the pixel values are converted to binary format in S. Additionally in some example embodiments methods of cleaning up or processing an image such as morphology based filtering based or edge detection methods for example may be applied to the image before or after the image is binarized. Once the image is binarized the image is divided into the plurality of zones in S. Each of the zones is processed to determine whether the quiet margin values are acceptable in S. By dividing the rows or columns along an edge into a small number of zones and determining whether the quiet margin values in each zone are acceptable restrictions on an amount of skew present in an image can be controlled. In the example of introduced above each edge is divided into two zones top left and top right zones upper right side and lower right side zones bottom left and bottom right zones and upper left side and lower left side zones and these zones are used to check quiet margins at each edge. The processing of each zone is discussed in greater detailed below with respect to .

In S it is determined whether all zones around the edges of the image have been processed. If it is determined at S that some zones have not been processed processing returns to S. S S S and S collectively are an example of the Determining Image Quality Measures S of .

After all zones have been determined to be processed at S it is determined whether all zones are sufficient i.e. each zone is determined to have a sufficient quiet margin size at S. In some example embodiments each individual zone is determined to have a sufficient quiet margin size if the quiet margin size is greater than a minimum margin size for example 2 of the image height or width . In other example embodiments each individual zone is determined to have a sufficient margin size if the quiet margin size is greater than or equal to a minimum margin size for example 2 of the image height or width and the quiet margin is less than or equal to a maximum margin size for example 8 of the image height or width . S is an example of the Quality Measures are Sufficient determination S of .

If any zone around the edge is determined to not be sufficient feedback is provided that edges are insufficient at S. S is an example of the Provide Feedback on Quality Measures S of . Processing is then returned to S of a new preliminary image is captured and the above discussed processes are repeated until all zones around the edges are determined to be sufficient.

If all the zones around the edge are determined to be sufficient the quiet margin size is determined to be sufficient in S and feedback is provided indicating that the edges are sufficient. S is an example of Provide Feedback that All Quality Measures Are Sufficient S of . Processing may then optionally continue to S of and a final image may be captured and subsequent processing such as OCR may be performed on the captured final image.

At the beginning analyzing the zone a row or column counter is set to 0 in S. This row column counter is be used to determine the pixel width of the quiet margin. In S it is determined whether the background color of the binary image has been previously determined so that the foreground pixel value e.g. pixel value of text diagrams or photos printed on the page of the physical document to be captured can be distinguished from the background pixel value e.g. pixel value of the blank areas of the page of the physical document to be captured based on the color.

If the background color of the binary image has not yet been determined the number of pixels having a pixel value of 0 is counted and the number of pixels having a pixel value of 1 is counted for the row or column closest to the image edge in S. The predominant color i.e. either 1 or 0 which has occurred the most in the row or column is set as the background color. In S it is determined whether the number of the non predominant pixels e.g. the number of pixels having a 0 value if the predominant value is determined to be 1 value pixels or the number of pixels having a 1 value if the predominant value is determined to be 0 value pixels exceed a threshold.

If the number of non predominant pixels is determined to not exceed the threshold value the predominant color is set as the background color in S. After the predominant color is set as the background color in S the row column counter is incremented i.e. increased by a value of 1 in S. Thus the row column counter represents a number of successive rows or columns from the outermost edge of the image that is determined to be quiet rows or quiet columns.

After the row column counter is incremented in S the process moves back to S where the analysis is repeated for a row or column located further inward of the row or column just analyzed. Thus in S it is again determined whether the background pixel values of the binary image have been previously determined. In some example embodiments the background and foreground pixel values of the binary image are determined once based on the values of the first quiet row or column and the same background and foreground colors are assumed for successive rows or columns in S. However in alternative embodiments the same background or foreground colors need not be assumed between each row or column and each row or column may be separately evaluated in S S and S to determine what predominant color is present and a row or column could be considered quiet if most pixels in the row column have the value associated with background of the page of the physical document.

In example embodiments where the background pixel value for the whole zone is set in S the analysis of the further inward row or column proceeds to S and it is determined whether the number of foreground pixels i.e. non predominant pixels exceeds the threshold number. This threshold number represents a maximum number of foreground pixels allowed in a row or column and the row or column still be classified as quiet i.e. a blank row or column . A small number of foreground pixels may be allowed to account for noise that may occur in the margins of the preliminary image.

If the number of foreground pixels is determined to not exceed the threshold in S the row or column is determined to be a quiet row or quiet column i.e. a row or column that may be part of a quiet margin i.e. a margin is quiet if all the rows columns in the margin are quiet and the row column counter is again incremented i.e. increased by a value of 1 in S. Again the row column counter represents a number of successive rows or columns from the outermost edge of the image that is determined to be quiet rows or quiet columns. As discussed above by counting the number of rows or columns that are blank the threshold allows for some noise or quiet the size of a quiet margin can be determined.

S S and S are repeated in sequence for successively further inward rows or columns until the number of foreground pixels is determined to exceed or is equal to e.g. not less than the threshold value in S.

If the number of foreground pixels in a row or column exceeds or is equal to the threshold value the row or column is determined to not be quiet i.e. not a blank row or column and the row column counter is returned indicating the number of rows or columns that were determined to be quiet i.e. blank rows or columns in S. The size of the quiet margin is determined as the number of consecutive rows or columns of pixels from an edge that were determined to be quiet blank .

If during S it is determined that the number of non predominant pixels is greater than the threshold the row or column is also determined to not be quiet i.e. not a blank row or column and the row column counter is returned indicating the number of rows or columns that were determined to be quiet i.e. blank rows or columns in S. Again the size of the quiet margin may be determined based on the number of consecutive rows or columns of pixels from an edge that were determined to be quiet blank and the size of an individual row or column e.g. row column counter value pixel size quiet margin size . In some example embodiments each individual zone is determined to have a sufficient quiet margin size if the quiet margin size is greater than a minimum margin size for example 2 of the image height or width . In other example embodiments each individual zone is determined to have a sufficient margin size if the quiet margin size is greater than or equal to a minimum margin size for example 2 of the image height or width and the quiet margin is less than or equal to a maximum margin size for example 8 of the image height or width . In some example embodiments the computed quiet margins for different edges may have unequal values such that the area of interest is slightly offset.

As discussed above after each zone is analyzed to determine the quiet margin size it is determined in S of whether all of the zones are acceptable i.e. the zones have a sufficient quiet margin size . Based on the determination either a new preliminary image is captured and the process is repeated S or an indication that the image quiet margins are acceptable is provided at S and a secondary i.e. final image is captured and subsequent post processing such as an Optical Character recognition OCR process can be performed.

Block diagrams illustrating an example method of finding quiet margins using character size connected components as features is shown in . illustrates the method for processing the whole image and provides example embodiments of S S S and S of the process of . illustrates a method of processing an individual zone making up a portion of the image in the method of .

In S the image is analyzed to first identify connected components for example regions of connected text using one or more computer vision techniques. For example computer vision tools from the OpenCV library could be used. However computer vision techniques are not limited to the OpenCV library and other computer vision techniques could be used to identify connected components as would be apparent to a person of ordinary skill in the art.

After the connected components are identified in S the connected components that do not have a size and or aspect ratio consistent with text characters are filtered out in S. A location of a box bounding for each connected component that is not filtered out in S is computed and assigned into one or more zones for processing in S. For example all bounding boxes which have a centroid in the top half of the image may be assigned to the top zone along the left margin and all bounding boxes which have a centroid in the bottom half of the image may be assigned to the bottom zone along the left margin.

After all bounding boxes that were not filtered out based on size and or aspect ratio are assigned to one or more zones in S each zone is processed to determine margin size in S and . The processing of each individual zone is discussed in greater detail with reference to below. In S it is determined whether all zones have been processed. S is repeated until all zones are processed. S S S S and S collective are an example of the Determining Image Quality Measures S of .

After all zones are processed it is determined whether all zones are sufficient i.e. each zone is determined to have a sufficient quiet margin size at S. In some example embodiments each individual zone is determined to have a sufficient quiet margin size if the quiet margin size is greater than a minimum margin size for example 2 of the image height or width . In other example embodiments each individual zone is determined to have a sufficient margin size if the quiet margin size is greater than or equal to a minimum margin size for example 2 of the image height or width and the quiet margin is less than or equal to a maximum margin size for example 8 of the image height or width . S is an example of the Quality Measures are Sufficient determination S of .

If any zone around the edge is determined to not be sufficient feedback is provided that edges are insufficient at S. S is an example of the Provide Feedback on Quality Measures S of . Processing is then returned to S of a new preliminary image is recaptured and the above discussed processes are repeated until all zones around the edges are determined to be sufficient.

If all the zones around the edge are determined to be sufficient the quiet margin size is determined to be sufficient in S and feedback is provided indicating that the edges are sufficient. S is an example of the Provide Feedback that All Quality Measures Are Sufficient S of . Processing may then optionally continue to S of and a final image may be captured and subsequent processing such as OCR may be performed on the captured final image.

In S the margin size of a zone is estimated based on the difference between the location of the edge of the image in the zone e.g. 0 image width or image height and the edge of the bounding box closest to said the image edge in the zone. For example the difference between the leftmost remaining value not ignored as noise and the left edge of the image may be used to the estimate the size of the left margin. To compensate for non text regions in some embodiments the number of characters in the region is determined. If there are fewer characters than a certain minimum number of characters the bounding box is noted as containing too few characters.

In S a margin may be considered sufficient if at least one of the zones associated with the margin is sufficient and the other zones associated with the margin are either also sufficient or have been recorded as containing too few characters. In some example embodiments each individual zone is determined to have a sufficient quiet margin size if the quiet margin size is greater than a minimum margin size for example 2 of the image height or width . In other example embodiments each individual zone is determined to have a sufficient margin size if the quiet margin size is greater than or equal to a minimum margin size for example 2 of the image height or width and the quiet margin is less than or equal to a maximum margin size for example 8 of the image height or width . Though the example embodiments discussed above refer to the left margin the zones belonging to the right top and bottom margins may be computed using similar methods.

Any of the above discussed methods for estimating the size of the margins can be combined in different ways. In some embodiments the framing may be considered sufficient if one or both methods determine that the framing of the whole preliminary image is determined to be sufficient. In other example embodiments if at least one of the methods considers one zone of an edge sufficient of the preliminary image is considered sufficient then the whole edge zone is determined to be sufficient. Alternatively a whole edge may be determined sufficient only if all zones of that edge are determined to be sufficient.

To produce a quality image the text in a preview image should not be blurry as shown in . Thus sharpness may be measured to determine whether the preview image is blurry. The technique used to measure sharpness is not particularly limited. One example technique for measuring sharpness is described in U.S. patent application Ser. No. 13 293 845 filed on Nov. 10 2011 which could be used in an example embodiment. This measure was developed for text but also works for images. Alternative sharpness measuring techniques could be used as would be apparent to a person of ordinary skill in the art.

Some sharpness estimating techniques only involve estimating sharpness in the horizontal direction. The techniques of U.S. patent application Ser. No. 13 293 845 estimate the sharpness of an image in both the vertical and horizontal directions. As text tends to have predominantly vertical and horizontal strokes the techniques of U.S. patent application Ser. No. 13 293 845 estimate the sharpness more accurately when the text lines are aligned with an edge of the camera image. Thus document sharpness tends to be greater when text lines are aligned with an edge of a captured image. Techniques estimating sharpness only in a horizontal direction could be used in example embodiments but stronger feedback regarding text orientation may be provided by estimating sharpness in horizontal and vertical directions.

The image sharpness and quiet margins measures may be used independently from each other or may be combined in various ways. In some example embodiments a first indication may first be provided when the preliminary image is in focus and then a second indication may be provided when the framing is sufficient. In some alternative embodiments the separate image sharpness and quiet margin measures may be combined into one score for example a weighted average may be used .

In some example embodiments the document sharpness and the quiet margins measures being used in combination as discussed above may address some or all of the document image issues outlined in the background section above.

Several different example approaches to using the example methods discussed above may be used to capture high quality scans. For example two approaches the hover approach and the lift and lower approaches are discussed below.

For mobile devices with autofocus a user may assume that the mobile device will eventually focus given enough time. Thus most of the user s attention can be directed to adequately framing the page. By using the image sharpness calculating methods discussed above an image sharpness indicator can be provided when the image is sharp as the mobile device is moved around by the user attempting to frame the image.

Further though the camera continuously refocuses the focus process itself can sometimes take several seconds. Additionally sometimes the image is blurry even when the camera focus has settled requiring that the mobile device be moved away and back into position to attempt to cause the mobile device to readjust into a sharper focus. Using an image sharpness indicator may better insure that a captured image will be sharp.

With these issues in mind a hover approach to scan physical documents using the above sharpness and quiet margin measuring methods is as follows 

In some embodiments the sharpness measure may be thresholded such that once a minimal level of sharpness is achieved the sharpness is determined as sufficient and the feedback becomes steady. For example if audio feedback is provided as low pitched beeps for poor sharpness values and higher pitched beeps for better sharpness values the pitch of the beeps will remain high and constant as long as the sharpness is sufficient . If visual feedback is used the LED can be colored e.g. green to indicate that the image is sharp.

If the pitch is not high and constant or the LED is not steadily green either the camera autofocus may be unable to focus and the mobile device may need to be moved away and back so that the camera can try to autofocus again or there may be a problem with the user s setup. For example the lighting may be too dark too much of the image may have shadows or the text may be very small. With lighting issues the user may need to either adjust the ambient lighting or move so that the source of the shadow e.g. the user is no longer between the light source and the page. In some embodiments the threshold sharpness value may be adjustable by the user.

Once the image is sharp the user can focus on adjusting the camera position until the framing is sufficient. With this approach the primary task of the user is to frame the desired text and or images. While it may be difficult to manually check that two to four sides are correctly aligned simultaneously a mobile device can recognize when two to four sides are correctly aligned and in some embodiments a final image is captured automatically. By having the camera automatically capture an image the user does not need to press a button which can shake or jolt the camera.

Thus when taking a picture of a subsection of the physical document the user can hold the camera close to the section and slowly move the mobile device. Similarly when taking a picture of the entire document the user can hold the mobile device above the physical document and slowly move the mobile device toward the mobile device alternatively the user can use the lift gesture and simply move through the first local maximum . In both cases assuming sufficient alignment the predicted image quality should reach a local maximum.

If that local maximum itself is still not high enough that a satisfactory image can be obtained then the user can try strategies to improve the sharpness of the image such as moving to better light leaning away from the physical document so as not to cast a shadow on it or adjusting the focus thresholds.

As with the hover method after taking a picture the picture is displayed and the same options are available.

A drawback with this approach is that the user would need to wait for the camera to refocus at each level. However to address this drawback the user could optionally adjust settings so that autofocusing of the camera only occurs when manually triggered by the user e.g. with a button press .

Many other capture methods are possible. For example when shooting a multi page document the user could set and lock the focus when capturing the first page and use the lift and lower technique to match the capture position of the camera for subsequent pages.

In another example feedback of only the framing score could be given. When feedback indicates that the framing is sufficient the user would hold the camera still and push a button to tell the camera to autofocus and take a picture.

Alternatively the camera may use instant autofocus techniques that always maintain focus so that once the framing is sufficient a picture can be automatically captured.

Additionally feedback of only the sharpness score can be used to indicate to a user when the camera is focused for multi page documents. In this case the user could frame the image by looking at a viewfinder. An alternative method of framing such as finding the edges of the page of the physical document could be employed. For example techniques of automatically identifying the edges of a business card could be used in example embodiments.

Even though a preview image is sharp a captured final image is not guaranteed to be sharp. It currently takes long enough to process each frame that by the time capture occurs the camera may attempt to refocus or the camera may move or be shaken. To address this in some example embodiments estimating the sharpness of the captured final image could be done automatically and if the final image is determined to not be sharp final images could be continuously captured and processed until a final image of sufficient sharpness is achieved. Another optional alternative is to continuously capture a fixed number of images and selecting the one with the best sharpness score. Another optional alternative is to include the movement feature as one of the features so that feedback on quality can indicate if there is too much movement. In other words in an example embodiment the amount of movement can be measured and if the amount of movement measured is excessive i.e. exceeds a movement threshold feedback indicating that movement is too large to capture a good quality image may be provided.

Sometimes the quality will be poor for one of the previously described factors and a user will want to capture an image anyway. A slider may be provided to allow the user to adjust the image quality on a menu or the mobile device.

Though examples of audio feedback e.g. audible tones and visual feedback e.g. illuminating LEDs are discussed above other ways of providing feedback may be used. For example visual feedback may be overlaying a feedback bar graphically indicating a sharpness and or a framing score directly on the screen during preview image display as shown in . This direct feedback can be coupled with an interactive slider to allow the user to set the threshold for capturing images automatically. When the sharpness and or framing exceed the threshold as shown in the image may automatically be captured. Further if a user moves the threshold icon to an area beyond the feedback bar automatic image capture may be effectively turned off but a user may still capture an image manually.

Additionally other visual feedback cues could be used to help users improve the image quality score. illustrate an alternate interface where visual feedback indicating whether each margin matches the configured constraints helping the user better frame the physical document is provided. In margins which do not match configured constraints i.e. either too big or too small are shown as greyed out. In the top and right margins are not greyed out as they match configured constraints but the bottom and left margins remain greyed out because they do not match the configured constraints. In only the bottom margin remains greyed out and an image may be automatically captured because three of the four margins match the configured constraints. Optionally when an image is captured an audio indicator for example a camera shutter sound effect of snap or visual indicator for example a screen flash could be provided to inform a user that an image was captured. Though the non conforming margins are greyed out in this example embodiment an example embodiment is not limited to the non conforming margins being greyed out and may include using a different color to highlight the non conforming margins or may alternatively include highlighting conforming rather than non conforming margins as would be apparent to a person of ordinary skill in the art. Additionally the non conforming margins may be represented in other ways. For example rather than coloring a rectangle the edges of the non conforming rectangle may be a different color. The color for highlighting may include transparent for example a conforming margin may disappear .

Additionally rather than requiring sufficient quiet margins to occur at three edges alternative modes may be provided that may allow a user to center the quiet margins in one direction for example horizontally and requiring sufficiently quiet margins at the edges vertically. In such an embodiment rather than looking for quiet margins only at the edges gutters in the center of the page may also need to be identified. This could be done by computing projection profiles and identifying where the values are close to 0 or 1 if the binary image color is inverted . For example if pixel features are used the number of pixels that are 0 could be counted. If characters are used the bounding box widths heights in a row column could be summed respectively or the number of bounding boxes that are in a row column could be counted. To compute a projection profile one example image analysis method the pixel values in each column or row are summed projected . If background pixels are 0 then when the sum is close to 0 there may be a gutter.

Additionally rather than requiring sufficient quiet margins to occur at three edges quiet margins along a different number of edges can be required. For example if only whole pages are to be captured the quiet margin sizes could be adjusted so that all four edges are required to fall in quiet margins. As other examples only two quiet margins or no quiet margins may be required in other applications.

Computing device can be communicatively coupled to input user interface output device interface and an image capture device . Either one or all of the input user interface output device interface or image capture device can be wired or wireless connected and can be detachable. The Input user interface may include any device component sensor or interface physical or virtual that can be used to provide input e.g. keyboard a pointing cursor control e.g. a mouse microphone camera braille motion sensor optical reader and or the like . Output device interface may include a display monitor printer speaker braille or the like. The image capture device may include any device component or sensor that can be used to capture an image e.g. camera motion sensor optical reader and or the like . In some example embodiments input user interface and output device interface can be embedded with or physically coupled to the computing device e.g. a mobile computing device with buttons or touch screen input user interface and an output or printing display or a television . Additionally in some example embodiments the image capture device may also be embedded with or physically coupled to the computing device e.g. a mobile computing device with an integrated camera .

Computing device can be communicatively coupled to external storage and network for communicating with any number of networked components devices and systems including one or more computing devices of same or different configuration. Computing device or any connected computing device can be functioning as providing services of or referred to as a server client thin server general machine special purpose machine or by other label.

I O interface can include but is not limited to wired and or wireless interfaces using any communication or I O protocols or standards e.g. Ethernet 802.11x Universal System Bus WiMax modem a cellular network protocol and the like for communicating information to and or from at least all the connected components devices and network in computing environment . Network can be any network or combination of networks e.g. the Internet local area network wide area network a telephonic network a cellular network satellite network and the like .

Computing device can use and or communicate using computer usable or computer readable media including transitory media and non transitory media. Transitory media include transmission media e.g. metal cables fiber optics signals carrier waves and the like. Non transitory media include magnetic media e.g. disks and tapes optical media e.g. CD ROM digital video disks Blu ray disks solid state media e.g. RAM ROM flash memory solid state storage and other non volatile storage or memory.

Computing device can be used to implement techniques methods applications processes or computer executable instructions to implement at least one embodiment e.g. a described embodiment . Computer executable instructions can be retrieved from transitory media and stored on and retrieved from non transitory media. The executable instructions can be originated from one or more of any programming scripting and machine languages e.g. C C C Java Visual Basic Python Perl JavaScript and others .

Processor s can execute under any operating system OS not shown in a native or virtual environment. To implement a described embodiment one or more applications can be deployed that include logic unit application programming interface API unit input unit output unit framing unit sharpness unit and inter unit communication mechanism for the different units to communicate with each other with the OS and with other applications not shown . For example the framing unit and sharpness unit may implement one or more processes shown in and . The described units and elements can be varied in design function configuration or implementation and are not limited to the descriptions provided.

Though certain embodiments of the present invention may be implemented as software running on for example a mobile device example embodiments are not limited to software implementations. illustrates an example hardware implementation of an example embodiment.

In an image quality feedback providing apparatus is shown. The image quality feedback providing apparatus may include an image capture device framing analyzer controller and optionally one or more of a sharpness analyzer a display a visual feedback providing component an audio feedback providing component and an image processing device all communicatively interconnected. The image capture device may include any device component or sensor that can be used to capture an image e.g. camera motion sensor optical reader and or the like . The framing analyzer may be configured to detect analyze the framing for example the quiet margins of a captured image using one or more of the processes shown in and and discussed above. Similarly the sharpness analyzer may analyze the sharpness of a captured image using one or more of the processes discussed above.

Additionally the display may be any device or component that can be used display information to a user e.g. a CRT display Plasma display LED display LCD display and or the like and may be used to provide preliminary images and other information to a user for example information regarding state information such as flash settings f stop settings shutter speed settings lighting levels and or the like .

Additionally the audio feedback providing component may include any device or component that can be used to provide audio information to a user e.g. a sound producing device and a speaker and or the like to convey feedback regarding the framing e.g. quiet margin and or sharpness analysis performed by the framing analyzer and the sharpness analyzer respectively. Similarly the visual feedback providing component may include any device or component that can be used to provide visual information to a user e.g. an LED device light second display or controller for causing the display to provide information and or the like to convey feedback regarding the framing e.g. quiet margin and or sharpness analysis performed by the framing analyzer and the sharpness analyzer respectively.

Additionally the image processing device may include any device or component that can be used to perform one or more image recognition processes e.g. Optical Character recognition OCR processes facial recognition processes object recognition processes and or the like on a captured image and provide the results of the image recognition process or processes to a user.

The controller may control one or more of the other components to carry out one or more of the operations discussed above. For example the controller may control the display to display a preliminary image captured by the image capture device and may determine whether the framing analysis and or sharpness analysis performed by the framing analyzer and the sharpness analyzer respectively indicate the preliminary image is sufficient for image capture. Further the controller may control the audio feedback providing component and or the visual feedback providing component to provide feedback based on the framing analysis and or sharpness analysis performed by the framing analyzer and the sharpness analyzer respectively. The controller may also control the image capture device to capture a final image when the framing analysis and or sharpness analysis performed by the framing analyzer and the sharpness analyzer respectively indicate that the preliminary image is sufficient for image capture. Further the controller may control the image processing device to perform image recognition processes on the final image when the final image is captured by the image capture device.

Although a few example embodiments have been shown and described these example embodiments are provided to convey the subject matter described herein to people who are familiar with this field. It should be understood that the subject matter described herein may be embodied in various forms without being limited to the described example embodiments. The subject matter described herein can be practiced without those specifically defined or described matters or with other or different elements or matters not described. It will be appreciated by those familiar with this field that changes may be made in these example embodiments without departing from the subject matter described herein as defined in the appended claims and their equivalents.

Aspects related to the example embodiment have been set forth in part in the description above and in part should be apparent from the description or may be learned by practice of the invention. Aspects of the example embodiment may be realized and attained by means of the elements and combinations of various elements and aspects particularly pointed out in the following detailed description and the appended claims.

It is to be understood that both the foregoing descriptions are exemplary and explanatory only and are not intended to be limiting.

