---

title: Methods and systems configured for processing video frames into animation
abstract: Methods and systems configured for processing video frames into animation is provided. An example method includes receiving a video including a plurality of frames, and determining an amount of change per pixel across a portion of the plurality of frames. Based on the amount of change per pixel, a first masking frame may be generated indicative of modifiable pixels and a second masking frame may be generated indicative of static background pixels. The first masking frame can be applied to frames of the plurality of frames to generate a plurality of partial frames, and the second masking frame can be applied to a given frame to generate a background template frame. The background template frame can be combined with each of the plurality of partial frames to generate a plurality of altered frames, and the plurality of altered frames can be processed into a second animation.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09014543&OS=09014543&RS=09014543
owner: Google Inc.
number: 09014543
owner_city: Mountain View
owner_country: US
publication_date: 20121023
---
Unless otherwise indicated herein the materials described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.

Many video editing applications exist today offering for ability to customize animations of video to create interesting effects. Often video editing applications require manual interaction and editing by an artist to create the effects. Example video editing includes editing segments of motion video footage to remove or add images providing special effects such as digital additions to the video and providing other or additional sound recordings in a post production process.

In one example a method is provided that includes receiving a video including a plurality of frames and the video has a first animation. The method also includes determining an amount of change per pixel across a portion of the plurality of frames and based on the amount of change per pixel generating a first masking frame indicative of modifiable pixels and a second masking frame indicative of static background pixels. The method also includes applying the first masking frame to frames of the plurality of frames to generate a plurality of partial frames and applying the second masking frame to a given frame to generate a background template frame. The method also includes combining the background template frame with each of the plurality of partial frames to generate a plurality of altered frames and processing the plurality of altered frames into a second animation.

In another example a computer readable medium having stored therein instructions that when executed by a computing device cause the computing device to perform functions is provided. The functions may comprise receiving a video including a plurality of frames and the video has a first animation. The functions may also comprise determining an amount of change per pixel across a portion of the plurality of frames and based on the amount of change per pixel generating a first masking frame indicative of modifiable pixels and a second masking frame indicative of static background pixels. The functions further comprise applying the first masking frame to frames of the plurality of frames to generate a plurality of partial frames and applying the second masking frame to a given frame to generate a background template frame. The functions also comprise combining the background template frame with each of the plurality of partial frames to generate a plurality of altered frames and processing the plurality of altered frames into a second animation.

In a further example a system is provided that comprises at least one processor and data storage having stored therein instructions executable by the processor to perform functions. The functions comprise receiving a video including a plurality of frames and the video has a first animation. The functions also comprise determining an amount of change per pixel across a portion of the plurality of frames and based on the amount of change per pixel generating a first masking frame indicative of modifiable pixels and a second masking frame indicative of static background pixels. The functions also comprise applying the first masking frame to frames of the plurality of frames to generate a plurality of partial frames and applying the second masking frame to a given frame to generate a background template frame. The functions further comprise combining the background template frame with each of the plurality of partial frames to generate a plurality of altered frames and processing the plurality of altered frames into a second animation.

The foregoing summary is illustrative only and is not intended to be in any way limiting. In addition to the illustrative aspects embodiments and features described above further aspects embodiments and features will become apparent by reference to the figures and the following detailed description.

The following detailed description describes various features and functions of the disclosed systems and methods with reference to the accompanying figures. In the figures similar symbols identify similar components unless context dictates otherwise. The illustrative system and method embodiments described herein are not meant to be limiting. It may be readily understood that certain aspects of the disclosed systems and methods can be arranged and combined in a wide variety of different configurations all of which are contemplated herein.

The disclosure generally describes among other items examples for using frames from a video and removing selected movement from portions of scenes to create interesting effects. In one example a video may be received in which portions of a scene may not be moving or may only be moving slightly. Frames of the video may be aligned to remove movements of a camera that captured the video. Frames can be compared to each other to determine an amount of change per pixel across a portion of the plurality of frames. Based on the amount of change per pixel a first masking frame indicative of modifiable pixels may be generated. For example a mask that assigns a value to each pixel quantifying the amount of change across all frames is generated. Example values include assigning the value of the variance of the corresponding pixel across all frames. The variance calculation may depend on a choice of color distance metric to use to describe the pixel and one example includes a Euclidean color metric. The modifiable pixels may indicate those pixels to be carried forward in the animation.

In additional examples a second masking frame indicative of static background pixels can also be generated. The first masking frame can be applied to all frames of the plurality of frames to generate a plurality of partial frames and the second masking frame can be applied to a given frame to generate a background template frame. The background template frame can be combined with each of the plurality of partial frames to generate a plurality of altered frames and the plurality of altered frames can be processed into a second animation.

In some examples the background template frame can be chosen to represent non moving parts of the scene video. This may be arbitrarily selected or may be determined based on an average of a few frames. Then to generate the altered frames for each frame and for each pixel with a low mask value the pixel can be replaced with the value of the background template frame. For each pixel with a high mask value the original pixel value can be used.

Referring now to a block diagram of a video editing system is illustrated. The video editing system includes a frame alignment engine a masking frame engine and a video animation engine . The video editing system may be configured to receive a video and to perform video editing processes on the video. For example the frame alignment engine may estimate a path of a camera that recorded the video based on motion of objects within the received video to align frames within the video. The masking frame engine may then determine masking frames to generate a background frame template and customizable frame templates and the video animation engine may recast the received video according to animation based on modifications of the frames by the masking frame engine .

One or more of the described functions or components of the system may be divided up into additional functional or physical components or combined into fewer functional or physical components. In some further examples additional functional and or physical components may be added to the examples illustrated by . Still further any of the frame alignment engine the masking frame engine and or the video animation engine may include or be provided in the form of a processor e.g. a micro processor a digital signal processor DSP etc. configured to execute program code including one or more instructions for implementing logical functions described herein. The system may further include any type of computer readable medium non transitory medium for example such as a storage device including a disk or hard drive to store the program code. In other examples the camera path translation system may be included within other systems.

Turning to the individual entities illustrated on each client A N may be used by a user to request video hosting services. For example a user can use the client A to send a request for uploading a video for sharing or playing a video. The clients A N can be any type of computer device such as a personal computer e.g. desktop notebook tablet laptop computer as well as devices such as a mobile telephone personal digital assistant or IP enabled video player. The clients A N may include a processor a display device or output to a display device and a local storage such as a hard drive or flash memory device to which the clients A N store data used by the user in performing tasks and a network interface for coupling to the video hosting service via the network .

The clients A N may include a video player A N for playing a video stream. The video player A N may be a standalone application or a plug in to another application such as a network or Internet browser. Where the client A N is a general purpose device e.g. a desktop computer mobile phone the player A N may be implemented as software executed by the computer. Where the client A N is a dedicated device e.g. a dedicated video player the player A N may be implemented in hardware or a combination of hardware and software. The player A N may include user interface controls and corresponding application programming interfaces for selecting a video feed starting stopping and rewinding a video feed. Also the player A N can include in a user interface a video display format selection configured to indicate a video display format e.g. a standard definition TV or a high definition TV . Other types of user interface controls e.g. buttons keyboard controls can be used as well to control the playback and video format selection functionality of the player A N.

The network enables communications between the clients A N and the video hosting service . In one embodiment the network is the Internet and uses standardized internetworking communications technologies and protocols known now or subsequently developed that enable the clients A N to communicate with the video hosting service . In another embodiment the network may be a wireless cellular network that enables wireless communication between the clients A N and the video hosting service .

The video hosting service comprises the video editing system a video server an ingest server and a video database . The video server may be configured to serve videos from the video database in response to user video hosting service requests. The ingest server may be configured to receive user uploaded videos and store the videos in the video database . The video database may be configured to store user uploaded videos and videos processed by the camera path translation system . In one embodiment the video database stores a large video corpus.

The video editing system may include a frame alignment engine a masking frame engine and a video animation engine . The video editing system may be configured to receive user uploaded videos from the ingest server and to perform editing of the videos based on instructions received from users from other devices or based on programmed instructions.

In one example the video editing system may process videos to provide altered videos that have a limited or partial movement as compared to original received videos. Regions of frames of videos e.g. selected pixels or modifiable pixels may be determined and content within the regions may be maintained. A remaining portion of frames of the video may be removed and replaced with background content. In such examples a resulting video of altered frames may illustrate partial movement as compared to the originally received video.

In addition for the method and other processes and methods disclosed herein the flowchart shows functionality and operation of one possible implementation of present embodiments. In this regard each block may represent a module a segment or a portion of program code which includes one or more instructions executable by a processor for implementing specific logical functions or steps in the process. The program code may be stored on any type of computer readable medium for example such as a storage device including a disk or hard drive. The computer readable medium may include a non transitory computer readable medium for example such as computer readable media that stores data for short periods of time like register memory processor cache and Random Access Memory RAM . The computer readable medium may also include non transitory media such as secondary or persistent long term storage like read only memory ROM optical or magnetic disks compact disc read only memory CD ROM for example. The computer readable media may also be any other volatile or non volatile storage systems. The computer readable medium may be considered a computer readable storage medium a tangible storage device or other article of manufacture for example.

In addition for the method and other processes and methods disclosed herein each block in may represent circuitry that is wired to perform the specific logical functions in the process.

At block the method includes receiving a video including a plurality of frames. The video may be received by any computing device and in some examples the video may be received by a device configure to capture the video. In other instances the video may be captured by a camera and provided to another computing device.

As one example a video may be captured by a mobile phone that includes a camera and may have a given animation. Within the video parts of a scene may not be moving or may be moving slightly compared to other parts or objects in the scene. One example of this may include a beach scene in which waves are moving across the water and a sea crab is walking across the beach. Other parts of the scene include the sky or trees may not be moving. Movement of portions of the scene may be interpreted due to pixels changing which includes lighting changes as well as movement of an object.

At block the method includes determining an amount of change per pixel across a portion of the plurality of frames. As an example pixels may be compared to determine movement across the pixels and movement may be determined due to change across the pixels. Pixels within one frame of the video may be compared to corresponding pixels of a subsequent frame to identify changes per pixel.

In one example pixels may be assigned values and a difference in values of corresponding pixels from frame to frame can be determined to identify where a difference is largest. A largest amount of difference between pixels of corresponding frames may indicate where within the frame a largest amount of movement may be found.

Any type of value may be assigned to a pixel and the value may be used to describe content of the pixel. In one example pixel values may include color metric values and an amount of change per pixel value may be determined based on Euclidean distances between color metric values.

In other examples a pixel value may include information indicating how bright that pixel is to be displayed and or what color the pixel should display. In some examples a pixel value may be a 1 bit number indicating either foreground or background. In other examples such as for a grayscale image a pixel value may be a number that represents a brightness of the pixel. Still further a pixel value may be an 8 bit integer giving a range of possible values from 0 to 255 where zero may be considered to be black and 255 may be considered to be white along the color scale so that values in between comprise different shades of gray. To represent color images separate red green and blue components may be specified for each pixel and so the pixel value may be a vector of three numbers. In yet further examples actual grayscale or color component intensities for each pixel may not actually be stored explicitly and what is stored for each pixel can be an index to a color map in which actual intensity or colors can be looked up.

In another example an amount of change per pixel across the portion of the plurality of frames may be determined based on a variance of a pixel value across the portion of the plurality of frames of a corresponding pixel. Variance between color values of a pixel across frames can be determined and characterized as an amount of change per pixel. A mathematical variance between color values of pixels across frames is one way to determine an amount of change and any of the example methods described herein may be used instead or in combination to determine or characterize an amount of change per pixel.

In some examples prior to determining an amount of change per pixel across frames frames of the video may be aligned. For example frames may be temporally aligned or in some examples aligned based on a time of recording. Frames may also be aligned based on spatial aspects of content of the frame such as location of pixel content. Many methods exist for alignment of video frames and one of ordinary skill in the art will appreciate that any of known methods may be used for frame alignment.

At block the method includes generating a first masking frame indicative of modifiable pixels and a second masking frame indicative of static background pixels. For example based on the amount of change per pixel a map may be generated indicating where frames include movement and pixels in the map may be characterized to indicate an amount of movement per pixel. For pixels that are characterized as having movement above a threshold e.g. referring to the beach scene pixels including the sea crab walking across the scene may be determined to have movement above a threshold those pixels may be classified as modifiable pixels. A first masking frame may classify pixels as modifiable pixels so as to indicate content from pixels to be included within a resulting video. Other pixels in the first masking frame not classified as modifiable pixels may be set to null.

For remaining pixels that do not have movement characterized as being above a threshold amount those pixels may be classified as static background pixels within a second masking frame. Other pixels in the second masking frame not classified as static background pixels may be set to unity. Thus in some examples pixel values per pixel may be compared to a threshold and based on the comparison the first masking frame and the second masking frame may be generated. Information indicative of the threshold may be received from a user input and a value of the threshold may characterize a number of pixels to include as the modifiable pixels.

In some examples pixel values of modifiable pixels of the first masking frame may be set to one and pixel values of the static background pixels of the second masking frame may be set to zero. Thus the second masking frame may be an inverse of the first masking frame.

In some examples the method may also include receiving a selection of one or more pixels to include as the modifiable pixels or a selection of one or more pixels to include as the static background pixels. Thus a user may preset portions of frames as background or moving objects.

At block the method includes applying the first masking frame to frames of the plurality of frames to generate a plurality of partial frames. At block the method includes applying the second masking frame to a given frame to generate a background template frame. To apply the masking frames to frames of the video the masking frames may be multiplied by the frames of the video.

The first masking frame may be multiplied by each frame of the video to identify pixels to use from the frames of the video that identify moving parts of the image for example. And because the second masking frame indicates background static pixels the second masking frame may be multiplied by a frame of the video to identify pixels of the frame to be used as background.

Any frame may be chosen to be applied to the second masking frame to generate the background template frame. In one instance a selection of the given frame from the plurality of frames to use to generate the background template frame may be received or may be arbitrary. In another example an average of a few frames may be used to generate a background template frame. For instance the second masking frame may be applied to a number of frames and a resulting average of the number of frames may be used as the background template frame.

In one example applying the first masking frame to frames of the plurality of frames to generate a plurality of partial frames includes for each frame of the plurality of frames using the value of the pixel as a given value for the modifiable pixels indicated by the first masking frame to generate a given partial frame. As mentioned values of the modifiable pixels in the first masking frame may be set to one and thus application e.g. multiplication of the first masking frame by the other frames results in content of the pixels corresponding to the modifiable pixels in the frames passing to the partial frames.

At block the method includes combining the background template frame with each of the plurality of partial frames to generate a plurality of altered frames. In one example the frames may be combined by adding content of the frames to each other.

In addition combining the background template frame with each of the plurality of partial frames to generate a plurality of altered frames includes for the altered frames replacing the value of pixels with a corresponding pixel value of the background template frame for the static background pixels indicated by the second masking frame.

At block the method includes processing the plurality of altered frames into a second animation. In one example the second animation illustrates movement across the plurality of altered frames only within pixels characterized by the modifiable pixels of the first masking frame. In other examples the second animation illustrates background movement as indicated by the background template frame and other movement as indicated by the altered frames.

In some examples the method may be performed to compare frames of a video and create a mask that assigns a value to each pixel of a given frame so as to quantify an amount of change across frames. A user may have an option to modify the mask for example by indicating areas for static pixels and areas for modifiable pixels.

In some examples the method may be performed by a server. A video may be uploaded to the server and the server may edit the video to provide a subsequent video with partial animation.

In one example for pixel P1 a Euclidean color metric may be used. Taking example values of P1 for each frame to be P1 F1 1 1 0 P1 F2 1 0 0 and P1 F3 1 1 0 then a mean of P1 across frames may be calculated as follows 1 1 0 1 0 0 1 1 0 1 0 A variance of P1 for each of frames F1 F2 and F3 may then be calculated as follows var F1 1 1 1 0 0 1 27 var F2 1 1 0 0 0 4 27 var F3 1 1 1 0 0 1 27 A resulting variance of P1 across frames F1 through F3 may then be calculated as follows var F1 var F2 var F3 6 27 0.2222 Thus the pixel value assigned to pixel P1 may be 0.22 or the pixel value determined as a result of a change in content of data for a pixel location at P1 may be 0.22 as shown in . A variance may be calculated across any number of frames and used as a pixel value for example.

In one example illustrates masking M1 with such variance values for each pixel and pixel values for pixel P1 are shown to be 0.22 and other example pixel values are shown as well.

A value of the threshold may be preset received from a user selection or determined based on a histogram analysis or variance minimization of all values of pixels for example.

As seen in values of pixels in masking frames M2 and M3 are based on the amount of change per pixel across frames F1 to F3. For example those pixels that have values above a threshold are set to a unity value in masking frame M2 and a null value in masking frame M3. Masking frame M2 may be considered an action masking frame since pixel locations are identified as unity value for those identified as including movement or lighting changes. Masking frame M3 may be considered a background masking frame since pixels locations are identified as null for those identified as including movement or lighting changes.

Content in the empty pixel locations is replaced by respective content for frames F1 F2 and F3 by applying the masking frame M2 to frames F1 F2 and F3 to generate partial frames F1 M2 F2 M2 and F3 M2 and the partial frames are combined with the background template frame to generate altered resulting frames F1 F2 and F3 as follows 1 12 13 2 22 13 3 32 13 

Thus as shown in altered resulting frames F1 F2 and F3 include content of the background frame at pixel locations identified as unity in masking frame M3 and content from the respective original frame at pixels identified as unity in masking frame M2 e.g. modifiable pixel locations . This preserves content identified as moving across the frame sequence of frames for example.

Subsequently each of altered resulting frames F1 F2 and F3 may be processed into an animation by recombining the frames into a video. The video may illustrate partial motion movement lighting changes or other changes per pixel as compared to the original video. Using the example methods described herein selected movement in a video may be highlighted and maintained across frames while other movement e.g. movement of a lower amount may be removed. As shown from content in pixels of the second and third rows and second and third columns of the 6 6 frame changes from frame F1 to F2 to F3 and content within all remaining pixels remains constant. An amount of pixels within which content may change can be determined based on a value of the threshold of masking frame M1 for example.

Example methods described herein may be performed in any manner to select highlight or maintain portions of video and effectively remove any movement or changes in the video of other portions.

Depending on the desired configuration the system memory can be of any type including but not limited to volatile memory such as RAM non volatile memory such as ROM flash memory etc. or any combination thereof. System memory may include one or more applications and program data . Application may include a video algorithm that is arranged to provide inputs to the electronic circuits in accordance with the present disclosure. Program Data may include video content information that could be directed to any number of types of data. In some example embodiments application can be arranged to operate with program data on an operating system.

Computing device can have additional features or functionality and additional interfaces to facilitate communications between the basic configuration and any devices and interfaces. For example data storage devices can be provided including removable storage devices non removable storage devices or a combination thereof. Examples of removable storage and non removable storage devices include magnetic disk devices such as flexible disk drives and hard disk drives HDD optical disk drives such as compact disk CD drives or digital versatile disk DVD drives solid state drives SSD and tape drives to name a few. Computer storage media can include volatile and nonvolatile non transitory removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data.

System memory and storage devices are examples of computer storage media. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by computing device . Any such computer storage media can be part of device .

Computing device can also include output interfaces that may include a graphics processing unit which can be configured to communicate to various external devices such as display devices or speakers via one or more A V ports or a communication interface . The communication interface may include a network controller which can be arranged to facilitate communications with one or more other computing devices over a network communication via one or more communication ports . The communication connection is one example of a communication media. Communication media may be embodied by computer readable instructions data structures program modules and includes any information delivery media. By way of example and not limitation communication media can include wired media such as a wired network or direct wired connection and wireless media such as acoustic radio frequency RF infrared IR and other wireless media.

Computing device can be implemented as a portion of a small form factor portable or mobile electronic device such as a cell phone a personal data assistant PDA a personal media player device a wireless web watch device a personal headset device an application specific device or a hybrid device that include any of the above functions. Computing device can also be implemented as a personal computer including both laptop computer and non laptop computer configurations.

Each of the frame elements and and the extending side arms may be formed of a solid structure of plastic and or metal or may be formed of a hollow structure of similar material so as to allow wiring and component interconnects to be internally routed through the head mounted device . Other materials may be possible as well.

One or more of each of the lens elements may be formed of any material that can suitably display a projected image or graphic. Each of the lens elements may also be sufficiently transparent to allow a user to see through the lens element. Combining these two features of the lens elements may facilitate an augmented reality or heads up display where the projected image or graphic is superimposed over a real world view as perceived by the user through the lens elements.

The extending side arms may each be projections that extend away from the lens frames respectively and may be positioned behind ears of a user to secure the head mounted device to the user. The extending side arms may further secure the head mounted device to the user by extending around a rear portion of the head of the user. Additionally or alternatively for example the HMD may connect to or be affixed within a head mounted helmet structure. Other possibilities exist as well.

The HMD may also include an on board computing system a video camera a sensor and a finger operable touch pad . The on board computing system is shown to be positioned on the extending side arm of the head mounted device however the on board computing system may be provided on other parts of the head mounted device or may be positioned remote from the head mounted device e.g. the on board computing system could be wire or wirelessly connected to the head mounted device . The on board computing system may include a processor and memory for example. The on board computing system may be configured to receive and analyze data from the video camera and the finger operable touch pad and possibly from other sensory devices user interfaces or both and generate images for output by the lens elements and .

The video camera is shown positioned on the extending side arm of the head mounted device however the video camera may be provided on other parts of the head mounted device . The video camera may be configured to capture images at various resolutions or at different frame rates. Many video cameras with a small form factor such as those used in cell phones or webcams for example may be incorporated into an example of the HMD .

Further although illustrates one video camera more video cameras may be used and each may be configured to capture the same view or to capture different views. For example the video camera may be forward facing to capture at least a portion of the real world view perceived by the user. This forward facing image captured by the video camera may then be used to generate an augmented reality where computer generated images appear to interact with the real world view perceived by the user.

The sensor is shown on the extending side arm of the head mounted device however the sensor may be positioned on other parts of the head mounted device . The sensor may include one or more of a gyroscope or an accelerometer for example. Other sensing devices may be included within or in addition to the sensor or other sensing functions may be performed by the sensor .

The finger operable touch pad is shown on the extending side arm of the head mounted device . However the finger operable touch pad may be positioned on other parts of the head mounted device . Also more than one finger operable touch pad may be present on the head mounted device . The finger operable touch pad may be used by a user to input commands. The finger operable touch pad may sense at least one of a position and a movement of a finger via capacitive sensing resistance sensing or a surface acoustic wave process among other possibilities. The finger operable touch pad may be capable of sensing finger movement in a direction parallel or planar to the pad surface in a direction normal to the pad surface or both and may also be capable of sensing a level of pressure applied to the pad surface. The finger operable touch pad may be formed of one or more translucent or transparent insulating layers and one or more translucent or transparent conducting layers. Edges of the finger operable touch pad may be formed to have a raised indented or roughened surface so as to provide tactile feedback to a user when the finger of a user reaches the edge or other area of the finger operable touch pad . If more than one finger operable touch pad is present each finger operable touch pad may be operated independently and may provide a different function.

The lens elements may act as a combiner in a light projection system and may include a coating that reflects the light projected onto them from the projectors . In some embodiments a reflective coating may not be used e.g. when the projectors are scanning laser devices .

In alternative embodiments other types of display elements may also be used. For example the lens elements themselves may include a transparent or semi transparent matrix display such as an electroluminescent display or a liquid crystal display one or more waveguides for delivering an image to the eyes of the user or other optical elements capable of delivering an in focus near to eye image to the user. A corresponding display driver may be disposed within the frame elements for driving such a matrix display. Alternatively or additionally a laser or LED source and scanning system could be used to draw a raster display directly onto the retina of one or more eyes of the user. Other possibilities exist as well.

As shown in the HMD may include a single display which may be coupled to the device. The display may be formed on one of the lens elements of the HMD such as a lens element described with respect to and may be configured to overlay computer generated graphics in the physical world view of the user. The display is shown to be provided in a center of a lens of the HMD however the display may be provided in other positions. The display is controllable via the computing system that is coupled to the display via an optical waveguide .

The HMD may include a single lens element that may be coupled to one of the side arms or the center frame support . The lens element may include a display such as the display described with reference to and may be configured to overlay computer generated graphics upon the physical world view of the user. In one example the single lens element may be coupled to the inner side i.e. the side exposed to a portion of a head of a user when worn by the user of the extending side arm . The single lens element may be positioned in front of or proximate to an eye of the user when the HMD is worn by a user. For example the single lens element may be positioned below the center frame support as shown in .

In some embodiments the disclosed methods may be implemented as computer program instructions encoded on a computer readable storage media in a machine readable format or on other non transitory media or articles of manufacture. is a schematic illustrating a conceptual partial view of an example computer program product that includes a computer program for executing a computer process on a computing device arranged according to at least some embodiments presented herein. In one embodiment the example computer program product is provided using a signal bearing medium . The signal bearing medium may include one or more program instructions that when executed by one or more processors may provide functionality or portions of the functionality described above with respect to . Thus for example referring to the embodiments shown in one or more features of blocks may be undertaken by one or more instructions associated with the signal bearing medium . In addition the program instructions in describe example instructions as well.

In some examples the signal bearing medium may encompass a computer readable medium such as but not limited to a hard disk drive a Compact Disc CD a Digital Video Disk DVD a digital tape memory etc. In some implementations the signal bearing medium may encompass a computer recordable medium such as but not limited to memory read write R W CDs R W DVDs etc. In some implementations the signal bearing medium may encompass a communications medium such as but not limited to a digital and or an analog communication medium e.g. a fiber optic cable a waveguide a wired communications link a wireless communication link etc. . Thus for example the signal bearing medium may be conveyed by a wireless form of the communications medium e.g. a wireless communications medium conforming with the IEEE 802.11 standard or other transmission protocol .

The one or more programming instructions may be for example computer executable and or logic implemented instructions. In some examples a computing device such as the computing device of may be configured to provide various operations functions or actions in response to the programming instructions conveyed to the computing device by one or more of the computer readable medium the computer recordable medium and or the communications medium .

It should be understood that arrangements described herein are for purposes of example only. As such those skilled in the art will appreciate that other arrangements and other elements e.g. machines interfaces functions orders and groupings of functions etc. can be used instead and some elements may be omitted altogether according to the desired results. Further many of the elements that are described are functional entities that may be implemented as discrete or distributed components or in conjunction with other components in any suitable combination and location.

While various aspects and embodiments have been disclosed herein other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting with the true scope being indicated by the following claims along with the full scope of equivalents to which such claims are entitled. It is also to be understood that the terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting.

