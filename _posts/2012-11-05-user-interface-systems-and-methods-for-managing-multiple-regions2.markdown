---

title: User interface systems and methods for managing multiple regions
abstract: A user interface system includes a plane registration module configured to identify a first plane within an environment, and a gesture and posture recognition (GPR) module configured to observe a first allocation gesture, a second allocation gesture, a first modal gesture, a second modal gesture, and a third modal gesture within the environment. A region definition module is configured to determine a first region comprising a first portion of the first plane based on the first allocation gesture, and to determine a second region comprising a second portion of the first plane based on the second allocation gesture. A mode determination module is configured to determine different interaction modes for the various regions. A visual feedback module is configured to provide visual feedback associated with a parameter of the first region.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09575562&OS=09575562&RS=09575562
owner: Synaptics Incorporated
number: 09575562
owner_city: San Jose
owner_country: US
publication_date: 20121105
---
This invention generally relates to electronic devices and more specifically relates to user interfaces associated with such electronic devices.

Recent years have seen an increased interest in advanced user interfaces particularly those used in connection with personal computers tablet computers smart phone devices and other electronic systems. Input devices often used in connection with such systems include proximity sensor devices also commonly called touchpads or touch sensor devices . A proximity sensor device typically includes a sensing region often demarked by a surface in which the proximity sensor device determines the presence location and or motion of one or more input objects. Proximity sensor devices may be used to provide interfaces for the electronic system. For example proximity sensor devices are often used as input devices for larger computing systems such as opaque touchpads integrated in or peripheral to notebook or desktop computers .

It is sometimes desirable to provide a virtual user interface experience through the use of interface components that mimic the operation of physical user interface devices such as touchpads keyboards and the like. Such virtual user interfaces might observe the gestures and or the postures made by a user s body e.g. the user s hand in a two dimensional or three dimensional space.

Presently known virtual user interfaces are unsatisfactory in a number of respects. For example user interfaces that make use of three dimensional space are seen as fatiguing and ergonomically inefficient. Furthermore real time interpretation of gestures and postures in three dimensions is computationally difficult and can lead to misinterpreting a user s actions i.e. reading unintentional gestures as intentional gestures. This is often referred to as the live mic problem.

Accordingly there is a need for improved virtual user interfaces that address these and other limitations of the prior art.

A user interface system in accordance with one embodiment includes a plane registration module a gesture and posture recognition GPR module a region definition module and a mode determination module. The plane registration module is configured to identify a first plane within an environment. The GPR module is configured to observe a first allocation gesture a second allocation gesture a first modal gesture a second modal gesture and a third modal gesture within the environment. A region definition module is configured to determine a first region comprising a first portion of the first plane based on the first allocation gesture and to determine a second region comprising a second portion of the first plane based on the second allocation gesture. A mode determination module is configured to determine a first interaction mode of the first region based on the first modal gesture determine a first interaction mode of the second region based on the second modal gesture and to determine a second interaction mode of the first region based on the third modal gesture wherein the first interaction mode of the first region and the second interaction mode of the first region are different. A visual feedback module including visual feedback circuitry the visual feedback module configured to provide visual feedback associated with a parameter of the first region.

A user interface method in accordance with one embodiment includes identifying a first plane within an environment observing a first allocation gesture a second allocation gesture a first modal gesture a second modal gesture and a third modal gesture within the environment determining a first region comprising a first portion of the first plane based on the first allocation gesture determining a second region comprising a second portion of the first plane based on the second allocation gesture determining a first interaction mode of the first region based on the first modal gesture determining a first interaction mode of the second region based on the second modal gesture determining a second interaction mode of the first region based on the third modal gesture wherein the first interaction mode of the first region and the second interaction mode of the first region are different and providing via a processor visual feedback associated with a parameter of the first region.

The following detailed description presents a number of example embodiments and is not intended to limit the invention or the application and uses of the invention. Furthermore there is no intention to be bound by any expressed or implied theory presented in the preceding technical field background brief summary or the following detailed description.

Various embodiments of the present invention provide input devices and human computer interaction HCI methods that facilitate improved usability. Such input devices may be roughly classified as physical or virtual as described in further detail below however it will be appreciated that some input devices may include both physical and virtual aspects. Accordingly it is helpful to first discuss the nature of an example input device.

The input device can be implemented as a physical part of the electronic system or can be physically separate from the electronic system. As appropriate the input device may communicate with parts of the electronic system using any one or more of the following buses networks and other wired or wireless interconnections. Examples include IC SPI PS 2 Universal Serial Bus USB Bluetooth RF and IRDA.

In the input device is shown as a proximity sensor device also often referred to as a touchpad or a touch sensor device configured to sense input provided by one or more input objects in a sensing region . Example input objects include fingers and styli as shown in .

Sensing region encompasses any space above around in and or near the input device in which the input device is able to detect user input e.g. user input provided by one or more input objects . The sizes shapes and locations of particular sensing regions may vary widely from embodiment to embodiment. In some embodiments the sensing region extends from a surface of the input device in one or more directions into space until signal to noise ratios prevent sufficiently accurate object detection. The distance to which this sensing region extends in a particular direction in various embodiments may be on the order of less than a millimeter millimeters centimeters or more and may vary significantly with the type of sensing technology used and the accuracy desired. Thus some embodiments sense input that comprises no contact with any surfaces of the input device contact with an input surface e.g. a touch surface of the input device contact with an input surface of the input device coupled with some amount of applied force or pressure and or a combination thereof. In various embodiments input surfaces may be provided by surfaces of casings within which sensor electrodes reside by face sheets applied over the sensor electrodes or any casings etc. In some embodiments the sensing region has a rectangular shape when projected onto an input surface of the input device .

The input device may utilize any combination of sensor components and sensing technologies to detect user input in the sensing region . The input device comprises one or more sensing elements for detecting user input. As several non limiting examples the input device may use capacitive elastive resistive inductive magnetic acoustic ultrasonic and or optical techniques.

Some implementations are configured to provide images that span one two three or higher dimensional spaces. Some implementations are configured to provide projections of input along particular axes or planes.

In some resistive implementations of the input device a flexible and conductive first layer is separated by one or more spacer elements from a conductive second layer. During operation one or more voltage gradients are created across the layers. Pressing the flexible first layer may deflect it sufficiently to create electrical contact between the layers resulting in voltage outputs reflective of the point s of contact between the layers. These voltage outputs may be used to determine positional information.

In some inductive implementations of the input device one or more sensing elements pick up loop currents induced by a resonating coil or pair of coils. Some combination of the magnitude phase and frequency of the currents may then be used to determine positional information.

In some capacitive implementations of the input device voltage or current is applied to create an electric field. Nearby input objects cause changes in the electric field and produce detectable changes in capacitive coupling that may be detected as changes in voltage current or the like.

Some capacitive implementations utilize arrays or other regular or irregular patterns of capacitive sensing elements to create electric fields. In some capacitive implementations separate sensing elements may be ohmically shorted together to form larger sensor electrodes. Some capacitive implementations utilize resistive sheets which may be substantially uniformly resistive.

Some capacitive implementations utilize self capacitance or absolute capacitance sensing methods based on changes in the capacitive coupling between sensor electrodes and an input object. In various embodiments an input object near the sensor electrodes alters the electric field near the sensor electrodes thus changing the measured capacitive coupling. In one implementation an absolute capacitance sensing method operates by modulating sensor electrodes with respect to a reference voltage e.g. system ground and by detecting the capacitive coupling between the sensor electrodes and input objects.

Some capacitive implementations utilize mutual capacitance or transcapacitance sensing methods based on changes in the capacitive coupling between sensor electrodes. In various embodiments an input object near the sensor electrodes alters the electric field between the sensor electrodes thus changing the measured capacitive coupling. In one implementation a transcapacitive sensing method operates by detecting the capacitive coupling between one or more transmitter sensor electrodes also transmitter electrodes or transmitters and one or more receiver sensor electrodes also receiver electrodes or receivers . Transmitter sensor electrodes may be modulated relative to a reference voltage e.g. system ground to transmit transmitter signals. Receiver sensor electrodes may be held substantially constant relative to the reference voltage to facilitate receipt of resulting signals. A resulting signal may comprise effect s corresponding to one or more transmitter signals and or to one or more sources of environmental interference e.g. other electromagnetic signals . Sensor electrodes may be dedicated transmitters or receivers or may be configured to both transmit and receive.

The pattern of sensor electrodes may be arranged substantially parallel to each other substantially perpendicular to each other or arranged in any other suitable pattern. Sensor electrodes are typically ohmically isolated from each other. In some embodiments such sensor electrodes are separated from each by one or more substrates. For example they may be disposed on opposite sides of the same substrate or on different substrates that are laminated together. Some sensor electrodes may be configured as receiver electrodes while other sensor electrodes are configured as transmitter electrodes. The capacitive coupling between the transmitter electrodes and receiver electrodes change with the proximity and motion of input objects in the sensing region associated with the transmitter electrodes and receiver electrodes.

The receiver sensor electrodes may be operated singly or multiply to acquire resulting signals. The resulting signals may be used to determine a capacitive frame representative of measurements of the capacitive couplings. Multiple capacitive frames may be acquired over multiple time periods and differences between them used to derive information about input in the sensing region. For example successive capacitive frames acquired over successive periods of time can be used to track the motion s of one or more input objects entering exiting and within the sensing region.

Referring again to a processing system is shown as part of the input device . The processing system is configured to operate the hardware of the input device to detect input in the sensing region . The processing system comprises parts of or all of one or more integrated circuits ICs and or other circuitry components. For example as described in further detail below a processing system for a mutual capacitance sensor device may comprise transmitter circuitry configured to transmit signals with transmitter sensor electrodes and or receiver circuitry configured to receive signals with receiver sensor electrodes .

In some embodiments the processing system also comprises electronically readable instructions such as firmware code software code and or the like. In some embodiments components composing the processing system are located together such as near sensing element s of the input device . In other embodiments components of processing system are physically separate with one or more components close to sensing element s of input device and one or more components elsewhere. For example the input device may be a peripheral coupled to a desktop computer and the processing system may comprise software configured to run on a central processing unit of the desktop computer and one or more ICs perhaps with associated firmware separate from the central processing unit. As another example the input device may be physically integrated in a phone and the processing system may comprise circuits and firmware that are part of a main processor of the phone. In some embodiments the processing system is dedicated to implementing the input device . In other embodiments the processing system also performs other functions such as operating display screens driving haptic actuators etc.

The processing system may be implemented as a set of modules that handle different functions of the processing system . Each module may comprise circuitry that is a part of the processing system firmware software or a combination thereof. In various embodiments different combinations of modules may be used. Example modules include hardware operation modules for operating hardware such as sensor electrodes and display screens data processing modules for processing data such as sensor signals and positional information and reporting modules for reporting information. Further example modules include sensor operation modules configured to operate sensing element s to detect input identification modules configured to identify gestures such as mode changing gestures and mode changing modules for changing operation modes.

In some embodiments the processing system responds to user input or lack of user input in the sensing region directly by causing one or more actions. Example actions include changing operation modes as well as GUI actions such as cursor movement selection menu navigation and other functions. In some embodiments the processing system provides information about the input or lack of input to some part of the electronic system e.g. to a central processing system of the electronic system that is separate from the processing system if such a separate central processing system exists . In some embodiments some part of the electronic system processes information received from the processing system to act on user input such as to facilitate a full range of actions including mode changing actions and GUI actions.

For example in some embodiments the processing system operates the sensing element s of the input device to produce electrical signals indicative of input or lack of input in the sensing region . The processing system may perform any appropriate amount of processing on the electrical signals in producing the information provided to the electronic system. For example the processing system may digitize analog electrical signals obtained from the sensor electrodes. As another example the processing system may perform filtering or other signal conditioning. As yet another example the processing system may subtract or otherwise account for a baseline such that the information reflects a difference between the electrical signals and the baseline. As yet further examples the processing system may determine positional information recognize inputs as commands recognize handwriting and the like. In one embodiment processing system includes determination circuitry configured to determine positional information for an input device based on the measurement.

 Positional information as used herein broadly encompasses absolute position relative position velocity acceleration and other types of spatial information. Example zero dimensional positional information includes near far or contact no contact information. Example one dimensional positional information includes positions along an axis. Example two dimensional positional information includes motions in a plane. Example three dimensional positional information includes instantaneous or average velocities in space. Further examples include other representations of spatial information. Historical data regarding one or more types of positional information may also be determined and or stored including for example historical data that tracks position motion or instantaneous velocity over time.

In some embodiments the input device is implemented with additional input components that are operated by the processing system or by some other processing system. These additional input components may provide redundant functionality for input in the sensing region or some other functionality. shows buttons near the sensing region that can be used to facilitate selection of items using the input device . Other types of additional input components include sliders balls wheels switches and the like. Conversely in some embodiments the input device may be implemented with no other input components.

In some embodiments the input device comprises a touch screen interface and the sensing region overlaps at least part of an active area of a display screen. For example the input device may comprise substantially transparent sensor electrodes overlaying the display screen and provide a touch screen interface for the associated electronic system. The display screen may be any type of dynamic display capable of displaying a visual interface to a user and may include any type of light emitting diode LED organic LED OLED cathode ray tube CRT liquid crystal display LCD plasma electroluminescence EL or other display technology. The input device and the display screen may share physical elements. For example some embodiments may utilize some of the same electrical components for displaying and sensing. As another example the display screen may be operated in part or in total by the processing system .

It should be understood that while many embodiments of the invention are described in the context of a fully functioning apparatus the mechanisms of the present invention are capable of being distributed as a program product e.g. software in a variety of forms. For example the mechanisms of the present invention may be implemented and distributed as a software program on information bearing media that are readable by electronic processors e.g. non transitory computer readable and or recordable writable information bearing media readable by the processing system . Additionally the embodiments of the present invention apply equally regardless of the particular type of medium used to carry out the distribution. Examples of non transitory electronically readable media include various discs memory sticks memory cards memory modules and the like. Electronically readable media may be based on flash optical magnetic holographic or any other storage technology.

Operation of user interface system of will now be described in conjunction with which depicts an example environment within a three dimensional space denoted by the Cartesian axes in . The environment depicted in may correspond for example to an office environment a vehicle environment an aircraft environment or any other environment in which user interface systems may be employed.

Plane registration module includes any combination of hardware and or software configured to identify planes within an environment. For example plane registration module might include one or more cameras one or more projectors and or lasers e.g. to project images onto an identified plane one or more distance sensors e.g. to determine points comprising a plane one or more motion sensors one or more microphones one or more processors e.g. CPUs or microcontrollers and suitable software configured to operate those components. In the interest of simplicity such components are not illustrated in . Plane registration module may be configured to identify planes within the environment automatically or with the assistance of a user i.e. through a registration process as described in further detail below.

Methods and systems for determining the shapes of objects in an environment e.g. identifying substantially planar surfaces are well understood and need not be described in further detail herein. For example certain systems such as Microsoft s KINECT system as well as open source alternatives to KINECT are widely available and have well documented application programming interfaces APIs .

Referring now to a plane has been identified by plane registration module . In this figure as well as the figures that follow plane is denoted by a dotted rectangle however it will be understood that this is merely a standard graphical convention and that in practice plane will typically extend outward in one or more directions. In this regard plane registration module may be configured to determine a boundary within plane . For example if plane is identified as substantially corresponding to the top of a desk a boundary of the desk within plane may be determined. While depicts a single horizontal plane any number of other planes may be identified in a particular environment as described in further detail below. The identified planes need not correspond to strictly planar surfaces. Surfaces with slight curvature or other insignificant non planarities may also be identified as may substantially planar portions of otherwise curvilinear or non planar surfaces e.g. a substantially planar region of the user s body .

GPR module includes any combination of hardware and or software configured to observe allocation gestures and modal gestures made by a user e.g. input object illustrated without loss of generality as a human hand within the environment. As used herein the term gesture or gestural refers to the movement of an input object such as one or more body parts and or one or more non anatomical objects such as a stylus or the like . Gestures include for example hand sweeps finger traces pinch movements and the like. In contrast the term posture or postural refers to the configuration of an input object irrespective of its movement. Postures include for example an L shape formation made with the thumb and index finger a clenched fist facial expressions and the like. In this regard the term gesture may be used without loss of generality to refer to both gestures and postures in some instances. For example in cases where an action takes place in response to a particular gesture e.g. a modal gesture in some contexts that action may also take place in response to a particular posture. In addition it will be appreciated that some user movements might include both postural and gestural components e.g. moving a hand while altering its finger configuration .

The term modal as used in connection with a gesture or posture refers to an indication of a desired interaction mode. Such interaction modes include for example a touch input mode e.g. a touchpad a key press interaction mode e.g. a number key or keyboard entry mode a handwritten text interaction mode and a display output mode e.g. resulting in the display of information on a plane . The invention is not so limited however and comprehends a wide range of interaction modes traditionally used in connection with user interfaces.

The term allocation as used in connection with a gesture or posture refers to an indication of how a plane should be allocated for subsequent interaction. A plane may for example be split by a boundary into a pair of adjacent regions. Alternatively multiple discrete regions may be defined as further detailed below.

The present invention comprehends that a wide range of gestures and postures may be used for determining interaction modes and for allocation of a plane. In one embodiment referring briefly to a wiping motion made by a hand or other object may be used as an allocation gesture. In another embodiment referring briefly to an L shape made by the thumb and index finger of hand may be used for allocation posture. In one embodiment referring briefly to a user may use the finger of a hand to draw an a character or any other character to indicate a text entry interaction mode. Similarly referring briefly to a roughly rectangular region may be traced out to indicate a touch input mode. In other embodiments modal gestures and allocation gestures may be characters associated with particular languages e.g. Chinese characters katakana characters Cyrillic characters or the like . More generally gestures may have shapes that are similar to evocative of or otherwise associated with a particular type of input system. For example a circle gesture might indicate a steering wheel input a cross might indicate a direction pad D pad game input the movement of fingers as if they were on sliders might indicate an audio mixer input and the drumming of fingers might indicate a drum or keyboard input. In general an appropriate gesture or posture might be mapped to any input surface or instrument panel. Therefore it will be appreciated that the particular gestures and or postures shown in are not intended to be limiting but are merely provided by way of example.

Referring again to GPR module is generally configured to observe allocation gestures made by a user within the planes identified by plane registration module . In that regard GPR module is preferably configured to ignore or otherwise disregard allocation gestures and modal gestures made in space at locations that are not sufficiently close to or do not substantially fall within the identified planes otherwise known as false positives . The determination of whether a gesture is sufficiently close to an identified plane may be predetermined or user configurable. Furthermore to prevent additional false positives the user may be able to specify e.g. through a plane registration process that certain identified planes not be considered for allocation or interaction.

Referring again to GPR module may be configured to observe any number of modal gestures and allocation gestures. In accordance with the illustrated example it will be assumed that GPR module observes at least a first allocation gesture a second allocation gesture a first modal gesture a second modal gesture and a third modal gesture.

Region definition module includes any combination of hardware and or software configured to determine a first interactive region or simply region comprising a first portion of a plane based on the first allocation gesture and to determine a second region comprising a second portion of the plane based on the second allocation gesture. Referring to for example first region comprises a portion of plane and second region also comprises a portion of plane . While regions and are shown as generally rectangular they may have a variety of shapes. For instance if plane were to correspond generally to the surface of a desk regions and might extend to one or more corners of that desk. Furthermore regions and may be adjacent i.e. having no significant space in plane therebetween but separated by a defined boundary. GPR module is preferably configured to disregard modal gestures that are made by a user within a defined plane e.g. plane but which fall outside a defined region e.g. outside of regions and .

Mode determination module includes any combination of hardware and or software configured to determine a first interaction mode of the first region based on the first modal gesture determine a first interaction mode of the second region based on the second modal gesture and to determine a second interaction mode different from the first interaction mode of the first region based on the third modal gesture. Stated another way mode determination module is configured such that regions and or may be changed from one interaction mode to another through the use of the corresponding appropriate modal gestures. Mode determination module may include one or more libraries or look up tables that provide a list of all possible or authorized gestures and postures. This library may be pre determined and or configurable by the user. That is the user may change or add additional gestures and postures.

Visual feedback module includes any combination of hardware and or software configured to provide visual feedback associated with a parameter of region and or . That is as described in further detail below visual feedback module provides the user with some form of visual feedback regarding the function shape or nature of the defined regions. For example the visual feedback might be associated with the boundary between regions and or the interaction mode of region i.e. whether it is in key press mode or touch pad mode . The visual feedback may be provided on a display e.g. a small display overlay positioned near the corner or through any other convenient display system.

The modules illustrated as part of user interface system may be incorporated into separate systems e.g. with different housings or consolidated into a single system. Furthermore in some embodiments the illustrated modules may share physical components. For example visual feedback module plane registration module and GPR module may share certain components such as cameras distance sensors projection display components and the like.

Having thus given a general overview of systems and methods in accordance with one embodiment provide a more detailed sequential illustration of the operation of a user interface system in accordance with one embodiment. will now be described in conjunction with the block diagram of .

First in plane registration module has identified four planes plane corresponding to a desk surface not shown plane substantially corresponding to the surface of a wall perpendicular to plane plane substantially corresponding to the surface of a computer monitor and plane substantially corresponding to the palm of the user s hand . As mentioned previously not all of the identified planes will ultimately be used as interaction regions by the user. Computer monitor may also be used to interact with the user during operation of user interface system e.g. allocation and mode selection as described in further detail below.

In the user has performed an allocation gesture via hand within plane and region definition module has consequently determined that region is to be used for interaction. Furthermore it is assumed that the user has performed a modal gesture that mode determination module has interpreted to mean that region is to be used thereafter for a specific interaction mode e.g. a key press interaction mode . In some embodiments an allocation gesture is not required or is implied by a particular modal gesture. That is a single gesture or posture may function to provide information regarding both allocation and mode of a region.

In the user has performed an allocation gesture via hand within plane and region definition module has consequently determined that region is to be used for interaction. Furthermore as with region above it is assumed that the user has performed a modal gesture that mode determination module has interpreted to mean that region is to be used thereafter for a specific interaction mode e.g. a touch input interaction mode . Subsequently the user may perform another modal gesture within region that changes the interaction mode for that region to a different interaction mode e.g. a display output mode .

In the user has performed an allocation gesture via hand within a second plane plane and region definition module has determined that region is to be used for interaction. Plane is non parallel to plane . At this point there are no regions defined within plane and thus only regions and will subsequently be used for interaction unless the user subsequently performs the above process with respect to plane or any other plane that is later identified within the environment.

In accordance with one embodiment GPR module is configured to observe a termination gesture made by the user and region definition module is configured to de allocate the first region based on the termination gesture. This may be used by the user for example when the defined region e.g. region is no longer needed for interaction. A variety of termination gestures may be used including for example a large X shape gesture a grasping gesture or a crumpling gesture.

In accordance with another embodiment GPR module is configured to observe a relocation gesture made by the user and region definition module is configured to change a geometrical attribute of the first region based on the relocation gesture. This may be used for example when the user wishes to move the defined region e.g. region to a different portion of plane or to change the size orientation or shape of region . A variety of relocation gestures may be used including for example a pinching gesture made by a pair of fingers a two handed grabbing carrying motion or a grasping gesture.

As mentioned above visual feedback module includes any combination of hardware and or software configured to provide visual feedback associated with one or more parameters of region and or . Referring to this functionality will be illustrated in more detail.

In an exemplary device is shown schematically as a sphere positioned above planes and . Device as illustrated is not intended to be limiting in shape or location but is merely meant to represent conceptually hardware that may be used by visual feedback module and or the other modules within user interface . For example in the illustrated embodiment device includes a projection device as is known in the art configured to project an arbitrary image on one or more of the surfaces corresponding to planes and . Thus visual feedback module is configured to provide a projection on or near plane that indicates the interaction mode of a region defined within plane e.g. region of . In other embodiments a virtual reality and or augmented reality device may be employed by the user for this purpose e.g. Google Glasses .

The rectangular projection might indicate for example a touch input interaction mode. Other projections such as a projected keyboard number pad arrow keys mixer board instrument panel or the like might also be used. Also illustrated in is a projection that indicates a boundary between two regions within plane e.g. regions and of . Such projections and would typically be helpful for the user to determine the location and mode of the defined regions and would also help remind the user that such regions are defined i.e. to prevent false positives . Device might also provide a variety of other projections that assist the user in for example the definition of regions and the determination of interaction modes for those regions.

Also illustrated in is a user interface component or icon that is displayed e.g. by any of the modules within user interface on monitor . As with the projections produced by device icon may provide information to the user regarding the allocation of the identified planes as well as the interaction modes used for particular regions. In one embodiment in accordance with an explicit plane registration protocol icon is used to reflect the shape and location of regions and planes defined within the environment. When the user provides an appropriate allocation gesture or posture within an identified plane icon blinks or otherwise provides a visual notification prompting the user to enter a modal gesture or the like to define a second region. Subsequently icon is changed to reflect the new allocation of regions within the plane.

When the posture of hand is changed or is removed from the plane in which region was defined the region may or may not be de allocated i.e. no longer used for input . In some embodiments the L shape posture makes region available for input i.e. without de allocating the region . When hand is moved within the plane in which region was defined region may be relocated to another location within the same plane. In another embodiment hand may be used to create a different plane and region will remain defined. In yet another embodiment a region modification gesture may be used by hand and or hand to modify the shape size or orientation of region .

While region is illustrated as rectangular the invention is not so limited. However in one embodiment in which region is in fact substantially rectangular one side of region has a length substantially equal to the length of the digit . That is as illustrated in the height h of region is substantially equal to the distance between the distal portion of finger and a point near the proximal portion of finger . More generally the height h may be a linear function of the length of the digit e.g. a linear function of that length . In other embodiments the shape and size of region is predetermined or user configurable .

In summary what has been described is a virtual user interface system in which multiple regions can be defined modified and de allocated within a single plane thereby allowing a user to make the best use of two dimensional objects such as a physical desk top within the user s environment. Because gestures and postures made by the user within a defined planar region are easier to observe and interpret than those made in a three dimensional space the live mic problem discussed above can be avoided. Furthermore a user accustomed to interacting with physical user interface devices such as touchpads and the like can replicate that experience through the use of similar virtual devices which are easy to configure depending on user needs. Furthermore a user may easily reprogram a plane and use it immediately if its modality has been forgotten by the user. In some embodiments the users may define a default setting for the various regions and planes in the case that the work environment stays unchanged for a sufficient length of time.

Thus the embodiments and examples set forth herein were presented in order to best explain the present invention and its particular application and to thereby enable those skilled in the art to make and use the invention. However those skilled in the art will recognize that the foregoing description and examples have been presented for the purposes of illustration and example only. The description as set forth is not intended to be exhaustive or to limit the invention to the precise form disclosed.

