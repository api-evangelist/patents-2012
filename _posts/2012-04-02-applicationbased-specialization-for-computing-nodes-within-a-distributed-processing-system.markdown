---

title: Application-based specialization for computing nodes within a distributed processing system
abstract: A distributed processing system is described that employs “application-based” specialization. In particular, the distributed processing system is constructed as a collection of computing nodes in which each computing node performs a particular processing role within the operation of the overall distributed processing system. Each of the computing nodes includes an operating system, such as the Linux operating system, and includes a plug-in software module to provide a distributed memory operating system that employs the role-based computing techniques. An administration node maintains a database that defines a plurality of application roles. Each role is associated with a software application, and specifies a set of software components necessary for execution of the software application. The administration node deploys the software components to the application nodes in accordance with the application roles associates with each of the application nodes.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08656355&OS=08656355&RS=08656355
owner: CA, Inc.
number: 08656355
owner_city: Islandia
owner_country: US
publication_date: 20120402
---
This application is a continuation of U.S. Pat. No. 8 151 245 application Ser. No. 11 305 850 filed Dec. 16 2005 by Steven M. Oberlin et al. and entitled APPLICATION BASED SPECIALIZATION FOR COMPUTING NODES WITHIN A DISTRIBUTED PROCESSING SYSTEM which claims the benefit of U.S. Provisional Application Ser. No. 60 637 050 filed Dec. 17 2004 by Steven M. Oberlin et al. and entitled APPLICATION BASED SPECIALIZATION FOR COMPUTING NODES WITHIN A DISTRIBUTED PROCESSING SYSTEM .

The invention relates to distributed processing systems and more specifically to multi node computing systems.

Distributed computing systems are increasingly being utilized to support high performance computing applications. Typically distributed computing systems are constructed from a collection of computing nodes that combine to provide a set of processing services to implement the high performance computing applications. Each of the computing nodes in the distributed computing system is typically a separate independent computing system interconnected with each of the other computing nodes via a communications medium e.g. a network.

Conventional distributed computing systems often encounter difficulties in scaling computing performance as the number of computing nodes increases. Scaling difficulties are often related to inter device communication mechanisms such as input output I O and operating system OS mechanism used by the computing nodes as they perform various computational functions required within distributed computing systems. Scaling difficulties may also be related to the complexity of developing and deploying application programs within distributed computing systems.

Existing distributed computing systems containing interconnected computing nodes often require custom development of operating system services and related processing functions. Custom development of operating system services and functions increases the cost and complexity of developing distributed systems. In addition custom development of operating system services and functions increases the cost and complexity of development of application programs used within distributed systems.

Moreover conventional distributed computing systems often utilize a centralized mechanism for managing system state information. For example a centralized management node may handle allocation of process and file system name space. This centralized management scheme often further limits the ability of the system to achieve significant scaling in terms of computing performance.

In general the invention relates to a distributed processing system that employs role based computing. In particular the distributed processing system is constructed as a collection of computing nodes in which each computing node performs one or more processing roles within the operation of the overall distributed processing system.

The various computing roles are defined by a set of operating system services and related processes running on a particular computing node used to implement the particular computing role. As described herein a computing node may be configured to automatically assume one or more designated computing roles at boot time at which the necessary services and processes are launched.

As described herein a plug in software module referred to herein as a unified system services layer may be used within a conventional operating system such as the Linux operating system to provide a general purpose distributed memory operating system that employs role based computing techniques. The plug in module provides a seamless inter process communication mechanism within the operating system services provided by each of the computing nodes thereby allowing the computing nodes to cooperate and implement processing services of the overall system.

In addition the unified system services layer USSL software module provides for a common process identifier PID space distribution that permits any process running on any computing node to determine the identity of a particular computing node that launched any other process running in the distributed system. More specifically the USSL module assigns a unique subset of all possible PIDs to each computing node in the distributed processing system for use when the computing node launches a process. When a new process is generated the operating system executing on the node selects a PID from the PID space assigned to the computing node launching the process regardless of the computing node on which the process is actually executed. Hence a remote launch of a process by a first computing node onto a different computing node results in the assignment of a PID from the first computing node to the executing process. This technique maintains global uniqueness of process identifiers without requiring centralized allocation. Moreover the techniques allow the launching node for any process running within the entire system to easily be identified. In addition inter process communications with a particular process may be maintained through the computing node that launches a process even if the launched process is located on a different computing node without need to discover where the remote process was actually running.

The USSL module may be utilized with the general purpose operating system to provide a distributed parallel file system for use within the distributed processing system. As described herein file systems associated with the individual computing nodes of the distributed processing system are projected across the system to be available to any other computing node. More specifically the distributed parallel file system presented by the USSL module allows files and a related file system of one computing node to be available for access by processes and operating system services on any computing node in the distributed processing system. In accordance with these techniques a process executing on a remote computing node inherits open files from the process on the computing node that launched the remote process as if the remote processes were launched locally.

In one embodiment the USSL module stripes the file system of designated input output I O nodes within the distributed processing system across multiple computing nodes to permit more efficient I O operations. Data records that are read and written by a computing node to a file system stored on a plurality of I O nodes are processed as a set of concurrent and asynchronous I O operations between the computing node and the I O nodes. The USSL modules executing on the I O nodes separate data records into component parts that are separately stored on different I O nodes as part of a write operation. Similarly a read operation retrieves the plurality of parts of the data record from separate I O nodes for recombination into a single data record that is returned to a process requesting the data record be retrieved. All of these functions of the distributed file system are performed within the USSL plug in module added to the operating system of the computing nodes. In this manner a software process executing on one of the computing nodes does not recognize that the I O operation involves remote data retrieval involving a plurality of additional computing nodes.

The details of one or more embodiments of the invention are set forth in the accompanying drawings and the description below. Other features objects and advantages of the invention will be apparent from the description and drawings and from the claims.

The collection of computing nodes in one embodiment includes a plurality of application nodes A H each labeled APP NODE on interconnected to a plurality of system nodes . Further system nodes include a plurality of input output nodes A F each labeled I O NODE and a plurality of mass storage devices A F coupled to I O nodes . In one embodiment system nodes may further include a command node labeled CMD NODE an administration node labeled ADMIN NODE and a resource manager node labeled RES MGR NODE . Additional system nodes may also be included within other embodiments of distributed processing system . As illustrated the computing nodes are connected together using a communications network to permit internode communications as the nodes perform interrelated operations and functions.

Distributed processing system operates by having the various computing nodes perform specialized functions within the entire system. For example node specialization allows the application nodes A H collectively application nodes to be committed exclusively to running user applications incurring minimal operating system overhead thus delivering more cycles of useful work. In contrast the small adjustable set of system nodes provides support for system tasks such as user logins job submission and monitoring I O and administrative functions which dramatically improve throughput and system usage.

In one embodiment all nodes run a common general purpose operating system. One examples of a general purpose operating system is the Windows operating system provided by Microsoft Corporation. In some embodiment the general purpose operating system may be a lightweight kernel such as the Linux kernel which is configured to optimize the respective specialized node functionality and that provides the ability to run binary serial code from a compatible Linux system. As further discussed below a plug in software module referred to herein as a unified system services layer is used in conjunction with the lightweight kernel to provide the communication facilities for distributed applications system services and I O.

Within distributed computing system a computing node or node refers to the physical hardware on which the distributed computing system runs. Each node includes one or more programmable processors for executing instructions stored on one or more computer readable media. A role refers to the system functionality that can be assigned to a particular computing node. As illustrated in nodes are divided into application nodes and system nodes . In general application nodes are responsible for running user applications launched from system nodes . System nodes provide the system support functions for launching and managing the execution of applications within distributed system . On larger system configurations system nodes are further specialized into administration nodes and service nodes based on the roles that they run.

Application nodes may be configured to run user applications launched from system nodes as either batch or interactive jobs. In general application nodes make up the majority of the nodes on distributed computing system and provide limited system daemons support forwarding I O and networking requests to the relevant system nodes when required. In particular application nodes have access to I O nodes that present mass storage devices as shared disks. Application nodes may also support local disks that are not shared with other nodes.

The number of application nodes is dependent on the processing requirements. For example distributed processing system may include 8 to 512 application nodes or more. In general an application node typically does not have any other role assigned to it.

System nodes provide the administrative and operating system services for both users and system management. System nodes typically have more substantial I O capabilities than application nodes . System nodes can be configured with more processors memory and ports to a high speed system interconnect.

To differentiate a generic node into an application node or system node a node role is assigned to it thereby dedicating the node to provide the specified system related functionality. A role may execute on a dedicated node may share a node with other roles or may be replicated on multiple nodes. In one embodiment a computing node may be configured in accordance with a variety of node roles and may function as an administration node application nodes command node I O nodes a leader node a network director node a resources manager node and or a Unix System Services USS USS node . Distributed processing system illustrates multiple instances of several of the roles indicating that those roles may be configured to allow system to scale so that it can adequately handle the system and user workloads. These system roles are described in further detail below and typically are configured so that they are not visible to the user community thus preventing unintentional interference with or corruption of these system functions.

The administration functionality is shared across two types of administration roles administration role and leader role. The combination of administration and leader roles is used to allow the administrative control of large systems to easily scale. Typically only one administration role is configured on a system while the number of leader roles is dependent on the number of groups of application nodes in the system. The administration role along with the multiple leader roles provides the environment where the system administration tasks are executed.

If a system node is assigned an administration role it is responsible for booting dumping hardware health monitoring and other low level administrative tasks. Consequently administration node provides a single point of administrative access for system booting and system control and monitoring. With the exception of the command role this administration role may be combined with other system roles on a particular computing node.

Each system node with the leader role e.g. leader node monitors and manages a subset of one or more nodes which are referred to as a group. The leader role is responsible for the following discovering hardware of the group distributing the system software to the group acting as the gateway between the system node with the administration role and the group and monitoring the health of the group e.g. in terms of available resources operational status and the like.

A leader node facilitates scaling of the shared root file system and offloads network traffic from the service node with the administration role. Each group requires a leader node which monitors and manages the group. This role can be combined with other system roles on a node. In some cases it may be advisable to configure systems with more than 16 application nodes into multiple groups.

The system node with the administration role contains a master copy of the system software. Each system node with a leader role redistributes this software via an NFS mounted file transfer and is responsible for booting the application nodes for which it is responsible.

The resource management network director I O and command roles directly or indirectly support users and the applications that are run by the users. Typically only one instance of the network director and resource manager roles are configured on a system. The number of command roles can be configured such that user login and the application launch workload are scaled on system . The need for additional system nodes with an I O role is optional depending on the I O requirements of the specific site. Multiple instances of the I O roles can be configured to allow system to scale to efficiently manage a very broad range of system and user workloads.

Command node provides for user logins and application builds submission and monitoring. The number of command roles assigned to system is dependent on the processing requirements. At least one command role is usually always configured within system . With the exception of the administration role this role can be combined with other system roles on a node.

In general I O nodes provide for support and management of file systems and disks respectively. The use of the I O roles is optional and the number of I O roles assigned to a system is dependent on the I O requirements of the customer s site. An I O role can be combined with other system roles on a node. However a node is typically not assigned both the file system I O and network I O roles. In some environments failover requirements may prohibit the combination of I O roles with other system roles.

Network director node defines the primary gateway node on distributed processing system and handles inbound traffic for all nodes and outbound traffic for those nodes with no external connections. Typically one network director role is configured within distributed processing system . This role can be combined with other system roles on a node.

Resources manager node defines the location of the system resource manager which allocates processors to user applications. Typically one resource manager role is configured within distributed processing system . This role can be combined with other system roles on a node. A backup resource manager node not shown may be included within system . The backup resource manager node may take over resource management responsibility in the event a primary resource manager node fails.

An optional USS node provides the Unix System Services USS service on a node when no other role includes this service. USS services are a well know set of services and may be required by one or more other Unix operating system services running on a computing node. Inclusion of a USS computing role on a particular computing node provides these USS services when needed to support other Unix services. The use of the USS role is optional and is intended for use on non standard configurations only. The number of USS roles assigned to distributed processing system is dependent on the requirements of the customer s site. This role can be combined with other system roles on a node but is redundant for all but the admin leader and network director roles.

While many of the system nodes discussed above are shown using only a single computing node to support its functions multiple nodes present within system may support these roles either in a primary or backup capacity. For example command node may be replicated any number of times to support additional users or applications. Administration node and resource manager node may be replicated to provide primary and backup nodes thereby gracefully handling a failover in the event the primary node fails. Leader node may also be replicated any number of times as each leader node typically supports a separate set of application nodes .

User application represents an example application executing within user space . User application interacts with a messaging passage interface MPI to communicate with remote processes through hardware interface modules . Each of these interface modules provide interconnection using a different commercially available interconnect protocol. For example TCP module provides communications using a standard TCP transport layer. Similarly GM module permits communications using a Myrinet transport layer from Myricom Inc. of Arcadia Calif. and Q module permits communications using a QsNet systems transport layer from Quadrics Supercomputers World Ltd. of Bristol United Kingdom. Hardware interface modules are exemplary and other types of interconnects may be supported within distributed processing system .

User application also interacts with operating system services within kernel space using system calls to kernel . Kernel provides an application programming interface API for receiving system calls for subsequent processing by the operating system. System calls that are serviced locally within computing node are processed within kernel to provide user application requested services.

For remote services kernel forwards system calls to USSL module for processing. USSL module communicates with a corresponding USSL module within a different computing node within distributed processing system to service the remote system calls . USSL module communicates with remote USSL modules over one of a plurality of supported transport layer modules . These transport layer modules include a TCP module a GM module and a Q module that each support a particular communications protocol. Any other commercially available communications protocol may be used with its corresponding communications transport layer module without departing from the present invention.

In one example embodiment kernel is the Linux operating system and USSL module is a plug in module that provides additional operating system services. For example USSL module implements a distributed process space a distributed I O space and a distributed process ID PID space as part of distributed processing system . In addition USSL module provides mechanisms to extend OS services to permit a process within computing node to obtain information regarding processes I O operations and CPU usage on other computing nodes within distributed processing system . In this manner USSL module supports coordination of processing services within computing nodes within larger distributed computing systems.

Processor virtualization module provides communications and status retrieval services between computing node and other computing nodes within distributed processing system associated with CPU units with these computing nodes. Processor virtualization module provides these communication services to make the processors of the computing nodes within distributed computing system appear to any process executing within system as a single group of available processors. As a result all of the processors are available for use by applications deployed within system . User applications may for example request use of any of these processors through system commands such as an application launch command or a process spawn command.

Process virtualization module provides communications and status retrieval services of process information for software processes executing within other computing nodes within distributed processing system . This process information uses PIDs for each process executing within distributed processing system . Distributed processing system uses a distributed PID space used to identify processes created and controlled by each of the computing nodes. In particular in one embodiment each computing node within distributed processing system is assigned a set of PIDs. Each computing node uses the assigned set when generating processes within distributed processing system . Computing node for example will create a process having a PID within the set of PIDs assigned to computing node regardless of whether the created process executes on computing node or whether the created process executes remotely on a different computing node within distributed processing system .

Because of this particular distribution of PID space any process executing within distributed processing system can determine the identity of a computing node that created any particular process based on the PID assigned to the process. For example a process executing on one of application nodes may determine the identity of another one of the application nodes that created a process executing within any computing node in distributed processing system . When a process desires to send and receive messages from a given process in distributed processing system a message may be sent to the particular USSL module corresponding to the PID space containing the PID for the desired process. USSL module in this particular computing node may forward the message to the process because USSL module knows where its process is located. Using this mechanism the control of PID information is distributed across system rather than located within a single node in distributed processing system .

Distributed I O virtualization module provides USSL module communications services associated with I O operations performed on remote computing nodes within distributed processing system . Particularly distributed I O virtualization module permits application nodes to utilize storage devices A F collectively mass storage devices coupled to I O nodes as if the mass storage devices provided a file system local to application nodes .

For example I O nodes assigned the file system I O role support one or more mounted file systems. I O nodes may be replicated to support as many file systems as required and use local disk and or disks on the nodes for file storage. I O nodes with the file system I O role may have larger processor counts extra memory and more external connections to disk and the hardware interconnect to enhance performance. Multiple I O nodes with the file system I O role can be mounted as a single file system on application nodes to allow for striping parallelization of an I O request via a USSL module .

I O nodes assigned the network I O role provide access to global NFS mounted file systems and can attach to various networks with different interfaces. A single hostname is possible with multiple external nodes but an external router or single primary external node is required. The I O path can be classified by whether it is disk or external and who or what initiates the I O e.g. the user or the system .

Distributed processing system supports a variety of paths for system and user disk I O. Although direct access to local volumes on a node is supported the majority of use is through remote file systems so this discussion focuses on file system related I O. For exemplary purposes the use of NFS is described herein because of the path it uses through the network. All local disk devices can be used for swap on their respective local nodes. This usage is a system type and is independent of other uses.

System nodes and application nodes may use local disk for temporary storage. The purpose of this local temporary storage is to provide higher performance for private I O than can be provided across the distributed processing system. Because the local disk holds only temporary files the amount of local disk space does not need to be large.

Distributed processing system may assume that most file systems are shared and exported through the USSL module or NFS to other nodes. This means that all files can be equally accessed from any node and the storage is not considered volatile. Shared file systems are mounted on system nodes .

In general each disk I O path starts at a channel connected to one of I O nodes and is managed by disk drivers and logical volume layers. The data is passed through to the file system usually to buffer cache. The buffer cache on a Linux system for example is page cache although the buffer cache terminology is used herein because of the relationship to I O and not memory management. On another embodiment of distributed processing system applications may manage their own user buffers and not depend on buffer cache.

Within application nodes the mount point determines the file system chosen by USSL module for the I O request. For example the file system s mount point specifies whether it is local or global. A local request is allowed to continue through the local file system. A request for I O from a file system that is mounted globally is communicated directly to one of I O node where the file system is mounted. All processing of the request takes place on this system node and the results are passed back upon completion to the requesting node and to the requesting process.

Application I O functions are usually initiated by a request through USSL module to a distributed file system for a number of bytes from to a particular file in a remote file system. Requests for local file systems are processed local to the requesting application node . Requests for global I O are processed on the one of the I O nodes where the file system is mounted.

Other embodiments of system provide an ability to manage an application s I O buffering on a job basis. Software applications that read or write sequentially can benefit from pre fetch and write behind while I O caching can help programs that write and read data. However in both these cases sharing system buffer space with other programs usually results in interference between the programs in managing the buffer space. Allowing the application exclusive use of a buffer area in user space is more likely to result in a performance gain.

Another alternate embodiment of system supports asynchronous I O. The use of asynchronous I O allows an application executing on one of application nodes to continue processing while I O is being processed. This feature is often used with direct non buffered I O and is quite useful when a request can be processed remotely without interfering with the progress of the application.

Distributed processing system uses network I O at several levels. System must have at least one external connection to a network which should be IP based. The external network provides global file and user access. This access is propagated through the distributed layers and shared file systems so that a single external connection appears to be connected to all nodes. The system interconnect can provide IP traffic transport for user file systems mounted using NFS.

A distributed file system provided by distributed I O virtualization module provides significantly enhanced I O performance. The distributed file system is a scalable global parallel file system and not a cluster file system thus avoiding the complexity potential performance limitations and inherent scalability challenges of cluster file system designs.

The read write operations between application nodes and the distributed file system are designed to proceed at the maximum practical bandwidth allowed by the combination of system interconnect the local storage bandwidth and the file record structure. The file system supports a single file name space including read write coherence the striping of any or all file systems and works with any local file system as its target.

The distributed file system is also a scalable global parallel file system that provides significantly enhanced I O performance on the USSL system. The file system can be used to project file systems on local disks project file systems mounted on a storage area network SAN disk system and re export a NFS mounted file system.

Transport API and supported transport layer modules provide a mechanism for sending and receiving communications between USSL module and corresponding USSL modules in other computing nodes in distributed processing system . Each of the transport layer modules provide an interface between a common transport API used by processor virtualization module process virtualization module distributed I O virtualization module and the various communication protocols supported within computing node .

API provides a two way application programming interface for communications to flow between kernel and processor virtualization module process virtualization module distributed I O virtualization module within USSL module . API module provides mechanisms for the kernel to request operations be performed within USSL module . Similarly API module provides mechanisms for kernel to provide services to the USSL module . IOCTL API module provides a similar application programming interface for communications to flow between the kernel and USSL module for I O operations.

Initially a user or software agent interacts with distributed processing system through command node that provides services to initiate actions for the user within distributed processing system . For an application launch operation command node uses an application launch module that receives the request to launch a particular application and processes the request to cause the application to be launched within distributed processing system . Application launch module initiates the application launch operation using a system call to kernel to perform the application launch. Because command node will not launch the application locally as user applications are only executed on application nodes kernel passes the system call to USSL module for further processing.

USSL module performs a series of operations that result in the launching of the user requested application on one or more of the application nodes within distributed processing system . First processor virtualization module within USSL module determines the identity of the one or more application nodes on which the application is to be launched. In particular processor virtualization module sends a CPU allocation request through a hardware interface shown for exemplary purposes as TCP module to resource manager node .

Resource manager node maintains allocation state information regarding the utilization of all CPUs within all of the various computing nodes of distributed processing system . Resource manager node may obtain this allocation state information by querying the computing nodes within distributed processing system when it becomes active in a resource manager role. Each computing node in distributed processing system locally maintains its internal allocation state information. This allocation state information includes for example the identity of every process executing within a CPU in the node and the utilization of computing resources consumed by each process. This information is transmitted from each computing node to resource manager node in response to its query. Resource manager node maintains this information as processes are created and terminated thereby maintaining a current state for resource allocation within distributed processing system .

Resource manager node uses the allocation state information to determine on which one or more of application nodes the application requested by command node is to be launched. Resource manager node selects one or more of application nodes based on criteria such as a performance heuristic that may predict optimal use of application nodes . For example resource manager node may select application nodes that are not currently executing applications. If all application nodes are executing applications resource manager node may use an application priority system to provide maximum resources to higher priority applications and share resources for lower priority applications. Any number of possible prioritization mechanisms may be used.

Once resource manager node determines the identity of one or more application nodes to be used by command node a list of the identified application nodes may be transmitted as a message back to USSL module within command node . Processor virtualization module within USSL module of command node uses the list of application nodes to generate one or more remote execute requests necessary to launch the application on the application nodes identified by resource manager node . In general a remote execute request is a standard request operation that specifies that an application is to be launched. The identity of the application may be provided using a file name including a path name to an executable file stored on one of the I O nodes .

Processor virtualization module transmits the remote execute requests to each of the one or more application nodes identified by resource manager node to complete the remote application launch operation. Each remote execute request include a PID for use when the application is launched. Each of the application nodes uses the PID provided in the remote execute request in order to properly identify the launching node command node in this example as the node creating the process associated with the launch of the application. In other words the PID provided within remote execute request will be selected by command node from within the PID space allocated to the command node.

Upon creation of one or more software processes corresponding to the launch of the application each targeted application node returns a response message to process virtualization module to indicate the success or failure of the request. When a process is successfully created process virtualization module updates a local process information store that contains state information relating to launched application. This information store maintains an identity of the processes created using their PIDs and related process group IDs and session IDs as well as an identity of the one of application nodes upon which the process is running. A similar message may be transmitted to resource manager node to indicate that the process is no longer utilizing processing resources within a particular one of the application nodes . Resource manager node may use this message to update its allocation state data used when allocating app nodes to process creation requests.

In general within all computing nodes within distributed processing system applications executing in user space interact with operating system kernel operating in kernel space through the use of a system call . This system call is a procedure call to a defined interface for a particular O S service. In distributed processing system a subset of these system calls are forwarded as calls by kernel to USSL module to provide a set of services and related operations associated with a collection of computing nodes operating as a distributed computing system. In this manner USSL module may be used within a conventional operating system such as the Linux operating system to provide a general purpose distributed memory operating system that employs role based computing techniques.

In the example of kernel receives system call and determines whether the system call is supported by the kernel or whether the system call needs to be forwarded to the USSL module . In contrast in the application launch example of kernel forwarded system call to USSL module as all application launch operations are typically performed as remotely executed commands.

In processing other commands kernel may desire to perform the command locally in some circumstances and remotely in other circumstances. For example an execute command causes creation of a software process to perform a desired operation. This process may be executed locally within command node or may be executed within one of application nodes of distributed processing system . Similarly other system calls may be performed locally by kernel or forwarded to USSL for remote processing.

In order to determine where the process is to be created a kernel hook is included within of kernel to make this determination. In general kernel hook is a dedicated interface that processes all system calls that may be executed in multiple locations. For example kernel hook processes exec calls and determines whether the process to be created should be created locally or remotely on one of application nodes .

To make this determination kernel hook maintains a list of programs that are to be remotely executed depending upon the identity of calling process that generated system call . If the program that is to be executed as part of system call is found on the list of programs maintained by kernel hook the kernel hook issues system call to USSL module for processing. If the program requested in system call is not on the list of programs kernel hook passes the system call to kernel for processing. Because the list of programs used by kernel hook is different for each calling process control of which system calls are passed to USSL module may be dynamically controlled depending upon the identity of the process making the call.

Upon receiving system call kernel hook within determines whether the process to be signaled is local using the specified PID. If the signal message is to be sent to a remote process kernel issues a corresponding signaling message call to USSL module for transmission of the signaling message to the remote application node B. Process virtualization module within USSL module generates a message that is transmitted to a corresponding USSL module within application node B. A process virtualization module within USSL module forwards the signaling message to kernel in application node B for ultimate transmission to process . A return message if needed is transmitted from process to application module in similar fashion.

In this manner application module need not know where process is located within distributed processing system . Application module may for example only know the PID for process to be signaled. In such a situation USSL module in application node A forwards signaling message to the computing node within which the PID for process is assigned. The USSL module within this computing node via its process virtualization module identifies the application node on which the process is executing. If process is located on a remote computing node such as application node B the signaling message is forwarded from application node A owning the PID of the process to process for completion of the signaling operation.

Due to this inheritance remote application utilizes the same open files located on application node A that created remote application . As such when remote application performs an I O operation to one of inherited open files the I O operation is automatically transmitted from application node B to application node A for completion. In particular remote application attempts to perform the I O operation through its kernel . Because these open files are remote to kernel the kernel passes the I O operation to USSL module . USSL module using its distributed I O virtualization module forwards the I O operation request to USSL module within application node A. USSL module then makes an I O call to kernel to perform the appropriate read or write operation to open files .

Kernel and kernel map I O operations to these open files to specific memory address locations within the respective kernels. As such kernel knows to pass I O operations at that particular memory address to the USSL module for processing. Kernel does not know or need to know where USSL module ultimately performs the I O operation. Similarly kernel receives an I O request from USSL module with an I O operation to its particular memory address corresponding to the open files . Kernel performs the I O operation as if the I O request was made locally rather than remotely through a pair of USSL modules located on different computing nodes. In this manner the techniques provide for the seamless inheritance of open file references within distributed processing system .

In general distributed processing system supports one or more file systems including 1 a multiple I O node parallel file system 2 a non parallel single I O node version of the file system 3 a global node file system that provides a view of the file system tree of every node in the system and 4 a global gproc file system that provides a view of the processes in the global process space.

In distributed processing system most file systems are typically shared and exported through USSL module executing on each node. The use of shared file systems through USSL module means that all files can be accessed equally from any node in distributed processing system and that storage is not volatile. On system every node has a local root that supports any combination of local and remote file systems based on the file system mounts. The administrative infrastructure maintains the mount configuration for every node. Local file systems may be used when performance is critical. For example application scratch space and on the service nodes for bin lib and other system files. The remote file system can be of any type supported by distributed processing system .

Distributed I O virtualization module within USSL module implements a high performance scalable design to provide global parallel I O between I O nodes and system nodes or application nodes . Similar to NFS the implemented file system is stacked on top of any local file system present on all of the I O nodes in distributed processing system . Metadata disk allocation and disk I O are all managed by the local file system. USSL module provides a distribution layer on top of the local file system which aggregates the local file systems of multiple I O nodes i.e. system nodes with I O roles into a single parallel file system and provides transparent I O parallelization across the multiple I O nodes. As a result parallel I O can be made available through the standard API presented by kernel such as the standard Linux file API open read write close and so on and is transparent to application program . Parallelism is achieved by taking a single I O request read or write and distributing it across multiple service nodes with I O roles.

In one embodiment any single I O request is distributed to I O nodes in a round robin fashion based on stripe size. For example referring again to the example of a read operation performed by application module retrieves a data record from both I O node A and I O node B. One portion of the data record is stored in mass storage device A attached to I O node A and a second portion of the data record is stored on mass storage device A attached to I O node B. Data records may be striped across a plurality of different I O nodes in this fashion. Each of the portions of the data record may be asynchronously retrieved with application node A requesting retrieval of the portions as separate read requests made to each corresponding I O node A B. These read requests may occur concurrently to decrease data retrieval times for the data records. Once all of the portions of the data records are received the portions may be combined to create a complete data record for use by application module . A data write operation is performed in a similar manner as application node A divides the data record into portions that are separately written to I O nodes A and B. The file system implemented by distributed processing system does not require disks to be physically shared by multiple nodes. Moreover the implemented file system may rely on hardware or software RAID on each service node with an I O role for reliability.

In this manner the use of USSL module as a plug in extension allows an I O node e.g. I O node A to project a file system across distributed processing system to as many application nodes as mounted the file systems. The projecting node is a server that is usually a service node with an I O role i.e. an I O node and the nodes that mount the file system as clients can have any role or combination of roles assigned to them e.g. application nodes or system nodes . The purpose of this single I O node version of the implemented file system is to project I O across the system. The single I O node version is a subset of the implemented file system which performs the same function grouping several servers together that are treated as one server by the client nodes.

The node file system allows access to every node s root directory without having to explicitly mount every node s root on every other node in the system. Once mounted the node file system allows a global view of each node s root directory including the node s dev and proc directories. On distributed processing system which does not use a single global device name space each node has its own local device name space dev . For example dev on node RED can be accessed from any node by looking at node RED dev. The node file system is made accessible by mounting the file system via the mount utility.

The gproc file system aggregates all the processes in all nodes proc file system allowing all process IDs from all the nodes in the system to be viewed from the gproc file system. Opening a process entry in this file system opens the proc file entry on the specified node providing transparent access to that node s proc information.

The distributed I O virtualization module within USSL module automatically performs the file open operation by generating and sending message to corresponding USSL module and USSL module in I O nodes A and B respectively requesting the file within their respective file systems be opened. While the file name reference used by application module appears to be a logical file name within the distributed file system distributed I O virtualization module is actually opening a plurality of files within the file systems of each I O node A B on which the data records are striped. The respective USSL modules pass the open file requests to their respective kernels and which open the files on behalf of application module .

Once these files have been opened the logical file that consists of the separate files on mass storage devices and of I O nodes A B is available for use by application module . Application module may read and write data records using a similar set of operations. When a read operation occurs application module transmits another I O command to kernel which in turn transmits another corresponding I O command to USSL module . Distributed I O virtualization module within USSL module identifies the I O nodes A and B on which the portions of the data record to be read are located and sends a series of concurrent I O messages to USSL module and USSL module to retrieve the various portions of the data record. In response USSL modules retrieve and return their respective portion of the data record to USSL module . Distributed I O virtualization module automatically combines each portion of the data record to generate the complete data record which is passed through kernel to application module .

I O nodes A B map the distributed file system across their respective mass storage devices A B under the control of an administration node at the time the I O nodes are booted. In this manner this file system mapping information for how data records are striped across multiple I O nodes A B is made available for all computing nodes within distributed processing system .

As discussed above distributed processing system supports node level specialization in that each computing node may be configured based one or more assigned roles. As illustrated in node of in this embodiment each node within distributed processing system contains a common set of operating system software e.g. kernel . Selected services or functions of the operating system may be activated or deactivated when computing node is booted to permit the computing node to efficiently operate in accordance with the assigned computing roles.

Computing node provides a computing environment having a user space and a kernel space in which all processes operate. User applications operate within user space . These user applications provide the computing functionality to perform processing tasks specified by a user. Within kernel space an operating system kernel and associated USSL module provide operating system services needed to support user applications .

In kernel space operating system kernel and related USSL module operate together to provide services requested by user applications . As discussed in reference to USSL module may contain a processor virtualization module a process virtualization module and a distributed I O virtualization module that perform operations to provide file system and remote process communications functions within distributed processing system .

As illustrated in kernel includes a set of standard OS services module to provide all other operating services within computing node . USSL module updates PID space data to contain a set of PIDs from the administration node for use by computing node creating a process on any computing node within system.

In addition kernel accesses roles configuration data and PID space data maintained and updated by USSL module . Roles configuration data causes kernel to operate in coordination with administration node in distributed processing system . In particular kernel is configured in accordance with roles configuration data to provide services needed to implement the assigned computing role or roles.

Using this data computing node may operate in any number of computing roles supported within distributed processing system . Each of these processing roles requires a different set of services that are activated when computing node is booted. The inclusion and subsequent use of these operating system services within computing node provide the functionality for computing node to operate as one or more of the system node roles or application node role discussed above.

In other words the use of processing roles may be viewed as a mechanism for providing computing resource isolation to reduce competition between different processes for particular resources within a computing node. For example I O nodes within distributed processing system provide access to data stored on attached mass storage devices for application nodes . These I O operations all utilize a common set of resources including the mass storage devices system buses communications ports memory resources and processor resources. The scheduling of operations to provide efficient data retrieval and storage operations may be possible if only I O operations are being performed within the particular computing node. If I O operations and other system operations such as operations performed by a resource manager role or an administration role are concurrently operating within the same node different sets of resources and operations may be needed. As a result the same level of efficiency for each computing role may not be possible as the computing node switches between these different roles.

The isolation that is provided through the use of computing roles also achieves a reduced reliance on single points of failure within distributed processing system . In particular a given node s reliance on a single point of failure is reduced by separating roles across a plurality of identical nodes. For example as illustrated in consider two sets of isolated computing nodes 1 a first set of nodes that includes application node F I O node A and I O node D and 2 a second set of nodes that includes application node H I O node C and I O node F. In general different user applications would be running on each of these different sets of nodes. Due to the isolation between the sets if any one of the nodes in either the first set of nodes or the second set of nodes fails the operation of the other set of nodes is not affected. For example if I O node A fails the second set of nodes is still able to carry out its assigned applications. Additionally the failed node may be replaced in some circumstances by another node in distributed processing system that is configured to perform the same computing role as the failed computing node.

Moreover if a system node such as resource manager node fails all other nodes in distributed processing system will continue to operate. New requests for computing nodes needed to launch a new application cannot be allocated while the resource manager node is inoperable. However a different computing node within distributed processing system may be activated to perform the role of a resource manager node. Once the new resource manager node is operating and has obtained process status information used by the resource manager role to allocate nodes to new processes is obtained from all active nodes in the system the new node may continue operation of system as if the resource manager node had not failed. While this recovery process occurs existing processes running on computing nodes in distributed processing system continue to operate normally. Similar results may be seen with a failure of all other computing nodes. Because most status information used in system nodes such as administration node and resource manager node is replicated throughout the computing nodes in distributed processing system currently existing nodes of all types may continue to operate in some fashion using this locally maintained information while a failure and subsequent recovery of a particular node occurs.

In this manner this node specialization and isolation of nodes into roles supports an increase in the scalability of functions within distributed processing system . Whenever additional processing resources of a particular type are needed an additional node of the needed type may be added to system . For example a new process may be launched on a new application node when additional application processing is needed. Additional I O capacity may be added in some circumstances by adding an additional I O node . Some system nodes such as a command node may be added to support additional user interaction. In each case the use of plug in USSL module with a conventional operating system such as Linux allows additional nodes to easily be used as any computing nodes of a particular computing role merely by booting a generic computing node into a particular computing role.

More specifically within configuration data store a data entry exists for each type of computing role supported within distributed processing system . In the example embodiment of configuration data store includes an application node data entry a command node data entry and an I O node data entry . For each particular data entry a specific list of operating system services is listed. This list of services specified by each data entry controls the services that are launched when a particular computing node is booted. Although not shown data store may have entries for each node of distributed processing system and for each node associate the node with one or more of the defined roles. In this manner configuration data store controls the services executing by application nodes command node I O nodes administration node resource manager node leader node network director node USS node and any other type of node in distributed processing system .

The following sections describe in further detail one example embodiment in which operating system services provided by a node are selectively enabled and disabled in accordance with the one or more roles associated with the node. As noted above kernel may be a version of Linux operating system in one example embodiment. In this example embodiment Red Hat Linux 7.3 for IA32 systems from Redhat Inc. of Raleigh N.C. is described for use as kernel . Consequently the operating system services provided by kernel that are selectively turned on or off based on the one or more roles assigned to a computing node correspond to well known operating system services available under Linux. As discussed below a specific mapping of services enabled for each type of computing node role is defined and each computing node in distributed processing system is assigned one or more roles.

The following tables and explanations show the node specialization process and list the services that are ultimately enabled for each defined node role. Table 1 does not show every system service but only those services that are enabled after the installation or configuration process has completed and reflects the system services as defined for example in a etc rc.d init.d directory as defined in Red Hat Linux 7.3.

In this example Table 1 defines the system services that are initially enabled after a base Linux installation. In particular column 1 defines the Linux system services that are enabled after a base Linux distribution installation. Column 2 defines the Linux system services that are enabled after an Unlimited Linux installation. Column 3 defines the Linux system services that are enabled after the initial Unlimited Linux configuration tasks are completed but before the roles are assigned to the nodes in system . In columns 2 and 3 the services specific to the Unlimited Linux system are called out in bold font see Table 2 for a description of these services.

During the final stage of system configuration the USSL module selectively enables and disables the system services based on the type of system interconnect that is used on the system and by the role or roles assigned to a node. Table 3 lists the Linux system services that are further modified based on the role that is assigned a node. In one embodiment the roles are processed in the ordered shown in Table 3 because the nfs and nfs.leader services are not compatible.

After the Linux installation and configuration process is completed the Linux system services that are enabled for a particular computing node is generally the set of services shown in column 3 of Table 1 as modified by the results of Table 3 and the disabling of the eth discover ipleader and uss services before the role modifications are made.

For example a computing node that is assigned the leader computing role would have all of the services in column 3 of Table 1 plus the nfs.leader dmonp dhcpd ipforward ipleader and eth discover services on and uss off. In this leader node the nfs service is turned off even though it is already off while dhcpd is turned on even though it is already on as indicated in column 3 of Table 1 respectively. This procedure is utilized to ensure that correct system services are on when a computing node has more than one role assigned to it. If a computing node has combined roles the sets of services defined in Table 3 are logically ORed. For example if a particular computing node has both a leader node role and a command node role assigned to it the set of role modified system services on this node would be as follows uss on nfs off nfs.leader on dmonp on dhcpd on ipforward on ipleader on and eth discover on.

While the example embodiment illustrated herein utilizes Red Hat Linux 7.3 system services other operating systems may be used by enabling corresponding operating system services typically supported by well known operating systems without departing from the present invention.

In one example embodiment configuration utility application provides a user with a set of control columns that permit the configuration of one of the computing nodes in the system. The control columns include a system group column a group items column an item information column a node role column and other information column . Users interact with control options shown in each column to configure the specific node level roles assigned to a computing node.

System group column provides a listing of all groups of computing nodes available within distributed processing system. Users select a particular group of nodes from a list of available groups for configuration. When a particular group is selected the group item column is populated with a list of computing nodes contained within the selected group of nodes. Group items column permits a user to select a particular computing node within a selected group for configuration. A selects a node from the list of available nodes to specify computing node parameters listed in the remaining columns.

Item information column provides a user with a list of computing resources and related resource parameter settings used by the computing node during operation. In the example of the list of computing resources includes an entry for processor information for the particular computing node and a plurality of entries for each network connection present in the particular computing node . Processor information entry provides useful system parameter and resource information for the processors present within the selected computing node. Each of the network connection entries provides network address and related parameter information for each respective network connection available in the selected computing node. Users may view and alter these system parameters to configure the operation of the selected computing node.

Node role column provides a list of available computing node roles present within distributed processing system . A user may configure the selected computing node to perform a desired computing node role by selecting a checkbox or similar user interface selection control from the list of available roles . Configuration utility application may provide an entry in the list of available roles that may be supported by the set of computing resources available in a node. For example an I O node may not be included within the list of available roles if necessary storage devices are not attached to the selected computing node. Once a user selects a desired computing node role and alters any parameters as needed configuration utility application passes necessary information to the selected computing node to reconfigure the computing node as specified. The needed configuration information may be obtained from a template used for each type of computing node role available within system .

Configuration utility application includes other information column to provide any other useful system parameters such as network gateway IP addresses and other network IP addresses that may be known and needed in the operation of the selected computing node. Configuration utility application may pre configure the system parameters to desired values and may prohibit a subset of parameters from being altered under user control to minimize conflicts within various computing nodes of system . Particularly IP addresses for computing node connections network gateways and related values may not be available for altering by individual users as the alteration of these parameters may cause conflict problems with other computing nodes within the system. Any well known user level authorization mechanism may be used to identify users who may and users who may not alter individual parameters using configuration utility application .

In one embodiment distributed processing system provides for the creation and definition of application roles as well as the node level roles discussed above. In particular distributed processing system provides another layer of abstraction by allowing application roles to be defined that detail a specific assembly of software applications for execution on application nodes .

An application role may for example specify for each software assembly one or more specific software applications such as database software applications accounting software inventory management applications travel reservation software word processing applications spreadsheet applications computer aided design CAD software or any other type of software application. An application role may also specify for each software assembly any application services that are required to support the execution of the respective software application such as java virtual machines web services business logic software services or other types of services. The software assemblies may be stored as a collection of software applications and services or as a loadable software images.

By making use of application roles as well as node roles distributed processing system virtualizes the specialization of a node beyond the operating services provided by the node. An enterprise may utilize these techniques to dynamically provision software applications for execution by application nodes within distributed processing system . In other words an enterprise or other organization may utilize the techniques to control the initial and dynamic configuration of the software applications and application services that execute on the operating system for each application node .

A node within distributed processing system such as command node or administration node presents an interface that allows a user or software agent to monitor and schedule the deployment of applications within the enterprise. The interface allows the user or software agent to define priorities between the application roles as well as schedule the deployment of various application roles to application nodes .

Moreover an administration or command node may employ a policy engine for policy driven configuration and reconfiguration of the application roles of application nodes within distributed enterprise system . In this manner the node monitors and proactively controls the configuration of the nodes and the application software executing on the nodes.

In one embodiment the interface allows the user to update a database to define both application roles and node roles and to dynamically assign the roles to specific nodes within distributed processing system . For example a user may interface with a graphical interface to select and drag an icon representing an application role from the database to an icon representing one of application nodes . Similarly the user may graphically drag application roles from nodes to remove assigned roles. In this manner the user is able to interact with the interface presented by a node e.g. administration node to dynamically provision the application level functions of application nodes .

In one embodiment an API is exposed to user space of application nodes for controlling the allocation of resources based on application roles. By interacting with this API software such as client specific middleware is able to dynamically control the allocation of application roles to application nodes of distributed processing system . Consequently the allocation of software resources available to each of application nodes can be dynamically controlled from for example business logic software associated with enterprise applications.

Consider the situation where distributed processing system is used by an enterprise to support a set of applications including a payroll services application and a word processing application used by a set of users. During parts of a normal business day users of the word processing application may provide a significant demand for processing resources during working hours. During off peak hours such as evening and night hours user demand for use of the word processing application may reduce. As such distributed processing system may automatically allocate a larger number of application nodes A H to support the word processing application during business hours as compared to evening and night hours.

Similarly more payroll functions may be performed during evening hours as part of a batch process as compared to payroll functions performed during business hours by users of the payroll application. As such distributed processing system may automatically reconfigure a larger number of application nodes A H to support evening hours. These application process resource demand may also vary over longer time periods as some processing functions may only be performed a few times a month and a few times a year rather than on a daily bases.

In particular first set has been reconfigured to use only two application nodes E and F as compared to using three application nodes in . In other words the WORD PROCESSING role has been removed from application node A. Second set has also been reduced from four in to one application node H in . However an inventory management software application has been deployed to system . In particular application nodes B D and application node G have been assigned a newly created inventory management INV application role and utilize I O nodes B and E. In this example the number of application nodes assigned to the different application roles dynamically changes based on changes to the demand for computing resources of the specific applications. Similar examples of reconfigured applications may utilize varying numbers of I O nodes without departing from the present invention. In this manner the techniques allow an enterprise to dynamically provision applications across distributed processing system to more closely match existing demand for processing resources at any given point in time.

Various embodiments of the invention have been described. These and other embodiments are within the scope of the following claims.

