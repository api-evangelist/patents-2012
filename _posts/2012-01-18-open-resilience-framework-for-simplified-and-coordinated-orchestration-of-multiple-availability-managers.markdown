---

title: Open resilience framework for simplified and coordinated orchestration of multiple availability managers
abstract: Availability manager orchestration systems and methods and, particularly, an open resilience framework for simplified and coordinated orchestration of multiple availability managers.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09208007&OS=09208007&RS=09208007
owner: INTERNATIONAL BUSINESS MACHINES CORPORATION
number: 09208007
owner_city: Armonk
owner_country: US
publication_date: 20120118
---
The present disclosure relates generally to availability manager orchestration systems and methods and particularly to an open resilience framework for simplified and coordinated orchestration of multiple availability managers.

It is conventionally very challenging to properly manage an IT system s resilience a broad concept that includes Fault Tolerance FA High Availability HA Disaster Tolerance and Planned Outages . There are typically significant difficulties in customers ability to describe clarify and quantify availability mechanisms and in their ability to configure operate and monitor these availability mechanisms. Further various customer engagements and surveys have highlighted a customer need to be able to more easily understand the effectiveness performance and costs of utilizing the various availability mechanisms that can be provided in today s and future virtualized IT infrastructures.

Present day application availability management technologies in themselves such as HA clustering are typically notoriously complex to analyze install configure deploy and operate. When commercial Unix and to a lesser extent Microsoft HA clusters were introduced by major vendors in the early to mid 1990s it was anticipated that the relatively inexpensive technology compared to proprietary FT that preceded it would become ubiquitous. However the limitations of human cost and skill driven by complexity have traditionally limited their adoption. To this day this typically remains a serious impediment to customer exploitation of these technologies.

In addition customer workloads along with their embedded availability management are increasingly deployed on virtual infrastructures which may have their own availability management functionality. While the former typically provide more precise recovery and the latter are typically somewhat easier to configure and much broader in application scope the potentially destructive and constructive interactions between the white box availability management and the black box availability management make coherent availability management of such structures even more daunting.

One conventional mechanism is provided by VMware and Veritas Clustering Service. In this configuration VMware provides the black box availability management functionality and Veritas Clustering Service provides the white box availability management functionality. VMware has provided a very complex interface into their proprietary virtualization management functionality that is used by the Veritas Clustering Service to interact tightly with the virtualization layer to achieve its white box availability objectives. The Veritas code then has to be modified to operate within the VMware environment using these interfaces. This type of configuration is typically extremely restrictive cumbersome and limited in that it is typically restricted to a single virtualization layer provider e.g. VMware and typically requires virtualization layer specific modifications to the white box availability manager e.g. Veritas to achieve coordinated availability management.

Various embodiments provide a mechanism for allowing a user to specify his or her resilience goal at a level of abstraction that is meaningful to that user and then automatically initializing and configuring the white box and the black box availability managers to meet the resilience goals of course various embodiments may be applied in the context of one user or multiple users . In one example various embodiments provide a novel systems management component called the Resilience Framework Orchestrator that based on the user s input and the availability managers that are deployed in the user s environment automatically computes the optimal configuration parameters and settings and then configures the availability managers using their existing interfaces and behaviors in such a way that the internal operation of the availability managers is not modified and in many cases in such a way that the availability managers need not be aware that they are operating within a composed system of availability managers. Thus various embodiments of the present invention address a customer complexity issue by reducing complexity exposed to the user and are intended to be useful in a wide range of black box and white box availability manager systems and environments.

In another embodiment a computer implemented system for configuring at least a first availability manager and a second availability manager is provided comprising a user interface wherein the user interface receives from a user an availability management goal associated with at least a the first availability manager and b the second availability manager a processing element in operative communication with the user interface wherein the processing element determines based at least in part upon the availability management goal a at least one setting associated with the first availability manager and b at least one setting associated with the second availability manager and a control element in operative communication with a the processing element b the first availability manager and c the second availability manager wherein the control element receives from the processing element the at least one setting associated with the first availability manager and provides to the first availability manager the associated setting and wherein the control element receives from the processing element the at least one setting associated with the second availability manager and provides to the second availability manager the associated setting.

In another embodiment a method implemented in a computer system for configuring at least a first availability manager and a second availability manager is provided comprising providing a user interface wherein the user interface receives from a user an availability management goal associated with at least a the first availability manager and b the second availability manager determining based at least in part upon the availability management goal a at least one setting associated with the first availability manager and b at least one setting associated with the second availability manager providing the at least one setting associated with the first availability manager to the first availability manager providing the at least one setting associated with the second availability manager to the second availability manager and running a program using a processor unit to execute one or more of said steps of a providing a user interface b determining c providing the at least one setting associated with the first availability manager to the first availability manager and d providing the at least one setting associated with the second availability manager to the second availability manager.

In another embodiment a program storage device readable by machine tangibly embodying a program of instructions executable by the machine to perform a method for configuring at least a first availability manager and a second availability manager is provided said method comprising providing a user interface wherein the user interface receives from a user an availability management goal associated with at least a the first availability manager and b the second availability manager determining based at least in part upon the availability management goal a at least one setting associated with the first availability manager and b at least one setting associated with the second availability manager providing the at least one setting associated with the first availability manager to the first availability manager providing the at least one setting associated with the second availability manager to the second availability manager and running a program using a processor unit to execute one or more of said steps of a providing a user interface b determining c providing the at least one setting associated with the first availability manager to the first availability manager and d providing the at least one setting associated with the second availability manager to the second availability manager.

For the purposes of describing and claiming the present invention the term availability manager or AM is intended to refer to a systems management element e.g. comprising program code and or hardware that detects and responds to failures and other undesired anomalies e.g. at different levels in an information technology IT environment.

For the purposes of describing and claiming the present invention the term white box availability manager is intended to refer to an availability manager that detects and responds to failures associated with one or more application level resources hereafter sometimes referred to as application resources or AR s such as for example an operating system process a container a file system a storage device a disk volume a network interface or an IP number address .

For the purposes of describing and claiming the present invention the term black box availability manager is intended to refer to an availability manager that detects and responds to failures associated with one or more virtual machines VM or Logical Partitions LPAR .

For the purposes of describing and claiming the present invention the term availability management goal is intended to refer to one value selected from a set of quantized values wherein the set of quantized values characterize operation of a plurality of interrelated availability managers in one example the quantized values may be expressed in terms that are meaningful to a user . Of note the quantized values may be quantitative e.g. specific numbers or ranges of numbers or the quantized values may be qualitative e.g. descriptors such as Good Better Best or Gold Silver Bronze or Value Standard Enterprise . In another example the quantized values may comprise Tiers . In another more specific example the quantized values may comprise Resilience Tiers such Resilience Tiers may for example relate to a level of probable service reliability .

For the purposes of describing and claiming the present invention the term spatial composition is intended to refer to how a plurality of IT resources are placed relative to one another e.g. so as to limit the effects of failures .

For the purposes of describing and claiming the present invention the term behavioral composition is intended to refer to how a plurality of availability managers are configured to work together to react to failures e.g. so as to limit the effects of failures .

For the purposes of describing and claiming the present invention the term grammatical construct is intended to refer to information e.g. scripts configuration files or the like that is in a form understandable by a given availability manager such that the given availability manager is able to act upon the information presented in such a grammatical construct. In one example a grammatical construct may include semantics and syntax.

For the purposes of describing and claiming the present invention the term real time is intended to refer to cause and effect occurring approximately contemporaneously in time e.g. without significant time lag between cause and effect but not necessarily instantaneously .

With reference now to a computer implemented system according to an embodiment of the present invention is shown. The system is for configuring at least a first availability manager and a second availability manager . The system includes a user interface wherein the user interface receives from a user an availability management goal associated with at least a the first availability manager and b the second availability manager . Further a processing element is in operative communication with the user interface wherein the processing element determines based at least in part upon the availability management goal a at least one setting associated with the first availability manager and b at least one setting associated with the second availability manager . Further still a control element is in operative communication with a the processing element b the first availability manager and c the second availability manager wherein the control element receives from the processing element the at least one setting associated with the first availability manager and provides to the first availability manager the associated setting and wherein the control element receives from the processing element the at least one setting associated with the second availability manager and provides to the second availability manager the associated setting.

With reference now to a Resilience Framework Orchestrator systems management component shown as Resilience Framework Orchestrator according to an embodiment of the present invention is shown. As seen in this Resilience Framework Orchestrator may consume simplified resilience settings from a user interface not shown and then orchestrate the deployment and configuration of white box availability managers and black box availability managers as described herein. In one example the functionality associated with Resilience Framework Orchestrator may reside where shown in this . In another example the functionality associated with Resilience Framework Orchestrator may reside elsewhere in the system. In another example Resilience Framework Orchestrator may be productized as a component of an existing systems management product e.g. IBM Systems Director a Tivoli product such as TSAM or both . Of note this shows that the orchestration is applicable to a range of white box availability managers e.g. TSA A PowerHA B and WebSphere Cloudburst C and black box availability managers e.g. VMware A VMControl B and VMControl C . As described herein the white box availability managers are responsible for managing the availability of ARs A B C which are resources typically managed conventionally by an HA high availability infrastructure. Also as described herein the black box availability managers are responsible for managing virtual machines or logical partitions with no knowledge of their contents or inner workings. While much of the discussion herein focuses on usage of the IBM Tivoli System Automation product as the white box availability manager and the IBM VMControl product as the black box availability manager it should be understood that those are examples only do not represent limitations as to scope and are intended to be illustrative not restrictive .

As described herein various embodiments may be applied in the context of IT environments where multiple availability managers are installed and need to work together.

In various examples embodiments may be applied to IT environments in which black box availability managers e.g. VMware HA Services IBM s VMControl and Cloud based Availability Managers such as in Amazon EC2 and white box availability managers e.g. Veritas Cluster Services Microsoft s Cluster Services IBM s Tivoli System Automation and IBM s PowerHA are in use and could be used together beneficially .

In one example embodiments may be implemented by a Resilience Framework Orchestrator e.g. a software component a hardware component or a component comprising a combination of software and hardware .

In one embodiment a computer implemented system for configuring at least a first availability manager and a second availability manager is provided comprising a user interface wherein the user interface receives from a user an availability management goal associated with at least a the first availability manager and b the second availability manager a processing element in operative communication with the user interface wherein the processing element determines based at least in part upon the availability management goal a at least one setting associated with the first availability manager and b at least one setting associated with the second availability manager and a control element in operative communication with a the processing element b the first availability manager and c the second availability manager wherein the control element receives from the processing element the at least one setting associated with the first availability manager and provides to the first availability manager the associated setting and wherein the control element receives from the processing element the at least one setting associated with the second availability manager and provides to the second availability manager the associated setting.

In one example the first availability manager may comprise a white box availability manager and the second availability manager may comprise a black box availability manager.

In another example the user interface may receive the availability management goal from the user via a network.

In another example the availability management goal may comprise one value from a predetermined set of quantized values.

In another example the predetermined set of quantized values may be provided to the user via the user interface.

In another example the user interface may be configured to receive from the user a selection of one value from the set of predetermined quantized values.

In another example the processing element may determine based at least in part upon the availability management goal a spatial composition associated with the first availability manager and the second availability manager the spatial composition may be reflected in the at least one setting associated with the first availability manager that is determined by the processing element and the spatial composition may be reflected in the at least one setting associated with the second availability manager that is determined by the processing element.

In another example the processing element may determine based at least in part upon the availability management goal a a behavioral composition associated with the first availability manager and b a behavioral composition associated with the second availability manager the behavioral composition associated with the first availability manager may be reflected in the at least one setting associated with the first availability manager that is determined by the processing element and the behavioral composition associated with the second availability manager may be reflected in the at least one setting associated with the second availability manager that is determined by the processing element.

In another example the processing element may determine based at least in part upon the availability management goal a spatial composition associated with the first availability manager and the second availability manager the processing element may determine based at least in part upon the availability management goal a a behavioral composition associated with the first availability manager and b a behavioral composition associated with the second availability manager the processing element may determine based at least in part upon the spatial composition and the behavioral composition associated with the first availability manager a grammatical construct for expressing the spatial composition and the behavioral composition associated with the first availability manager to the first availability manager and the processing element may determine based at least in part upon the spatial composition and the behavioral composition associated with the second availability manager a grammatical construct for expressing the spatial composition and the behavioral composition associated with the second availability manager to the second availability manager.

In another example the grammatical construct for expressing the spatial composition and the behavioral composition associated with the first availability manager may be reflected in the at least one setting associated with the first availability manager that is determined by the processing element and the grammatical construct for expressing the spatial composition and the behavioral composition associated with the second availability manager may be reflected in the at least one setting associated with the second availability manager that is determined by the processing element.

In another example the configuring may include controlling the failure handling and other behavior of at least one of a the first availability manager while the first availability manager is in a post initialized state and b the second availability manager while the second availability manager is in a post initialized state.

In another example the configuring may include controlling the failure handling and other behavior in real time of at least one of a the first availability manager and b the second availability manager.

In another example the configuring may include initializing at least one of a the first availability manager while the first availability manager is in a pre initialized state and b the second availability manager while the second availability manager is in a pre initialized state.

In another embodiment a method implemented in a computer system for configuring at least a first availability manager and a second availability manager is provided comprising providing a user interface wherein the user interface receives from a user an availability management goal associated with at least a the first availability manager and b the second availability manager determining based at least in part upon the availability management goal a at least one setting associated with the first availability manager and b at least one setting associated with the second availability manager providing the at least one setting associated with the first availability manager to the first availability manager providing the at least one setting associated with the second availability manager to the second availability manager and running a program using a processor unit to execute one or more of said steps of a providing a user interface b determining c providing the at least one setting associated with the first availability manager to the first availability manager and d providing the at least one setting associated with the second availability manager to the second availability manager.

In another example the first availability manager may comprise a white box availability manager and the second availability manager may comprise a black box availability manager.

In another example the configuring may include controlling the failure handling and other behavior of at least one of a the first availability manager while the first availability manager is in a post initialized state and b the second availability manager while the second availability manager is in a post initialized state.

In another example the configuring may include controlling the failure handling and other behavior in real time of at least one of a the first availability manager and b the second availability manager.

In another example the configuring may include initializing at least one of a the first availability manager while the first availability manager is in a pre initialized state and b the second availability manager while the second availability manager is in a pre initialized state.

In another embodiment a program storage device readable by machine tangibly embodying a program of instructions executable by the machine to perform a method for configuring at least a first availability manager and a second availability manager is provided said method comprising providing a user interface wherein the user interface receives from a user an availability management goal associated with at least a the first availability manager and b the second availability manager determining based at least in part upon the availability management goal a at least one setting associated with the first availability manager and b at least one setting associated with the second availability manager providing the at least one setting associated with the first availability manager to the first availability manager providing the at least one setting associated with the second availability manager to the second availability manager and running a program using a processor unit to execute one or more of said steps of a providing a user interface b determining c providing the at least one setting associated with the first availability manager to the first availability manager and d providing the at least one setting associated with the second availability manager to the second availability manager.

In another example the first availability manager may comprise a white box availability manager and the second availability manager may comprise a black box availability manager.

In another example the configuring may include controlling the failure handling and other behavior of at least one of a the first availability manager while the first availability manager is in a post initialized state and b the second availability manager while the second availability manager is in a post initialized state.

In another example the configuring may include controlling the failure handling and other behavior in real time of at least one of a the first availability manager and b the second availability manager.

In another example the configuring may include initializing at least one of a the first availability manager while the first availability manager is in a pre initialized state and b the second availability manager while the second availability manager is in a pre initialized state.

In another embodiment a system for configuring at least a first availability manager and a second availability manager is provided the system comprising one or more processor units configured for providing a user interface wherein the user interface receives from a user an availability management goal associated with at least a the first availability manager and b the second availability manager determining based at least in part upon the availability management goal a at least one setting associated with the first availability manager and b at least one setting associated with the second availability manager providing the at least one setting associated with the first availability manager to the first availability manager providing the at least one setting associated with the second availability manager to the second availability manager and running a program using a processor unit to execute one or more of said steps of a providing a user interface b determining c providing the at least one setting associated with the first availability manager to the first availability manager and d providing the at least one setting associated with the second availability manager to the second availability manager.

In another embodiment an article of manufacture is provided comprising at least one tangible computer readable device having a computer readable program code logic tangibly embodied therein to execute at least one machine instruction in at least one processing unit for configuring at least a first availability manager and a second availability manager said computer readable program code logic when executing performing the following steps providing a user interface wherein the user interface receives from a user an availability management goal associated with at least a the first availability manager and b the second availability manager determining based at least in part upon the availability management goal a at least one setting associated with the first availability manager and b at least one setting associated with the second availability manager providing the at least one setting associated with the first availability manager to the first availability manager providing the at least one setting associated with the second availability manager to the second availability manager and running a program using a processor unit to execute one or more of said steps of a providing a user interface b determining c providing the at least one setting associated with the first availability manager to the first availability manager and d providing the at least one setting associated with the second availability manager to the second availability manager.

As described herein various principles of a resilience framework that allow a customer to specify deploy manage and analyze the resilience of a complex IT system in a consumable way is described. Also described are various customer needs and the principles of the resilience framework intended to meet those needs. These principles may include one or more of the following 

Further the use of some of these principles to specify deploy and manage the availability of an IT structure containing for example x86 servers the Xen hypervisor SLES virtual machines application resources using Tivoli System Automation as the application level availability manager and a simplified interpretation of VMControl as the virtual machine level availability manager is demonstrated.

Further the use of analytical techniques to transform the customer s simplified Resilience Tier and desired Figure of Merit FOM within that Resilience Tier into the parameters necessary to inform the deployment of the resilience structure is demonstrated.

Further a Resource Relationship Modeling Framework that can be a powerful specification and analysis tool for modeling and understanding a wide range of resilience related behaviors at various points in a system s life cycle is disclosed.

In other embodiments a resilience framework that alleviates some or all of the customer pain points discussed herein may be provided. Such embodiments may be provided specifically in support of one or more of the following activities 

In other embodiments one or more of the following resilience framework elements may be applied the resilience framework may reside within the context of an overal systems management structure as illustrated in that allows the client to 

In this example workload model one step in defining and understanding the ramifications of deploying a workload into and configuring complex resilience structures is to clearly specify the characteristics and requirements of the workload itself. Since there is typically no single description possible of the vast range of workloads that need to be supported in this example we identify common axes of discrimination into which workloads can be categorized with which customers can tag their workload and which inform the coherent configuration and operation of resilience services.

In this example we focus on those axes in particular that inform availability management recognizing and ignoring for this example that factors from other management disciplines such as performance and security may strongly interact with availability management

For the purposes of this example a workload consists of one or more Application Resources e.g. specific customer defined tasks some number of which must be available to achieve the customer s business goals. There may be many Application Resources AR s in a customer s workload. Each AR is thought of in this example as an atomic manageable application resource such as a process container an HA resource or process group whose availability could potentially be managed by a white box availability management WAM product such as HACMP TSA Veritas Cluster Services WPAR Manager LinuxHA WebSphere CloudBurst DB2 HA DR or Microsoft Clustering Services.

A customer s collection of Application Resources is deployed in this example on an infrastructure consisting of one or more Operating Systems Hypervisors and Physical Servers. Because this example focuses on a virtualized environment it is assumed that the Operating System and any Application Resources and associated availability management that execute within it are encapsulated within a Virtual Machine.

A Service Degradation Event is defined in this example to be any resource outage e.g. Application Resource Operating System Hypervisor or Physical Server that reduces the number of Application Resources in operation.

In this example any workload has intrinsic functional characteristics such as how its components in this case ARs work together dependently or independently how work is distributed among them and how they must be recovered in the event of failures. A simple taxonomy example is 

In one example the application may be amenable to WAM consisting of independent workload units granularly scalable and gracefully degradable. In one example no particular assumptions are made relative to the statefulness of the Application Resources as this may be considered an internal issue for the WAM recovery scripts.

In one example the customer s workload and its unique resilience figures of merit FOM may determine the minimum cost e.g. spatial composition resource allocation strategy for resilience e.g. within a resilience tier .

The main FOM considered in this example is a reliability oriented workload such as a SOA or e Commerce workload in which the predominant concern is to meet a minimum Service Level Agreement at any given point in time. Such workloads typically require that a given minimum number of Application Resources be available at all times in order to provide acceptable throughput and are impacted by the number of Application Resources that are disabled at any given time by one or more Service Degradation Events SDE . For such a reliability oriented workload it is asserted in this example that the FOM to be minimized is the probability that fewer than a given baseline number of Application Resources required to meet an SLA are available at any point in time and the amount of time that the AR deficit lasts when it does occur.

In one example this particular FOM may be denoted as P SLA Violation . This may be a parameter that is provided by the user to the Resilience Orchestrator to allow the Orchestrator to determine where and how resources should be distributed across the IT system to achieve the Resilience goals.

Other resilience related figures of merit that may be of interest but that are not calculated explicitly in this example include but ore not limited to 

In various embodiments an analytical framework may be provided that allows the calculation and minimization of additional FOMs as such FOMs are determined to be important.

With reference now to cost figures of merit in one example for each workload Resilience Tier and resultant mapping and component costs it may be possible to count up the number of servers hypervisor instances operating system instances and various licenses needed to implement that Tier. In this regard however an element of the cost calculation may be an accurate prediction of the spatial and temporal overheads of executing a particular availability management subsystem. To the extent feasible these costs may be based on quantitative benchmarking.

Reference will now be made to definition of server resilience tiers according to another embodiment of the present invention. In one example the resilience of servers and their associated software stacks and workloads may be considered with respect to Unplanned Outages and Planned Outages. Unplanned Outages may include any unpredictable impairments to the dependability of any element in the IT stack. Under the heading of Planned Outages predicted failures as well as user initiated outages of selected IT elements for whatever purposes may be included.

In one example the Resilience Tiers for unplanned and planned outages are uncoupled because it is possible that customers may want have a different level of capability for the two. When the handling of planned outage and unplanned outages was bundled into a single level there was typically pressure to unbundle the two for added flexibility as determined in a demonstration . However for conceptual simplicity parallelism may be retained between the Tiers for the two cases in this example.

The term recover is used to indicate that a resource s e.g. VM or higher level element operation has been restored after the occurrence of a fault. There can be many forms of recovery ranging from resumption from a boot image to restarting an application to resuming from a micro checkpoint. The term evacuate is used to indicate that a resource e.g. VM or higher level element is to be gracefully migrated from a condemned element to an approved element in the IT stack. Modes of evacuation can range from the known active migration capabilities of virtual machines to a clean shutdown and restart of an affected component.

Reference will now be made to Automated Intra Tier Configuration of Resilience Capabilities according to an embodiment of the present invention. In one example the Resilience Tiers described above may be primarily defined by the capabilities of the software resources and associated management structures comprising those tiers. Within each Tier the composition and configuration of those resources including redundancy level redundancy management policy location constraints etc. may be automatically set based on the user s specification of the desired Resilience FOMs. An example of a simple optimization calculation is described elsewhere herein.

In one example any such automatically selected configuration may be overridden by customer input. In this case it may be the responsibility of the Resilience Auditing capability of the Resilience Framework to quantitatively indicate to the customer the effect of this override.

Reference will now be made to Composition according to an embodiment of the present invention. In one example getting multiple Availability Managers to work together coherently may be a high dimensionality problem. The Resilience Framework may accommodate abstract and coordinate the composition of managed resources as well as resource managers. In this example there may be three key dimensions of composability vertical versus horizontal composition spatial composition and behavioral composition. Among the challenges in defining a useful Resilience Framework with respect to solving the high dimensionality composability problem are to isolate useful axes and motifs from within this space analyze them and demonstrate that they can be implemented. Reference will now be made to Vertical Horizontal and Hybrid Composition according to an embodiment of the present invention. A complex resilience structure may comprise several availability managers each operating within its own domain of control.

In some cases the managed resources of a given AM will be encapsulated within the managed resources of another AM. An example of this is when a WAM such as TSA is running on a cluster that is itself running on a collection of virtual machines managed by a VAM Black Box Availability Manager . The nontrivial problem of getting the AMs responsible for these nested resources to work together coherently is referred to as vertical composition.

In other cases there may be disjoint resource groups that are managed by one or more AMs. An example of this is the case in which one collection of servers and application resources reside in one bare metal TSA cluster and another disjoint but related collection of servers and application resources reside in a bare metal HACMP cluster. The problem of getting the AMs responsible for these disjoint resources to work together coherently is referred to as horizontal composition.

Vertically composed managed resources can also be horizontally composed at some levels. For example the resources managed by a TSA WAM could reside on one subset of a collection of VMs managed by a VAM and the resources managed by an HACMP WAM could reside on another subset of that collection of VMs managed by that same VAM. This issue is referred to herein as hybrid composition in one specific example a TSA cluster working together with an HACMP cluster neither of which is managed by a VAM may be considered or two VAM managed clusters say one on x and one P that have to work together may be considered .

Reference will now be made to Spatial Composition according to an embodiment of the present invention. Implementation of vertical horizontal or hybrid composition structures e.g. resilient IT structures requires consideration of how active and inactive i.e. backup resources are spatially located with respect to peer resources and with respect to resources with which they are composed and or dependent upon as described herein. This is referred to as the spatial composition problem and is typically determined by the collection of collocation anticollocation affinity and antiaffinity constraints that different managers may levy. Such requirements influence initial and subsequent placements failovers and migrations of application White Box resources within virtual machines Black Box VMs within physical servers and physical servers within data centers. In a coherent resilience configuration all such constraints must typically be jointly satisfied at all times.

In one example automatically determining and implementing the appropriate spatial composition constraints based on Resilience Tier and quantitative specification of a user s Resilience figures of merit may be a key attribute.

Since many management concerns may levy spatial composition constraints it is useful to construct a management domain neutral taxonomy. An example hierarchy of resilience motivated pairwise constraints may be defined as described below the anti colocation constraint examples are given for failure only but there may also be a planned maintenance aspect to collocation 

In another example two key elements of coordinated availability manager AM operation may be to 1 correctly interpret and convey the spatial composition requirements between availability managers and 2 accurately transform a given set of spatial composition requirements into a particular availability manager s lingua franca. Because different AMs typically have different placement interfaces capabilities and underlying models no single placement model will typically apply to all availability managers. Therefore it may be important to be able to translate from general notions of placement constraints as outlined below to the particular placement capabilities of a given AM.

Reference will now be made to Behavioral Composition according to an embodiment of the present invention. In this example Behavioral Composition refers to the principles by which composed Availability Managers dynamically interoperate in response to faults over and above the simple but sometimes unavoidable principle of disabling one or more AMs. There may be numerous complicating factors. Composed AM mechanisms may interfere AM mechanisms may be nonuniform within a given AM and by event type for example Web App DB Server AM mechanisms may vary within a given Resilience Tier.

In one example behavioral composition may be primarily determined by the specification of the Resilience Tier and secondarily determined by settings of detailed configuration parameters such as heartbeat intervals checkpointing intervals and so forth within the constituent Availability Managers.

One important Resilience Framework principle may be to seek interoperability modes in which no availability manager within a composition needs modification to operate coherently within that composition. Thus in one example solutions do not require modification of say HACMP or VMControl. Instead existing functionality and interfaces are manipulated to achieve coherent interoperability. This may be facilitated by another Resilience Framework principle which may be to limit the possible behaviors of the constituent Availability Managers to a significantly smaller number than they are capable of.

This constrained composition of multiple managers may be performed by a systems management entity called the Resilience Framework Orchestrator see e.g. In one example such a Resilience Framework Orchestrator may provide one or more useful aggregate behaviors.

Reference will now be made to an Architectural Setting according to an embodiment of the present invention. A Resilience Framework Orchestrator may consume the simplified resilience settings from a user interface and then orchestrate the deployment and configuration of White and Black box Availability Managers. shows one example of where this functionality may reside. In one specific example a Resilience Framework Orchestrator may be productized as a component of an existing systems management product such as IBM Systems Director a Tivoli product such as TSAM or both.

In one example it may be desirable for the Resilience Framework Orchestrator to not have a runtime role in orchestrating responses to failures in this example this function may be the province of the individual Availability Managers which may go about their business not knowing that they are part of a larger Resilience Management Structure . In another example the Resilience Framework Orchestrator may have a runtime role in orchestrating responses to failures.

Reference will now be made to Geographically Dispersed Disaster Resilience according to an embodiment of the present invention.

In various examples the Resilience Framework is also intended to describe IT structures that have the capability of being geographically dispersed and tolerating planned and or unplanned site level outages as described by the site level spatial composition terminology.

In this example the responsibility for determining that a site is impaired and workload needs to be recovered on or otherwise migrated to another site either as a result of a whole site failure a whole site planned outage or a partial site failure belongs to the Tivoli Systems Automation TSA product. TSA contains at least one instance on a server or VM in each geographically distributed site. It performs cross site heartbeating and exchange of certain metadata monitors the site in which it is resident and orchestrates the teardown of resources on a primary site and bringup of resources on a secondary site.

In this example detailed monitoring and manipulation including Availability Management of Black Box VMs within a site is the responsibility of the VMControl function of IBM System Director. At least one instance of ISD VMC resides on each site for this purpose. TSA contains scripting that is responsible for bringing up virtual machines on a given pool of hardware via invoking the VMControl REST APIs. This scripting could be extended if appropriate to coherently deploy and configure White Box Availability Managers as well using the Resilience Framework Orchestrator in accordance with the principles described herein.

In this example the scripting is also responsible for configuring the shared storage needed for the Virtual Machines. This consists of ensuring the replication of VM metadata OVF files etc. and other metadata from the primary site to a place whence the secondary site can access this data and perform VM recovery. This can be accomplished either via utilizing conventional replication features of the shared storage e.g. PPRC in high end storage systems or utilization of more advanced Cloud base storage schemes.

Reference will now be made to Disaster Tolerance using Virtualization according to an embodiment of the present invention see .

A demonstration has prototyped the capability to detect and orchestrate a smallish site scale failover of a collection of virtual machines to a backup site. This project demonstrated several but not necessarily all of the means of the fundamental principles of a Disaster Recovery DR solution .

In this example two collections of x86 servers analogous to sites were created each capable of running the CAM Continuous Availability Manager product see elements and . This CAM product was released in 2007 as part of the Director Virtual Systems Management product. The two sites were monitored see elements wide area cluster cluster control and cluster control by the open source LinuxHA product. Several LinuxHA scripts were written to orchestrate site monitoring data replication and site failover. The sitemon script was written to determine whether the CAM cluster is operational or not and to bring up the CAM cluster on another site using the private CAM command line APIs equivalent to the eventual VMC REST APIs . A replicamon script was implemented that was responsible for copying a VM image to the secondary site whenever a new VM was added into the CAM cluster and for ensuring that a record that this VM needed to be restarted was copied to the secondary site. At the time of site failover this information was fed into the research version of Virtual Resource Placement Services to determine where to place the VMs on the secondary site it was not assumed that the primary and secondary site were identical . Based on the recommended placement the sitemon script deployed and started the VMs on the physical servers on the secondary site.

In this example there was no run time replication of the VM s image and data from the primary site to the secondary site. As mentioned above the image and data were replicated only when the VM was instantiated on the primary site. More frequent updating of the secondary site s image and application data is a capability that may be applied for a usable DR solution and in one example may leverage IBM s data geo replication products and research activities.

In this example instantiation of the VMs on the secondary site was performed using the private CAM command line API. In another example instantiation of the VMs on the secondary site may be performed using the VMControl REST API.

In this example the LinuxHA clustering product was used. In another example the IBM TSA product may be used.

Reference will now be made to Vertically Composed White Box and Black Box Availability Management according to an embodiment of the present invention. In a demonstration regarding the feasibility of White Box and Black Box Availability Managers working together according to the Resilience Framework principles and selected Resilience Tiers deployment and fault handling using two vertically composed Availability Managers under two Resilience Tiers by automatically creating and executing scripts that configured the Availability Managers based on the Resilience Tier was done.

With regard to Vertically Composed Availability Managers in this example a simple Virtual Machine Availability Manager that was modeled on the VMControl product was implemented. This software has the capability to detect that a physical server or VM has failed and respond to that failure either by notifying the user or restarting the affected VMs either locally in the case of an isolated VM failure or on another physical server in the case of a physical server failure. This simple VAM was implemented using scripting and is capable of managing the availability of Xen VMs on x86 hardware. In this example the SLES operating system was used within the VMs.

In this example the White Box Availability Manager was implemented within the Xen VMs using the Tivoli Systems Automation product. This product has the capability of monitoring the health of user defined application resources such as processes and other user level objects and either restarting them using their resource specific recovery scripts or ignoring the failure depending on the Resilience Tier. If enabled resource recovery is performed either locally in the event of an isolated resource failure or on another OS instance in the event of an OS or server failure. In another example more sophisticated White Box behavior based on HACMP or PowerHA and WebSphere may be applied.

With regard to Spatial Composition and Controls in this example spatial composition refers to how managed resources are placed into their containing resources. Note that spatial composition may need to be considered to some extent at almost any Resilience Tier.

In this example there are two sets of managed resources Application Resources and Virtual Machines. Application Resource placement is implemented in this example using TSA and VM placement is managed in this example using VMControl.

With regard to Dispersion for the purposes of this disclosure a shorthand notation is adapted for the degree to which resources are dispersed across their containing resources in a vertical composition structure.

More particularly a real valued Application Resource Dispersion is defined that determines how those ARs are distributed across Virtual Machines. An AR Dispersion of 0 implies that the ARs are all placed on one VM and AR Dispersion of 1 implies that each AR is placed on a different VM. Intermediate values imply commensurately intermediate degrees of dispersion. In this example demonstration only a small number of AR Dispersion factors were realizable because limited disk space on the x86 servers limited the number of VM images 3 that could be instantiated.

Similarly a real valued Virtual Machine Dispersion is defined that determines how the VMs are distributed across Physical Machines. A VM Dispersion of 0 implies that the VMs are all placed on one Physical Machine and a VM Dispersion of 1 implies that each VM is placed on a different Physical Machine. In this example demonstration only a small number of VM Dispersions 0 and 1 were achievable since the demonstration only had four physical machines.

The effect of these dispersions is shown in the example of in which Application Resources A D Virtual Machines A D and Physical Servers A D are vertically composed.

The demonstration of this example supports AR Dispersions of 0 and 1. An AR Dispersion of 0 implies that all Application Resources can be placed into a single OS image and an AR Dispersion of 1 implies that no two Application Resources should be placed into the same OS image.

In this example each Tier may be augmented with the following factors each of which may strongly influence the Resilience Figures of Merit within a Tier and which may be automatically determined based on the customer s resilience requirements 

Reference will now be made to an interpretation and implementation of White Box spatial composition according to an embodiment of the present invention.

In the TSA specification of OS level AntiCollocation constraints for the Application Resources is performed using the mkrel command in the script that creates the resources although mkrel can be performed at any time after the resource has been defined . In one example the Resilience Framework Orchestrator automatically generates the TSA relationships based on the Resilience Tier and the AR Dispersion and inserts them into the resource creation script. If the AR Dispersion is equal to 0 then no AntiCollocation constraints are needed. The following automatically generated script segment describes a collection of three AntiCollocated e.g. AR Dispersion 1 Application resources wally betty and elizabeth .

Note that in this limited resource demo AntiAffinity constraints are used instead of AntiCollocation constraints. This allows TSA to distribute resources across OSes when enough are available to support full anticollocation but to collocate resources in the event of OS failures that reduce the number of OSes to fewer than the number of resources thus keeping all resources online albeit in an undesirable collocated state. TSA redistributes the resources when sufficient OS images are online to support full anticollocation.

Reference will now be made to an interpretation and implementation of Black Box spatial composition according to an embodiment of the present invention.

In the Virtual Machine Availability Manager Server level AntiCollocations are specified using the Virtual Resource Placement Services Advice VRPS interface. Based on the VM Dispersion factor these AntiCollocation advices are dynamically created and fed into VRPS prior to requesting a VM placement.

The following XML example snippet illustrates the syntax of the VRPS Coalesced Advice that indicates that VM websrv2 and websrv3 cannot be located on the same physical machine. When VM Dispersion is equal to one similar pair wise AntiCollocations are set up for all VMs in this example. Note that these constraints are maintained and enforced when VRPS is asked to compute a re placement when a server fails or is taken out of service.

Reference will now be made to Behavioral Composition Controls according to an embodiment of the present invention.

In this example the selected Resilience Tier primarily influences the behavioral composition of the Availability Managers. The behaviors of the various Availability Managers were not modified in any of the compositions created within this demonstration of the Resilience Framework.

In this example when Resilience Tier 5 is selected both the VAM and the WAM are activated and capable of responding to failures. Note in this example that VM AntiCollocation constraints must be made consistent with AR AntiAffinity constraints both at initial deployment and after fault recovery. This is a responsibility of the Resilience Framework since neither WAM nor VAM are typically aware of each other

The following Table 1 shows the actions performed under the failure modes that have been demonstrated in this example Resilience Tier 5 . Note that the generic term recover refers to restarting the Application Resource or VM in this demo but in more sophisticated VAM and WAM policies it could represent more sophisticated recovery mechanisms such as recover from checkpoint. 

When Resilience Tier 4 is activated in this example the VAM is activated but the WAM is deactivated. This corresponds to the level of availability management that is typically provided by conventional Black Box virtual machine availability managers. The following Table 2 shows the fault handling behavior of this Tier for this example Resilience Tier 4 .

For example if the IBM VMControl product is used then it may be configured to respond or not to relevant failures via its externally published REST API http publib.boulder.ibm.com infocenter director sdk index.jsp topic com.ibm.vmcontrol.ws.doc html vmc22 api sdk.html the entire contents of which is incorporated by reference herein .

Reference will now be made to a Demo Structure according to an embodiment of the present invention see .

With regard to the Managed Environment of this example boxes A B C and D represent the collection of x86 physical servers that were used to host the VMs and the ARs. This heterogeneous ensemble of servers is variously running the Xen hypervisor with SLES or SLES running in dom0 see boxes A B C and D Red Hat bare metal Windows bare metal and Windows Hyper V. No shared storage was available so all VM images were placed directly on the local storage of the physical servers. Also no Virtual Availability Management agent code per se was placed into dom0 in this demonstration all VM and server fault detection was implemented via ssh monitoring external to the managed systems.

All VMs boxes A B C were based on SLES and compatible only with the SLES based hypervisors in this example ensemble. Consequently the server s architectural capabilities were detected and the architectural compatibility constraints capability in the Virtual Resource Placement Services were exercised to prevent incompatible placements. This is automatically determined by examining a VM s vmconfig file comparing its contents to the harvested Physical Server s OS parameters and setting up the VRPS constraint file appropriately.

Each VM of this example had Tivoli System Automation boxes A B C pre installed although in another example the Resilience Orchestrator box could have installed it at deployment time . Furthermore each VM was configured such that it was capable of running any or all Application Resources boxes A B C . Note that each VM Tivoli System Automation combination A A B B C C is of the type shown at boxes . Further note that each Application Resource A B C is of the type shown at box .

With regard to the Management Components of this example various components were provided that were responsible for managing the initial deployment and the run time availability of the managed environment.

In this example a Simple Portable Availability Manager SPAM box may be implemented in lieu of the product level VMControl software. See also showing Simple Portable Availability Manager Window and websrv1 window . In this example this SPAM component is responsible for collecting all hardware and software inventory assessing the health of the physical servers and their associated hypervisors detecting architectural incompatibilities and assessing the health of each VM for which it is responsible.

When asked to perform a VM deployment SPAM intakes the list of physical and virtual servers and the Resilience Tier dynamically constructs the VRPS Advice files containing architectural compatibility and AntiCollocation constraints requests a placement from the VRPS prototype code and executes the VM placement using ssh sessions to the dom0. When asked to perform runtime Availability Management SPAM monitors all the VMs for which it is responsible using both ssh commands into dom0 and ssh commands into the VM itself to assess its health. If the VM fails then SPAM instructs the dom0 to confirm delete the VM and then restarts that VM on the same server.

SPAM also monitors all Physical Servers using ssh commands into dom0. If one or more Physical Server fails then SPAM removes that Physical Server s from its configuration file makes another call into VRPS to determine new placements of the orphaned Virtual Machines and executes a redeployment process for those VMs.

When for example Resilience Tier 5 is enabled Tivoli Systems Automation monitors all Application Resources using the monitor method of the resource management scripts and executes the start scripts either locally or remotely when the resource is determined to have failed. TSA is configured by the Resilience Framework Orchestrator as described below. In this example the central component that configures initializes and to a lesser degree orchestrates the above mentioned components is the Resilience Framework Orchestrator. It intakes the list of Application Resources and their associated TSA resource definition files and management scripts the list of VMs the list of Physical Servers the Resilience Tier and the AR and VM Dispersions.

When asked to deploy a particular Resilience Tier the Orchestrator in this example performs the following functions 

In one example the Orchestrator is not involved in the various failure recoveries outlined above. In another example the Orchestrator may be involved in the various failure recoveries. In this regard consider the following case having a Resilience Tier 5 configuration in which both WAM and VAM are engaged 

Reference will now be made to a number of High Level Commands used to Deploy according to an embodiment of the present invention this section contains a list and brief description of the major scripts used by a demonstration of a Resilience Framework Orchestrator 

Various utility scripts not listed here may inject Application Resource Virtual Machine and Physical Server failures restore failed components shut down the TSA resources disband the TSA cluster and or shut down the Virtual Machine collection.

Reference will now be made to a number of example demo scenarios according to an embodiment of the present invention in this demo there were four Physical Servers capable of running the workload three VMs and three ARs 

Reference will now be made to Analysis of Resilience Tiers according to an embodiment of the present invention.

It is generally inconsistent with the simplification principles of the Resilience Framework to expect most customers to be concerned with behavioral and spatial composition dispersions resilience orchestration and so forth. They would typically be more interested in the resilience of their workload and the cost required to achieve it and specifically the resilience of their workload in terms of resilience figures of merit described herein

In this regard a reliability oriented workload such as a SOA or e Commerce workload is one in which the predominant concern is to meet a minimum Service Level Agreement at each point in time. Such workloads require that a given minimum number of Application Resources be available at all times in order to provide acceptable throughput and are impacted by the number of Application Resources that are disabled at any given time by one or more Service Degradation Events. For such a reliability oriented workload the FOM to be minimized may be the probability that fewer than a given baseline number of Application Resources required to meet an SLA are available at any point in time. For the purposes of this example this FOM is denoted as P SLA Violation .

There are a number of possible usages of the modeling. The usage demonstrated in this example is to determine the parameters e.g. number of spare ARs dispersions and so forth necessary for a new deployment to meet the FOM within a Tier.

Reference will now be made to a Modeling Strategy according to an embodiment of the present invention.

Since for this example it is presumed that customers want to specify P SLA Violation it is necessary to transform those requirements into actionable parameters such as number of ARs VMs Physical Servers and Dispersions. Determination of these parameters such that they meet these customer specifications constitutes a multidimensional search problem that can be formulated as this is an outline of an example algorithm for transforming a customer s Resilience FOM requirement into a spatial composition mapping 

An example method for performing analytical optimizations e.g. to meet the requirements x while minimizing y that may be used in the context of various embodiments of the present invention may be found in US patent publication 2009 0157855 filed Feb. 21 2009 in the name of Adam et al. entitled DECENTRALIZED APPLICATION PLACEMENT FOR WEB APPLICATION MIDDLEWARE the entire contents of which is incorporated by reference herein .

Once the ARs and the Dispersions are found it is possible to map these settings into the spatial compositions of specified AMs as outlined in this disclosure.

Also calculated may be the expected duration MTTR in hours of an SLA violation and the expected time between individual Service Degradation Events that may or may not cause an SLA violation this is not to be confused with SLA violations but is mainly a measure of availability management activity . Reference will now be made to a Model Description according to an embodiment of the present invention. An example analytical model to demonstrate the principle is presented. Reasonable assumptions are made about e.g. failure and recovery rates such that results are valid over a wide range of reasonable possibilities. Modeling assumptions are discussed further below.

In a search and optimization loop according to one embodiment the figures of merit are calculated as follows Calculation of the Probability that SLA is Violated due to Multiple Concurrent Service Degradation Events 

Reference will now be made to a Model Usage and Results according to an embodiment of the present invention.

As mentioned earlier various embodiments of the present invention may provide an analytical modeling framework that is used to buffer the customer from the details of the configuration and allow them to view the resilience of their system in terms that have business significance to them. Thus this modeling may be used to determine the lowest cost configuration given the number of Application Resources the customer needs to accomplish their business objectives and the maximum allowable probability that that number of ARs is not available. An example of this usage is provided here.

Suppose for example that the customer specifies that he needs at least 500 ARs to meet his SLA and will accept at most a 0.0001 probability that 500 ARs are not available. All this is to be achieved at the lowest cost.

Referring first to for example Resilience Tier 5 in this example Resilience Tier 5 is the case in which both WAM and VAM are engaged 

The table above optimization results shows the summary output of the example optimization run regarding Resilience Tier 5 as it walks out into the search space starting with the lowest cost point i.e. no spare ARs and all Dispersions equal to 0 . Each row of the table represents an improvement with respect to P SLA Violation over the preceding row.

For this optimization example which took about 4 seconds to compute on a T61p if 500 ARs are needed and a maximum acceptable probability of having fewer than 500 ARs is 1.000000e 04 then the minimum cost configuration is 11 servers 5 VMs per server 55 VMs 10 ARs per VM and 550 ARs in another example a smaller cost configuration can probably be found at the expense of more computation time by increasing the search granularity and or searching more intelligently. 

Reference will now be made to for example Resilience Tier 4 In this example the modeled difference between Resilience Tier 5 and Resilience Tier 4 is that there is no White Box Availability Manager. The analytical impact of this is that the mean time to recover the AR is not 5 minutes an interval consistent with automated recovery but increased arbitrarily to 50 minutes an interval considered to be on the time scale to get a human administrator to pay attention to and manually restart the AR 

The table above optimization results shows the summary output regarding Resilience Tier 4. For this optimization example if 500 ARs are needed and a maximum probability of having fewer than 500 ARs is 1.000000e 04 same as the Tier 5 requirements then the minimum cost configuration is 15 servers 4 VMs per server 60 VMs 9 ARs per VM and 540 ARs.

In another example the optimization loop may be enhanced to minimize both the P SLA Violation and the MTTR SLA Viol. In such analysis the objective function may be a vector not a scalar.

Reference will now be made to a number of Analytical Modeling Assumptions according to an embodiment of the present invention 

Reference will now be made to a number of typical customer use cases according to an embodiment of the present invention 

Reference will now be made to a number of Deployment Options according to an embodiment of the present invention.

The framework may admit sufficient generality to allow deployment into multiple environments using multiple deployment engines. Two examples are Tivoli s TSAM and the open source XCAT tool.

TSAM In one example augment the TSAM dependency model and TSAM execution environment to support this resource relationship mode and the capabilities that are required. Use TSAM to allow a customer to specify a resilience policy at the highest level of a service stack application service and automatically create resources upon which that application service depends and their dependencies and automatically propagate all availability policies to all resources on which that high level resource depends. This may include automatically adding additional resources to the service topology to meet the needed availability goals. This results in a reusable Service Template that meets availability goals.

Transform TSAM customer defined service topologies and work flows into EM API and manipulatives as available may opportunistically use for example existing CAM APIs or may use an appropriate ensemble definition . For example 

XCAT In one example work with Toks to deploy set up KVM ensembles using perl module through REST API.

Reference will now be made to another embodiment of the present invention. This embodiment may provide for a combination of definitional instrumentation experimental measurement and analytical tasks 

Reference will now be made to a Workload Specification according to an embodiment of the present invention.

In this example a workload WL consists of a virtual machine an operating system image a virtualized application a non virtualized application an LPAR an xLPAR a WPAR or other logical container for computing work which can be assigned resource requirements CPU memory etc constraints location collocation anticollocation etc start and stop order dependencies priorities and a value indicating the color of the WL which is described below. The workload resource requirements can be provided via the WL image information system administration or from a workload characterization and specification tool such as TSAM.

Note that the systems and methods of this disclosure are equally applicable without loss of generality to non virtualized workloads combinations of virtualized and non virtualized workloads application virtualization containers xLPAR partitions and WPARs. The term WL may be used to refer to individual units of work in any such environment.

Reference will now be made to a Hosting Environment Specification according to an embodiment of the present invention see .

Hardware in this example is considered to be a collection of Components that reside in one or more Central Electronics Complexes CECs . Such Components include but are not limited to CPUs caches Memory I O and Storage none of which are assumed to be totally homogeneous. In this example in and of themselves Components cannot be used as a Server. However in a highly configurable design such as e.g. Phoenix Components may be flexibly composed to create a Logical Server of a desired size. Thus a CEC shown as element on the left in containing two CPU Components two Memory Components two I O Components and two Storage Components could be composed into two smaller Logical Servers shown as Server element and Server element in the middle in each containing one of each Component or one larger Logical Server shown as Server A element on the right in containing all Components. CECs containing more or fewer Components can of course be engineered and this disclosure is intended to apply to any number of CECs containing any number of Components.

Furthermore Logical Servers can sometimes be composed from Components that reside in different CECs with a possible loss in performance because the Components may be further apart than when they are in the same CEC. Finally in some cases Components cannot be combined with each other at all because for example they are in different CECs and there is no electrical connection between them or because they are architecturally incompatible.

Reference will now be made to a Resource Relationship Modeling of Composable Hardware according to an embodiment of the present invention.

In this example each Component CPU memory . . . may be viewed as a node in a Resource Relationship graph. This graph is called the Hardware Resource Graph HRG . Two Components can be composed to constitute part or all of a Logical Server if they are connected by a Composability Relationship in the Hardware Resource Graph. If Components are not connected by a Composability Relationship then they cannot be composed together to constitute part or all of a Server perhaps either because they are architecturally incompatible not resident in the same CEC or any other reason.

One extension to this scheme would be to associate a cost between any two Components that could potentially be composed to allow expression of locality preference or other measure of how desirable costly it would be to compose these Components.

As defined above the Resource Graph of this example only allows the description of the potential resource compositions. To allow the description of an actualized resource composition into a Logical Server an overlay graph that is a subset of the Resource Graph may be defined called the Composed Hardware Resource Graph. In this graph an edge between any two resources indicates that those two resources have been composed into a part of a Logical Server. Note that for the composition to be valid the edges in the Composed Resource Graph must be a subset of the edges in the Hardware Resource Graph. Hardware Resources that have been composed into a Logical Server are unioned in the Resource graph according to the union rules of the Resource Relationship Modeling framework.

In addition to flexibly composing Logical Servers from Components a further degree of configuration is possible in terms of creating firmware partitions within a Server Logical or otherwise as is possible in the xLPAR or PowerVM technologies. This constitutes another level of configurability equivalent to statically assigning workloads to a server and may be addressed as a Workload Placement Services capability.

In another example allocation and configuration of storage based on resource requirements may be provided.

In various embodiments of the present invention a broad concept for configuring the settings of complex combinations of resiliency management software for virtual machines and applications based on simplified resiliency settings is provided. Of note the disclosed framework may be capable of one or more of the following 

Referring now to this FIG. shows a hardware configuration of computing system according to an embodiment of the present invention. As seen this hardware configuration has at least one processor or central processing unit CPU . The CPUs are interconnected via a system bus to a random access memory RAM read only memory ROM input output I O adapter for connecting peripheral devices such as disk units and tape drives to the bus user interface adapter for connecting a keyboard mouse speaker microphone and or other user interface device to the bus a communications adapter for connecting the system to a data processing network the Internet an Intranet a local area network LAN etc. and a display adapter for connecting the bus to a display device and or printer e.g. a digital printer or the like .

As will be appreciated by one skilled in the art aspects of the present invention may be embodied as a system method or computer program product. Accordingly aspects of the present invention may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may all generally be referred to herein as a circuit module or system. Furthermore aspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium s having computer readable program code embodied thereon.

Any combination of one or more computer readable medium s may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the computer readable storage medium would include the following an electrical connection having one or more wires a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device or any suitable combination of the foregoing. In the context of this document a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system apparatus or device. The containment or storage of the program may be non transitory.

A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein for example in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms including but not limited to electro magnetic optical or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate propagate or transport a program for use by or in connection with an instruction execution system apparatus or device.

Program code embodied on a computer readable medium may be transmitted using any appropriate medium including but not limited to wireless wireline optical fiber cable RF etc. or any suitable combination of the foregoing.

Computer program code for carrying out operations for aspects of the present invention may be written in any programming language or any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like or a procedural programming language such as the C programming language or similar programming languages. The program code may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider .

Aspects of the present invention may be described herein with reference to flowchart illustrations and or block diagrams of methods systems and or computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable medium that can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function act specified in the flowchart and or block diagram block or blocks.

The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus or other devices provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

The flowcharts and block diagrams in the figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present invention. In this regard each block in the flowcharts or block diagrams may represent a module segment or portion of code which comprises one or more executable instructions for implementing the specified logical function s . It should also be noted that in some implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustrations and combinations of blocks in the block diagrams and or flowchart illustrations can be implemented by special purpose hardware based systems that perform the specified functions or acts or combinations of special purpose hardware and computer instructions.

It is noted that the foregoing has outlined some of the objects and embodiments of the present invention. This invention may be used for many applications. Thus although the description is made for particular arrangements and methods the intent and concept of the invention is suitable and applicable to other arrangements and applications. It will be clear to those skilled in the art that modifications to the disclosed embodiments can be effected without departing from the spirit and scope of the invention. The described embodiments ought to be construed to be merely illustrative of some of the features and applications of the invention. Other beneficial results can be realized by applying the disclosed invention in a different manner or modifying the invention in ways known to those familiar with the art. In addition all of the examples disclosed herein are intended to be illustrative and not restrictive.

