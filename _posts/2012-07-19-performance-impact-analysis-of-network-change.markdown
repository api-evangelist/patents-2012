---

title: Performance impact analysis of network change
abstract: A network server analyzes a change in the network, including performing a machine-learning analysis of an extrapolation space. The server accesses observed data from multiple counters that each record samples for a metric in the network. The server performs a CART (classification and regression tree) analysis of the observed data to select the counters whose metrics affect a target network performance, such as latency. The server estimates an extrapolation space based on the observed data for the selected counters. The server then performs a machine-learning analysis of the extrapolation space based on a kriging model of the selected counters.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08903995&OS=08903995&RS=08903995
owner: NetApp, Inc.
number: 08903995
owner_city: Sunnyvale
owner_country: US
publication_date: 20120719
---
Embodiments described are related generally to network analysis and embodiments described are more particularly related to analyzing a performance impact associated with a network change.

Portions of the disclosure of this patent document can contain material that is subject to copyright protection. The copyright owner has no objection to the reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever. The copyright notice applies to all data as described below and in the accompanying drawings hereto as well as to any software described below Copyright 2012 NetApp Inc. All Rights Reserved.

Changes in an operating parameter of a network system can impact network performance with respect to another operating parameter of the network system. Systems designs attempt to provide tools to allow the systems to predict system behavior when the system detects a network change. Current prediction tools are based on predictive system models which are models of system variables and relationships among various variables.

Predictive models can traditionally be classified as white box or black box models. White box modeling models changes based on known data and known system configurations. Thus white box modeling predicts behavior for known circumstances. Black box modeling allows for interpolation which is prediction not based on prior observation but instead based on training data. The training data is data that estimates what might happen if a variable in system operation changes. Both types of modeling predict system behavior when a change is introduced into the system. Both types of modeling can be used to proactively assess system behavior prior to making the change that is assessed by the modeling.

Increasingly white box modeling is becoming more difficult and time consuming due to highly complex system configurations and the complex interplay between the various hardware and software components in network systems. An example of a network system that is increasing in complexity is a network storage system which can experience significant interaction between hardware and software components under different workload conditions.

Similarly black box modeling is becoming increasingly difficult and time consuming. Black box models are only good for known configurations and system configuration operation combinations for which they have been trained. Performing accurate a priori training for black box models is impractical for the combinatorially expansive number of hardware and software combinations of current network systems.

Additionally the dynamic runtime nature of system configurations and system operating conditions negatively impacts the ability to model the system. Even if the possible combinations of system configurations can be foreseen there may not be a practical way to model all the combinations in a real system with limited computational resources. The combinations of system configurations can lead to configurations that are impractical to accurately model with any degree of confidence which makes it impractical to train the system for black box modeling.

A network server analyzes a change in the network including performing a machine learning analysis of an extrapolation space. The server accesses observed data from multiple counters that each record samples for a metric in the network. The server performs a CART classification and regression tree analysis of the observed data to select the counters whose metrics affect a target network performance. The server estimates an extrapolation space based on the observed data for the selected counters. The server then performs a machine learning analysis of the extrapolation space based on a kriging model of the selected counters.

Descriptions of certain details and embodiments follow including a description of the figures which can depict some or all of the embodiments described below as well as discussing other potential embodiments or implementations of the inventive concepts presented herein.

As described herein a network server performs extrapolation based on a kriging model of a network system. Thus the network server can predict system behavior for unobserved regions of system operation in a dynamic environment. The network server analyzes an impact of an expected change in the network. The server accesses observed data from multiple counters that each record samples for a metric in the network the metrics together defining the system behavior. The server predicts system behavior for unobserved regions of system operation by using a combination of analysis techniques.

The server first performs a CART classification and regression tree analysis of the observed data to select the counters whose metrics affect a target network performance. Such an operation could be referred to as pruning the system counters down to only those that affect the desired target performance. Then the server performs a machine learning analysis with a kriging based analysis on the relevant nodes as selected or identified by the CART analysis. The overall analysis with the combination of analysis techniques is a black box modeling approach.

In one embodiment the analysis is applied to a storage system where the analysis approach can be referred to as M LISP Machine Learning based Incremental Storage Provisioning . In storage systems the question to be answered is how response time of the system will change for a workload if more load of the same workload type is added to the system. The question is traditionally a hard problem given the requirement to address an unobserved region of system operation and given the fact that system behavior will not necessarily be linear in the unobserved region.

With the combination of analysis techniques as described herein the system can build a model of the system that allows it to proactively predict the impact of adding a workload to the system. The system observes the behavior of the storage system by storing data with multiple counters for past usage and builds a machine learning based black box model using the counters. For any extra workload the system extrapolates the model behavior and predicts the performance. In one embodiment the prediction is generated incrementally by making iterative predictions into the extrapolation space for sub portions of the desired change until achieving a prediction of the entire desired change.

The use of the CART analysis allows the system to prune data to the data of interest such as latency versus IOPS input output transactions per second for a storage system while also enabling runtime modeling and prediction. The CART analysis provides the system model with the ability to respond to non linear behavior in the unobserved region. The kriging based analysis provides the system model with confidence bands for the ultimate predictions.

Clients access resource over network from one or more resource servers . Resource corresponds to the type of system e.g. resource can be a database in a database system or storage in a storage server system . The load on a given resource server is generated by requests from clients . The requests can be in the form of data access requests service requests I O input output requests which are requests to read and or write data or other client access. Some clients such as clients directly access resource server over network . Other clients such as client can access resource server via host device which in turn access the server via network . Host device can be a proxy or a distributed node.

In one embodiment access requests are made and serviced in system as workloads or loads not specifically shown . Workloads are separate or distinct streams of requests from different sources or streams of requests that originate from different applications or different clients. Thus each workload associated with requests from a different application can be referred to as a distinct workload. The different workloads can access either the same or different resource . System can monitor the loading on resource server with management server .

In one embodiment management server is part of resource server . In an alternate embodiment management server is separate from resource server . Management server implements analysis of expected changes to a load of resource server . Management server can evaluate the expected effect on system loading for any system change. Management server represents the components involved in performing a CART analysis in combination with a kriging based analysis. In one embodiment the kriging based analysis is a modified kriging analysis modified as described in more detail below.

System includes server side and client side . Server side includes one or more storage servers which serve data to clients over network . Storage server manages storage including I O input output transactions to and from the storage resources. In one embodiment access requests to storage server are managed as separate workloads and have associated SLOs service level objectives which can also be referred to as service level agreements or SLAs . The SLO for a workload indicates a quality of service at which storage server should service the workload.

As mentioned previously clients are the sources of the workloads. It is common for the workloads to be dynamic both in terms of how many resources are requested by each individual workload over time as well as in the number of active workloads at any given point in time. System includes various metric monitoring components not shown see below to monitor various system metrics such as latency bandwidth I O transactions and other metrics. The metrics change over time with the dynamic nature of system . Management server can use the monitored data to predict how an increase in a workload or workload type would affect system . More particularly in one embodiment management server predicts whether the increase in workload in system would prevent storage server from fulfilling the SLOs for the active workloads in system .

In one embodiment host of client side includes hardware resources which represent one or more hardware resources on which a client can execute. In one embodiment hardware resources are shared resources in a virtualized environment. VMs virtual machines can be clients or host environments for clients. Virtualized resources represent the VMs or other virtualization components implemented on hardware resources . It will be understood that virtualized resources include one or more logical instances of mappings or allocations of hardware resources to create a logical environment in which programs can be executed. As used herein instantiation refers to creating an instance or a copy of a source object or source code. The source code can be a class model or template and the instance is a copy that includes at least some overlap of a set of attributes which can have different configuration or settings than the source. Additionally modification of an instance can occur independent of modification of the source.

Management server predicts expected system behavior through the use of a CART analysis followed by a kriging based analysis. The combination of the CART analysis and the kriging based analysis enables management server to extrapolate into unobserved behavior and predict how increased loading in system will affect the system behavior managed by storage server . Contrast such an approach to traditional white box modeling which is increasingly ineffective due to the dynamism of the system behavior and the innovations in the storage system stack. Additionally contrast such an approach to traditional black box modeling which is increasingly ineffective due to the difficulty to a priori train the model for dynamic and multitenant data center environments.

Management server represents one embodiment of a management server according to any embodiment described herein. Counters can be located in one or more locations throughout system . For example management server or a resource server or a storage server can include one or more counters . In one embodiment one or more counters are located at client host . In one embodiment one or more counters are located at computer server . Compute server represents a server other than a resource server of system which provides computation services to the resource server or otherwise offloads a load of the resource server.

Counters can include any monitoring or storing or logging of behavior information for system . The information can be stored as specific samples of conditions in the system at a given point in time. Behavior information refers to any operating condition or environment configuration for the system. Each such condition can also be referred to as a system metric. System metrics can include system specific metrics and or workload specific metrics. System specific metrics can refer to overall bandwidth utilization system wide read to write ratio CPU central processing unit utilization or any other metric or hardware or software state measurement.

Workload specific metrics can refer to metrics that are specific to a given workload. It will be understood that such metrics are typically already collected in many systems to evaluate service level objectives SLOs for the workloads. Workload specific metrics can include any metric that can be recorded for individual workloads and include but are not limited to IOPS input output per second data throughput latency random sequential read ratio or I O input output size. Management server includes measurement collector to access counters to collect the metrics monitored by the counters.

Measurement collector executes through network access hardware that is part of hardware resources of management server to interface with counters . The network access hardware can include any form of network connection including network interface circuits over which measurement collector can access counters and receive data from them. In one embodiment measurement collector includes driver software or access to driver software to enable the access to counters .

In one embodiment aspects of management server are implemented as part of a management layer of the resource server. Thus management server can be implemented as part of a MADE monitoring analysis planning and execution loop of a management routine of a resource server. Management server provides the ability to evaluate resource utilization changes in system to allow the system to aggressively increase resource utilization while not violating any performance protection or availability SLOs. Management server accomplishes the evaluation by providing predictions that answer the following two questions 1 whether the SLO requirements of an application or workload being provisioned can be satisfied by the underlying resource server and 2 whether deploying the new workload will negatively impact the SLOs of the already deployed applications or workloads.

Evaluation module executes on processing resources that are part of hardware resources of management server . Processing resources can include CPUs central processing units or discrete processors memory or other caching hardware registers logic units e.g. ALUs arithmetic logic units multipliers or other logic. Evaluation module can be executed at least partially in software and used to configure a hardware device to perform the functions described and or can be executed at least partially in hardware devices specifically programmed to execute the functions described.

Evaluation module generates a model of the behavior of system to make the predictions. Measurement collector measures system counters to gather the data recorded or observed for the behavior of system . All data contributes to the observed region of system behavior. The observed behavior as collected by all counters includes data that is relevant to a number of different possible target performance indicators. Management server can make predictions about any target performance informed by the system counters. In one embodiment latency versus IOPS is a target performance indicator of interest in storage systems. Other target performance indicators will be of interest for various different systems and network configurations.

CART module of evaluation module filters the data from counters to focus only on data relevant to the target performance indicator. Kriging module extrapolates system behavior based on the relevant data provided by CART module . In one embodiment a complete set of measurement data is collected by management server periodically and stored as raw measurements by measurement collector . In one embodiment evaluation module is configured to evaluate a specific target performance of system . For example evaluation module can be configured to evaluate the system for latency to allow it to predict how additional loading in system will affect latency of existing loads.

Not all counters will record data relevant to latency. The counters that are relevant to a specific target performance indicator such as latency for a workload type will not always be the same counters for various workloads or system configurations. Thus CART module determines a signature for the target performance indicator. The signature identifies a set which can be considered a subset of all the counters of counters that are interdependent and thus affect the target performance indicator. Counters not in the set identified by the signature do not influence the counters in the signature. The data from counters as filtered by CART module then indicates observed behavior or observed data samples for a range of system behavior. For purposes here call the observed range 0 to X.

Kriging module extrapolates in contrast to interpolation for which kriging analysis has traditionally been used. Based on the observed data filtered by CART module kriging module generates a model with which to predict system behavior for a range of X to X X. In one embodiment as described in more detail below the kriging module makes the prediction in the unobserved region by incrementally predicting the samples. The prediction can be sample by sample on the same granularity as the observed samples or it can be on a coarser granularity. Thus in one embodiment kriging module makes predictions for each X from X to X X where is an integer multiple of .

Management server is illustrated architecturally with certain functions shown in a framework based on flow of operation by management server . Storage system includes various counters that take samples or measurements of system state metrics. Measurement collector accesses the counters of storage system to gather the sampled data from the counters. It will be understood that the gathered data will be data that is relevant to a particular network performance of interest or a target performance indicator to be evaluated by management server . In one embodiment storage system internally monitors various metrics at regular intervals e.g. every 30 seconds which can be collected by measurement collector periodically e.g. every hour . Measurement collector stores the collected data as raw measurements .

In one embodiment management server cleans up and samples data with data filter to create working sample set of measurements . The filtered data represented by working sample set forms one input to the evaluation module . An external source not shown generates evaluation request which indicates a change and prompts evaluation module to perform an extrapolation analysis to predict the behavior of storage system in light of the requested change. In one embodiment the external source is an application or a module that provisions storage in storage system or other resource in a different type of system .

In one embodiment evaluation request is broken up into a series of incremental requests and evaluation module iteratively evaluates each incremental request building one analysis upon the results of the previous request. In one embodiment evaluation module separates a requested change into incremental portions and iteratively evaluates the change by basing each iteration on the previous iteration until achieving a result. Thus evaluation module uses learned system behavior e.g. based on working sample set to provide a result. The result is provided to the requester as provisioning advice or other advice depending on the system type.

Thus it will be understood that management server performs live evaluation of a change in the system referring to the fact that management server provides evaluation during the runtime of the system using data gathered during runtime of the system when the system is operational. The modeling performed by management server is thus current to the system and can therefore respond to the dynamic nature of modern networked systems and multitenant environments.

Management server models the system dynamically and is thus system and workload agnostic in contrast to traditional modeling approaches which require information about specific configurations. As discussed above management server determines what counters are relevant to the target performance indicator and bases its analysis on those counters. The determining what counters are relevant could be referred to as constructing a load signature for the loading in the system that will be the subject of the analysis. Thus part of the analysis is to determine what specific counters should be modeled to make the desired prediction. In one embodiment a CART model includes a certain amount of variance in each leaf node for the observed samples for the target performance indicator e.g. latency . The variance allows for modeling nonlinear behavior of the system.

In one embodiment storage system includes components such as compute servers that operate in parallel with a storage server to perform management operations. In such an embodiment measurement collector raw measurements data filter and or working sample set can exist within storage system . Thus management server or the functions of management server as shown can be distributed in system . Alternatively the primary system server e.g. the storage server can perform one or more of the enumerated functions. Either with the storage server or other compute servers performing operations in the storage system the ability of the system to perform the operations depends on availability of resources in the system. In one embodiment management server performs evaluation services for multiple storage systems .

In one embodiment management server always uses cleansed data for predicting future behavior. Thus data filter can include cleansing functions to produce working sample set . Data cleansing includes functions such as checking for missing counter values checking for constant values and determining if any counters are inactive. Data filter can eliminate counters from consideration when they are not active or counters that are constant meaning they are measuring a metric that is not dynamic and therefore do not need to be considered when predicting how a change will affect the system. If the number of instances of missing values of a counter is larger than a predetermined threshold the particular counter can also be eliminated because it does not provide enough good data to guide the prediction.

In one embodiment management server collects more data than can be used in a practical implementation due to limits on computational capacity and or time. Thus in one embodiment data filter includes a sampling function to sample raw measurements to reduce the number of measurements under consideration in working sample set . It will be understood that the reduction on the number of measurements in working sample set is different than the pruning of data performed by CART analysis in evaluation module . The CART analysis prunes samples based on relevance to the target performance indicator. Data filter can reduce the amount of data by sampling collected data.

It will be understood that the larger working sample set is the more accurate the prediction is expected to be. However the larger working sample set is the more time the evaluation or analysis is expected to take. Thus there is a tradeoff between accuracy and time of prediction. In one example implementation working sample set size of 500 1000 samples was observed to work within desired constraints of accuracy and time. Each implementation will be different. In one embodiment data filter samples more heavily from the most recent data thus weighting the prediction to the most current system operation which can function to more accurately predict current behavior trends of the system.

Consider an example where a particular user e.g. an application generates a workload that uses storage system up to 1000 IOPS while still meeting a latency requirement for an SLO for the workload. If the user makes a request that will increase the workload to 1500 IOPS the storage system can determine whether increasing the workload to 1500 would result in a violation of the workload s SLO or would cause other non permitted results in the system. Interpolation techniques as are traditional are not effective at predicting the results of increasing to 1500 IOPS because all past samples i.e. observed samples or the observed region are for system operation with IOPS of less than 1000. In one embodiment management server extrapolates the effect of increasing to 1500 IOPS by iteratively generating synthetic or artificial samples. For example management server can generate a sample that contains the IOPS workload signature counters of interest and latency at 1050 IOPS and then proceed to 1100 IOPS and so forth until modeling the behavior at 1500 IOPS.

In one embodiment evaluation module includes workload signature computation which evaluates data from working sample set in response to evaluation request . Evaluation module accesses working sample set to obtain the system behavior samples to be used for making the prediction. In one embodiment evaluation module constructs a workload signature with workload signature computation . In one example embodiment a workload signature can be thought of as a set of counters that determine latency against IOPS for a workload. In such a workload signature workload signature computation determines which counters should be included in the signature by computing what counters belong to a set of counters related to latency and IOPS. Thus the workload signature should include all counters that influence the signature and no counter that influences the signature should be outside the signature set.

Despite the illustration of workload signature computation as a separate component in one embodiment the workload signature is actually computed through evaluation or analysis via CART. Evaluation module stores the resulting calculated workload signatures labeled as CARTs for ease of labeling in system . In one embodiment evaluation module increases IOPS to fulfill evaluation request . The increase could be a full increase to the desired IOPS or could be incremental as discussed above. The increase can be a fixed amount configured into evaluation module or indicated by evaluation request .

Evaluation module obtains the relevant CARTs which were previously computed for computing a prediction. In one embodiment evaluation module stores the CARTs as metadata for the sample data. Evaluation module can obtain relevant CARTs and extract relevant samples from the obtained CARTs.

In one embodiment the CART analysis as illustrated prunes the obtained observed data to counters that have spatial dependency with the target performance indicator such as IOPS versus latency as illustrated in system . Spatial dependency can be understood mathematically as a statistical relationship between multiple random variables in a collection or set of variables. The statistical relationship is such that with spatially dependent variables a result of a statistical computation on one of the variables can be used to predict a result of a computation on the other variable. Thus counters are spatially dependent when an analysis of their data can be used to predict an outcome of an analysis of another counter. In particular the dependency can be related to a target network performance.

Another way to understand the concept of the dependency is that evaluation module determines a transitive closure of the counters. A transitive closure of the counters is a minimal set of counters together with the target of interest such as IOPS and latency which results in a set where every member of the set except the independent variable in the model e.g. IOPS is dependent on at least one member within the set and not dependent on any other counter outside the set.

In one embodiment evaluation module generates the samples from the stored data to reduce an amount of data used for a kriging analysis to the relevant data. It will be understood that kriging analysis is traditionally used for interpolation in geostatistics but not for extrapolation. In one embodiment kriging of evaluation module is a modified kriging analysis. The kriging analysis can be modified in that it can make a prediction based on observed data and training data. In one embodiment synthetic samples include relevant samples of observed data from CARTs as well as synthetic samples generated based on an assumption of increased IOPS .

The CART analysis defines an extrapolation space or extrapolation region and kriging computes predictions in the extrapolation space which is outside the range of observed data from working sample set . Synthetic samples can be fed back into kriging in combination with observed samples to generate a prediction in the extrapolation space. In an embodiment where the IOPS is iteratively incremented each increase of IOPS can compute a prediction based on the previously generated samples that represent the results of kriging which are stored in synthetic samples . Kriging not only makes a latency prediction in the extrapolation space but also produces a confidence band along with the prediction. Whereas traditional kriging interpolates based on an assumption of linearity in a relationship between variables evaluation module can predict nonlinear behavior by first filtering data with a CART analysis prior to a kriging analysis. Thus evaluation module can provide a kriging analysis with confidence predictions for nonlinear data.

When all iterations of the analysis are completed synthetic sample stores a result of the analysis. The result is analysis output which is sent to the requester. In the illustrated example of a storage system evaluation the effect of IOPS versus latency evaluation module can produce and send back provisioning advice to indicate how resources should be allocated. Such provisioning advice can be whether or not to increase the IOPS as requested or to only increase the IOPS if more resources are allocated in the system.

Assume for purposes of the following that evaluation module iterates the CART and kriging analyses for increments of IOPS as shown in system . As evaluation module increases IOPS the relevant samples that can be used for predicting the extrapolated space can be extracted from CARTs . In one embodiment CARTs stores one CART model for each counter. For each increment in IOPS evaluation module can retrieve previously generated synthetic samples . Kriging uses both the samples obtained from CARTs and synthetic samples together to train a kriging model of the system behavior.

In one embodiment kriging performs an unconstrained kriging analysis or computation for each counter separately to predict a new value for each counter for the increased IOPS. Kriging can then use the new counter values to predict an expected latency value for the increased IOPS. Kriging can then store the new counter values together with the new expected latency value and increased IOPS value as a new synthetic sample . Kriging can also store the confidence band for the latency prediction. Synthetic samples are all considered together to generate the predicted behavior in the extrapolation space analysis output .

Extrapolation illustrates an embodiment that incrementally generates outputs which are then used as a basis for a next calculation. Thus extrapolation generates as output latency values for a number of samples X X. Extrapolation determines a new sample IOPS based on previous sample IOPS plus X using kriging with IOPS and previous sample counters as input. Extrapolation extrapolates latency for the new sample using modified kriging as IOPS and signature counter values as input. Kriging receives training data from getTrainingData which is explained below.

More specifically referring to the pseudo code in line 1 the code initiates a value z that is outside the observed space i.e. in the space beginning with k 1 . In line 2 the code initializes a next sample Ciz for every Ci that is in the input set S. In line 3 the code begins a loop for IOPS from X to X X with increments of X.

In line 4 the code sets the current extrapolation IOPSz equal to the previous value IOPS. In lines 5 8 the code enters a nested loop where for every counter Ci in the set S the code computes values for Test Train and Ciz. Test is a value based on IOPSz Cj z 1 which is all previous Ci in the set S except for the current Ci and latency. Train is a value computed by the routine getTrainingData with inputs Ci and the just computed Test. Ciz is a value computed by a routine modifiedKriging with Test and Train as inputs. The nested loop ends in line 9.

In line 10 the code again computes Test but this time with the current Ci and thus Cjz as opposed to Cj z 1 IOPSz. Latency is not a basis of Test in line 10. In line 11 the code computes Train as a function getTrainingData with Latency and Test as inputs. It will be observed that in the nest loop of lines 5 9 Latency is used as an input to Test and Ci is an input to Train whereas in lines 10 and 11 Latency is not an input to Test but Latency is a direct input to Train instead of indirectly through Test being an input to Train. The nested loop generates samples Ciz that represent the predictions of what the counters will be in the extrapolation space. Outside the nested loop the code predicts latency and confidence for the generated counters. Thus in line 12 the code computes latency and confidence for each latency prediction by computing modifiedKrigingConfidence with the newly computed Test and Train as inputs.

Thus it is observed that in lines 5 9 the code extrapolates counter values based on previous values for all counters related to an independent variable Latency. In lines 10 12 the code computes latency for the current predicted counter values. Finally in line 13 the code increments z and continues the loop in line 3 for each increment of IOPS until X X is reached.

Pseudo code getTrainingData is also shown at a high level. In line 1 the code constructs a Tree based on the input counters the C input components from Test and Train . The counters in the inner loop include the previous counters while the outer loop only includes the current counter values. The Tree is a model of the network behavior with the input counters as leaf nodes in a tree.

In line 2 the code computes values for the nodes of the Tree based on a routine findNode that accepts as inputs Tree and vector. The Tree input is the model created in line 1. The vector is the Test vector which includes values for each counter based on IOPS and Latency. Thus findNode computes a value for each node in the tree under the evaluated conditions. In line 3 the code returns all samples corresponding to Nodes. The samples are the value generated for each node of the Tree based on the input evaluation conditions.

As described above the measurement server measures system counters to collect data for making a prediction. The measurement server prunes the data of irrelevant counters using CART and then uses a kriging based analysis to extrapolate system behavior. Extrapolation provides one example embodiment of how the kriging based analysis can be performed. It will be understood that a kriging based analysis can include execution of a pure kriging algorithm as is understood in the art or a modified kriging algorithm.

As seen in extrapolation training data is provided at each point that a kriging based algorithm is implemented. A naive method of providing training data is simply to provide the entire set of available data e.g. working sample set to the kriging based algorithm. However when the number of samples is large training can become time consuming and cannot be done in real time. Selectively choosing samples which are close to current sample values of the counter e.g. by implementing CART as described herein reduces the training time. The CART analysis can be performed by 1 generating a CART tree for every counter Ci.T where Ci S where S is the set of all counters or the signature as discussed above and 2 retaining a the tree Ci.T Ci S and b a node sample mapping for every Ci in S Node Sample Mapping Ci S . Thus the evaluation can generate a CART tree using the rest of the counters as input. The CART tree basically divides the space into small regions with each region corresponding to a node in the CART tree. The evaluation can then divide the entire data of k samples into smaller sets for each counter.

A management program or agent in the system requests an evaluation of a change to the system block . The counters collect measurements for whatever configuration is currently present in the system including loads in the system. A user e.g. client application can request an increase to its permitted load in which case the management agent can request the evaluation of what effect that change in system configuration or change in system behavior would have on the rest of the system behavior. It will be understood that the system behavior is a sum of all performance metrics of the system. In response to the request to evaluate the change a management server evaluates the change. Typically the request for evaluation will be a request to evaluate an effect on a specific target performance.

In one embodiment the management server computes a workload signature for the load for which the change is being requested block . The workload signature includes all counters in the system statistically related to the performance effect being evaluated. In one embodiment the management server determines what the target performance metric is and determines a change sub portion size block . As mentioned above the target performance metric or target performance indicator can be identified by the request or it can be configured into the evaluation module of the management server.

The management server performs a CART analysis to identify the relevant metrics or the relevant counters for making the evaluation block . In one embodiment the management server also generates kriging training data block . The management server performs a kriging based analysis on the relevant counters identified by the CART analysis block . The kriging based analysis also receives any training data generated to inform the analysis.

As mentioned above the analysis can be separated into sub portions of the whole change requested. The sub portions can be portions of the evaluation space or extrapolation space to be evaluated to generate a prediction of behavior. If sub portions are used the management server determines whether all sub portions of the extrapolation space have been evaluated block . If not all sub portions have been evaluated NO branch the management server increments the size of the extrapolation space to include the next sub portion in the analysis block . The management server then iterates through the analysis extending the extrapolation based on the results of the previous iteration block .

The analysis can then continue in one of two ways depending on how the evaluation is configured to operate. In one embodiment the management server generates a new or next request to an evaluation module based on the incremented extrapolation space size and the process continues at block . In an alternate embodiment the process can be considered to iterate back at block by performing the CART analysis on the new extrapolation space. The difference in perspective of where the process continues can be based on looking at the management server itself starting back at block or looking at the evaluation module of the management server starting back at block .

When the management server has performed the analysis on all sub portions of the extrapolation space YES branch of block the evaluation module of the management server provides an output analysis result block . The output result identifies a prediction of what will happen to system behavior for the requested change. In one embodiment the output result includes advice of how to respond to the requested change. Evaluation process is then complete.

As described herein a system includes a management server that provides black box modeling techniques designed for modern dynamic and multitenant environments. Dynamic environments are those where new workloads can be dynamically added and removed during runtime of the system. Multitenant environments are those where different workload types share a common storage infrastructure because a network system isolates multiple different user organizations tenants from each other while sharing an underlying hardware system. The black box model as described combines CART analysis and kriging machine learning techniques.

The combination of the techniques provides a system that can 1 execute in live mode 2 execute a prediction in an untrained extrapolation region 3 predict nonlinear behavior 4 provide modeling prediction error and 5 perform model correction via iterative live learning. Live mode or live learning refers to the ability of the evaluation system to generate models for evaluation dynamically and quickly. Quickly is a relative concept and here refers to providing model changes on the order of minutes versus hours of computation time required for traditional systems. Thus the evaluation can change as the system configuration and workload changes.

Executing the prediction in an untrained region prevents the need to a priori train a model for all possible system and workload combinations. Rather the training data can be generated on the fly for a region of unobserved behavior. The system can also predict nonlinear behavior which is important in systems where the prediction is for an indicator that is nonlinear in nature. For example as storage systems have evolved increased utilization of the system increases the different combinations of varying workload types that are co located. Thus as the utilization of the system increases the relationship between IOPS and latency tends to be nonlinear in nature. Only an evaluation module that allows for nonlinear prediction can make an accurate prediction model of the system.

Finally kriging analysis can provide a prediction error to indicate an expected accuracy of the prediction. The prediction error allows higher level management tools e.g. provisioning and migration management tools to choose to ignore the advice provided by the model or to accept the advice.

The combination of CART and kriging based analysis as described herein also provides benefits relative to other known approaches such as BASIL Relative Fitness or CMU CART which are proposed for modeling storage systems. In BASIL a system model is created via live learning by observing the impact of workload parameters. However BASIL is specifically designed to model the system behavior in the interpolation region and does not provide guidance with respect to modeling error. Additionally BASIL is only effective at predicting IOPS versus latency when there is a linear relationship between IOPS and latency.

The Relative Fitness approach creates a relative model between two systems. The model captures how two systems behave for the same workload. Subsequently when the behavior of one of the systems is known for another workload the known behavior is combined with the previously created model to predict the behavior of the second system for that workload. However modern data centers experience very dynamic behavior where users continuously add and remove workloads and devices. Thus it is impractical to build the relative models needed between different device types for effective prediction.

Both in Relative Fitness and CMU CART the model predicts performance of a storage system based on past observed samples. Thus these approaches are limited to predictions in the interpolation region and the predictions are only accurate to the extent the predicted sample is similar to the past observed samples.

In known machine learning techniques support vector regression SVR with RBF radial basis function kernels has been used to provide prediction in the interpolation region. SVR with polynomial kernels has been used for the extrapolation but is unable to associate a confidence level with the prediction. Without the confidence level prediction the receiver has no way of evaluating the results for reliability.

Storage of data in storage units is managed by storage servers which receive and respond to various read and write requests from clients directed to data stored in or to be stored in storage units . Storage units constitute mass storage devices which can include for example flash memory magnetic or optical disks or tape drives illustrated as disks A B . Storage devices can further be organized into arrays not illustrated implementing a Redundant Array of Inexpensive Disks Devices RAID scheme whereby storage servers access storage units using one or more RAID protocols known in the art.

Storage servers can provide file level service such as used in a network attached storage NAS environment block level service such as used in a storage area network SAN environment a service which is capable of providing both file level and block level service or any other service capable of providing other data access services. Although storage servers are each illustrated as single units in a storage server can in other embodiments constitute a separate network element or module an N module and disk element or module a D module . In one embodiment the D module includes storage access components for servicing client requests. In contrast the N module includes functionality that enables client access to storage access components e.g. the D module and the N module can include protocol components such as Common Internet File System CIFS Network File System NFS or an Internet Protocol IP module for facilitating such connectivity. Details of a distributed architecture environment involving D modules and N modules are described further below with respect to and embodiments of a D module and an N module are described further below with respect to .

In one embodiment storage servers are referred to as network storage subsystems. A network storage subsystem provides networked storage services for a specific application or purpose and can be implemented with a collection of networked resources provided across multiple storage servers and or storage units.

In the embodiment of one of the storage servers e.g. storage server A functions as a primary provider of data storage services to client . Data storage requests from client are serviced using disks A organized as one or more storage objects. A secondary storage server e.g. storage server B takes a standby role in a mirror relationship with the primary storage server replicating storage objects from the primary storage server to storage objects organized on disks of the secondary storage server e.g. disks B . In operation the secondary storage server does not service requests from client until data in the primary storage object becomes inaccessible such as in a disaster with the primary storage server such event considered a failure at the primary storage server. Upon a failure at the primary storage server requests from client intended for the primary storage object are serviced using replicated data i.e. the secondary storage object at the secondary storage server.

It will be appreciated that in other embodiments network storage system can include more than two storage servers. In these cases protection relationships can be operative between various storage servers in system such that one or more primary storage objects from storage server A can be replicated to a storage server other than storage server B not shown in this figure . Secondary storage objects can further implement protection relationships with other storage objects such that the secondary storage objects are replicated e.g. to tertiary storage objects to protect against failures with secondary storage objects. Accordingly the description of a single tier protection relationship between primary and secondary storage objects of storage servers should be taken as illustrative only.

In one embodiment system includes evaluation modules A B server side. Evaluation modules include logic that allows system to perform an analysis of the system in light of a predicted change in the system. In one embodiment evaluation modules are implemented on separate physical machine from storage servers . Evaluation modules interpolate system behavior in light of a system change based on a CART analysis and a kriging based model analysis.

Nodes can be operative as multiple functional components that cooperate to provide a distributed architecture of system . To that end each node can be organized as a network element or module N module A B a disk element or module D module A B and a management element or module M host A B . In one embodiment each module includes a processor and memory for carrying out respective module operations. For example N module can include functionality that enables node to connect to client via network and can include protocol components such as a media access layer Internet Protocol IP layer Transport Control Protocol TCP layer User Datagram Protocol UDP layer and other protocols known in the art.

In contrast D module can connect to one or more storage devices via cluster switching fabric and can be operative to service access requests on devices . In one embodiment the D module includes storage access components such as a storage abstraction layer supporting multi protocol data access e.g. Common Internet File System protocol the Network File System protocol and the Hypertext Transfer Protocol a storage layer implementing storage protocols e.g. RAID protocol and a driver layer implementing storage device protocols e.g. Small Computer Systems Interface protocol for carrying out operations in support of storage access operations. In the embodiment shown in a storage abstraction layer e.g. file system of the D module divides the physical storage of devices into storage objects. Requests received by node e.g. via N module can thus include storage object identifiers to indicate a storage object on which to carry out the request.

Also operative in node is M host which provides cluster services for node by performing operations in support of a distributed storage system image for instance across system . M host provides cluster services by managing a data structure such as a relational database RDB RDB A RDB B which contains information used by N module to determine which D module owns services each storage object. The various instances of RDB across respective nodes can be updated regularly by M host using conventional protocols operative between each of the M hosts e.g. across network to bring them into synchronization with each other. A client request received by N module can then be routed to the appropriate D module for servicing to provide a distributed storage system image.

As described above evaluation modules include logic that allows system to perform an analysis of the system in light of a predicted change in the system. In one embodiment evaluation modules are implemented at certain nodes but not all nodes of system . Where evaluation modules are implemented they interpolate system behavior in light of a system change based on a CART analysis and a kriging based model analysis.

It will be noted that while shows an equal number of N and D modules constituting a node in the illustrative system there can be different number of N and D modules constituting a node in accordance with various embodiments. For example there can be a number of N modules and D modules of node A that does not reflect a one to one correspondence between the N and D modules of node B. As such the description of a node comprising one N module and one D module for each node should be taken as illustrative only.

Memory includes storage locations addressable by processor network adapter and storage adapter for storing processor executable instructions and data structures associated with a multi tiered cache with a virtual storage appliance. A storage operating system portions of which are typically resident in memory and executed by processor functionally organizes the storage server by invoking operations in support of the storage services provided by the storage server. It will be apparent to those skilled in the art that other processing means can be used for executing instructions and other memory means including various computer readable media can be used for storing program instructions pertaining to the inventive techniques described herein. It will also be apparent that some or all of the functionality of the processor and executable software can be implemented by hardware such as integrated currents configured as programmable logic arrays ASICs and the like.

Network adapter comprises one or more ports to couple the storage server to one or more clients over point to point links or a network. Thus network adapter includes the mechanical electrical and signaling circuitry needed to couple the storage server to one or more client over a network. Each client can communicate with the storage server over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

Storage adapter includes a plurality of ports having input output I O interface circuitry to couple the storage devices e.g. disks to bus over an I O interconnect arrangement such as a conventional high performance FC or SAS Serial Attached SCSI Small Computer System Interface link topology. Storage adapter typically includes a device controller not illustrated comprising a processor and a memory for controlling the overall operation of the storage units in accordance with read and write commands received from storage operating system . As used herein data written by a device controller in response to a write command is referred to as write data whereas data read by device controller responsive to a read command is referred to as read data. 

User console enables an administrator to interface with the storage server to invoke operations and provide inputs to the storage server using a command line interface CLI or a graphical user interface GUI . In one embodiment user console is implemented using a monitor and keyboard.

In one embodiment computing device includes cache controller . While shown as a separate component in one embodiment cache controller is part of other components of computer . Cache controller is a cache controller that enables selective caching based on sequentiality of data associated with data access requests. The cache controller identifies address range information and bypass caching for data whose range is greater than a threshold.

When implemented as a node of a cluster such as cluster of the storage server further includes a cluster access adapter shown in phantom having one or more ports to couple the node to other nodes in a cluster. In one embodiment Ethernet is used as the clustering protocol and interconnect media although it will be apparent to one of skill in the art that other types of protocols and interconnects can by utilized within the cluster architecture.

Multi protocol engine includes a media access layer of network drivers e.g. gigabit Ethernet drivers that interface with network protocol layers such as the IP layer and its supporting transport mechanisms the TCP layer and the User Datagram Protocol UDP layer . The different instances of access layer IP layer and TCP layer are associated with two different protocol paths or stacks. A file system protocol layer provides multi protocol file access and to that end includes support for the Direct Access File System DAFS protocol the NFS protocol the CIFS protocol and the Hypertext Transfer Protocol HTTP protocol . A VI virtual interface layer implements the VI architecture to provide direct access transport DAT capabilities such as RDMA as required by the DAFS protocol . An iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a FC driver layer receives and transmits block access requests and responses to and from the storage server. In certain cases a Fibre Channel over Ethernet FCoE layer not shown can also be operative in multi protocol engine to receive and transmit requests and responses to and from the storage server. The FC and iSCSI drivers provide respective FC and iSCSI specific access control to the blocks and thus manage exports of luns logical unit numbers to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing blocks on the storage server.

The storage operating system also includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on storage devices. Information can include data received from a client in addition to data accessed by the storage operating system in support of storage server operations such as program application data or other system data. Preferably client data can be organized as one or more logical storage objects e.g. volumes that comprise a collection of storage devices cooperating to define an overall logical arrangement. In one embodiment the logical arrangement can involve logical volume block number vbn spaces wherein each volume is associated with a unique vbn.

File system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustrated as a SCSI target module . SCSI target module is generally disposed between drivers and file system to provide a translation layer between the block lun space and the file system space where luns are represented as blocks. In one embodiment file system implements a WAFL write anywhere file layout file system having an on disk format representation that is block based using e.g. 4 kilobyte KB blocks and using a data structure such as index nodes or indirection nodes inodes to identify files and file attributes such as creation time access permissions size and block location . File system uses files to store metadata describing the layout of its file system including an inode file which directly or indirectly references points to the underlying data blocks of a file.

Operationally a request from a client is forwarded as a packet over the network and onto the storage server where it is received at a network adapter. A network driver such as layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to file system . There file system generates operations to load retrieve the requested data from the disks if it is not resident in core i.e. in memory . If the information is not in memory file system accesses the inode file to retrieve a logical vbn and passes a message structure including the logical vbn to the RAID system . There the logical vbn is mapped to a disk identifier and device block number disk dbn and sent to an appropriate driver of disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the storage server. Upon completion of the request the node and operating system returns a reply to the client over the network.

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the storage server adaptable to the teachings of the invention can alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path can be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware embodiment increases the performance of the storage service provided by the storage server in response to a request issued by a client. Moreover in another alternate embodiment of the invention the processing elements of adapters can be configured to offload some or all of the packet processing and storage access operations respectively from processor to increase the performance of the storage service provided by the storage server. It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

When implemented in a cluster data access components of the storage operating system can be embodied as D module for accessing data stored on disk. In contrast multi protocol engine can be embodied as N module to perform protocol termination with respect to a client issuing incoming access over the network as well as to redirect the access requests to any other N module in the cluster. A cluster services system can further implement an M host e.g. M host to provide cluster services for generating information sharing operations to present a distributed file system image for the cluster. For instance media access layer can send and receive information packets between the various cluster services systems of the nodes to synchronize the replicated databases in each of the nodes.

In addition a cluster fabric CF interface module CF interface modules A B can facilitate intra cluster communication between N module and D module using a CF protocol . For instance D module can expose a CF application programming interface API to which N module or another D module not shown issues calls. To that end CF interface module can be organized as a CF encoder decoder using local procedure calls LPCs and remote procedure calls RPCs to communicate a file system command between D modules residing on the same node and remote nodes respectively.

In one embodiment cache controller operates in parallel to storage operating system . In one embodiment cache controller is executed as a process below OS . Cache controller is a cache controller that enables selective caching based on sequentiality of data associated with data access requests in accordance with any embodiment described herein. The cache controller identifies address range information and bypass caching for data whose range is greater than a threshold.

In one embodiment evaluation logic implements an evaluation module for operating system . In one embodiment evaluation logic can be implemented in D module . In an alternate embodiment evaluation logic implements an evaluation module separate from other modules of operating system . Evaluation logic includes logic to perform an analysis of the system in light of a predicted change in the system including interpolating system behavior in light of a system change based on a CART analysis and a kriging based model analysis.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and can implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

Flow diagrams as illustrated herein provide examples of sequences of various process actions. Although shown in a particular sequence or order unless otherwise specified the order of the actions can be modified. Thus the illustrated embodiments should be understood only as an example and the process can be performed in a different order and some actions can be performed in parallel. Additionally one or more actions can be omitted in various embodiments thus not all actions are required in every embodiment. Other process flows are possible.

Various operations or functions are described herein which can be described or defined as software code instructions configuration and or data. The content can be directly executable object or executable form source code or difference code delta or patch code . The software content of the embodiments described herein can be provided via an article of manufacture with the content stored thereon or via a method of operating a communications interface to send data via the communications interface. A machine readable medium or computer readable medium can cause a machine to perform the functions or operations described and includes any mechanism that provides i.e. stores and or transmits information in a form accessible by a machine e.g. computing device electronic system or other device such as via recordable non recordable storage media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices or other storage media or via transmission media e.g. optical digital electrical acoustic signals or other propagated signal . A communication interface includes any mechanism that interfaces to any of a hardwired wireless optical or other medium to communicate to another device such as a memory bus interface a processor bus interface an Internet connection a disk controller. The communication interface can be configured by providing configuration parameters and or sending signals to prepare the communication interface to provide a data signal describing the software content.

Various components described herein can be a means for performing the operations or functions described. Each component described herein includes software hardware or a combination of these. The components can be implemented as software modules hardware modules special purpose hardware e.g. application specific hardware application specific integrated circuits ASICs digital signal processors DSPs etc. embedded controllers hardwired circuitry etc.

Besides what is described herein various modifications can be made to the disclosed embodiments and implementations without departing from their scope. Therefore the illustrations and examples herein should be construed in an illustrative and not a restrictive sense.

