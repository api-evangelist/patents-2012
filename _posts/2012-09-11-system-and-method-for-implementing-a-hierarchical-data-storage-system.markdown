---

title: System and method for implementing a hierarchical data storage system
abstract: A system and method for efficiently storing data both on-site and off-site in a cloud storage system. Data read and write requests are received by a cloud data storage system. The cloud storage system has at least three data storage layers. A first high-speed layer, a second efficient storage layer, and a third off-site storage layer. The first high-speed layer stores data in raw data blocks. The second efficient storage layer divides data blocks from the first layer into data slices and eliminates duplicate data slices. The third layer stores data slices at an off-site location.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09189421&OS=09189421&RS=09189421
owner: STORSIMPLE, INC.
number: 09189421
owner_city: Santa Clara
owner_country: US
publication_date: 20120911
---
The present patent application is a continuation of the U.S. Patent Application entitled SYSTEM AND METHOD FOR STORING DATA OFF SITE filed on Jan. 6 2011 having Ser. No. 12 930 502.

The present invention relates to the field of digital computer systems. In particular but not by way of limitation the present disclosure teaches techniques for storing large amounts of data off site using a network connection.

Computer systems have become an indispensable tool used in modern life. Nearly every business and government agency is dependent upon computer systems for communication information storage transaction processing project development inventory management finance and a large number of other mission critical services.

Although computer hardware and computer software can easily be replaced by an entity using computer systems by purchasing new equipment the entity s specific data cannot. Thus data storage and protection is one of the most critical parts of any modern information technology infrastructure.

Recently online data storage providers have become available that offer reliable data storage services. The stored data is available through the internet and is thus referred to as data stored in the cloud . Data storage clouds provide storage on demand at very low costs while minimizing concerns over capacity planning storage provisioning data center space power cooling data backup replacing failed drives and tape management.

However it is currently difficult to user these cloud storage services since the interfaces are not compatible with existing applications. Transitioning to a new system would risk disruption of existing systems. Furthermore there are concerns about data security. But one of the most difficult aspects is the relatively slow speed of using internet storage systems.

The following detailed description includes references to the accompanying drawings which form a part of the detailed description. The drawings show illustrations in accordance with example embodiments. These embodiments which are also referred to herein as examples are described in enough detail to enable those skilled in the art to practice the invention. It will be apparent to one skilled in the art that specific details in the example embodiments are not required in order to practice the present invention. For example although some of the example embodiments are disclosed with reference to computer processing systems used for packet switched networks the teachings can be used in many other environments such as optimized data transfers to cell phones or other wireless devices on a cellular telephone network. The example embodiments may be combined other embodiments may be utilized or structural logical and electrical changes may be made without departing from the scope of what is claimed. The following detailed description is therefore not to be taken in a limiting sense and the scope is defined by the appended claims and their equivalents.

In this document the terms a or an are used as is common in patent documents to include one or more than one. In this document the term or is used to refer to a nonexclusive or such that A or B includes A but not B B but not A and A and B unless otherwise indicated. Furthermore all publications patents and patent documents referred to in this document are incorporated by reference herein in their entirety as though individually incorporated by reference. In the event of inconsistent usages between this document and those documents so incorporated by reference the usage in the incorporated reference s should be considered supplementary to that of this document for irreconcilable inconsistencies the usage in this document controls.

The present disclosure concerns digital computer systems. illustrates a diagrammatic representation of a machine in the example form of a computer system that may be used to implement portions of the present disclosure. Within computer system of there are a set of instructions that may be executed for causing the machine to perform any one or more of the methodologies discussed within this document.

In a networked deployment the machine of may operate in the capacity of a server machine or a client machine in a client server network environment or as a peer machine in a peer to peer or distributed network environment. The machine may be a personal computer PC a tablet PC a set top box STB a Personal Digital Assistant PDA a cellular telephone a web appliance a network server a network router a network switch a network bridge or any machine capable of executing a set of computer instructions sequential or otherwise that specify actions to be taken by that machine. Furthermore while only a single machine is illustrated the term machine shall also be taken to include any collection of machines that individually or jointly execute a set or multiple sets of instructions to perform any one or more of the methodologies discussed herein.

The example computer system of includes a processor e.g. a central processing unit CPU a graphics processing unit GPU or both and a main memory and a non volatile memory which communicate with each other via a bus . The non volatile memory may comprise flash memory and may be used either as computer system memory as a file storage unit or both. The computer system may further include a video display adapter that drives a video display system such as a Liquid Crystal Display LCD or a Cathode Ray Tube CRT . The computer system also includes an alphanumeric input device e.g. a keyboard a cursor control device e.g. a mouse or trackball a disk drive unit a signal generation device e.g. a speaker and a network interface device . Note that not all of these parts illustrated in will be present in all embodiments. For example a computer server system may not have a video display adapter or video display system if that server is controlled through the network interface device .

The disk drive unit includes a machine readable medium on which is stored one or more sets of computer instructions and data structures e.g. instructions also known as software embodying or utilized by any one or more of the methodologies or functions described herein. The instructions may also reside completely or at least partially within the main memory and or within a cache memory associated with the processor . The main memory and the cache memory associated with the processor also constitute machine readable media.

The instructions may further be transmitted or received over a computer network via the network interface device . Such transmissions may occur utilizing any one of a number of well known transfer protocols such as the well known File Transport Protocol FTP .

While the machine readable medium is shown in an example embodiment to be a single medium the term machine readable medium should be taken to include a single medium or multiple media e.g. a centralized or distributed database and or associated caches and servers that store the one or more sets of instructions. The term machine readable medium shall also be taken to include any medium that is capable of storing encoding or carrying a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies described herein or that is capable of storing encoding or carrying data structures utilized by or associated with such a set of instructions. The term machine readable medium shall accordingly be taken to include but not be limited to solid state memories optical media battery backed RAM and magnetic media.

For the purposes of this specification the term module includes an identifiable portion of code computational or executable instructions data or computational object to achieve a particular function operation processing or procedure. A module need not be implemented in software a module may be implemented in software hardware circuitry or a combination of software and hardware.

To make enterprise data centers more efficient the concept of a storage area network SAN was introduced. A storage area network allows computer applications to access remote computer storage devices such as hard disk arrays magnetic tape libraries and optical disc storage devices in a manner wherein the remote storage devices appear the same as storage devices attached to the local computer system. The use of a storage area network SAN allows multiple applications and servers to share storage systems. The use of shared storage simplifies storage administration since fewer storage systems need to be maintained.

Storage area networks simplify the task of creating disaster recovery systems for computer systems. When unforeseen damage due to man made or natural disaster renders a particular storage system inoperable a mission critical computer system must be able to quickly resume operation. With a storage area network SAN an independent secondary storage system located at a distant location can be used to replicate the data being stored on a primary storage system at a primary location. Thus if a disaster damages the primary storage system at the primary location the secondary storage system can be brought online to continue operations.

A storage area network generally operates as an integrated part of an operating system. Specifically the operating system provides the basic file system that is responsible for creating files writing to files reading from files duplicating files deleting files and other various file system operations. The storage area network SAN operates below the file system and only provides raw logical volume and logical block address level operations.

The difference between a traditional direct access storage system and a storage area network SAN is illustrated in . Referring to several server applications and are running on a server system . The several server applications and will generally write and read data files using a file system that is part of the operating system running on the server system . However applications may also bypass the file system to read and write raw data blocks directly to storage. In a typical computer system with a direct attached storage system the file system accesses a direct attached storage controller to access a local storage system . To use a storage area network system the direct attached storage controller is replaced with a storage area network controller . Instead of accessing a local storage device the storage area network controller issues storage requests on a storage area network . The appropriate storage device or will respond to the storage request. Applications that bypassed the file system to directly use the direct attached storage system may similarly by pass the file system to directly access the storage area network controller and use the storage area network SAN system .

With a storage area network system additional storage devices can be added as necessary. Decoupling server systems from their storage components allows the system to be more robust and easier to maintain. For example if a particular server system malfunctions then that malfunctioning server system can be quickly replaced with a new server system that can immediately access the data for that server which remains available on the storage area network.

Although storage area networks provide many advantages most small and medium sized enterprises do not create storage area networks. Although the task of providing computer storage may seem to be a relatively simple task it is neither easy nor inexpensive to design build and maintain a high quality fault tolerant data storage area network. The storage area network equipment itself tends to be expensive. And even if an entity were willing to purchase the needed storage area network equipment the creation of a good fault tolerant storage area network requires cooling systems back up power systems and off site secondary storage for disaster recovery.

Due to the relatively high costs of deploying and maintaining a high quality fault tolerant storage area network most small entities just use traditional computer server systems that have direct attached storage. The data stored on such computer server systems is typically protected with periodic data back ups. Although periodic data back ups provide some data security periodic data back ups lack many of the features provided by a high quality fault tolerant storage area network. For example when a problem with a server occurs it takes time to bring a new server system online and restore the data from most recent back up. Furthermore new data that was created since the last back up operation may be lost. Such systems also generally lack the high end reliability features such as cooling systems back up power systems and off site secondary storage for disaster recovery.

Cloud computing is a new paradigm for computer usage. With traditional personal computers a computer user runs applications on a local personal computer system and stores application data on a local storage system. With cloud computing some or most of the processing and or data storage is handled by remote computer systems coupled with the internet the cloud . Thus with cloud computing various technical details are abstracted away from the users who no longer need to maintain the technology infrastructure that resides in the cloud on the internet . The term cloud derives from the common usage of drawing a cloud as an abstraction in computer network diagrams. For example provides a few examples of cloud computing wherein the internet is represented as an abstract cloud.

Many different forms of cloud computing are presented by cloud computing service providers. Some companies provide computer processing services wherein a customer may provide one or more programs that are to be run on computer systems run by the cloud computing service. Internet website hosting can be viewed as a cloud computing service wherein the cloud computing service provider provides the computer system the internet connection the storage and a web server application such as Apache. This is a very commonly used cloud computing service since web sites require good reliable internet connections thus it is best to locate web servers at internet service provider facilities.

Many cloud computing providers offer software as a service wherein various user application programs are made available to users. For example a typical cloud computing user application provider allows users to run desired application programs on a remote server across the internet. The users access the user application programs using a client system . The client system may be a web browser or a thin client system that simply accepts user input in the form of keyboard and mouse input and displays video output. In such a cloud computing model the user application software and data are stored on remote server systems that are accessible on the internet . Thus the application user does not need to maintain the software application with patches and upgrades the computer system hardware that executes the software application or the data storage system used to store application data. The user simply needs an appropriate client system and connection to the internet .

One relatively simple form of cloud computing is a remote data storage service . With remote data storage services the cloud computing provider receives and reliably stores data for the benefit of the customer. Although this may seem to be a relatively simple service it is neither easy nor inexpensive to design build and maintain a high quality fault tolerant data storage system as set forth in the previous section.

If a small entity were easily able to outsource the creation of a fault tolerant data storage system to a cloud computing provider that offers data storage services then that small entity would be relieved of a very difficult information technology task. However at the present time it is not very easy to use cloud computing storage systems. Specifically there are several compatibility security and performance issues that prevent most small entities from using such cloud computing storage systems.

A first problem is that most cloud computing based storage services do not provide an interface that is easy for a small entity to use. A typical cloud computing based storage service uses the REpresentational State Transfer REST system for data transport. With a typical cloud computing based storage service customers are allowed to create objects of data that are named and transported to the storage service. Some implementations use the standard the Hypertext Transport Protocol HTTP . The REST data system works well for communication between a client application and server system. However it is not designed to handle raw data transfers handled by storage systems.

Another problem with the use of cloud computing based storage services is data security. Computer systems are often used to store highly confidential information such as credit card numbers trade secrets financial data medical records etc. Thus there are financial moral legal and economic reasons to avoid using any external data storage service that cannot be fully trusted. Furthermore even if one fully trusted a cloud storage service provider the data communication link between the customer and the cloud computing based storage service itself could be monitored by others.

Even if the interface compatibility and security issues could be addressed one of the greatest challenges in using a cloud computing based storage service is the relatively slow speed of such internet based data storage services. The rate at which data can be transmitted to and received from a cloud computing based storage service is limited by the internet connection between an entity and the cloud computing based storage service. Due to these various difficulties the adoption of cloud computing data storage services has been limited.

To provide small and medium sized entities with a high quality fault tolerant storage system the present disclosure introduces a hybrid storage system that combines advantages and flexibility of a storage area network with the reliability and infinite capacity of an outsourced cloud based data storage system. The hybrid storage system is referred to as a cloud storage array . The cloud storage array couples to local server systems with a first interface using standard storage area network protocols to provide data storage for the local server systems. The cloud storage array is also coupled to a cloud storage provider through a second interface using an internet network connection in order to take advantage of the benefits of a reliable cloud based data storage service.

The use of a storage area network interface on the cloud storage array allows administrators to use the cloud storage array like a conventional storage area network storage device. Multiple server systems may share a cloud storage array using a standard storage area network.

The use of a second interface coupled to a cloud storage provider allows the cloud storage array to provide infinite storage resources that can be used as needed. Furthermore the use of an outsourced cloud storage system provides many of the features generally only available in high end data center such as cooling systems back up power systems off site secondary storage systems.

As set forth in an earlier section storage area networks allow network administrators to decouple the data storage function away from server systems such that only a single unified data storage system needs to be maintained. Thus all of the server systems and are coupled to a storage area network that is used to handle raw data storage reads and writes. A cloud storage array is coupled to the storage area network to handle data storage operations for the storage area network . Note that additional cloud storage arrays or conventional storage devices may also be coupled to the storage area network for additional storage. 

In the particular embodiment of the cloud storage array includes two different controller units controller A and controller B . These two different controller units may be used to provide a fault tolerant mirrored storage system wherein either controller can take over if the other unit fails. Alternatively the two controllers and may be used to statically load balance data volumes so that the controllers are each servicing half of the data storage requests while both controllers are healthy thereby increasing performance. When either controller fails in such a configuration the remaining functioning controller takes on double workload slowing down to some degree but providing continuous availability for all of the data volumes.

Controller unit A and controller unit B each have a local data storage and respectively . The local data storage and handle all data write operations from the server systems and . The local data storage and also handle data read operations for data portions that happen to be stored in the local data storage and . The cloud storage array attempts to keep all frequently accessed data within the local data storage and such that the vast majority of read operations can be handled locally. However only a subset of all the available data will reside in local data storage and due to size limitations. The remainder of the data will be stored with a cloud based data storage provider available on the internet . This allows the user of the cloud storage array to take advantage of an infinitely large storage system that is professionally maintained by experts the data stored in the cloud based data storage provider while having local storage performance for frequently accessed data the data stored in the local data storage and .

As illustrated in the cloud storage array acts as an intermediary between an on site storage area network and an off site cloud based data storage provider . As set forth in the previous section on cloud computing the cloud storage array must reconcile the significant differences between the front end interface to the local storage area network and the back end interface to the cloud based data storage system on the internet .

One the most significant differences between the two interfaces on the cloud storage array is the differential in speed between the connection to the storage area network and the connection to the cloud based data storage provider . For example the storage area network may operate at speeds of one gigabit per second and the internet connection to the data storage provider may operate at ten megabits per second.

To compensate for the speed differential the cloud storage array takes advantage of the manner in which data storage systems are generally used. Most data storage systems only need to handle a relatively small amount of dynamic information that changes frequently. For example an email server needs to receive and store new email messages every day and a file server needs to handle a limited number of files that are actively being used. However most of the information stored on a large data storage system is generally static and infrequently accessed. For example file servers may store archives of old documents and data related to projects that are no longer active. Thus since only a relatively small amount of data stored in a large data storage system is actively used that limited amount of active data can be stored in a large local data storage and that can be repeatedly accessed at a high data rate and with low latency. The data that is rarely accessed can be stored at the cloud based data storage provider and retrieved when necessary. Accessing data from the cloud based data storage provider will often result in increased latency however such latency may be acceptable in certain applications or use patterns.

A core concept of the cloud storage array is efficient use of the local data storage available in the cloud storage array . As long as the cloud storage array accurately determines the data that is most frequently accessed and keeps that data in the local data storage and the vast majority of storage requests both reads and writes received on the connection to the storage area network can be serviced using only the local data storage and . This will greatly reduce the amount of traffic on the connection to the cloud based data storage provider thus hiding the speed differential between the two interfaces from users of the cloud storage array . To most efficiently use the local storage available within the cloud storage array the cloud storage array uses both intelligent caching algorithms and storage space optimization techniques. The caching algorithms attempt to keep the most frequently accessed data in the local storage and use intelligent buffering systems like read ahead caching to prevent cache misses. The storage space optimization techniques make the most of the available memory by using techniques such as the identification and elimination of duplicated data.

In one embodiment the large local data storage systems and are implemented with Solid State Drive SSD systems. Solid state drive systems are generally implemented with a nonvolatile memory technology such as Flash memory. Using flash memory instead of hard disk drives that are used in most storage area network device provides several advantages. For example flash memory is faster uses less power generates less noise and is more reliable than hard disk drive storage. Thus as long as a desired data item is available in a flash memory based local data storage system or that data may be returned faster than from a traditional hard disk based storage device.

Although this document will concentrate on system that uses flash memory for the local data storage systems and other technologies may also be used such as hard disk drives battery backed RAM memory and any combination of these storage technologies. Various different storage technologies may be combined in a tiered manner. For example battery backed dynamic random access memory DRAM may be used for very frequently accessed data flash memory may be used for frequently accessed data hard disk drives may be used for less frequently accessed data and a cloud based storage system may be used for data that is rarely accessed. In this manner the strengths of each different storage system may be taken advantage of. Furthermore the administrator of the cloud storage array may be allowed to allocate and configure data storage in an application dependent manner. For example if a particular application uses a certain set of data infrequently but when that data is accessed a low latency response is needed then an administrator may be allowed to specify this limitation for that application or for that specific data such that the cloud storage array does not store that data with the cloud based storage provider . Other data sets may be explicitly marked as archive data such that such archive data is quickly sent off to the cloud based storage provider . This prevents such archive data from taking up memory space in the local storage system until the caching system determines the data is not being accessed.

To successfully build the cloud storage array of a novel storage system architecture was developed. The storage architecture was designed to handle the unique technical challenges that had to be overcome in order to provide the desired functions. In addition to many of the requirements for a typical high quality data storage system such as host multi pathing volume creation management back ups restore RAID configurations etc. the cloud storage array needs to handle the following issues 

The front end interface will generally use typical storage area network protocols. Examples include the industry standard Internet Small Computer System Interface iSCSI protocol and the Fiber Channel Protocol FCP . These protocols allow storage clients to perform operations such as start stop read write and format on storage units addressed by logical unit numbers LUNs .

The back end interface will generally use some type of REST ful protocol on an internet connection. The back end interface will generally allow the cloud storage array to issue commands such as create a data storage object update an object read an object deleting an object and list objects. The cloud storage array addresses the individual data objects using some type of unique object identifier that it must create for each data storage object.

In between the front end interface and the back end interface the cloud storage array contains a complex system for responding to data storage requests using a local data storage system when possible or responding to data storage requests with the help of the data storage provider when necessary. The system for handling data storage requests is implemented in a manner similar to a typical protocol stack made up of independent protocol layers. This document will refer to the layered system for handling data storage requests as the data storage request handling stack . The details of the data storage request handling stack will be disclosed layer by layer.

The top layers handle some formalities in processing storage requests. Beneath the formality layers are at least three different layers that actually handle the storage of data. A first data storage layer the linear storage layer is optimized for quickly handling raw data. A second data storage layer the deduplicated storage layer is optimized for organizing data that has been received and locally storing the data in a more space efficient manner. And a third data storage layer the cloud storage layer is optimized for limitless storage size by storing data off site at a data storage provider . All three of these data storage layers use the local data storage system to some degree. In one embodiment the linear storage layer stores uses approximately 35 to 45 the available storage space the deduplicated storage layer uses approximately 50 to 60 the available storage space and the cloud storage layer uses approximately 5 the available storage space of the local data storage system . These values may be configurable to optimize the system for a particular installation. In one embodiment the system uses a heuristics to dynamically adjust the storage allocations in order to automatically optimize the performance of the system.

Referring to at the top of the cloud storage array block diagram is an administration component . The administration component is not part of the storage request handling stack but is instead a system for configuring controlling and monitoring a cloud storage array . For example the administration component can be used to schedule periodic snapshots of the data in the cloud storage array . An administrator may access the administration component of the cloud storage array through an interface coupled to a local area network .

For the initial configuration an administrator specifies which virtual storage to expose to hosts on the storage area network SAN . This is similar to legacy systems where the administrator specifies which LUNs in a storage array to expose to hosts. The administrator also specifies the addresses and access information for the cloud storage provider that will be used. The administrator may specify a storage limit but this is generally not advisable since the system should be allowed to grow as needed. The administrator may also specify bandwidth constraints of the communication link to the data storage provider and bandwidth constraints of the data storage provider itself the maximum rate at which the data storage provider will handle read and write requests . The bandwidth constraints of the communication link can be used to ensure that the cloud storage array does not attempt to send data faster than the communication link can handle the data. Furthermore if the communication link is shared by other users such as an internet connection shared with human users mail servers and other internet link users the cloud storage array can be configured to use less than the full bandwidth available on the communication link.

The administration component will collect operation statistics that may be used to gauge the performance of the cloud storage array . The operation statistics may be analyzed and used to alter the configuration of the cloud storage array for improved performance. Each layer in storage request handling stack may generate its own individual statistics. The administration component may periodically poll the different storage request handling layers and various other parts of the cloud storage array to create a centralized collection of all the system statistics.

The core of the cloud storage array is made up of the storage request handling stack. The storage request handling stack starts at the top with storage area network interface and travels down through a volume abstraction layer a snapshot layer a linear storage layer a deduplicated storage layer and finally to a cloud storage layer . Each of these layers in the storage request handling stack will be described in further detail individually.

At the top of the storage request handling stack is the storage area network interface . In one particular implementation that will be considered in detail the storage area network interface implements the well known iSCSI protocol that is used to accept SCSI commands carried on a TCP IP network. However any other storage protocol may be implemented at the top of the storage request handling stack.

The storage area network interface exposes iSCSI volumes to hosts on the storage area network SAN . The storage area network interface then receives iSCSI data storage requests from the hosts such that the cloud storage array must respond to those requests. The storage area network interface parses these iSCSI commands and determines how the commands should be handled. Many of the iSCSI requests that are not directly related to reading and writing can be handled by the storage area network interface layer . Storage requests that cannot be handled by the storage area network interface layer are passed down the storage request handling stack to the next layer.

Beneath the storage area network interface layer is a volume abstraction layer . The volume abstraction layer handles many of the formalities in keeping track of the different volumes stored by the cloud storage array . For example the volume abstraction layer keeps track of the volumes that exist the size of each volume access control lists ACLs and other administrative information. Thus the volume abstraction layer handles some of the volume management tasks such that the lower layers can concentrate on actual data storage.

The layer beneath the volume abstraction layer is a snapshot layer . The snapshot layer is used for taking snapshots of specified volumes in the cloud storage array . In the present disclosure a snapshot is the state of a volume at a particular moment in time. However it is impractical if not impossible to actually make an instant copy of all the data in a particular volume. Instead the snapshot layer creates a new volume that initially only consists of a time map for the snapshot volume that specifies when the snapshot was taken and a pointer to the parent volume. If there are no new writes to the parent volume then the current data of that parent volume can be used as the data for the snapshot volume. However when a new write is received that changes data in the parent volume that is referenced by the snapshot volume the old existing data must be copied out of the parent volume and placed in the snapshot volume before the write occurs in order to save the data that existed when the snapshot was created. The copying of the old data when a new write operation is received is known as a copy on write operation that is used to build the snapshot volume from the parent volume. In addition the snapshot layer may use also use a system known as Copy Reference On Write CROW that makes a copy of a reference to another volume. This reduces the amount of storage required when multiple dependent snapshots exist and one snapshot may refer to data in another snapshot as long as the two snapshots are aligned in time.

To take a snapshot of a data volume the snapshot layer first freezes access to a particular volume. Then the snapshot layer creates an age volume map for the volume. The age volume map consists of age entries for all the data for the volume including data within the local storage and data stored in the cloud storage . Then the snapshot layer unfreezes the data volume such that the data volume may continue to be used. However the snapshot layer will now examine every read and write to the volume in order to protect the data associated with the snapshot volume.

When a read is received for data volume that has had a snapshot taken the read is processed as normal. When a write is received for data volume that has had a snapshot taken the system determines if this is the first new write to a particular area of data since the snapshot was taken. If so the existing data is copied and placed into a snapshot storage area to preserve the data.

Snapshot volumes themselves may also be read from and written to. When a read is received for a snapshot volume the snapshot layer will first determine if the data is within the snapshot storage area. If the data is within the snapshot storage area the snapshot layer will use that data which was preserved due to an incoming write that would have destroyed the old data to service the read request. If the requested data is not within the snapshot storage area the snapshot layer will then fetch the requested data from the parent volume which has not yet changed or else the data would have been stored in the snapshot storage area .

When a write is received for a snapshot volume the snapshot layer will first determine if it has the current data for the volume in the snapshot storage area. If it does not yet have the data within the snapshot storage area the snapshot layer will first fetch the data from the parent volume which has not yet changed yet and place that data in the storage area. Then snapshot layer will then overwrite the old data in the storage area with the new data being written to the snapshot volume.

In one embodiment the copy on write policy of the snapshot layer may be configured to make data copies propagate to the data storage provider . In this manner when a snapshot is taken the data contents of the snapshot will be preserved in the data storage provider . This allows back up snapshots to be stored at the data storage provider such that no local back up media system is required. However since the bandwidth on the connection to the data storage provider is generally limited this is a very slow procedure. Furthermore this may consume needed bandwidth on the connection for normal operations. Thus such a system may be instructed to only send data when bandwidth is available or only during non peak times such as overnight .

After performing any needed snapshot operations in the snapshot layer a data storage request is then passed to the linear storage layer . The linear storage layer is the first level of actual data storage in the cloud storage array . The linear storage layer is designed to handle hot data that is frequently accessed and changed. To provide fast performance to the hosts for this hot data the linear storage layer is optimized for speed.

The linear storage layer will generally receive data storage requests addressed in traditional data storage terms such as logical volumes and logical block address LBA ranges. As set forth earlier the front end of the cloud storage array can implement many different possible data storage protocols that use different data storage addressing systems. However as long as the cloud storage array properly responds to data storage requests received the cloud storage array is free to use any different type of addressing system internally. In the cloud storage array data will often be stored in nonvolatile memory or with a cloud based data storage provider instead of on a disk system like a traditional storage system. Thus an addressing system more suited to storing information in a memory system will be used within the cloud storage array instead of the more disk centric addressing system used by the storage area network interface .

In one embodiment the cloud storage array uses a flat linear addressing system for each volume wherein each logical volume is divided into fixed sized blocks. A very simple translation system can be used to translate data requests made in terms of logical block address LBA ranges on a disk or any other data storage addressing system into the linear memory addressing system used within the cloud storage array . In one specific embodiment each fixed size block is one megabyte long and each block may be handled either by the linear storage layer or a lower data storage layer. Data stored by the linear storage layer is always stored in the local data storage .

To keep track of where the all the data is stored the linear storage layer maintains a linear storage map . The linear storage map specifies where data resides and thus how the data may be obtained . For data blocks that are stored by the linear storage layer the linear storage map may specify a specific physical memory address in the local data storage . For data that is not stored by the linear storage layer the linear storage map may specify a set of data fingerprints used to uniquely identify data slices in lower levels of the storage request handling stack such as the deduplicated storage layer and the cloud storage layer . In one embodiment a thirty two byte long SHA 256 fingerprint is used to uniquely identify data slices stored in the lower storage layers.

The linear storage map may be implemented with an ordered linked list that links together entries that each contain a pointer to a block of data in the linear storage area or a fingerprint of data stored in a lower layer. For the data stored in lower layers the linked list will contain a series of entries with fingerprints where the total size of the data slices referred to by the fingerprints equals one block size. To provide optimal performance the linked list may also have an additional data structure used to improve the search of the linked list. For example a red black tree a hash table or another similar data structure whose elements are pointers to the linked list nodes may be used to improve the speed of searching the linked list.

A description of how the linear storage layer handles read requests will be disclosed with reference to a flow chart in and the conceptual diagram of . Referring to a read request is received from a host client at the top of the flow chart. The read request is first processed by the SAN interface volume abstraction and snap shot layers at stage . The read request is then passed to the linear storage layer .

The linear storage layer first examines the linear storage map at stages and to determine how to respond to the read request. If the requested data is available in the linear storage area of the local data storage system then handling the response can be done very quickly and easily. Specifically the linear storage layer simply reads the data from the linear storage area and responds to the read request at stage . The system may then update some statistics such as statistics used to determine if the data is hot warm or cold at stage and it is done handling the read request.

If the data was not found to be in the linear storage area at stage then the linear storage layer requests the needed data from lower layers of the storage request handling stack at stage . The request is made by providing the fingerprints of the needed data items. Note that a request may only need a few slices of data if the storage read request only requested a small amount of data within a particular block of data. In this particular embodiment the next lower layer is the deduplicated storage layer in . This document may use the term dedup when referring to aspects the deduplicated layer.

At stage the deduplicated storage layer examines the deduplicated storage map to determine if it has all the requested data slices in the deduplicated storage area of the local data storage system . If the deduplicated storage layer does have all the needed data slices then the deduplicated storage layer can respond with the requested data at stage . If the deduplicated storage layer does not have all the needed data slices then at stage the deduplicated storage layer will request the needed data items from the next lower layer of the storage request handling stack the cloud storage layer in this embodiment.

At stage the cloud storage layer fetches the requested data from the cloud storage provider . More details on this stage will be presented in the section on the cloud storage layer . Upon receiving the requested data the deduplicated storage layer will place the requested data into the deduplicated storage area of the local data storage system . The deduplicated storage layer can then respond to the linear storage layer with the requested data at stage .

Upon receiving the requested data slices from deduplicated storage layer the linear storage layer will assemble the requested data from the received data slices at stage . Finally the linear storage layer can then respond to the read request with the requested data at stage . The statistics counters can then be updated at stage .

It can be seen that servicing the read request at stage will be faster than servicing the read request when the data must be fetched from the lower data storage layers. This is especially true if data slices must be fetched from the cloud based data storage provider .

Write requests are handled in a similar manner. All write operations to the cloud storage array are initially written into the linear storage are associated with the linear storage layer . The handling of a write request will be disclosed with reference to the flow chart of and the conceptual diagram of . The example of describes a write to a single data block. However the same steps may be performed multiple times to handle writes to multiple data blocks.

Referring to a write request is received from a host client at the top of the flow chart. As with a read request the write request is first processed by the SAN interface volume abstraction and snap shot layers at stage . The write request is then passed to the linear storage layer .

The linear storage layer first examines the linear storage map at stages and to determine how to handle to the write request. If the write is directed at a data block that is already available in the linear storage area of the local data storage system then handling the write request can be done easily. Specifically the linear storage layer simply writes the data into the appropriate block within the linear storage area at stage . The system may then also update some statistics at stage . At this point the write request has been fully handled.

If the data block that the write is directed at was not found to be in the linear storage area at stage then the linear storage layer will generally first pull the data for target data block into the linear storage layer . The reason that data is pulled up into the linear storage layer before it is overwritten is so that if a failure occurs during a write the failure will at least leave the old data which has been partially over written by new data. This is the way that a traditional disk based storage system operates such that applications are already prepared to handle corrupted data due to such a write failure.

To pull the data up into the linear storage area the linear storage layer may first need to allocate a new block of memory in the linear storage area at stage . This may be performed by pushing data from an existing block in the linear storage area down to the next lower data storage layer. Pushing data down to the next data storage layer will be described in the next section of this document about the deduplicated storage layer .

The linear storage layer then requests all the data slices for that data block from the lower data storage layers of the storage request handling stack at stage . The request for the slices is made by providing the fingerprints of the needed data slices. Note that all of the slices of data for the data block are required since the entire data block will now be represented in the linear storage area as a single data block. If the deduplicated storage layer does not have all the needed data slices for the block in the deduplicated storage area then the deduplicated storage layer will request the needed data slices from the next lower layer of the storage request handling stack the cloud storage layer in this particular embodiment .

After receiving the requested data slices the linear storage layer then assembles the data slices in a buffer at stage . The fully assembled data block is then copied into the free memory block in linear storage area such that the linear storage layer is now fully responsible for that particular data block. Thus the linear storage layer updates the linear storage map to reflect that the linear storage layer now has that particular block of memory represented within the linear storage area .

It should be noted that the fetched data slices will generally be allowed to remain down in the deduplicated storage area . A primary reason that these data slices will continue to be in the deduplicated storage area is that other areas of the data volume or other data volumes may refer to the fetched data slices. If a data slice is not referenced by another data block then a garbage collection mechanism will eventually discard that unreferenced data slice. However even unreferenced data slices may be allowed to remain in the deduplicated storage area for some time. The reason is that there is a benefit in keeping unused data slices for a period of time since a data block that was pulled up from the deduplicated storage layer up into the linear storage layer may eventually be pushed back down to the deduplicated storage layer . When this occurs the pre existing data slice in the deduplicated storage area may be used again if it is still appropriate.

After the data block has been fully moved back up to the linear storage area the linear storage layer may then over write the data block at stage . In the unlikely event of a failure during the write the data block will contain a mix of new data overwritten onto old data. As set forth above this is a situation that existing applications are already prepared to handle. Finally at stage the system may update some statistics. For example a counter associated with the data block may be incremented to indicate that the data block has recently been accessed.

As set forth earlier the cloud storage array acts roughly similar to a cache memory system where in the local storage system acts as a cache memory and the data storage provider acts as a main memory system. As such the linear storage layer can be configured to act as a write through type of cache system or a write back type of cache system.

If the system is configured as a write through type of cache the linear storage layer may push every new write to lower layers of the storage request handling stack such that data will eventually be stored in the data storage provider . Note that various writes may be superseded by newer writes to the same location such that not all writes will actually propagate all the way through to the data storage provider . Configuring the system as a write through cache will reduce the performance of the cloud storage array but may be desirable for applications wherein reliable data storage is paramount since the data will almost always be propagated out the cloud based data storage provider for safe keeping.

When the system is configured as a write back type of cache as it will be in most cases the linear storage layer will only push data blocks down to lower layers of the storage request handling stack when the linear storage layer determines that a particular data location needs to be evicted from the linear storage area . For example a data block may be evicted to make room for new data in the linear storage layer . The eviction policy may use any cache replacement strategy such as the well known least recently used LRU least recently allocated LRA or least frequently used LFU cache replacement policies. With a write back cache replacement policy the full contents of the cloud storage array generally not fully propagate to the data storage provider . However there is nothing wrong with this since all of the data stored within cloud storage array is stored in some nonvolatile form such as a solid state drive or a hard disk drive . Snapshots of data in the cloud storage array can be backed up locally as will be described in a later section of this document or proactively pushed to the cloud for disaster recovery purposes.

Referring back to when the linear storage layer determines that a particular data block is not being frequently accessed the linear storage layer sends that data block down to the deduplicated storage layer for memory efficient storage in the deduplicated storage area . The deduplicated storage layer acts as a repository for warm data. Warm data is defined as data that is not as frequently accessed as the hot data in the linear storage layer but still accessed regularly and typically read more often than written. As the name implies the deduplicated storage layer removes duplicates from the data such that data is stored much more efficiently.

In the deduplicated storage layer the fingerprint such as a SHA 256 fingerprint of a data slice is used as an identifier for the data slice. The deduplicated storage layer uses deduplicated storage map to keep track of where each data slice is stored within the deduplicated storage area . illustrates a conceptual diagram of the deduplicated storage map and the deduplicated storage area .

As illustrated in the deduplicated storage map is a table that identifies the location of each data slice received from the linear storage layer . In practice the deduplicated storage map may be implemented as a hash table or similar data structure to optimize search performance. Referring back to the conceptual diagram of the deduplicated storage map table maps data slice fingerprints to data locations the cloud storage or both.

After selecting a data block to push down the linear storage layer then divides the data block into a set of individual data slices at stage . Many different techniques may be used to slice a data block into a set of data slices. The goal is to slice the data block up into individual data slices in a manner that will result in a high probability of finding duplicated data slices.

In one embodiment the data block is sliced up using Rabin fingerprints. A Rabin fingerprint is a progressive polynomial that is calculated over a defined field. It is progressive since successive Rabin fingerprints may be calculated by dropping of a byte from one end of the defined field and adding another byte to the other end. This allows a Rabin fingerprint to sweep through a data block. illustrates how a Rabin fingerprint calculator window may sweep through data block progressively calculating Rabin fingerprints. The Rabin fingerprint system may be used to sweep through the data block and periodically drop anchors to define data slices. An anchor may be dropped when the Rabin fingerprint equals some arbitrary value. In one embodiment the system creates data slices that start at a first anchor defined by the beginning of the data block or the previous anchor are at least 8K bytes long and end when a specified Rabin fingerprint is generated or a 64K limit is reached which ever occurs first . This will create data slices that are between 8K and 64K in length. If the arbitrary Rabin fingerprint value is selected as a value with 16 zeroes in the least significant bits of the binary Rabin fingerprint then the data slices will average out to be around 16K in size.

Referring back to at stage the system then may need to allocate space in the deduplicated storage area if no space is available. This may be done by selecting a least recently allocated block of space in the deduplicated storage area and pushing the data slices in that area down into the next lower layer the cloud storage layer in this embodiment . Note that like the linear storage layer the deduplicated storage layer may also have a background process running that always attempts to keep the deduplicated storage area approximately 85 filled such that it stores a large amount of data but can still always accept new data.

The linear storage layer then begins to push down individual data slices. At stage the linear storage layer first calculates a fingerprint for a data slice. The linear storage layer then provides the data slice and the fingerprint for the data slice to the deduplicated storage layer at stage .

Next at stage the deduplicated storage layer examines the fingerprint that it receives and searches the deduplicated storage map for redundant data. With sufficiently strong fingerprints that have a very low probability of aliasing simply comparing the fingerprints may be enough to identify duplicate data. In an alternative system the deduplication may be performed in two stages. A first stage can use probabilistic methods to locate potential duplication candidates. After identifying candidates for deduplication exhaustive algorithms verify the duplicated data and possibly adjust the data slice boundaries to obtain more duplicated data slices.

If the deduplicated storage layer identifies redundant data the deduplicated storage layer may discard the data and increase a reference counter for that data at stage . A reference counter may be used to keep track of how many different data blocks refer to a particular data slice. When a received data slice is not yet represented in the deduplicated storage layer the same fingerprint was not found in the deduplicated storage map then at stage the deduplicated storage layer stores the data slice in the deduplicated storage area and creates a new entry in the deduplicated storage map which may be a hash table that points to the newly added data slice.

At stage the linear storage layer determines if this was the last data slice of the data block to push down. If it is not the linear storage layer returns back to stage to push down another data slice. If this was the final data slice then the linear storage layer may now update the linear storage map by removing the reference to the data block and adding entries that refer to all the fingerprints of the data slices in the linear storage map . Thus when a subsequent memory request is received that refers to data in the range of that particular memory block the system will need to access the data slices now stored in the deduplicated storage area .

By removing duplicated data the deduplicated storage layer greatly increases the storage efficiency. This allows many more logical volumes of data to be stored in the local storage system than if the data were only stored in a raw unprocessed form as done by the linear storage layer . However this increased storage efficiency comes at a cost. The linear storage layer must slice up each data block and calculate fingerprints for each data slice. And the deduplicated storage layer must search for duplicated data. Furthermore pushing data into the deduplicated storage layer involves significant metadata updates to maintain the deduplicated storage map . However since processing power is now very inexpensive and the bandwidth of the intermediate layers is far greater than the cloud bandwidth this is a worthy trade off

Another cost for the improved memory efficiency is that when a read request is received that read must be satisfied with data from the deduplicated storage area . Thus the linear storage layer must fetch each needed data slice from the deduplicated storage layer and then reassemble the data slices to obtain the requested data. This means that the latency time for read requests that are serviced by the deduplicated storage layer will be higher than the latency time for read requests that are serviced by the linear storage layer . However this latency difference is relatively small and worth the trade off since it allows more data to be stored within the local data storage . Storing more data in the local data storage will mean fewer accesses to the cloud data storage provider which will have a much greater latency time.

Referring back to the deduplicated storage layer acts as a local tier of data storage. The data in the deduplicated storage layer is not accessed as frequently as the data in the linear storage layer but data in the deduplicated storage layer is still accessed on a regular basis. Although the deduplicated storage layer stores data more efficiently the deduplicated storage layer will eventually run out of storage space. When the deduplicated storage layer runs out of storage space the deduplicated storage layer must begin to evict data slices. The deduplicated storage layer will push the evicted data slices further down the storage request handling stack to the cloud storage layer in this embodiment . Note that cache replacement policies used by the deduplicated storage layer may be the same or different than the cache replacement policies used by the linear storage layer .

In addition to pushing data down to the cloud storage layer in order to have available space in the deduplicated storage layer the deduplicated storage layer may proactively push data slices out to the cloud before it is necessary. However the data will also remain within the deduplicated storage layer such that read requests may be serviced quickly. However when data slices need to be evicted data slices that have already been evicted can simply be removed from the deduplicated storage area . This allows the bandwidth of the communication link to the data storage provider to be used more efficiently.

The data evicted by the deduplicated storage layer need to be handled the cloud storage layer . And as already disclosed data storage requests that can not fully be serviced by the above two layers need the help of the cloud storage layer . The cloud storage layer does not store data locally except for the temporary barrier buffer . Instead the cloud storage layer stores data out at the data storage provider . The cloud storage layer is used to store cold data that is rarely accessed. Since it takes time to retrieve data from the off site data storage provider there will generally be a larger latency period for any data storage request that requires access to the off site data storage provider . Ideally such latency should only occur when accessing old data archives since the vast majority of the frequently used data should be represented in the local storage system of the cloud storage array .

When the cloud storage layer receives evicted data data slices in this particular embodiment the cloud storage layer first prepares that data to be sent to the data storage provider . The cloud storage layer first compresses the data using compression stage . The compression of the data accomplishes two goals. First the compression reduces the bandwidth requirements for the internet connection to the data storage provider since less data needs to be transmitted. This is very important since this reduces the large disparity between the bandwidth at the front end storage area network connection and this back end internet connection to the data storage provider . Second the compression also reduces the amount of data that needs to be stored by the data storage provider such that outsourced data storage costs are minimized.

Any suitable compression system may be used but in one embodiment the BZIP compression system is used. In another embodiment the compression system allows for multiple different compression systems to be used. To do this the compression stage may prepend compression information to the compressed data as illustrated in . The compression information may include a code to that specifies a particular compression algorithm and version. This allows the compression stage to select the proper decompression system when multiple different compression systems are used. Such a system may be able to select the optimum compression system for a particular data slice.

After compressing the data the compressed data is then encrypted with encryption stage . By encrypting the data the owner of the cloud storage array does not need to fear for their data security. The encryption prevents any person tapping the internet connection or examining the data at the storage provider from being able to view the real meaning of the data.

Many different encryption systems may be used. In one particular embodiment the AES 256 encryption system was implemented within the encryption stage . As with the compression stage the encryption stage may allow multiple different encryption systems to be used. To do this the encryption stage may prepend encryption information to the encrypted data as illustrated in . The encryption information allows the encryption stage to select the proper decryption system and version when multiple different decryption systems are used. The prepended encryption information may also specify the size of the data since some encryption systems only operate on fixed size data and thus require padding bytes.

The use of prepended encryption information may also be used to help with key management. Encryption keys may be changed on a regular basis to improve the data security. A code may be placed into the prepended encryption information to help select the proper key for data decryption. In one embodiment the system allows an administrator to use a passphrase to generate an encryption key. Multiple levels of authority may be used to protect keys from be lost. In addition a built in system may allow a customer to contact the manufacturer of the system if the passphrase has been lost.

Next a barrier stage stores a copy of the compressed and encrypted data in a barrier storage area in of the local storage system . The barrier storage area is used to temporarily store a copy of data that is in the process being transmitted to the data storage provider . The data is kept in the barrier storage area for a settlement period that allows the data storage provider to perform its own data storage tasks. If data sent to the data storage provider were requested too soon the data storage provider may fail at providing the data since it would not be ready to respond yet. Thus when the cloud storage layer receives a read request for data the cloud storage layer first checks the barrier storage area of the local storage system and serves the data from the barrier storage area if the data is found there.

Note that by compressing the data before the data is stored in the barrier storage area the cloud storage layer efficiently uses its allocated area of the local storage system . However this comes at the expense of having to decrypt and decompress the data if a read request is received for data stored in the barrier storage area. In an alternate embodiment the data is stored in the barrier layer before compression and decryption. In such an embodiment there will be a lower latency period when responding from the data stored in the barrier storage area.

In addition to allowing transmitted data to settle at the data storage provider the barrier stage serves additional purposes. One important purpose is to handle storage request serialization. Many cloud data storage provider will perform data storage requests received in close time proximity out of the order that they were received in. Thus if a purge request is transmitted and then a write request to the same data were subsequent transmitted the cloud data storage provider might reverse the order of these requests and thus destroy data To prevent this potential disastrous occurrence the barrier stage will place a long waiting period between data storage requests that refer to the same data.

After storing a copy in the barrier storage area the compressed encrypted data is then provided to a cloud transport interface stage that is responsible for transmitting data to the data storage provider . The cloud transport interface stage first creates a new data object within the cloud data storage provider to store the data. In one embodiment the system uses the same fingerprint identifier from the deduplicated storage layer as the name for the data object. The cloud transport interface stage then writes transmits the data to the newly created object. The cloud transport interface stage then allows for a settlement period wherein it waits a specified amount of time before the data can be read back from the data storage provider . This settlement period is a value that may be configured based upon the particular data storage provider that is being used. Once the settlement period expires the cloud transport interface stage deletes the copy of the data that was placed in the barrier storage area . Thus subsequent read operations must be serviced by requesting the data from the data storage provider .

To ensure that the data was properly stored with the data storage provider the cloud transport interface stage may calculate a checksum value of data using the same type of checksum used by the data storage provider . After receiving data the data storage provider may provide a checksum value back in an acknowledgement. If the two checksum values do not match the cloud transport interface stage may retransmit the data. If checksums are used the copy of the data in the barrier section should not be removed until matching checksums have been achieved and the settlement period has expired.

Data read requests are handled by the cloud storage layer in basically the same manner but in reverse order. As set forth above the cloud storage layer will first attempt to serve a data request using data stored in the barrier storage area . If the request cannot be served from data in the barrier storage area the deduplicated storage layer will then send a read request to the cloud data storage provider using the fingerprint as the name of the requested data object.

After receiving a response from the cloud data storage provider the cloud transport interface stage can perform data integrity check on the received data by calculating a checksum the received data. If the calculated checksum does not match the checksum received then the cloud data storage provider may have corrupted the data. Retries may be attempted to obtain the proper data from the cloud data storage provider . If the proper data cannot be retrieved a media error message will be propagated up the data storage request handling stack.

When verified data has been received that verified data is then provided to the encryption stage for decryption. Next the decrypted data is given to the compression stage where the data is decompressed.

After requested data has been retrieved decrypted and decompressed the cloud storage layer passes the data back up the request handling stack. In one embodiment the system performs a second data integrity check by recomputing the fingerprint of the decrypted decompressed data.

The deduplicated storage layer will receive the data that was fetched from the cloud and place that data back into its duplicated storage area and adjust the its data structures to indicate that the data is now available in the duplicated storage area of the local storage system . The data will remain in the duplicated storage area until the deduplicated storage layer again evicts the data or it is removed by the garbage collection system since it was over written.

Similarly data that is read back from the cloud data storage provider will continue to remain in the cloud data storage provider . If the deduplicated storage layer again evicts data that was already once stored in the cloud data storage provider and that data has not changed then that data does not need to be retransmitted back to the cloud data storage provider since it already exists there. Thus the deduplicated storage layer can just delete its copy of the data.

Migrating from an existing storage area network system to the disclosed cloud storage array must be performed carefully in order to not lose any data. An ideal migration would allow the data center to continue functioning normally without any noticeable service interruption.

To provide a seamless migration this document presents a method for inserting a cloud storage array into an existing data center with virtually no down time. The method operates by inserting a migration layer into the storage request handling stack to handle data migrations.

Once configured for migration the cloud storage array system can then receive data storage requests from hosts on the storage area network . The SAN interface layer may process data storage requests in a normal manner. The data request is then provided to a migration layer . The migration layer may call the linear storage layer to determine if the linear storage layer has the requested data or knows where the data is. When a new system is installed and configured for migration the linear storage layer will have very little or no data. Thus when the linear storage layer informs the migration layer that it does not have requested data the migration layer will request the needed data from then the migration layer requests the needed data from the legacy storage system as any other host on the storage area network would access the legacy storage system .

The migration layer will pass the requested data back to the host that requested the data. In addition the migration layer may provide the data to the linear storage layer . The linear storage layer adds the data to linear storage area of the local storage system . In accordance to the cache policies the linear storage layer will eventually evict data down to the lower storage layers starting with the deduplicated layer . Similarly the deduplicated layer will eventually evict data down to the cloud storage layer which will store data with the cloud based data storage provider . Over time this process will move the data from the legacy storage system over to the new cloud storage array system .

In addition to the passive transition system outlined above an active transition process may also operate concurrently within the migration layer . The active transition process will sweep through the existing legacy storage system and copy data into the new cloud storage array . Eventually all of the data will be handled by the cloud storage array . Note that during this processes the data retrieved from the legacy storage system will enter in the linear storage layer and eventually move to the other layers. This is similar to the manner in which data that is received from client hosts enters the cloud storage array except that data is entering into the linear storage layer from the migration layer instead from the storage network interface . The migration system disclosed in allows for a seamless insertion of the cloud storage array into an existing data center with no data downtime beyond the insertion event itself

If a particular data center wishes to transitions away from a cloud storage array a similarly elegant system can be used to perform the transition. In such a transition away from the cloud storage array the cloud storage array may be configured to operate in a read only manner. The migration layer will process data write requests by sending the writes to the replacement storage system.

Reads will be handled in a similar fashion. Specifically when a read request is received the migration layer will determine if the replacement system has valid data for the read request. This may be performed by requesting the data from the replacement system to see if it provides valid data or requesting the linear storage layer if it has valid data. If the replacement system has valid data that data will be used. If the linear storage layer has valid data that data will be served to the requesting host and provided to the replacement storage system. That data that was just served will then be marked as invalid in the cloud storage array such that that particular data in the cloud storage array will no longer be used.

Eventually this passive system will move data to the replacement system. In addition to the passive system an active process may sweep though the cloud storage array system to copy over any remaining data.

The primary reason for providing a system for transitioning away is when the host workload changes to be incompatible with the cost performance model of the cloud storage array system. A secondary reason for providing a system for transitioning away is to provide an assurance to customers that they can get their data back if the product line were to be eliminated.

As set forth in an earlier section and with reference to the cloud storage array includes a snapshot layer for capturing snapshots of the current state of data volumes. The snapshot system creates a data structure with the state of a data volume frozen in time. When subsequent writes are received that would destroy the state of the data in the frozen volume copies of the data that is about to change are made. In this manner the data from a volume at a specific instant of time may be saved. In order to construct back ups of data volumes in the cloud storage array the data from the snapshot layer can be provided to direct a back up system.

As set forth in the earlier sections the cloud storage array will store data in at least three different locations the linear storage area the deduplicated storage area and the cloud storage. The data in all of these locations must be backed up for a full back up. Since this type of storage system differs significantly from conventional storage systems wherein all the data is available locally a different method for performing back ups was required.

To handle the unorthodox back up procedure the back up system for the cloud storage array uses the Network Data Management Protocol NDMP system. NDMP allows an agent to decide how to back up a particular system. The agent is allowed to define its own data organization and format for a back up system. Thus the back up system for the cloud storage array uses its agent that uses its own data organization which is significantly different from conventional systems.

In the back up system for the cloud storage array the back up agent consults a volume snapshot and the linear storage map to determine where all of the data is located. The data will be spread out in the snapshot storage area the linear storage area the deduplicated storage area the barrier storage area and the cloud storage service . The back up agent will use the volume snapshot and the linear storage map as a guide to locate all of the data in the cloud storage array.

The back up agent will first back up the data blocks in the snapshot storage area and the snapshot storage area and the linear storage area . This creates a back up of the data copied due to the copy on write policy in the snapshot layer and the active data in the linear storage area.

Next the back up agent will back up the data slices in the deduplicated storage area and the barrier storage area . Each of these data items is backed up with its fingerprints and any other necessary data such as the length of the data slice . At this point all of the data for the volume within the local storage system has been backed up.

Finally the data storage provider is instructed to make a duplication of all the data slices for the volume that are currently stored with the data storage provider . This duplication request can be made using the mapping table information retrieved from the deduplicated storage layer that specifies what data is stored in the data storage provider . The data storage provider is responsible for making an independent copy of the data slices as requested. The fingerprints for all the data slices of the volume that are currently stored with the data storage provider are then written to the media device.

At this point an entire data volume has been independently backed up. The local media device such as a back up tape may now be completed and placed into storage. Part of the back up data is stored on the media device and part of the back up resides within the data storage provider .

To restore the back up the cloud storage array is instructed to first prepare for a restoration. The cloud storage array then reads the back up media and first restores the data that was saved from the snapshot storage area and the linear storage area . This data will initially be placed into the linear storage area . However the data will be pushed down the data storage layers as needed. Next the cloud storage array reads all the data slices that were stored on the back up media. These data slices will be stored into the deduplicated storage layer . However the data slices will be pushed down the storage handling stack as needed. Finally the back up agent will request the data storage provider to restore the data slices that earlier asked to back up. At this point the volume has been complete restored and may be exposed to hosts.

The preceding technical disclosure is intended to be illustrative and not restrictive. For example the above described embodiments or one or more aspects thereof may be used in combination with each other. Other embodiments will be apparent to those of skill in the art upon reviewing the above description. The scope of the claims should therefore be determined with reference to the appended claims along with the full scope of equivalents to which such claims are entitled. In the appended claims the terms including and in which are used as the plain English equivalents of the respective terms comprising and wherein. Also in the following claims the terms including and comprising are open ended that is a system device article or process that includes elements in addition to those listed after such a term in a claim is still deemed to fall within the scope of that claim. Moreover in the following claims the terms first second and third etc. are used merely as labels and are not intended to impose numerical requirements on their objects.

The Abstract is provided to comply with 37 C.F.R. 1.72 b which requires that it allow the reader to quickly ascertain the nature of the technical disclosure. The abstract is submitted with the understanding that it will not be used to interpret or limit the scope or meaning of the claims. Also in the above Detailed Description various features may be grouped together to streamline the disclosure. This should not be interpreted as intending that an unclaimed disclosed feature is essential to any claim. Rather inventive subject matter may lie in less than all features of a particular disclosed embodiment. Thus the following claims are hereby incorporated into the Detailed Description with each claim standing on its own as a separate embodiment.

