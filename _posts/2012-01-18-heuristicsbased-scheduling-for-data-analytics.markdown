---

title: Heuristics-based scheduling for data analytics
abstract: A scheduler may receive a plurality of jobs for scheduling of execution thereof on a plurality of computing nodes. An evaluation module may provide a common interface for each of a plurality of scheduling algorithms. An algorithm selector may utilize the evaluation module in conjunction with benchmark data for a plurality of jobs of varying types to associate one of the plurality of scheduling algorithms with each job type. A job comparator may compare a current job for scheduling against the benchmark data to determine a current job type of the current job. The evaluation module may further schedule the current job for execution on the plurality of computing nodes, based on the current job type and the associated scheduling algorithm.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09183058&OS=09183058&RS=09183058
owner: SAP SE
number: 09183058
owner_city: Walldorf
owner_country: DE
publication_date: 20120118
---
This application claims priority under 35 U.S.C. 119 to Chinese Patent Application No. 201110347633.6 filed on Nov. 7 2011 entitled HEURISTICS BASED SCHEDULING FOR DATA ANALYTICS which is incorporated by reference herein in its entirety.

It is often difficult for users of computing resources to utilize such computing resources in an efficient cost effective manner. For example such users may have computing needs which vary greatly in time and may not wish to invest in a quantity of computing resources necessary to manage computational loads during periods of high usage. For example a business with a large number of employees may wish to process payroll for all employees only once e.g. at the end of each month. In such scenarios the business may require computing resources at the end of each month in a quantity that is disproportionate to computing resources needed during the remainder of the month. Consequently it may be inefficient for the business to invest in sufficient computing resources to handle the payroll processing of the above example since a significant portion of such computing resources would sit idle during the remainder of the month. In these and other scenarios users of computing resources may be unwilling or unable to bear the costs associated with purchasing and managing a level of computing resources necessary to meet all of the user s computing needs.

Therefore providers of software infrastructure and other computing resources have implemented a business model in which computing resources are provided to users on an as needed on demand dynamic basis according to the particular and fluctuating demands of the customers of such providers. Such providers are known as or may be referred to as software as a service Saas providers and or infrastructure as a service Iaas providers. Such providers typically invest in the relatively large amount of servers and related computer network equipment needed to remotely provide computing resources to their customers. For example in the example described above the business with the described payroll processing needs may have an agreement in place with a Saas provider to utilize a relatively low amount of computing resources through much of the month and to have access to a relatively larger amount of computing resources at the end of each month when payroll processing occurs.

Thus such service providers may provide software infrastructure and other computing resources to a large number of businesses or other customers and may therefore bear the burden of utilizing available computing resources in an efficient cost effective manner. In order to do so such service providers typically receive requests for computing resources and dynamically schedule the received requests among available servers or other computing nodes. In many cases such service providers may deploy a very large quantity of computing resources and may have service agreements in place with a relatively large number of service consumers each of which may require varying and potentially large amounts of the available computing resources. As a result it may be difficult for service providers to allocate the available computing resources in a desired efficient and cost effective manner.

According to one general aspect a computer system may include instructions recorded on a computer readable medium and executable by at least one processor. The system may include a scheduler configured to cause the processor to receive a plurality of jobs for scheduling of execution thereof on a plurality of computing nodes and an evaluation module configured to cause the processor to provide a common interface for each of a plurality of scheduling algorithms. The system may further include an algorithm selector configured to cause the processor to utilize the evaluation module in conjunction with benchmark data for a plurality of jobs of varying types to associate one of the plurality of scheduling algorithms with each job type and a job comparator configured to cause the processor to compare a current job for scheduling against the benchmark data to determine a current job type of the current job. The evaluation module may be further configured to schedule the current job for execution on the plurality of computing nodes based on the current job type and the associated scheduling algorithm.

According to another general aspect a computer implemented method may include receiving a plurality of jobs for scheduling of execution thereof on a plurality of computing nodes and providing a common interface for each of a plurality of scheduling algorithms. The method may include utilizing the common interface in conjunction with benchmark data for a plurality of jobs of varying types to associate one of the plurality of scheduling algorithms with each job type and comparing a current job for scheduling against the benchmark data to determine a current job type of the current job. The method may further include scheduling the current job for execution on the plurality of computing nodes based on the current job type and the associated scheduling algorithm.

According to another general aspect a computer program product may be tangibly embodied on a computer readable medium and may include instructions that when executed are configured to receive a plurality of jobs for scheduling of execution thereof on a plurality of computing nodes and provide a common interface for each of a plurality of scheduling algorithms. The instructions when executed may be further configured to utilize the common interface in conjunction with benchmark data for a plurality of jobs of varying types to associate one of the plurality of scheduling algorithms with each job type and compare a current job for scheduling against the benchmark data to determine a current job type of the current job. The instructions when executed may be further configured to schedule the current job for execution on the plurality of computing nodes based on the current job type and the associated scheduling algorithm.

The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features will be apparent from the description and drawings and from the claims.

In the example of the owner operator may represent any person business or other entity which seeks to provide computing resources to the consumers e.g. as a means of earning a profit thereby. For example the owner operator may represent a business entity specializing in the remote provisioning of software or may represent a business which has excess computing capacity which the business then uses to provide computing resources to the consumers .

As referenced above the simplified example of illustrates the infrastructure as including the servers A B . . . N. Of course the infrastructure may include many additional or alternative elements not specifically illustrated in the example of some of which are described in more detail below. For example it may be appreciated that the servers . . . N may represent virtually any computing device node or cluster thereof which may be configured to provide remote data storage and processing to the consumers . Thus for example the infrastructure may be understood to include various network connections for providing communication between and among the servers B . . . N as well as between the servers A B . . . N and the scheduler and also between the infrastructure and any and all data accessed over one or more networks external to the infrastructure . Moreover the infrastructure may be understood to include or be in communication with one or more computer memories utilized by the servers A B . . . N in storing data associated with the jobs and or on behalf of the consumers . As would be apparent to one of skill in the art the such memories may be associated with the servers A B . . . N in a one to one one to many many to one or many to many relationship.

The consumers may represent virtually any person business or other entity which sends the computing jobs to the infrastructure for utilization of computing resources of the infrastructure in conjunction therewith. For example as described in the examples above the consumer may represent a business having variable processing needs e.g. payroll processing. In some example scenarios the consumer may represent an owner of an independent infrastructure not illustrated in i.e. may itself be a provider of software and or infrastructure as a service. In such examples a consumer may be unable to meet a level of demand of its own consumers and may therefore utilize computing resources of the infrastructure provided by the owner operator in order to meet its own customer demands.

The jobs may represent virtually any request for a computing related task that may be submitted by the consumers to the infrastructure e.g. over an applicable computer network. For example the jobs may include one or more requests for specific computations to be performed with respect to one or more datasets. In other example implementations the jobs may include requests for temporary or long term storage of data.

In many cases receipt and processing of the jobs may be governed by a service level agreement SLA between the owner operator and each of the consumers . For example such SLAs may dictate a priority of each job relative to other jobs or job types received from a single consumer and or relative to other jobs or job types received from other different consumers . Somewhat similarly such SLAs may govern a relative availability of computing resources of the infrastructure with respect to each consumer . Further such SLAs may be associated with cost based measures governing availability and processing times associated with the computer resources. For example the SLA may dictate that the consumer may receive more availability and or processing times in exchange for increased cost. Other example terms and characteristics of such SLAs are well known and are only described herein in additional detail to an extent necessary or helpful understanding features and functions of the system of .

As described in detail below e.g. with respect to each of the jobs may include a plurality of tasks or subtasks which are individually associated with specific computing requirements. Thus as also described in detail below data required by a given job may be received in conjunction with receipt of the job may be received in conjunction with completion of an earlier task of the job or as illustrated in the example of and described in more detail below e.g. with respect to may include network data or other data which is at least initially external to or separate from the corresponding job .

The scheduler may thus be configured as referenced above and described in more detail below to receive the jobs and to distribute the jobs within and among the servers A B . . . N. In so doing the scheduler may be configured to distribute the jobs in a manner which satisfies the various SLAs between the consumers and the owner operator and which otherwise partially or completely accomplishes one or more goals of the owner operator .

For example the owner operator may wish to configure the scheduler to distribute the jobs in a manner which minimizes an execution time of the jobs . In so doing the owner operator may achieve a high level of satisfaction of the consumers who generally may be presumed to desire to have their respective jobs completed as quickly as possible.

On the other hand it may occur that minimizing an execution time of the jobs may be associated with an under utilization of resources of the infrastructure . For example scenarios may exist in which the owner operator may wish to maximize utilization of the servers A B . . . N even at the cost of somewhat reducing an overall execution time of the jobs . For example such an outcome may be satisfactory to the owner operator as long as the relevant execution times are within the parameters dictated by the various SLAs negotiated with and purchased by the consumers .

Thus it may be observed that a number of dynamic variables exist which govern a manner in which the scheduler distributes the jobs . Of course many other such variables exist and would be known to one of skill in the art and are therefore not described herein in further detail. For example such variables may include a current changed number of available servers e.g. such as when new servers are added and or when one or more servers experiences a failure .

Moreover in many cases it may occur that a total number N servers and a total number of jobs may be quite large. Further it may be observed that a number of possibilities for distributing the jobs among the N servers grow exponentially with the addition of further jobs servers.

Thus a function of the scheduler may be understood to be an identification of the distribution of the jobs from among all possible or potential distributions which best satisfy relevant SLAs and or which otherwise accomplishes desired goals of the owner operator . However as just described a search space of possible distributions of the jobs required to be searched by the scheduler to identify the best possible distribution may be too large for the scheduler to accomplish the identification within a practical time limit.

Thus the scheduler includes a plurality of algorithms which are designed to assist the scheduler in investigating a search space of possible distributions of the jobs and in thereby identifying a best nearly best or acceptable distribution of the jobs . Moreover the scheduler includes an evaluation module which is configured to provide a common interface with or which otherwise has an ability to communicate with all of the various algorithms . That is the evaluation module as described in more detail below may be configured to evaluate and compare various potential job distributions using any one of the algorithms and relative to a desired goal of the owner operator . For example the evaluation module may be configured to evaluate possible job distributions relative to one another and with respect to a desired minimization of an overall job execution time a desired maximization of utilization of resources of the infrastructure or other metric s or combinations thereof.

As described in detail below each of the algorithms may be particularly well suited to search for an identified desired job distribution in a particular corresponding context. For example the jobs may have varying characteristics and or types and particular ones of the algorithms may be well suited to select or identify a desired job distribution for jobs of a corresponding type or having corresponding characteristics.

Somewhat similarly in additional or alternative examples particular ones of the algorithms may be particularly useful in determining a desired job distribution in the presence of the external network data dependent upon a type or extent of the external network data . In still further examples particular ones of the algorithms may be relatively more useful in scenarios in which a relatively small number of jobs are to be distributed among a relatively small number of servers while other ones of the algorithms may be well suited for selecting identifying a job distribution in the presence of a large number of jobs to be distributed over a large number of servers. Nonetheless because the evaluation module is configured to interoperate with any and all of the algorithms in a fast efficient and practical manner the scheduler may be configured to select and utilize an available algorithm from the algorithms in order to implement a current distribution of the jobs .

Specifically as shown the scheduler may accumulate benchmark data which includes collected data and other related heuristics associated with previous calculations of job distributions. For example in some implementations the benchmark data may be collected by the scheduler over time in conjunction with normal operations of the scheduler . In additional or alternative examples the benchmark data may be collected in conjunction with operations of the scheduler which are implemented specifically for the purpose of collecting the benchmark data .

Thus the benchmark data may generally be understood to include e.g. previous job distribution processes conducted by the scheduler along with related data which may be useful in selecting an algorithm for use in conducting a current or future job distribution. For example the benchmark data may include data related to a name nature type or other characteristic of relevant jobs included in the benchmark job distribution. The benchmark data also may include performance data related to subsequent computing operations performed in accordance with the resulting selected job distribution.

Thus in a specific example one or more sets of jobs may initially be used to create the benchmark data . For example initial jobs may be received by the scheduler and scheduled using any and all applicable algorithms of the algorithm i.e. using the evaluation module as a common middleware for implementing the algorithms with respect to the initial set of jobs . Then an algorithm selector may evaluate the resulting job distributions associated with the various algorithms e.g. may evaluate a length of time required by each algorithm to select a resulting job distribution and or may evaluate a resulting computing time utilization measure or other metric associated with the completion of the initial set of jobs. In this way the algorithm selector may select a particular one of the algorithms and may store the algorithm in conjunction with related job performance data within the benchmark data .

Then when a current or a future set of jobs is received a job comparator may be configured to analyze the received jobs . relative to the benchmark data whereupon the algorithm selector may be configured to select an algorithm from the algorithms which is determined to be best suited for conducting a distribution of the current set of jobs . Once selected the selected algorithm may be utilized by the evaluation module to proceed with conducting the identification and selection of a job distribution to be used in assigning the jobs within and among available ones of the servers A B . . . N. In this way the scheduler provides a customized optimized dynamic technique for distributing the jobs in a manner which is most likely to accomplish desired goals of the owner operator and or the consumers .

Finally with respect to the scheduler of a task scheduler is illustrated which may be configured to arrange individual tasks of one or more of the jobs for execution thereof on an individual assigned one of the servers A B . . . N. That is as described herein the jobs may each include a plurality of separate discrete tasks. Each task may be processed using e.g. data received from a previously completed task and or using the external network data . Consequently once operations of the evaluation module are completed and the jobs are distributed each server which receives one of the jobs may proceed with execution of the tasks thereof.

However if there is a delay in executing a specific one of the tasks such as may occur when an executing task requires external network data which is not currently available then an overall delay in operations of the relevant server may be experienced. Consequently the task scheduler may be configured to reschedule individual tasks being implemented by a specific server so as for example to begin execution of an otherwise subsequent task if and when a current task experiences a delay such as just described. In particular the task scheduler may be configured to use any one or more of the algorithms as described herein to execute such scheduling re scheduling operations. In this way operations of the infrastructure may be further optimized. Specific example operations of the task scheduler are described in detail below e.g. with respect to .

In the example of the scheduler is illustrated as being executed by at least one computing device . As illustrated the at least one computing device may include at least one processor A as well as a computer readable storage medium B. Thus for example it may be appreciated that many different implementations of the scheduler are contemplated by the example of . For example although the scheduler is illustrated in as executing on a single computing device it may be appreciated that various modules and associated functions of the scheduler may be implemented using different computing devices which may be in communications with one another. Somewhat similarly it may be appreciated that the scheduler may be implemented on a single computing device having a plurality of processing cores so that operations of the scheduler may be implemented using parallel processing and associated techniques.

Further although illustrated on the at least one computing device as being separate from the servers A B . . . N it may be appreciated that the scheduler may be implemented on or using one or more of the various servers or other computing nodes of the infrastructure . Finally with respect to although the scheduler is illustrated and described as including various separate discrete components it may be appreciated that functionality of any one of the illustrated components may be implemented using two or more subcomponents. Conversely it may be appreciated that functionalities of any two or more of the components may be implemented using a single component. Thus many possible variations of the system of may be implemented by one of skill in the art some examples of which are provided below in more detail with respect to .

Referring first to a workload is illustrated as being provided to a distributed system . As shown the workload includes jobs . Each job includes tasks e.g. the job includes tasks . a scheduling portal accepts the incoming job submission requests pre processes the jobs and dispatches them onto the distributed system i.e. to nodes .

As may be appreciated from the above description of the scheduling portal in the example of may represent or include the scheduler of . The portal as shown accepts incoming jobs pre processes or compiles the queries and dispatches the jobs onto the computation nodes . The portal may itself be a distributed component but for purposes of this description only the portal s scheduling responsibilities are described while specific details of internal implementation of the scheduling portal are not described here in detail.

For purposes of the non limiting examples described herein as just referenced a job comprises multiple tasks. For example a query rocessing request can have its individual statements be compiled down to physical operators such as reading from logs performing map reduce execution and writing to an output. The tasks of each job are assumed to form a directed acyclic graph DAG with respect to other tasks of the same job.

Each task in a job can take two forms of input 1 the output of an upstream task and 2 external data objects read from outside the distributed computing system such as data being read from an external SQL database or a file being read from a web or FTP server. For 2 data is cached locally and can be reused by a subsequent task executing on the same node but network transmission delay is incurred for the initial fetch. illustrates an example job with tasks forming a DAG in which tasks receive data from preceding tasks and tasks receive data from external data objects of a network . Tasks in the graph can be configured to executed serially but a complex graph can be executed so long as it remains a DAG configuration. Task input and output can branch for example a task performing a join operation can take multiple inputs and a task performing a partitioning can produce multiple outputs.

For purposes of example different jobs may share the same task for example a stored procedure to perform a complex calculation may be re used by different jobs. Once tasks are assigned to a compute node the node executes the tasks sequentially where each task is given maximum user mode CPU utilization on that server aside from whatever cycles are given to the operating system and middleware components .

Arriving tasks at the portal are enqueued in FIFO order and scheduling is performed one workload at a time. That is at periodic intervals all jobs currently in the portal s queue are scheduled to be run which is an approach which allows the scheduler to perform a tight packing of tasks onto the compute nodes . Jobs that arrive while the current workload is executing will be enqueued for the next periodic iteration of the scheduler.

Thus the system of operating in the context of the examples of may be configured to consider not only the job task execution time but also a data load time from client sites to the execution platform hosted by the SaaS IaaS providers. Further the system of provides resource level scheduling for concurrent execution of multiple jobs tasks in order to maximize the throughput of the computation platform. Still further the system of takes into consideration the competitive deadline of each job scheduling at the job level and can be used as a system level scheduling tool as well as by the IaaS providers to maximize usage of their computation capacities.

As shown in and described below a placement of jobs onto servers impacts the execution of the tasks and as a result the workload as a whole. In the example embodiment of and related examples described herein it is assumed that all tasks of the same job respectively are assigned to the same server as shown where the configuration of the task instances must conform to a directed acyclic graph. External data objects are cached on demand using caches and can be reused by multiple tasks.

In alternative examples illustrates an alternative execution model where tasks and rather than jobs are the units of work that are assigned by the scheduler respectively to compute nodes such that the task execution continues to conform to the DAG restriction. This mode of execution provides an additional degree of freedom for the scheduler to leverage more parallelism. Specifically for example since tasks are placed without restriction there would then be ST possible placements of T tasks onto the S servers rather than Spossible placements of J jobs onto the S servers . Aside from the degenerate case where a job comprises only one task the number of tasks T is greater than the number of jobs J as defined herein. In the example of dashed lines indicate cross node data communication between an upstream producer and a downstream consumer. Also in particular tasks may belong to different jobs and again need not be located on the same node as the other tasks of corresponding jobs.

Thus the execution model of allows for more flexibility for scheduling and more opportunities for parallelism but the cost of runtime orchestration may be higher. Dependencies between upstream producers and downstream consumers becomes more complicated e.g. since the nodes are shared nothing any results from a producer may need to be transferred over the network to a consumer that resides on a different node. This approach might be preferable e.g. in instances where the consumer takes multiple inputs and where co location of the consumer on a node where another cached data object or another upstream producer is located may outweigh the cost of cross node communication.

Thus with reference to it is the goal of the scheduling portal and or to place tasks onto compute nodes in such a manner that the placement optimizes some metric. As examples described herein the scheduler is described as optimizing against either or both of two metrics. Specifically a first metric is a workload completion time commonly called the makespan . The scheduler attempts to minimize the workload completion time as a whole where the completion time is defined as the time that the last job in the workload completes. A second metric is a business value for which a business value is assigned for each task s completion based on some agreed upon service level agreement SLA between the users and the infrastructure owner.

As referenced above SLAs by themselves are well known. For purposes of this description it is assumed that the agreement is defined in terms of a generic business value metric that allows users to prioritize the scheduling preference of their jobs in a clear manner. The per job metric may be written in terms of corresponding constants e.g. i a wallclock soft deadline ii a wallclock hard deadline iii a positive business value e.g if the task completes before the deadline iv a negative business value e.g. if the task completes at after the soft deadline and before the hard deadline and v a negative business value e.g. if the task completes at after the hard deadline.

For example by placing a large positive value for a and a large negative value for the user can define that the job should be given high scheduling preference. The illustrated scheduling design can be configured to optimize against either workload completion time or business value e.g. by virtues of the evaluation module and other features and functions of the system of . For the sake of consistency the present description optimizes by finding a minimum in either case even though technically speaking the business value metric leads to higher numerical business values being more beneficial so in the examples the negation of the business value may be minimized.

Other optimization metrics may be used. For example a weighted combination of the makespan and business value result may be used. Such a combination may provide weighted coefficients which the users or the providers can change to fit their needs.

The solution space for assigning jobs tasks to nodes encompasses situations e.g. where tasks of the same job can be either co located on the same server or placed on separate servers. In the former case or where data can be sent from one task to the next directly either directly through memory or through an intermediary file. In the latter case data must be sent between tasks over the network which is slower in most cases but allows tasks to be executed in parallel within the tasks DAG structure. In the present description the first case is assumed in which all tasks of a job must be placed together on the same server e.g. the example of .

As also described each task may read data from an upstream sources and can download external data objects in both cases if the task is re used by multiple jobs or is re used in the same job but as a different instance of the task then it is beneficial to have producing and consuming tasks co located as well as external data objects and the tasks that read them.

Equations 1 6 represent examples of formulae with the objective of minimizing the makespan and may be used to examine how the decisions of the scheduler affect this value. For example Let Ps be the execution time of server s from the set of servers S. The makespan is defined as the longest server completion time as shown in Equation 1 

Let Tbe the set of task instances t assigned to a server s. The execution time of each server is then the aggregate sum of over all Pt the completion time for an individual task t as shown in Equation 2 

The execution time Pfor a task may include several components. For example if Lis the time to download each external data object required by the task Cmay be the time to execute its computation and At may be a stepped indicator function that indicates whether or not data from upstream tasks is available as shown in Equation 3 Equation 3

For purposes of the examples a time to download an external data object may be assumed to be dominated by the network transmission delay Pfor that object. Also the system caches each data object so that other tasks can reuse it as described herein.

Then if Iis a boolean indicator function that is 1 if d is in the cache and 0 otherwise then a resulting time Lto acquire all external data objects for a task is then a sum over all data objects d in the set of needed data objects Dt for the task as shown in Equation 4 

The availability of data from upstream tasks is represented by the indicator function A as shown in Equation 5. If Uis the set of upstream tasks needed by a task t and u is one task from this set and if lis a boolean indicator function with a value of 1 if upstream task u was scheduled before the task and 0 otherwise then an availability function Aof Equation 4 is then 1 if all upstream tasks were run already and 1 if not where the latter case s value indicates that the task waits for an infinite amount of time because its upstream data is not available. In programmatic terms this value is represented by a large value such as MAX INT that would allow the scheduler to easily pick lower valued results for the overall makespan.

The makespan formulation described above illustrates impact of the decisions of the scheduler . For example to allow tasks to consume data from its needed upstream tasks the scheduler should place jobs onto the same processing node so that the needed tasks are placed together. If a task is placed onto a compute node without a needed upstream task then the task will not complete which would result in At 1. However the fact that all tasks of the same job are assumed to be assigned to the same server implies that At will always be 1. Furthermore since the system caches external data objects it is beneficial to co locate tasks that share similar needed data objects so that the network transmission cost is not incurred more than once. A good schedule will produce a high number of co locations which will reduce Lt the time to acquire these external objects. In the most degenerate case all tasks are placed onto only one server which maximizes co location. On the other hand the scheduler is given the flexibility to distribute jobs and their tasks across multiple servers so that they can run in parallel. This parallelism is manifest in the formulation by taking the maximum value over the summation over all tasks t in the set Ts assigned to a given server. If the tasks are spread out evenly then each server will have an equal number of tasks.

Given this formulation the objective of the scheduler is then to find the job placements that minimize the makespan. However as referenced above for J unique jobs and S unique server nodes the search space of the scheduler is exponential in the number of jobs. Assuming all tasks of the same job are assigned to the same server there are then Splacements of the J jobs onto the S servers. Within this search space is the optimal placement that satisfies the scheduler s goals. However this exponential search space becomes too large for exhaustive search even in the smallest typical cases of S and J.

As referenced above with respect to and described and illustrated in more detail below with respect to the system may use a number of different scheduling algorithms . A number of example algorithms are provided below however it may be appreciated that additional or alternative example algorithms may be used as well.

One example algorithm is an exhaustive search EXH algorithm. Exhaustive search is guaranteed to find the optimal solution in the search space by iterating through all possible placements of jobs to the server nodes. As mentioned this approach does not scale well when the search space is exponentially large.

A greedy approximation heuristic GREEDY approximation is fast to determine and produces reasonably good results. For example a greedy heuristic may be useful where each job in the workload is stable sorted by hard deadline and then soft deadline then the resulting ordered jobs may be assigned round robin across the servers. This approach is greedy because at any given placement decision the job is assigned to a server without regard to previous or future decisions.

Purely random assignments RAND involve repeatedly evaluating random job assignments to servers and is the first of four different random search heuristics provided herein. As shown in Algorithm 1 RAND proceeds in the following manner 1 Choose a random job placement as the initial best result and evaluate it according to the optimization criteria. 2 Choose another random job placement if the resulting evaluation value is less than the value from the previous step then this job placement is the new optimum. 3 Repeat step 2 for a fixed number of rounds. RAND is fast to execute and explores a wide range of regions in the search space however it does not take advantage of clustered neighborhoods where a good solution leads to an adjacent better solution in the search space.

Steepest ascent hill climbing with random restart HC an example of which is shown in Algorithm 2 tries to take advantage of neighboring solutions in the search space. Like RAND it repeatedly evaluates candidate solutions and compares them to the best solutions in the previous round to determine the final best solution but at each iteration a neighborhood of candidate solutions is evaluated at a time with the best scoring solution in the neighborhood being selected as the current best solution in the next round. The next neighborhood is found by mutating the current best solution in a deterministic fashion. To find the neighborhood of a given candidate solution new neighbors are created by incrementing and decrementing each array element by one. For an array of length N therefore 2N neighbors may be formed. HC is called steepest ascent because the best scoring neighbor is picked to serve as the best solution for the next round. Additionally or alternatively a random restart may be implemented where if the neighbors become homogeneous the resulting solution is saved and a random restart of the HC algorithm from another location is executed. Then after a fixed number of such restarts the best solution across all retries is selected.

A genetic algorithm GA simulates natural selection by repeatedly having members of a population compete against each other over several generations where the highest evaluating population member of the last generation is chosen as the best solution. At a high level the algorithm proceeds as shown Algorithm 3. Specifically an initial population of candidate solutions is chosen. The population is then culled for the top N best candidates to serve as parents for the next generation. Candidate solutions are interbred using inspiration from biological chromosome combination to produce a new population of children and the cycle continues for a fixed number of generations. In example implementations a two point crossover technique may be used between two parent solutions to produce a child solution. Two cut points are randomly chosen in the solution array of . Array elements from the first parent between the two cut points inclusive and array elements outside the cut points from the second parent are given to the child. To increase the range of solutions in the search space that we explore mutation may be included where a produced child will undergo a random numerical change to a different valid value in one of its array element.

Simulated annealing SA is another randomized searcher which is an algorithm that explores the solution space over a series of iterations in a manner similar to RAND as shown in Algorithm 4. An initial candidate is chosen as the best solution and a subsequent candidate is then derived from the best solution so that this new candidate is a local neighbor. At this point the new candidate evaluation metric is compared with the best solution so far with the winner being chosen as the best solution for the next iteration. The key point with simulated annealing is that with some probability a candidate can be chosen as the current best solution even if its evaluation metric is worse than its immediate competitor. This probability is largest at the start and decreases over the iterations with the end result being that the algorithm explores a large portion of the solution space early but then focuses only on the clear winners late. Simulated annealing draws inspiration from a metallurgical thermodynamic process of heating a metal to allow atoms to move about with high energy and then letting it to cool to allow the atoms to settle into a lower energy configuration.

Thus the system of supports the execution model described above and provides a suite of random search algorithms discussed above i.e. HC SA RAND and GA. In example implementations the random search algorithms HC SA RAND and GA initially produce modest results when run with a large search space e.g. on the order of 100 servers and 1000 jobs . To improve their execution each of these algorithms may be primed with an initial candidate solution taken to be the result of the GREEDY algorithm.

As described above with respect to the evaluation module may provide an evaluation function that is a common component used by all the placement algorithms EXH GREEDY RAND HC GA and SA that returns a metric for the workload as a whole. As mentioned previously two example workload metrics may be used e.g. the makespan and the aggregate business value.

To calculate the overall makespan each server s completion time is calculated using the pseudocode in Algorithm 5 below and the maximum completion time is returned as the workload makespan. Analogously to calculate the overall business value each server s aggregate business value is calculated using the pseudocode in Algorithm 6 below and the sum of the business values across all servers is returned as the aggregate business value.

Both functions execute in the following manner. Each server receives a list of tasks from the schedule placement. When assuming jobs are assigned to a server all of the jobs tasks are provided in this list. The list of tasks are initially sorted by topological order to preserve the DAG relationship among the tasks. For every task in the list of tasks the task s required external data objects are downloaded into a cache if they are not already there incurring network transmission delay. Once the external data is available along with data from any upstream tasks in the DAG the current task executes and produces output which can then be read by the downstream consuming task s .

In the example of a plurality of jobs may be received for scheduling of execution thereof on a plurality of computing nodes . For example the scheduler or may be configured to receive the jobs for distribution among computing nodes including the servers A B . . . N.

A common interface may be provided for each of a plurality of scheduling algorithms . For example the evaluation module may provide a common interface for any and all of the algorithms .

The common interface may be utilized in conjunction with benchmark data for a plurality of jobs of varying types to thereby associate one of the plurality of scheduling algorithms with each job type . For example the algorithm selector may utilize the evaluation module in conjunction with the benchmark data to associate at least one for the algorithms with each determined job type. As described the benchmark data may further be used to associate each of the plurality of scheduling algorithms with other characteristics of operations of the scheduler such as e.g. a number of jobs to be distributed and or a number of currently available computing nodes e.g. servers .

A current job for scheduling may be compared against the benchmark data to determine a current job type of the current job . For example the job comparator may be configured to compare a current plurality of the jobs relative to the benchmark data so as to thereby determine one or more current job types of the current plurality of jobs.

The current plurality of jobs may be scheduled for execution on the plurality of computing nodes based on the current job type and the associated scheduling algorithm . For example the evaluation module may be configured to schedule the plurality of current jobs for an execution on the servers A B . . . N using a selected algorithm from the plurality of scheduling algorithms .

In this way as described herein benchmark data may be collected which characterizes jobs for distribution and related data. In the example of if an insufficient amount of benchmark data has been collected then operations may continue until sufficient benchmark data is collected. For example the algorithm selector may be configured to determine that a certain quantity or quality of the benchmark data has been collected and is sufficient for future operations of the job comparator in selecting algorithms for current and future sets of jobs.

Thus as new jobs are received the algorithm selector may perform an initial filtering of the available algorithms to remove algorithms from further consideration which are not suitable for the newly received jobs . For example sets of jobs of a certain size may exceed a threshold associated with potential use of an exhaustive algorithm.

Subsequently the job comparator may be configured to compare the new job s to previous jobs and associated data within the benchmark data . For example the job comparator may be configured to extract features from or associated with the new job s and to thereafter classify the new job s based thereon. For example in addition to other examples provided herein exemplary features that may be used to represent the new job s may include but are not limited to data cardinality of each task of the job s a degree of fan in and or fan out of each task of each task of the new job s operator names of each task of the new job s and or a topological ordering of the tasks of the new job s .

In this way the algorithm selector may select an algorithm of the algorithms which was previously utilized in a successful manner in conjunction with the most similar jobs of the benchmark data . That is for example the algorithm selector may implement a machine learning classifier algorithm to identify a best suited algorithm of the algorithms .

In this way the evaluation module may proceed to utilize the selected algorithm using the common interface of the evaluation module to thereby obtain an actual job distribution to be used in scheduling the new jobs . Once such scheduling has been completed the task scheduler may be configured to further arrange tasks of the distributed jobs on each server in a manner which minimizes server delays associated with attempted executions of the distributed tasks . Example operations of the task scheduler in this regard are described below in more detail with respect to .

More specifically as referenced above it may be appreciated that upon operations of the evaluation module in providing a distribution of the jobs each computing node e.g. each server A B . . . N contains a specific number of assigned jobs and the various tasks of each job where each job has its task topologically sorted so that by the time a specific task is ready to execute all of its data input dependencies have been resolved. That is all needed data has been staged either from upstream tasks and or from data objects that have been placed into an external data object cache. Although this approach fixes the ordering of each job s tasks the approach also allows tasks from different jobs to be run in an intermingled order. Thus tasks may be sequentially executed or in additional or alternative implementations parallelism may be allowed so that tasks may execute while external data objects are being downloaded into the relevant external data object cache. Then during the time that an external object is being downloaded there may be a gap in execution since the task awaiting the object is blocked. As described rather then experiencing this gap as an overall delay the task scheduler may be utilized to allow other tasks to run at that time. Specifically as described in detail below the task scheduler may execute its own search space algorithm to find an optimal or near optimal ordering that will identify a task sequence out of a space of all permissible sequence permeations that utilizes the above described execution gaps.

In particular the task scheduler may utilize any appropriate one or more of the algorithms to find the optimal or near optimal ordering e.g. one or more of a genetic algorithm a greedy algorithm a simulated annealing algorithm and or a hill climbing algorithm . However as may be appreciated from the above description the various algorithms are implemented to perform searches through a discrete search space whereas the scheduling of tasks into execution gaps as described above represent scheduling with respect to a continuous non discrete time span. Thus in the specific examples of the attached scheduler may be configured to consider an available execution time span in a discretized manner which enables utilization of a desired one of the algorithms e.g. the genetic algorithm described above.

Thus in the example of a timeline illustrates the nature of the above described problem i.e. scheduling sequences of tasks within an open continuous time period containing an infinite number of points and possibly failing to identify a specific N time point beyond which tasks may not be placed.

Thus timeline illustrates identification of an end time point while timeline illustrates a subsequent division of the time span of the timeline into specific time execution units. For example the timeline might be understood to represent one minute per illustrated time unit.

In the example of the timeline the various time units and the end point may be selected to represent the tightest packing of all the tasks together i.e. representing a minimum time span in which the tasks can run and assuming that each task begins execution at the start of a time execution unit. That is in an initial iteration the timeline may be defined on the initial assumption that no execution gaps associated with delays associated with downloading external data objects will be necessary.

Thus the task scheduler may implement selected scheduling algorithm for scheduling the tasks within and among the various time units of the timeline . However assuming the presence of execution gaps as described above the task scheduler may be unable to define an appropriate solution. Consequently as illustrated in the example of the timeline a new further end point of time may be selected in order to enlarge the available search space and or the fixed length of each execution time slot may be made smaller so that again more potential schedules may be considered. In other words by expanding the time span to produce gaps where downloading of external data objects may occur tasks may be swapped into the gaps in order to create a more optimized schedule. The downloading gaps may occur anywhere in time along the timeline as long as each gap comes before an execution of a corresponding task requiring the downloaded data. Thus the selected search algorithm may be utilized to explore such variations in the time span of the timeline .

Subsequently as described above with respect to the timeline an end time point may continually be extended and or execution time units may be made smaller until the search algorithm returns an acceptable solution. In so doing however it may occur that the end time point is unnecessarily extended beyond an optimal point required for minimizing an overall execution time of the tasks while nonetheless taking advantage of any execution gaps. Similarly it may occur that the execution time units may be made smaller than required.

Consequently as illustrated with respect to a timeline once a solution is found by the task scheduler using the appropriate scheduling algorithm a new earlier end time point may be selected in order to make the search space smaller and or the execution time units may be made larger in order to decrease a number of potential schedules that may be considered.

Similarly as described above such manipulations of the end time point and or execution time units in an opposite direction may result in an un workability of the search algorithm thereby defining a bound and requiring further adjustments within the thus bounded search space as illustrated in the example timeline to thereby obtain a final solution.

In specific example implementations an optimal time span may be found through a binary search. For example T may be understood to represent a time span resulting from the tightest packing of tasks e.g. corresponding to the timeline . Then subsequent timelines e.g. the timeline may be defined as multiples 2T 4T 8T . . . of the original time span T. In this way iterations through a binary search may proceed until the time span is located in which the gaps may be scheduled and the tasks may be placed into the gaps as part of a feasible task execution plan for a node in question. Then as described above with respect to the timeline once a candidate time span is found where a feasible execution schedule exists refinements to the time span may be made by performing a further binary search between the current time span and the time span from a previous iteration.

A length of time from a first task to a last task may be selected as described above with respect to the timeline . Subsequently execution time slots for the defined timeline may be further defined as illustrated with respect to the timeline .

As described above the search may be bootstrapped by initially assuming a tightest possible packing of the tasks within the timeline and the selected scheduling algorithm may be executed . If a solution is not reached then the length of the time span may be increased and or the lengths of the execution time slots may be decreased as illustrated above with respect to the timeline . Specifically as in the example provided above a binary search may be conducted in which the length of the time span may be doubled.

In this way as described above execution gaps associated with the identified stalling candidates may be included within possible scheduling solutions considered by the selected scheduling algorithm during a subsequent execution thereof . If a solution which includes the desired execution gaps is still not reached then adjustments to increase the time span length and or decrease the time slot lengths may continue until an execution of the scheduling algorithm results in the reaching of a feasible scheduling solution . Again in the specific example provided above a binary search may be executed in which the time span length is doubled and or in which the execution time slot length is halved at each iteration.

Once a solution is reached then the time span length may be decreased and or the slot lengths may be increased . The scheduling algorithm maybe executed again in order to determine whether a feasible solution may be reached .

If a feasible solution may still be reached even with the decreased time span length and or increased time slot lengths then continued decreases in time span lengths and or increases in time slot lengths may continue until execution of the scheduling algorithm determines that lengths have been reached at which no feasible solution exists . In other words iterations may continue until a bound is reached.

In this way bounds may be established which define a range in which an optimal solution exists so that any desired final adjustments may be made within the thus defined range. In particular for example a further binary search may be performed within the defined range in order to quickly narrow possible task schedules and identify an optimal or near optimal task schedule which considers the presence of execution gaps due to requirements for the use of network data.

Implementations of the various techniques described herein may be implemented in digital electronic circuitry or in computer hardware firmware software or in combinations of them. Implementations may implemented as a computer program product i.e. a computer program tangibly embodied in an information carrier e.g. in a machine readable storage device or in a propagated signal for execution by or to control the operation of data processing apparatus e.g. a programmable processor a computer or multiple computers. A computer program such as the computer program s described above can be written in any form of programming language including compiled or interpreted languages and can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.

Method steps may be performed by one or more programmable processors executing a computer program to perform functions by operating on input data and generating output. Method steps also may be performed by and an apparatus may be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit .

Processors suitable for the execution of a computer program include by way of example both general and special purpose microprocessors and any one or more processors of any kind of digital computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. Elements of a computer may include at least one processor for executing instructions and one or more memory devices for storing instructions and data. Generally a computer also may include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. Information carriers suitable for embodying computer program instructions and data include all forms of non volatile memory including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory may be supplemented by or incorporated in special purpose logic circuitry.

To provide for interaction with a user implementations may be implemented on a computer having a display device e.g. a cathode ray tube CRT or liquid crystal display LCD monitor for displaying information to the user and a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input.

Implementations may be implemented in a computing system that includes a back end component e.g. as a data server or that includes a middleware component e.g. an application server or that includes a front end component e.g. a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation or any combination of such back end middleware or front end components. Components may be interconnected by any form or medium of digital data communication e.g. a communication network. Examples of communication networks include a local area network LAN and a wide area network WAN e.g. the Internet.

While certain features of the described implementations have been illustrated as described herein many modifications substitutions changes and equivalents will now occur to those skilled in the art. It is therefore to be understood that the appended claims are intended to cover all such modifications and changes as fall within the scope of the embodiments.

