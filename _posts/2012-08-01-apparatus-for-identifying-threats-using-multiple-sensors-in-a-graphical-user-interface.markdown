---

title: Apparatus for identifying threats using multiple sensors in a graphical user interface
abstract: A system of imaging and non-imaging sensors are used in combination with a graphical user interface (GUI) system on a vehicle to detect items of interest. In particular, a GUI has been developed that seamlessly integrates high magnification, Narrow Field of View (NFOV) imaging sensors and Wide Field of View (WFOV) imaging sensors. The GUI is capable of displaying both WFOV and NFOV images, gimbal controls, and allow NFOV sensor to be pointed to any location within the wide field of view efficiently by a single touch of a touch screen display. The overall goal is to allow an operator to select which imagery from multiple WFOV sensors to display in order to prescreen regions of interest that require further investigation using sensors with more magnification.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08405494&OS=08405494&RS=08405494
owner: The United States of America as represented by the Secretary of the Army
number: 08405494
owner_city: Washington
owner_country: US
publication_date: 20120801
---
This is a divisional patent application of copending application Ser. No. 12 505 622 filed Jul. 20 2009 entitled Method and Apparatus for Identifying Threats Using Multiple Sensors in a Graphical User Interface. The aforementioned application is hereby incorporated herein by reference.

The invention described herein may be manufactured used sold imported and or licensed by or for the United States Government.

A system of imaging and non imaging sensors are used in combination with a graphical user interface GUI system on a vehicle to detect items of interest.

Cameras have been used to visually identify unknown objects. But cameras with wide angle lenses do not have sufficient resolution to identify faint objects in their natural background.

There is a need for new or improved systems to rapidly evaluate the threat potential of faint objects at a relatively long distance. This technology is also applicable to other platforms whether or not moveable.

In one aspect there is disclosed an exemplary GUI system to integrate multiple low magnification Wide Field of View WFOV sensors with high magnification sensors in different combinations to point Narrow Field of View NFOV sensors at regions of interest. The operator can thereby maintain situational awareness and context while viewing those regions of interest.

In another aspect of the disclosure an exemplary GUI system is provided with an electrical T connection between the GUI system and the Gyrocam gimbal to integrate an analog sensor system with a digital GUI system. This approach minimizes the amount of hardware necessary to accomplish the overall goal.

In yet another aspect of the disclosure a method of identifying threats is disclosed using multiple sensors in a graphical user interface. Such a method comprises the steps of reading x and y coordinate values of an object image as located in a context image provided by a sensor capable of a context field of view using the context field of view to convert the x and y coordinate values to a first set of azimuth and elevation values for the object image from the context sensor projecting a vector from the context sensor location in a direction indicated by the first set of azimuth and elevation values for the object image to calculate an intercept with a flat ground plane at a point T calculating a set of derived azimuth and elevation values based on a spatial relationship between the gimbal sensor and the point T and using the derived azimuth and elevation values as a basis for pointing the gimbal sensor at T.

In yet another aspect of the disclosure a computer program for execution by a computing device is disclosed for identifying threats using multiple sensors in a graphical user interface. Such a computer program comprises the executable steps of requesting an image frame from a context sensor reading a gimbal location to get a current pointing angle and commanding the gimbal to point in a direction.

An exemplary GUI system is based on multiple low magnification Wide Field of View WFOV sensors and and high magnification sensors in different combinations which operate to point Narrow Field of View NFOV sensors at regions of interest based on a multi axis stabilization .

In order to achieve a stabilized high magnification NFOV sensor system a commercially available Gyrocam gimbal system was selected which has multi axis stabilization three sensors color visible medium wave IR MWIR FLIR and an image intensifier coupled to a CCD e.g. for digital output and various high magnification settings for all the sensors. The Gyrocam gimbal system has analog signal interfaces to control movement of the gimbal head control the multiple sensors inside the gimbal head and control which sensor s imagery to display to the operator as well as interface s for the image output from each sensor. Such a Gyrocam gimbal system was configured to have a gimbal sensor head a kneepad controller a touch screen display and an interface module . To allow digital interface connections to the GUI system server an interface unit was implemented based on a T connection to serve as a system junction box that ensures the proper voltages and currents are applied to and read from the gimbal sensor head based on operator inputs through e.g. the GUI touch screen and or the kneepad controller . An exemplary WFOV IR camera chosen was the Drivers Vision Enhancer DVE produced by DRS Technologies Inc. Such DVE sensors can have an additional port on DVE to display the sensor output on another second separate display. The Gyrocam extendable mast with sensor head implemented as the NFOV sensor system is mounted on the rear of a vehicle which may be a HMMMV. A DVE IR camera and color visible camera are mounted near or in front of the windshield as WFOV sensors which facilitates viewing by vehicle s driver of the surroundings.

The GUI system display with which an operator interacts is shown in . The system is configured with inputs e.g. and from multiple sensors and sensor types e.g. and . It can include e.g. an Illunis XMV 2020C HDTV format color visible camera and DVE IR camera used as the WFOV sensors with their FOV set at approximately 40 degrees horizontally. Alternatively in another example the WFOV color context camera can be implemented with a JAI Pulnix CV M8CL camera which utilizes a 3 CCD technology.

The NFOV sensors in this exemplary configuration can be those within the commercially available Gyrocam Triple Sensor TS a mid wave IR a color visible and an image intensifier coupled to a CCD e.g. for digital output . These can be set to FOVs that are approximately 2.5 degrees horizontally. Imagery from these sensors can be displayed in the upper right hand side of the GUI touch screen such as the color image shown in from e.g. the Gyrocam color visible sensor . A reticule is overlaid on the WFOV image being displayed to show the operator where the NFOV sensor e.g. of is pointing. The mid lower right side of the GUI can have buttons e.g. that implement controls necessary to move a gimbal sensor head e.g. of a Gyrocam gimbal system in various directions. A gimbal home button shown in is provided to control the gimbal system to move the gimbal sensor head to a known position e.g. in case of operator confusion. There can be tabs e.g. along e.g. the upper right hand side of the GUI touch screen display to allow an operator to select which NFOV sensor imagery e.g. or to display. A large button e.g. below the gimbal controls can allow an operator to select which WFOV sensor imagery e.g. or to display e.g. on the context sensor WFOV interface area .

Optionally there can also be a full screen mode selectable by touching a Full Screen button . As also shown in Mark and Decl buttons can be provided to allow user touch screen marking confirm of an object image or to declare by touch screen marking a suspicious object image. These Mark and Decl touch screen buttons have been primarily used for testing and evaluation purposes.

Further based on data inputs to the connections to the server e.g. from GPS and or other sensors system messages such as time and vehicle location can be displayed to the operator in a display area e.g. below the button controls .

When touching the WFOV image area e.g. the pixel x y location at the point in the image being touched is translated from pixel x y coordinates into angle space in order to issue commands to move the gimbal to a particular azimuth and elevation. The conversion from image coordinates to angle space is done through a flat earth approximation.

Touching the gimbal control buttons e.g. on the right hand side of the display moves a gimbal sensor head e.g. at a constant rate while they are held down in the direction of the arrow shown on the button e.g. .

Touching the NFOV image e.g. being displayed can cause the gimbal sensor head to move at a variable rate based on the distance from the center of the image touch closer to the center can cause smaller or slower incremental moves while touching closer to the outer edge causes faster or larger incremental moves of the gimbal sensor head . Direction of movement is identical to the arrow buttons e.g. and touching the top or bottom of the image causes the gimbal sensor head to move up or down respectively and the same when touching the left or right side of the image.

The GUI system can also have the capability to provide a proximity warning to an operator. This proximity warning is based on Global Positioning System GPS coordinates of previous incidents. In an exemplary embodiment a Garmin GPS 18 5 Hz Garmin part 010 00321 07 can provide positional updates to the GUI system . This device is connected e.g. via a serial port to the server connection e.g. on an X7DA8 motherboard via a DB9 connector. At system power up the Garmin device is switched into binary mode by sending it a Garmin specific command sequence. This is done to place it into a known state. After this is complete the GPS communicates to the GUI system via a binary protocol providing position updates at e.g. 5 Hz rate. A thread of execution manages the receipt of data from the GPS .

The digital compass can be a Sparton SP3003D device. Such a compass can be connected to the server by a serial connection e.g. via a Serial to USB translator. Using such a translator such a compass can be connected as a server connection . Similar in operation to the GPS device the compass communicates to the GUI system in binary mode via a defined command set. An API was written to facilitate use of this device. The compass functionality can provide heading updates to the system within the same thread used to access the GPS device e.g. at 5 Hz updates.

Further the proximity warning can be accomplished by turning the border of the GUI touch screen display from its natural grey color to yellow meaning that the vehicle system is approaching an incident and then finally red when the vehicle system is closer to the incident. As the vehicle system passes and travels further away from the previous incident the sequence of border colors reverses indicating to the operator the incident area has passed.

Such a GUI system can be implemented based on the use of WFOV sensors e.g. or to point NFOV high magnification sensors with a multi sensor GUI . The computer system e.g. which provides the processing throughput necessary to acquire the sensors data e.g. and or manipulate pointing angles process signals write graphics on the touch screen display record data and process operator inputs can be a Supermicro X7 DB8 motherboard with a Intel Xeon X5355 Dual Quad 2.66 GHz with 4 GB of RAM. An NVIDIA GeForce 8800 GTX graphics card can be used to display information on the Hope Industrial Systems Touch screen Monitor HIS ML2I eLo Driver v4.40. Coreco Bandit III CV and X64 CL Dual frame grabbers can be utilized to acquire data from the sensors. An Areca ARC 1220 PCI E to SATA II 8 port RAID controller can be used to form a RAID using eight Seagate 750 GB SATA II disk drives. A Garmin GPS 18 5 Hz USB puck antenna can provide access to the GPS signal input to the system server via an interface connection .

The programming language used to implement the system can be a mix of C and C compiled using Microsoft s Visual Studio 2005 Professional v8.0.50727.42 and Intel s C compiler v 10.0. Other software used includes NVIDIA CUDA v0.8 Perl v5.8.7 Trolltech Qt Commercial 4.3.1 Intel Performance Primitives v5.2 Coreco SaperaLT v5.30.00.0596 and Garmin nRoute v2.6.1. The underlying operating system platform for the NVESD GUI development can be Microsoft Windows XP Pro Version 2002 SP2.

An exemplary code written to implement such a system operates as follows. Such a user interface subsystem can be written using e.g. Qt GUI widgets. For example each display thread can begin with some typical initialization steps it then enters a loop. At the top of this loop such a thread sets a windows event to signal that it is ready to receive a frame of sensor data e.g. from or . It then waits on an appropriate frameAvailable event. This event is signaled by a callback after the callback moves a frame into a buffer visible to the display thread. When this event is signaled the display thread formats the frame for the appropriate display mode and then moves it to the appropriate Qt display surface. After this is complete a Qt display event is posted to an applications main Qt Event Loop. The display thread then repeats from the top of its main loop.

1 point at which takes pixel coordinates for a specific Context sensor or and returns the pointing angles azimuth and elevation of the Target e.g. as seen from the NFOV sensor e.g. as displayable in NFOV sensor system interface area and 2 point to pixel which takes the current pointing angles of the NFOV sensor see e.g. and translates it back to Context sensor or pixel coordinates for positioning of the reticle or other on image graphics in the context sensor WFOV interface area .

The exemplary GUI and Cal Server codes e.g. and were developed as separate applications. For performance the Cal Server application could be refactored into a separate thread within the GUI Application . For runtime operation with the current GUI System the aforementioned point at and point to pixel can be implemented and C C can be used to optimize the computations. point at and point to pixel can be viewed as inverse operations. For example point at can be viewed as taking a Context pixel location and returning the pointing direction to the Target e.g. from the NFOV sensor see e.g. . The second point to pixel can be viewed as taking the direction of the NFOV sensor see e.g. and returns the Context pixel location for positioning of the reticle or other on image graphics.

The Calibration Programming Interface CPI is described to exemplify a program implementation of a pointing algorithm. Such a calibration programming interface can be provided in addition to the GUI software system to provide a remote function call capability for calibration. Such an application programming interface API provides functions to allow applications e.g. a calibration application or Cal Server to perform the complete set of operations for system sensor calibration as exemplified in .

Step . Request a frame with possible metadata from a sensor. This step requests data from a given WFOV sensor e.g. or with optional meta data. The latest frame from a WFOV sensor e.g. or is returned with the timestamp frame data and meta data.

Step . Read gimbal location to get current pointing angle azimuth elevation . This step reads current gimbal position information e.g. azimuth and elevation with optional meta data. Status timestamp azimuth elevation in degrees and meta data are returned.

Step . Command the gimbal to point in a direction. This step requests a move of the gimbal to point the NFOV sensor head to the desired direction e.g. azimuth and elevation . Status timestamp and meta data at the end of the move are returned.

Such exemplary steps can provide a set of capabilities to allow an application to perform measurements to calibrate and to verify calibration for such a GUI System . With such controls for the cameras and the gimbal e.g. the functionality to perform such a pointing process may be implemented in the control application.

Such a client server interface approach as exemplified in can have certain advantages for the GUI system Referring to such a client server interaction allows for the possibility of multiple GUI Application hosts by supporting multiple computers and or multiple applications controlling the different sensors. Each instance need only run its own CPI server . It allows development and software for calibration to evolve independently from the GUI application . The more independent the component efforts are the simpler the management implementation and testing becomes. Further the calibration application can use a set of tools independent of the GUI System software scripting languages utilities . . . allowing for rapid prototype development. Once the application is developed it can be ported to C C to allow for maximum performance and tighter system integration.

Data from sensors and or can be obtained using e.g. Sapera frame grabber callbacks that acquire imagery from the sensors through acquisition cards and submit them to the user interface subsystem that displays the images to the operator. Data from such sensors can be set to be always acquired. Buttons e.g. and or on the GUI display can be used by the operator to signal the user interface subsystem as to which sensor s data to display. Simultaneously a gimbal control subsystem can constantly monitor the gimbal position by monitoring gimbal position signals from the NFOV sensor system . Operator actions such as touching the GUI touch screen to move the gimbal sensor head translate into messages being created by the user interface subsystem and sent to the gimbal control subsystem to reposition the gimbal sensor head

The gimbal control subsystem translates the x y image coordinates into angle space azimuth and elevation values as aforementioned. These values are then placed on the gimbal position control lines e.g. interfaced to the T connection to move the gimbal sensor head to the desired position. When the operator touches any of the camera control buttons e.g. such as focus e.g. or or zoom e.g. or the user interface subsystem places the appropriate signals on a server connection e.g. serial I O connections to the sensors e.g. .

A GPS subsystem constantly monitors GPS words obtained from GPS e.g. GPS 18 5 Hz from Garmin which may be raised through a GPS antenna to determine vehicle position. The GPS subsystem then sends a message with the vehicle location to the user interface subsystem for display to the operator. The GPS subsystem can also maintain the list of previous incidents. The GPS subsystem can send to the user interface subsystem a message indicating that the current vehicle position is within a threshold distance from a previous incident. When the user interface subsystem receives this message the display border can indicate a warning by changing the display border color to an appropriate color. The operator can control the entire GUI system through interactions with the user interface subsystem e.g. as described above.

With a Gyrocam system an electrical T connection or T box was provided between the GUI system server and the Gyrocam gimbal to integrate such an analog sensor system with a digital GUI system . The electrical T connection was implemented to integrate with a Gyrocam interface module and a Gyropad kneepad controller . For example the T connection was accomplished by severing various controller lines between the Gyrocam interface unit and the kneepad controller and inserting a T connection implemented with commercially available commercial modules capable of converting analog signals to digital and vice versa. It is not necessary to physically cut the wires themselves to implement the connection. Gyrocam cable connection between the Gyrocam Interface Module GIM and the Gyrocam Kneepad Controller GKC was disconnected at the GIM and reconnected to the T connection module .

Inside this module control signals were connected to analog to digital converter ADC and digital to analog converter DAC modules while other signals were passed through untouched. The T connection module was then connected through a cable to the GIM . Additional signal lines and Gyrocam gimbal pointing angles were connected from the GIM to the T connection module since these lines did not go to the GKC . Connectivity between the T connection module and a PC was accomplished using a USB connection.

Upon completion of the electrical T connection module software was written to read the values produced by the ADC to monitor various aspects of the sensor head including its current pointing angle in azimuth and elevation and gimbal head height. Software routines were also written to place analog signals on the control lines through the DAC module. These routines included sensor zoom and de zoom focus and de focus and sensor head azimuth and elevation position changes e.g. and . The software routines were then written to be initiated through the operator s interaction with the GUI touch screen display . Thus the analog Gyrocam gimbal system e.g. is integrated with and controlled by a digital based touch screen GUI system .

The GUI system can enhance the operator s ability to investigate potential threats while maintaining situational awareness while on the move or being stationary. It allows the operator to efficiently and effectively point a NFOV high magnification sensor to multiple positions for further investigation without becoming disoriented.

An algorithm for the touch screen pointing was implemented for the GUI System. The touch screen pointing functionality allows an operator to touch a point of interest in a context sensor WFOV interface area to slew the NFOV sensor system to point to that location e.g. for high magnification interrogation of the region of interest ROI .

A touch screen interface allows an operator to select points of interest in the Context sensor WFOV image . The NFOV sensor system is then commanded to point the sensor head to the target location for a NFOV sensor system view under high magnification.

The GUI system uses the relative geometry between the Context camera or and the NFOV sensor or together with the location of the Target e.g. in space direction and range from the context sensor to determine the relative bearing from the NFOV sensor or to the Target the pointing angle .

To calculate the pointing angle i.e. azimuth and elevation from the Zoom sensor e.g. to the Target geometrically we use the following input parameters 

1. Distance and Orientation of NFOV and Context sensors with respect to the vehicle e.g. GUI system . These parameters can be determined by measurement or a calibration process.

2. NVOF and Context sensor total fields of view FOV and focal plane dimensions used to convert between pixels coordinates x y and the pointing angles azimuth elevation relative to each sensor. Because the pointing direction is of interest for the NFOV sensor the FOV and focal plane information are not needed for the NFOV sensor . 3. Range from the context sensor or to Target. For example a Flat Earth approximation was used to obtain the range. Errors in the range estimate can result in the pointing direction errors and the Target may be off center or even out of the FOV of the NFOV sensor.

1. Read the pixel coordinates x y of the target in a Context image from touch screen touch location or mouse click coordinates.

2. Use the context sensor geometry and optical parameters to calculate the direction vector from the context sensor location C to the target. For example use the Context FOV to convert the context image pixel coordinates x y to coordinates az el of target from the Context sensor or . 3. Compute the 3D position T where that direction vector through target pixel intersects a ground plane. For example as shown in project a vector from the context sensor location C in the az el direction to calculate the intercept with a flat ground plane E at a point T i.e. the Flat Earth target position estimate . This is the flat earth approximation. 4. Calculate the vector GT from the NFOV sensor position G to the estimated point T of target. Normalize the vector GT to unit length and determine the corresponding direction angles by which to point the NFOV sensor gimbal to the target location. For example calculate the azimuth and elevation az el values corresponding to the unit vector pointing from the NFOV sensor location G to T by normalizing T G to a unit length and determine the corresponding direction angles by which to point the NFOV sensor head e.g. to the target location in terms of azimuth and elevation. 5. Use the derived azimuth and elevation az el values as a basis for pointing the NFOV sensor head at T.

For this example the inputs are the x y pixel of target in a context image . The output is the azimuth and the elevation information calculated for the NFOV sensor head to the Target. The pointing errors from this approach can be related to the range difference between T and the target. Further when the target is disposed above the horizon as seen from the Context sensor the viewing ray from the Context sensor center through the target pixel location may not intersect the ground plane E. Accordingly a heuristic approach can be used to determine the range from the Context sensor to the target Rif the target is disposed above the horizon.

The full coordinate location T is calculated based on the vehicle coordinate system as follows sin cos square root over Given the coordinates of the NFOV sensor in vehicle coordinates 0 h 0 where hrelates to the mast height for NFOV sensor and the target location in vehicle coordinates X 0 Z we can now solve for the NFOV sensor declination and azimuth angles as follows 

Based on the system evaluation and testing the current GUI system WFOV imagery allows operators to better focus on important regions of interest ROIs without getting sucked into the soda straw of the NFOV sensors e.g. . Operators always have a live feed of what is approaching and an indication of where the current NFOV sensor view is within the WFOV . The automatic pointing of the NFOV sensor head to ROIs e.g. from the WFOV sensor or allows for much simpler investigation of candidate ROIs. The automatic slew to the desired azimuth and elevation together with the stabilization of the NFOV sensor head means that the operator can immediately investigate potential targets e.g. fine tune the location manually with the touch interface to the NFOV image window and switch focus of attention between the WFOV context view and the current NFOV zoomed view . The stabilization means that the ROIs e.g. will maintain location while the operator attention is on the context information. Further the ability to toggle between the alternative WFOV sensors for example between the XMV 2020C and the DVE based on a touch selection allows the operator to switch to the sensor view that offers the most information for the scene and viewing conditions. The GUI then allows for a quick change to the corresponding NFOV sensor e.g. or and zoom to best view to interrogate the potential target ROI e.g. .

The GUI system has been developed to support the use of varying sets of sensors for both the WFOV sensors e.g. and and the high magnification NFOV sensors e.g. . The software is designed to allow additional sensors to be added in a symmetric fashion to the existing ones. For example it is a simple configuration change to go between the XMV 2020C sensor and an alternate 3 CCD color sensor for the context color camera . Likewise the hardware e.g. makes use of the Camera Link Interface standard to allow a common video acquisition card be used across multiple different imaging sensors. This card could include a library of known target items against which the information obtained from the sensors can be compared. For example if the sensors capture the image of a pipe bomb the GUI could be designed to flash the word pipe bomb. Finally the GUI system can constantly receive data from all the sensors. This allows for the recording of image data and meta data GPS position time vehicle attitude gimbal pose for data collection purposes post processing for algorithm studies or after action analysis.

Field tests have shown improved operator performance in terms of range to object and search times. For example an exemplary vehicle mounted GUI system included a WFOV HDTV format visible color camera used for context imaging high magnification NFOV Mid wave Infrared and visible color 640 480 format cameras on a mast with a pan tilt head electronic stabilization for NFOV imaging and a touch screen based graphical user interface .

In such vehicle mounted field tests an operator pointed the pan tilt head by e.g. touching the WFOV image . When touching the WFOV image the pixel x y location at the point in the image being touched is translated from pixel x y coordinates into angle space in order to issue commands to move the pan tilt head to a particular azimuth and elevation. The conversion from image coordinates to angle space is done through a flat earth approximation. The testing showed detection ranges more than doubling from an average of 20 m out to an average of 55 m. Vehicle speeds in the testing were 5 10 and 20 MPH.

The invention has been described in an illustrative manner. It is to be understood that the terminology which has been used is intended to be in the nature of words of description rather than limitation. Many modifications and variations of the invention are possible in light of the above teachings. Therefore within the scope of the appended claims the invention may be practiced other than as specifically described.

