---

title: Layered architecture for hybrid controller
abstract: Approaches for implementing a controller for a hybrid memory that includes a main memory and a cache for the main memory are discussed. The controller comprises a hierarchy of abstraction layers, wherein each abstraction layer is configured to provide at least one component of a cache management structure. Each pair of abstraction layers utilizes processors communicating through an application programming interface (API). The controller is configured to receive incoming memory access requests from a host processor and to manage outgoing memory access requests routed to the cache using the plurality of abstraction layers.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09529724&OS=09529724&RS=09529724
owner: SEAGATE TECHNOLOGY LLC
number: 09529724
owner_city: Cupertino
owner_country: US
publication_date: 20120706
---
Embodiments described herein involve a controller for a hybrid memory that includes a main memory and a cache for the main memory. The controller comprises a hierarchy of abstraction layers wherein each abstraction layer is configured to provide at least one component of a cache management structure. Each pair of abstraction layers utilizes processors communicating through an application programming interface API . The controller is configured to receive incoming memory access requests from a host processor and to manage outgoing memory access requests routed to the cache using the plurality of abstraction layers.

These and other features and aspects of the various embodiments disclosed herein can be understood in view of the following detailed discussion and the accompanying drawings.

Some memory devices use at least two types of memory in a hybrid or tiered memory system where at least one type of memory is used as a main memory and at least one other type of memory is used as a secondary. The main memory may have greater storage capacity but slower access times than the secondary memory for example. In such cases the secondary memory may serve as a cache for data accesses to and from the main memory. One example of such a tiered memory device is a hybrid drive in which the main memory may comprise nonvolatile memory such as magnetic disk magnetic tape and or optical disk and the cache may also be memory such as solid state flash memory. In some implementations a first cache and a second cache may be used wherein the second cache is flash memory and the first cache comprises memory e.g. dynamic random access memory DRAM . Note that the terms main primary secondary first and second are used herein to denote differences in memory e.g. usage capacity performance memory class or type etc. and not necessarily order or preference. Furthermore although many examples provided herein refer to the main memory as magnetic disk and to the cache as flash and or DRAM or to some types of memory as volatile and others as nonvolatile these designations are used to facilitate explanation and the disclosed approaches are applicable to any types of memory.

Embodiments described herein involve layered architectures for hybrid memory controllers. The layered architectures include a hierarchy of abstraction layers wherein each abstraction layer configured to provide at least one component of a cache management structure. Each pair of abstraction layers is implemented using processors at least one processor per layer that communicate through an application programming interface API . The hybrid controller is configured to receive incoming memory access requests from a host processor and to manage various types of memory access requests routed through the hierarchy abstraction layers to the cache.

The term abstraction layer in the context of various embodiments denotes a module configured to do some portion of the work involved in managing the hybrid memory cache. Hierarchy of abstraction layers refers to a plurality of abstraction layers arranged from a higher level that communicates with a host processor to a lower level that communicates with a flash memory. Each pair of layers in the abstraction layer hierarchy is communicatively coupled using an application programming interface that comprises a set of requests that can be issued from one layer to the next layer in the hierarchy. A layer receives requests from the layer above it in the hierarchy and issues requests to the layer below it in the hierarchy.

The host sends memory access requests to the hybrid drive to read or write data. The memory access requests may specify a host LBA range used for the operation of the memory access request. For example a memory access request from the host may request that a host LBA range be written to the hybrid drive and or a memory access request may request that a host LBA range be read from the hybrid drive . The memory access requests are managed by the hybrid controller to cause data to be written to and or read from the hybrid drive with optimal efficiency. The second cache in this example may optionally be read only in that only data marked for read operations by the host are placed in the second cache . In such a configuration data marked for writing are sent directly to the main storage either directly or via the first cache .

According to some embodiments the hybrid drive may be implemented as a hierarchy of abstraction layers . Pairs of the abstraction layers are communicatively coupled through application programming interfaces APIs . The organization of the hybrid controller into abstraction layers to some extent allows each layer to work relatively independently and or can reduce potential conflicts that arise from processing multiple threads of execution. For purposes of discussion some examples provided below are based on the use of a magnetic disk as the main memory dynamic random access memory as the first or primary cache and solid state flash memory as the second or secondary cache. It will be apparent to those skilled in the art that the various memory components are not restricted to these types of memory and may be implemented using a wide variety of memory types.

In some configurations the cache may be configured as a secondary cache being faster and smaller than the main storage . The cache can be a primary cache being faster and smaller than the secondary cache which may or may not be nonvolatile. Generally the terms primary and secondary or first and second refer generally to hierarchy of time and or priority relative to commands received via the host interface . For example current read write requests from the host may be processed first via the primary cache e.g. identified by the data s logical block address . This enables host commands to complete quickly should the requested data be stored in the primary cache . If there is a miss in the primary cache the requested data may be searched for in the secondary cache . If not found in either requested data may be processed via the main storage .

Some of the data stored in the primary cache may either be copied or moved to the secondary cache as new requests come in. The copying movement from primary cache to secondary cache may also occur in response to other events e.g. a background scan. Both copying and moving involve placing a copy of data associated with an LBA range in the secondary cache and moving may further involve freeing up some the LBA range in the primary cache for other uses e.g. storing newly cached data.

The host processor communicates with the hybrid memory device also referred to herein as hybrid drive through a host interface . The host interface is responsible for managing transfer of data from the first cache to the host processor As previously discussed the main memory includes a memory space that corresponds to a number of memory sectors each sector addressable using a unique a logical block address LBA . The sectors of the main memory are directly accessible by the host using the LBAs and thus the corresponding LBAs of the main memory are referred to herein as host LBAs.

The host sends memory access requests to the hybrid drive for example the host may request that data be written to and or read from the hybrid memory device. The host interface is configured to transfer memory access requests from the host to the hybrid memory device and to transfer data between the host and the hybrid memory device.

The hybrid controller illustrated in includes hierarchy abstraction layers wherein each layer communicates to its nearest neighboring layer s e.g. through an application programming interface API . For example each layer may only communicate to its nearest neighboring layer s without communicating to other layers. As an example the layer may only communicate directly to layer and layer without communicating directly with the layer or to the host interface . As an operation such as a memory access request from the host is being carried out each layer is configured to pass control to the next lower layer as the operation is implemented.

The example illustrated in includes four abstraction layers which are described in terms applicable to the use of flash memory as a cache. It will be appreciated that these terms are not restrictive and if other types of memory were used as the secondary memory if desired different terminology could be used to reflect the type of secondary memory. Nevertheless the basic functions of the layers are similar regardless of the type of memory used for primary and or secondary memory and or the terminology used to describe the layers.

The layers illustrated in include the flash cache interface FCI layer the flash cache control and transfer management FCTM layer the solid state drive SSD layer and the programmable state machine PSM layer . Requests may be passed using APIs represented by arrows from a higher layer to the next lower layer for example requests from the FCI layer are sent to FCTM layer requests from the FCTM layer are sent to the SSD layer and requests from the SSD layer are sent to the PSM layer which interacts directly with the flash memory . The layered architecture of the hybrid controller described herein allows for handling host memory access requests which can be serviced from either the magnetic memory or one of the caches The layered structure used in conjunction with the flash cache can be configured to achieve specified rates and response times for servicing memory access requests.

The FCI layer implements processes to determine which data should be promoted to the flash cache and or the DRAM cache based on various criteria to achieve optimal workload for the hybrid drive. The FCI layer decides whether a host read request should be serviced from the primary memory or from one of the caches . If the required read data exists entirely in the first cache the first cache is used as the source for the transfer. If the required data exists entirely in the second cache the second cache is used as the source for the transfer. In order to determine whether a host requested LBA range is fully present in the flash cache a lookup request is issued to the lower layers. All other cases use the main store as the source for a transfer.

In response to write requests the FCI is responsible for generating invalidate requests to the lower layers to indicate that the cached data is no longer current. In response to flash cache misses the FCI determines whether the data retrieved from the main store into the first cache needs to be promoted to the flash cache . In order to make this decision a hotness tracking algorithm e.g. using a Bloom Filter may be used. If data is determined to be hot a promotion request is issued to the lower layers. The FCI layer may implement read look ahead and or read on arrival optimizations that cause additional data to be fetched into the DRAM . The FCI includes this data into the promotion request to the second cache to enhance the likelihood of a cache hit in the second cache due to locality in most typical workloads.

The flash content and transfer management FCTM layer maintains a mapping e.g. a fully associative mapping as discussed below of the host LBAs to a memory space corresponding to the flash memory space arranged as LBAs which are referred to as solid state drive SSD LBAs. The FCTM layer maintains a mapping of host LBAs to SSD LBAs using a hash table. The mapping is maintained in units of clusters. Host and SSD LBA spaces may be partitioned into equal sized clusters. In some implementations a fully associative cache is maintained. In response to a lookup request hash table operations are performed to determine whether the requested host LBA range is completely present in the flash cache . In response to read promotion and or invalidate requests the host LBAs are translated into SSD LBAs and corresponding requests are issued to the SSD layer. FCTM invalidates generate unmap requests which may also be denoted trim requests to the SSD layer. Requests are prioritized to reduce host response times by giving reads higher priority than writes promotes . Overlap checks are performed to ensure data integrity due to out of order processing caused by the prioritization requests.

The SSD layer can abstract the flash as an SSD device. The SSD layer interacts with programmable state machine PSM layer and performs tasks such as optimal scheduling of promotion requests among dies of the flash referred to as die scheduling wear leveling garbage collection and so forth. The SSD layer maps the SSD LBAs of the FCTM layer to physical flash locations die block and page locations .

The PSM layer accepts requests such as to read program and erase flash locations. The PSM layer programs hardware controllers to generate the required signals to read from and write to the flash for example.

Note that in some embodiments as indicated the broken lies at elements and the functions of the FCTM layer and the SSD layer may be combined into one layer in which case the communication protocol would be unnecessary. The combined abstraction layer would include all the functions of the FCTM and the SSD layers.

In some cases one or more of the layers of the hybrid controller may be implemented by circuitry and or by one or more processors e.g. such as reduced instruction set computer RISC processors available from ARM. In some cases each layer may be implemented by a separate processor. The processes discussed herein are implementable in hardware interconnected electronic components that carry out logic operations and or by a processor implementing software instructions and or by any combination of hardware and software.

Components are arranged to implement functions that facilitate movement of data in and out of the caches based on certain metrics e.g. importance metrics and or other situations. Component is arranged to implement functions that facilitate movement of data in and out of the caches based on tracking the history of read operations. Component is arranged to implement functions that facilitate movement of data in and out of the caches based on speculative read operations. Component is arranged to implement functions that facilitate movement of data in and out of the caches based on certain criteria.

Component includes a history tracking module that tracks host operations affecting the data storage device such as host read requests that are received over some period of time. An analysis module is configured to determine one or more criteria associated with the host operations. The criteria may be at least indicative of future read requests of certain logical block addresses such as data that is not yet requested but has a likelihood of being requested in the future. The analysis module may be coupled to the history tracking module to obtain this information. A caching module is configured to cause data from the main storage to be copied to the secondary cache if a particular criterion meets a threshold. The caching module may be coupled to at least the analysis module to obtain the criterion other components of the FCI layer FCTM layer and or host interface to cause these transfers of data to the cache . Any combination of the history data criteria and current thresholds can be stored in a database that is coupled to any of the modules as well as being coupled to other components of the FCI layer .

One goal of the secondary cache design is to minimize access to the main storage for read operations that fall within particular access patterns. These patterns may identify some amount of data that is likely to be requested in the future and which can be move to the secondary cache before it is requested. For example some host data operations such as the reading of a commonly used file may involve predictable data access patterns e.g. sequential data reads over contiguous address ranges repeated access to a particular range of addresses and so may benefit from secondary caching. Other operations such as benchmarking tests or updating random pages of virtual memory may not benefit from secondary caching. For example the overhead incurred in moving small blocks data in and out of the secondary cache may override any improvement in data transfer speed provided by the cache.

The embodiments described below may retrieve host requested data from a secondary cache to avoid overhead in retrieving data from the main storage media. The data may be selected for storage into the secondary cache based on among other things a likelihood of being hit on a read request after a miss in the primary cache. As such data in the secondary cache may be selected to avoid overlap with valid data on the primary cache. The data may also be selected so as to avoid data that can be predicted as conforming to a raw access pattern from the main store e.g. 100 sequential or random reads. A 100 sequential read may be serviced nearly as efficiently from the main store and or may be the type of data that has low likelihood of being re requested e.g. streaming media . A 100 random read may also have low likelihood of LBAs being requested again and so the overhead of caching large amounts of random LBAs may offset any benefit in the off chance a previously requested LBA is requested again.

The embodiments described herein have features that may be configured for use under enterprise workloads. Computing resources of enterprise servers may be heavily utilized. For example processors disks network interfaces etc. of an enterprise server may be used at steady relatively high activity levels for long periods of time. As this pertains to persistent data storage access it has been found that for enterprise workloads it may be better to cache read data than write data and may also be better to cache speculative read data rather than requested read data.

One technique that may be employed to efficiently utilize the secondary cache is to determine a cache importance metric to be used as a criterion for whether data should be placed in the secondary cache. This may include data moved from the primary cache to the secondary cache and data moved directly from main storage to the secondary cache. The cache importance metric may use any combination of the following considerations 1 frequency of recent host read requests for an LBA 2 spatial distribution of recent host read LBA counts 3 predicted disk access penalty for LBA cache miss and 4 write frequency.

The first consideration noted above frequency of requests indicates regions in memory experiencing hot activity e.g. recent repeated accesses . Hot read activity can by itself or in combination with other factors indicate LBAs that may be more likely to be read again. Similarly the second condition spatial distribution relates to how close in the LBA address space e.g. spatial locality recent requests are grouped. Under some conditions LBAs in or near recent active LBAs ranges may themselves be read in the future. The third condition disk access penalty relates to the characteristics of the architecture such as particulars of the main data store. For example if a frequently accessed range of LBAs is stored in physically diverse sectors on the main data store e.g. belonging to a highly fragmented file it may take longer to retrieve this than similarly accessed data that is stored in a contiguous range of physical sectors. Finally the fourth consideration write frequency can identify data that is less likely to benefit from being in the secondary cache. For example if the secondary cache only stores read data the importance metric of an LBA may be reduced based on write activity targeted to the LBA.

An example of determining at least the first and second considerations for particular LBA ranges is shown in the block diagram of . The block diagram illustrates aspects of what is referred to herein as a zone table . The zone table includes data structures e.g. table list map set array etc. usable to store a number of entries e.g. entries . Each of the entries is associated with a range of LBAs e.g. ranges respectively. The entries store indicators e.g. access operation counters that indicate recent operations affecting the associated LBA ranges .

In this example the entries accumulate counts of host read requests targeted to the associated LBA ranges . For example read request affects a range of nine addresses within range and so counter of entry is incremented by nine in response to the request being fulfilled. The read request may trigger incrementing the count even if the requested LBAs are already stored in the secondary cache. For example recent levels of activity on secondary cache data may be indicative that uncached neighboring LBAs e.g. in an adjacent address range might benefit from caching. Tracking continued activity for secondary cached ranges may also help determine whether those regions should remain in the cache. In the event some data needs to be ejected from the secondary cache it may be preferable to eject cached data that has exhibited relatively lower levels of recent activity.

The size of the ranges may be preconfigured before runtime or determined at runtime. The size of the ranges may be set based on characteristics of the primary and or secondary caches. For example if the secondary cache organizes cached content into predetermined sized lines it may be useful to match the size of the ranges to the cache line size s . In this way cached lines may be treated as a unit in the zone table being moved in and out of the secondary cache based on values of the counter entries .

Practical considerations may limit the choice of range sizes. For example if too fine of a granularity is chosen the zone table might grow to be too large for the available memory. This a trade off that may be considered when deciding whether to use a zone table or a counting Bloom filter implementation as discussed below. When using a zone table the zones may have relatively large granularity e.g. 1 GB . Counting Bloom filters tend to provide high granularity tracking using a relatively small amount of memory while the zone table may perform better with medium granularity tracking e.g. on the order of the capacity of the secondary cache .

The caching importance metrics may be based on individual counter entries and or combinations of the counter entries . For example activity that occurs on the boundary between two adjacent address ranges may not indicate significant activity in either of the ranges compared to activity as a whole. However if two adjacent ranges are considered together in view of other adjacent ranges e.g. sum of counters and compared to sum of counters this may be enough to raise the cache importance metric of the adjacent ranges together as a unit.

A number of the zone table entries with the highest counts e.g. hot range of LBAs may also be linked to a sorted list . The list is sorted by access counts so that hotness attributes can be determined quickly. A cache analysis module e.g. module shown in may be able to access this list to assign cache importance metrics to the address ranges possibly in combination with other data such as write request activity targeted to the ranges. The cache importance metric can trigger moving data from the ranges in or out of the secondary cache e.g. via caching module in if some threshold is met.

In response to some event the entire zone table may be decayed e.g. by dividing all the counts in half. This avoids having the counters saturate their maximum values over time. This may also cause more recent activity to have a greater influence in the sorted list . The event that triggers the decay may include any combination of the passage of time value of one more counter entries the sum of the counter entries etc. Assuming this decay is applied evenly this should not affect the sorted list because relative magnitude of the entries should remain the same right after the entries are decayed. Some aspects of the entries or list may be stored elsewhere e.g. database in before the entries are adjusted. This enables longer term historical activity to be tracked and analyzed.

The zone table enables determining hot zones e.g. those that are accessed more than most other zones. High spatial locality is detected by the presence of hot zones within particular ranges that are not cooling significantly due to recently activity. For example any of the ranges may exhibit brief bursts of high activity over time but those that sustain high activity over larger time spans or at least more recently may be more indicative of spatial locality of the recent activity. The restriction of the activity to a particular range or combination of ranges provides clues about spatially concentrated activity which may increase cache importance for those ranges.

The use of a table is only one example of how caching priority metrics may be determined. An alternate embodiment uses counting Bloom filters to track usage and is shown in the block diagram of which may be used instead of or in addition to the table mapping shown in . Generally a Bloom filter is a data structure used to determine whether an element is a member of a set. The conventional Bloom filter may utilize a bit array all elements of which are initialized to zero. Adding an element to the set involves applying k hash functions to the element which results in k bits of the array being set to one. Testing whether an element is a member of the set involves applying the same hash functions to the element and testing whether all the resulting k bits are already set to one. If at least one of the bits are not set to one then the tested element is not in the set. A Bloom filter will always predict correctly if an element is not in a set however there is a probability of a false positive i.e. predicting that an element is in the set when it has not been added. The odds of a false positive can be minimized to an acceptable level by selecting Bloom filter parameters such as the size of the bit array the number of hashing functions and the type of hashing functions.

In a conventional Bloom filter an element cannot be removed from the filter. This is because two or more elements may set the same bit to one and so changing a bit back to zero risks unintentionally removing other elements from the filter. To resolve this a counting Bloom filter CBF uses an array of counters instead of a bit array to store the hash results. Adding an element to a CBF involves incrementing the counters for the k hash results instead of changing a bit from zero to one. Checking whether an element is in the CBF set is similar to a conventional Bloom filter in that each element is checked for a non zero value for the k hash value results. Unlike a conventional Bloom filter an element can be removed in a CBF by decrementing the appropriate counters.

In a collection of time ordered CBFs are used to store keys related to LBAs of incoming read requests . The key may be an index of LBA ranges such as an index associated with each of the ranges shown in . The key may be dynamically calculated derived from a lookup etc. For example the key may be numeral ranging from value from zero to n 1 for n LBA ranges. It will be appreciated that multiple keys may be derived from a read request such as where a request spans a range or ranges covered by two or more keys.

When first initialized all CBFs start off empty and one is designated as the current CBF e.g. CBF in . The new key is added to the current CBF by incrementing each of the counters of the CBF according to the results of the hash values applied on the key . This incrementing occurs so long as none of the incremented counters in the CBF are at a maximum value. If so then the CBF may be move out of the current position as described below.

When the current CBF reaches a predefined fullness threshold which may include a counter being at maximum a total of the counters reaching a threshold etc. an empty CBF then becomes current. The fullness threshold may be defined as a maximum number of non zero counts e.g. 50 of CBF entries . Whenever the last remaining empty CBF is assigned as current the oldest non empty CBF is designated emptying e.g. CBF in . As each key is added by incrementing the current CBF the equivalent or greater number of unprocessed counters is cleared if not already clear in the emptying CBF .

A CBF is considered to be active if it is not empty and not emptying e.g. CBFs are active in this example. A key is considered to be a member of the host read history if at least one active CBF has a non zero value for all of its hashed counters. A key may also be used to query the active CBFs e.g. to see if an LBA or LBA range have exhibited recent activity.

As keys are added to the current CBF a distribution of hashed entry counts for that CBF may be updated. The hash entry distribution tracks the number of hashed entry counters vertical axis in the current CBF with a specific count value horizontal axis . The maximum value m on the horizontal axis may represent the maximum counter value or some other threshold beyond which the CBF may be retired e.g. 50 of maximum . The hash entry distribution may be used to determine when the percentage of non zero counts in the current CBF has exceeded the fullness threshold. Because the hash entry distribution tracks the number of current CBF counts with a value of zero value A the number of non zero counts is the total number of counts minus the number of zero counts A in the current CBF.

The hash entry distribution may also be used to estimate spatial locality of host reads. When spatial locality is low then the keys added to the host read history are not repeated very often and the corresponding hashed entries are evenly distributed. This may be expressed as a percentage TP of non zero count values that are less than a threshold LCV . Exact definitions of TP and LCV can be tuned to the specific application. For example low spatial locality may be defined if 95 of all non zero hashed counts in the current CBF are less than three TP 95 and LCV 3 .

The hash entry distribution can give a first order approximation of whether or not the most recent host reads have low spatial locality. An additional enhancement may be provided by looking not only at the current distribution but at a distribution history that allows spatial locality trends to be spotted. The distribution history may be updated as each new key is added to the current CBF and captures a partial snapshot of the hash entry distribution at those times. The distribution history need not track a complete distribution of all count values over time only those that are needed for the spatial locality computation. For example only the total non zero counts and the total non zero counts 

In a flowchart illustrates an example of how spatial locality may be tested using hash entry distribution history according to an example embodiment. An event triggers a test for low spatial locality e.g. indicative that there is no closeness between LBA values associated with recent host operations. The test can be triggered for example as a criterion for calculating a cache importance metric related to a range of LBAs. The cache importance metric may be evaluated on a regular schedule in response to host data requests in response to or preparation for a caching operation etc.

The test involves determining whether the current hash entry distribution indicates that TP non zero count values are 

The first determination is whether there been additions causing a net increase in the number of non zero entries. The second determination is whether those net increases been to count values 

The CBF arrangement shown in may also be used to determine recent levels of activity or hotness. Hotness lookups can be performed on all active CBFs . The corresponding hashed counter values from all active CBFs are summed to form the overall sequence of counter values for that key. For example shows a table with CBF results for a particular key according to an example embodiment. This example shows a row for each of three CBFs numbered CBF0 CBF2. Each column in the table represents an index to a Bloom filter counter of the three CBFs. Each index is generated for eight hash functions applied to the key . For example if the CBF has 128 counters each with an index from 0 127 the indices IDX0 IDX7 in table may include 8 27 49 55 102 112 115 120 each determined by performing the respective hash functions on the key .

The bottom row in table represents the sum of counters across each CBF for this key. In this example all CBFs 0 2 are given the same weight however in some implementations the most current active CBFs may be given a weight value that is higher than less recent active CBFs. In one variation if any hash counter value for a given key is zero then all counts for that CBF are set to zero. This is because a zero in any of the counters in rows 0 2 indicates the key has not been inserted into the CBF. For each key added the minimum of the summed count values from all active CBFs is defined as the estimated repeat count ERC for that key. As keys are added the ERC over all keys is tracked. In this example ERC 4 for key because that is the minimum count sum across all CBFs in the table lowest value of the SUM row in table . An alternate procedure for obtaining ERC may involve summing just the minimum value in each row of table . This alternate approach would yield ERC 3 for key because the minimum value for each row is one.

The ERC derived as above may be used to determine read hotness. For example three levels of hotness may be defined 1. Cold not present in CBFs 2. Tepid present in CBFs but infrequently requested and 3. Hot present in CBFs and frequently requested . Any combination of absolute and relative metrics may be used to determine this metric. For example a key may be cold when ERC 0 and hot when ERC ERC ERCbeing a predefined threshold that is tuned for the implementation. The key is tepid if 0 ERC ERC. In another example the key may be considered hot when it falls within the top Nth percentile of non zero hash entry values up to the maximum ERC. Again N is tuned for a particular implementation and the key is tepid if above zero but below this percentile.

In reference now to a flowchart illustrates a procedure according to an example embodiment. The procedure involves tracking host read operations affecting a first logical block address of a data storage device. The data storage device has a main storage and a secondary cache that mirrors a portion of data of the main storage. One or more criteria associated with the host read operations are determined .

The criteria determined at is indicative of future read requests of second logical block address associated with the first logical block address. This criteria may be used to cache the second address in a secondary cache if the criteria meets a threshold. The criteria may be indicative of a recent proximity in time of the host read operations and or a closeness of address values of the host read operations. In one arrangement the secondary cache stores read only data. In such a case an importance of the criteria is reduced based on write activity associated with the first logical block address.

In another example the procedure may optionally involve maintaining a plurality of data structures storing indicators. Each of the data structures may store indicators of one or more of A recent operations affecting LBAs associated with the structures e.g. zone table as shown in and or B spatial locality of LBAs within a time range associated with the data structures e.g. CBFs shown in . The indicator s determined at may optionally be used to determine the criteria associated with the first logical block address. Finally data of at least the second logical block address from the main storage is copied to the secondary cache if the criteria meets a threshold.

Referring back to component includes an analysis module that can be arranged to track history of address ranges and may assign an importance metric to the address ranges that influence whether the ranges should be targeted for the primary cache and or secondary cache . Examples of determining cache importance metrics have been described above. The importance metrics may also be used to determine the importance of both newly requested data and currently cached blocks of data. Metrics of cached data can be compared to the same metric applied to a block of unrequested speculative read data that is being considered for caching. If the importance metric of the speculative data meets or exceeds that of the cached data the speculative data can be loaded from the main storage and cached which may cause eviction of the currently cached data from at least one of the caches .

A speculative read module may define ranges of speculative data to be pulled from the main storage for purposes of fulfilling speculative e.g. predicted or anticipated host read requests. For example it may be more efficient to place speculative data in the secondary cache that fills out an entire line of the cache even if not all data placed in the cache has been requested. Due to certain conditions e.g. a latency incurred in later retrieving the data it may be beneficial to speculatively read data proximate to host requested data on the disk. The speculative read data might otherwise be passed over when reading the host requested data and then have to be re loaded from the main storage if later requested resulting in seek delays and other latencies. The speculative data may include data that is before the requested block or after the host requested data. The speculative data may be placed in one or both caches when fulfilling the immediate request in the anticipation that the unrequested speculative data may be the subject of a later request.

The speculative read module may select LBA ranges of speculative data based upon available rotational latency. In cooperation with the analysis module the LBA ranges may also be selected based on any combination of overall spatial locality of the read workload the specific cache importance of the host requested and or neighboring LBAs and the relative cache importance of any LBAs that might be evicted from the secondary cache should data be selected for caching. If the LBA ranges meet some combination of these criteria a range of data is moved into at least one of the caches from the main storage .

A caching module is configured to cause data from the main storage to be copied to the primary cache and or secondary cache . The caching module may be coupled to at least the speculative read module to determine ranges of data being cached to determine priority of data during transfer from the main storage to the cache. In conventional arrangements reading of speculative data may be interrupted to fulfill another host request. In contrast the caching module of this arrangement may be instructed not to interrupt reading of identified ranges of speculative data in response to another host request at least until a minimum LBA range has been read and cached. Speculative read metadata such as the identification of a speculative range addresses and a minimum range size may be provided to the caching module from the speculative read module . The caching module may also be coupled to the main storage other components of the FCI layer FCTM layer host interface to cause transfers of data to the caches . Any combination of the criteria and speculative read metadata can be stored in a database that is coupled to any of the modules

In reference now to a block diagram illustrates speculative read data according to example embodiments. Data represents a contiguous range of LBAs at least part of which is the subject of a read request. The requested data may be associated with a host read request and or may also be associated with other requests. The requested data may be retrieved from a main storage for placement into a primary e.g. volatile cache with the intention that the data may also be sent to a secondary e.g. non volatile cache.

In order to improve system efficiency when retrieving the requested data a range of LBAs is defined that encompasses the requested data as well as data that are not requested. This range is referred to herein as a speculative read LBA range. When reading data requested data one or both of additional ranges are identified for speculative reading. Range includes addresses located before the requested LBAs of request and so is referred to as read look behind data. Range includes addresses located after the requested LBAs of request and so is referred to as read look ahead data. The relative order of LBAs in is indicated by line and this may also correspond to relative orders of physical addresses e.g. disk sectors. While range includes both read look behind data and read look ahead data the concepts may apply to one or the other in combination with request data as shown in speculative read ranges and .

The attempt to read the full extent of range may be optional e.g. the reading of at least speculative ranges may be interrupted truncated if a new read write request is received. Alternatively the range may be defined as a minimum range that is not interruptible in until at least the LBAs within range are read. A hard drive controller may be able to adjust seek delays in order to obtain speculative data without incurring significant penalties. For example if the drive is currently reading speculative data and a new request is received the device can calculate a delay time that allows the head to seek to the new position just in time to access the new track. This seek delay may allow reading additional read ahead data without introducing additional latency in accessing the new track. In the present embodiments this delay may be extended long enough to fill up the speculative range. If the request to read range is defined as non interruptible the entire range will be read even if the additional delay introduces some additional latency in processing new requests. In either case whatever amount of the data is obtained is placed in one or more of the caches. In the arrangement shown in the data would be placed in the primary cache .

Even though the obtained data of range may be placed in a primary cache the extents of range e.g. starting LBA size of data may be defined based on characteristics of the secondary cache. For example the size of range may correspond to a cache line size of the secondary cache or multiples thereof. Similarly a start LBA of the range may be evenly divisible by the cache line size. Where the secondary cache is implemented as an SSD the cache lines may be defined as a predetermined number of pages of the SSD. In such a case secondary cache line sizes may ultimately be based on page sizes of the SSD.

The range may also be defined based on characteristics of the main storage. For example on magnetic disk drives a commonly used minimum physically addressable unit of data is the sector. Therefore the range may also be defined to consider sectors sizes as well as or instead of mapping to whole lines of secondary cache. For example a maximum speculative read latency for the system may be defined so that a maximum number of read behind or read ahead sectors e.g. the number of sectors in one disk rotation can be speculatively read without substantially impacting average seek delay times. As a result the data selected for storage as shown in the range may also depend on particular physical characteristics of the main storage.

It will be appreciated that a storage device may already employ some form of read look ahead and read look behind capability. In such a case however the decisions which drive selection and retrieval of speculative data may not take into account the characteristics of a cache particularly a secondary cache. An example of how a speculative read algorithm may be tailored for a secondary cache is shown in . The blocks in represent sectors within tracks of a hard drive magnetic media. The read head e.g. contained in a hard drive slider reads in a direction indicated by arrow . Because the slider is usually stationary within a track relative to the media the media would be moving in a direction opposite to arrow .

In this example the read head is located on track to read requested sectors that are indicated by shaded region A. The next read request is indicated by shaded blocks A of track . In order to fulfill this next request the read head will need to seek from track to and for purposes of clarity in the drawings it will be assumed that the seek will occur in the space of one block. How and when the sensor seeks from track to track after reading requested data A may vary depending on priorities used by speculative read algorithms as well as other features that may consume rotational latency.

In some hard drives for example the algorithms for speculative reading may favor read ahead data over read behind data. In such a system data following A would be read up until the seek departure time indicated by point where it would then be able to seek to point in time to read data at A. Thus the read ahead data D following request A would take precedence over read behind data D before A.

Consider a similar scenario as above but with an algorithm that favors filling out secondary cache lines without regards to whether the data is read behind or read ahead. In that case assume the size of regions C and C shown in corresponds to a secondary cache line size of the system and that the beginning of block B has an LBA suitable as a first LBA of a cache line should the secondary cache have such restrictions. Also region B has been already been read in ahead of region A. As a result the read head only needs to read to point to fill out a cache line and reading up to point will not provide enough read ahead data to fill out another cache line. As a result the system may choose to use an earlier seek departure time at point and be able to obtain read behind speculative data starting at point . This may provide a higher probability that enough speculative data proximate request A can be used to fill out a secondary cache line such as where the reading of speculative data may be interrupted by another request.

For example assume that another request will require the read head to seek away from track at point . If the drive had been configured to optimize read ahead data so that reading of data A had begun at point instead of point then there would not be enough data between points and to fill out a secondary cache line. Maximizing read behind data D minimizes the amount of read ahead data B needed to fill out the cache line which will lower the probability that another request could interrupt the read ahead data B.

The read behind or read ahead operations shown in may be triggered by the cache importance criteria at or near the requested LBA range or an overall assessment of spatial locality. The desired amounts of speculative read may be specified in order to align backward to the nearest boundary corresponding to the secondary cache line size. Even if the speculative data is first placed into a primary cache this selection of particular speculative ranges can improve hit rates of the secondary cache. This is because LBA ranges that fill an entire line are the first or only ones to be promoted to secondary cache. This may place more emphasis on read behind data than in current storage devices. This is because the endpoint of read ahead segments may be opportunistic e.g. more subject to interruption than read behind in cases where the read ahead is not specified as non interruptible . Also the secondary cache policy may truncate the read ahead ranges at the cache line boundary when being copied to the secondary cache thus causing some read ahead data to be discarded anyway.

In reference now to a block diagram illustrates an example of filling a secondary cache with speculative data according to an example embodiment. A block of data is retrieved from a main storage e.g. in response to an initial host read request for data that has not previously been cached. From the main storage data flows to a primary e.g. volatile cache and then to a secondary e.g. non volatile cache . Subsequent read requests for the data will first attempt to find the data in the primary cache and then the secondary cache. If the request misses in both caches the data are retrieved from the main storage .

When reading a block of data from the main storage the storage device may perform speculative reading e.g. it may read non requested user data blocks following or preceding the requested data and place the data blocks in a buffer in anticipation of a future read request.

Using criteria described in greater detail below a storage device may mark one or both of blocks as speculative ranges. Speculative ranges may have priority for retrieval from main storage e.g. another read write request will not interrupt loading of the speculative data so that one or both blocks get loaded into one or both of the caches together with the requested data . The data may be first loaded into the primary cache then later moved to the secondary cache . Alternatively the data may be moved into both caches at the same time or directly into the secondary cache without first being placed in the primary cache .

The total size of one or both speculative LBAs and requested data may be selected to make caching operations more efficient. In the illustrated examples the total size of blocks is selected to be the same as cache lines e.g. line of the secondary cache . As such when the blocks are determined to be speculative candidates the blocks are loaded into the secondary cache either directly or via the primary cache . It will be appreciated that alternate configurations may be tuned for cache line sizes of the primary cache . In such a case the speculative candidate blocks are loaded into the primary cache and selection of address ranges for movement to the secondary cache may depend on other factors rather than secondary cache line size.

A device according to the embodiments described herein may extend the speculative reading algorithm of a conventional HDD by one or more speculative read operations. The speculative read operations be based on host read history locality determination and target speculative ranges may be selected to align with secondary cache line size and starting address conventions. The speculative feature may ensure that read data is provided in the primary cache so that the secondary cache can efficiently utilize the data should it be a candidate for a fill operation.

In some embodiments a speculative read operation may attempt to read a specified minimum number of LBAs beyond the requested range in LBA space. In reference now to a flowchart illustrates a more detailed example of how speculative LBAs may be selected. The procedure starts in response to some data request event such as a host read request for a target LBA range. A candidate speculative read ahead range is determined . This range is a number of LBAs following the host requested range that will be targeted for read ahead if certain criteria are met.

The speculative read ahead LBAs may be chosen to optimize overall performance for enterprise class workloads and may account for the secondary cache line size and filling speed as well as the disk data rate and potential access time penalties incurred by invoking speculative reads. After the candidate speculative range is determined a maximum cache importance of the candidate range is determined . This determination may involve deriving a cache importance metric as previously described above.

The cache importance metric may be found by looking at spatial locality of recent requests e.g. LBA ranges experiencing higher than average activity and levels of activity associated with those requests e.g. read hotness of the LBA ranges . The cache importance may be generic or specific to one of the primary or secondary cache. For example the primary cache may have differing sets of priorities relative to LBA ranges than the secondary cache e.g. due to the expected time that data is expected to be retained in the respective caches. In particular where data ranges are selected to comply with cache line characteristics of the secondary cache the cache metric importance may also be calculated with respect to the secondary cache as well.

The metric found at is compared to a threshold value at . If the metric meets the threshold reading of the candidate speculative LBA range is enabled . In some embodiments enabling the speculative read may also ensure retrieval of the speculative data is not aborted prior to reading the minimum range if a new read request is received. In other embodiments the enabling may just result in a best effort to read in the range but will allow pre emption of the speculative read for some requests so as to minimize latency in fulfilling the requests.

If comparison at determines the cache importance metric does not meet the threshold a speculative range of preceding LBAs is determined . In this example the read hotness of the preceding range is determined and compared to a threshold. As described elsewhere herein read hotness may refer to activity levels of recent read requests within the preceding LBAs. The spatial locality of the recent read requests may optionally be considered at e.g. high levels of activity proximate to preceding LBAs. If the read hotness of the preceding LBA range does not meet the threshold then speculative read is disabled for the range. This disabling may involve going to a fallback strategy for speculative reading e.g. maximizing read ahead data regardless of cache importance or size temporarily disabling speculative reading etc.

If the read hotness of the preceding LBA range meets the threshold then a determination is made regarding the cache importance metric CIM of LBAs that would be evicted from the secondary cache if the candidate preceding range was added to the cache. This metric CIMis compared to the cache importance metric of the candidate range CIM. This latter metric CIMmay have been previously determined at as read hotness may be one component of the cache importance metric. If it is determined at that CIM CIM then speculative reading is disabled for the preceding LBA range otherwise it is enabled . It should be noted that determination will always return no false if no data would be evicted from the secondary cache in response to adding the speculative data.

It should be noted that the determination at may be performed even if the speculative data is not intended to be placed directly in the secondary e.g. non volatile cache and evict the cached data. For example the speculative data may be placed in the primary e.g. volatile cache. This or another event may trigger evicting data from the primary cache and a determination is made as to what should be evicted. The evicted data may either be moved to the secondary cache causing the above noted secondary cache eviction or deleted from the primary cache. The latter situation may occur for example where low priority read write cache data is considered for eviction from the primary cache. If the secondary cache is used for read only data then the read write cache data would not be targeted for secondary caching and would just be cleared from the primary cache.

In reference now to a flowchart illustrates a procedure according to an example embodiment. A host read request affecting a request address range e.g. LBA range of a main storage is received . In response to the read request a speculative address range proximate to the request address range is defined . Speculative data stored in the speculative address range is not requested via the host read request. The speculative address range may be selected based on a cache line size of at least one of the secondary cache and the primary cache.

A criterion indicative of future read requests of associated with the speculative data is determined . For example the speculative address range may located before and or after the request address range and the criterion may include a cache importance metric of the secondary cache regarding the speculative data. In either of these cases the criteria may further include comparing a cache importance metric of the speculative data to a cache importance metric of cached data that would be evicted from the secondary cache in response the copying of the speculative data from the main storage to at least one of the secondary cache and the primary cache.

The speculative data is copied from the main storage to at least one of a secondary cache and a primary cache together with data of the host read request in response to the criteria meeting a threshold. The secondary cache and the primary cache mirror respective portions of the main storage. The copying of the speculative data may involve not interrupting the copy operation in response to another host request being received at least until the speculative address range has been copied.

The main storage may include a magnetic disk drive. In such a case copying the speculative data from the main storage to the secondary cache may involve adjusting a seek departure time of the magnetic disk drive to read the speculative data. For example to read speculative data addresses before the host requested data the seek may occur early e.g. interrupting truncating an in progress low priority operation such as other speculative reads so that the preceding data can be retrieved. If the speculative data addresses are located after the host requested data a seek for subsequent requests can be delayed until the speculative address range is copied to one or more caches.

In reference now to a flowchart illustrates a procedure according to another example embodiment. A host read request is received affecting a request address range of a main storage. A speculative address range proximate to the request address range is defined . Speculative data stored in the speculative address range is not requested via the host read request. The speculative address range may be selected based on at least one of a size and an allowable starting address of cache lines of the secondary cache. The speculative address range may include a first range preceding the request address range and a second range following the request address range.

A cache importance metric of the speculative range is determined . The metric is relative to a secondary cache that mirrors a second portion of the main storage. The speculative data is copied from the main storage to a primary cache in response to the criterion meeting a threshold the primary cache mirroring a first portion of the main storage.

Referring back to the analysis controller module of component may track history of address ranges and may assign an importance metric to the address ranges that influence whether the ranges should be targeted for the primary cache and or secondary cache .

The importance metrics may also be used to determine the importance of both newly requested data and currently cached blocks of data. Metrics of cached data can be compared to the same metric applied to a block of unrequested speculative read data that is being considered for caching. If the importance metric of the speculative data meets or exceeds that of the cached data the speculative data can be loaded from the main storage and cached which may cause eviction of the currently cached data from at least one of the caches or movement therebetween.

The analysis controller module may also define optimum ranges of speculative data to be pulled from the main storage into at least one of the caches for purposes of fulfilling host read requests. Due to certain conditions e.g. a latency incurred in later retrieving the data it may be beneficial to speculatively read data that has not been requested but is proximate to host requested data on the disk. Examples of how speculative cache ranges may be determined are described in the Cache Range Selection reference which has been incorporated by reference.

The size of the speculative range may depend on characteristics of the caches and expected use conditions of the storage apparatus. For example it may be more efficient to place speculative data in the caches that fills out an entire line of the secondary cache . The speculative data may include data that is before the requested block read look behind or after the host requested data read look ahead . The speculative data may be placed in one or both caches when fulfilling the immediate request in the anticipation that the unrequested speculative data may be the subject of a later request.

One or more cache monitors may track operations affecting both the primary and secondary caches and cause cache controller module discussed further below to perform certain actions to cached data based on system policies. Generally the cache monitor may track conditions used by the cache controller module to evict data from one of the caches or move data between caches . Where the caches are configured to be independent e.g. not storing the same mirrored data of the main storage then moving data may involve copying data associated with an LBA range to the one cache and removing data of the LBA range from the other cache. There may be a single cache monitor that monitors both primary and secondary caches or the functionality may be separately provided individual monitors associated with each of the caches .

The cache controller module is configured to cause data from the main storage to be copied to the cache and or cache . The cache controller may control both primary and secondary caches or the functionality may be separately provided by individual controller modules associated with each cache . The cache controller module may be coupled to FCTM layer to cause transfers of data to the cache . Any combination of the criteria and speculative read metadata can be stored in a database . The database may be coupled to any of the modules

In reference now to a block diagram illustrates an example of cache monitoring and control using various modules shown in . A host command processor processes commands received via a host interface e.g. host interface in . The command processor may queue de queue sort prioritize and otherwise process commands that cause data to be written to and read from a storage device. Example command is a host request for data at starting address LBA1 and length SIZE. This command is first directed to the cache controller which will perform a lookup in the primary cache and if needed the secondary cache . This may be performed using a single lookup or may involve a second lookup request not shown directed to the secondary cache .

In this example it may be assumed that the lookup results in a cache miss in both the primary cache and the secondary cache . In response to the cache miss the data corresponding to LBA1 is retrieved from the main storage and placed in the primary cache at location . The data is also returned to the host command processor for communication to the host. While path indicates the data is read from location of the primary cache it may be returned directly to the controller from the main storage before during or after being placed in the cache . Subsequent requests for LBA1 will return the data directly from the primary cache until such time that the data is evicted.

In order to make room for addition of the data to location of the primary cache a corresponding amount of data from another cache location may need to be evicted from the primary cache . The selection of the location for eviction is based on data maintained by a cache monitor of the primary cache . It should be noted that the policy for eviction from the primary cache may be separate and independent from the policy for copying data from the primary cache to the secondary cache . For example although data being copied to the primary cache may trigger copying of data from primary to secondary caches this is not necessarily done to free up space in the primary cache. This event may be a convenient trigger to evaluate data for the secondary cache and may also be in response to other conditions discussed below such as high spatial locality of host read history.

In this example the primary cache monitor maintains sorted lists of metadata related to priority of segments within the primary cache . In this disclosure the term list is used generically and may be include using any data collection structure known in the art e.g. array linked list set map binary tree etc. Each cache segment identified in the cache priority lists may correspond to a fixed size data unit of the cache e.g. integer number of cache lines or may be variable sized data units e.g. corresponding to a contiguous ranges of LBAs where the range of each segment may be different sizes . For purposes of this illustration each block of data in the primary cache corresponds to a variable length segment a segment generally referring to a range of LBAs that are acted on together e.g. contiguous range .

The primary cache monitor may use the lists as part of a multi level priority scheme. One level of priority is based on the usage type of the data e.g. write read promoted . Another level of priority is based on the order of access to the data e.g. least recently used most recently used . The result of this assignment of priority is the set of lists one list for each usage type where each list is sorted from most recently used MRU to least recently used LRU . Finding an eviction candidate involves first locating a non empty list with the lowest retention priority and then selecting the LRU element from that list.

1The usage type lists can be used to efficiently filter candidate data based on read usage. For example a usage type of promoted is given lower retention priority in the primary cache . Usage types of read and write have the next higher orders of priority in that order. When a read segment e.g. segment is copied to cluster of the secondary cache the primary cache entry in lists related to segment may be lowered in retention priority by moving it from a read list to a promoted list as indicated by arrow . This may only be done if all of the cluster aligned data from the entry has been copied to the secondary cache . Otherwise the entry remains on the read list of the primary cache but may be marked do not promote to prevent repeated selection.

Besides usage type the cache monitor may maintain other attributes of primary cache entries related to promotion of primary cache segments to the secondary cache . This metadata may be stored in one or more of the lists . These attributes may include reserved for promotion promoted to flash cache prohibit promotion reservation etc. These attributes may be used in determining whether to evict a segment from primary cache as well as controlling whether the segment is copied to the secondary cache .

The analysis controller may be used to determine a cache importance metric of segments such as described herein. This metric may take on a value within a numerical range e.g. 1 10 and the metric may be adjusted over time. For example some data may have this importance metric determined at the time the data was requested e.g. in cases where the metric triggered the obtaining of speculative data from the primary storage along with the requested data. In other cases this metric may be determined later e.g. when determining whether to copy non speculative data to the secondary cache.

The cache importance metric along with other cache metadata noted above may govern the selection of a segment such as for copying to the secondary cache. For example as long as a primary cache segment is marked with promoted to flash cache or prohibit promotion reservation attributes it will not be considered for copying to the secondary cache. Also a logical address range of the primary cache segments may govern whether or not they are eligible for copying to the secondary cache. For example segments may have to satisfy size and alignment conditions of the secondary cache in order for its LBA range to be eligible for copying.

The copying of data in segment to the secondary cache may trigger an eviction of a cluster of data from the secondary cache . For purposes of this discussion the term cluster will be used to annotate ranges of data within the secondary cache . Similar to a segment of the primary cache a cluster generally refers to a range of LBAs within the secondary cache that are acted on together. As with the primary cache it is assumed in this example that the secondary cache has at least one cluster ready to receive the data although another cluster may need to be evicted either immediately or some time thereafter in order to facilitate insertion into cluster . A secondary cache monitor may maintain a separate list for purposes of determining priorities of the secondary cache .

The secondary cache priority list may maintain a set of metrics targeted to clusters within the secondary cache . These metrics may at least include the last access time and may be maintained in response to events such recent host interface activity. Elements of the list may make reference to fixed or variable sized clusters of the secondary cache and a single insertion may include multiple evictions from the secondary cache . The secondary cache monitor may also access cache importance metrics data from analysis controller and or database e.g. using one or more LBAs as an index.

It should be noted that in this example the secondary cache is assumed to be read only and so eviction is shown as the only option. However if the secondary cache is configured to store both read and write data then another option not shown may be to sync the data with the main storage before clearing the cluster for other uses. Similarly if the primary cache stores read write data then eviction may also involve synchronizing the cached data with the main storage .

In reference now to a block diagram illustrates an example of how segments are selected and retained for primary cache . The primary cache distinguishes read data from write data. Read data may be distinguished from write data by assigning a cache use identifier to primary cache segments. For example segments are identified as read data that are available for movement to the secondary cache . In this example each block within the segments represents a unit of data e.g. n1 LBAs where n1 is the same for all segments . The blocks with the segments are also linked together e.g. a contiguous range of LBAs within each segment . The primary cache may also have write data and it is assumed in this example that the secondary cache is populated only with read data from segments in the primary cache.

The process of searching for new read data to copy into secondary cache may be initiated when new valid read data is added to the primary cache . This search may also be dependent of the host read history spatial locality being high which can be determined via the analysis controller module and or database . Generally spatial locality refers to grouping of a relatively large number of requests over a relatively small range of LBAs and over a particular time range. High spatial locality of requests may be indicative that data in or near that range may be read or re read at some point in the future. The likelihood of some of the data being requested near term may not be high enough to justify storage in the primary cache but might be sufficient to move the data to the larger secondary cache .

A retention priority is used to guide the selection of segments that have eligible data for the secondary cache . The retention priority of the segments in the primary cache will be lowered when all possible data from that segment have been copied into secondary cache. For example segment is shown having all data copied to segment in the secondary cache and so segment will have a low or zero retention priority. In such a case a reference to the segment may be maintained in a sorted list and a low retention priority relative to other segments causes segment to be at the top of the list for removal.

Read data that is not yet copied to the secondary cache is given higher retention priority while read data that has been copied to secondary cache is given lower retention priority. A primary cache segment may include some portions that have been copied and others that have not been. Other retention priority relates to the last access time of LBAs within the segment.

One way to manage retention priority is to store a reference to eligible segments on one or more lists . For example the lists may have a collection of elements each referencing segments . Additional data may also be stored with the list elements such as access time cache priority etc. Lists and are ordered from least recently used LRU to most recently used MRU based on cache use of at least part of the segment. These lists may be combined into a single list e.g. sorted on primary and secondary fields use a combined sorting value or be used to collectively to define a retention priority. When an eviction is necessary the lists may be consulted in order of retention priority the lowest priority elements being at the top of the lists .

The elements placed in the lists may be limited to read segments e.g. using cache use identifier noted above that are not yet copied to the secondary cache . This can provide boundaries on the search for candidate segments to move to the secondary cache . In order to qualify for selection the segment may need to satisfy any or all of the following criteria a not already be reserved for copying to secondary cache b not prohibited from being copied to secondary cache c not already copied to secondary cache d overlap with the range of LBAs that may be copied into secondary cache and e contain at least one full cluster within its LBA range for implementations in which the secondary cache is organized into fixed sized clusters f contain data supplied by a read look ahead or read look behind operation or have a maximum cache importance that is greater than or equal to that of any LBA predicted to be evicted from the secondary cache if qualifying segment data were copied into secondary cache. The last criteria relates to additional speculative reading that may have been performed expressly for the purpose of promoting to secondary cache. In that case a cache importance eviction test may have already been performed at the time the speculative reading was being determined and the data would already have been marked promotion recommended .

Criterion a above refers to a segment being reserved but not yet having been copied to the secondary cache . For example segments already included in the lists that have not yet been copied over would not need to be further considered for addition to the list s . If an when such segment is copied to the secondary cache it would be removed from list s and while still resident in the secondary cache would not later be added in to the list s due to criteria c . Criterion b refers to situations where the number of LBAs to copy into the secondary cache from a single segment has a maximum value that may be smaller than some segments. When that is the case the range of data to copy is selected in a way that gives priority to the lowest non requested LBAs in the segment. The lowest non requested LBA is maintained within the segment by the primary cache. When such a segment is returned to primary cache it remains on the list of read segments rather than being moved to the list of segments already copied to secondary cache and is marked as prohibited from copying to secondary cache.

For example a high water mark may be maintained of the last requested LBA within each read segment of the primary cache . When a limit is imposed on the amount of data that may be copied into secondary cache from a single segment the data selected from that segment is chosen to give higher priority to non requested LBAs over requested LBAs and highest priority to the lowest non requested LBAs. Using segment as an example LBAs from the beginning leftmost end of the segment up to and including block A have been requested. Further as shown with cache line in the secondary cache the cluster size of the secondary cache is six blocks. As a result region B is given priority for copying into the secondary cache over any other region of segment . If the segment extended beyond the end of region B e.g. there was another range of LBAs to the right of B then B would be given priority over that other range.

There may be a maximum LBA range that is permitted to be copied into secondary cache. Criterion d enforces that some part of the segment is within the allowable range. The LBA range to be copied from the segment may be truncated if necessary.

Criterion e relates to selecting data for the secondary cache that satisfies an address criterion of the secondary cache. For example eligible segments may include those that satisfy a minimum size e.g. fully filling out one or more cache lines. Also a secondary cache cluster may define both alignment and size constraints of the LBA ranges that may be copied into secondary cache assuming that it is organized into fixed size cache lines. For example if the cluster size is 30 LBAs then any LBA range to be copied into secondary cache should have a starting LBA and number of LBAs that are each multiples of 30. Using LBAand LBAas respective start and final addresses of the segment the addresses in this example may have to satisfy the following where represents the integer modulus operator LBA 30 0 and LBA LBA 1 30 0. After selecting a segment its LBA range may further be adjusted to comply with the cluster alignment size requirements. For example the start LBA is rounded up to the nearest cluster boundary if not already aligned and the end LBA is rounded down to the nearest cluster boundary 1 if not already so aligned. If that is not possible then the segment may be disqualified.

Criterion f relates to an algorithm for filling the primary cache with speculative data as discussed in the Cache Range Selection reference. Speculative data generally refers to data that is adjacent to data of an existing host access operation e.g. sectors preceding and following read requested sectors that are not currently requested but are read anyway. By loading speculative data into the cache latency of future requests for speculatively cached data can be greatly reduced.

When determining candidates for copying to the secondary cache a secondary cache eviction test may be performed if it was not already performed during the setup of speculative reading. The test involves comparing the maximum cache importance of the next n clusters to be evicted from the secondary cache to the maximum cache importance of the candidate data to be added. The n value is the maximum size of data that can be added to secondary cache at a time but may also be matched to the size of the candidate data to be added. Because the secondary cache eviction test may already be performed during set up of speculative reading it will be marked as promotion recommended. In such a case there is no need to do an eviction test again when evaluating that data for promotion into the secondary cache

The controller for the caches should be able to service cache hits and invalidations on a primary cache segment while copying that segment s data into the secondary cache . Once a segment is selected for copy it may be reserved for the duration of the copy operation into the secondary cache . During this time the segment can be used to serve cache hits from the primary cache and won t be evicted. If a host issues an invalidate request for that segment the segment will be disabled for further cache hits but will otherwise remain intact until the copy into secondary cache completes at which time the remaining processing for the invalidation can be performed.

A qualifying segment that successfully copies all of its cluster aligned data into secondary cache will be removed from the list of read segments e.g. lists and may be added to a list not shown of segments already copied to secondary cache . Otherwise the segment remains on the list s of available read segments. If criteria d e or f above are not met the segment is not eligible to be copied to the secondary cache although at some later time it might be e.g. if secondary cache priorities are changed.

In reference now to a flowchart illustrates a procedure for selecting primary cache segments available for copying to a secondary cache according to an example embodiment. The procedure iterates through all the currently used segments as indicated by loop entry point . Alternatively the loop may iterate just through eligible segments with a read usage type which excludes data of promoted or write usage types. Decision blocks represent criteria a e described above. Note that decision block is indicated as optional based on whether the system is designed to ensure data is selected to fully fill out secondary cache lines. Decision blocks relate to criterion f above.

Note that if loop only iterates through eligible read usage type segments then the check at may not be needed. However the check at block may still be used. For example in some cases write data is characterized during idle time and or write data can be re characterized as read data via segment merging or other events. In such a case a segment marked as a read usage may still have unverified write data. In such a case the segment would be disqualified for copying to secondary cache.

If the respective decision blocks have the indicated outcomes yes or no as appropriate the selected segment is added to a list of segments eligible for copying to the secondary cache otherwise the selected segment is not added . This repeats for each segment until exiting the loop at . It should be noted that the order of decision blocks is shown for purposes of example and can be arranged in any order. For example the decision blocks that most often leads to finishing loop via would be placed first to reduce processing time in the loop . After the procedure is completed the list is updated with primary segments eligible for copy to the secondary cache. This list may be sorted e.g. based on last access time primary cache priority etc. For example by starting a search with the lowest priority entries and proceeding toward the higher priority entries lowers the probability of losing eligible data to eviction before it gets copied to secondary cache. Segments corresponding to elements at the end of the list lowest primary cache priority will be copied to the secondary cache under the appropriate conditions.

In reference now to a flowchart illustrates a procedure according to an example embodiment. The process involves causing a block of data to be copied in a primary cache based on a host data read access request. The primary cache mirrors a first portion of a non volatile main storage. The procedure further involves determining a criterion for movement of data from the primary cache to a secondary cache that mirrors a second portion of the main storage. The criterion gives higher priority to blocks having addresses not yet selected for reading by the host. The criterion may also optionally give higher priority to the blocks having the longest elapsed time since last host access. The criterion may optionally prevent the movement of the data based on whether an address range of the blocks overlaps with a range of addresses already copied or reserved for copying to the secondary cache and or the data not satisfying a minimum size associated with a cache line of the secondary cache.

In response to the block of data being copied to the primary cache a selected segment of data is moved from the primary cache to the secondary cache and in response to the selected segment satisfying the criterion. Moving the selected segment of data from the primary cache to the secondary cache may cause a cluster of the secondary cache to be evicted. In such a case determining the criterion may optionally involve determining whether the selected segment has an equal or higher priority than the evicted cluster. In another arrangement the secondary cache may be used for storing read data only. In such a case the criterion may optionally prevent the movement of the data based the segment containing write data.

In reference now to a flowchart illustrates a procedure according to another example embodiment. The procedure involves defining a list that references segments of a primary cache eligible for moving to a secondary cache. The primary and secondary caches respectively mirror first and second portions of a non volatile main storage. The list may include any data structure that stores a collection of data objects in this example a reference to segments of the primary cache.

Candidate segments from the primary cache are added to the list in response to logical block address ranges of the candidate segments satisfying an address criterion of the secondary cache. The address criterion of the secondary cache may include a cache line size of the secondary cache. In such a case the candidate segments satisfy the criterion if a size of the candidate segments is greater than or equal to the cache line size. The candidate range may also need to meet alignment requirements relating to the line size of the secondary cache. For example the address criterion may also include an allowable starting logical block address of a cache line of the secondary cache. In that case the candidate segments satisfy the criterion if a logical block address of the candidate segments satisfies the allowable starting logical block address. In an arrangement where the secondary cache stores read data only the candidate segments may be added to the list further based on the candidate segments not including write data.

In optional block additional conditions may be used to add candidate segments from the primary cache. Those conditions may include candidate segments not including write data not having already been copied or reserved for copying to the secondary cache having addresses not yet selected for reading by the host and or having a secondary cache importance metric greater than an equivalent metric of a segment of the secondary cache that would be evicted in response to moving the candidate segments to the secondary cache.

In response to new data being added to the primary cache at least one of the candidate segments is moved to the secondary cache. In one arrangement the list is optionally sorted based on a primary cache priority of the candidate segments. In response to this sorting candidate segments having a lower cache priority are selected for moving to the secondary cache ahead of candidate segments with a higher cache priority.

Now referring back to some embodiments described above relate to priority schemes carried out by the FCI layer to prioritize how data is written to the primary and or secondary caches. Additional information regarding the functionality of the FCI layer is provided in the following concurrently filed U.S. patent application Ser. Nos. 13 542 990 13 543 036 and 13 543 123 which are incorporated herein by reference.

Some embodiments described herein involve processes implemented in the FCTM layer to manage memory access requests to store or retrieve information to from the secondary cache etc. received from the FCI layer and sent to the SSD layer . The memory access requests may involve reading the flash memory second cache writing to the flash memory and so forth. In various embodiments management and implementation of the memory access requests is accomplished in the FCTM layer using a set of incoming queues and a set of outgoing queues. is a flow diagram that illustrates a process of managing memory access requests in the FCTM layer. Memory access requests are received by the FCTM layer from the FCI layer and SSD memory access requests are sent to the SSD layer by the FCTM layer. Memory access requests sometimes referred to herein as incoming memory access requests or as incoming requests because these requests are incoming from the perspective of the FCTM layer are received by the FCTM layer from the FCI layer. The incoming requests are routed into a set of incoming queues. The memory access requests queued in the incoming queues are transformed into SSD requests sometimes referred to herein as outgoing memory access requests or outgoing requests because these requests are outgoing from the perspective of the FCTM layer . The outgoing requests are routed to a set of outgoing queues. The outgoing requests in the outgoing queues are sent to the SSD layer which carries out the SSD requests to perform the operations specified in the SSD requests.

The overall structure of the incoming queues and the outgoing queues is illustrated in . The incoming queues include an incoming free queue of nodes which are used to control the flow of memory access requests into the receive queue . Generally the FCTM scheduler routes incoming memory access requests from the FCI layer into the receive queue only if a node is available in the incoming free queue . In other words the number of nodes in the incoming free queue represent the capacity of the FCTM layer at any particular time to process incoming memory access requests.

As illustrated in if a node is available in the incoming free queue that node becomes occupied by an incoming memory access request when the memory access request is routed into the receive queue . When a node is occupied by a memory access request information about the memory access request is stored in the node. For example the node may store information about the type of memory access request the host LBAs involved in the memory access request information about the progress of the memory access request such as how much data has been transferred in conjunction with the memory access request how much work to complete the memory access request is pending and so forth. If a node is not available in the incoming free queue then the FCTM layer does not have the capacity to process the incoming memory access request and an error message is generated.

The FCTM layer can process a number of types of memory access requests received from the FCI layer. illustrate the process for three types of memory access requests read requests promotion requests and invalidate requests. As explained in more detail herein read requests are requests from the FCI layer to read host LBAs from the flash promotion requests are requests to promote write host LBAs into the flash and invalidate requests are requests to mark certain host LBAs in the flash as invalid not containing valid data . In the illustrated example of the FCTM layer includes a separate ready queue for each type of request. However in some implementations at least some of the queues may be shared queues between different types of requests. Memory access requests are moved to the appropriate ready queue when they do not have any overlaps with other requests in the execute or ready queues and they are waiting to be routed to the execute queue to begin execution. A memory access request remains in the ready queue until there is at least one SSD resource available to execute the request. In illustrated example SSD nodes that are available in the outgoing free queue represent SSD resources thus if there is at least one available SSD node the memory access request can be moved to the execute queue. The incoming memory access requests are transformed into a number of outgoing SSD requests which transferred to the SSD execute queues as outgoing nodes in the outgoing free become available.

Each read request promotion request and invalidate request has associated with it a particular address range host LBA range . The FCTM layer may transform one incoming memory access request e.g. read request promotion request invalidate request from the FCI layer into one or multiple SSD requests which the FCTM layer issues to the SSD layer. Requests issued by the FCI layer and received by the FCTM layer are referred to herein as memory access requests or incoming FCI requests or incoming memory access requests. Requests issued by the FCTM layer to the SSD layer are referred to herein as SSD requests outgoing SSD requests or as outgoing memory access requests. The FCTM layer transforms each incoming memory access request that has an associated host LBA range into one or multiple outgoing memory access requests that each have an associated SSD LBA range or cluster of SSD LBAs. For example the FCTM layer implements an incoming a read request by generating one or more SSD requests that include an SSD LBA range. The FCTM layer implements an incoming promotion request by generating one or more outgoing SSD requests that include a cluster of SSD LBAs the FCTM layer implements an incoming invalidate request by generating one or more outgoing SSD requests that include a cluster of SSD LBAs.

As previously discussed the FCTM layer includes an overlap checker which operates in conjunction with an overlap queue . As best illustrated in FIG. B requests in the receive queue may be routed into the overlap queue when an overlap in memory access requests is identified by the overlap checker . An overlap may occur if there is an overlap in the address range e.g. host LBA range of two memory access requests. For example an overlap can occur if a read request and a promotion request have overlapping address ranges. In this example assuming the promotion request is received in the FCTM layer before the read request the promotion request may be completed before the read request is moved to the read ready queue to avoid overlap conflicts. During the time that the promotion request is in the promotion ready queue or the execute queue the read request remains in the overlap queue . Appropriate management of overlapped memory access requests avoids erroneous data being read from or written to the flash memory . After the overlap has been cleared the memory access request is routed into the appropriate ready queue 

If the FCTM layer has at least one resource an SSD node available the memory access request is transferred from a ready queue to the execute queue . Execution of a memory access request in the execute queue involves transforming the memory access request in the execution queue into a number of SSD requests that provide instructions to the SSD layer to carry out the memory access request. A memory access request from the FCI layer includes a host LBA range and this host LBA range is transformed to an SSD LBA range by the FCTM layer. In some implementations the FCTM internally keeps track of the host LBA range in terms of clusters groups of n host LBAs referred to as host LBA clusters and keeps track of the SSD LBA range in terms clusters of n SSD LBAs. Transformation of contiguous clusters of host LBAs may or may not be transformed into contiguous clusters of SSD LBAs. After the SSD request is transferred to the SSD layer the SSD layer may convert the SSD LBA range included in the SSD request to flash address die block and page . For execution of a memory access request to begin the outgoing free queue must have available at least one available node otherwise an error message is generated. Different types of memory access requests may be transformed into different numbers of SSD requests which is associated with the amount of work required by the type memory access request. For example an invalidate request may occupy a first number of nodes e.g. only one node whereas a read or promotion request may occupy a larger number of nodes.

In some cases when a particular memory access request in the execute queue is transformed into a number of SSD requests one e.g. only one of the SSD execute queues will include all the SSD requests associated with the incoming memory access request in the execute queue . Each of the SSD requests represents outstanding work to the SSD layer. As nodes become available in the outgoing free queue to execute a memory access request in the execute queue those available outgoing nodes become occupied by the SSD requests associated with the memory access request which is being executed. The SSD requests associated with the memory access request being executed are transferred to an SSD execute queue . The memory access request being executed may remain in the execute queue occupying a node from the incoming free queue until execution of the memory access request by the FCTM layer is complete. Execution of a memory access request in the FCTM layer may be deemed to be complete when the responsibility for processing the memory access request is transferred from the FCTM layer to the SSD layer . This occurs after all the SSD requests associated with a memory access request are issued to the SSD layer. For example responsibility may be transferred when the last SSD request associated with a memory access request is successfully transferred to the SSD layer or when the last SSD request associated with the memory access request has been successfully completed by the SSD layer and acknowledgement of the successful completion of the SSD request has been received by the FCTM layer.

When execution of a memory access request is complete the node from the incoming free queue that was previously occupied by the incoming memory access request in the execute queue is returned to the incoming free queue . The previously occupied node becomes available again for being occupied by subsequent memory access requests. Each of the nodes in the SSD execute queue associated with the memory access request being executed are returned to the outgoing free queue as the SSD requests occupying these nodes are completed. The previously occupied SSD nodes become available again to be occupied by subsequent SSD requests. In some cases an error occurs when one or more SSD requests are transferred to the SSD layer. When an error occurs in the processing of SSD requests associated with a memory access request the node used to process the incoming memory access request may be returned to the incoming free queue and the SSD nodes used to process the outgoing SSD requests may be returned to the outgoing free queue. In other words the processing of the incoming memory access request is cancelled and not completed when an error occurs.

In some implementations incoming memory access requests from the FCI layer to the FCTM layer is restricted meaning that during a time that the FCTM layer is processing a memory access request then the FCI layer is barred from issuing another memory access request to the FCTM layer. Implementations that restrict additional incoming memory access requests from the FCI layer protects the FCTM layer from excessive combinations of possible events affecting the FCTM layer and enhances the thread safety of the layer. In some implementations the code e.g. all of the code that manages the queues is executed on a single thread and none of the data structures of the FCTM layer e.g. the queues can be used are manipulated by external entities e.g. other layers of the hybrid controller.

A priority scheme may be used for transferring the incoming and or outgoing memory access requests between queues. In some cases the priority scheme may be multi tiered wherein a first level of priority is implemented by the FCTM scheduler to select incoming memory access requests from the ready queue and a second level of priority is implemented by the FCTM scheduler when assigning SSD nodes from the outgoing free queue.

A priority scheme e.g. first level priority may be used to select requests from the ready queues for transfer to the execute queue. According to one priority scheme requests that require the least resources and or are faster to execute may be selected for execution before requests that require more resources and or are slower to execute. For example invalidate requests present in the invalidate ready queue may be selected for execution before read or promotion requests in the read ready queue or promotion ready queue respectively because invalidate requests are the faster to execute. In general invalidate requests execute faster than either read or promotion requests and read requests execute faster than promotion requests thus the priority scheme may follow this order. For example the invalidate requests may not require input output I O transferred via the PSM layer to the flash and may be executed by updating metadata in the FCTM layer and performing an unmap in the SSD layer which also only involves the updating of metadata in the SSD layer. Requests that do not require I O to the flash typically take the least amount of time to execute. Despite not requiring I O to the flash the FCTM scheduler may use an SSD node to keep track of and regulate the flow of invalidate requests.

The flow diagram of conceptually illustrates an overview of one priority scheme that may be implemented by the FCTM scheduler for routing memory access requests from the ready queues to the execute queue. As previously discussed for each incoming memory access request the FCTM scheduler routes the memory access request to the receive queue if there is a node available in the incoming free queue . If there are no nodes available in the incoming free queue the FCTM scheduler generates an error response which is sent to the FCI layer.

The FCTM overlap checker determines if the address range host LBA range of the memory access request that is routed to the receive queue overlaps with the address range of other memory access requests. If an overlap is not detected the memory access request is routed to the appropriate ready queue. If an overlap is detected the memory access request is routed to the overlap queue. Data access requests routed to the overlap queue wait there until the overlap is resolved. If the overlap is resolved the previously overlapped memory access request is routed to the appropriate ready queue.

Data access requests wait in the ready queue until at least one SSD node in the outgoing free queue is available for execution of the memory access request. Once an SSD node is available the priority scheme for routing memory access requests to the execute queue is implemented. If there is an invalidate request in the ready queue the invalidate request is routed to the execute queue and the process returns to the implementation of the priority scheme at step . According to the priority scheme if multiple invalidate requests are present in the invalidate ready queue these invalidate requests would be processed until the invalidate ready queue is empty. If the invalidate ready queue is empty and there is a read request in the read ready queue the read request is routed to the execute queue and the process returns to the implementation of the priority scheme at step . If there are no invalidate requests or read requests in their respective ready queues and there is a promotion request in the promotion ready queue the promotion request is routed to the execute queue and the process returns to the implementation of the priority scheme at step .

In some scenarios a priority scheme may be pre emptive involving pre empting requests in the execute queue with requests in the ready queue. In some implementations such a pre emption takes place if the request in the ready queue would take less time resources for execution than the request in the execute queue. In one example invalidate requests in the ready queue preempt promotion requests in the execute queue. Execution of the invalidate requests may cause a delay in the completion of the execution of the promotion request however this delay may be minimal because the invalidate requests can be executed very quickly if there is no I O to the flash.

One possible implementation of a pre emptive priority scheme is conceptually illustrated by the flow diagram of . Such a priority scheme may be implemented alone or as a second level of priority in conjunction with another priority scheme e.g. the priority scheme discussed in connection with . After the work associated with an SSD request occupying an SSD node is completed the SSD node is returned to the outgoing free queue and becomes available again. A priority scheme is implemented that determines the memory access request to which this available SSD node is next assigned. According to the priority scheme of if there is an invalidate request in the execute queue the SSD node is assigned to the invalidate request. If there is a read request in the execute queue the SSD node is assigned to the read request. If there is an invalidate request in the ready queue the invalidate request is moved to the execute queue and the SSD node is assigned to the invalidate request. If there is a read request in the ready queue the read request is moved to the execute queue and the SSD node is assigned to the read request. If there is a promotion request in the execute queue the SSD node is assigned to the promotion request. If there is a promotion request in the ready queue the promotion request is moved to the execute queue and the SSD node is assigned to the promotion request. The priority scheme illustrated in provides for an optimal ordering in the execution of requests to achieve minimal host request latency.

Note that the priority scheme illustrated in may mean that a request in the execute queue may be pre empted by a request for which execution has not yet started. The preempting request may be in the ready queue and if so the pre empting request would be moved to the execute queue and the available SSD node would be assigned to it. Thus the pre empting request may delay the execution of a request in the execute queue that is currently being executed.

In some scenarios the flash memory may be full when a promotion request is executed. If so the FCTM may cause some data stored in flash to be evicted. To implement evictions as illustrated in the FCTM layer maintains least recently used LRU or a most valuable least valuable MVLV list of clusters which ranks the value of the clusters according to some criteria which may be based on one or a number of factors such as which of the clusters was most least recently used and or which of the clusters is most frequently least frequently used for example. One end of the MVLV list is referred to herein as the head which is the position of the currently most valuable cluster and the opposite end of the MVLV is referred to as the tail which is the position of the currently least valuable cluster. If the flash memory is full and a promotion request is executed the cluster at the tail of the MVLV list is selected for eviction. In some implementations when a cluster is read or written that cluster becomes the most valuable cluster because it was most recently used and is moved to the head of the MVLV list .

The FCTM layer maintains list e.g. linked list of free SSD LBA clusters denoted the free list and or maintains a list e.g. linked list of in use SSD clusters denoted the use list . The free list includes SSD clusters that are available for use. The use list includes SSD clusters that contain valid data and are not available to accept new data. In some cases one or more SSD clusters may be in a detached state during which the SSD clusters are not in either the use state or the free state. These SSD clusters do not appear in either of the free list or the use list. An SSD cluster may be in a detached state for example during the time that the clusters are involved in execution of a request e.g. during the time that data is written to the clusters.

The flow diagrams of conceptually illustrate some steps involved in the execution of invalidate read and promotion requests respectively performed by the FCTM layer. As previously discussed each incoming memory access request includes a command portion and a host LBA range. The command portion identifies the type of request and the host LBA range indicates the host LBAs involved in the request. In addition a promotion request is associated with the data to be written to the LBA range specified in the promotion request.

An invalidate request issued by the FCI layer identifies a cluster aligned range of host LBAs to be invalidated. A cluster aligned LBA range means that the start of the LBA range and the end of the LBA range are not arbitrary but are multiples of n which is the number of sectors per cluster. Referring to the flow diagram of the invalidate request is transferred to the execute queue. The FCTM layer maps the cluster aligned LBA range of the incoming memory access request to the SSD LBA clusters and determines the SSD LBA clusters involved in the invalidate request. The SSD clusters are invalidated marked as containing invalid data in the FCTM metadata. An SSD request sent by the FCTM layer to the SSD layer comprises an unmap request for the corresponding flash memory clusters. The invalidated SSD LBA clusters can be moved to the free cluster list maintained by the FCTM layer in its metadata. Note that implementation of an invalidate request does not require any work performed by the flash

A read request involves reading data corresponding to an arbitrary range of LBAs from the flash memory. The host LBA range of a read request from the FCI is not necessarily cluster aligned. There may be an upper bound on the number of LBAs that can be included in the read request. In the example illustrated in the FCTM layer initially performs a check to determine if the range of host LBAs specified by the read request is fully present in the flash. If the range of host LBAs is not fully present the read request is rejected and an error response to the FCI layer is generated . The error response notifies the FCI layer to obtain the data requested from the primary memory e.g. the magnetic disk. If the range of LBAs specified by the read request is fully present in the flash memory then the read request is moved to the execute queue. The FCTM maps the host LBA range to the SSD LBAs. A list of SSD LBAs in the read request is created . The FCTM layer issues one or more SSD requests to the SSD layer that specify the SSD LBAs to be read. The list of SSD LBA clusters that include the SSD LBAs of the read request may be made most valuable e.g. moved to the head of the MVLV list.

As shown in a promotion request involves writing a cluster aligned range of host LBAs to the flash memory. There may be an upper bound imposed on the number of LBAs that can be included in one promotion request. The promotion request is moved to the execute queue . A list of the SSD LBA clusters corresponding to the cluster aligned host LBA range specified in the promotion request that are already present in the flash is created . The clusters already present in the flash are denoted overlapped clusters. A bitmap is created to skip over the SSD LBA clusters that are already present in the flash. The process of determining the clusters already present in the flash and creating the bitmap mask facilitates conservative use of the flash memory space by maintaining a single copy of any host LBA in the flash. The overlapped SSD LBA clusters and the non overlapped SSD LBA clusters are made most valuable by moving these clusters to the head of the MVLV list. The FCTM determines if there are a sufficient number of clusters available to store the clusters to be written into the flash. The clusters to be written to the flash are the clusters implicated by the promotion request that are not already present in the flash. If there are a sufficient number of clusters available then clusters for storing the data are allocated and the SSD LBA clusters to be stored are transferred via the SSD layer to the flash. The metadata of the FCTM layer i.e. the use list is updated to indicate that these clusters are in use. If a sufficient number of clusters is not available the SSD LBA space is saturated then the FCTM layer will perform evictions to free up a sufficient number of clusters.

Eviction overlap may lead to data errors. Eviction overlap can occur when the address range being evicted overlaps with the address range of an outstanding command that is in the ready queue or the execute queue. The FCTM scheduler described in various embodiments discussed herein is arranged to operate so that eviction overlap is avoided.

If the flash memory is not saturated i.e. there is a sufficient free space in the flash for promotion without evictions being performed non overlapping requests from the FCI layer can execute in any order. For an unsaturated flash only overlapped requests are placed in the overlap queue. If the flash is saturated evictions must take place in order to make room for promotion requests to be implemented.

As illustrated in certain steps are carried out during the ready queue to execute transition for read promotion and invalidate requests. These steps may be implemented as atomic operations that are completed without interruption. Performing these steps atomically without interruption ensures that no other requests remove these SSD LBA clusters from the SSD LBA space before the request has executed. For example if an invalidate request is received while a read request is executing the invalidate request will move to the overlap queue so that the invalidate request does not interfere with the execution of the read request. If the invalidate request were serviced during execution of the read request there is a possibility that the invalidate request would invalidate some LBAs involved in the read request.

For read requests during the ready queue to execute queue transition the FCTM scheduler verifies if the LBA range in specified in the request is fully present in the flash. If the range is not fully present the read request is not executed and an error response is generated. The SSD clusters that correspond to the host LBA range of the read request whether or not fully present are made most valuable by moving these clusters to the head of the MVLV list. If the SSD clusters that correspond to the host LBA range of the read request are fully present in the flash the FCTM scheduler creates a list the SSD clusters and implements the read request as previously discussed.

For promotion requests during the ready queue to execute queue transition the FCTM scheduler checks to determine which SSD LBA clusters are already present in the flash and creates a bitmap of the overlapped SSD LBA clusters already present in the flash. The bitmap is used to skip writing the overlapped clusters to the flash. If the flash is saturated the required number of clusters may be evicted to make room for the new clusters to be written as part of the promotion request.

For invalidate requests implemented during the ready queue to execute queue transition the FCTM scheduler migrates the SSD LBA clusters being invalidated into the free list of SSD clusters. The FCTM scheduler issues an unmap SSD request for the invalidated clusters.

Note that evictions can occur in response to promotion requests. To analyze the potential for eviction overlap the following scenarios are considered as illustrated in .

1. P e R a read request R precedes a promotion request with eviction P e . Eviction overlap in this situation is not possible because during the read requests transition from the ready queue to the execute queue the SSD clusters involved in the read request were moved to the head of the MVLV list. The SSD clusters of the read request will not be selected for eviction during the promotion operation with eviction because the clusters evicted prior to the promotion will be selected from the tail of the MVLV list.

2. R P e a read request follows a promotion request with eviction . The host LBA range of the read request may no longer be fully present in the flash when the read request is executed . If this occurs the read request will be handled accordingly e.g. by sending an error message to the FCI layer.

3. P e I an invalidate request I precedes a promotion request with eviction . Eviction overlap in this situation is not possible because invalidate requests are completed synchronously and never remain in the execute queue. The same call chain that places an invalidate request in the execute queue also moves the node occupied by the invalidate request from the execute queue back to the free queue.

4. I P e an invalidate request follows a promotion request with eviction . When the invalidate request reaches the execute queue the cluster range may not be fully present in the flash. In this scenario only clusters present in the flash will be invalidated.

5. P e P a promotion request e.g. without eviction P precedes a promotion request with eviction . Eviction overlap in this is not possible because when the preceding promotion request P is being executed the sectors being written to are detached temporarily removed from the use list and the free list and thus will not be evicted. The sectors written by the preceding promotion request P are moved to the head of the MVLV list after they are written.

6. P P e a promotion request e.g. without eviction follows a promotion request with eviction . When the following promotion request P reaches the execute queue there is a possibility that clusters specified in its SSD LBA cluster range may no longer be present in the flash if these clusters were evicted by the preceding P e request. If the clusters are no longer present this may result in these clusters being written to the flash.

The mapping of the host LBA clusters to the SSD clusters by the FCTM layer is fully associative meaning that any host LBA cluster can be mapped to any of the SSD LBA clusters so long as there is room in the cache. diagrammatically depicts mapping of the host LBA space to the SSD LBA space . In the FCTM layer the host LBA space is clustered into clusters of host LBAs and the SSD LBA space is clustered into clusters of SSD LBAs. In the host LBA space each cluster of host LBAs is uniquely identified by a number between 0 and N 1 and each cluster includes n contiguous sectors. In the SSD LBA space each SSD cluster is uniquely identified by a number between 0 and K 1 K is typically less than N and each cluster includes n sectors. The number of sectors per cluster n may be fixed and can depend on the size of a host sector the geometry of the flash memory the error correction code ECC used to store data in the flash memory and or other factors. In the example illustrated in n 32 however in other implementations n may be greater than or less than 32. Furthermore in general n need not be a power of two.

In some implementations and as shown in the host sectors are aligned with the cluster boundaries. In other words a host LBA is not allowed to span more than one host LBA cluster.

The mapping from host LBA space to SSD LBA space is accomplished by a hash function . As previously discussed the hash function can support fully associative caching with regard to clusters. In other words the hash function allows any host cluster to be mapped to any SSD cluster as indicated by arrows . However the mapping may be constrained such that any host LBA can exist in only one SSD cluster at any given time. The offset within a cluster where an LBA is located within a cluster is fixed and is can be determined by the host LBA modulo the number of host LBAs per cluster i.e. the remainder resulting from dividing the host LBA by n. Allowing a host LBA cluster to be mapped into any SSD cluster and ensuring that promotes and invalidates implemented by the FCTM layer are aligned to cluster boundaries avoids cache fragmentation.

The hash function is used to convert the tag upper L bits of the host LBA into a hash table index in the hash table . The entry in the hash table indicated by the hash table index the tag converted by the hash function points to one or more clusters in the SSD LBA space. For example for a host LBA of L M bits the lower M bits can be used as a sector offset to identify the sector within an SSD cluster. The remaining L bits are used for the tag. The hash function operates on the tag to generate the index into the hash table . For example the hash function may discard the upper L H bits of the tag and use the lower H bits as the hash table index. Discarding a portion of the tag means that in some cases a number of different host LBAs will map to the same entry in the hash table and a collision will occur. An index in the hash table is associated with more than one cluster identification ID only if a collision occurs. In this scenario 2host LBAs mapped to a cluster will all have the same tag. If the hash function discards the upper bits leaving only H lower bits for the hash table index the theoretical maximum number of possible collisions i.e. the number of clusters that map into the same SSD LBA space is 2. The L H bits of the tag identify the cluster ID. The collisions are resolved using a linked list . The linked list contains the cluster IDs that are hashed to the same entry in the hash table i.e. have the same hash index . To access a particular cluster the linked list is scanned for an entry with the correct cluster ID. For example when the FCI layer requests a look up involving a particular host LBA cluster the FCTM layer applies the hash function and if there is a collision two clusters that map to the same space then the FCTM layer traverses through the linked list to locate the requested cluster.

The above description assumes that the number of host sectors per cluster is a power of two. However non power of two sector sizes may also be used. A representative set of host sector sizes that are supportable by the fully associative cache structure described herein include but is not limited to the following sector sizes 512 520 524 528 4096 4192 and 4224 bytes. For example based on sector to cluster mapping calculations there may be 30 5XX byte sectors per cluster assuming a cluster is 16 KB of the flash such as an 8 KB flash page size with dual plane support .

Non powers of two can be handled by modifying the mapping described above as follows The tag is determined as tag host LBA sectors per cluster where indicates an integer division via truncation and the host sector offset within the cluster is determined by host LBA modulo the sectors per cluster i.e. the remainder after dividing the host LBA by the sectors per cluster.

The division and modulo operations can be implemented by executing a multiply instruction e.g. a 64 bit multiply instruction on the FCTM processor assuming the FCTM processor supports 64 bit multiple instructions. To facilitate the multiply the value p 0xFFFFFFFF sectors per cluster is pre computed is a constant value. The tag is now determined by tag host LBA p 32 where indicates a 64 bit multiply operation and where 32 means that the result of host LBA p is right shifted 32 times. Using this process there is a possibility that the tag is off by one. To correct for this occurrence the tag is incremented by one if the following condition is satisfied Host LBA tag sectors per cluster sector per cluster. The remainder can be similarly determined.

Additional information regarding the functionality of the FCTM layer is provided in the following concurrently filed U.S. patent application Ser. Nos. 13 543 079 and 13 543 100 which are incorporated herein by reference.

The various embodiments described above may be implemented using circuitry and or software modules that interact to provide particular results. One of skill in the computing arts can readily implement such described functionality either at a modular level or as a whole using knowledge generally known in the art. For example the flowcharts illustrated herein may be used to create computer readable instructions code for execution by a processor. Such instructions may be stored on a computer readable medium and transferred to the processor for execution as is known in the art. The structures and procedures shown above are only a representative example of embodiments that can be used to facilitate managing caching in data storage devices as described above.

The foregoing description of the example embodiments has been presented for the purposes of illustration and description. It is not intended to be exhaustive or to limit the inventive concepts to the precise form disclosed. Many modifications and variations are possible in light of the above teaching. Any or all features of the disclosed embodiments can be applied individually or in any combination are not meant to be limiting but purely illustrative. It is intended that the scope be limited not with this detailed description but rather determined by the claims appended hereto.

