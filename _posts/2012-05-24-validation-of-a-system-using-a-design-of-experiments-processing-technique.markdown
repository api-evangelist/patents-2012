---

title: Validation of a system using a design of experiments processing technique
abstract: A validation system includes a test block that operates to apply a set of inputs to a system under test, such as a test system or an executable test algorithm, and receive from said system under test a first set of outputs produced by operation of the system under test in response to application of the set of inputs. The first set of outputs, as well as a second set of outputs reflecting output produced by operation of a reference system or executable reference algorithm in response to application of the same set of inputs, is processed to make a validation determination. A validation processing block compares the first and second sets of outputs to validate the system under test as an equivalent to the reference system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08856597&OS=08856597&RS=08856597
owner: STMicroelectronics, Inc.
number: 08856597
owner_city: Coppell
owner_country: US
publication_date: 20120524
---
The present invention relates to a system such as a System on Chip SoC which executes an algorithm and in particular to a method and apparatus for validating a new version of that algorithm on the system using a Design of Experiments DoE processing technique.

Those skilled in the art understand that a System on Chip SoC combines the components of a complex computing or electronic system on a single integrated circuit. The included components of the system comprise both hardware components and software components. Hardware components include timers power supplies and regulators input output interfaces analog and digital microprocessors or microcontrollers and volatile non volatile memories. Software components include an operating system applications and drivers. A block diagram of an exemplary SoC is provided in .

It is common over the course of the operating life of an SoC for an update of the SoC software components to occur. A new version of an algorithm may be provided to replace an existing version of the algorithm. Such a new version may fix problems with the existing version such as bugs present a more efficient implementation of functionality supported by the existing version or add new functionality to that provided by the existing version. Validation of the new version of the algorithm is important to ensure that the new version can be used in place of the existing version without adversely affecting operation of the SoC. If validation is confirmed the existing version may be deleted and the new version thereafter executed in its place.

The present invention is directed to method and apparatus for validating a new version of an algorithm in regard to an existing or reference algorithm. A comparison of the algorithms is performed by executing both algorithms modifying predetermined inputs to both algorithms and comparing the resulting outputs. Proper configuration of the inputs will allow using the method of Design of Experiments DoE the determination of whether the differences in output are due to significant functional differences between the subject algorithms. The result of the comparison process leads to a determination of whether the new algorithm is basically the same as the existing or reference algorithm and may further lead to a characterization of the new version as better or worse performing than the existing or reference algorithm.

The present invention is also directed to method and apparatus for validating a new system in regard to an existing or reference system. A comparison of the systems is performed by operating both systems modifying predetermined inputs to both systems and comparing the resulting outputs. Proper configuration of the inputs will allow using the method of Design of Experiments DoE the determination of whether the differences in output are due to significant functional differences between the subject systems. The result of the comparison process leads to a determination of whether the new system is basically the same as the existing or reference system and may further lead to a characterization of the new system as better or worse performing than the existing or reference system.

In an embodiment a system comprises software components including a reference algorithm hardware components including a processing component operable to execute the reference algorithm and a validation system configured to test execution of a test algorithm in comparison to execution of said reference algorithm and in response thereto validate the test algorithm to replace said reference algorithm for execution by said processing component.

In an embodiment a system comprises software components including a test algorithm hardware components including a processing component operable to execute the test algorithm and a built in self test system configured to test execution of the test algorithm in comparison to execution of a reference algorithm and in response thereto validate that execution of test algorithm by said processing component conforms with execution of said reference algorithm.

In an embodiment a method comprises accessing a reference algorithm executing the accessed reference algorithm in response to a set of inputs to produce a first set of outputs accessing a test algorithm executing the accessed test algorithm in response to the same set of inputs to produce a second set of outputs and comparing the first and second sets of outputs to validate the test algorithm as an equivalent to the reference algorithm.

In an embodiment a validation system comprises a test block configured to apply a set of inputs to a system under test and receive from said system under test a first set of outputs produced by operation of the system under test in response to application of the set of inputs a memory storing the first set of outputs and further storing a second set of outputs said second set of outputs reflecting output produced by operation of a reference system in response to application of the same set of inputs and a validation processing block configured to compare the first and second sets of outputs to validate the system under test as an equivalent to the reference system.

In an embodiment a method comprises applying a set of inputs to a system under test receiving from said system under test a first set of outputs produced by operation of the system under test in response to application of the set of inputs storing the first set of outputs storing a second set of outputs said second set of outputs reflecting output produced by operation of a reference system in response to application of the same set of inputs and comparing the first and second sets of outputs to validate the system under test as an equivalent to the reference system.

Updating of the software components of an SoC is a conventional activity. Here the phrase software components is understood to mean and refer to any algorithmic based process executed by the SoC including without limitation firmware FPGA code software microcode driver code application code operating system code and the like.

Regression testing is an important element in the course of software component improvement and evolution. Regression testing is performed to give a level of assurance that already working systems are not compromised by any unanticipated adverse effects from planned innovative changes and fixes. Embodiments described herein focus on implementation of a method and system for a generic relatively automated means of regression testing that can be made a ready attribute of a software system and in particular the software system of an SoC. The method and system are further applicable to testing a software system of an SoC to ensure proper operation in a test mode such as built in self test GIST operation. The method and system are further applicable to testing an overall system in the context of making a replacement of a reference system with a new system.

Reference is now made to . An SoC of the type shown in is functionally enhanced over prior art SoC implementations to include an on board validation engine . The validation engine would typically be included in the firmware of the SoC . In an alternative implementation the validation engine is provided within the SoC using a micro controller device. In yet another implementation the validation engine is provided using a software driver of the SoC that is specific to the validation purpose. The validation engine may for example comprise a component of a BIST or other test mode function for the SoC .

In the scenario of an algorithm update the validation engine functions to implement a validation process which seeks to determine whether a new version test algorithm for an SoC software component can be used by the SoC in place of an existing reference algorithm. In the scenario of a test mode validation the validation engine functions to implement a validation process which seeks to determine whether the current version test algorithm is functioning properly in comparison to the functionality of a reference algorithm.

A number of input settings are provided in accordance with a Design of Experiments DoE methodology and both the reference and test software components are executed in response to those input settings. Numerical output from each execution of the software components is collected and processed to tabulate means and variances. A table of DoE effects is generated from the tabulated means and variances as intervals to the desired level of confidence concerning whether the software component provided by the test algorithm can be used in place of or operates equivalently to a software component provided by the reference algorithm. A validation determination with respect to the test algorithm is made based on a selected confidence interval CI . If the CI is approximately centered on zero the validation determination is that there is no significant difference between the test algorithm and the reference algorithm and thus the test algorithm can be used by the SoC in place of the reference algorithm . However if the CI is not approximately centered on zero the validation determination is there is a significant difference between the test algorithm and the reference algorithm and thus the test algorithm cannot be used by the SoC in place of the reference algorithm . The CI value may be output as quantitative value indicative of the degree of difference between the test algorithm and the reference algorithm.

Reference is now made to which illustrates a functional block diagram of the validation engine of . The validation engine is a wrapping control algorithm over two software components the reference algorithm and the test algorithm.

The validation engine includes an algorithm block . The algorithm block stores both the reference algorithm and the test algorithm . In another implementation the testing of the reference algorithm as described herein is a historical action completed in the past with the resulting test data stored and thus the algorithm block includes just the test algorithm .

The validation engine further includes a test block . The test block comprises an execution block and an output block . The execution block functions to execute an algorithm selected from the algorithm block through algorithm application programming interface API . Execution of the selected algorithm comprises applying certain input data to the selected algorithm having the selected algorithm process the input data and further having the selected algorithm generate output data. The input data comprise certain reference Design of Experiments DoE treatments as a parameter set. A plurality of parameter sets M are loaded through a test interface into the execution block .

Reference is now made to which illustrates a flow diagram for operation of the validation engine . The reference algorithm is selected in step and loaded in step for execution by the execution block . Setup is initialized in step for a parameter set . The reference algorithm is then executed in step with the parameter set from the step initialization. Data output from the execution of the reference algorithm is then formatted in step and saved in step . The steps are then repeated as required so that each parameter set is evaluated and corresponding output data is collected.

Again the testing of the reference algorithm may comprise a historical action completed in the past. The output data from such testing is formatted in step and saved in step for use in making a validation determination with respect to the test algorithm as will now be described.

Next the test algorithm is selected in step and loaded in step for execution by the execution block . Setup is initialized in step for a parameter set . The test algorithm is then executed in step with the parameter set from the step initialization. Data output from the execution of the test algorithm is then formatted in step and saved in step . The steps are then repeated as required so that each parameter set is evaluated and corresponding output data is collected.

It will be noted that the same plurality of parameter sets M are used in the execution of the reference algorithm and in the execution of the test algorithm.

Reference is once again made to . The execution block of the validation engine includes input data storage for storing the plurality of parameter sets M . This input data storage need not be separate from the volatile non volatile memory of the SoC. The output block of the validation engine produces output data M corresponding to execution of the parameter sets M by the selected algorithm and collects that data in a dataset which contains all data generated by execution of the algorithm in response to the plurality of parameter sets M . Thus there is a first output dataset R produced containing the output data resulting from execution of the reference algorithm. Again the first output dataset R may comprise historical data. There is also a second output dataset T produced containing the output data resulting from execution of the test algorithm. The output datasets are stored in a data store as DoE formatted data . This output data store need not be separate from the volatile non volatile memory of the SoC.

In the event that each parameter set includes multiple samples for each parameter the algorithm is executed on those samples and the output data from execution by the execution block will be tabulated by the execution block to calculate mean and variance data which will comprise the output datasets .

Referring again to after the output data has been collected from execution of the parameter sets by both the reference algorithm and the test algorithm DoE processing is performed in step and DoE analysis for making the validation decision is performed in step .

With reference once again to the validation engine still further includes a DoE processing block which processes the DoE formatted data and performs the operations of steps and of . The DoE processing block functions to generate a table of DoE effects from the DoE formatted data for example from the tabulated means and variances as intervals to the desired level of confidence reference step of . This table data is then processed by the DoE processing block to make a validation decision reference based on a selected confidence interval CI step of . The validation results are provided at validation output . The validation output data may comprise for example pass fail information for the test algorithm i.e. pass if the test algorithm is can be used by the SoC in place of the reference algorithm and fail if not . The validation output data in such an implementation may function as a control signal which drives SoC acceptance of a proposed test algorithm. If the validation output is fail the SoC will continue to use the reference algorithm. However the validation output is pass the SoC will replace the reference algorithm with the test algorithm.

The validation decision reference based on a selected confidence interval CI may comprise an evaluation as to whether the CI is approximately centered on zero which would indicate that there is no significant difference between the test algorithm and the reference algorithm. Such a CI suggests that the test algorithm can be used by the SoC in place of the reference algorithm. A CI other than approximately centered on zero however would indicate that there is a significant difference between the test algorithm and the reference algorithm. In such case the test algorithm should not be used by the SoC in place of the reference algorithm.

In an alternative implementation the validation output may comprise the CI value itself as a quantitative value indicative of the degree of difference between the test algorithm and the reference algorithm.

The validation engine may be installed within the SoC and function in test mode such as a BIST mode to evaluate test algorithm in comparison to stored historical DoE information relating to the reference algorithm. Such a test mode may be initiated at start up of the SoC or initiated on a periodic basis. If the test mode execution indicates a negative validation decision reference with a validation output of fail the SoC may be disabled from operation or generate an error report or be enabled for limited operation. However if the validation output is pass the SoC will be enabled for full operation upon exit from test mode.

A more specific summary of the configuration of and processing performed by the validation engine is as follows 

4 Using the reference algorithm and test algorithm means and standard deviations generate a table of DoE effects

5 Generate validation result for example go no go based on predetermined significance level from DoE effects

With reference to the steps are performed in the above summarization in step as an initialization operation. As an alternative and as described above the steps for the reference algorithm may instead be performed in the context of evaluating the test algorithm.

Reference is now made to . There may occur instances when a given reference system is to be replaced by a new system or where a new system must be evaluated in comparison to the reference system. In connection with such a system replacement or evaluation a validation process may be performed by a validation engine to ensure that the new system functions in a manner which is consistent with the reference system with respect to common functionalities . A number of input settings are provided in accordance with a Design of Experiments DoE methodology and both the reference and new systems are operated in response to those input settings. Numerical output from each operation of the systems is collected and processed to tabulate means and variances. A table of DoE effects is generated from the tabulated means and variances as intervals to the desired level of confidence concerning whether the new system can be used in place of the reference system. A validation determination with respect to the new system is made based on a selected confidence interval CI . If the CI is approximately centered on zero the validation determination is that there is no significant difference between the new system and the reference system and thus the new system can be used in place of the reference system . However if the CI is not approximately centered on zero the validation determination is there is a significant difference between the new system and the reference system and thus the new system cannot be used in place of the reference system . The CI value may be output as quantitative value indicative of the degree of difference between the new system and the reference system.

The validation engine includes a test block . The test block comprises an execution block and an output block . The execution block functions to operate a system selected from the test bed through a system interface . Operation of the selected system comprises applying certain input data to the selected system having the selected system process or operate on the input data and further having the selected system generate output data. The input data comprise certain reference Design of Experiments DoE treatments as a parameter set. A plurality of parameter sets M are loaded through a test interface into the execution block .

Reference is now made to which illustrates a flow diagram for operation of the validation engine . The reference system is selected in step for operation under the control of the execution block . Setup is initialized in step for a parameter set . The reference system is then operated in step with the parameter set from the step initialization. Data output from the reference system in response to the test input is then formatted in step and saved in step . The steps are then repeated as required so that each parameter set is evaluated and corresponding output data is collected.

Again the testing of the reference system may comprise a historical action completed in the past. The output data from such testing is formatted in step and saved in step for use in making a validation determination with respect to the new system as will now be described.

Next the new system is selected in step for operation under the control of the execution block . This selection could be made for example in connection with a test mode or built in self test GIST functionality supported by the system. Setup is initialized in step for a parameter set . The new system is then operated in step with the parameter set from the step initialization. Data output from the new system in response to the test input is then formatted in step and saved in step . The steps are then repeated as required so that each parameter set is evaluated and corresponding output data is collected.

It will be noted that the same plurality of parameter sets M are used in the test operation of the existing reference system and in the test operation of the new system.

Reference is once again made to . The execution block of the validation engine includes input data storage for storing the plurality of parameter sets M . The output block of the validation engine produces output data M corresponding to execution of the parameter sets M by the selected system and collects that data in a dataset which contains all data generated by operation of the system in response to the plurality of parameter sets M . Thus there is a first output dataset R produced containing the output data resulting from execution of the reference system. Again the first output dataset R may comprise historical data. There is also a second output dataset N produced containing the output data resulting from execution of the new system. The output datasets are stored in a data store as DoE formatted data .

In the event that each parameter set includes multiple samples for each parameter the systems are operated in response to those samples and the output data from execution by the execution block will be tabulated by the execution block to calculate mean and variance data which will comprise the output datasets .

Referring again to after the output data has been collected from operation of the system s in response to the parameter sets DoE processing is performed in step and DoE analysis for making the validation decision is performed in step .

With reference once again to the validation engine still further includes a DoE processing block which processes the DoE formatted data and performs the operations of steps and of . The DoE processing block functions to generate a table of DoE effects from the DoE formatted data for example from the tabulated means and variances as intervals to the desired level of confidence reference step of . This table data is then processed by the DoE processing block to make a validation decision reference based on a selected confidence interval CI step of . The validation results are provided at validation output . The validation output data may comprise for example pass fail information for the new system i.e. pass if the operation of the new system is consistent with that of the reference system and fail if not . The validation output data in such an implementation may function as a control signal which enables of the new system for operation outside of a test mode i.e. enabled for normal operation . If the validation output is fail the new system may be disabled from operation or generate an error report or be enabled for limited operation. However the validation output is pass the new system will be enabled for full operation.

The validation decision reference based on a selected confidence interval CI may comprise an evaluation as to whether the CI is approximately centered on zero which would indicate that there is no significant difference between the operation of the new system and the operation of the reference system. Such a CI suggests that the new system can be used in place of or is functionally and operationally equivalent to the reference system. A CI other than approximately centered on zero however would indicate that there is a significant difference between operation of the new system and the reference system. In such case the new system should not be used in place of the reference system.

In an alternative implementation the validation output may comprise the CI value itself as a quantitative value indicative of the degree of difference between the new system and the reference system.

The validation engine may in an embodiment be installed within the new system itself and function in test mode such as a BIST mode to evaluate new system operation in comparison to stored historical DoE information relating to the reference system. Such a test mode may be initiated at start up of the system or initiated on a periodic basis. If the test mode execution indicates a negative validation decision reference with a validation output of fail the new system may be disabled from operation or generate an error report or be enabled for limited operation. However if the validation output is pass the new system will be enabled for full operation upon exit from test mode.

The foregoing description has provided by way of exemplary and non limiting examples a full and informative description of the exemplary embodiment of this invention. However various modifications and adaptations may become apparent to those skilled in the relevant arts in view of the foregoing description when read in conjunction with the accompanying drawings and the appended claims. However all such and similar modifications of the teachings of this invention will still fall within the scope of this invention as defined in the appended claims.

