---

title: Techniques for executing normally interruptible threads in a non-preemptive manner
abstract: A technique for executing normally interruptible threads of a process in a non-preemptive manner includes in response to a first entry associated with a first message for a first thread reaching a head of a run queue, receiving, by the first thread, a first wake-up signal. In response to receiving the wake-up signal, the first thread waits for a global lock. In response to the first thread receiving the global lock, the first thread retrieves the first message from an associated message queue and processes the retrieved first message. In response to completing the processing of the first message, the first thread transmits a second wake-up signal to a second thread whose associated entry is next in the run queue. Finally, following the transmitting of the second wake-up signal the first thread releases the global lock.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08856801&OS=08856801&RS=08856801
owner: International Business Machines Corporation
number: 08856801
owner_city: Armonk
owner_country: US
publication_date: 20120904
---
This application is a continuation of U.S. patent application Ser. No. 13 107 896 filed on May 14 2011 entitled Techniques For Executing Threads In A Computing Environment which is incorporated herein by reference in its entirety for all purposes.

The present invention relates in general to threads and in particular to techniques for executing threads in a computing environment.

The term utility computing has been used to refer to a computational model in which processing storage and network resources software and data are accessible to client computer systems and other client devices e.g. mobile phones or media players on demand much like familiar residential utility services such as water and electricity. In some implementations the specific computational resources e.g. servers storage drives etc. allocated for access and use by client devices are specified by service agreements between the utility computing provider and its customers. In other implementations commonly referred to as cloud computing details of the underlying information technology IT infrastructure are transparent to the utility computing customers.

Cloud computing is facilitated by ease of access to remote computing websites e.g. via the Internet or a private corporate network and frequently takes the form of web based resources tools or applications that a cloud consumer can access and use through a web browser as if the resources tools or applications were a local program installed on a computer system of the cloud consumer. Commercial cloud implementations are generally expected to meet quality of service QoS requirements of cloud consumers which may be specified in service level agreements SLAs . In a typical cloud implementation cloud consumers consume computational resources as a service and pay only for the resources used.

Adoption of utility computing has been facilitated by the widespread utilization of virtualization which is the creation of virtual rather than actual versions of computing resources e.g. an operating system a server a storage device network resources etc. For example a virtual machine VM also referred to as a logical partition LPAR is a software implementation of a physical machine e.g. a computer system that executes instructions like a physical machine. VMs can be categorized as system VMs or process VMs. A system VM provides a complete system platform that supports the execution of a complete operating system OS such as Windows Linux AIX Android etc. as well as its associated applications. A process VM on the other hand is usually designed to run a single program and support a single process. In either case any application software running on the VM is limited to the resources and abstractions provided by that VM. Consequently the actual resources provided by a common IT infrastructure can be efficiently managed and utilized through the deployment of multiple VMs possibly associated with multiple different utility computing customers.

The virtualization of actual IT resources and management of VMs is typically provided by software referred to as a VM monitor VMM or hypervisor. In various implementations a VMM may run on bare hardware Type 1 or native VMM or on top of an operating system Type 2 or hosted VMM .

In a typical virtualized computing environment VMs can communicate with each other and with physical entities in the IT infrastructure of the utility computing environment utilizing conventional networking protocols. As is known in the art conventional networking protocols are commonly premised on the well known seven layer Open Systems Interconnection OSI model which includes in ascending order physical data link network transport session presentation and application layers. VMs are enabled to communicate with other network entities as if the VMs were physical network elements through the substitution of a virtual network connection for the conventional physical layer connection.

The UNIX operating system OS was designed to be a portable multi tasking and multi user OS. UNIX is characterized by the use of plain text for storing data a hierarchical file system treating devices and certain types of inter process communications IPCs as files and the use of a large number of software tools. UNIX includes various utilities along with a master control program i.e. the UNIX kernel. The UNIX kernel provides services to start and stop programs handles the file system and other common low level tasks that most programs share and schedules access to hardware to avoid conflicts if multiple programs try to access the same resource or device simultaneously. Because of the acknowledged security portability and functionality of UNIX a large number of similar operating system have been developed including Linux Minix Mac OS FreeBSD NetBSD OpenBSD AIX Solaris and HP UX . All of these operating systems together with UNIX are referred to herein as Unix like OSs. Unix like OSs can be installed on a wide variety of hardware e.g. mobile phones tablet computers video game consoles networking gear mainframes and supercomputers.

In computer science a thread of execution thread is usually the smallest unit of processing that can be scheduled by an OS. A thread generally results from a fork of a computer program into two or more concurrently running tasks. The implementation of threads and processes may differ between OSs but in most cases a thread is included within a process. Multiple threads can exist within the same process and share resources e.g. memory while different processes do not share the same resources. On a single processor system multi threading is implemented through time division multiplexing i.e. a processor switches between executing different threads . On a multi processor or multi core system multiple threads may run at the same time on different processors or processor cores. Many modern OSs directly support both time sliced and multi processor threading with a process scheduler. The kernel of an OS allows programmers to manipulate threads via a system call interface.

Portable operating system interface for UNIX POSIX is the name of a family of related standards that define an application programming interface API along with shell and utilities interfaces for software compatible with Unix like OSs. POSIX threads pthreads refer to threads that are compliant with the POSIX standard for threads i.e. POSIX.1c IEEE 1003.1c 1995 which defines an API for creating and manipulating threads . Implementations of the POSIX API are available on many Unix like POSIX conformant OSs e.g. FreeBSD NetBSD GNU Linux Mac OS X and Solaris .

A technique for executing normally interruptible threads of a process in a non preemptive manner includes in response to a first entry associated with a first message for a first thread reaching a head of a run queue receiving by the first thread a first wake up signal. In response to receiving the wake up signal the first thread waits for a global lock. In response to the first thread receiving the global lock the first thread retrieves the first message from an associated message queue and processes the retrieved first message. In response to completing the processing of the first message the first thread transmits a second wake up signal to a second thread whose associated entry is next in the run queue. Finally following the transmitting of the second wake up signal the first thread releases the global lock.

According to the present disclosure techniques are disclosed that facilitate running legacy software i.e. a legacy bare metal OS on top of a Unix like OS such as Linux. In various cases running legacy software on top of a Unix like OS enables reuse of certain legacy code while improving scalability through access to a multi core multi processor OS. While the discussion herein is directed to running legacy software on top of a Unix like OS in a physical network switch that is implemented in a virtualized computing environment it is contemplated that the techniques disclosed herein are broadly applicable to virtualized and non virtualized computing environments and may be implemented in devices other than physical network switches. As used herein a stem thread refers to a legacy proprietary user space thread that ran non preemptively i.e. that was uninterruptible in a serialized manner in an order that messages were posted to a stem message queue provided by the legacy software . The legacy software also implemented a stem dispatcher that was in charge of processing stem messages and scheduling stem threads. As used herein the term pthreads by itself refers to Unix like OS threads that run preemptively i.e. interruptible in no particular order within a priority class and that are normally completely controlled by an OS scheduler of a Unix like OS. As is also used herein a stem pthread is a stem thread that has been converted to be similar to a pthread.

To ensure application code compatibility restrictions are enforced on stem pthreads that differentiate stem pthreads from pthreads. One restriction serializes stem pthreads by employing a global mutual exclusion mutex that functions as a global lock to cause stem pthreads to run like stem threads i.e. as uninterruptible threads . That is only one stem pthread i.e. the stem pthread with the global lock can process a message at any given time. Another restriction causes stem pthreads to self regulate by waiting after being woke up for a global lock before performing message processing. To ensure that stem pthreads get scheduled in the same order as the messages are posted to emulate the legacy software a single run queue is maintained for all stem pthreads with messages to be processed. In various embodiments all stem pthreads sleep while waiting for an associated message to reach a head of the run queue. In one or more embodiments each stem pthread is configured to run a continuous entry function that waits for a global lock process one message in an associated message queue upon receiving the global lock and then relinquish the global lock.

Before an active stem pthread goes back to sleep the active stem pthread is configured to check the run queue and wake up a stem pthread associated with a next entry in the run queue. This ensures that when the global lock is released by the active stem pthread a stem pthread associated with a next entry in the run queue retrieves the global lock and begins executing an associated message processing function. Before an active stem pthread goes back to sleep the active stem pthread is also configured to check an associated message queue and if the active stem pthread has additional messages to process the active stem pthread queues an entry for one of the additional messages to the end of the run queue. When the additional message reaches the head of the run queue at a subsequent time the stem pthread receives a wake up signal and begins processing the additional message. For example an entry in the run queue may correspond to a global identifier ID for a given stem pthread.

In contrast to the legacy software discussed above software configured according to the present disclosure does not implement stem thread dispatcher code as the Unix like OS scheduler performs basic thread scheduling. The Unix like OS scheduler maintains various background information on scheduled threads and an API layer that executes on top of the Unix like OS scheduler maintains housekeeping information e.g. whether a scheduled thread is a stem pthread or a pthread . According to the present disclosure each thread i.e. stem pthreads and pthreads includes code that is configured to process associated message queues and messages. Any new modules written for the system may employ pthreads that are not usually restricted. For example if pthreads implement their own locks the pthreads can run independently on the parts of the code that are not shared with common data structures. If a pthread needs to access any data structure that is shared with a stem pthread the pthread can be temporarily serialized by placing the pthread on the run queue and waiting for the global lock when the pthread is woken up.

Communication between pthreads and stem pthreads may be implemented in a number of different ways. For example inter thread communication may be achieved on the send side using stem messages which include a command and a message an OS interface OSIX event send call which includes a command and a message pointer for a thread with a single associated message queue or an OSIX event queue send call which includes a command and a message pointer for a thread with multiple associated message queues . In various embodiments a sending thread is configured to store a message for a receiving thread in a message queue associated with the receiving thread. In determining a message queue for a receiving thread a sending thread may for example employ an API to retrieve a global thread ID for the receiving thread which also indicates for example a location for a message queue for the receiving thread from a data structure e.g. a table which may be maintained by an API layer that executes on top of the Unix like OS scheduler .

In one or more embodiments stem pthreads receive messages i.e. stem messages or OSIX messages from a wrapper routine which dequeues the message from an associated message queue of the stem pthread and calls an entry routine of the stem pthread with the message as a parameter. The wrapper routine may for example be implemented as a Unix like OS module. In one or more embodiments pthreads wait on blocking condition variables when receiving messages. When a blocking condition variable is signaled a pthread reads an event and retrieves a message pointer to an associated message queue. In various embodiments the pthreads use an OSIX event receive call or an OSIX event queue receive call to retrieve a message pointer. In order to facilitate transparent thread communication from an application layer perspective applications may be configured to send stem messages or OSIX messages to any kind of receiver i.e. stem pthreads or pthreads . When a stem message is destined for a pthread the stem message is converted to an OSIX event condition and an OSIX queue message by a stem send routine which may be implemented as a Unix like OS module . When an OSIX message is destined for a stem pthread the OSIX message is converted to a stem message by an OSIX send routine which may be implemented as a Unix like OS module . This ensures that both stem pthreads and pthreads can maintain their receive processing in an unmodified state. Since the mechanism for message conversion is hidden with respect to application code application code does not require modification.

With reference now to the figures and with particular reference to there is illustrated a high level block diagram of an exemplary data processing environment in accordance within one embodiment of the present disclosure. As shown data processing environment which in the depicted embodiment is a cloud computing environment includes a collection of computing resources commonly referred to as a cloud . Computing resources within cloud are interconnected for communication and may be grouped not shown physically or virtually in one or more networks such as private community public or hybrid clouds or a combination thereof. In this manner data processing environment can offer infrastructure platforms and or software as services accessible to client devices such as personal e.g. desktop laptop netbook tablet or handheld computers smart phones server computer systems and consumer electronics such as media players e.g. set top boxes digital versatile disk DVD players or digital video recorders DVRs . It should be understood that the types of client devices shown in are illustrative only and that client devices can be any type of electronic device capable of communicating with and accessing services of computing resources via a packet network.

As depicted cloud includes a physical layer a virtualization layer a management layer and a workloads layer . Physical layer includes various physical hardware and software components that can be used to instantiate virtual entities for use by the cloud service provider and its customers. As an example the hardware components may include mainframes e.g. IBM zSeries systems reduced instruction set computer RISC architecture servers e.g. IBM pSeries systems IBM xSeries systems IBM BladeCenter systems storage devices e.g. flash drives magnetic drives optical drives tape drives etc. physical networks and networking components e.g. routers switches etc. . The software components may include operating system software e.g. AIX Windows Linux etc. network application server software e.g. IBM WebSphere application server software which includes web server software and database software e.g. IBM DB2 database software . IBM zSeries pSeries xSeries BladeCenter WebSphere and DB2 are trademarks of International Business Machines Corporation registered in many jurisdictions worldwide.

The computing resources residing in physical layer of cloud are virtualized and managed by one or more virtual machine monitors VMMs or hypervisors. The VMMs present a virtualization layer including virtual entities e.g. virtual servers virtual storage virtual networks including virtual private networks virtual applications and virtual clients. As discussed previously these virtual entities which are abstractions of the underlying resources in physical layer may be accessed by client devices of cloud consumers on demand.

The VMM s also support a management layer that implements various management functions for the cloud . These management functions can be directly implemented by the VMM s and or one or more management or service VMs running on the VMM s and may provide functions such as resource provisioning metering and pricing security user portal services service level management and SLA planning and fulfillment. The resource provisioning function provides dynamic procurement of computing resources and other resources that are utilized to perform tasks within the cloud computing environment. The metering and pricing function provides cost tracking as resources are provisioned and utilized within the cloud computing environment and billing or invoicing for consumption of the utilized resources. As one example the utilized resources may include application software licenses. The security function provides identity verification for cloud consumers and tasks as well as protection for data and other resources. The user portal function provides access to the cloud computing environment for consumers and system administrators. The service level management function provides cloud computing resource allocation and management such that required service levels are met. For example the security function or service level management function may be configured to limit deployment migration of a virtual machine VM image to geographical location indicated to be acceptable to a cloud consumer. The service level agreement SLA planning and fulfillment function provides pre arrangement for and procurement of cloud computing resources for which a future requirement is anticipated in accordance with an SLA.

Workloads layer which may be implemented by one or more consumer VMs provides examples of functionality for which the cloud computing environment may be utilized. Examples of workloads and functions which may be provided from workloads layer include mapping and navigation software development and lifecycle management virtual classroom education delivery data analytics processing and transaction processing.

With reference now to there is illustrated a high level block diagram of an exemplary data processing system that can be utilized to implement a physical host computing platform in physical layer of or a client device of . In the illustrated exemplary embodiment data processing system includes one or more network interfaces that permit data processing system to communicate with one or more computing resources in cloud via cabling and or one or more wired or wireless public or private local or wide area networks including the Internet . Data processing system additionally includes one or more processors that process data and program code for example to manage access and manipulate data or software in data processing environment . Data processing system also includes input output I O devices such as ports displays and attached devices etc. which receive inputs and provide outputs of the processing performed by data processing system and or other resource s in data processing environment . Finally data processing system includes data storage which may include one or more volatile or non volatile storage devices including memories solid state drives optical or magnetic disk drives tape drives etc. Data storage may store for example software within physical layer and or software such as a web browser that facilitates access to workloads layer and or management layer .

Referring now to there is depicted a high level block diagram of a portion of a data processing environment employing physical network switches configured in accordance with one or more embodiments of the present disclosure. For example data processing environment can implement a portion of cloud depicted in .

In the depicted embodiment data processing environment includes an Internet protocol IP network including a plurality of network segments each of which is coupled to a respective one of physical network switches . As is depicted each of physical network switches includes a respective data structure e.g. a respective forwarding table F by which physical network switches forward incoming data packets toward the packets destinations based upon for example OSI Layer 2 e.g. media access control MAC addresses contained in the packets. As is discussed in further detail with reference to physical network switches are configured to run legacy software on top of a Unix like OS. As noted above legacy stem threads are converted to stem pthreads and new modules are written to implement pthreads. Physical hosts are coupled to network segment and physical host is coupled to network segment . Each of physical hosts can be implemented for example utilizing a data processing system as depicted in .

Each of physical hosts executes a respective one of VMM which virtualizes and manages the resources of its respective physical host for example under the direction of a human and or automated cloud administrator at a management console coupled to physical hosts by IP network . VMM on physical host supports the execution of VMs VMM on physical host supports the execution of VMs and VMM on physical host supports the execution of VMs . It should be appreciated the while two VMs are illustrated as being deployed on each of physical hosts more or less than two VMs may be deployed on a physical host. In various embodiments VMs can include VMs of one or more cloud consumers and or a cloud provider. In the depicted embodiment each of VMs has one and may include multiple virtual network interface controller VNIC1 VNIC6 which provides network connectivity at least at Layers 2 and 3 of the OSI model.

VM utilizes VNIC1 to facilitate communication via a first port of virtual switch and VM utilizes VNIC2 to facilitate communication via a second port of virtual switch . For example when the first and second ports of virtual switch are configured as virtual Ethernet bridge VEB ports communications between VM and VM may be completely routed via software e.g. using memory copy operations . As another example when the first and second ports of virtual switch are configured as virtual Ethernet port aggregator VEPA ports communications between VM and VM are routed through physical NIC and on network segment to physical switch which routes the communications back to virtual switch via network segment and physical NIC . Similarly VM and VM utilize VNIC3 and VNIC4 respectively to facilitate communication via different ports of virtual switch . Likewise VM and VM utilize VNIC5 and VNIC6 respectively to communicate via different ports of virtual switch

Referring now to a relevant portion of physical network switch in data processing environment of is depicted in accordance with one embodiment of the present disclosure. As is illustrated physical network switch includes four ports labeled P1 P4 a crossbar switch a processor and a data storage e.g. a memory subsystem . While physical network switch is shown with four ports it should be appreciated that a physical network switch configured according to the present disclosure may include more or less than four ports. Processor which is coupled to crossbar switch controls crossbar switch to switch traffic between ports P1 P4.

Data storage includes a legacy OS that runs on top of a Unix like OS hereafter assumed to be Linux OS which implements a scheduler that employs a global mutex GM that functions as a global lock and an implementation appropriate number of applications . Processor executes a process which is illustrated as including five threads T1 T5. Thread T1 is shown in communication with threads T1 and T3. For example thread T1 may communicate a wake up signal to thread T2 see when thread T2 has a message whose associated entry is next in run queue . As another example thread T1 may communicate with thread T3 by storing a message for thread T3 in a message queue associated with thread T3 according to the process of . As is illustrated two message queues and run queue are allocated by Linux OS within data storage . While only two message queues are illustrated in it should be appreciated that more or less than two message queues may be implemented in a physical network switch configured according to the present disclosure. In a typical case each scheduled thread has at least one allocated message queue. Message queues are used to store messages for stem pthreads and may be used to store messages for Linux pthreads that have been serialized according to the present disclosure.

In one or more embodiments scheduler implements a routine that allocates GM to a single thread at any given point in time. The GM functions as a global lock to cause stem pthreads which are normally interruptible threads and Linux pthreads that have been serialized e.g. for accessing a common data structure with one or more stem pthreads to run like stem threads i.e. as uninterruptible serialized threads . As is illustrated in the exploded view of run queue run queue includes associated entries for five messages for four threads i.e. threads T1 T4 that are included in process . In the illustrated example thread T5 represents a regular Linux thread that is included in process that remains interruptible. As depicted an entry associated with a first message for thread T1 is allocated e.g. by scheduler at a head indicated with an arrow of run queue and an entry associated with a second message for thread T1 is allocated at a tail of run queue . Associated entries for threads T2 T3 and T4 respectively are allocated entries by scheduler in run queue between the head and the tail of run queue . As is discussed in further detail in messages for the threads T1 T4 are serially processed in the order of the associated entries in run queue .

As noted above to ensure compatibility with applications restrictions are enforced on stem pthreads that differentiate stem pthreads from Linux pthreads. To ensure that stem pthreads get scheduled in the same order as the messages are posted to emulate the legacy software an associated entry in run queue is maintained for each stem pthread message that requires processing. In one or more embodiments a stem pthread or serialized Linux pthread may have more than one associated message queue each of which has a different priority. In this case messages with a higher priority may be routed by for example a sending thread to a higher priority message queue and messages with a lower priority may be routed to a lower priority message queue. In this case messages in a higher priority message queue if any are processed before messages in a lower priority message queue.

Scheduler implemented within Linux OS maintains various background information on scheduled threads and an API layer that executes on top of the scheduler maintains housekeeping information e.g. whether a scheduled thread is a stem pthread or a Linux pthread . As noted above according to one or more aspects of the present disclosure when a thread is a stem pthread the thread always requires serialization. According to the present disclosure each thread i.e. stem pthreads and Linux pthreads includes code that is configured to process messages within message queues . As mentioned above new software modules that are added to physical network switch may employ Linux pthreads. As noted above when a Linux pthread needs to access any data structure that is shared with a stem pthread the Linux pthread can be temporarily serialized by creating an entry for the Linux pthread in run queue . For example threads T1 and T4 may be stem pthreads and threads T2 and T3 may be serialized Linux pthreads.

With reference now to there is illustrated a high level logical flowchart of an exemplary method of determining whether threads in a computing environment require serialization in accordance with one embodiment of the present disclosure. As previously mentioned threads that require serialization my be stem pthreads or Linux pthreads that require access to a data structure that is shared with a stem pthread. The flowchart of depicts steps in logical rather than strictly chronological order. Thus in at least some embodiments at least some steps of a logical flowchart can be performed in a different order than illustrated or concurrently. The process illustrated in can be performed by Linux OS of each physical network switch in data processing environment of .

The process begins at block and then proceeds to decision block where Linux OS determines whether a thread that is to be scheduled requires serialization. In response to thread not requiring serialization in block control transfers to block . In response to the thread requiring serialization in block control transfers to block . In block Linux OS allocates an entry for the thread in run queue . Then in block Linux OS allocates one or more message queues for the thread. For example a single message queue may be allocated when a thread does not receive messages with different priorities. As another example a high priority message queue and a low priority message queue may be allocated when a thread receives messages with two different priorities. Following block control passes to block where the process depicted in ends.

With reference now to there is illustrated a high level logical flowchart of an exemplary method of performing inter thread communication in a computing environment in accordance with one embodiment of the present disclosure. In this case the serialized threads may be stem pthreads or Linux pthreads that require access to a data structure that is shared with a stem pthread. The flowchart of depicts steps in logical rather than strictly chronological order. Thus in at least some embodiments at least some steps of a logical flowchart can be performed in a different order than illustrated or concurrently. The process illustrated in can be performed by for example each physical network switch in data processing environment of .

The process begins at block and then proceeds to decision block where a sending thread e.g. thread T1 determines whether an inter thread communication e.g. to share data is indicated. In response to an inter thread communication not being indicated in block control transfers to block where the process depicted in ends. In response to an inter thread communication being indicated in block control transfers to block where the sending thread retrieves e.g. from scheduler a global ID which indicates a particular message queue for a receiving thread e.g. thread T3 . Next in block the sending thread determines if a priority for the communication is indicated i.e. whether the thread has multiple associated message queues . Then in block the sending thread stores a message for the receiving thread in a message queue of the receiving thread. For example if a low priority message is indicated e.g. based on a traffic type the message for the receiving thread is stored by the sending thread in a low priority message queue. Similarly if a high priority message is indicated the message for the receiving thread is stored by the sending thread in a high priority message queue. Following block control passes to block where the process depicted in ends.

With reference now to there is illustrated a high level logical flowchart of an exemplary method of executing a serialized thread in a computing environment in accordance with one embodiment of the present disclosure. The serialized thread may be a stem pthread or a Linux pthread that requires serialization to for example access a data structure that is shared with a stem pthread. The flowchart of depicts steps in logical rather than strictly chronological order. Thus in at least some embodiments at least some steps of a logical flowchart can be performed in a different order than illustrated or concurrently. The process illustrated in can be performed by for example each physical network switch in data processing environment of .

The process begins at block and then proceeds to decision block where a sleeping thread e.g. thread T1 of process which may be a stem pthread or a Linux pthread that has been serialized determines whether a wake up signal has been received e.g. from another thread . As noted above a sleeping thread receives a wake up signal to process a message in response to an associated entry for the message reaching a head of run queue . In various embodiments the wake up signal for a sleeping thread is triggered following an event e.g. receipt of a message for the first thread from another thread arrival of a packet for the first thread a timer expiration associated with the first thread or establishment of a link for the first thread and is generated subsequent to the associated entry for the message for the sleeping thread reaching the head of run queue . Next in block the sleeping thread wakes up in response to receiving the wake up signal thus becoming an active thread. Then in block the active thread waits to receive the global lock GM from scheduler . Upon receiving GM from scheduler the active thread retrieves the message from an associated message queue in block and processes the retrieved message in block .

Next in block the active thread checks run queue for additional entries. Then in block the active thread determines whether run queue is empty. In response to run queue being empty in block control transfers to decision block . In response to run queue not being empty in block control transfers to block where the active thread wakes up another thread e.g. thread T2 having a message associated with a next entry in run queue . Following block control transfers to block . In block the active thread determines whether it has another message in an associated message queue to process. In response to the active thread not having another message to process in block control transfers to block where the active thread releases the global lock. In response to the active thread having another message to process in block control transfers to block where the active thread queues an associated entry for the message to an end of run queue .

Next in decision block the active thread determines whether it is the only thread with an entry in run queue . In response to the active thread determining that it is the only thread with an entry in run queue the active thread continues the process of at block where a next message in an associated message queue for the active thread is retrieved. In response to the active thread determining that it is not the only thread with an entry in run queue control transfers to block where the active thread releases the global lock. Next in block the active thread goes to sleep. Following block control passes to block where the process depicted in ends.

In the event that it is desirable for an active thread to release the global lock even when no other thread currently has a message to be processed and the active thread currently has another message to be processed blocks and may be omitted. In this case the output of block is routed directly to block .

Accordingly techniques have been disclosed herein that facilitate seamless communication between multiple heterogeneous execution threads within a single process. The disclosed techniques also facilitate operating Linux pthreads as freely schedulable threads or as stem threads i.e. Linux pthreads may serialized to be uninterruptible .

While the present invention has been particularly shown as described with reference to one or more preferred embodiments it will be understood by those skilled in the art that various changes in form and detail may be made therein without departing from the spirit and scope of the invention. For example it should be understood that although the detailed description provided herein provides multiple embodiments of cloud computing environments the teachings disclosed herein are not limited to cloud computing environments. Rather embodiments can be implemented in any other type of computing environment now known or later developed including client server and peer to peer computing environments. The disclosed techniques are broadly applicable to virtualized and non virtualized computing environments and may be implemented in devices e.g. host platforms other than physical network switches.

Further although aspects have been described with respect to computer systems executing program code that direct the functions described herein it should be understood that embodiments may alternatively be implemented as a program product including a storage medium e.g. data storage storing program code that can be processed by a data processing system to cause the data processing system to perform one or more of the described functions.

