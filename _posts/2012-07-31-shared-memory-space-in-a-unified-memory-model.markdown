---

title: Shared memory space in a unified memory model
abstract: Methods and systems are provided for mapping a memory instruction to a shared memory address space in a computer arrangement having a CPU and an APD. A method includes receiving a memory instruction that refers to an address in the shared memory address space, mapping the memory instruction based on the address to a memory resource associated with either the CPU or the APD, and performing the memory instruction based on the mapping.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09009419&OS=09009419&RS=09009419
owner: ATI Technologies ULC
number: 09009419
owner_city: Markham, Ontario
owner_country: CA
publication_date: 20120731
---
The present invention is generally directed to computer systems. More particularly the present invention is directed to architecture and methods for unifying computational components of a computer system.

The desire to use a graphics processing unit GPU for general computation has become much more pronounced recently due to the GPU s exemplary performance per unit power and or cost. The computational capabilities for GPUs generally have grown at a rate exceeding that of the corresponding central processing unit CPU platforms. This growth coupled with the explosion of the mobile computing market e.g. notebooks mobile smart phones tablets etc. and its necessary supporting server enterprise systems has been used to provide a specified quality of desired user experience. Consequently the combined use of CPUs and GPUs for executing workloads with data parallel content is becoming a volume technology.

However GPUs have traditionally operated in a constrained programming environment available primarily for the acceleration of graphics. These constraints arose from the fact that GPUs did not have as rich a programming ecosystem as CPUs. Their use therefore has been mostly limited to two dimensional 2D and three dimensional 3D graphics and a few leading edge multimedia applications which are already accustomed to dealing with graphics and video application programming interfaces APIs .

With the advent of multi vendor supported OpenCL and DirectCompute standard APIs and supporting tools the limitations of the GPUs in traditional applications has been extended beyond traditional graphics. Although OpenCL and DirectCompute are a promising start there are many hurdles remaining to creating an environment and ecosystem that allows the combination of a CPU and a GPU to be used as fluidly as the CPU for most programming tasks.

Existing computing systems often include multiple processing devices. For example some computing systems include both a CPU and a GPU on separate chips e.g. the CPU might be located on a motherboard and the GPU might be located on a graphics card or in a single chip package. Both of these arrangements however still include significant challenges associated with i efficient scheduling ii providing quality of service QoS guarantees between processes iii programming model iv compiling to multiple target instruction set architectures ISAs and v separate memory systems all while minimizing power consumption.

Many of the challenges associated with separate memory systems are minimized with the use of unified computing system environment. However within the unified computing system environments APD s and CPUs are unable to efficiently access a shared physical memory and execute one or more instructions that reference this shared memory space.

What is needed therefore is a method and system providing a shared address memory space accessible by a CPU and an APD so that either the CPU or the APD physical memory can execute a memory instruction that refers to the shared address memory space.

Although GPUs accelerated processing units APUs and general purpose use of the graphics processing unit GPGPU are commonly used terms in this field the expression accelerated processing device APD is considered to be a broader expression. For example APD refers to any cooperating collection of hardware and or software that performs those functions and computations associated with accelerating graphics processing tasks data parallel tasks or nested data parallel tasks in an accelerated manner compared to conventional CPUs conventional GPUs software and or combinations thereof. APD s combine heterogeneous i.e. dissimilar processors e.g. CPUs GPUs DSPs etc. into a singular system. Such singular systems may include as noted herein multiple separate devices or a singular device which incorporates the dissimilar processors into one package or one semiconductor integrated circuit.

Embodiments of the present invention provide under certain circumstances a method of mapping a memory instruction to a shared memory address space in a computer arrangement configured for unified operation between a CPU and an APD. The method includes receiving the memory instruction from an application wherein the memory instruction refers to an address in the shared memory address space and mapping the memory instruction to a memory resource based on the address wherein the memory resource is associated with at least one of the CPU and the APD. Information corresponding to the mapping is sent by a memory instruction mapper to the CPU or the APD where the memory instruction is performed by the CPU or the APD based on the mapping.

Additional features and advantages of the present invention as well as the structure and operation of various embodiments of the present invention are described in detail below with reference to the accompanying drawings. It is noted that the present invention is not limited to the specific embodiments described herein. Such embodiments are presented herein for illustrative purposes only. Additional embodiments will be apparent to persons skilled in the relevant art s based on the teachings contained herein.

In the detailed description that follows references to one embodiment an embodiment an example embodiment etc. indicate that the embodiment described may include a particular feature structure or characteristic but every embodiment may not necessarily include the particular feature structure or characteristic. Moreover such phrases are not necessarily referring to the same embodiment. Further when a particular feature structure or characteristic is described in connection with an embodiment it is submitted that it is within the knowledge of one skilled in the art to affect such feature structure or characteristic in connection with other embodiments whether or not explicitly described.

The term embodiments of the invention does not require that all embodiments of the invention include the discussed feature advantage or mode of operation. Alternate embodiments may be devised without departing from the scope of the invention and well known elements of the invention may not be described in detail or may be omitted so as not to obscure the relevant details of the invention. In addition the terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. For example as used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will be further understood that the terms comprises comprising includes and or including when used herein specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

In one example system also includes a system memory an operating system OS and a communication infrastructure . The OS and the communication infrastructure are described in greater detail below.

The system also includes a kernel mode driver KMD a software scheduler SWS and a memory management unit such as input output memory management unit IOMMU . CPU and APD can be implemented on a single integrated circuit chip or on multiple chips. A person skilled in the relevant art will appreciate that system may include one or more software hardware and firmware components in addition to or different from that shown in the embodiment shown in .

CPU can include not shown one or more of a control processor field programmable gate array FPGA application specific integrated circuit ASIC or digital signal processor DSP . CPU for example executes the control logic including the OS KMD SWS and applications that control the operation of computing system . In this illustrative embodiment CPU according to one embodiment initiates and controls the execution of applications by for example distributing the processing associated with that application across the CPU and other processing resources such as the APD . CPU can include one or more single or multi core CPUs.

APD includes its own compute units not shown such as but not limited to one or more single instruction multiple data SIMD processing cores. Each APD compute unit can include one or more of scalar and or vector floating point units and or arithmetic and logic units ALU . The APD compute unit can also include special purpose processing units not shown such as inverse square root units and sine cosine units. The APD compute units are referred to herein collectively as shader core .

Having one or more SIMD compute units in general makes APD ideally suited for execution of data parallel tasks such as are common in graphics processing.

A set of related operations executed on a compute unit can also be referred to as a compute kernel. In graphics pipeline operations such as pixel processing and other parallel computation operations can require that the same instruction stream or compute kernel can be performed on streams or collections of input data elements. Respective instantiations of the same compute kernel can be executed concurrently on multiple compute units in shader core in order to process such data elements in parallel. A single data item within a stream or collection to which a compute kernel is applied is referred to as a work item. A set of work items across which the instructions of a compute kernel are applied in lock step within a single SIMD processing core is referred to as a thread. Stated another way the term thread refers to a single instance of a program execution with a unique data state.

In an illustrative embodiment each compute unit e.g. SIMD processing core can execute a respective instantiation of a particular thread or process to process incoming data.

A group of threads that are processed under a shared instruction state in a SIMD style process are referred to as a wavefront. For example shader core can simultaneously execute a predetermined number of wavefronts each wavefront comprising a predetermined number of threads.

APD includes its own memory such as graphics memory . Graphics memory provides a local memory for use during computations in APD and each compute unit of the shader core may have its own local data store not shown . In one embodiment APD can include access to local graphics memory as well as access to the system memory . In another embodiment APD can also include access to dynamic random access memory DRAM or other such memories attached directly to the APD separately from system memory .

APD also includes a command processor CP . CP controls the processing within APD . CP also retrieves instructions to be executed from command buffers in system memory and coordinates the execution of those instructions on APD .

In one example CPU inputs commands based on applications into appropriate command buffers . A plurality of command buffers can be maintained with each process scheduled for execution on the APD having its own command buffer .

Command processor can be implemented in hardware firmware or software or a combination thereof. In one embodiment command processor is implemented as a RISC engine with microcode for implementing logic including scheduling logic.

APD may also include a dispatch controller . Dispatch controller includes logic to initiate threads and wavefronts in the shader core. In some embodiments dispatch controller can be implemented as part of command processor .

System also includes a hardware scheduler HWS for selecting a process from a run list for execution on APD . HWS can select processes from run list using round robin methodology based upon priority level or based on other scheduling policies. By way of example the priority level can be dynamically determined. HWS can also include functionality to manage the run list for example by adding new processes and by deleting existing processes from a run list. The run list management logic of HWS is sometimes referred to as a run list controller RLC .

In various embodiments of the present invention when HWS initiates the execution of a process from run list CP begins retrieving and executing instructions from the corresponding command buffer . In some instances command processor can generate one or more commands to be executed within APD which correspond with each command received from CPU . In one embodiment command processor together with other components implements a prioritizing and scheduling of commands on APD in a manner that improves or maximizes the utilization of the resources of APD resources and or system .

APD can have access to or may include an interrupt generator . Interrupt generator can be configured by APD to interrupt the OS when interrupt events such as page faults are encountered by APD . For example APD can rely on interrupt generation logic within IOMMU to create the page fault interrupts noted above.

APD can also include preemption and context switch logic which includes logic to preempt a process currently running within shader core . More specifically context switch logic can include functionality to coordinate the preemption for example by stopping the process and saving the current state of the process e.g. shader core state CP state .

Preemption and context switch logic can also include logic to context switch another process into the APD . The functionality to context switch another process into running on the APD may include instantiating the process for example through the command processor and dispatch controller to run on APD restoring any previously saved state for that process and starting its execution.

System memory includes non persistent memory such as DRAM. System memory can store e.g. processing logic instructions constant values and variable values during execution of portions of applications or other processing logic. For example in one embodiment parts of control logic to perform one or more operations on CPU can reside within system memory during execution of the respective portions of the operation by CPU . The term processing logic or logic as used herein refer to control flow instructions instructions for performing computations and instructions for associated access to resources.

During execution respective applications OS functions processing logic instructions and system software can reside in system memory . Control logic instructions fundamental to OS will generally reside in system memory during execution. Other software instructions including for example kernel mode driver and software scheduler can also reside in system memory during execution of system .

System memory includes command buffers that are used by CPU to send commands to APD . System memory also contains process lists and process information e.g. active list and process control blocks . These lists as well as the information are used by scheduling software executing on CPU to communicate scheduling information to APD and or related scheduling hardware. Access to system memory can be managed by a memory controller which is coupled to system memory . For example requests from CPU or from other devices for reading from or for writing to system memory are managed by the memory controller .

IOMMU is a multi context memory management unit. IOMMU includes logic to perform virtual to physical address translation for memory page access for devices including APD . IOMMU may also include logic to generate interrupts for example when a page access by a device such as APD results in a page fault. IOMMU may also include or have access to a translation lookaside buffer TLB . TLB as an example can be implemented in a content addressable memory CAM to accelerate translation of logical i.e. virtual memory addresses to physical memory addresses for requests made by APD for data in system memory .

Communication infrastructure interconnects the components of system as needed. Communication infrastructure can include not shown one or more of a Peripheral Component Interconnect PCI bus extended PCI PCI E bus Advanced Microcontroller Bus Architecture AMBA bus Advanced Graphics Port AGP or such communication infrastructure. Communications infrastructure can also include an Ethernet or similar network or any suitable physical communications infrastructure that satisfies an application s data transfer rate requirements. Communication infrastructure includes the functionality to interconnect components including components of computing system .

OS includes components and software firmware providing functionality to manage the hardware components of system and to provide common services. In various embodiments processes defined by OS can execute on CPU and provide common services. These common services can include for example scheduling applications for execution within CPU fault management interrupt service as well as processing the input and output of other applications.

In various embodiments based on interrupts generated by an interrupt controller such as interrupt controller OS invokes an appropriate interrupt handling routine. For example upon detecting a page fault interrupt OS may invoke an interrupt handler to initiate loading of the relevant page into system memory and to update corresponding page tables.

OS is configured to have functionality to protect system by ensuring that access to hardware components is mediated through OS managed kernel functionality. In effect OS ensures that applications such as applications run on CPU in user space. OS also ensures that applications invoke kernel functionality provided by the OS to access hardware and or input output functionality. According to an embodiment of the present invention the OS includes an OS memory manager and an OS scheduler . OS memory manager has the functionality required to manage memory objects such as but not limited to page tables and page event queues . Page tables are tables that indicate the location of pages currently loaded in memory. Page event queue is a queue in which page related events such as page fault events are enqueued by other devices such as IOMMU in order to communicate page related information to the OS. OS scheduler includes the functionality according to an embodiment to determine the status of page faults and to determine if a GPU context switch should be initiated in response to a page fault.

By way of example applications include various programs or instructions to perform user computations which are also executed on CPU . The unification concepts of embodiments of this invention enable in certain circumstances CPU to seamlessly send selected instructions for processing on the APD . Under this unified APD CPU framework input output requests from applications will be processed through corresponding OS functionality.

KMD implements an application program interface API through which CPU or applications executing on CPU or other logic can invoke APD functionality. For example KMD can enqueue commands from CPU to command buffers from which APD will subsequently retrieve the commands. Additionally KMD can together with SWS perform scheduling of processes to be executed on APD . SWS for example can include logic to maintain a prioritized list of processes to be executed on the APD.

In other embodiments of the present invention applications executing on CPU can entirely bypass KMD when enqueuing commands.

In some embodiments SWS maintains an active list in system memory of processes to be executed on APD . SWS also selects a subset of the processes in active list to be managed by HWS in the hardware. In an illustrative embodiment this two level run list of processes increases the flexibility of managing processes and enables the hardware to rapidly respond to changes in the processing environment. In another embodiment information relevant for running each process on APD is communicated from CPU to APD through process control blocks PCB .

Processing logic for applications OS and system software can include instructions specified in a programming language such as C and or in a hardware description language such as Verilog RTL or netlists to enable ultimately configuring a manufacturing process through the generation of maskworks photomasks to generate a hardware device embodying aspects of the present invention described herein.

A person skilled in the relevant art will understand upon reading this description that computing system can include more or fewer components than shown in . For example computing system can include one or more input interfaces non volatile storage one or more output interfaces network interfaces and one or more displays or display interfaces.

In graphics pipeline can include a set of blocks referred to herein as ordered pipeline . As an example ordered pipeline includes a vertex group translator VGT a primitive assembler PA a scan converter SC and a shader export render back unit SX RB . Each block within ordered pipeline may represent a different stage of graphics processing within graphics pipeline . Ordered pipeline can be a fixed function hardware pipeline. Other implementations can be used that would also be within the spirit and scope of the present invention.

Although only a small amount of data may be provided as an input to graphics pipeline this data will be amplified by the time it is provided as an output from graphics pipeline . Graphics pipeline also includes DC for counting through ranges within work item groups received from CP pipeline . Compute work submitted through DC is semi synchronous with graphics pipeline .

Compute pipeline includes shader DCs and . Each of the DCs and is configured to count through compute ranges within work groups received from CP pipelines and

The DCs and illustrated in receive the input ranges break the ranges down into workgroups and then forward the workgroups to shader core .

Since graphics pipeline is generally a fixed function pipeline it is difficult to save and restore its state and as a result the graphics pipeline is difficult to context switch. Therefore in most cases context switching as discussed herein does not pertain to context switching among graphics processes. An exception is for graphics work in shader core which can be context switched.

After the processing of work within graphics pipeline has been completed the completed work is processed through a render back unit which does depth and color calculations and then writes its final results to memory .

Shader core can be shared by graphics pipeline and compute pipeline . Shader core can be a general processor configured to run wavefronts. In one example all work within compute pipeline is processed within shader core . Shader core runs programmable software code and includes various forms of data such as state data.

Generally speaking some embodiments described herein use memory instruction mapper and shared memory address space to map memory instructions generated by CPU and APD to physical memory resources. As noted with the description of above in an embodiment portions of both the CPU and APD physical memory are available for use by memory instructions. For example using approaches detailed herein a CPU thread is not limited to only accessing CPU physical memory. A memory instruction running on a APD thread is also not limited to only accessing APD physical memory .

As noted in the description of one approach to enabling the sharing of physical memory detailed above uses a shared memory address space to access different physical memory resources. For example rather than having an application use the conventional approach of explicitly marshalling memory between a CPU virtual address space and a APD virtual address space the shared memory address space described herein can be used. An application using both a CPU and a APD for execution can have both CPU and APD memory instructions accessing the same shared memory address space.

One approach to enabling the shared memory address space detailed herein is to use a memory instruction mapper to map an instruction to a physical memory CPU or APD based on the shared memory address referenced. For example in a shared memory address space having addresses from the memory instruction mapper can be configured to map addresses to a CPU physical memory and addresses to an APD physical memory .

According to a conventional approach to enable a use of physical memories as described above a memory instruction is copied and transferred from one physical memory portion to another pre allocated physical memory portion. After processing results are transferred back to an accessible memory portion. In contrast to this conventional copy and transfer approach embodiments of the present invention describe a memory instruction mapping system and method.

In an embodiment application generates memory instruction . As noted above in the description of embodiments of the present invention allow applications to utilize both CPU and APD in parallel using parallel programming techniques. In a non limiting illustrative example application is written in C or other high level x86 64 programming language.

In embodiments memory instruction originates in either an APD thread or a CPU thread. One example of an APD thread is used in application where application is a graphics based application. One example of a CPU thread is used in application where application is a compute based application. In a conventional approach memory instruction operating on a CPU thread would be executed by CPU while memory instruction operating on an APD thread would be executed by APD . In contrast to this conventional approach in embodiments using the shared memory address approaches described herein memory instruction can be executed by either APD or CPU regardless of from which thread memory instruction originated.

In an embodiment memory instruction is mapped by memory instruction mapper into CPU physical memory or APD physical memory . Memory instruction references an address in shared memory address space . Based on the address referenced by memory instruction in shared memory address space memory instruction mapper maps memory instruction to either CPU or APD to execute memory instruction .

Memory instruction references an address in shared memory address space that requires the execution of memory instruction by APD . In another embodiment memory instruction references an address in shared memory address space that requires the execution of memory instruction by CPU . In another embodiment memory instruction references an address in shared memory address space that allows execution of memory instruction by either CPU or APD . As would be appreciated by one having skill in the relevant art s given the description herein shared memory address space allows for example a pointer in application to access both CPU physical memory and APD physical memory in shared memory address space . In application a pointer that references shared memory address space resolves to the same physical memory address regardless of whether CPU or APD is used.

Because application operates in a single shared memory address space the conventional need for multiple representations of addresses is removed. In an embodiment shared memory address space is referenced by a full 64 bit virtual address. In another embodiment shared memory address space is internally limited to 48 bits e.g. sign extended to 64 bits from bit in the same manner as a x86 64 CPU.

The computer arrangement described herein uses different memory heaps. One approach to accessing the above noted memory heaps is to use memory instruction mapper to map to the memory heaps with shared memory address space . Memory heaps mapped into shared memory address space by embodiments can be accessed by memory instruction .

In an embodiment memory instruction mapper uses IOMMU to access physical memory resources. As described in above IOMMU includes logic to perform virtual to physical address translation for memory page access for devices including APD . One approach used by IOMMU to enable the use of shared memory address space uses full x86 page tables to allow x86 user code and APD code to share the same memory page tables. Because of this page table sharing in an embodiment an APD context corresponds to standard x86 user context. Using the IOMMU each APD context participates fully in paging translations and protections. APD is enabled to use standard TLB translation caching techniques. Because of this expanded use of page tables by embodiments OS may be required to propagate page invalidations and page table flushes to IOMMU .

Memory instruction mapper maps memory instruction that references shared address in shared memory address space into either physical memory of CPU or APD . As would also be appreciated by one having skill in the relevant art s given the description herein physical memory of CPU or APD are represented by memory heaps that are accessible by a unique shared address in shared memory address space . Memory heaps that are accessible by unique shared address in shared memory address space allow pointers to be passed directly between CPU and APD without modification. Memory heaps that are accessible by unique shared address in shared memory address space allow all pointers to be stored in generic pointer containers native to a given language e.g. void in C .

As would be appreciated by one having skill in the relevant art s given the description herein not all memory heaps are accessible by CPU and APD . In an embodiment if CPU attempts to reference an inaccessible memory heap using a mapped address in shared memory address space CPU is required to take a protection fault. Because in an embodiment not all memory heaps are accessible to CPU and APD library calls can be supplied to guide and control memory placement. In another embodiment application is prevented from accessing memory allocated to only APD from CPU . Similarly in yet another embodiment application is prevented from accessing memory allocated to only CPU from APD .

Physical memory in CPU or APD executes memory instruction based memory selector that determines which physical memory will execute memory instruction . Memory instruction is mapped based on that selection by memory instruction mapper into and executed by virtual memory heaps located in CPU or APD .

G1. Local Data Store As shown in Table 1 below a local data store memory heap is a private memory region accessible only by APD . Local data store is private to a work group. The memory size allocated to local data store varies on an application basis. Load data store is pinned memory.

G2. Global Data Store As shown in Table 1 below a global data store memory heap is a shared memory region accessible only by APD . Global data store is private to application . The memory size allocated to global data store varies on an application basis. Global data store is pinned.

G3. Scratch Memory As shown in Table 1 below a scratch memory heap is accessible by APD . Scratch memory can be used as a per work item extension to APD general purpose registers. Addresses in a scratch aperture undergo a transformation of their virtual addresses such that each location in the virtual address space is replicated and private to the work item. The resultant address maps to an APD local memory APD coherent memory or system coherent memory.

G4. APD Local Memory As shown in Table 1 below this memory heap is exclusively accessible by APD and mapped using APD internal virtual memory hardware. In different implementations portions of APD can be mapped to APD DRAM or to unsnooped system Memory. The x86 page table entries corresponding to APD local memory addresses are marked as not present.

G5. System Coherent Memory As shown in Table 1 below the system coherent memory heap is accessible by both CPU and APD and is located in memory connected to the CPU memory controller. In different implementations the system coherent memory heap maps into shared memory address space using x86 page tables is not required to be pinned and can generate memory page faults. In an implementation OS memory manager manages memory allocation and page faults for the system coherent memory heap.

G6. APD Coherent Memory As shown in Table 1 below this memory heap is accessible by both CPU and APD . The APD coherent memory heap is not required to be pinned and can generate memory page faults. In implementations APD coherent memory is located in APD . x86 page tables are not placed in APD coherent memory.

Example memory heaps G1 G6 detailed above are intended to be a non limiting illustration of different implementation characteristics. As would be appreciated by one having skill in the relevant art s given the description herein without departing from the spirit of embodiments herein any characteristic noted above can be beneficially varied based on implementation specific factors. Additional memory heap structures may also be used by embodiments.

As discussed with the descriptions of below different approaches may be used for the mapping of memory instruction to available physical memory resources. In for example memory instruction originating in an APD thread is mapped to an APD private memory using a video memory manager. In for example memory instruction originating in an APD thread is mapped to APD coherent memory using the IOMMU described above. In memory instruction originating in an APD thread is mapped to shared system memory using the IOMMU. In memory instruction originating in a CPU thread is mapped to private system memory using a CPU memory manager. memory instruction originating in a CPU thread is mapped to shared system memory using the IOMMU.

System includes APD thread relaying memory instruction to computer arrangement . Computer arrangement includes memory instruction mapper CPU system memory APD IOMMU and OS . CPU includes CPU memory manager system memory includes private system memory and shared system memory and APD includes video memory manager APD private memory and APD coherent memory .

Memory instruction originates with APD thread and contains an address reference to shared memory address space . Memory instruction mapper uses the included address reference to map memory instruction to APD private memory . As described above the particular shared memory address referenced by memory instruction is used by memory instruction mapper to direct the execution of memory instruction to a particular physical memory address accessible by computer arrangement .

Referring to and based on the address referenced by memory instruction in shared memory address space memory selector selects APD private memory for the execution of memory instruction . Mapping determiner maps the memory instruction to a physical memory resource based on the reference to shared memory address space .

In an example address range to in shared memory address space is defined as the portion of the shared memory address space allocated to APD private memory . Memory instruction refers to address . Memory instruction thus references an address in the above defined shared memory address space that is allocated to APD private memory and memory instruction mapper maps memory instruction to that memory resource.

As further depicted on one approach used to map to APD private memory is to refer memory instruction to video memory manager . In an embodiment memory instruction mapper has specific memory access pathways defined and mapping determiner uses such pathways to direct memory instructions from particular thread types to particular memory managers and to particular physical memory resources.

In the illustrative example depicted on the use of video memory manager is determined by memory instruction mapper based on the shared memory address reference in memory instruction . In another embodiment video memory manager is directed by memory instruction mapper to access APD private memory . In yet another embodiment video memory manager receives memory instruction based on memory instruction mapper and then selects APD private memory based on other criteria e.g. the shared memory address the originating thread or the type of memory instruction.

For it should be appreciated that though memory instruction mapper is depicted in a pathway from originating threads and to the memory mapping destinations e.g. APD memory instruction mapper can be used as a reference component that directs each thread to directly access the physical memory resources. In an example not shown APD thread having memory instruction can inquire directly to memory instruction mapper to request a mapping path for memory instruction memory access. Receiving the shared memory address reference from APD thread memory instruction mapper performs the above described mapping function and responds directing APD thread to use video memory manager and APD private memory .

Adding further detail to the example depicted in APD thread is a graphics based application thread originated by a graphics application not shown . Memory instruction is a graphics based instruction and the address that memory instruction refers to in shared memory address space defines an address in shared memory address space that is to be executed by APD private memory .

Memory instruction originates with APD thread and contains an address reference to shared memory address space . Memory instruction mapper uses the included address reference to map memory instruction to APD coherent memory . As described above the particular shared memory address referenced by memory instruction is used by memory instruction mapper to direct the execution of memory instruction to a particular physical memory address accessible by computer arrangement .

Referring to and based on the address referenced by memory instruction in shared memory address space memory selector selects APD coherent memory for the execution of memory instruction . Mapping determiner maps the memory instruction to a physical memory resource based on the reference to shared memory address space .

In an example address range to in shared memory address space is defined as the portion of the shared memory address space allocated to APD coherent memory . Memory instruction refers to address . Memory instruction thus references an address in the above defined shared memory address space that is allocated to APD coherent memory and memory instruction mapper maps memory instruction to that memory resource.

As further depicted on one approach used to map to APD coherent memory is to refer memory instruction to IOMMU . As noted above with the description of in embodiments memory instruction mapper has specific memory access pathways defined and mapping determiner uses such pathways to direct memory instructions from particular thread types to particular memory managers and to particular physical memory resources.

In the illustrative example depicted on the use of IOMMU is determined by memory instruction mapper based on the shared memory address reference in memory instruction . In another embodiment IOMMU is directed by memory instruction mapper to map memory instruction to APD coherent memory . In yet another embodiment IOMMU receives memory instruction based on memory instruction mapper and then selects APD coherent memory based on other criteria e.g. the shared memory address the originating thread or the type of memory instruction.

Memory instruction may be a conventional instruction e.g. an x86 instruction which can be executed using either a CPU thread or an APD thread. Because the instruction can be executed by either type of thread the shared memory address reference that memory instruction refers to an address in shared memory address space that enables the alternative use of multiple physical memory resources.

In this example the shared address reference defines an address in shared memory address space that enables the use of either APD coherent memory or shared system memory . As noted above APD coherent memory is a shared memory in APD that can be accessed by both APD and CPU and shared system memory is a shared memory in system memory that can be accessed by both APD and CPU . In an embodiment IOMMU is configured to select from the available memory resources based on different criteria.

Based on the address referenced by memory instruction in this example memory selector selects IOMMU to determine whether APD coherent memory or shared system memory should be provided to memory instruction . In an embodiment IOMMU selects APD coherent memory to execute memory instruction . In an embodiment IOMMU selects from available memory resources using the shared address reference while in other embodiments different criteria are used e.g. performance enhancement.

In yet another embodiment OS memory manager manages IOMMU to ensure IOMMU maps the page tables for memory instruction properly and ensures that CPU and APD recognize the mappings. Additional detail describing the operation of IOMMU is provided in the IOMMU Applications noted above.

System includes APD thread relaying memory instruction to computer arrangement . Computer arrangement includes memory instruction mapper CPU system memory APD IOMMU and OS . CPU includes CPU memory manager system memory includes private system memory and shared system memory and APD includes video memory manager APD private memory and APD coherent memory .

Memory instruction originates with APD thread and contains an address reference to shared memory address space . Memory instruction mapper uses the included address reference to map memory instruction to shared system memory . As described above the particular shared memory address referenced by memory instruction is used by memory instruction mapper to direct the execution of memory instruction to a particular physical memory address accessible by computer arrangement .

Referring to and based on the address referenced by memory instruction in shared memory address space memory selector selects shared system memory for the execution of memory instruction . Mapping determiner maps the memory instruction to a physical memory resource based on the reference to shared memory address space .

In an example address range to in shared memory address space is defined as the portion of the shared memory address space allocated to shared system memory . Memory instruction refers to address . Memory instruction thus references an address in the above defined shared memory address space that is allocated to shared system memory and memory instruction mapper maps memory instruction to that memory resource.

As further depicted on one approach used to map to shared system memory is to refer memory instruction to IOMMU . As noted above with the description of in embodiments memory instruction mapper has specific memory access pathways defined and mapping determiner uses such pathways to direct memory instructions from particular thread types to particular memory managers and to particular physical memory resources.

In the illustrative example depicted on the use of IOMMU is determined by memory instruction mapper based on the shared memory address reference in memory instruction . In another embodiment IOMMU is directed by memory instruction mapper to map memory instruction to shared system memory . In yet another embodiment IOMMU receives memory instruction based on memory instruction mapper and then selects shared system memory based on other criteria e.g. the shared memory address the originating thread or the type of memory instruction. As noted above with the description of an example of the operation and functions of an IOMMU operating within a similar computer arrangement can be found in the IOMMU Applications.

Adding further detail to the example depicted in memory instruction is a conventional instruction e.g. an x86 instruction which can be executed using either a CPU thread or an APD thread. Because the instruction can be executed by either type of thread the shared memory address reference that memory instruction refers to an address in shared memory address space that enables the alternative use of multiple physical memory resources.

In this example the shared address reference defines an address in shared memory address space that enables the use of either APD coherent memory or shared system memory . As noted above APD coherent memory is a shared memory in APD that can be accessed by both APD and CPU and shared system memory is a shared memory in system memory that can be accessed by both APD and CPU . In an embodiment IOMMU is configured to select from the available memory resources based on different criteria.

Based on the address referenced by memory instruction in this example memory selector selects IOMMU to determine whether APD coherent memory or shared system memory should be provided to memory instruction . In an embodiment IOMMU selects APD coherent memory to execute memory instruction . In an embodiment IOMMU selects from available memory resources using the shared address reference while in other embodiments different criteria are used e.g. performance enhancement.

In yet another embodiment OS memory manager manages IOMMU to ensure IOMMU maps the page tables for memory instruction properly and ensures that CPU and APD recognize the mappings. Additional detail describing the operation of IOMMU is provided in the IOMMU Applications noted above.

System includes CPU thread relaying memory instruction to computer arrangement . Computer arrangement includes memory instruction mapper CPU system memory APD IOMMU and OS . CPU includes CPU memory manager system memory includes private system memory and shared system memory and APD includes video memory manager APD private memory and APD coherent memory .

Memory instruction originates with CPU thread and contains an address reference to shared memory address space . Memory instruction mapper uses the included address reference to map memory instruction to private system memory . As described above the particular shared memory address referenced by memory instruction is used by memory instruction mapper to direct the execution of memory instruction to a particular physical memory address accessible by computer arrangement .

Referring to and based on the address referenced by memory instruction in shared memory address space memory selector selects private system memory for the execution of memory instruction . Mapping determiner maps the memory instruction to a physical memory resource based on the reference to shared memory address space .

In an example address range to in shared memory address space is defined as the portion of the shared memory address space allocated to private system memory . Memory instruction refers to address . Memory instruction thus references an address in the above defined shared memory address space that is allocated to private system memory and memory instruction mapper maps memory instruction to that memory resource.

As further depicted on one approach used to map to private system memory is to refer memory instruction to CPU memory manager . As noted above with the description of in embodiments memory instruction mapper has specific memory access pathways defined and mapping determiner uses such pathways to direct memory instructions from particular thread types to particular memory managers and to particular physical memory resources.

In the illustrative example depicted on the use of CPU memory manager is determined by memory instruction mapper based on the shared memory address reference in memory instruction . In another embodiment CPU memory manager is directed by memory instruction mapper to map memory instruction to private system memory . In yet another embodiment CPU memory manager receives memory instruction based on memory instruction mapper and then selects private system memory based on other criteria e.g. the shared memory address the originating thread or the type of memory instruction.

Memory instruction is a conventional instruction e.g. an x86 instruction which using computer arrangement requires access to private system memory . The address that memory instruction refers to in shared memory address space defines an address in shared memory address space maps to private system memory . As noted above with the description of memory heaps private system memory is a private memory accessible only to CPU .

In an embodiment CPU device driver allocates a piece of private system memory to execute memory instruction at a specific address. CPU device driver sends a request to OS memory manager as described in to execute the allocation. OS memory manager determines the shared address to tag to memory instruction . OS memory manager manages CPU memory manager to ensure CPU memory manager maps the page tables for memory instruction properly and ensures that CPU and APD recognize the mappings.

System includes CPU thread relaying memory instruction to computer arrangement . Computer arrangement includes memory instruction mapper CPU system memory APD IOMMU and OS . CPU includes CPU memory manager system memory includes private system memory and shared system memory and APD includes video memory manager APD private memory and APD coherent memory .

Memory instruction originates with CPU thread and contains an address reference to shared memory address space . Memory instruction mapper uses the included address reference to map memory instruction to APD coherent memory . As described above the particular shared memory address referenced by memory instruction is used by memory instruction mapper to direct the execution of memory instruction to a particular physical memory address accessible by computer arrangement .

Referring to and based on the address referenced by memory instruction in shared memory address space memory selector selects APD coherent memory for the execution of memory instruction . Mapping determiner maps the memory instruction to a physical memory resource based on the reference to shared memory address space .

As noted with the example described with address range to in shared memory address space is defined as the portion of the shared memory address space allocated to APD coherent memory . Memory instruction refers to address . Memory instruction thus references an address in the above defined shared memory address space that is allocated to APD coherent memory and memory instruction mapper maps memory instruction to that memory resource.

In contrast to the example detailed with the description of where memory instruction originates with APD thread in memory instruction originates with CPU thread . It should be noted that the two different types of threads are able to refer to the same shared memory address space and utilize the same memory resources without the conventional copying and transferring approaches described above.

As further depicted on as with one approach used to map to APD coherent memory is to refer memory instruction to IOMMU . As noted above with the description of in embodiments memory instruction mapper has specific memory access pathways defined and mapping determiner uses such pathways to direct memory instructions from particular thread types to particular memory managers and to particular physical memory resources.

In the illustrative example depicted on the use of IOMMU is determined by memory instruction mapper based on the shared memory address reference in memory instruction . In another embodiment IOMMU is directed by memory instruction mapper to map memory instruction to APD coherent memory . In yet another embodiment IOMMU receives memory instruction based on memory instruction mapper and then selects APD coherent memory based on other criteria e.g. the shared memory address the originating thread or the type of memory instruction. As noted above with the description of an example of the operation and functions of an IOMMU operating within a similar computer arrangement can be found in the IOMMU Applications.

Adding further detail to the example depicted in memory instruction is a conventional instruction e.g. an x86 instruction which can be executed using either a CPU thread or an APD thread. Because the instruction can be executed by either type of thread the shared memory address reference that memory instruction refers to an address in shared memory address space that enables the alternative use of multiple physical memory resources.

In this example the shared address reference defines an address in shared memory address space that enables the use of either APD coherent memory or shared system memory . As noted above APD coherent memory is a shared memory in APD that can be accessed by both APD and CPU and shared system memory is a shared memory in system memory that can be accessed by both APD and CPU . In an embodiment IOMMU is configured to select from the available memory resources based on different criteria.

Based on the address referenced by memory instruction in this example memory selector selects IOMMU to determine whether APD coherent memory or shared system memory should be provided to memory instruction . In an embodiment IOMMU selects APD coherent memory to execute memory instruction . In an embodiment IOMMU selects from available memory resources using the shared address reference while in other embodiments different criteria are used e.g. performance enhancement.

In yet another embodiment OS memory manager manages IOMMU to ensure IOMMU maps the page tables for memory instruction properly and ensures that CPU and APD recognize the mappings. Additional detail describing the operation of IOMMU is provided in the IOMMU Applications noted above.

This section and summarizes one of the techniques described herein by presenting a flowchart of an exemplary method of mapping a memory instruction to a shared memory space in a computer arrangement having a CPU and an APD in accordance with a unified memory model. While method is described with respect to an embodiment of the present invention method is not meant to be limiting and may be used in other applications.

As shown in an embodiment of method begins at step where the memory instruction that refers to an address in the shared memory space is received from an application. In an embodiment as shown in a memory instruction receiver such as memory instruction receiver receives the memory instruction such as memory instruction that refers to an address in the shared memory space from an application such as application . Once step is complete method proceeds to step .

At step the memory instruction is mapped based on the address to a memory resource that is associated with either the CPU or the APD. In an embodiment as shown in a memory instruction mapper such as memory instruction mapper maps the memory instruction such as memory instruction based on the address to a memory resource such as CPU physical memory or APD physical memory . Once step is complete method proceeds to step .

At step information is sent corresponding to the mapping to a processor where the memory instruction is performed by the processor based on the mapping. In an embodiment as shown in information is sent corresponding to the mapping by the memory instruction mapper such as memory instruction mapper to a processor such as CPU or APD where the memory instruction such as memory instruction is performed by the processor such as CPU or APD based on the mapping. Once step is completed method ends.

Embodiments described herein relate to the mapping of memory instructions in a shared memory address space. The summary and abstract sections may set forth one or more but not all exemplary embodiments of the present invention as contemplated by the inventors and thus are not intended to limit the present invention and the claims in any way.

The embodiments herein have been described above with the aid of functional building blocks illustrating the implementation of specified functions and relationships thereof. The boundaries of these functional building blocks have been arbitrarily defined herein for the convenience of the description. Alternate boundaries may be defined so long as the specified functions and relationships thereof are appropriately performed.

The foregoing description of the specific embodiments will so fully reveal the general nature of the present invention that others may by applying knowledge within the skill of the art readily modify and or adapt for various applications such specific embodiments without undue experimentation without departing from the general concept of the present invention. Therefore such adaptations and modifications are intended to be within the meaning and range of equivalents of the disclosed embodiments based on the teaching and guidance presented herein. It is to be understood that the phraseology or terminology herein is for the purpose of description and not of limitation such that the terminology or phraseology of the present specification is to be interpreted by the skilled artisan in light of the teachings and guidance.

The breadth and scope of the present invention should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the claims and their equivalents.

