---

title: Highly scalable storage array management with reduced latency
abstract: Systems and methods for increasing scalability and reducing latency in relation to managing large numbers of storage arrays of a storage network. Separate, dedicated, communication channels may be established between an array manager running on a server and each of a number of storage arrays for respectively performing reading and writing operations to limit the delays imposed by repeated array connection setup and teardown and improve array communication stability (e.g., as compared to performing read/write operations over the same array connection). The read connection can be used to maintain current state information (e.g., volumes, capacities, and the like) for a plurality of storage arrays in a local cache of the array manager that can be quickly accessed by the array manager, such as for presenting substantially current, summary-type state information of the various storage arrays to a user (e.g., upon the user requesting to configure a particular storage array).
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09513807&OS=09513807&RS=09513807
owner: Oracle International Corporation
number: 09513807
owner_city: Redwood City
owner_country: US
publication_date: 20120613
---
The present invention generally relates to the management of storage area networks and more particularly to systems and methods that achieve increased scalability and reduced latency while managing large numbers of storage arrays.

Storage systems such as storage arrays disk arrays and the like typically use special hardware and software along with disk and or tape drives to provide fast and reliable storage for computing and data processing. Storage systems may be complex and thought of as a special purpose computer designed to provide storage capacity along with advanced data protection features. Generally storage systems can provide either block accessed storage delivered over protocols such as Fibre Channel Internet Small Computer System Interface iSCSI or Fiber Connectivity FICON or file accessed storage provided over protocols such as Network File System NFS or Common Internet File System CIFS protocols.

To provide access to a large volume of block level data storage storage area networks SANs can be used to interconnect a large number of storage systems or devices e.g. disk arrays tape libraries and the like to one or more servers so that the storage devices appear as locally attached devices to the operating system s of the one or more servers. Client devices can then access the storage systems via the servers via one or more networks e.g. LAN WAN SAN Internet and or the like . Sharing storage usually simplifies storage administration and adds flexibility as for instance cables and storage devices generally do not have to be physically moved to shift storage from one server to another. A storage network typically includes one or more array managers e.g. storage management software implemented as instances on the one or more servers that allow for management of the plurality of storage devices. For instance Oracle s Sun Storage Common Array Manager CAM includes a number of interfaces e.g. browser interface common line interface CLI and the like that may be accessed from a client device and that allow users to configure storage arrays e.g. provisioning or allocating storage from particular disk arrays to one or more volumes monitor storage arrays e.g. in relation to current state health of arrays errors and alerts and the like.

New or additional storage arrays to a particular network typically need to be registered with the array manager in a manner akin to installing a new printer on a workplace network e.g. via logically representing the array to the array manager . However array managers can sometimes experience slowdowns and timeouts due to a user attempting to register more than a particular number of arrays e.g. 10 arrays 15 arrays or more with the array manager. For instance a user accessing an array manager e.g. via a user browser on a client machine to configure one or more arrays e.g. add update delete one or more volumes is typically first presented with some sort of summary type screen or page that provides the user with the current configuration state of each array registered with the array manager e.g. in relation to volume usage capacity allocations and the like . To populate and generate the summary screen array managers typically are required to separately communicate with each array to collect such configuration state information at the time the summary page is to be generated.

That is upon a user desiring to reconfigure one or more of the storage arrays the array managers subsequently reach out to each of the various storage arrays to obtain current state information for such storage arrays. However substantial amounts of time and bandwidth are consumed as part of having to separately collect such configuration state information from each storage array after the user desires to perform a storage array reconfiguration or the like. This can lead to interface timeouts the inability of users to view all state data on a single page inconsistent results long wait times and the like. Such inefficiencies become more pronounced with an increasing number of storage arrays in the SAN or other type of storage network.

In this regard disclosed herein are systems and methods that function to cache state information e.g. in relation to storage array configurations faults and health for the various storage arrays of a SAN or other storage network in a local cache that is directly accessible by any modules that configure the storage arrays such as by each of one or more array managers of the SAN. More specifically an array manager can at the time the state information is needed retrieve data for a storage array summary page directly from the quickly accessible cache instead of having to separately communicate with each storage array for its respective state information. The local cache can be refreshed in various manners according to any appropriate schedule and or according to one of various occurrences so that the array manager or other module reading the cache has access to substantially real time state information for the various storage arrays. In one arrangement at least some of the state information in the local cache can be updated refreshed in an event driven manner independent of the time the state information is needed e.g. for preparing a summary page such as upon each respective configuration change of a particular storage array e.g. in relation to disk utilization . In the case that many configuration changes to a particular storage array are made in sequence the last or most recent state may be retrieved to improve communication efficiency.

In one aspect a method for use in managing a plurality of storage arrays in a storage network includes receiving at an array manager over at least a first network a request to configure at least a first of the storage arrays accessing in response to the receiving configuration state information for the plurality of storage arrays from a cache of the array manager and sending the storage array configuration state information over the first network for presentation to a user.

Furthermore previous current read and write operations between an array manager and a particular storage array are performed over the same connection or thread. Stated differently current storage networks typically allow for a static number of connections or threads to be established between an array manager and each of the plurality of storage arrays where each connection is used for both read and write operations between the array manager and the storage array. Also each such connection is opened or set up for reading writing operations and then torn down upon completion of the operations. However this arrangement often leads to various communication failures and delays. In some situations all connections to a storage array may become consumed and thus prevent a desired operation from commencing. For instance all connections to a particular storage array may become consumed by read operations thus preventing a write operation from occurring. Still further numerous delays and other communication errors associated with the continual setup and teardown of array connections often occur. Having to continually recover from lost communication failures creates delays and latency in response time with regard to a particular array configuration change read write operation and or the like.

In this regard disclosed herein are systems and methods that persist one or more array connections for an extended period of time e.g. a period of time longer than that required for a particular read write operation in order to service multiple requests in sequence and avoid or at least limit the delays imposed by repeated array connection setup and teardown. Furthermore separate connections may be respectively used for read operations e.g. to satisfy user requests as part of maintaining array manager cache coherence and the like and write operations to improve array communication stability e.g. as compared to performing read and write operations over the same array connection .

In another aspect a method for use in managing a plurality of storage arrays in a storage network includes receiving at an array manager over at least a first network at least a first request to configure at least a first of the storage arrays selecting from a thread pool including a plurality of storage array handlers and accessible to the array manager a handler corresponding to the first storage array first using the selected handler to configure the first storage array according to the first request and second using the selected handler to obtain current state information from the first storage array in response to the first using. For instance the first using may include configuring the first storage array according to the first request over a first write connection established between the selected handler and the first storage array via at least a second network and the second using may include obtaining the first storage array current state information over a read connection established between the selected handler and the first storage array via the at least second network where the read connection is distinct from the first write connection.

In one arrangement a particular storage array may be quickly probed to determine whether the array is available e.g. one or more IP addresses of a controller of the array before attempting to establish a read and or write connection with the storage array. For instance the array manager may probe the storage array using a standard application programming interface API e.g. a standard Java API to initially determine whether the array is available e.g. for read write operations . If the array is available the array manager may then proceed to establish a persistent array connection with the storage array e.g. using a storage array vendor specific API perform any appropriate management operations and cache the connection for future use. If the array is not available the array manager may then proceed to probe additional storage arrays e.g. with the standard API to determine whether such storage arrays are available and then appropriately establish persistent array connections depending upon the availability. Significant quantities of time can be saved by quickly initially probing a storage array e.g. with a standard API to determine whether or not the array is available e.g. 2 4 seconds as opposed to proceeding directly to attempting establishment of a read write connection with a particular storage array e.g. with a storage array vendor specific API and then waiting for the connection to fail e.g. 10 or more seconds . For instance an array manager can move much more quickly through a long list of storage arrays when several of the arrays may be experiencing network communication failures.

Certain management operations e.g. in relation to volume creation may require connection to a particular controller of a storage array rather than to any of a number of controllers of the storage array. In this regard the array manager may in some arrangements be configured to probe only the IP addresses of one or more particular controllers of a storage array i.e. as opposed to necessarily probing all IP addresses regardless of controller . In the event that an array connection to an IP address of a desired particular controller is already cached and indexed by the desired particular controller an operation on the particular controller can substantially seamlessly be performed by accessing the cached connection.

In another aspect a system is provided for use with an array manager configured to manage a plurality of storage arrays of a storage model. The system includes a processing module and a memory module logically connected to the processing module. The processing module includes a set of computer readable instructions executable by the processing module to probe a first storage array of the plurality of storage arrays using a standard application programming interface API to determine whether or not the first storage array is available. Then responsive to the first storage array being available one of a plurality of threads of a thread pool is used to establish a write connection between the array manager and the first storage array. Often the write connection is used to configure the first storage array. The processing module then may probe responsive to the first storage array being unavailable a second storage array of the plurality of storage arrays using the standard API to determine whether or not the second storage array is available.

In addition to the exemplary aspects and embodiments described above further aspects and embodiments will become apparent by reference to the drawings and by study of the following descriptions.

The present disclosure is generally directed to systems and methods that allow for increased scalability and reduced latency in relation to managing large numbers of storage arrays of a storage area network. Separate dedicated and persistent communication channels or connections may be established between an array manager running on a server and each of a number of storage arrays for respectively performing reading and writing operations to limit the delays imposed by repeated array connection setup and teardown and improve array communication stability e.g. as compared to performing read and write operations over the same array connection . The read connection can be used to maintain current state information e.g. volumes health and the like for a plurality of storage arrays of a SAN or other network in a local cache of the array manager. The state information can be quickly accessed by the array manager or other module such as for use in presenting substantially current summary type state information of the various storage arrays to a user e.g. upon the user requesting to configure a particular storage array such as to add a file request more space and the like . The local cache can be refreshed in various manners according to any appropriate schedule and or according to one of various events or occurrences e.g. changes in array state or configuration such as change in disk utilization so that the array manager or other module reading the cache has access to substantially real time state information for the various storage arrays.

Turning now to a block diagram of a storage networking model that provides storage array access to clients via servers over one or more networks is illustrated. Broadly the storage model may be made up of a plurality of client devices e.g. each running any appropriate operating system such as UNIX NetWare NT Solaris or the like a plurality of servers e.g. mail servers application servers or database servers and a plurality of storage arrays e.g. Fibre Channel arrays which provide redundant and parallel storage capacity to the client devices via the servers .

Each client device may be interconnected or otherwise have access to one or more of the servers e.g. for use in configuring or taking other actions with respect to the storage arrays via any appropriate network s e.g. LAN WAN Internet . Furthermore each of the servers may be interconnected or otherwise have access to one or more of the storage arrays via any appropriate network s e.g. LAN SAN . For instance the network s may be in the form of one or more LAN and or SAN fabrics including any appropriate number and configuration of switches and the like that allow for communications between the servers and the storage arrays .

One or more of the servers may run an instance of any appropriate array manager e.g. software such as Oracle s CAM that broadly serves to provide a common interface for most or all storage functions and facilitate out of band management and or in band management of one or more of the storage arrays . As just one example a user may utilize a browser running on a client device to access an interface e.g. user interface or command line interface of the array manager of one of the servers and appropriately request to configure one or more of the storage arrays e.g. create a volume of a storage array . As part of requesting a configuration change of one or more of the storage arrays a user is typically presented with a summary page or screen on the client device e.g. via the browser that provides an indication of the current state e.g. capacity allocations volume usage free space health status and the like of the various storage arrays registered with the array manager to allow the user to make a more informed decision regarding the particular configuration change. While each of the servers has been illustrated as running an array manager other embodiments envision that some servers may actually utilize an array manager running on another server to manage the storage arrays .

As discussed above previous and current array managers are generally required to separately communicate with each storage array to collect storage array configuration state information e.g. in relation to volume usages capacity allocations versions and the like in order to prepare the summary page in response to the user requesting to configure or take other action with respect to the storage arrays. That is upon a user requesting to reconfigure one or more of the storage arrays e.g. provision storage space current array managers subsequently reach out to each of the various storage arrays to obtain current configuration state information for such storage arrays at the time the request is made. However substantial amounts of time and bandwidth are consumed as part of having to separately collect such state information from each storage array at the time the state information is needed. This leads to interface timeouts an inability of users to view all state data on a single page inconsistent results long wait times and the like. Such inefficiencies become more pronounced with an increasing number of storage arrays in the SAN or other type of storage network.

In this regard and turning now to one or more of the array managers may store substantially current or up to date state information for the various storage arrays e.g. for storage arrays registered with the array manager and or for other storage arrays in a local persistent in memory cache i.e. storage in quickly accessible memory of the server on which the array manager is implemented that may be quickly accessed by any appropriate module to obtain a current state of one or more of the storage arrays . That is instead of having to separately communicate with each storage array to obtain current configuration state information for the storage arrays e.g. at the time at which generation of a state summary of all of the storage arrays is desired a module or component of the array manager may simply access the local cache to obtain such state information.

For instance each storage array may maintain current or up to date state information e.g. in relation to configuration health and the like in an object bundle OB e.g. OB OB OB respectively for Storage Array Storage Array Storage Array which may be stored on any appropriate persistent data store not shown of the storage arrays . Upon a change in state of a storage array any appropriate controller or module may function to update the OB with the changed new state information e.g. adding the new information to the OB writing over old information and or the like .

Furthermore the array manager may store a respective OB Instance OBI e.g. OBI OBI OBI of each OB in the cache . Each OBI includes information representing a substantially current state of a respective storage array e.g. in relation to storage array configuration such as volumes capacities and the like and in some arrangements additionally in relation to health faults status and the like . As will be discussed more fully in the following description each OBI may be updated refreshed in an event driven manner that may be independent of the updating refreshing of other OBIs in the cache and which may occur before the time at which the state information of the OBI is needed by the array manager e.g. for preparing a storage array summary page . For instance upon a user requesting and implementing a configuration change of a particular storage array e.g. creating a volume the array manager may request a copy of the current OB of the particular storage array where the current OB reflects the new configuration change and store an OBI of the current OB in the cache e.g. overwriting the previous version of the OBI updating only those portions of the previous OBI that have changed and or the like . If at some time later the array manager needs the current state information for the particular storage array e.g. as part of preparing a summary state information page for all of the storage arrays the array manager can simply retrieve the updated OBI of the particular storage array from the cache .

Before discussing the cache and its interaction with other modules components in more detail reference will be made to other components of the array manager . As shown the array manager may include a Fault Management Application FMA e.g. fault management module that broadly serves to provide automated diagnosis of faulty hardware e.g. disk drive clusters of the storage arrays take proactive measures to correct such hardware related faults e.g. offlining a CPU and or the like. The FMA generally runs in the background capturing telemetry and other related hardware software errors until a diagnosis can be completed or a fault can be predicted. In one arrangement the FMA may be configured to automatically request and obtain certain types of fault management FM state information e.g. health over temperature alerts and the like from each of the various storage arrays according to any desired schedule e.g. every 5 minutes every 15 minutes or the like and then appropriately store such information in the cache . In another arrangement a storage array e.g. a controller of the storage array may be configured to sense a fault e.g. over temperature failure and the like and then automatically send information related to the fault e.g. asynchronous notifications back to the FMA which may store the information in the cache in any appropriate manner. The obtained FM state information for each storage array may be stored in the cache as part of its respective OBI and or in the cache but separate from its respective OBI e.g. in a table or another object in any appropriate manner .

The array manager also may include at least one interface e.g. CLI or UI that communicates with browser or other software component on a client device and allows a user to manipulate or otherwise make use of the array manager . For instance administrators and troubleshooters can utilize a browser of a client device to access the FMA via the interface as part of fault management activities in relation to the storage arrays . As another example users may utilize a browser of a client device to implement configuration changes of one or more of the storage arrays via the interface .

With continued reference to the array manager may also include or at least have access to a thread pool . The thread pool includes references to a plurality of threads or OB Handlers OBH . Each OBH is specifically configured for a particular one of the storage arrays e.g. OBH OBH OBHfor Storage Array Storage Array Storage Array respectively . Broadly each OBH is operable to handle asynchronous notifications e.g. in relation to faults received from its particular storage array maintain a current OBI in the cache for its particular storage array and perform read and or write operations on the particular storage array .

For instance the FMA may access a particular OBH in the thread pool to obtain updated state information for a particular storage array . As another example the interface may access a particular OBH in the thread pool to perform a write operation on a particular storage array per commands from a user via a browser on client device . While the thread pool has been shown as being included as part of the array manager or its respective server in some arrangements envision that one or more of the array managers may access a thread pool implemented as part of another array manager on another server . In this regard a plurality of array managers of a particular storage networking model may utilize a common thread pool .

With additional reference to additional details relating to each storage array a respective OBH and separate and dedicated read and write connections between the same are illustrated. Each storage array may generally be made up of a number of clusters of disk drives e.g. RAIDs which may be appropriately manipulated by a number of controllers . The controllers may also be responsible for maintaining current or up to date state information in the OB . While not shown it should be appreciated that the controllers OB and or other components of the storage array may be implemented or stored on one or more servers in communication with the clusters .

Again the OBH is generally responsible for facilitating read and write operations in relation to the storage array . As discussed above previous and current array managers conduct read and write operations for a particular storage array over a common connection. Doing so often leads to various communication failures and delays due to all connections to a storage array becoming consumed which prevents a desired operation from commencing e.g. all connections to a particular storage array may become consumed by read operations thus preventing a write operation from occurring and due to the continual setup and teardown of array connections. In this regard the OBH is configured to create a number of separate and distinct connections between the OBH and thus the array manager and a particular storage array for respective read and write operations between the array manager and the storage array .

As seen in the OBH may create at least one persistent read channel or connection with the storage array . The connection may be used to handle asynchronous notifications e.g. in relation to faults received from the storage array and maintain a current OBI in the cache for the storage array . While the read connection is illustrated as being with Controller of the storage array the read connection may be with other modules or components of the storage array e.g. with Controller Controller and or other modules so long as the modules have access to the OB and the ability to pass asynchronous notifications e.g. in relation to faults back to the array manager over the read connection .

The OBH may also create a number of write channels or connections where each write connection is established between the OBH and thus the array manager and a particular controller of the storage array . For instance upon a user desiring to configure a particular disk drive e.g. create a volume edit data of a particular storage array the interface of the array manager may select the appropriate OBH from the thread pool which serves to establish a write connection with a corresponding controller of the storage array responsible for configuring or otherwise managing the particular disk drive and or its cluster . Advantageously the operations performed over the write connections may be substantially unaffected by those occurring over the read connection s and vice versa. The OBH may utilize IP addresses of the storage array as supplied by the FMA for use in establishing the read and write connections .

The OBH may include or be associated with a cache that may be used to store metadata corresponding to the various read and or write connections . For instance upon initially establishing write connections with the various respective controllers the OBH may store metadata e.g. IP address and or the like for each of the write connections e.g. Write Write Write in the cache e.g. indexed by controller number or ID . Thereafter subsequent writes to one or more of the controllers via the write connections may be sped up or otherwise facilitated with the metadata e.g. by avoiding or limiting the need to go through the steps necessary to open a new connection such as having to newly open a socket at a specific symbol port . Upon occurrence of a connection time out or other type of exception or dropped connection the OBH may clean both the particular write connection and its respective metadata in the cache .

Turning now to a flow diagram showing one method for use in managing a plurality of storage arrays in a SAN or other type of storage network is illustrated. In conjunction with a discussion of the method reference will also be made to to facilitate the reader s understanding of the method . At state information may generally be collected from one or more storage arrays of a storage model and appropriately stored in a local cache accessible by an array manager . With reference to for instance the array manager may utilize OBHs of thread pool to obtain state information from respective storage arrays via respective read connections e.g. via asynchronous notifications from storage arrays in relation to fault conditions and or via polling a storage array for a current copy of the OB of the storage array . Steps and may be generally thought of as state information collection and storage processes substantially continuously going on in the background of an array manager as opposed to necessarily upon generation of a storage array summary page or the like.

With reference again to the method may query whether a request has been received to reconfigure at least one storage array . For instance a user may access interface of array manager via a browser of a client device over network s to add a volume to edit data in and or the like for a particular storage array . Responsive to a negative answer to the query at the method may flow back to and to continue to collect and store state information for storage arrays of the storage model . If a reconfiguration request has been received e.g. at interface of array manager the method may proceed to access storage array state information from a local array manager cache and send the state information to a user.

That is and in contrast to previous arrangements whereby an array manager would separately reach out to each of the storage arrays e.g. via network s to obtain current state information for the storage arrays for use in preparing a summary page to be sent to a user for presentation on a client device and the associated complications in relation to slow rendering of the summary page due to communication delays inconsistent results and the like the present method retrieves the state information for the various storage arrays of the storage model from the local and quickly accessible cache and sends the same to a user which may be presented in the form of a storage array summary . For instance the interface may retrieve copies of each of the various substantially up to date OBIs from the cache and utilize the OBIs to generate the summary page or otherwise send the state information to the client device . The substantially up to date state information can be utilized by a user as part of determining in what manner one or more storage arrays are to be configured.

The method may then include using the selected OBH to configure the particular storage array s in the manner desired by the user. For instance step may entail initially accessing the OBH cache and determining whether connection metadata exists for a write connection to a controller corresponding to the cluster or disk drive of the storage array that the user wants to configure reconfigure. In the event that appropriate connection metadata exists the OBH utilizes the same to configure the storage array via the corresponding write connection . See . Otherwise the OBH creates a new write connection to the corresponding controller and stores corresponding connection metadata in the OBH cache .

In one arrangement the OBH may be configured to probe a particular storage array to determine whether the storage array is available before attempting to establish a read and or write connection with it. For instance before establishing a write connection with a particular controller of a particular storage array the array manager may probe the controller using a standard API e.g. a standard Java API to initially determine whether the storage array is online or otherwise available. If the storage array is available the array manager may then proceed to establish a write connection with the controller e.g. using a storage array vendor specific API .

If the storage array or its controllers is not available the array manager may then proceed to probe additional storage arrays e.g. with the standard API to determine whether such additional storage arrays are available and then appropriately establish write connections depending upon the availability. Significant quantities of time can be saved by quickly and initially probing a storage array and determining whether the array is unavailable e.g. 2 4 seconds as opposed to proceeding directly to attempting establishment of a read write connection with a particular storage array and then waiting for the connection to fail e.g. 10 or more seconds . For instance an array manager can move much more quickly through a long list of storage arrays when several of the arrays may be experiencing network communication failures.

With continued reference to the method may include using the selected OBH to obtain a copy of the current OB s from at least one storage array that was just configured reconfigured via the write connection s . As mentioned previously each storage array has any appropriate logic or the like that serves to maintain current state information in its respective OB upon changes in state of the storage array e.g. configuration health and the like . In one arrangement the storage array may signal or otherwise notify the array manager of a change or update in its OB . In any event the interface may in some arrangements then appropriately signal e.g. message the FMA to request and obtain a copy of the current OB from the particular storage array over a corresponding read connection via the corresponding OBH .

The array manager may retrieve the current OB from the particular storage array for refreshing the cache on a manner that is configuration event driven rather than driven by the time the information in the cache is actually needed e.g. for preparing a summary page . In this way the cache may be configured to substantially always maintain current or up to date information for the various storage arrays which is quickly accessible to array manager or other components of the storage model .

The method may then include updating the OBI of the particular storage array in the array manager cache . See . For instance the updating may include overwriting or otherwise storing a copy of the current updated OB received from the storage array over the read connection in the cache. In the event that multiple copies of the OB exist in the cache the interface or other module accessing the OBIs in the cache for presentation to a user e.g. as part of a storage array state summary page or for other purposes may be appropriately designed to select the most recent OBI for each particular storage array e.g. as identified by a time stamp or other metadata tagged to the OBI .

It will be readily appreciated that many deviations may be made from the specific embodiments disclosed in the specification without departing from the spirit and scope of the invention. Also it should be understood that the functionalities performed by many of the processes and modules discussed herein may be performed by other modules devices processes etc. The illustrations and discussion herein has only been provided to assist the reader in understanding the various aspects of the present disclosure. Furthermore one or more various combinations of the above discussed arrangements and embodiments are also envisioned.

Embodiments disclosed herein can be implemented as one or more computer program products i.e. one or more modules of computer program instructions encoded on a computer readable medium for execution by or to control the operation of data processing apparatus. The computer readable medium can be a machine readable storage device a machine readable storage substrate a memory device a composition of matter affecting a machine readable propagated signal or a combination of one or more of them. Each of the client devices servers and storage arrays may encompasses one or more apparatuses devices and machines for processing data including by way of example a programmable processor a computer or multiple processors or computers. In addition to hardware the storage model may include code that creates an execution environment for the computer program in question e.g. code that constitutes processor firmware a protocol stack a database management system an operating system or a combination of one or more of them.

A computer program also known as a program software software application script or code used to provide the functionality described herein e.g. the dynamic consumption cost determination and associated resource scheduling disclosed herein can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data e.g. one or more scripts stored in a markup language document in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules sub programs or portions of code . A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.

The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by and apparatus can also be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit . Processors suitable for the execution of a computer program may include by way of example both general and special purpose microprocessors and any one or more processors of any kind of digital computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. Generally the elements of a computer are one or more processors for performing instructions and one or more memory devices for storing instructions and data. The techniques described herein may be implemented by a computer system configured to provide the functionality described. Furthermore the flow diagrams disclosed herein have merely been presented as examples of manners in which the present teachings can be implemented and do not in all cases necessarily require all the disclosed steps and or the particular order in which the steps have been presented.

In different embodiments distributed computing environment may include one or more of various types of devices including but not limited to a personal computer system desktop computer laptop notebook or netbook computer mainframe computer system handheld computer workstation network computer application server storage device a consumer electronics device such as a camera camcorder set top box mobile device video game console handheld video game device a peripheral device such as a switch modem router or in general any type of computing or electronic device.

Typically a computer will also include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. However a computer need not have such devices. Moreover a computer can be embedded in another device e.g. a mobile telephone a personal digital assistant PDA a mobile audio player a Global Positioning System GPS receiver a digital camera to name just a few. Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory media and memory devices including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry. To provide for interaction with a user embodiments of the subject matter described in this specification can be implemented on a computer e.g. client system in having a display device e.g. a CRT cathode ray tube or LCD liquid crystal display monitor for displaying information to the user and a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input.

While this disclosure contains many specifics these should not be construed as limitations on the scope of the disclosure or of what may be claimed but rather as descriptions of features specific to particular embodiments of the disclosure. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover although features may be described above as acting in certain combinations and even initially claimed as such one or more features from a claimed combination can in some cases be excised from the combination and the claimed combination may be directed to a subcombination or variation of a subcombination.

Similarly while operations are depicted in the drawings in a particular order this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order or that all illustrated operations be performed to achieve desirable results. In certain circumstances multitasking and or parallel processing may be advantageous. Moreover the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments and it should be understood that the described program components and systems can generally be integrated together in a single software and or hardware product or packaged into multiple software and or hardware products.

