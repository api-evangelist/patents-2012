---

title: Anomaly sensor framework for detecting advanced persistent threat attacks
abstract: A threat detection system for detecting threat activity in a protected computer system includes anomaly sensors of distinct types including user-activity sensors, host-activity sensors and application-activity sensors. Each sensor builds a history of pertinent activity over a training period, and during a subsequent detection period the sensor compares current activity to the history to detect new activity. The new activity is identified in respective sensor output. A set of correlators of distinct types are used that correspond to different stages of threat activity according to modeled threat behavior. Each correlator receives output of one or more different-type sensors and applies logical and/or temporal testing to detect activity patterns of the different stages. The results of the logical and/or temporal testing are used to generate alert outputs for a human or machine user.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09378361&OS=09378361&RS=09378361
owner: EMC Corporation
number: 09378361
owner_city: Hopkinton
owner_country: US
publication_date: 20121231
---
Computer systems are vulnerable to various kinds of attacks or threats including so called advanced persistent threats or APTs which can be very sophisticated and dangerous from a security perspective. The term APT generally refers to an attacker having capabilities and resources to persistently target a specific entity along with the techniques they use including malware that infects a system under attack. An APT may be capable of locating and taking harmful action with respect to sensitive data in a computer system such as copying confidential data to an external machine for criminal or other ill intended activities. The APT performs its tasks in a stealthy manner so as to avoid detection. In some cases an APT may be active in a system for a very long period of weeks or months and create corresponding levels of damage.

It is desirable to protect computer systems and their data from threats such as APTs. Among the mechanisms that can be utilized are detection mechanisms for detecting the presence of an APT in a computer system. Generally any malware that has entered a system can only be removed or disabled after it has first been identified in some manner. Identification in turn requires detection i.e. acquiring knowledge indicating possible or actual presence of an APT or similar threat in a protected system.

The detection of APTs and similar threats in protected computer systems has relied upon human security specialists who have experience with the threats and can interrogate a protected system for operational information that may be indicative of APT presence. Generally such a security specialist is looking for patterns of known bad activities such as communications logs identifying an external host known or suspected to be used or controlled by computer criminals. The process can be labor intensive difficult to generalize and non scalable. Existing anomaly detection schemes commonly focus on very obvious anomalies such as volume based outliers but these are ill suited for low and slow APT attacks and also suffer from high false positive rates.

A presently disclosed technique for threat detection is both automated and sufficiently sophisticated to detect the more subtle operation signatures presented by APTs and similar threats. The technique is based partly on a hypothesis that however stealthy an attacker might be its behavior in attempting to steal sensitive information or subvert system operations should be different from that of a normal benign user. Moreover since APT attacks typically include multiple stages e.g. exploitation command and control lateral movement and objectives each move by an attacker provides an opportunity to detect behavioral deviations from the norm. Correlating events that may otherwise be treated as independent can reveal evidence of an intrusion exposing stealthy attacks in a manner not possible with previous methods.

The technique includes use of detectors of behavioral deviations referred to as anomaly sensors where each sensor examines one aspect of hosts or users activities. For instance a sensor may examine the external sites hosts contact to identify unusual connections potential command and control channels to profile the machines each user logs into to find anomalous access patterns potential pivoting behavior in the lateral movement stage to study users regular working hours to flag suspicious activities in the middle of the night or to track the flow of data between internal hosts to find unusual locations where large amounts of data are being gathered potential staging servers before data exfiltration .

More particularly a computer system realizes a threat detection system for detecting threats active in a protected computer system. The system includes a set of anomaly sensors of distinct types including user activity sensors host activity sensors and application activity sensors with each sensor being operative 1 to build a history of pertinent activity over a training period of operation and 2 during a subsequent detection period of operation compare current activity to the history of pertinent activity to detect new activity not occurring in the training period. The new activity is identified in respective sensor output. The system further includes a set of correlators of different distinct types corresponding to different stages of threat activity according to modeled threat behavior. Each correlator receives output of one or more sensors of different types and applies logical and or temporal testing to received sensor outputs to detect activity patterns of the different stages. Each correlator uses results of the logical and or temporal testing to generate a respective alert output for a human or machine user of the threat detection system.

While the triggering of one sensor indicates the presence of a singular unusual activity the triggering of multiple sensors suggests more suspicious behavior. A framework is used in which templates are defined by a human analyst for the correlations implemented by the correlators. The human analyst is given the flexibility of combining multiple sensors according to known attack patterns e.g. command and control communications followed by lateral movement to look for abnormal events that warrant investigation or to generate behavioral reports of a given user s activities across time.

The protected system is generally a wide area distributed computing system such as a large organizational network. It may include one or more very large datacenters as well as a number of smaller or satellite datacenters all interconnected by a wide area network that may include public network infrastructure Internet along with private networking components such as switches and routers firewalls virtual private network VPN components etc. Each datacenter includes local resources such as server computers servers client computers and storage systems coupled together using local intermediate networks such as local area networks LANs metro area networks MANs storage area networks SANs etc.

The SIEM system is a specialized computing system including hardware computing components executing specialized SIEM software components including a large database for storing the parsed logs . The SIEM system receives raw logs not shown generated by logging devices in the system and performs basic parsing into fields e.g. IP address timestamp Msg ID etc. to produce the parsed logs . In one embodiment the SIEM system may utilize a SIEM product known as enVision sold by RSA Security Inc. the security division of EMC Corporation. The SIEM system gathers the raw logs generated by different devices within the protected system and stores the parsed logs in the database functioning as a centralized repository. The logs need to be stored for some period of time e.g. at least several months in order to enable the analysis described herein.

The threat detection system may be primarily software implemented utilizing hardware resources of the SIEM system or in some cases its own dedicated hardware computers. The threat detection system is described herein as a collection of functional components. As described below these are to be understood as one or more general purpose computers executing specialized software for realizing each function.

The major component of the threat detection system is the monitor analyzer . As described more below it employs both top down and bottom up components. The top down component builds and utilizes templates based on known information about current and prior APT attacks and these templates are used in analysis for detecting behavior that may be indicative of such attacks. The bottom up component gathers stores and processes the system activity information as reflected in the sanitized logs from the preprocessor .

As shown in the monitor analyzer includes sensors and correlators . Various types of sensor are deployed including for example a command and control C C sensor new login sensor new applications sensor and critical servers sensor . The correlators work from sensor output in the form of reports . Examples of correlators include C C and new application correlator unusual login correlator and C C and new login correlator . Functions of these elements are described below.

Because of inconsistencies in the parsed logs out of order events time skew missing events etc. the log data needs to be processed and sanitized before any analysis is attempted. To this end the preprocessor includes a time sanitizer and a host address sanitizer . The host address sanitizer includes a first sanitizer STAT for static host addresses and a second sanitizer DYN for dynamic host addresses. Example parsed log inputs are shown including logs from web proxies firewalls domain controllers VPN components and Dynamic Host Control Protocol DHCP servers. Specific aspects of the preprocessor are described below.

Regarding the above mentioned top down component it may utilize published information about current and past APT attacks documented in existing literature as well as human analyst experience. This information is used to build templates for APT detection. Template definition and deployment may be based on an assumed model for APT attacks. In one model a typical APT attack consists of the following stages 1 Exploitation 2 Command and control C C 3 Lateral movement and 4 Objectives e.g. data exfiltration service disruption . Exploitation refers to a threat s entering a system at points of vulnerability. Command and control refers to a resident threat s communications with the attacker e.g. to receive new attack directives or to report back the results and operations in the system such as gathering sensitive data. Lateral movement refers to propagation or migration of threat components activities within a system and objectives refers to activities that provide the desired data or other results from operation of the threat.

Starting from the general APT model different classes of attacks can be defined and the classes used to guide template creation. Examples of attack classes include attackers that propagate through social media attackers that gather data to a central location and exfiltrate the data attackers that exfiltrate from multiple machines in the enterprise. Templates for such different classes of attack can be built and deployed for use. The templates can also be refined during use as more knowledge is gathered about attacker behavior. Note that instead of utilizing known attack signatures such as external sites known to be controlled by computer criminals or specific text strings used in known exploits the disclosed system relies on patterns of attack behavior and hence is more difficult to evade.

The bottom up component processes and analyzes information in event logs received by the SIEM system in an enterprise and gathers evidence for abnormal user or host behavior as determined by different anomaly detection sensors . The bottom up component generally consists of the following layers 

The SIEM system gathers logs generated by different devices within an enterprise in a centralized repository and stores these logs sufficiently long e.g. at least several months in order to enable analyses as described herein.

Because of log inconsistencies out of order events time skew missing events the data from the parsed logs needs to be processed and sanitized before any analysis is attempted. Below are described three particular processing tools that may be used.

The role of individual sensors is to maintain a history of normal user and host activity in a particular respect perform profiling related to that aspect of user or host behavior and generate a report when activity deviating from the typical user or host profile is observed. The following are examples of observable activity along with the specific sensor involved as well as the model stage in which it occurs 

Each individual sensor may use statistical and machine learning techniques for profiling and more details are given for several example sensors below. The system is preferably general enough to accommodate new types of sensors being added to the system and or refinement of existing sensor based on the experience based feedback.

The reports from multiple sensors are correlated and matched against the attack templates built by the top down component. An attack template might involve several sensors and could use different operators applied to the reports from the sensors as described below.

Alerts may be triggered from the correlators when suspicious events according to defined templates are detected. The alerts may be provided to a human analyst for further investigation. Alternatively the correlators may export an application programming interface API via which an external machine user can receive alerts and or reports.

The system preferably allows human analysts evaluating the alerts to give feedback based on the severity of the alerts and the usefulness of the alerts in detecting real attacks. Based on the analyst feedback the system may undertake continuous refinement. For instance new sensors may be added reports generated by various sensors may be refined alert prioritization may be improved etc. New templates can also be generated as more attacks are discovered by the human analysts and in general more knowledge about various attackers is gathered.

One philosophy that may be used in designing the anomaly sensor framework is to make each individual sensor relatively weak or coarse meaning that it employs a low threshold in selecting event information to be included in its output reports leaving the stronger detection logic for the correlators . In this way generally more lower level information is obtained that may pertain to a variety of threats both known and unknown and strong flexible correlations between sensors can be used to boost the quality of the overall result.

As mentioned the sensors utilize information from logs collected by the SIEM system e.g. enVision in an enterprise environment. These include logs generated by proxy web servers VPN servers domain controllers firewalls and DHCP servers for example. The parsed logs require pre processing before different event types can be correlated. In particular it is generally necessary to address inconsistencies arising from dynamic IP address assignments as well as to develop lists of static IP addresses active in the enterprise. It is also necessary to address inconsistencies in the time stamping of log data from devices in different time zones and using different time zone configurations.

To deal with dynamic IP addresses IPs it is necessary to develop a consistent mapping between network IP addresses and hostnames MAC addresses. This is done by parsing DHCP and VPN logs as described in more detail below. The outcome of this pre processing may be stored in a database table with certain fields as shown below. Users can query this table directly. There may also be an interface e.g. a web form for accessing this information.

To study hosts that are assigned static IP addresses IP addresses may be examined that do not appear in DHCP and VPN logs. For example these IPs may be obtained from security gateway logs and host operating system e.g. Windows event logs. The hostname associated with those IP addresses may be looked up e.g. by reverse DNS resolution using tools such as nslookup or host repeatedly over time. An IP address that always resolves to the same hostname is considered static.

For time sanitization the parsed logs are sanitized so that all log entries for all devices are reported in one consistent time such as UTC time. The sanitization procedure is done by the time sanitizer as described more below.

As mentioned each of the sensors focuses on a particular aspect of host or user network behaviors. The goal of a sensor is to 1 profile understand common behaviors during a training period covering a sufficiently long e.g. at least one or two months of network activity and 2 identify outliers during a detection period following the training period. As output each sensor generates a respective report with an ordered list of alerts or suspicious behavior identified during the detection period. The alerts are given a priority and score based on their relevance.

The goal of this sensor is to identify possible C C Command and Control domains according to the following heuristics 

Analysis may be done using proxy logs of security gateway devices such as a product known as IronPort . The C C sensor constructs a history of web domains contacted by each host over a training period e.g. one or two months . Afterward a domain is flagged if it shows up in a log but not included in the history. A flagged domain is included in a watch list and actively monitored for some configurable time interval e.g. one week . If at the end of the week the activity to this domain looks normal in terms of connection rate and frequency of domain contact from internal hosts then the domain is added to the history. Otherwise the domain is included in the report output of the C C sensor .

The goal of this sensor is to identify new login patterns not observed previously in the infrastructure using Windows log events. It profiles a user based on the set of machines the user commonly logs onto over the training period e.g. one or two months . Similarly for each hostname it finds the set of users that commonly log onto the host. A history is built of user host login events observed during training. Then during detection new logins are identified and flagged defined as new user host login events not already in the history . The sensor outputs in its report the new logins events found during detection.

The goal of this sensor is to identify human like activities generated by a machine when the user is not at the terminal. During the training period a pattern of regular working hours is built per user. In addition a whitelist of domains contacted by many different machines at night corresponding to automated processes is created. The output of the sensor is a report containing a list of hosts and non whitelisted web domains contacted outside regular working hours.

The goal of this sensor is to profile installed software applications on a host. This may be done using user agent string fields in HTTP requests that are recorded in security gateway proxy logs for example. During the training period a history including the list of all observed user agent strings for each host is created. Afterward new software corresponding to new user agent strings not observed previously is identified. The output is a list of suspicious hosts and new possibly suspicious applications they have installed.

This sensor profiles the internal communication among enterprise hosts to determine potential staging servers used for gathering data before exfiltrating it to external sites. The sensor can use either firewall logs or records of network traffic analyzers if available. While firewall logs normally report only basic connection information among different hosts source and destination IP address and port number and whether the connection is accepted or denied network traffic analyzer records provide additional details such as the amount of data transferred in a connection. During training a map of the communication pattern within the enterprise can be built. This includes pairs of internal hosts communicating and the average volume of traffic observed if this information is available from traffic analyzer data . In the detection phase the sensor flags pairs of hosts either initiating communication or deviating from the communication pattern recorded in the map.

The goal of this sensor is to identify critical machines services in the enterprise network using the firewall logs. During profiling for each host the sensor examines its number of incoming and outgoing connections and whether those connections were allowed or denied. After the training period it identifies hosts that have a high ratio of incoming outgoing connections. These are considered servers.

As mentioned in the top down component an attack template can be flexibly defined by a human operator as correlating outputs of different sensors using various policies. Examples of operators that can be used when building attack templates using the outputs reports of multiple sensors are 

Given a set of reports for various sensors in the system and an attack template describing known aspects of an attack using statement with operators as above the correlators generate and output a list of prioritized alerts that satisfy the attack template. The priority and score of these alerts is computed based on the priority and score of the alerts generated by each sensor used by the template. The alerts may be presented to a human user analyst in a graphical representation. The system may also provide a graphical interface tool through which the analyst can provide feedback. For instance the human analyst may rank each alert according to its indication of a real attack on a 1 10 scale. Based on the analyst feedback the system is continuously refined by adding new sensors augmenting reports generated by various sensors refining attack templates and prioritizing alerts raised by either individual sensors or correlation among multiple sensors.

As generally known an IP address can either be static or dynamic. A static IP address is one that is assigned to a particular machine for an indefinitely long time generally until some event such as network reconfiguration requires address reassignment. This period may be months or years in duration. A dynamic IP address is one that is only temporarily assigned leased to any given machine for a generally much shorter period and may be assigned to several different machines over different periods. One well known network protocol for managing the assignment of dynamic IP address is the Dynamic Host Configuration Protocol DHCP .

Distinguishing between static and dynamic IP addresses is important to the goal of mapping IP addresses to unique hosts. However in large enterprise networks documentation about the use and configuration of individual IP address ranges is often scarce or non existent. While a large subset of dynamic IP addresses e.g. those administered by the corporate IT department can be inferred from logs collected from designated DHCP servers it is much more difficult to identify static IP addresses and also dynamic IP addresses managed by private DHCP servers i.e. local DHCP servers deployed in small or remote networks whose DHCP traffic is not visible to more centralized monitoring systems such as the SIEM system .

The following address classification may be used. The set of all network addresses is divided between known dynamic addresses and other addresses which are further divided into static addresses and other dynamic addresses. Known dynamic IP addresses are those generally managed centrally by the IT department while other dynamic IP addresses are typically managed by private DHCP servers to which the IT department lacks visibility.

In one embodiment a tool for classifying IP addresses regularly e.g. daily extracts all the IP addresses that appear in network logs to create a large enterprise IP address pool. Similarly it extracts dynamic IP addresses from logs that are known to include only dynamic IP addresses such as the logs from IT managed DHCP servers collected by the SIEM system . By taking the difference between these two IP address pools resolving the names of the hosts assigned to the resulting IP addresses and continuously monitoring those hosts for IP address re assignments the tool automatically maintains up to date and self correcting lists of static IP addresses and other dynamic IP addresses.

Address sanitization is described with reference to . In general the address sanitizer may be designed to perform a one time bootstrap cycle to initialize its operations and data and then run a periodic e.g. daily update cycle. These two separate cycles are both explained below with reference to . As shown the process operates using the data from the time sanitized logs T. Further below is a brief description of operation of the time sanitizer in creating those logs.

During this cycle the tool builds its IP address pools and identifies a set of undetermined but potentially static IP addresses.

As shown the combination of the set of known dynamic IP addresses and the set of static and other dynamic IP addresses make up the address sanitized logs A that are provided to the monitor analyzer for use in the system monitoring analysis functions.

The tool automatically runs an UPDATE cycle at regular intervals e.g. daily in order to update the IP address pools and classify IP addresses in the set as static or dynamic.

The tool may be designed to auto correct its outputs over time. The longer an IP address is monitored the higher the confidence in its classification.

Ideally a mapping between IP addresses and corresponding unique hosts should include the following information The IP address a unique identifier for the host and the start and end timestamps of the period during which the host is assigned this IP address. In one embodiment the MAC address of a host s network interface is used as the unique identifier for the host.

In contrast to static IP addresses which are permanently assigned to the same machine dynamic IP addresses are allocated to different machines at different time periods. This process takes place over the DHCP protocol. In DHCP dynamic IP addresses are leased to hosts for a given time period after which the host must renew its IP assignment request otherwise the DHCP server may reclaim that IP for use by another host. DHCP logs collected by the SIEM system may be parsed to collect information usable to construct a mapping of the known dynamic IP addresses to unique hosts.

It is assumed that a list of all logging devices that report to the SIEM system is known e.g. a list of IP addresses of all logging devices . It is also assumed that the log timestamp translation is done after the logs are collected by the SIEM system i.e. administrator privileges to the logging devices are not available so that the devices clock configurations cannot be modified.

The output of the time sanitization technique is the time zone configuration of each logging device. This is stored in the following format 

Given the above information for each logging device all log timestamps can be translated into UTC by adding the corresponding value to the device timestamp. For example if a log from a device has a timestamp of T2 the adjusted log timestamp for that log message becomes T2 .

One direct approach to detect a device s configured time zone is to send it probes over the network soliciting responses containing clock information. This is difficult in practice because neither the IP UDP or TCP headers include timestamps. Also for security reasons many machines ignore packets sent to unused ports.

In an alternative active approach rather than contacting a logging network device directly events are generated that will be logged and time stamped by the device. For example a Windows domain controller validates user logon events and generates logs describing the outcome of the logon attempts as it does so. Thus log entries and timestamps can be created by performing logons. As another example a web proxy forwards clients HTTP requests and generates logs describing the network connection at the same time. Log entries and timestamps can be created by issuing HTTP requests.

Let the known time at which a testing event E is generated be TE which is represented in UTC time. After the logging device processes this event a log message is created with the device s timestamp TD. In terms of elapsed time the difference between TE and TD is very small e.g. on the order of milliseconds because the same device often performs event processing and log generation. This is true in both the above examples Windows domain controller web proxy .

The difference value TD TE can be calculated rounded off to the nearest 15 minutes since that is the level of granularity at which time zones are set . Since TE is represented in UTC time the device s time zone is hence known to be configured as UTC time .

While the active approach can be quite accurate and efficient it may not be suitable for use in a large network with many different logging devices. In this case events may be directed to different processing logging devices depending on the source host s geographic location or network configuration. Without a comprehensive understanding of the topology of the enterprise network and access to multiple distributed client machines the active approach may become infeasible.

An alternative passive approach may leverage information available in logs collected by a SIEM system to determine the devices clock configuration. The clock configuration in the SIEM system may be static which simplifies the processing. For example the SIEM system may generate all its timestamps in UTC time.

At a high level the passive approach compares the device timestamp TD with the SIEM system timestamp TS for all log messages generated by a device where the SIEM system timestamp TS reflects the time that the SIEM system received the log messages. Let be the difference between TD and TS rounded off to the nearest 15 minutes. From a set of possibly inconsistent values derived from all logs generated by a device over a certain time period e.g. one month a process is employed to determine the correct actual time correction value for the device.

This template could be indicative of an APT attack consisting of the four stages documented in the literature 1 Exploitation 2 Command and control 3 Lateral movement 4 Data exfiltration. The template requires that an entity be found in three out of four reports from respective sensors . The C C sensor output corresponds to phases 2 and or 4 New logins can be mapped to phase 3 After hours could be connected to phase 2 or 4 and Internal staging to phase 4 of the attack.

Since each APT attack proceeds uniquely the above template does not require an exact match of all these sensors but rather a relaxed 3 out of 4 sensor matches. The alerts generated by this template contain highly suspicious hosts that should be further investigated by a human analyst.

This template correlates users who contact new domains and subsequently log in to a machine they have never accessed in the past. This succession of steps is also highly indicative of an APT attack. For instance an internal host under the control of the attacker contacts a new domain corresponding to a C C server the attacker obtains the credentials of the users typically logging in to the compromised host the attacker then decides to use one of the compromised user credentials to perform lateral movement and obtain access to other internal hosts servers.

1 Training Build a history of users and the hosts that the users logged into over a profiling period of Tp days. Build a history of hosts and the web domains they contact over the same profiling period of Tp days.

2 New login Sensor For a detection window of Td days for each day find new operating system logins that were not observed over the profiling period. These are the suspicious users.

3 C C Sensor Find new web domains that were not contacted over the profiling period. These are the suspicious domains. Let the first time or last time host h contacted a suspicious domain be denoted as t1 h or t2 h . Only consider domains where t1 h t2 h.

Further filtering can be applied to domains and users based on the domains connection rate. Such a threshold may be an adjustable threshold that can be tuned by an analyst.

While various embodiments of the invention have been particularly shown and described it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the spirit and scope of the invention as defined by the appended claims.

