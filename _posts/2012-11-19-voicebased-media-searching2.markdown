---

title: Voice-based media searching
abstract: Methods and systems for searching for media items using a voice-based digital assistant are described. Natural language text strings corresponding to search queries are provided. The search queries include query terms. The text strings may correspond to speech inputs input by a user into an electronic device. At least one information source is searched to identify at least one parameter associated with at least one of the query terms. The parameters include at least one of a time parameter, a date parameter, or a geo-code parameter. The parameters are compared to tags of media items to identify matches. In some implementations, media items whose tags match the parameter are presented to the user.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09547647&OS=09547647&RS=09547647
owner: APPLE INC.
number: 09547647
owner_city: Cupertino
owner_country: US
publication_date: 20121119
---
This application claims priority to U.S. Provisional Application Ser. No. 61 703 176 filed Sep. 19 2012 which is incorporated herein by reference in its entirety.

The disclosed implementations relate generally to digital assistant systems and more specifically to a method and system for voice based media searching.

Advances in camera technology image processing and image storage technology have enabled humans to seamlessly interact with and capture their surroundings through digital photography. Moreover recent advances in technology surrounding hand held devices e.g. mobile phones and digital assistant systems have improved image capture and image storage capabilities on hand held devices. This has led to a substantial increase in the use of hand held devices for photo acquisition and digital photo storage.

The growing volume of digital photographs acquired and stored on electronic devices has created a need for systematic cataloging and efficient organization of the photographs in order to enable ease of viewing and searching. Tagging of photographs for example by associating with the photograph names of people or places facilitates the ease of organizing and searching for photographs. Other information such as time date and GPS coordinate data are also increasingly associated with photographs allowing efficient sorting and organization.

While photo capture and digital image storage technology has improved substantially over the past decade traditional approaches to photo searching can be non intuitive arduous and time consuming.

Accordingly there is a need for a simple intuitive user friendly way to search for photographs. The present invention provides systems and methods for voice based photo searching implemented at an electronic device.

Implementations described below provide a method and system of voice based photo searching through the use of natural language processing techniques. Natural language processing techniques are deployed to enable users to interact in spoken or textual forms with hand held devices and digital assistant systems whereby digital assistant systems can interpret the user s input to deduce the user s intent translate the deduced intent into actionable tasks and parameters execute operations or deploy services to perform the tasks and produce output that is intelligible to the user.

Voice based photo searching dramatically increases the speed and convenience of photo searching. For example by combining speech recognition techniques with intelligent natural language processing the disclosed implementations enable users simply to speak a description of the photographs that they want to search for such as show me my vacation photos and the photos will be automatically identified and returned to the user for viewing editing and the like. The disclosed techniques are able to process this speech based input in order to find and retrieve relevant photographs even where the photographs have not been previously associated with user generated textual tags such as vacation or beach. Rather metadata that is stored with digital photographs when they are captured or saved is cross referenced with other user information to facilitate searching. For example a calendar entry indicating that a user s vacation spans a certain set of days can be used to create a search query to find photographs taken or saved on those dates. As another example a table associating geo codes with locations may be consulted to determine a range of geo codes that corresponds to a location identified in a search query. The user s photographs can then be searched to find those whose geo codes correspond to the identified location.

Thus the implementations disclosed herein provide methods systems and computer readable storage media that enable voice based natural language photo searching.

Some implementations provide a method for searching for media items using a voice based digital assistant. The method is performed at an electronic device with a processor and memory storing instructions for execution by the processor. The method includes providing multiple media items. Media items may include photographs videos and or audio. At least some of the media items are each associated with a respective tag comprising at least one of a time tag a date tag or a geo code tag. In some implementations tags are stored with media items as metadata.

The method further includes providing a natural language text string corresponding to a search query for one or more media items where the search query includes one or more query terms. In some implementations the text string corresponds to a speech input from a user. In some implementations the speech input is converted to text using speech to text processing.

The method further includes searching at least one information source to identify at least one parameter associated with at least one of the one or more query terms. In some implementations the information source is a calendar email text messages social network postings a contact book and or the like. The at least one parameter comprises at least one of a time parameter a date parameter or a geo code parameter. The at least one parameter is separate from the tags associated with the multiple media items e.g. the parameter is not part of a media item s metadata.

The method further includes comparing the respective tags to the at least one parameter to identify at least one media item whose tag matches the identified parameter and facilitating the presentation of the at least one media item to a user. In some implementations facilitating the presentation includes displaying the at least one media item to the user.

In accordance with some embodiments an electronic device including one or more processors memory and one or more programs stored in the memory and configured to be executed by the one or more processors include instructions for performing the operations of any of the methods described above. In accordance with some embodiments a non transitory computer readable storage medium has stored therein instructions which when executed by an electronic device cause the device to perform the operations of any of the methods described above

Specifically a digital assistant system is capable of accepting a user request at least partially in the form of a natural language command request statement narrative and or inquiry. Typically the user request seeks either an informational answer or performance of a task by the digital assistant system. A satisfactory response to the user request is generally either provision of the requested informational answer performance of the requested task or a combination of the two. For example a user may ask the digital assistant system a question such as Where am I right now Based on the user s current location the digital assistant may answer You are in Central Park near the west gate. The user may also request the performance of a task for example by stating Please invite my friends to my girlfriend s birthday party next week. In response the digital assistant may acknowledge the request by generating a voice output Yes right away and then send a suitable calendar invite from the user s email address to each of the user friends listed in the user s electronic address book or contact list. There are numerous other ways of interacting with a digital assistant to request information or performance of various tasks. In addition to providing verbal responses and taking programmed actions the digital assistant can also provide responses in other visual or audio forms e.g. as text alerts music videos animations etc. .

As shown in in some implementations a digital assistant system is implemented according to a client server model. The digital assistant system includes a client side portion e.g. and hereafter digital assistant DA client executed on a user device e.g. and and a server side portion hereafter digital assistant DA server executed on a server system . The DA client communicates with the DA server through one or more networks . The DA client provides client side functionalities such as user facing input and output processing and communications with the DA server . The DA server provides server side functionalities for any number of DA clients each residing on a respective user device also called a client device .

In some implementations the DA server includes a client facing I O interface one or more processing modules data and models an I O interface to external services a photo and tag database and a photo tag module . The client facing I O interface facilitates the client facing input and output processing for the digital assistant server . The one or more processing modules utilize the data and models to determine the user s intent based on natural language input and perform task execution based on the deduced user intent. Photo and tag database stores fingerprints of digital photographs and optionally digital photographs themselves as well as metadata and user and or automatically generated tags associated with the digital photographs. Photo tag module creates tags stores tags in association with photographs and or fingerprints automatically tags photographs and links tags to locations within photographs.

In some implementations the DA server communicates with external services e.g. navigation service s messaging service s information service s calendar service telephony service photo service s social networking service s etc. through the network s for task completion or information acquisition. The I O interface to the external services facilitates such communications.

Examples of the user device include but are not limited to a handheld computer a personal digital assistant PDA a tablet computer a laptop computer a desktop computer a cellular telephone a smartphone an enhanced general packet radio service EGPRS mobile phone a media player a navigation device a game console a television a remote control or a combination of any two or more of these data processing devices or any other suitable data processing devices. More details on the user device are provided in reference to an exemplary user device shown in .

Examples of the communication network s include local area networks LAN and wide area networks WAN e.g. the Internet. The communication network s may be implemented using any known network protocol including various wired or wireless protocols such as Ethernet Universal Serial Bus USB FIREWIRE Global System for Mobile Communications GSM Enhanced Data GSM Environment EDGE code division multiple access CDMA time division multiple access TDMA Bluetooth Wi Fi voice over Internet Protocol VoIP Wi MAX or any other suitable communication protocol.

The server system can be implemented on at least one data processing apparatus and or a distributed network of computers. In some implementations the server system also employs various virtual devices and or services of third party service providers e.g. third party cloud service providers to provide the underlying computing resources and or infrastructure resources of the server system .

Although the digital assistant system shown in includes both a client side portion e.g. the DA client and a server side portion e.g. the DA server in some implementations a digital assistant system refers only to the server side portion e.g. the DA server . In some implementations the functions of a digital assistant can be implemented as a standalone application installed on a user device. In addition the divisions of functionalities between the client and server portions of the digital assistant can vary in different implementations. For example in some implementations the DA client is a thin client that provides only user facing input and output processing functions and delegates all other functionalities of the digital assistant to the DA server . In some other implementations the DA client is configured to perform or assist one or more functions of the DA server .

For example in some implementations a motion sensor e.g. an accelerometer a light sensor a GPS receiver a temperature sensor and compass and a proximity sensor are coupled to the peripherals interface to facilitate orientation light and proximity sensing functions. In some implementations other sensors such as a biometric sensor barometer and the like are connected to the peripherals interface to facilitate related functionalities.

In some implementations the user device includes a camera subsystem coupled to the peripherals interface . In some implementations an optical sensor of the camera subsystem facilitates camera functions such as taking photographs and recording video clips. In some implementations photographs and video clips are associated with metadata when they are taken by the camera subsystem . Metadata includes for example a date tag a time tag and a location tag e.g. a geo code tag . These tags identify respectively the date time and location of the photo or video. These tags can be used for search and categorization functionality by the user device and or the digital assistant system as a whole as described below.

In some implementations the user device includes one or more wired and or wireless communication subsystems provide communication functions. The communication subsystems typically includes various communication ports radio frequency receivers and transmitters and or optical e.g. infrared receivers and transmitters. In some implementations the user device includes an audio subsystem coupled to one or more speakers and one or more microphones to facilitate voice enabled functions such as voice recognition voice replication digital recording and telephony functions.

In some implementations an I O subsystem is also coupled to the peripheral interface . In some implementations the user device includes a touch screen and the I O subsystem includes a touch screen controller coupled to the touch screen . When the user device includes the touch screen and the touch screen controller the touch screen and the touch screen controller are typically configured to for example detect contact and movement or break thereof using any of a plurality of touch sensitivity technologies such as capacitive resistive infrared surface acoustic wave technologies proximity sensor arrays and the like. In some implementations the user device includes a display that does not include a touch sensitive surface. In some implementations the user device includes a separate touch sensitive surface. In some implementations the user device includes other input controller s . When the user device includes the other input controller s the other input controller s are typically coupled to other input control devices such as one or more buttons rocker switches thumb wheel infrared port USB port and or a pointer device such as a stylus.

The memory interface is coupled to memory . In some implementations memory includes a non transitory computer readable medium such as high speed random access memory and or non volatile memory e.g. one or more magnetic disk storage devices one or more flash memory devices one or more optical storage devices and or other non volatile solid state memory devices .

In some implementations memory stores an operating system a communications module a graphical user interface module a sensor processing module a phone module and applications and a subset or superset thereof. The operating system includes instructions for handling basic system services and for performing hardware dependent tasks. The communications module facilitates communicating with one or more additional devices one or more computers and or one or more servers. The graphical user interface module facilitates graphic user interface processing. The sensor processing module facilitates sensor related processing and functions e.g. processing voice input received with the one or more microphones . The phone module facilitates phone related processes and functions. The application module facilitates various functionalities of user applications such as electronic messaging web browsing media processing navigation imaging and or other processes and functions. In some implementations the user device stores in memory one or more software applications and each associated with at least one of the external service providers.

As described above in some implementations memory also stores client side digital assistant instructions e.g. in a digital assistant client module and various user data e.g. user specific vocabulary data preference data and or other data such as the user s electronic address book or contact list to do lists shopping lists etc. to provide the client side functionalities of the digital assistant.

In various implementations the digital assistant client module is capable of accepting voice input text input touch input and or gestural input through various user interfaces e.g. the I O subsystem of the user device . The digital assistant client module is also capable of providing output in audio visual and or tactile forms. For example output can be provided as voice sound alerts text messages menus graphics videos animations vibrations and or combinations of two or more of the above. During operation the digital assistant client module communicates with the digital assistant server e.g. the digital assistant server using the communication subsystems .

In some implementations the digital assistant client module utilizes various sensors subsystems and peripheral devices to gather additional information from the surrounding environment of the user device to establish a context associated with a user input. In some implementations the digital assistant client module provides the context information or a subset thereof with the user input to the digital assistant server e.g. the digital assistant server to help deduce the user s intent.

In some implementations the context information that can accompany the user input includes sensor information e.g. lighting ambient noise ambient temperature images or videos of the surrounding environment etc. In some implementations the context information also includes the physical state of the device e.g. device orientation device location device temperature power level speed acceleration motion patterns cellular signals strength etc. In some implementations information related to the software state of the user device e.g. running processes installed programs past and present network activities background services error logs resources usage etc. of the user device is also provided to the digital assistant server e.g. the digital assistant server as context information associated with a user input.

In some implementations the DA client module selectively provides information e.g. at least a portion of the user data stored on the user device in response to requests from the digital assistant server. In some implementations the digital assistant client module also elicits additional input from the user via a natural language dialogue or other user interfaces upon request by the digital assistant server . The digital assistant client module passes the additional input to the digital assistant server to help the digital assistant server in intent deduction and or fulfillment of the user s intent expressed in the user request.

In some implementations memory may include additional instructions or fewer instructions. Furthermore various functions of the user device may be implemented in hardware and or in firmware including in one or more signal processing and or application specific integrated circuits and the user device thus need not include all modules and applications illustrated in .

The digital assistant system includes memory one or more processors an input output I O interface and a network communications interface . These components communicate with one another over one or more communication buses or signal lines .

In some implementations memory includes a non transitory computer readable medium such as high speed random access memory and or a non volatile computer readable storage medium e.g. one or more magnetic disk storage devices one or more flash memory devices one or more optical storage devices and or other non volatile solid state memory devices .

The I O interface couples input output devices of the digital assistant system such as displays a keyboards touch screens and microphones to the user interface module . The I O interface in conjunction with the user interface module receives user inputs e.g. voice input keyboard inputs touch inputs etc. and process them accordingly. In some implementations when the digital assistant is implemented on a standalone user device the digital assistant system includes any of the components and I O and communication interfaces described with respect to the user device in e.g. one or more microphones . In some implementations the digital assistant system represents the server portion of a digital assistant implementation and interacts with the user through a client side portion residing on a user device e.g. the user device shown in .

In some implementations the network communications interface includes wired communication port s and or wireless transmission and reception circuitry . The wired communication port s receive and send communication signals via one or more wired interfaces e.g. Ethernet Universal Serial Bus USB FIREWIRE etc. The wireless circuitry typically receives and sends RF signals and or optical signals from to communications networks and other communications devices. The wireless communications may use any of a plurality of communications standards protocols and technologies such as GSM EDGE CDMA TDMA Bluetooth Wi Fi VoIP Wi MAX or any other suitable communication protocol. The network communications interface enables communication between the digital assistant system with networks such as the Internet an intranet and or a wireless network such as a cellular telephone network a wireless local area network LAN and or a metropolitan area network MAN and other devices.

In some implementations the non transitory computer readable storage medium of memory stores programs modules instructions and data structures including all or a subset of an operating system a communications module a user interface module one or more applications and a digital assistant module . The one or more processors execute these programs modules and instructions and reads writes from to the data structures.

The operating system e.g. Darwin RTXC LINUX UNIX OS X iOS WINDOWS or an embedded operating system such as VxWorks includes various software components and or drivers for controlling and managing general system tasks e.g. memory management storage device control power management etc. and facilitates communications between various hardware firmware and software components.

The communications module facilitates communications between the digital assistant system with other devices over the network communications interface . For example the communication module may communicate with the communications module of the device shown in . The communications module also includes various software components for handling data received by the wireless circuitry and or wired communications port .

In some implementations the user interface module receives commands and or inputs from a user via the I O interface e.g. from a keyboard touch screen and or microphone and provides user interface objects on a display.

The applications include programs and or modules that are configured to be executed by the one or more processors . For example if the digital assistant system is implemented on a standalone user device the applications may include user applications such as games a calendar application a navigation application or an email application. If the digital assistant system is implemented on a server farm the applications may include resource management applications diagnostic applications or scheduling applications for example.

Memory also stores the digital assistant module or the server portion of a digital assistant . In some implementations the digital assistant module includes the following sub modules or a subset or superset thereof an input output processing module a speech to text STT processing module a natural language processing module a dialogue flow processing module a task flow processing module a service processing module and a photo module . Each of these processing modules has access to one or more of the following data and models of the digital assistant or a subset or superset thereof ontology vocabulary index user data categorization module disambiguation module task flow models service models search module and local tag photo storage .

In some implementations using the processing modules e.g. the input output processing module the STT processing module the natural language processing module the dialogue flow processing module the task flow processing module and or the service processing module data and models implemented in the digital assistant module the digital assistant system performs at least some of the following identifying a user s intent expressed in a natural language input received from the user actively eliciting and obtaining information needed to fully deduce the user s intent e.g. by disambiguating words names intentions etc. determining the task flow for fulfilling the deduced intent and executing the task flow to fulfill the deduced intent. In some implementations the digital assistant also takes appropriate actions when a satisfactory response was not or could not be provided to the user for various reasons.

In some implementations as discussed below the digital assistant system identifies from a natural language input a user s intent to search for photographs. The digital assistant system processes the natural language input so as to determine what photographs may be relevant to the user s search query. In some implementations the digital assistant system performs other tasks related to photographs as well such as tagging digital photographs as described for example in Voice Based Image Tagging and Searching U.S. application Ser. No. 13 801 534 filed Mar. 13 2013 which is incorporated by reference herein in its entirety.

As shown in in some implementations the I O processing module interacts with the user through the I O devices in or with a user device e.g. a user device in through the network communications interface in to obtain user input e.g. a speech input and to provide responses to the user input. The I O processing module optionally obtains context information associated with the user input from the user device along with or shortly after the receipt of the user input. The context information includes user specific data vocabulary and or preferences relevant to the user input. In some implementations the context information also includes software and hardware states of the device e.g. the user device in at the time the user request is received and or information related to the surrounding environment of the user at the time that the user request was received. In some implementations the I O processing module also sends follow up questions to and receives answers from the user regarding the user request. In some implementations when a user request is received by the I O processing module and the user request contains a speech input the I O processing module forwards the speech input to the speech to text STT processing module for speech to text conversions.

In some implementations the speech to text processing module receives speech input e.g. a user utterance captured in a voice recording through the I O processing module . In some implementations the speech to text processing module uses various acoustic and language models to recognize the speech input as a sequence of phonemes and ultimately a sequence of words or tokens written in one or more languages. The speech to text processing module is implemented using any suitable speech recognition techniques acoustic models and language models such as Hidden Markov Models Dynamic Time Warping DTW based speech recognition and other statistical and or analytical techniques. In some implementations the speech to text processing can be performed at least partially by a third party service or on the user s device. Once the speech to text processing module obtains the result of the speech to text processing e.g. a sequence of words or tokens it passes the result to the natural language processing module for intent deduction.

The natural language processing module natural language processor of the digital assistant takes the sequence of words or tokens token sequence generated by the speech to text processing module and attempts to associate the token sequence with one or more actionable intents recognized by the digital assistant. As used herein an actionable intent represents a task that can be performed by the digital assistant and or the digital assistant system and has an associated task flow implemented in the task flow models . The associated task flow is a series of programmed actions and steps that the digital assistant system takes in order to perform the task. The scope of a digital assistant system s capabilities is dependent on the number and variety of task flows that have been implemented and stored in the task flow models or in other words on the number and variety of actionable intents that the digital assistant system recognizes. The effectiveness of the digital assistant system however is also dependent on the digital assistant system s ability to deduce the correct actionable intent s from the user request expressed in natural language.

In some implementations in addition to the sequence of words or tokens obtained from the speech to text processing module the natural language processor also receives context information associated with the user request e.g. from the I O processing module . The natural language processor optionally uses the context information to clarify supplement and or further define the information contained in the token sequence received from the speech to text processing module . The context information includes for example user preferences hardware and or software states of the user device sensor information collected before during or shortly after the user request prior interactions e.g. dialogue between the digital assistant and the user and the like.

In some implementations the natural language processing is based on an ontology . The ontology is a hierarchical structure containing a plurality of nodes each node representing either an actionable intent or a property relevant to one or more of the actionable intents or other properties. As noted above an actionable intent represents a task that the digital assistant system is capable of performing e.g. a task that is actionable or can be acted on . A property represents a parameter associated with an actionable intent or a sub aspect of another property. A linkage between an actionable intent node and a property node in the ontology defines how a parameter represented by the property node pertains to the task represented by the actionable intent node.

In some implementations the ontology is made up of actionable intent nodes and property nodes. Within the ontology each actionable intent node is linked to one or more property nodes either directly or through one or more intermediate property nodes. Similarly each property node is linked to one or more actionable intent nodes either directly or through one or more intermediate property nodes. For example the ontology shown in includes a restaurant reservation node which is an actionable intent node. Property nodes restaurant date time for the reservation and party size are each directly linked to the restaurant reservation node i.e. the actionable intent node . In addition property nodes cuisine price range phone number and location are sub nodes of the property node restaurant and are each linked to the restaurant reservation node i.e. the actionable intent node through the intermediate property node restaurant. For another example the ontology shown in also includes a set reminder node which is another actionable intent node. Property nodes date time for the setting the reminder and subject for the reminder are each linked to the set reminder node. Since the property date time is relevant to both the task of making a restaurant reservation and the task of setting a reminder the property node date time is linked to both the restaurant reservation node and the set reminder node in the ontology .

An actionable intent node along with its linked concept nodes may be described as a domain. In the present discussion each domain is associated with a respective actionable intent and refers to the group of nodes and the relationships therebetween associated with the particular actionable intent. For example the ontology shown in includes an example of a restaurant reservation domain and an example of a reminder domain within the ontology . The restaurant reservation domain includes the actionable intent node restaurant reservation property nodes restaurant date time and party size and sub property nodes cuisine price range phone number and location. The reminder domain includes the actionable intent node set reminder and property nodes subject and date time. In some implementations the ontology is made up of many domains. Each domain may share one or more property nodes with one or more other domains. For example the date time property node may be associated with many other domains e.g. a scheduling domain a travel reservation domain a movie ticket domain etc. in addition to the restaurant reservation domain and the reminder domain .

While illustrates two exemplary domains within the ontology the ontology may include other domains or actionable intents such as initiate a phone call find directions schedule a meeting send a message and provide an answer to a question tag a photo and so on. For example a send a message domain is associated with a send a message actionable intent node and may further include property nodes such as recipient s message type and message body. The property node recipient may be further defined for example by the sub property nodes such as recipient name and message address. 

In some implementations the ontology includes all the domains and hence actionable intents that the digital assistant is capable of understanding and acting upon. In some implementations the ontology may be modified such as by adding or removing domains or nodes or by modifying relationships between the nodes within the ontology .

In some implementations nodes associated with multiple related actionable intents may be clustered under a super domain in the ontology . For example a travel super domain may include a cluster of property nodes and actionable intent nodes related to travels. The actionable intent nodes related to travels may include airline reservation hotel reservation car rental get directions find points of interest and so on. The actionable intent nodes under the same super domain e.g. the travels super domain may have many property nodes in common. For example the actionable intent nodes for airline reservation hotel reservation car rental get directions find points of interest may share one or more of the property nodes start location destination departure date time arrival date time and party size. 

In some implementations each node in the ontology is associated with a set of words and or phrases that are relevant to the property or actionable intent represented by the node. The respective set of words and or phrases associated with each node is the so called vocabulary associated with the node. The respective set of words and or phrases associated with each node can be stored in the vocabulary index in association with the property or actionable intent represented by the node. For example returning to the vocabulary associated with the node for the property of restaurant may include words such as food drinks cuisine hungry eat pizza fast food meal and so on. For another example the vocabulary associated with the node for the actionable intent of initiate a phone call may include words and phrases such as call phone dial ring call this number make a call to and so on. The vocabulary index optionally includes words and phrases in different languages.

In some implementations the natural language processor shown in receives the token sequence e.g. a text string from the speech to text processing module and determines what nodes are implicated by the words in the token sequence. In some implementations if a word or phrase in the token sequence is found to be associated with one or more nodes in the ontology via the vocabulary index the word or phrase will trigger or activate those nodes. When multiple nodes are triggered based on the quantity and or relative importance of the activated nodes the natural language processor will select one of the actionable intents as the task or task type that the user intended the digital assistant to perform. In some implementations the domain that has the most triggered nodes is selected. In some implementations the domain having the highest confidence value e.g. based on the relative importance of its various triggered nodes is selected. In some implementations the domain is selected based on a combination of the number and the importance of the triggered nodes. In some implementations additional factors are considered in selecting the node as well such as whether the digital assistant system has previously correctly interpreted a similar request from a user.

In some implementations the digital assistant system also stores names of specific entities in the vocabulary index so that when one of these names is detected in the user request the natural language processor will be able to recognize that the name refers to a specific instance of a property or sub property in the ontology. In some implementations the names of specific entities are names of businesses restaurants people movies and the like. In some implementations the digital assistant system can search and identify specific entity names from other data sources such as the user s address book or contact list a movies database a musicians database and or a restaurant database. In some implementations when the natural language processor identifies that a word in the token sequence is a name of a specific entity such as a name in the user s address book or contact list that word is given additional significance in selecting the actionable intent within the ontology for the user request.

For example when the words Mr. Santo are recognized from the user request and the last name Santo is found in the vocabulary index as one of the contacts in the user s contact list then it is likely that the user request corresponds to a send a message or initiate a phone call domain. For another example when the words ABC Caf are found in the user request and the term ABC Caf is found in the vocabulary index as the name of a particular restaurant in the user s city then it is likely that the user request corresponds to a restaurant reservation domain.

User data includes user specific information such as user specific vocabulary user preferences user address user s default and secondary languages user s contact list and other short term or long term information for each user. The natural language processor can use the user specific information to supplement the information contained in the user input to further define the user intent. For example for a user request invite my friends to my birthday party the natural language processor is able to access user data to determine who the friends are and when and where the birthday party would be held rather than requiring the user to provide such information explicitly in his her request.

In some implementations natural language processor includes categorization module . In some implementations the categorization module determines whether each of the one or more terms in a text string e.g. corresponding to a speech input associated with a digital photograph is one of an entity an activity or a location as discussed in greater detail below. In some implementations the categorization module classifies each term of the one or more terms as one of an entity an activity or a location.

Once the natural language processor identifies an actionable intent or domain based on the user request the natural language processor generates a structured query to represent the identified actionable intent. In some implementations the structured query includes parameters for one or more nodes within the domain for the actionable intent and at least some of the parameters are populated with the specific information and requirements specified in the user request. For example the user may say Make me a dinner reservation at a sushi place at 7. In this case the natural language processor may be able to correctly identify the actionable intent to be restaurant reservation based on the user input. According to the ontology a structured query for a restaurant reservation domain may include parameters such as Cuisine Time Date Party Size and the like. Based on the information contained in the user s utterance the natural language processor may generate a partial structured query for the restaurant reservation domain where the partial structured query includes the parameters Cuisine Sushi and Time 7 pm . However in this example the user s utterance contains insufficient information to complete the structured query associated with the domain. Therefore other necessary parameters such as Party Size and Date are not specified in the structured query based on the information currently available. In some implementations the natural language processor populates some parameters of the structured query with received context information. For example if the user requested a sushi restaurant near me the natural language processor may populate a location parameter in the structured query with GPS coordinates from the user device .

In some implementations the natural language processor passes the structured query including any completed parameters to the task flow processing module task flow processor . The task flow processor is configured to perform one or more of receiving the structured query from the natural language processor completing the structured query and performing the actions required to complete the user s ultimate request. In some implementations the various procedures necessary to complete these tasks are provided in task flow models . In some implementations the task flow models include procedures for obtaining additional information from the user and task flows for performing actions associated with the actionable intent.

As described above in order to complete a structured query the task flow processor may need to initiate additional dialogue with the user in order to obtain additional information and or disambiguate potentially ambiguous utterances. When such interactions are necessary the task flow processor invokes the dialogue processing module dialogue processor to engage in a dialogue with the user. In some implementations the dialogue processing module determines how and or when to ask the user for the additional information and receives and processes the user responses. In some implementations the questions are provided to and answers are received from the users through the I O processing module . For example the dialogue processing module presents dialogue output to the user via audio and or visual output and receives input from the user via spoken or physical e.g. touch gesture responses. Continuing with the example above when the task flow processor invokes the dialogue processor to determine the party size and date information for the structured query associated with the domain restaurant reservation the dialogue processor generates questions such as For how many people and On which day to pass to the user. Once answers are received from the user the dialogue processing module populates the structured query with the missing information or passes the information to the task flow processor to complete the missing information from the structured query.

In some cases the task flow processor may receive a structured query that has one or more ambiguous properties. For example a structured query for the send a message domain may indicate that the intended recipient is Bob and the user may have multiple contacts named Bob. The task flow processor will request that the dialogue processor disambiguate this property of the structured query. In turn the dialogue processor may ask the user Which Bob and display or read a list of contacts named Bob from which the user may choose.

In some implementations dialogue processor includes disambiguation module . In some implementations disambiguation module disambiguates one or more ambiguous terms e.g. one or more ambiguous terms in a text string corresponding to a speech input associated with a digital photograph . In some implementations disambiguation module identifies that a first term of the one or more terms has multiple candidate meanings prompts a user for additional information about the first term receives the additional information from the user in response to the prompt and identifies the entity activity or location associated with the first term in accordance with the additional information.

In some implementations disambiguation module disambiguates pronouns. In such implementations disambiguation module identifies one of the one or more terms as a pronoun and determines a noun to which the pronoun refers. In some implementations disambiguation module determines a noun to which the pronoun refers by using a contact list associated with a user of the electronic device. Alternatively or in addition disambiguation module determines a noun to which the pronoun refers as a name of an entity an activity or a location identified in a previous speech input associated with a previously tagged digital photograph. Alternatively or in addition disambiguation module determines a noun to which the pronoun refers as a name of a person identified based on a previous speech input associated with a previously tagged digital photograph.

In some implementations disambiguation module accesses information obtained from one or more sensors e.g. proximity sensor light sensor GPS receiver temperature sensor and motion sensor of a handheld electronic device e.g. user device for determining a meaning of one or more of the terms. In some implementations disambiguation module identifies two terms each associated with one of an entity an activity or a location. For example a first of the two terms refers to a person and a second of the two terms refers to a location. In some implementations disambiguation module identifies three terms each associated with one of an entity an activity or a location.

Once the task flow processor has completed the structured query for an actionable intent the task flow processor proceeds to perform the ultimate task associated with the actionable intent. Accordingly the task flow processor executes the steps and instructions in the task flow model according to the specific parameters contained in the structured query. For example the task flow model for the actionable intent of restaurant reservation may include steps and instructions for contacting a restaurant and actually requesting a reservation for a particular party size at a particular time. For example using a structured query such as restaurant reservation restaurant ABC Caf date 3 12 2012 time 7 pm party size 5 the task flow processor may perform the steps of 1 logging onto a server of the ABC Caf or a restaurant reservation system that is configured to accept reservations for multiple restaurants such as the ABC Caf 2 entering the date time and party size information in a form on the website 3 submitting the form and 4 making a calendar entry for the reservation in the user s calendar. In another example described in greater detail below the task flow processor executes steps and instructions associated with tagging or searching for digital photographs in response to a voice input e.g. in conjunction with photo module .

In some implementations the task flow processor employs the assistance of a service processing module service processor to complete a task requested in the user input or to provide an informational answer requested in the user input. For example the service processor can act on behalf of the task flow processor to make a phone call set a calendar entry invoke a map search invoke or interact with other user applications installed on the user device and invoke or interact with third party services e.g. a restaurant reservation portal a social networking website or service a banking portal etc. . In some implementations the protocols and application programming interfaces API required by each service can be specified by a respective service model among the service models . The service processor accesses the appropriate service model for a service and generates requests for the service in accordance with the protocols and APIs required by the service according to the service model.

For example if a restaurant has enabled an online reservation service the restaurant can submit a service model specifying the necessary parameters for making a reservation and the APIs for communicating the values of the necessary parameters to the online reservation service. When requested by the task flow processor the service processor can establish a network connection with the online reservation service using the web address stored in the service models and send the necessary parameters of the reservation e.g. time date party size to the online reservation interface in a format according to the API of the online reservation service.

In some implementations the natural language processor dialogue processor and task flow processor are used collectively and iteratively to deduce and define the user s intent obtain information to further clarify and refine the user intent and finally generate a response e.g. provide an output to the user or complete a task to fulfill the user s intent.

In some implementations after all of the tasks needed to fulfill the user s request have been performed the digital assistant formulates a confirmation response and sends the response back to the user through the I O processing module . If the user request seeks an informational answer the confirmation response presents the requested information to the user. In some implementations the digital assistant also requests the user to indicate whether the user is satisfied with the response produced by the digital assistant .

In some implementations the digital assistant includes a photo module . In some implementations the photo module acts in conjunction with the task flow processing module to search for digital photographs in response to a user input.

The photo module performs operations on and searches for photographs. For example in some implementations the photo module creates tags retrieves tags associated with fingerprints of a digital photograph associates tags with photographs e.g. tagging the photograph searches a photo database e.g. the photo and tag database based on a user input to identify photographs and locally stores photographs each in association with one or more tags.

In some implementations the photo module includes a search module . In some implementations the search module generates search queries used for searching digital photographs based on speech input as explained in further detail with reference to Method below. For example in some implementations the photo module receives a natural language text string corresponding to a photo search request and consults an information source such as a calendar social network etc. e.g. information service calendar service to identify information with which to perform a search of photo metadata. As described below the information may include date parameters time parameters or geo code parameters. The search module identifies from a collection of photographs e.g. from the photo and tag database one or more photographs associated with a tag that matches one of the parameters. For example for a received voice input corresponding to the search string find photos of my last vacation the search module consults a calendar to determine a date range of the user s last vacation and searches among the user s photographs for any that were taken within that date range.

In some implementations the photo module includes local tag photo storage . In some implementations after the camera subsystem takes a photograph or a video the local tag photo storage stores the photographs and tags including for example metadata tags including date time and geo code tags in association with the digital photograph or video. In some implementations the local tag photo storage stores the tags jointly with the corresponding digital photograph s . Alternatively or in addition the local tag photo storage stores the tags in a remote location e.g. on a separate memory storage device from the corresponding photograph s but stores links or indexes to the corresponding photographs in association with the stored tags.

Photographs are typically associated with metadata tags when they are captured and or stored by a camera scanner etc. For example digital photographs often include metadata tags including a date stamp and or a time stamp that identifies when a photograph was taken and a geo code that identifies where the photograph was taken. Metadata tags are written into or otherwise associated with the digital file e.g. the data is stored in or appears as the file s properties. Metadata formats such as Exchangeable Image File Format EXIF and or Extensible Metadata Platform XMP may be used. Taken alone this information is of limited use. Specifically a photograph taken during a user s birthday will be tagged with a date stamp of that day e.g. 8 16 2010 but in order to search for this photograph the user must actually enter the actual date. Of course the user could take the time to review his photographs and assign keyword tags to the photographs to make searching easier e.g. tagging the photo with the word birthday but this is cumbersome and time consuming and many uses do not take the time to prospectively assign tags for later searching and or categorization.

However there are many information sources that are available to a digital assistant system that can be used to correlate calendar dates as well as times and geo codes with names of events places people etc. For example a calendar associated with the user e.g. the calendar service or a calendar application stored on and or provided by a user device may contain information that links a semantic meaning such as birthday to a date such as August 16th. By using such information sources the digital assistant system is able to resolve natural language search queries e.g. show me pictures from my birthday into a query based on date time or geo code so that photographs can be searched based on the metadata tags that are already associated with the photographs.

Many types of information sources can be used to resolve natural language search queries into dates times and or geo codes that can be used to search for photographs. An electronic calendar for example may contain significant amounts of information relating to events people times or locations that can be used to identify specific dates and or times to be used for photo searching. For example a calendar may include an entry indicating that a friend s birthday party is on a certain day and at a certain time e.g. Tina s Birthday Party . If a user requests Pictures from Tina s Party the digital assistant will determine the date on which Tina s Party occurred and search for photographs having corresponding date tags and time tags. As discussed below the digital assistant may expand the search range by an appropriate margin so that all relevant photographs are identified even if they fall outside the explicit time date or location range indicated by the information source. As another example a calendar may associate the names of holidays e.g. national holidays religious holidays work holidays etc. with specific dates. Thus a user can search for Pics from the Labor Day Picnic and the digital assistant after accessing the calendar will search for photos taken on Labor Day e.g. 10 5 2011 . In some implementations if the calendar simply indicates the date of the Labor Day holiday then the digital assistant searches for any photograph taken on that day or over that weekend . In some implementations the digital assistant uses other words in the user s natural language query to refine or modify the dates times or locations of the photo search. In this case the term picnic may be associated with events that occur primarily after 12 00 AM and or that occur at a park or recreational area. Accordingly the digital assistant may search for photographs taken in the afternoon or at likely picnic locations on Labor Day. In some implementations if the user has a calendar entry for Labor Day Picnic that spans a certain time range the search will focus on photographs taken within that specific time range plus an appropriate margin if necessary or desired .

In some implementations calendars or other data services e.g. information services convert relative or ambiguous date time references in a user s query to actual dates and or times. For example in some implementations a calendar or service converts terms such as yesterday last week this morning the other day the other week and the past week e.g. relative and or ambiguous time date references to absolute date and or time values or ranges. Where the query term is relative but not ambiguous it will be converted into the absolute date time to which it corresponds e.g. yesterday and last week refer to specific absolute date time ranges but are couched in relative terms . Where the query term is ambiguous e.g. the other day or earlier today the calendar or service will convert the reference to a date time range that is appropriate given the query term. For example the term the other day may be converted to a date range of the previous 1 3 5 or days or any other number of days . The term the other week may be converted to a date range of the previous 1 3 5 or 10 weeks or any other number of weeks . The term this morning may be converted to a time range of 12 00 AM to 12 00 PM on that same day.

Also calendar entries often contain location information that can be used by the digital assistant to facilitate photo searching. For example a calendar entry that states Vacation in Maui or Picnic at Foothills Park provides information linking location information to dates and event information e.g. vacation and picnic . Thus the digital assistant can determine what dates are associated with a query that specifies only a location such as pics from Maui or what geo codes are associated with a query that specifies only an event such as display vacation photos. These dates or geo codes can then be used as search parameters to identify photographs with corresponding date tags or geo code tags.

While the above examples describe using calendars to identify dates times and or geo codes for photo searching other information sources may have the same or similar information. For example such information may be available from a user s social networks accounts contact lists digital wallet travel itineraries receipts and the like. In the case of social networks a user may post information to the social network such as just returned from a vacation in Maui Accordingly if a user requests vacation photos a query that lacks time or location information the social network post can be used to determine what geo codes are likely to be associated with her vacation photos e.g. geo codes associated with Maui . Also contact lists may include information such as birthdays anniversaries and or addresses. Accordingly if a user says to a digital assistant bring up pictures from Jim s birthday Jim s birthdate can be identified from the contact list in order to search for photos with a corresponding date tag. In another example if a user requests Photos from last week s trip the digital assistant may use travel itineraries or receipts e.g. for planes trains rental cars etc. to determine dates times and or locations of that vacation. Information sources may be available from any number of sources and or locations. For example in some implementations calendars contact lists digital wallets travel itineraries and the like may be stored on a portable electronic device such as a smartphone e.g. user device . In some implementations this information is additionally or alternatively stored by one or more remote computers or services such as the digital assistant server information service calendar service social networking service etc. .

Where photographs are to be searched using location information it may be necessary to translate names of locations into geo codes. Specifically if photographs include automatically generated location information it is often in the form of a geo code such as a latitude longitude coordinate pair. But when location information for photo searching is identified using an information source such as a calendar flight itinerary social network post etc. it is often in the form of a location name or an address. This information thus requires some translation before it may be used to search for photographs that are tagged with geographical coordinate pairs. Accordingly in some implementations geo code lookup tables or databases are used to convert location information into geo codes for searching. In some implementations when a location is identified e.g. Maui Hi. a range of geo codes corresponding to Maui Hi. are identified and the digital assistant searches for photographs that fall within the geographical area specified by those geo codes. In some implementations locations identified in a user s natural language search query are converted to a range of possible geo codes and the digital assistant identifies photos whose geo code tags match the range of possible geo codes. In some implementations the digital assistant determines locations associated with the geo code tags of the photographs to be searched e.g. instead of or in addition to determining geo codes of the location identified in the search query and determines whether any of those geo code tags correspond to the location identified in the search query. In some implementations both of these techniques are used.

In some implementations the digital assistant uses metadata tags of one photograph that is associated with user generated information in order to identify time date or geo code parameters to use to search for other photographs. For example if a user posts a picture to a website of a social network and the user or another person in the social network comments on captions or otherwise tags that photograph it is plausible that the caption or comment describes some aspect of the photograph. Thus the comment From our vacation suggests that the photograph with which it is associated was taken during the user s vacation. Accordingly it follows that the time date and or geo code tag of that photograph corresponds with a time date and or location of the user s vacation. These tags are then used as search parameters to locate other photographs with the same or similar metadata tags. Thus even where a comment caption or tag for one photograph does not itself include time date or location information such information can still be determined from the time date or geo code tags of that one photograph.

In general it is preferred that a photo search return all available relevant photographs and no irrelevant photographs. However search algorithms are often either under inclusive i.e. they miss some relevant results or are over inclusive i.e. they include some irrelevant results along with the relevant ones . Thus in some implementations the digital assistant uses combinations of information sources and search parameters to identify a useful set of candidate photographs without including too many irrelevant photos or too few relevant ones. For example a user may travel to many different locations during a vacation such that searching for photos that took place in a specific country or city will return too few results. Accordingly the digital assistant may perform searches based on any combination of time date and geo code information in order to provide a useful and comprehensive search result.

As one example a person may vacation in Maui for a few days take a day trip to one of the other Hawaiian Islands and then return to Maui for the remainder of the vacation. In some implementations in order to respond to a search query for pictures from my Maui trip the digital assistant will determine that the user recently flew to Maui e.g. using a recent travel itinerary and identify a geo code parameter for Maui for the photo search. In some implementations the digital assistant will also determine that the user blocked off 7 days in her calendar as vacation. Thus the digital assistant can also search for photographs that took place in a different location but are within the dates specified by the user s calendar. In some implementations the digital assistant searches for photographs taken at a particular location but excludes photographs that fall outside a time or date range. For example if a user has taken several trips to a certain location only those photographs that were taken during the previously identified vacation time range will be returned. Even where no specific date or time range has been identified photographs can be inferred to relate to a particular event or type of event based on their dates and times by identifying a grouping of photographs and or ignoring photographs with outlying dates. For example if a user has 100 photos all taken in Hawaii during a 1 week span and 2 additional photos from Maui taken 5 weeks away from the first grouping it can be inferred that the 2 additional photos are not associated with the user s vacation.

In some implementations the digital assistant searches for photographs taken during a date range e.g. corresponding to a vacation entry in a calendar but excludes photographs that are taken at a location that is not associated with the user. For example a user may save photographs that were taken by someone else and that have nothing to do with their vacation but happen to have been taken on the same day or in the same location. In some cases these photographs should not be identified as the user s vacation photos. In some implementations if the user requests my vacation photos or photos from my vacation for example the digital assistant will recognize the term my and determine that the user only wishes to see their own photographs. The digital assistant then limits the results to only the user s photos e.g. by excluding photos taken at a location not associated with the user . In some implementations the digital assistant does not attempt to limit photographs to those that are taken by or otherwise associated with a user. For example a user may request photos from Paris as a general image search. In such a case the digital assistant may perform a general search to retrieve photos that are taken in Paris even if they were not taken by the user.

In some implementations searches that combine date time and location information as described above get the information from any combination of sources. For example a query may include multiple types of information such as when a user says get my Hawaii pics from last week. This query provides both location and temporal information. In some implementations a query will include only one type of information but another type of information is inferred from the query. For example a query that says get my Hawaii photos can be cross referenced with a travel itinerary showing when the user was scheduled to travel to Hawaii. The multiple types of information can then be used to provide an ideal set of search results as the above examples describe.

In some implementations the digital assistant engages in a dialogue with a user in order to refine and or disambiguate a search query to acquire additional information that may help limit search results to a more relevant set or to increase a confidence that the digital assistant has correctly understood the query. For example if a user searches for photos from last summer s vacation the digital assistant may respond to the user e.g. via audible and or visual output by asking did you mean all photos or photos taken in a particular area The user can then respond with the additional information e.g. by speaking just Hawaii or all of them . In this example the digital assistant may have initially identified multiple groups of photos that relate to a search query for vacation and summer but were taken at different locations. Thus the digital assistant requests the additional information from the user in order to determine which photographs the user wishes to see. The digital assistant may also or additionally identify that a search query does not contain sufficient information with which to generate a relevant result set. For example the digital assistant may recognize that the search query for photos from last summer s vacation is likely to return too many results or that those photos that are returned do not match a model or profile of vacation pictures e.g. they were taken in too many different locations they span too long a time there are significant gaps between returned photos etc. . Accordingly in some implementations the digital assistant will request additional information from the user in order to generate a search query that will return a more appropriate result set.

The photographs that are searched in response to a search query are any photographs that are associated with the user or are publicly accessible and accessible by the digital assistant. In some implementations the photographs are stored on the device with which a user accesses or interfaces with a digital assistant. For example when the user accesses a digital assistant with a smart phone tablet computer or camera among other possibilities the digital assistant may search photographs stored on that device. Although processing the natural language input consulting with information sources etc. may be performed by that device or other devices such as the digital assistant server one or more external services etc. either individually or in combination. In some implementations the photographs are stored remotely from the device with which the user accesses or interfaces with the digital assistant. For example in some implementations photographs are stored in the cloud i.e. at a remote server connected to the Internet. The remote server may be operated by a storage provider a social network a network accessible home media server or storage unit a photo hosting website etc. In some implementations the digital assistant is authorized to access e.g. search for view edit download etc. remotely stored images using an authorization technique. For example in some implementations the digital assistant is associated with credentials that allow the digital assistant to programmatically access the image sources. Specifically the digital assistant is provided with a user s credentials to a social networking site so that the assistant can use the social network as an information source e.g. to determine dates times or locations indicated by a natural language search query or as a photograph source e.g. to search for images that may be relevant to the natural language search query .

According to some implementations the following methods allow a user to search for photographs on an electronic device such as a smart phone portable music player laptop computer tablet computer etc. When a user wishes to retrieve photographs for viewing the user may simply provide a natural language input and the digital assistant system processes the natural language input to determine what parameters to use in order to search for relevant photographs. To do so as described above the digital assistant system uses one or more information sources to determine specific date time and or geo code search parameters that correspond to the natural language input. Then photographs are searched to identify those that have metadata tags matching the date time and or geo code parameters of the search. Accordingly the photographs need not be first tagged with or otherwise associated with natural language keywords in order for the natural language search feature to work.

Returning to method represents a method for searching for media items using a voice based digital assistant. Method is optionally governed by instructions that are stored in a non transitory computer readable storage medium and that are executed by one or more processors of one or more computer systems of a digital assistant system including but not limited to the server system the user device and or the photo service . Each of the operations shown in typically corresponds to instructions stored in a computer memory or non transitory computer readable storage medium e.g. memory of client device memory associated with the digital assistant system . The computer readable storage medium may include a magnetic or optical disk storage device solid state storage devices such as Flash memory or other non volatile memory device or devices. The computer readable instructions stored on the computer readable storage medium may include one or more of source code assembly language code object code or other instruction format that is interpreted by one or more processors. In various implementations some operations in method may be combined and or the order of some operations may be changed from the order shown in . Moreover in some implementations one or more operations in method is performed by modules of the digital assistant system including for example the natural language processing module the dialogue flow processing module the photo module and or any sub modules thereof.

Multiple media items are provided wherein at least some of the media items are each associated with a respective tag comprising at least one of a time tag a date tag or a geo code tag . In some implementations a geo code tag includes GPS coordinates. In some implementations GPS coordinates comprise a latitude longitude pair. In some implementations the tags are metadata that is stored with the media items.

Respective ones of the multiple media items may be stored in various locations. For example in some implementations at least a subset of the multiple media items is stored on the electronic device. In some implementations at least a subset of the multiple media items is stored remotely from the electronic device. In some implementations the remotely stored media items are associated with a social networking account of a user of the electronic device. For example a user may allow a voice based digital assistant to access the user s social networking account so that the digital assistant can search for and retrieve media items and or other information associated with the user s account. In some implementations the digital assistant has permission to access the social networking account by virtue of a device wide authorization.

In some implementations the multiple media items are photographs . In some implementations the multiple media items are videos .

A natural language text string is provided the text string corresponding to a search query for one or more media items wherein the search query includes one or more query terms . In some implementations the one or more query terms do not include any of a date a time or a geo code.

In some implementations prior to step a speech input is received from a user wherein the natural language text string corresponds to the speech input and speech to text processing is performed on the speech input to generate the natural language text string . For example a user may provide a speech input representing a query to the digital assistant such as show me my vacation pics. The speech input e.g. recorded and or cached audio is converted to text using speech to text processing. The resulting text string is provided to the digital assistant. In some implementations the speech to text processing i.e. step is performed by the same device on which the speech input was recorded. In some implementations the speech to text processing i.e. step is performed by a different device such as a remote speech to text processing server.

Method continues on . At least one information source is searched to identify at least one parameter associated with at least one of the one or more query terms . The at least one parameter comprises at least one of a time parameter a date parameter or a geo code parameter and the at least one parameter is separate from the tags associated with the multiple media items. In some implementations the at least one information source is selected from the group consisting of a calendar email messages text messages social network postings a contact book travel itineraries and an event schedule. . In some implementations a calendar is a publicly accessible calendar that correlates public holidays religious holidays public events and the like. In some implementations a calendar is a private calendar associated with a user of the digital assistant. A user s private calendar may be stored for example on a user device e.g. user device or on an external service e.g. calendar service or digital assistant server . In some implementations a user s calendar is stored on a combination of these or other devices.

In some implementations text messages e.g. step include messages that include a textual component such as SMS simple message service messages instant messages and the like. In some implementations the contents of the text messages may be accessed in order to identify possible context clues that may identify media items. Specifically a text message sent or received by a user very near in time to a time stamp of a photograph may contain information that identifies the contents of the picture. Or the textual content of a text message that included a media item e.g. a photograph as an attachment may provide clues about the contents of the media item. For example a text message sent 2 minutes after the user took a photograph of the beach may say We re at Polo Beach meet us when you can. This text can be used to identify that the photograph is likely to have been taken at a Beach and to facilitate search and retrieval of that photograph using a natural language input.

As noted above at least one parameter associated with at least one of the one or more query terms is identified i.e. step . In some implementations a time parameter comprises a time range surrounding a time specified in the one or more query terms . For example if a query includes the terms yesterday around 2 the time parameter may comprise a time range of 12 00 to 4 00 PM of the previous day.

In some implementations a date parameter comprises a date range surrounding a date specified in the one or more query terms . For example if a query term is Christmas the date parameter may comprise a date range from December 23 to December 27.

In some implementations a calendar associated with a user is searched to identify an event spanning a range of dates including and or near the holiday. For example a calendar may have a vacation event spanning a holiday such as the Fourth of July. Thus the date range of the vacation event can serve as a date parameter corresponding to a search for media items from the Fourth of July. After searching the calendar the date tags associated with the multiple media items are searched e.g. step above to identify at least one media item whose associated date tag falls within the range of dates.

In other examples if a query includes a term indicating a range of a single day the time parameter may comprise a range including 1 2 or 3 or more days on either side of the indicated day. If a query includes a term indicating a range of one or more weeks the time parameter may comprise a range including an additional 1 2 or 3 days on either side of the specified week or 1 2 or 3 or more additional weeks on either side of the specified week s . If a query includes a term indicating a range of one or more hours the time parameter may comprise a range including 1 2 or 3 or more hours on either side. If a query includes a term indicating a range of one or several minutes i.e. less than 1 hour the time parameter may comprise a range including 5 10 15 or 30 or more minutes on either side. If a query includes a term indicating a range of one or several years the time parameter may comprise a range including 2 3 or 6 or more months on either side. Non numerical query terms may also cause the digital assistant to expand a time parameter to include a greater range. For example in some implementations when a query includes a holiday e.g. Labor Day the time parameter may comprise the entire week or weekend on which that holiday falls. The digital assistant may also expand or adjust other parameters such as a geo code parameter based on the user s query. For example if a user specifies a city in a search query the geo code parameter may be expanded to include surrounding cities a county state region surrounding the city etc. If a user specifies a country the geo code parameter may be expanded to include surrounding countries.

In some implementations a geo code parameter comprises a range of geo codes associated with a location specified in the one or more query terms . In some implementations the range of geo codes corresponds to geographical boundaries of the location . For example the range of geo codes may correspond to the geographical boundaries of a city a state a park a town an island etc. In some implementations the range of geo codes corresponds to a principal geo code and one or more distances from the principal geo code in one or more directions . For example the principal geo code may correspond to a representative geographical location for a particular location e.g. the center of a city the entrance of a park etc. and the one or more distances from the principal geo code may correspond to a radius around the principal geo code e.g. a 1 5 10 or 100 mile radius or other distances .

In some implementations the one or more query terms e.g. from the search query i.e. step include an event name and the at least one information source includes information that associates event names with dates . For example in some implementations a calendar associates event names with dates. In some implementations the event name is a religious holiday a national holiday a birthday a season a sporting event or the like .

Method continues on . The respective tags are compared to the at least one parameter . Specifically in some implementations the geo code tags time tags and or date tags are compared with the time geo code parameter time parameter and or the date parameter respectively that were identified by searching the at least one information source. For example a natural language text string may have included the query get my vacation photos which resulted in identifying from the user s calendar or through a disambiguation dialogue with the user described above that the user recently vacationed in Hawaii. The geo code tags of photographs and videos associated with the user may then be compared with a range of geo code parameters associated with the state of Hawaii to determine which photographs are likely to fall in the category of user s vacation photos. In some implementations locations of photographs are determined using image recognition techniques. For example the location of an image that lacks a geo code tag may be identified by comparing that photo to other photos with known locations. The determined location can then be used in place of a geo code tag to facilitate searching of that photograph. Image recognition techniques e.g. using image fingerprints to automatically tag images with location information for example are described in Voice Based Image Tagging and Searching U.S. application Ser. No. 13 801 534 filed Mar. 13 2013 which is incorporated by reference in its entirety.

Method continues on . The respective tags are compared to the at least one parameter . Specifically in some implementations the geo code tags time tags and or date tags are compared with the time geo code parameter time parameter and or the date parameter respectively that were identified by searching the at least one information source. For example a natural language text string may have included the query get my vacation photos which resulted in identifying from the user s calendar or through a disambiguation dialogue with the user described above that the user recently vacationed in Hawaii. The geo code tags of photographs and videos associated with the user may then be compared with a range of geo code parameters associated with the state of Hawaii to determine which photographs are likely to fall in the category of user s vacation photos. In some implementations locations of photographs are determined using image recognition techniques. For example the location of an image that lacks a geo code tag may be identified by comparing that photo to other photos with known locations. The determined location can then be used in place of a geo code tag to facilitate searching of that photograph. Image recognition techniques e.g. using image fingerprints to automatically tag images with location information for example are described in Voice Based Image Tagging and Searching U.S. Application Ser. No. 13 801 534 filed Mar. 13 2013 which is incorporated by reference in its entirety.

Presentation of the at least one media item to the user is facilitated . In some implementations such as where the method is performed at a handheld electronic device or a client computer system facilitating presentation of the at least one media item includes displaying the at least one media item on a display device . In some implementations such as where the method is performed at a server system facilitating presentation of the at least one media item comprises sending the at least one media item and or an address or identifier of the at least one media item to a client device for display .

In some implementations a confidence value is assigned to each identified media item wherein the confidence value corresponds to the strength of the match between the time tag the date tag or the geo code tag of the media item and the time the date or the geo code associated with the at least one of the one or more query terms . In some implementations the strength of a match is determined by the nearness of the tag of the media item to the search parameter. For example if a user searches for photos from Christmas a media item with a date tag of 12 25 11 may have a higher confidence value than one taken on December 20 of that year. Where a plurality of media items are to be presented the plurality of media items are ranked based on the confidence values and the plurality of media items are displayed in an order based on the ranking . Thus the media items that are more likely to match or more closely match the user s query are displayed to the user first.

It should be understood that the particular order in which the operations in have been described are merely exemplary and are not intended to indicate that the described order is the only order in which the operations could be performed.

The foregoing description for purpose of explanation has been described with reference to specific implementations. However the illustrative discussions above are not intended to be exhaustive or to limit the disclosed implementations to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The implementations were chosen and described in order to best explain the principles and practical applications of the disclosed ideas to thereby enable others skilled in the art to best utilize them with various modifications as are suited to the particular use contemplated.

It will be understood that although the terms first second etc. may be used herein to describe various elements these elements should not be limited by these terms. These terms are only used to distinguish one element from another. For example a first photograph could be termed a second photograph and similarly a second photograph could be termed a first photograph without changing the meaning of the description so long as all occurrences of the first photograph are renamed consistently and all occurrences of the second photograph are renamed consistently. The first photograph and the second photograph are both photographs but they are not the same photograph.

The terminology used herein is for the purpose of describing particular implementations only and is not intended to be limiting of the claims. As used in the description of the implementations and the appended claims the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will also be understood that the term and or as used herein refers to and encompasses any and all possible combinations of one or more of the associated listed items. It will be further understood that the terms comprises and or comprising when used in this specification specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

As used herein the term if may be construed to mean when or upon or in response to determining or in accordance with a determination or in response to detecting that a stated condition precedent is true depending on the context. Similarly the phrase if it is determined that a stated condition precedent is true or if a stated condition precedent is true or when a stated condition precedent is true may be construed to mean upon determining or in response to determining or in accordance with a determination or upon detecting or in response to detecting that the stated condition precedent is true depending on the context.

