---

title: Load test charts with standard deviation and percentile statistics
abstract: A method for real-time analysis of results from a load test performed on a target website includes calculating first-level aggregated test results within each of a plurality of load server instances that generate a load on the target website. The first-level aggregated test results are calculated from data points received by each of the load server instances from the target website. The first-level aggregated test results include a sum of the data points, a count of the number of the data points, a sum of squares of the data points, and an average of the data points. A standard deviation result (STDEV) is calculated and chart is generated on a display via a graphical user interface. The chart provides a visual representation of a performance metric for the load test based on the standard deviation result.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09251035&OS=09251035&RS=09251035
owner: SOASTA, Inc.
number: 09251035
owner_city: Mountain View
owner_country: US
publication_date: 20120209
---
This application is a continuation in part CIP application of Ser. No. 12 804 338 filed Jul. 19 2010 entitled R T M T LTRD which is assigned to the assignee of the present CIP application.

The present disclosure relates generally to cloud computing more particularly to automated systems and methods for functional and or load testing of websites or features of message based intranet Internet or browser based applications.

Information technology is now routinely used by many enterprises to receive process and provide information via widely accessible electronic communications networks such as the Internet. Yet most information technology systems will begin to deny service or fail to process message traffic efficiently when communications traffic exceeds a processing capacity of the system. Such failures in communication can significantly impair the operations of an enterprise in many ways. Slower website performance is also known to cause users visitors to leave the website sooner. Another consequence of poor performance is that the website may be downgraded in search engine results rankings.

In recent years enterprises and developers have sought an easy and affordable way to use cloud computing as a way to load and performance test their web based applications. Cloud computing gets its name from the fact that the machine storage and application resources exist on a cloud of servers. In cloud computing shared resources software and information are provided on demand like a public utility via the Internet. Cloud computing is closely related to grid computing which refers to the concept of interconnecting networked computers such that processing power memory and data storage are all community resources that authorized users can utilize for specific tasks.

Load testing a web based application or website can involve simulating a very large number e.g. up to or beyond 1 000 000 of virtual website users via Hypertext Transfer Protocol HTTP or HTTP Secure HTTPS message intercommunications with the target website. For very large tests sending and aggregating the test results data generated from all of the load servers to a database available to a dashboard in real time has been problematic. The huge overhead of receiving and processing a very large number of HTTP messages containing all of the requests and responses sent from each of the many load servers to the analytic servers responsible for analyzing the test results data can easily overwhelm the resources of the server. In addition communications bottlenecks can occur wherein messages get queued such that the test result s are no longer sent to the database in real time and therefore are not available in the dashboards in real time.

In extreme cases the load test servers can also back up causing them to not generate the appropriate load on the customers websites or web applications. Adding to the problem is the fact that some calculations such as standard deviation cannot easily be calculated at each server and then added up or totaled with the same calculations performed on other servers to form a unified result for the entire set of servers generating the load test.

In the following description specific details are set forth such as server types cloud providers structural features process steps etc. in order to provide a thorough understanding of the subject matter disclosed herein. However persons having ordinary skill in the relevant arts will appreciate that these specific details may not be needed to practice the present invention. It should also be understood that the elements in the FIG.s are representational and are not drawn to scale in the interest of clarity.

References throughout this description to one embodiment an embodiment one example or an example means that a particular feature structure or characteristic described in connection with the embodiment or example is included in at least one embodiment. The phrases in one embodiment in an embodiment one example or an example in various places throughout this description are not necessarily all referring to the same embodiment or example. Furthermore the particular features structures or characteristics may be combined in any suitable combinations and or sub combinations in one or more embodiments or examples.

In the context of the present application the term cloud broadly refers to a collection of machine instances storage and or network devices that work together in concert. A public cloud refers to a cloud that is publically available i.e. provided by a cloud provider that a user may access via the Internet in order to allocate cloud resources for the purpose of utilizing or deploying software programs and also for running or executing those programs thereon. Some public clouds deliver cloud infrastructure services or Infrastructure as a Service IaaS . By way of example Amazon Elastic Compute Cloud also known as EC2 is a web service that allows users to rent computers on which to run their own computer applications thereby allowing scalable deployment of applications through which a user can create a virtual machine commonly known as an instance containing any software desired. The term elastic refers to the fact that user can create launch and terminate server instances as needed paying by the hour for active servers.

Cloud platform services or Platform as a Service PaaS deliver a computing platform and or solution stack as a service. An example PaaS cloud provider is the Google App Engine which lets anyone build applications on Google s scalable infrastructure. Another leading software platform in the cloud provider is Microsoft Azure an application platform in the cloud that allows applications to be hosted and run at Microsoft datacenters.

A private cloud is a cloud that is not generally available to the public and which is typically located behind a firewall of a business. Thus a private cloud is only available as a platform for users of that business who are behind the firewall.

The term server broadly refers to any combination of hardware or software embodied in a computer i.e. a machine instance designed to provide services to client devices or processes. A server therefore can refer to a computer that runs a server operating system from computer executable code stored in a memory and which is provided to the user as virtualized or non virtualized server it can also refer to any software or dedicated hardware capable of providing computing services.

A message generally refers to a unit of data that can be sent via an electronics communications network e.g. the Internet to another computational or communications system or device e.g. to a server. By way of example a message could represent a communication sent to a queuing system a REST call or a HTTP request. A message could also be instantiated entirely or partially as a single operation such as a web service call in any one of a variety of forms e.g. XML JMS HTML JSON etc. A message clip or clip for short comprises a set of one or more messages that includes a specification of the timing and or dependencies within that set of messages. A clip typically comprises a plurality e.g. hundreds of thousands of sequenced messages that form part of a larger load test composition.

In the context of the present disclosure load servers also referred to as Maestro or test servers are servers deployed and utilized primarily to generate a test load on a target website. That is load servers play the test composition generating a load on a target customer website and web applications. Load servers also function to report back results of the load test and statistics in real time. Analytic or result servers are deployed and utilized primarily to collect the real time test results from the load servers aggregate those results stream the results to real time dashboards and store them in a database.

The term real time refers to a level of computer responsiveness that a user senses as sufficiently immediate or that enables the computer to keep up with some external process for example to present visualizations of load test results as it constantly changes . Thus real time is a mode of computer operation in which the computer collects data analyzes or computes with the data reports e.g. visually displays and or stores the results nearly instantaneously i.e. within seconds or milliseconds.

Run time refers to the time during which a program is executing or in the case of a load test composition when the composition is being played. In other words when a user starts a program that runs on a processor based system or computer it is run time for that program.

A grid or test grid refers to a collection of interconnected load servers and result servers that may be used to run a load test on a target website or web applications. As disclosed herein a computer program or grid wizard may be utilized to automatically determine the global cross cloud resources needed to execute a test by examining the test plan or script also referred to as a test composition . Furthermore the computer program can automatically allocate those server resources required for the test across multiple different cloud providers verifies that the allocated servers are operational and that the allocated servers are running proprietary load testing software or computer program product correctly. The computer program or product also monitors the allocated servers replacing non operational servers when allocated and during execution of the test and displays results from multiple globally distributed clouds in a real time streaming dashboard which requires no user initiated refresh.

In one embodiment a method and system is provided for calculating load test aggregated test results at three architectural levels first at the load server level second at the analytics server level and lastly at the system wide data store level. In a specific implementation detailed level raw data the content of a request sent to a website e.g. to access a homepage is not sent from any of the load servers to any analytic server. Thus system resources on the load server side are not wasted for the continual sending of raw data. Similarly system resources on the analytics server side are conserved since the need to receive and process raw data sent from the load servers is obviated.

Instead of sending the raw data web pages responses and their statistics obtained during a load test from each of the load servers to the analytic servers a level of aggregation is added within each of the load servers. That is in one embodiment each load server includes an embedded component or client referred to as a Results Service Client that performs analytics server functions at the load server level. This Results Service Client aggregates test result data and generates various results statistics or metrics e.g. average response time average response size etc. from the raw data that the load server received from the target website or application. The statistics computed by the Results Service Client in each of the load servers are then sent to their associated analytic server at periodic intervals e.g. once every five seconds .

In another embodiment the Results Service Client of each load server generates aggregates that include the sum of data elements obtained during a load test the sum of squares of the data elements a count of the number of data elements and an average of the data elements. These aggregates are calculated at each of the load servers in real time and then passed down the Result Server hierarchy in a communicative manner. A specialized mathematical formula or algorithm is utilized by an automated computer program to calculate standard deviation for the load test results across a massively distributed dataset in real time. A graphical user interface of an analytical dashboard allows a user to generate a graphical display in the form of a widget or chart of a selected percentile confidence interval using the standard deviation.

Target website is shown connected to a public cloud via Internet cloud . Public cloud includes a main instance coupled to a database . Database may be used to store test results store metadata indicative of the test definition and to store monitoring data e.g. CPU metrics generated during the load test. Main instance is also shown coupled to a pair of analytic servers and a pair of load servers within cloud consistent with a snapshot view of the start of a process of deploying a test grid. It is appreciated that cloud may comprise multiple clouds associated with multiple different cloud providers. In the example shown main instance is a virtual machine deployed on a server provided in cloud that communicates with a browser application. In one embodiment main instance may include a results service designated as a reader results service as opposed to all of the other remote writer results services which reads data from database and serves it to a web application which in turn formats the data and serves it to an analytic dashboard in the browser. In operation main instance executes the coded sequence of computer executed steps e.g. from code stored in a memory that allocates the server resources required for the test across one or multiple different cloud providers. The same application that allocates verifies server resources may also verify that the allocated servers are operational to conduct the website load test. The main instance may also execute code that implements the multi tiered load test results aggregation steps disclosed herein.

Connected to the front end of cloud through Internet cloud is a laptop computer associated with a user who may orchestrate deployment of the test of target website . It is appreciated that other implementations computer may comprise a desktop computer workstation or other computing device that provides a user interface that allows a user to create and execute the test composition define the parameters of the grid initiate the load test as well as analyze review results of the test in real time. The user interface may be web based so it can be accessed from any computer having web browser capabilities from any location in the world without installation of specialized software.

Persons of skill in the art will understand that the software which implements main instance may also be downloaded to the user s laptop computer or implemented on a separate hardware appliance unit located either at the user s premises e.g. behind the firewall or anywhere in clouds or . It is further appreciated that laptop is representative of a wide variety of computer devices such as workstations personal computers distributed computer systems etc. that may be utilized by the user to launch the method for provisioning running the cross CloudTest grid analyzing streaming real time results as well as monitoring the performance of the actual load test.

Continuing with the example of the application program running on main instance operates to create a graphical user interface GUI that allows a user of laptop to remotely interact with the application view monitor the test results in real time and modify parameters test conditions dynamically during the actual test. For purposes of the present disclosure the grid wizard is considered synonymous with the application program or system program that performs the method and operations described herein. In one embodiment main instance may include an embedded load server for running a relatively small load test that does not require the deployment of other load servers and an embedded results i.e. analytic server for collecting aggregating the real time test results. In another embodiment the main instance and the database provide a basic CloudTest environment that can be used to launch establish one or more grids with more or more cloud providers being utilized to provision each grid.

The overall testing process begins with the user creating a sophisticated test plan or composition via a GUI of either the same application program running on main instance or a GUI associated with another web browser application. The GUI may be utilized that generate complex parallel message streams for website testing. In one example the test plan may be created in the form of a visual message composition analogous to a music composition for testing and demonstrating web services such as that described in U.S. patent application Ser. No. 11 503 580 filed Aug. 14 2006 which application is herein incorporated by reference.

The process of deploying the test grid for a large scale test may start with the user of laptop indicating to main instance the number of virtual users wanted on each track of the test composition. For example the user of the system may wish test the target website with a load equal to 1000 users on each track of a test composition. The user may indicate the number of virtual users through an input entered on a browser page of the GUI as described below or alternatively invoke a grid wizard that automatically makes an intelligent allocation of the proper amount of resources needed to conduct the test based on examining the composition that this grid will be running. By way of example the system may determine that a single load server should be allocated to accommodate every 1000 virtual users.

Similarly the system via a grid wizard may determine a proper allocation of result servers needed to accommodate the number of load servers specified. In one embodiment users can specify how many load servers and how many result servers they want in each cloud and region. Alternatively users may employ the grid wizard to specify all parameters. That is users can simply specify a defined test composition and the grid wizard automatically analyzes the composition and determines how many servers they need in each cloud and region. It is appreciated that the determination of the number of load servers and result servers is typically made based on considerations that ensure each virtual user has a satisfactory amount of bandwidth CPU memory resources etc. such that it correctly simulates or behaves as a real world browser.

Once the test has been defined and the parameters set e.g. number of servers server locations etc. via the grid wizard upon user input the user main instance starts the process of actually deploying and allocating the specified resources by interacting with an application programming interface API of one or more cloud providers. By way of example a user may click on a Deploy Instances button provided in a page of the CloudTest program GUI in response the system software contacts all of the different cloud APIs it needs and starts to allocate the required servers.

For example if 1000 servers are to be allocated in EC2 there may be 40 simultaneous requests issued each request being for 25 servers. If another 200 servers need to be allocated in Microsoft Azure in two different geographically located data centers two simultaneous requests may be issued each for 100 servers in each data center due to the fact that Azure does not support allocating smaller groups into one single deployment . In other words the user may simply click on an icon button of a GUI to initiate the deployment allocation of resources e.g. machine instances needed to execute the test composition with the requests necessary to achieve that allocation being issued handled in an automated manner i.e. without user intervention.

Each of result servers is connected to a plurality of associated load Maestro servers . Each load server is shown having an embedded component or Result Service client which computes metrics or statistics from the raw data e.g. web pages received from the target website or application. As discussed previously the function of each load server is to provide a load to the target website by creating one or more virtual users that access information on the target website. Within each Maestro server is Result Service client which functions to compute statistics such as average response time average response size and the like. In one embodiment instead of sending all of the raw data received from the target website Result Service client computes relevant statistics and discards the data. Then once an interval e.g. every five seconds the statistics computed by client are sent to the associated result server .

Each of the result servers takes all of the statistics received from all of its associated load servers and further aggregates those statistics. In other words each result server aggregates the aggregated results received from all of the load servers that it is connected to. The resulting aggregated data is then further aggregated in database . Thus statistics such as average response time across all of load servers for the load test is stored in database and available on a real time basis to browser via database queries performed by the main instance which can perform further aggregation grouping filtering etc.

Practitioners in the art will appreciate that the disclosed multi tiered architecture does not overburden analytic servers with excessive messaging of raw data. Furthermore persons of skill will understand that aggregating statistical results data on multiple levels beginning at the point closest to the actual load test results creation allows a user to view results in real time on an analytic dashboard graphical user interface thereby permitting real time analysis across the entire testing infrastructure.

In a specific embodiment each load server includes an accumulator that stores the statistically aggregated data e.g. average response time computed on a second by second basis. Periodically e.g. every 5 seconds each load server sends an appropriate number of messages e.g. 5 messages one for each second to its associated result server . That is one batched message is sent every 5 seconds the batched message including data about all of the previous 5 seconds. Each message contains the data metrics computed every one second interval. These fine granularity metrics are then further aggregated in database . It is appreciated that by computing statistics metrics on a second by second basis the analytic dashboard running on browser can analyze the results on various levels of granularity. In other words the user may want to view statistical results of the load test on a minute by minute basis or all the way down to a second by second basis. Thus the architecture described herein allows a user to view real time streaming results in an analytic dashboard of various performance metrics on a second by second basis even when there are millions of virtual users on thousands of load servers.

As can be seen a set of combined charts are shown graphically in various window fields. For example field illustrates the number of virtual users shaded area and the send rate heavy line as a function of test time. Field illustrates error count vertical dark lines and the number of virtual users shaded area versus test time. Field shows the number of bytes sent and received vertical dark lines and the number of virtual users shaded area as a function of test time. It is appreciated that the user may select view a wide variety of charts combined correlated etc. using tabs . Collectively the charts provided in window allow a user to view analyze and monitor test results and information in real time so as to help identify root causes of performance problems their website or web application may be experiencing.

Persons of skill in the arts will appreciate that shows how the entire test grid comprising a huge number of interconnected load and result servers works in concert to send load receive responses aggregate and analyze those responses into a real time streaming graphical result displayed to the user. All this is accomplished regardless of how many server instances and different cloud providers are utilized to run the load test. Moreover the various result charts may be viewed in one or many real time streaming analytic dashboards. In each of the charts displayed on analytic dashboard window the user may change the time format or legend of the horizontal axis for reporting the testing analytics in real time on a varying time e.g. hour by hour minute by minute or second by second basis.

During the playback of the test composition and while the user is monitoring viewing the test results displayed on GUI window the user may pause or stop the test. Stopping the test closes the result and unloads the running test composition from all of the load servers. On the other hand pausing or temporarily halting the test stops the load from all of the load servers but keeps the test composition loaded and ready to resume playing into the same result. For instance the user may pause the test after identifying a problem that requires adjustment of the load balancer on the target website.

It should be understood that when the test is temporarily halted in this manner the grid remains fully provisioned and running. In other words the composition and running of the load test is independent from the provisioning and running of the grid. After any adjustments or reconfiguration of the target website the user may continue with the execution or playback of the test composition either beginning at the place where it was halted or re starting the test from the beginning. Persons of skill in the art will appreciate that the ability to start re start the test without affecting the state of the grid in conjunction with the ability to view test results metrics in real time e.g. second by second provides a powerful advantage over prior art methods for testing a customer website especially if the website is the production website. Owners of the website often want to know how the load applied from the load servers is affecting the experience of real users on the site and appreciate the ability to stop the load test pause the load test or pause the ramp up process wherein more virtual users are added during the test to make sure that real user behavior is not adversely affected by the load test.

The aggregated test results computed by the client running on each load server are periodically sent to their associated analytic server block . The period at which the aggregated results are sent to the analytic servers may be equal to or greater than the period at which the aggregated test results are computed within each load server. In a typical implementation aggregated test result data is computed by each load server every second with the results of those computations being sent to the analytic servers from each of the load servers every five seconds.

Next at each analytic server the aggregated test result data received from each of the associated load servers is further aggregated block . In other words each analytic server produces aggregated test result data across all of its associated load servers. For example if each analytic server is associated i.e. connected with 50 load servers each analytic server aggregates statistics metrics across the aggregated test result data received from each of the 50 load servers.

Finally at block the aggregated statistical data produced by each analytic server is further aggregated at the system wide data store in real time. For instance Structured Query Language SQL queries to the database can perform statistical functions e.g. AVG SUM etc. against tables rows which have been inserted from the individual analytics servers thereby producing further third level aggregated results. As explained above the results of this final level of aggregation is available in real time to a browser executing an analytic dashboard that provides a graphical display of the results in various charts.

In addition to aggregating test result statistics such as average response time in real time companies engaged in e commerce are also interested in metrics such as percentile calculations. For example businesses are very interested to know what a significant percentage e.g. 90 of their users experience in terms of average response time when they utilize a website or web application. This typically means discarding or throwing out anomalies or outlier test results and requires more sophisticated calculations be performed in real time such as standard deviation of a massively distributed dataset. In other words a metric of particular interest may be a one tailed distribution that tosses out 5 10 of the highest outliers worst performance . Such a calculation is referred to as a percentile calculation or confidence interval which requires the standard deviation for the dataset.

A method apparatus and computer program product is provided to calculate standard deviation formula in real time for a massively distributed dataset produced during a load test. The standard deviation calculation is made possible through the calculation of a number of aggregates or metrics in the load servers. In other words the disclosed subject matter makes it possible to aggregate across all of the servers and calculate standard deviation across a massively distributed dataset in real time. Once that is done it is also possible to calculate any confidence interval percentiles based on the calculated standard deviation for the entire dataset.

In one embodiment the problem of calculating standard deviation and confidence interval percentiles that typically require aggregations across a massively distributed dataset in real time is solved by aggregating data test results at each load server for all of the virtual users that it hosts then at the analytic servers for all of the load servers each supports and finally in the system wide data store for all of the results or analytic servers.

In one implementation an automated software tool provides a graphical user interface that allows a user to automatically calculate metrics such as percentile and standard deviation in real time from aggregated load test results. Utilizing the software program a user for example may calculate a one tailed confidence interval meaning that the outliers are removed to obtain the 90 or 95 or 98 etc. percentile for average response time of a website.

As discussed previously in one embodiment an embedded component or client i.e. a Results Service Client performs analytics server functions at the load server level. The Results Service Client aggregates test result data and generates various results statistics or metrics e.g. average response time average response size etc. from the raw data that the load server received from the target website or application. In other words the Results Service Client runs inside each load server to calculate the metric AVERAGE which is then passed down the result server hierarchy see to obtain a cumulative average i.e. an average of all averages across the entire dataset.

The inventors have discovered that the general standard deviation equation may be expanded and the terms rearranged resulting in the mathematically equivalent standard deviation equation or formula . Formula includes a sum of the squares of the test result data points SUM OF SQUARES term and a simple sum of the data points SUM term as well as AVERAGE and COUNT aggregates. In one embodiment the Results Service Client in each load server calculates the SUM SUM OF SQUARES COUNT and AVERAGE aggregates in real time. These first level aggregated test results are then aggregated down the Results Server hierarchy in the same manner as described in connection with . Utilizing formula STDEV can be calculated from the entire dataset when first launching an analytic dashboard that provides a graphical display of the results in various charts as well as after load testing has been completed. That is as a load test composition is running the load servers collect test result data points received from the website or web application. The Results Service Client running in each load server calculates the above first level aggregated test results in real time with the calculations being sent to the associated Result Servers. The Results Servers then aggregate the aggregates received from the load servers and forward the results down to the main instance and database.

Persons of skill in the art will appreciate that formula also works for calculating STDEV based on incremental or delta changes when the load test is running live in real time. To calculate standard deviation basement of the changes the delta SUM OF SQUARES aggregate is added to the SUM OF SQUARES and the delta SUM is added to the SUM. The new COUNT and new cumulative AVERAGE metrics are also calculated at each of the load servers and passed down their associated Result Servers. To produce the final result of the total aggregated standard deviation all of the aggregated terms or results that were calculated from all of the results servers are added up as shown in the nominator of formula . The number of observations or count is the denominator. A square root of this fraction is the standard deviation result across the entire dataset.

Once the standard deviation has been calculated a confidence interval X may be determined as shown by the set of equivalent equations shown in . By way of example the confidence interval X may be calculated to determine what will be the average response time of 90 95 or 98 of the users of a particular website or web application. As shown in the confidence interval X is calculated as the AVERAGE Z STDEV where Z is a value obtained from a known statistics table used for calculating a certain percentile e.g. 90 95 or 98 .

The Z value is well known in statistics as the result of a translation from a normally distributed variable. The Z value is thus a constant that may determined from a look up to a standard statistical table such as that provided at . Through the translation into a standard normal variable table can be used to calculate percentiles i.e. using a so called Z table . For instance the respective Z values for two tailed confidence intervals for 90 95and 98percentiles are 1.65 1.96 and 2.33. The Z values for one tailed confidence intervals for 90 95and 98percentiles are 1.29 1.65 and 2.06 respectively. So by way of example if the AVERAGE webpage response time is say 5 seconds and the STDEV is 2 seconds the 90percentile confidence interval X one tailed is 7.58 AVERAGE Z STDEV .

To summarize the SUM SUM OF SQUARES COUNT and AVERAGE aggregates are calculated at each of the load servers in real time which aggregates are passed down the Result Server hierarchy to be further aggregated for the entire dataset produced during the load test in a communicative manner. The CloudTest application software program uses these aggregates and formulas and to generate a graphical display in the form of a widget or chart of the selected percentile confidence interval. The application program is configured to generate a graphical user interface that allows a user to select a certain percentile e.g. 90 of average response time of the load test results in real time. In addition as the percentile widget is running in real time the user allows the user to change or modify the percentile e.g. change from a one tailed to a two tailed distribution or from 90 to 95 with the new results being produced on an analytic dashboard or chart in real time.

As shown for percentile charts the user can select from among the default percentile values e.g. 90 95 or 98 using up down selection arrows . For each of the percentile selections the corresponding Z Value is shown in field . In one embodiment a new Z value can be entered manually in field for any of the drop down percentile values. Selection of a new percentile results in a reset wherein the default z values for each of the percentiles appear in field . In the example window of the user may select between the bottom or top N collections by duration percentile.

In one embodiment standard deviation charges may include similar chart settings as described in connection with the collection based chart of with the option to include Stopped Collections or Failed Collections which are not included by default.

It should be understood that elements of the disclosed subject matter may also be provided as a computer program product which may include a machine readable medium having stored thereon instructions which may be used to program a computer e.g. a processor or other electronic device to perform a sequence of operations. Alternatively the operations may be performed by a combination of hardware and software. The machine readable medium may include but is not limited to floppy diskettes optical disks CD ROMs and magneto optical disks ROMs RAMs EPROMs EEPROMs magnet or optical cards or other type of machine readable medium suitable for storing electronic instructions.

Additionally although the present invention has been described in conjunction with specific embodiments numerous modifications and alterations are well within the scope of the present invention. Accordingly the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.

