---

title: Policy-based checkpointing fault tolerance across remote virtual machines
abstract: Embodiments include a checkpointing fault tolerance network architecture enables a first computer system to be remotely located from a second computer system. An intermediary computer system is situated between the first computer system and the second computer system to manage the transmission of checkpoint information from the first computer system to the second computer system in an efficient manner. The intermediary computer system responds to requests from the second computer system for updated data corresponding to memory pages selected by the second computer system, or memory pages identified through application of policy information defined by the second computer system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08898509&OS=08898509&RS=08898509
owner: VMware, Inc.
number: 08898509
owner_city: Palo Alto
owner_country: US
publication_date: 20121212
---
This application is a continuation in part application of U.S. patent application Ser. No. 13 450 249 filed Apr. 18 2012 which is a continuation in part application of U.S. Pat. No. 8 171 338 issued May 1 2012 both of which are entitled Method and System for Enabling Checkpointing Fault Tolerance Across Remote Virtual Machines. The entirety of both applications is hereby incorporated by reference herein.

As large scale enterprises continue to adopt virtualization platforms as the foundation of their data centers virtual machine VM fault tolerance has become an increasingly important feature to be provided by virtualization platform providers. Because a single host server in a virtualized data center can support multiple VMs failure of that host server can bring down a multitude of services that were provided by the different VMs running on the failed host server. As such virtualization platforms need to provide a mechanism to quickly resurrect a failed VM on a different host server so that the enterprise can maintain the quality of its service.

Currently providing fault tolerance for a primary VM is typically achieved by providing a backup VM that runs on a server residing in a different fault domain from the server of the primary virtual machine. A fault domain can generally be described as a set of host servers in a data center or data centers that share a number of specified attributes and or characteristics that results in a higher probability of failure of host servers in the fault domain upon a failure of one of the host servers in the fault domain. The attributes and or characteristics utilized by an enterprise to define its data center fault domains depend upon the type of disasters and the level of recovery that the enterprises desire to achieve. For example an enterprise may choose to define its fault domains based upon the physical proximity of host servers storage rack location geographic locations etc. the dependency of such servers on shared hardware networked storage power sources physical connections etc. or software technologies shared file systems etc. and the like. A well constructed fault domain minimizes the correlation of a failure of a VM in one fault domain with the failure of another VM in a different fault domain.

VM fault tolerance may be provided using deterministic replay checkpointing or a hybrid of the two which is disclosed in U.S. patent application Ser. No. 12 259 762 filed on Aug. 28 2008 the entire contents of which are incorporated by reference herein. With replay techniques essential portions of a primary VM s instruction stream e.g. non deterministic events within the primary VM s instruction stream are captured in real time e.g. by a hypervisor layer or virtual machine monitor component of the primary VM and transmitted to a backup VM e.g. presumably located in a different fault domain to replay the primary VM s execution in a synchronized fashion. If the primary VM fails the backup VM can then take over without discernible loss of time. While replay techniques provide a robust fault tolerance solution with fast recovery times they are less viable for example when non deterministic events become more frequent or more difficult to identify within instruction streams as is the case with virtual machines that support SMP symmetric multiprocessing architectures with multiple virtual CPUs.

In contrast to replay techniques checkpointing based fault tolerance techniques are more flexible in their capabilities to support a variety of virtual architectures including SMP based virtual machines. Techniques for generating and using checkpoints in a virtual computer system are disclosed in U.S. Pat. No. 7 529 897 the entire contents of which are incorporated by reference herein. With checkpointing the primary VM is periodically stunned i.e. execution is temporarily halted during the course of execution each such stun period referred to as a checkpoint to determine any modifications made to the state of the primary VM since a prior checkpoint. Once such modifications are determined they are transmitted to the backup VM which is then able to merge the modifications into its current state thereby reflecting an accurate state of the primary VM at the time of the checkpoint. Only upon notification of a failure of the primary VM does the backup VM begin running by loading the stored state of the primary VM into its own execution state. However due to the potentially large size of checkpoint information e.g. multiple gigabytes in a transmitted state and the need to stun the primary VM at periodic checkpoints to transmit such state to the backup VM the backup VM must be networked to the primary VM with sufficiently high bandwidth such that the stun period is not prolonged by network bandwidth limitations. This constraint currently restricts the ability to locate backup VMs in locations that are geographically distant from the primary VM or otherwise in a manner in which backup VMs are connected to primary VMs using network connections having insufficient bandwidth capacity to effectively transmit checkpoint information.

One or more embodiments of the present invention enable a backup VM that receives checkpointing information to be remotely located from a primary VM. Such embodiments situate an intermediary computer system between the primary VM and the backup VM to manage the transmission of checkpoint information to the backup VM in an efficient manner. In some embodiments the intermediary computer system is networked to the primary VM through a high bandwidth connection but is networked to the backup VM through a lower bandwidth connection. During each checkpoint the intermediary computer system receives and stores updated data from the primary VM that corresponds to memory pages in the state of the primary VM that have been modified since a previous checkpoint.

One or more embodiments described herein transmit an updated state of a first computer system to a second computer system through an intermediary computer system. The intermediary computer system receives checkpoint information packets from the first computer system. Each checkpoint information packet includes updated data corresponding to one or more memory pages of the first computer system that have been modified since a previously received checkpoint information packet. The intermediary computer system provides the second computer system with data describing the memory pages and receives from the second computer system a request for a copy of at least one of the memory pages. The intermediary computer system transmits the requested copy of the memory pages to the second computing device.

Alternatively or in addition the second computer system defines policy information describing one or more of the memory pages to select. The intermediary computing system applies the defined policy information to select the memory pages and transmits to the second computing system the selected memory pages.

This summary introduces a selection of concepts that are described in more detail below. This summary is not intended to identify essential features nor to limit in any way the scope of the claimed subject matter.

Embodiments described herein provide updated state of a first computer system to a second computer system via an intermediary computer system to provide fault tolerance for first computer system . In some embodiments the updated state corresponds to checkpoint information packets including data describing memory pages of a plurality of virtual machines VMs executing on first computer system .

In some aspects intermediary computer system pushes the memory pages to second computer system . In other aspects second computer system pulls the memory pages from intermediary computer system by selectively requesting particular memory pages. For example as described below with reference to and intermediary computer system responds to requests from second computer system for updated data relating to particular memory pages. Second computer system may also develop and send policy information to intermediary computer system that describes the memory pages of interest to the second computing device. The pull embodiments enable for example scalability to a plurality of second computer systems as well as better efficiency.

Still other aspects contemplate both push and pull environments. For example in a push and pull hybrid environment intermediary computer system pushes some memory pages to second computer system as described with reference to and while second computer system also pulls some memory pages from intermediary computer system as described with reference to and .

In some embodiments primary server and backup server are referred to as first computer system or first computing system and second computer system or second computing system respectively.

In step the hypervisor of primary server instantiates primary VM . In step the hypervisor of primary server takes an initial snapshot of the state of primary VM and transmits the snapshot to intermediary . The initial snapshot comprises a plurality of memory pages that make up the state of memory and in certain embodiments the state of emulated devices of primary VM . For example in one embodiment each memory page has a size of 4 KB such that a primary VM configured with a virtual RAM of 4 GB would have an initial snapshot size of approximately 1 million memory pages. In an embodiment utilizing VMware s ESX virtualization platform VMware s VMotion technology can be used to create and transmit such an initial snapshot. In step intermediary receives and stores the initial snapshot. In step the hypervisor of primary VM initiates a timer to periodically trigger the initiation of checkpoints to generate checkpoint information packets e.g. 50 to 100 times a second etc. . Prior to the expiration of the timer in step the hypervisor delays and queues any and all outbound network packets generated by primary VM .

Once the timer expires in step the hypervisor initiates a checkpoint by stunning primary VM i.e. freezes its execution state in step and generates a checkpoint information packet reflecting the current state of stunned primary VM and transmits the checkpoint information packet to intermediary in step . In one embodiment the checkpoint information packet comprises a subset of the memory pages or portions thereof of the initial snapshot that have been updated during execution of primary VM since a previously transmitted checkpoint information packet or since the initial snapshot for a first checkpoint information packet . It should be recognized that a variety of techniques may be utilized to identify updated memory pages in primary VM including for example leveraging hardware that may be available on primary server for detecting such modified pages through hardware controlled dirty bits of page tables and page directories used for memory virtualization. In step intermediary successfully receives the transmitted checkpoint information packet and in step transmits an acknowledgement of successful receipt back to primary server . Once the hypervisor of primary server receives the transmitted acknowledgement in step the hypervisor resumes execution of primary VM in step and releases all the queued up network packets from step in step before returning back to step . Delaying and queuing the outbound network packets in step and releasing them only after receiving acknowledgement from intermediary of receipt of a checkpoint information packet in step ensures that restoration of primary VM by backup server upon a failure of primary VM is based on a state of primary VM that can properly resume network communications with external entities i.e. re transmit outbound network packets since the recovered state without confusing recipients re receive inbound network packets that it is expecting etc. .

Meanwhile in step intermediary updates its stored snapshot of the state of primary VM with the updated memory pages or portions thereof in the checkpoint information packet received in step . Simultaneously with its continuous receipt of checkpoint information packets and updates to its stored snapshot of the state of primary VM in steps and intermediary also continuously e.g. via a separate running process or thread etc. determines and transmits those received memory pages that have been modified less or least recently by primary VM to backup server in step such less or least recently modified memory pages referred to herein as cold memory pages . In step the hypervisor of backup server receives these cold memory pages and in step incorporates the cold memory pages into its reconstructed state of primary VM for backup VM . It should be recognized that the reconstructed state of primary VM maintained by backup VM may not necessarily reflect a completed state of any particular past checkpointed state of primary VM since intermediary in step only transmits cold memory pages to backup server . That is memory pages that are considered hotter by intermediary i.e. modified more recently even if needed to provide backup server a complete set of memory pages reflecting the state of primary VM at a particular checkpoint are held back and not transmitted to backup server . Holding back such hotter memory pages conserves the limited bandwidth capacity of lower bandwidth connection between intermediary and backup server based upon a presumption that the hotter memory pages will be again modified before backup VM needs to take any action due to a failure of primary VM .

If in step intermediary detects a failure of primary VM or is otherwise notified thereof then in step intermediary notifies backup server of the failure of primary VM and transmits any unsent memory pages of its stored snapshot of primary VM to backup server . In step backup server receives notification of the failure of primary VM and the memory pages and in step incorporates the received memory pages into its reconstructed state for primary VM and resumes execution of primary VM as backup VM .

A thread referred to as receive thread manages the receipt of memory pages of primary VM from primary server e.g. from the initial snapshot in step as well as from each subsequent checkpoint information packet in step . In step for each memory page received from primary server via high bandwidth connection receive thread stores the memory page in the memory of intermediary . In step receive thread inserts the storage address of the stored memory page into the reference field of the entry in data structure corresponding to the received memory page. In step receive thread updates the checkpoint number field of the entry with the current checkpoint number.

A simultaneously running thread referred to as transmit thread manages the transmission of cold memory pages e.g. least recently modified to backup server as described in step of via low bandwidth connection . Transmit thread maintains a checkpoint variable indicating a checkpoint number that transmit thread is currently operating upon as well as a current array index that indicates the current entry in data structure upon which transmission thread is operating during its execution. Checkpoint variable is initialized to zero e.g. the value of checkpoint number field in each entry of data structure when such entry corresponds to the memory page received from the initial snapshot received in step of and current array index is initialized to the index of the first entry of data structure e.g. index of zero . Transmit thread begins with the first entry of data structure e.g. entry for memory page and if such entry s checkpoint number field matches checkpoint variable in step then in step transmit thread begins transmitting the memory page i.e. such memory page being a cold memory page referenced in the entry s reference field to backup server through lower bandwidth connection . In step upon receiving an acknowledgment from backup server of successful receipt of the memory page transmit thread determines whether current array index represents the index of the last entry of data structure . If transmit thread determines that current array index represents the index of the last entry in data structure then in step transmit thread increments checkpoint variable resets current array index to the index of the first entry of data structure e.g. index of zero and returns to the beginning of data structure . Otherwise transmit thread increments current array index and moves to the next entry in data structure in step .

If in step transmit thread receives notification of a failure of primary VM then in step transmit thread traverses through data structure transmitting memory pages referenced in each entry a whose checkpoint number is greater than checkpoint variable or b whose checkpoint number equals checkpoint variable and whose index is greater than or equal to current array index i.e. indicating that the memory page has not yet been transmitted to backup server . In one embodiment upon receiving notification of a failure of primary VM in step transmit thread begins to transmit the hotter memory pages first by transmitting those memory pages having the highest values in their checkpoint number fields in an effort to enable backup VM to start execution prior to receiving all unsent memory pages in the snapshot under a presumption for example that the hotter memory pages are more likely to be accessed during subsequent execution of backup VM than colder memory pages.

It should be recognized that transmit thread may traverse data structure and transmit cold memory pages to backup server at a significantly slower rate due to lower bandwidth connection than the rate that receive thread receives and updates memory pages at each checkpoint through high bandwidth connection . As such the value of checkpoint variable remains lower than the actual current checkpoint number of checkpoint information packets received by receive thread . By holding back hotter memory pages and transmitting cold memory pages intermediary thus reduces the possibility that the bandwidth capacity of lower bandwidth connection will be wasted on transmission of memory pages that would likely be overwritten with updated data in the near future i.e. fewer memory pages are transmitted by intermediary than are received .

It should be recognized that data structure and techniques described in are merely exemplary and that a variety of alternative data structures and techniques that may be utilized to determine whether memory pages are cold i.e. with a different conception of how cold may be defined or assessed . For example in an alternative embodiment of may include a transmission bit in each entry of data structure which would indicate whether the memory page corresponding to the entry has already been transmitted to backup VM . Another alternative embodiment utilizes an array of entries indexed by memory pages of the primary VM s snapshot similar to data structure where each entry in the array comprises a reference to the stored memory page similar to reference field and a counter value. In such an embodiment a receive thread increments the counter value for an entry each time a received checkpoint information packet includes a corresponding updated memory page. Simultaneously a transmit thread continually cycles through the array and transmits memory pages corresponding to entries that have a pre specified low counter value. Such an embodiment utilizes the concept of least frequently modified memory pages to define cold rather than least recently modified. Yet another alternative embodiment utilizes a data structure that maintains a list of checkpoint numbers for each memory page corresponding to the checkpoints in which such memory page was updated. Such a data structure provides flexibility to specify or define cold memory pages in a variety of ways such as for example memory pages with the smallest list of checkpoint numbers or memory pages that have remained unchanged for a consecutive number of checkpoints e.g. least frequently modified or least recently modified etc. .

Host computing device represents any computing device that includes a processor for executing instructions. For example host computing device may represent a group of processing units or other computing devices such as in a cloud computing configuration. Processor includes any quantity of processing units and is programmed to execute computer executable instructions for implementing aspects of the disclosure. The instructions may be performed by processor or by multiple processors executing within host computing device or performed by a processor or by multiple processors external to host computing device . In some embodiments executable instructions are stored in a memory . Memory is any device allowing information such as executable instructions and or other data to be stored and retrieved. Memory includes any quantity of computer readable media associated with or accessible by host computing device . Memory or portions thereof may be internal to host computing device external to host computing device or both. For example memory may include one or more random access memory RAM modules flash memory modules hard disks solid state disks and or optical disks.

Host computing device may include a user interface device for receiving data from a user and or for presenting data to user . User may interact indirectly with host computing device via another computing device such as VMware s vCenter Server or other management device. User interface device may include for example a keyboard a pointing device a mouse a stylus a touch sensitive panel e.g. a touch pad or a touch screen a gyroscope an accelerometer a position detector and or an audio input device. In some embodiments user interface device operates to receive data from user while another device e.g. a presentation device operates to present data to user . In other embodiments user interface device has a single component such as a touch screen that functions to both output data to user and receive data from user . In such embodiments user interface device operates as a presentation device for presenting information to user . In such embodiments user interface device represents any component capable of conveying information to user . For example user interface device may include without limitation a display device e.g. a liquid crystal display LCD organic light emitting diode OLED display or electronic ink display and or an audio output device e.g. a speaker or headphones . In some embodiments user interface device includes an output adapter such as a video adapter and or an audio adapter. An output adapter is operatively coupled to processor and configured to be operatively coupled to an output device such as a display device or an audio output device.

Host computing device also includes a network communication interface which enables host computing device to communicate with a remote device e.g. another computing device via a communication medium such as a wired or wireless packet network. For example host computing device may transmit and or receive data via network communication interface . User interface device and or network communication interface may be referred to collectively as an input interface and may be configured to receive information from user .

Host computing device further includes a storage interface that enables host computing device to communicate with one or more of datastores which store virtual disk images software applications and or any other data suitable for use with the methods described herein. In exemplary embodiments storage interface couples host computing device to a storage area network SAN e.g. a Fibre Channel network and or to a network attached storage NAS system e.g. via a packet network . The storage interface may be integrated with network communication interface .

The virtualization software layer supports a virtual machine execution space within which multiple virtual machines VMs may be concurrently instantiated and executed. Hypervisor includes a device driver layer and maps physical resources of hardware platform e.g. processor memory network communication interface and or user interface device to virtual resources of each of VMs such that each of VMs has its own virtual hardware platform e.g. a corresponding one of virtual hardware platforms each virtual hardware platform having its own emulated hardware such as a processor a memory a network communication interface a user interface device and other emulated I O devices in VM . Hypervisor may manage e.g. monitor initiate and or terminate execution of VMs according to policies associated with hypervisor such as a policy specifying that VMs are to be automatically restarted upon unexpected termination and or upon initialization of hypervisor . In addition or alternatively hypervisor may manage execution VMs based on requests received from a device other than host computing device . For example hypervisor may receive an execution instruction specifying the initiation of execution of first VM from a management device via network communication interface and execute the execution instruction to initiate execution of first VM .

In some embodiments memory in first virtual hardware platform includes a virtual disk that is associated with or mapped to one or more virtual disk images stored on a disk e.g. a hard disk or solid state disk of host computing device . The virtual disk image represents a file system e.g. a hierarchy of directories and files used by first VM in a single file or in a plurality of files each of which includes a portion of the file system. In addition or alternatively virtual disk images may be stored on one or more remote computing devices such as in a storage area network SAN configuration. In such embodiments any quantity of virtual disk images may be stored by the remote computing devices.

Device driver layer includes for example a communication interface driver that interacts with network communication interface to receive and transmit data from for example a local area network LAN connected to host computing device . Communication interface driver also includes a virtual bridge that simulates the broadcasting of data packets in a physical network received from one communication interface e.g. network communication interface to other communication interfaces e.g. the virtual communication interfaces of VMs . Each virtual communication interface for each VM such as network communication interface for first VM may be assigned a unique virtual Media Access Control MAC address that enables virtual bridge to simulate the forwarding of incoming data packets from network communication interface . In an embodiment network communication interface is an Ethernet adapter that is configured in promiscuous mode such that all Ethernet packets that it receives rather than just Ethernet packets addressed to its own physical MAC address are passed to virtual bridge which in turn is able to further forward the Ethernet packets to VMs . This configuration enables an Ethernet packet that has a virtual MAC address as its destination address to properly reach the VM in host computing device with a virtual communication interface that corresponds to such virtual MAC address.

Virtual hardware platform may function as an equivalent of a standard x86 hardware architecture such that any x86 compatible desktop operating system e.g. Microsoft WINDOWS brand operating system LINUX brand operating system SOLARIS brand operating system NETWARE or FREEBSD may be installed as guest operating system OS in order to execute applications for an instantiated VM such as first VM . Virtual hardware platforms may be considered to be part of virtual machine monitors VMM that implement virtual system support to coordinate operations between hypervisor and corresponding VMs . Those with ordinary skill in the art will recognize that the various terms layers and categorizations used to describe the virtualization components in may be referred to differently without departing from their functionality or the spirit or scope of the disclosure. For example virtual hardware platforms may also be considered to be separate from VMMs and VMMs may be considered to be separate from hypervisor . One example of hypervisor that may be used in an embodiment of the disclosure is included as a component in VMware s ESX brand software which is commercially available from VMware Inc.

For the operations illustrated and described with reference to and first computer system may correspond to a first server hosting a primary virtual machine and second computer system may correspond to a second server hosting a backup virtual machine. In such an example the first server may reside in a first fault domain while the second server resides in a second fault domain. Further second computer system may be networked to intermediary computer system through a bandwidth connection e.g. a low bandwidth connection that does not support timely transmission of modified states of first computer system received by intermediary computer system at checkpoints.

Further the operations illustrated in may be implemented as computer executable instructions stored on one or more computer readable storage media. The instructions when executed by a processor of intermediary computer system cause the processor to transmit an updated state of first computer system to second computer system . In some embodiments second computer system is networked to intermediary computer system through a low bandwidth connection that does not support timely transmission of modified states of first computer system received by intermediary computer system at checkpoints.

At intermediary computer system receives checkpoint information packets from first computer system . Each checkpoint information packet has updated data corresponding to one or more memory pages of first computer system that have been modified since a previously received checkpoint information packet. In some embodiments intermediary computer system receives the checkpoint information packets periodically through a high bandwidth connection with first computer system .

In some embodiments intermediary computer system maintains an array of entries corresponding to each memory page received from first computer system . Each entry of the array includes a reference to a copy of the corresponding memory page stored in intermediary computer system and a checkpoint number identifying a checkpoint at which the copy was received by intermediary computer system .

At intermediary computer system provides second computer system with data describing the memory pages stored at intermediary computer system . In some embodiments intermediary computer system publishes the data for access by second computer system . For example intermediary computer system publishes information about dirty memory pages e.g. modified or changed memory pages and the coldness of each memory page. The coldness may be based on how long ago the memory page was modified or how frequently the page is being modified. In general publishing the data includes for example publishing one or more of the following for each of the memory pages a memory page identifier a time based age of the memory page a frequency of modification for the memory page and a recency of modification to the memory page. Intermediary computer system may also provide an application programming interface API to enable second computer system to request one or more of the memory pages from intermediary computer system . For example the API may include a call in which the memory page identifier is an argument.

Second computer system analyzes or otherwise processes the data to identify memory pages of interest to second computer system . Second computer system may then request particular memory pages from intermediary computer system . For example if intermediary computer system receives from second computer system a request for a copy of at least one of the memory pages at intermediary computer system transmits the requested copy of the memory page to the second computing device at .

Alternatively or in addition second computer system may define policy information describing the type or kind of memory pages of interest to second computer system . The policy information includes factors or criteria for use in identifying the memory pages of interest. The factors or criteria include for example quantities of one or more of the following memory pages per epoch dirty pages e.g. changed pages guest pages user pages supervisor pages large pages e.g. more than four kilobytes and the like. Second computer system provides the policy information to intermediary computer system . Intermediary computer system receives and applies the policy information to select track and or monitor memory pages matching the criteria specified in the policy information. Intermediary computer system collects data describing the selected tracked and or monitored memory pages.

Alternatively or in addition to transmitting the updated state of first computer system to second computer system on demand as requested by second computer system as illustrated in intermediary computer system may automatically transmit the updated state e.g. without a request from second computer system . For example intermediary computer system may periodically determine whether the one or more memory pages stored at intermediary computer system have been updated by first computer system and have not been transmitted to second computer system . If so intermediary computer system transmits to second computer system without a request from second computer system the updated data corresponding to a memory page determined to be a least recently updated memory page. Determining whether any of the memory pages have been least recently updated includes for example traversing entries in an array storing received memory pages to compare a current checkpoint variable with the checkpoint number of each entry of the array. If the checkpoint number of the entry equals the current checkpoint variable intermediary computer system identifies the memory page corresponding to that entry as a least recently updated memory page. After analyzing a last entry in the array intermediary computer system increments the current checkpoint variable and immediately or subsequently proceeds to re traverse the array. In some embodiments the identified memory pages are then transmitted to second computer system . If updated data corresponding to a plurality of unsent memory pages is to be sent to second computer system intermediary computer system sends the memory pages that have been least recently modified first.

Similarly intermediary computer system may periodically determine whether the one or more memory pages stored at intermediary computer system have been updated by first computer system and have not been transmitted to second computer system . If so intermediary computer system transmits to second computer system without a request from second computer system the updated data corresponding to a memory page determined to be a least frequently updated memory page.

At second computer system receives or otherwise accesses data defining the memory pages available at the intermediary computing device. The data includes for example the published data described above with reference to . At second computer system defines policy information. As described herein the policy information includes factors or criteria for use in identifying the memory pages of interest.

At second computer system sends the defined policy information to intermediary computer system . Intermediary computer system applies the policy information to select track or monitor particular memory pages. Intermediary computer system sends updated data corresponding to the selected memory pages stored at intermediary computer system to second computer system . At second computer system receives the updated data from intermediary computer system .

In some embodiments second computer system may specify in the policy information or request explicitly copies of the same memory page corresponding to different times or time intervals. For example second computer system may request a copy of a particular memory page before application of a root toolkit and a copy of the same particular memory page after application of the root toolkit. Upon receipt of the two or more copies of the same particular memory page second computer system compares the received copies to identify differences in the copies. This enables second computer system to analyze the effects of applying the root toolkit.

Intermediary computer system maintains information describing the pages stored by intermediary computer system . Similarly each of second computer systems maintains information describing the pages stored by that second computer system . The information may be stored in any format and in any data structure. In the example of the information is stored in tables. An exemplary table correlates each page number to a checkpoint identifier ID version number epoch number transfer time or other means for identifying the version of the page represented by the page number.

The table maintained by intermediary computer system correlates page numbers associated with a plurality of the pages stored by intermediary computer system with checkpoint IDs. In some embodiments intermediary computer system updates the table as the pages are received from first computer system and stored by intermediary computer system .

The tables maintained by second computer systems correlate page numbers associated with a plurality of the pages stored by second computer systems with checkpoint IDs. In some embodiments each table is initially empty and then populated over time by second computer system maintaining the table as pages are received from intermediary computer system . For example second computer system may request pages from intermediary computer system on a page by page basis and thus update the table on a page by page basis.

Alternatively or in addition second computer system may receive a snapshot of the table stored by intermediary computer system . From the table second computer system selects one or more pages to request from intermediary computer system based on the recency of modification of the pages frequency of modification of the pages and or other policy. Second computer system requests the selected pages from intermediary computer system .

Intermediary computer system transmits the requested pages to intermediary computer system individually or as a batch of pages. For example depending on a cost of the link between intermediary computer system and second computer system transmission of the pages from intermediary computer system to second computer system can be throttled. The cost of the link may be defined as available bandwidth financially in terms of latency or any other measure.

As an example and as shown in the table or map obtained by second computer system indicates that Page X is at Checkpoint ID . Second computer system requests Page X from intermediary computer system but receives Page X with Checkpoint ID from intermediary computer system because Checkpoint ID is the latest version of Page X. For example the table or map maintained by second computer system became out of date because of a delay between the snapshot request and the request for Page X. Second computer system stores the received Page X and updates the table to reflect that the stored version or generation of Page X is Checkpoint ID . Page X stored by second computer system is thus a coherent copy of Page X stored by intermediary computer system with respect to page number and checkpoint ID.

Similarly the table or map obtained by second computer system indicates that Page Z is at Checkpoint ID . Second computer system requests Page Z from intermediary computer system but receives Page Z with Checkpoint ID from intermediary computer system because Checkpoint ID is the latest version of Page Z. Second computer system stores the received Page Z and updates the table to reflect that the stored version of Page Z is Checkpoint ID .

After receiving the initial snapshot of the table maintained by intermediary computer system and receiving the selected pages and updating the table second computer system may subsequently request another snapshot of the table stored by intermediary computer system . Second computer system compares this later snapshot with the table currently maintained by second computer system to identify those pages that have changed e.g. with different checkpoint IDs . Second computer system then requests updated versions of those changed pages from intermediary computer system . In this manner second computer system only updates those pages that have changed e.g. dirty pages since the last iteration.

The table maintained by intermediary computer system may include other columns not shown or described herein but only transmit selected columns based on the request from second computer system . As such intermediary computer system may dynamically create maps tables or snapshots on the fly.

Alternatively or in addition intermediary computer system may provide additional information from its table that has not been requested by second computer system . For example intermediary computer system may proactively send to second computer system copies of pages that have been recently requested by other second computer systems . In this manner intermediary computer system optimizes page transmission based on the likelihood of relevancy of the pages to second computer system . Page transmission may also be optimized based on the cost of the link between intermediary computer system and second computer system e.g. transmit additional pages if the added cost is minimal or reasonable .

In one scenario first computer system fails at some point in time. Upon detection of the failure by intermediary computer system or notification to intermediary computer system intermediary computer system transmits to second computer system the updated data corresponding to each unsent memory page stored at intermediary computer system . In this manner second computer system is then able to move from acting as a backup or secondary system to acting as first computer system . For example upon failure of first computer system intermediary computer system sends an evacuate signal to second computer system . Second computer system requests copies of each of the memory pages stored by intermediary computer system that have not yet been transmitted to second computer system . Upon the receipt of such a request intermediary computer system transmits the requested copies of the memory pages to second computer system .

In another example intermediary computer system responds to requests from a plurality of second computer systems . In such an example intermediary computer system may identify request patterns among the plurality of second computer systems . Exemplary request patterns include but are not limited to page based patterns e.g. second computer systems requesting similar sets of memory pages and time based patterns e.g. second computer systems requesting similar sets of memory pages at approximately the same time . Based on the request patterns intermediary computer system proactively shares copies of memory pages requested by one of second computer systems with other second computer systems e.g. in anticipation of receiving similar requests from these other second computer systems .

It should be recognized that various modifications and changes may be made to the specific embodiments described herein without departing from the broader spirit and scope of the invention as set forth in the appended claims. For example while the foregoing discussions have focused on embodiments in which primary server and intermediary transmitting complete memory pages if such memory page has been modified it should be recognized that alternative embodiments may apply difference techniques or other compression techniques on memory pages at either or both of primary server and intermediary prior to their transmission. Such alternative embodiments may thus transmit only updated data corresponding to the memory pages rather than the complete memory pages themselves. Similarly it should be recognized that although the foregoing embodiments have discussed a single intermediary embodiments may incorporate multiple intermediaries possible in different fault domains such that probability of failure of all intermediaries is negligible. Additionally while the foregoing embodiments have been generally described using primary and backup VMs other primary and backup computer systems including non virtualized systems may be used consistent with the teachings herein.

The operations described herein may be performed by a computer or computing device. The computing devices communicate with each other through an exchange of messages and or stored data. Communication may occur using any protocol or mechanism over any wired or wireless connection. A computing device may transmit a message as a broadcast message e.g. to an entire network and or data bus a multicast message e.g. addressed to a plurality of other computing devices and or as a plurality of unicast messages each of which is addressed to an individual computing device. Further in some embodiments messages are transmitted using a network protocol that does not guarantee delivery such as User Datagram Protocol UDP . Accordingly when transmitting a message a computing device may transmit multiple copies of the message enabling the computing device to reduce the risk of non delivery.

Embodiments of the disclosure may be described in the general context of computer executable instructions such as program modules executed by one or more computers or other devices. The computer executable instructions may be organized into one or more computer executable components or modules. Generally program modules include but are not limited to routines programs objects components and data structures that perform particular tasks or implement particular abstract data types. Aspects of the disclosure may be implemented with any number and organization of such components or modules. For example aspects of the disclosure are not limited to the specific computer executable instructions or the specific components or modules illustrated in the figures and described herein. Other embodiments of the disclosure may include different computer executable instructions or components having more or less functionality than illustrated and described herein.

Aspects of the disclosure transform a general purpose computer into a special purpose computing device when programmed to execute the instructions described herein.

One or more embodiments of the present disclosure may be implemented as one or more computer programs computer executable instructions or as one or more computer program modules embodied in one or more computer readable media. In some embodiments the term computer readable medium refers to any data storage device that stores data that can thereafter be input to a computer system. Computer readable media may be based on any existing or subsequently developed technology for embodying computer programs in a manner that enables them to be read by a computer. Exemplary computer readable media include memory such as hard drives network attached storage NAS read only memory random access memory flash memory drives digital versatile discs DVDs compact discs CDs floppy disks magnetic tape and other optical and non optical data storage devices. By way of example and not limitation computer readable media comprise computer storage media and communication media. Computer storage media include volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media are tangible exclude propagated data signals and are mutually exclusive to communication media. In some embodiments computer storage media are implemented in hardware. Exemplary computer storage media include hard disks flash drives and other solid state memory. In contrast communication media typically embody computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and include any information delivery media.

The computer readable medium can also be distributed over a network coupled computer system so that the computer readable code is stored and executed in a distributed fashion.

The various embodiments described herein may employ various computer implemented operations involving data stored in computer systems. For example these operations may require physical manipulation of physical quantities usually though not necessarily these quantities may take the form of electrical or magnetic signals where they or representations of them are capable of being stored transferred combined compared or otherwise manipulated. Further such manipulations are often referred to in terms such as producing identifying determining or comparing.

Although described in connection with an exemplary computing system environment embodiments of the disclosure are operative with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use with aspects of the disclosure include but are not limited to mobile computing devices personal computers server computers hand held or laptop devices multiprocessor systems gaming consoles microprocessor based systems set top boxes programmable consumer electronics mobile telephones network PCs minicomputers mainframe computers distributed computing environments that include any of the above systems or devices and the like.

Plural instances may be provided for components operations or structures described herein as a single instance. Further at least a portion of the functionality of the various elements illustrated in the figures may be performed by other elements in the figures or an entity e.g. processor web service server application program computing device etc. not shown in the figures. For example while boundaries between various components operations and data stores are illustrated in the context of specific illustrative configurations other allocations of functionality are envisioned that fall within the scope of the invention. In general structures and functionality presented as separate components in exemplary configurations may be implemented as a combined structure or component. Similarly structures and functionality presented as a single component may be implemented as separate components. These and other variations modifications additions and improvements may fall within the scope of the appended claims s .

In some embodiments the operations illustrated in the figures may be implemented as software instructions encoded on a computer readable medium in hardware programmed or designed to perform the operations or both. For example aspects of the disclosure may be implemented as a system on a chip or other circuitry including a plurality of interconnected electrically conductive elements.

The order of execution or performance of the operations in embodiments of the disclosure illustrated and described herein is not essential unless otherwise specified. That is the operations may be performed in any order unless otherwise specified and embodiments of the disclosure may include additional or fewer operations than those disclosed herein. For example it is contemplated that executing or performing a particular operation before contemporaneously with or after another operation is within the scope of aspects of the disclosure.

When introducing elements of aspects of the disclosure or the embodiments thereof the articles a an the and said are intended to mean that there are one or more of the elements. The terms comprising including and having are intended to be inclusive and mean that there may be additional elements other than the listed elements.

Having described aspects of the disclosure in detail it will be apparent that modifications and variations are possible without departing from the scope of aspects of the disclosure as defined in the appended claims. As various changes could be made in the above constructions products and methods without departing from the scope of aspects of the disclosure it is intended that all matter contained in the above description and shown in the accompanying drawings shall be interpreted as illustrative and not in a limiting sense.

