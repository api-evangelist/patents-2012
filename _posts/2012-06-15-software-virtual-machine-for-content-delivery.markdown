---

title: Software virtual machine for content delivery
abstract: In general, this disclosure is directed to a software virtual machine that provides high-performance transactional data acceleration optimized for multi-core computing platforms. The virtual machine utilizes an underlying parallelization engine that seeks to maximize the efficiencies of multi-core computing platforms to provide a highly scalable, high performance (lowest latency), virtual machine. In some embodiments, the virtual machine may be viewed as an in-memory virtual machine with an ability in its operational state to self organize and self seek, in real time, available memory work boundaries to automatically optimize maximum available throughput for data processing acceleration and content delivery of massive amounts of data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08645958&OS=08645958&RS=08645958
owner: uCIRRUS
number: 08645958
owner_city: San Mateo
owner_country: US
publication_date: 20120615
---
This application claims the benefit of U.S. Provisional Application No. 61 497 860 filed Jun. 16 2011 the entire content of which is incorporated herein by reference.

Explosive growth in global data volume ubiquity of devices connecting to networks lower barriers of entry for user content creation and sharing digitization of many formerly offline functions banking medical prescriptions etc. emergence of virtualized and offsite networked systems clouds among other factors have contributed to the emergence of the Big Data era. This presents challenges for systems as applications processing faces extreme massive volume throughput and requirements to deliver or distribute processed data to any number of destination points. These systems additionally must be scalable to keep pace with the continuing growth of Big Data and enable interactivity for pervasive large audience Internet and cloud applications.

The utilization of multi core processors has increased dramatically in the computing industries. In general the term processor refers to the unit of hardware that reads and executes program instructions. Historically processors originally utilized a single core which refers to the portion of the processor that reads and executes a sequence of instructions. A multi core processor refers to a single hardware unit in which two or more independent processing cores are integrated onto a single package. Recently computing systems having upwards of 128 to 256 processing cores have become available. Such multi core computing platforms present challenges over traditional programming techniques.

In general this disclosure is directed to a software virtual machine that provides high performance transactional data acceleration optimized for multi core computing platforms. The virtual machine utilizes an underlying parallelization engine that seeks to maximize the efficiencies of multi core computing platforms to provide a highly scalable high performance lowest latency virtual machine.

In some embodiments the underlying parallelization engine of the software virtual machine provides self organization in its ability to parallelize and store relevant data for transaction processing to data partitions each associated with different execution units for the software virtual machine. In addition tasks collectively processing transactions and corresponding transactional data provide self tuning in their ability to autonomously determine and migrate among execution units that process the tasks. As a result the software virtual machine may employ multiple distributed transaction delegation units and so avoid both a centralized transaction administrator to manage data organization and transactional delegation and the inherent bottlenecks associated with such centralized administration.

These techniques may be useful in systems required to address the particular needs of dynamic and interactive data acceleration for large audience web applications and Big Data clouds. In particular a system that implements the described techniques can aggregate transactional data to effectively manage ingestion of massive data emanating from manifold sources and received by the system as well as disaggregate transactional data to deliver processed data to select destinations. For example a unique data communications feature is the platform s ability to push interactively push broadcast selected data to individual devices users and create interactive private broadcast sessions channels within a homogenous mass broadcast data stream. The platform techniques may also enable customer provided transaction and messaging application acceleration in a system operating a scalable in memory database cache with integrated on demand real time indefinite run time extension to secondary storage. Such a system may leverage platform techniques for scaling out the cache beyond physical in memory boundaries and when required integrate as part of the virtual machine memory boundaries an automatic extension and use of physically external memory devices e.g. hard drives . The software virtual machine described herein in other words supports a move from static information architectures that have difficulty supporting or creating value from Big Data to a dynamic architecture model. With low latency scalable processing in conjunction with reduced complexity and increased cost effectiveness the described techniques specifically address the conditions of Big Data processing to provide the ability to concurrently consume and process massive transaction volumes from large numbers of data producers along with the ability to push processed data to billions of data consumers in an interactive manner.

In one example a device comprises a multi core hardware processor having a plurality of execution cores. The device also comprises an in memory database comprising data stored within a plurality of memory partitions wherein each of the memory partitions is associated with a different one of the execution cores. The device further comprises a content delivery engine that configures a plurality of private data channels to each deliver unique data to a corresponding one of a plurality of data consumers. The device also comprises a parallelization engine that deploys a plurality of tasks to concurrently execute on the cores to concurrently perform transactions on the in memory database in response to queries from the content delivery engine.

In another example a method comprises configuring with a content delivery engine a plurality of private data channels to each deliver unique data to a corresponding one of a plurality of data consumers. The method also comprises deploying with a parallelization engine executing on a multi core hardware processor having a plurality of execution cores a plurality of tasks to concurrently execute on the cores to concurrently perform transactions on an in memory database in response to queries from the content delivery engine wherein the in memory database comprises data stored within a plurality of memory partitions wherein each of the memory partitions is associated with a different one of the execution cores.

In another example a computer readable storage device comprises instructions that when executed cause a multi core hardware processor having a plurality of execution cores to configure with a content delivery engine a plurality of private data channels to each deliver unique data to a corresponding one of a plurality of data consumers. The instructions when executed also cause the multi core hardware processor to deploy with a parallelization engine a plurality of tasks to concurrently execute on the cores to concurrently perform transactions on an in memory database in response to queries from the content delivery engine wherein the in memory database comprises data stored within a plurality of memory partitions wherein each of the memory partitions is associated with a different one of the execution cores.

The details of one or more embodiments of the invention are set forth in the accompanying drawings and the description below. Other features objects and advantages of the invention will be apparent from the description and drawings and from the claims.

As described herein each of transactional data accelerators includes a parallelization engine that provides a massively parallel multi core operating environment for a virtual machine for ingesting and transactionally processing the ingested data. The software virtual machine provides high performance transactional data acceleration optimized for the underlying multi core computing platforms of transactional data accelerators . That is the parallelization engine provides a base platform on which data ingestion and processing can be parallelized in a manner that is highly scalable and optimized for independent execution across an arbitrary number of execution cores of multiple clustered computing devices. Incoming data transactions received from data sources are decomposed by the parallelization engines into operations that can execute independently on the individual cores of the underlying multi core computing platforms of transactional data accelerators . The virtual machines operate on an in memory database organized in a manner that allows the parallelization engine of each of the accelerators to execute transactional operations for inserting deleting updating and querying data from the in memory database in parallel across the cores of the underlying multi core processing environment.

Moreover during operation the parallelization engine of the software virtual machine executing on each of transactional data accelerators may dynamically self organize memory usage to scale the in memory database beyond the physical in memory boundaries. In this way the software virtual machines of transactional data accelerators may provide an automatic extension and use of physically external memory devices e.g. hard drives when processing the ingested transaction data. This allows the virtual machines to dynamically leverage virtual storage for the in memory database as needed with the continuous processing of the inbound transactions in parallel on the multi core computing environment.

Further the virtual machines executed by transactional data accelerators include parallelized communication features that allow the transactional data accelerators to interactively push selected data to data consumers . For example data consumers may be individual devices and the parallelized communication features of transactional data accelerators may create interactive channels within a mass broadcast data stream to push large amounts of individually requested data to high volumes on data consumers .

In this way transactional data accelerators execute virtual machines having underlying parallelization engines that seek to maximize the efficiencies of multi core computing platforms to provide highly scalable high performance lowest latency data transaction acceleration. Moreover the virtual machine may be viewed as an in memory virtual machine with an ability to self organize in its operational state and self seek in real time available memory work boundaries to automatically optimize maximum available throughput for data processing acceleration and content delivery of massive amounts of data.

The parallelized virtual machines described herein allow transactional data from data sources to be dynamically acted upon in flight then directed to data consumers for further processing operations or presentation to users in the most appropriate and usable form. In addition the parallelized virtual machines of transactional data accelerators may operate on the data stream in real time without necessarily requiring the data to be written to disk and acted on in its entirety. As such the parallelized virtual machines may accelerate the processing speed and relevancy of transactional data presented to data consumers .

The parallel processing virtual machines of software accelerators may be used to augment existing data processing infrastructures and applications in cloud mobile social and enterprise computing environments to deliver low latency highly scalable data acceleration with performance increase and operating cost reduction.

In the example of accelerator includes a platform abstraction layer that presents certain functionality of underlying operating system kernel to virtual processor such as memory management and hardware I O. In one example virtual processor may execute within an overall process space provided by operating system kernel . Operating system kernel may be for example a Linux Berkeley Software Distribution BSD another Unix variant kernel or a Windows operating system kernel available from Microsoft Corp.

Data ingest engine of virtual processor operates to ingest incoming transactional data. Data ingest engine may receive data from thousands to millions of concurrent inbound connections each streaming data inwards without needing to be prompted for new information. Data can be ingested from a few fat pipes or over thousands of connections from individual devices or sensors. Data types to be processed can be structured unstructured or both. For example data ingest engine may include one or more data adaptors to receive and process formatted data e.g. XML and CSV formatted data

As incoming data is ingested into the system by data ingest engine database acceleration engine operates on in memory database . Database acceleration engine provide a highly scalable data processing engine that has primary responsibility for coordination of activities between data ingest engine customer applications and content delivery engine . In some examples database acceleration engine exposes a standard SQL based API by which data ingest engine customer applications and content delivery engine interact with in memory database and execute under the control of parallelization engine such that extremely low latency processing occurs. In other words the database acceleration engine may provide a SQL accessible in flight view at incoming data as is ingested and stored within in memory database . In general database acceleration engine utilizes the parallelization engine to decompose incoming transactions or events into fine grained blocks of operations that are then deployed to the closest and most available hardware execution core relevant to the data required for processing. The database acceleration engine enables the decomposition processing concurrency checks and re assembly of transactions and events into computed results.

Content delivery engine may output data to one several or many destinations i.e. data consumers . That is content delivery engine provides the ability for processed data to be pushed delivered from in memory database as a targeted data stream to data consumers which may be other systems applications or databases. For example in some embodiments content delivery engine may be configured to deliver data via a single targeted stream to another computing system or data warehouse. In addition content delivery engine may provide an interactive bidirectional data delivery and communications engine to interactively and bi directionally broadcast data to large audiences or devices i.e. data consumers .

In some embodiments data ingest engine and content delivery engine may support select cast capabilities described herein that enable either a data source or a data consumer or both to tailor the content that is sent or received. This may take the form of private data channels i.e. interactive and personalized data streams unique to each source or consumer. Each connection may be a continuous open connection such that data consumers need not poll for changed data. Data ingest engine and content delivery engine may supports millions of open continuous connections to data sources and data consumers .

Parallelization engine of virtual processor provides an abstracted execution environment that utilizes multicore processors and motherboard architectures to form a highly integrated low latency scalable parallel platform for execution of data ingest engine database acceleration engine virtual storage manager and content delivery engine . That is parallelization engine provides an execution platform optimized for multi core and many core systems to provide real time concurrency memory management and task management capabilities with fine grained parallelism on a per core basis. Further parallelization engine ensures optimal use of instruction and data caches e.g. L1 L2 and L3 caches while implementing fine grained parallelism by decomposing all processing into tasks that can independently execute on individual cores and by minimizing the requirement for concurrent locking structures. This allows data ingest engine database acceleration engine virtual storage manager and content delivery engine to operate within virtual machine with a high degree of parallel execution on multi core computing platform . In this way parallelization engine may be viewed as a massively parallel multi core operating system that provides a virtual processor virtual machine for processing the ingested data.

Virtual storage manager of the software virtual machine provides self organization and allows the virtual machine to scale out beyond physical in memory boundaries and when required integrate the use of physically external memory devices e.g. hard drives . This allows in memory database to spool out to external memory while performing the transactions to expand its memory space to utilize persistent storage . For example virtual storage manager may temporarily spool transactional data if the data does not fit within the physical memory boundaries and push data out for storage and computation. Moreover all of these services for virtual storage management are parallelized within virtual machine and executed by parallelization engine for execution on multi core computing platform .

In addition virtual storage manager manages the persistent storage to allow for recovery from a failure or for users to shut down the system and bring it back up with no associated data loss. The disk persistence implementation guarantees no transaction loss in the event of a failure. As explained in further detail below copies of in memory database may be written to checkpoint files on a configurable time interval. Further in some cases transactions may be recorded in journal files and transaction commits only occur after entries are written to the journal files. To recover from a system failure the persistence subsystem of virtual storage manager may apply the latest checkpoint file and then apply all journal entries since the last checkpoint to recreate in memory database . In this way persistence may be implemented to be ACID atomicity consistency isolation durability compliant.

Subsets of cores combined in a multi core processor may share processor components while each core of the subset maintains at least an independent execution unit to perform instructions substantially independently of the other cores of the subset. For example cores A B may share a level 3 L3 cache and a memory management unit MMU for a multi core processor that includes the cores. However cores A B in this example each include a separate execution unit and separate level 1 L1 level 2 L2 caches. Alternatively cores A B may share L2 L3 caches and an MMU of the multi core processor. In some instances multi core computing platform may represent a cluster of separate motherboards or processing blades hereinafter cluster machines inserted within one or more chassis. Each cluster machine in such instances may include one or more multi core processors each having a subset of cores .

Data of in memory relational database is stored in one or more computer readable storage media that includes partitions A N partitions each located in a separate physical location and each associated with a respective one of cores . The computer readable storage media that store in memory relational database may present a non uniform memory access NUMA architecture. That is cores may not have equal memory access time to each of partitions . In some instances each of partitions associated with respective cores represent the partition of in memory relational database having a memory access time that is less than or equal to the memory access time to any other one of the partitions for the core. In other words cores may use respective partitions that offer the lowest memory latency for the cores to reduce overall memory latency.

Each of partitions comprises computer readable storage media such as non transitory computer readable mediums including a memory such as random access memory RAM including various forms of dynamic RAM DRAM e.g. DDR2 SDRAM or static RAM SRAM Flash memory content addressable memory CAM ternary CAM TCAM or another form of fixed or removable storage medium that can be used to carry or store desired instructions and in memory relational database data and that can be accessed by cores .

In some instances partitions may each represent partitions of a physical address space for a computer readable storage medium that is shared among one or more cores i.e. a shared memory . For example cores A may be connected via a memory bus not shown to one or more DRAM packages modules and or chips also not shown that present a physical address space accessible by the multi core processor and storing data for partition A. While partition A may offer the lowest memory access time to core A of any of partitions one or more of other partitions may be directly accessible to core A. In some instances partitions may also or alternatively each represent a cache of corresponding cores . For example partition A may comprise an on chip cache e.g. an L1 L2 L3 cache or a combination thereof for core A.

Partitions store non overlapping portions of decentralized database objects for in memory relational database . Such objects may include relational tables or indexes stored and managed using underlying data structures such as trees or tries of data objects flat files heaps hash buckets and B trees for instance. As described in detail below parallelization engine apportions a separate underlying data structure for respective database objects to each of partitions and also assigns data to be managed by the underlying data structures for database objects to different one of partitions effectively parallelizing the database objects among the partitions. Because each of cores reads and writes from a different one of partitions partitions are not subject to corruption due to concurrent operation of multiple cores . As a result tasks executing on cores may eschew locking partitions in many circumstances.

Client interface of parallelization engine presents an interface by which clients may issue requests to accelerator . In some aspects client interface implements transport layer e.g. Transmission Control Protocol TCP Internet Protocol IP or User Datagram Protocol UDP IP sockets to receive and return data from to clients that invoke the interface.

Parallelization engine decomposes incoming transaction into fine grained sub transactions A N sub transactions and distributes the sub transactions to multiple execution tasks that run on the one of cores that is logically associated with the one of partitions relevant to the data for the respective sub transactions . In some cases the relevant one of partitions is the partition that stores data to be returned in a query type transaction . In some cases the relevant one of partitions is the partition that stores for a database index object an underlying data structure that is to store the data for an insert type transaction that references the database index object.

Parallelization engine may automatically determine a number of cores of accelerator without requiring software configuration by an administrator. Upon a determination of the number of cores parallelization engine creates a corresponding one of partitions for each of cores . This feature may allow for parallel deployment to an arbitrary number of cores again without requiring reconfiguration of the underlying software.

Transaction is a transaction unit that is a self contained work unit received and performed by accelerator to alter a state of in memory relational database . Transaction may be ACID compliant in order to provide isolation between the transactions for concurrent execution and to provide for rollback in the event of failure. Transaction may include a request string that conforms for example to a declarative language statement a query language or query programming language program a functional programming language program or a rule language program that specifies the respective work unit to be performed by accelerator . Transaction contains one or more subunits of work that may be performed by an independent execution unit as individual sub transactions of the atomic parent transaction . Sub transactions may include with respect to in memory relational database reading writing manipulating and deleting data creating and managing database objects creating and managing metadata and arithmetic and string manipulation operations.

Compiler task compiler receives transaction and decomposes the transactions to sub transactions using transaction grammar grammar which describes the particular language of incoming transactions including transaction in combination with database schema for in memory relational database and step library . In one example grammar includes a set of one or more substitution rules each having variables to match data in database schema describing the organization of in memory relational database . Database schema may comprise a data dictionary. Each of the substitution rules of grammar references variables for additional substitution rules in grammar or steps in step library . Compiler parses transaction to generate tokenized request strings and then compiler applies grammar to each tokenized request string in view of database schema to yield for the transaction one or more series of steps stored by step library . Each series of steps constitutes a separate task that when executed in series by an execution task performs one of sub transactions . In this way compiler decomposes transaction to sub transactions for distribution to and execution by multiple cores .

Step library comprises a dictionary that maps groups of step instructions executable by cores to step keys referenced by grammar . Each group of step instructions may include pre compiled machine executable instructions for cores . To execute a series of steps i.e. a task each identified by a step key an execution unit maps the step keys for the steps to step library to obtain the corresponding mapped step instructions then executes the mapped step instructions on a step by step basis. Each of tasks and subordinate execution tasks A N illustrated as sub. execution A N represent respective series of steps for corresponding sub transactions .

Having decomposed transaction into sub transactions compiler spawns execution task to manage the execution of the sub transactions and return any required response for transaction . In this way compiler generates an execution plan and spawns execution task to perform the execution plan. Execution task spawns subordinate execution tasks to execute corresponding sub transactions . In some cases transaction may represent multiple separate database transactions. In such cases compiler may spawn a separate execution task to manage each transaction or reuse execution task to manage the transactions.

Sub transactions may each relate to different data stored by in memory relational database in separate partitions . For example transaction may comprise a query request for rows of a database table having field values that match multiple criteria e.g. a SELECT SQL statement with a WHERE clause where the database table has a corresponding index defined for the field. As another example transaction may comprise a request to add a row to a database table having multiple indices defined e.g. an INSERT SQL statement or to update with a new field value all rows of the database table that match one or more criteria e.g. an UPDATE SQL statement. As another example transaction may comprise a request to return a sum of all values for a row field of a database table. Execution task spawns subordinate execution tasks for sub transactions and assigns the tasks to different cores based on the related data. Execution task may provide a memory pointer to itself to enable subordinate execution tasks to return resulting data or status information. Any of subordinate execution tasks may in turn spawn additional subordinate execution tasks in a recursive decomposition of sub transactions .

Execution task inputs data for a sub transaction to an assignment algorithm such as a hash function that outputs an index or other identifier that identifies one of cores . For the request to add a row example above execution task may input to the assignment algorithm the complete row data or a subset of the row data such as the row data for fields for which indices are defined for the database table. The assignment algorithm may be for example an MD5 SHA 1 or a bitwise operation applied to the input data and modulo the number of cores or any other another function that produces a value within a range of the number of cores when provided arbitrary input data. For the query request and update statement examples above execution task may hash the criteria for the query request and then calculate the hash output modulo the number of cores. The core index output by the assignment algorithm provided database object data for sub transactions determines the execution one of cores of respective subordinate execution tasks for the sub transactions when spawned by execution task . In the illustrated example core A having index executes sub transaction A because data related to sub transaction A causes the assignment algorithm to output index . Core B having index executes sub transactions B because data related to sub transaction B causes the assignment algorithm to output index and so on. In this way the assignment algorithm associates data with different partitions and also with the cores that access the respective partitions when delegated sub transactions in accordance with the assignment algorithm. For the sum of all values example above execution task spawns one of sub transactions for each core . Each of the sub transactions causes respective sub ordinate execution tasks to calculate a partial sum of data for the database table stored by the associated one of partitions .

Each of cores operates on a different one of partitions . By directing subordinate execution tasks to different cores for execution execution task causes data related to respective sub transactions for subordinate execution tasks to be stored by different known partitions . In the illustrated example because core B logically associated with partition B executes subordinate execution task B partition B stores data related to sub transaction B. The techniques therefore effectively parallelize transactions as well as assign execution tasks to cores that offer a lowest memory access time to partitions that store the parallelized data related to the respective execution tasks. Subordinate execution tasks may migrate among cores when a corresponding one of sub transactions relates to data stored by multiple partitions . In this way tasks execute nearest by memory latency to the data required by the tasks.

Each of subordinate execution tasks comprises a series of steps. To execute the steps subordinate execution tasks may map the step keys for the steps to corresponding step instructions in step library and direct cores to execute the step instructions. Each of subordinate execution tasks executes on a different one of cores . Subordinate execution tasks may thus execute substantially in parallel despite performing in combination a single transaction . As a result parallelization engine may achieve a substantial improvement in transaction processing speed that scales to an arbitrary number of cores and the level of parallelization inherent within transaction . Furthermore parallelization engine achieves such parallelization in a single system with a consolidated in memory relational database that may nevertheless be accessed via multiple cores of the system rather than in a distributed system that partitions a database among multiple database servers and thus requires a separate load balancing server or controller to balance database data among the multiple partitions.

Subordinate execution tasks may provide respective return values to execution task referenced via a memory pointer to execution task referenced with the subordinate execution tasks. Return values may include requested data partial data e.g. a partial sum and execution status values e.g. success fail for example. Execution task generates a response string for transaction using the return values and outputs the response string to one or more requesting clients via client interface or execution task aggregates the return values for further processing with another task within parallelization engine .

While described with respect to operations performed on an in memory database the techniques of this disclosure apply to other applications that may benefit from parallelized processing of incoming transactions. For example transaction may represent a packet data unit PDU having a plurality of field data that must be individually processed. This field data can be decomposed by compiler into a number of sub transactions for execution by respective sub execution tasks . In some instances transaction may represent a code snippet that conforms to a programming language such as C C or Java. In such instances compiler may execute a compiler for the programming language to dynamically produce machine code for execution by sub execution tasks directly on respective cores to process inbound transactions e.g. packets . In this way parallelization engine may dynamically alter the executing program in accordance with received code snippets in order to support flexible parallelized processing of inbound outbound data e.g. PDUs .

Virtual processors may offer a uniform platform independent execution environment for virtual processor management task scheduling statement compilation and execution database transaction processing journaling virtual processor load balancing database persistence recovery and replication data ingestion and output and user defined actions. Virtual processors may implement the execution environment by offering a virtual instruction set architecture ISA that is uniform for each of the virtual processors. The virtual processors receive instructions for executing tasks and translate the instructions to kernel level library calls and or to instructions that conform to the native ISA provided by respective cores. In this way virtual processors provide a set of fully parallelized virtual machines with which to execute tasks .

In some embodiments a plurality of cores support a single instance of a kernel and a process to provide a virtual processor . For example core A B may execute threads for a single process. In such embodiments cores that cooperate to provide a virtual processor have access to a single physical or virtual address space provided by the process. Such virtual processors may also be referred to as process virtual machines. As a result tasks that execute on the cooperating cores can pass messages migrate and spawn other tasks among the cores by writing to and reading from the common address space.

Tasks include a set of steps that conform to the uniform platform independent programming environment provided by each of virtual processors. The steps may represent an example embodiment of the steps of step library described with respect to . That is virtual processors may translate steps to a set of instructions machine executable by cores . As a result each of tasks may seamlessly migrate to and execute on any of the virtual processors without requiring recompilation to a new ISA or translation to a new programming environment.

Caches of respective cores store data associated with the respective core and may represent example embodiments of partitions of . Each of caches includes one of partial structures A N partial structures that cache data for a collective data structure that represents a database object such as index defined for table of in memory relational database . In other words each of partial structures caches a non overlapping subset of the data for index . Partial structures and index may include tables trees linked lists and B trees for instance. In accordance with the techniques of this disclosure partial structures cache respective data for index when the data when input to an assignment algorithm executed by one of virtual processors results in an index value associated with one of cores that comprises the partial structure in its cache. Partial structures may include subsets of data stored and managed by any database object that may be partitioned including tables indices individual table rows and internal structures. In addition while illustrated as residing in caches partial structures may be distributed within any one or more computer readable storage media.

For example the collective data structure may be an index that includes field values for the database index field that each map to a pointer that resolves to a row of table stored in in memory relational database . In this example each of partial structures includes field value pointer mappings assigned to the one of cores that includes the partial structure. As a result tasks executing on virtual processors may quickly determine the location of field value pointer mappings among partial structures for the database index by hashing field value data. In some instances cores may maintain an array or other associative data structure for index that maps index values for cores to memory addresses in memory space for corresponding partial structures . Upon applying the assignment algorithm to determine an index value for lookup data tasks map the index value to the memory address for one of partial structures and migrate to the virtual processor corresponding to the one of cores associated with the index value for execution. In this way each of cores is logically associated with the data in respective partial structures and the techniques may improve cache performance by increasing the probability of partial structures remaining in the corresponding one of caches . In some instances a dedicated task manages the associative data structure for execution tasks.

In the illustrated example cores communicate to exchange data messages and tasks via system bus . In addition cores interface to system memory including in memory relational database via memory bus . Virtual processors separately execute tasks in parallel to perform the various external and internal functionality of accelerator . Tasks may be pointers that resolve to a task structure in system memory that includes a series of steps for execution by virtual processors . Tasks may therefore be uniquely identified by their address in the system memory address space. Each of tasks executes substantially independently of every other one of tasks . While tasks may exchange data with other tasks spawn additional tasks and be spawned from other tasks each of tasks self determines the one of cores that is to execute the task. There is no supervisory task or process to specify a core location for tasks . This heterarchy of cooperating tasks are thus self directed and self organizing substantially reducing the number of cores cycles devoted to task management consistency checking and other administrative functions.

Tasks may migrate among virtual processors and spawn additional tasks to execute on other virtual processors . In the illustrated example task A executing on virtual processor A spawns task B to execute on virtual processor N by sending message to virtual processor N. Message may specify a series of steps determined by task A for the spawned task B. Message may alternatively specify a pointer that resolves to a task structure in system memory that includes a series of steps to execute as task B. In addition task A subsequently copies itself to execute on virtual processor B by sending message to virtual processor B. Message may specify a series of steps that represents a remainder of task A requiring execution or a pointer that resolves to a task structure in system memory that includes at least a remaining series of steps for execution by virtual processor B.

Run list A of virtual processor A stores a list of tasks currently enqueued for execution by the virtual processor. In the illustrated example run list A is a circular queue that stores memory pointers that resolve to respective task structures in the memory space for virtual processor A. Run list A like other data structures that support virtual processor A may be stored in cache A and or in main memory. Scheduler A iteratively invokes tasks in run list A. Scheduler A performs time division multiplexing with variable time divisions that depend upon instructions in tasks. Scheduler A may spawn separate threads to each execute one task in run list A. Alternatively scheduler A may use a single worker thread for run list A. Besides threads for executing run list A scheduler A may use additional threads to perform specialized tasks. Scheduler A invokes a task of run list A to execute for a time division then invokes a next task of A. Because run list A is a circular queue scheduler A iteratively executes the tasks of the run list from the list head to the list tail then upon completing executing at least a portion of the task at the list tail again executes the task at the list head.

Tasks migrate among virtual processors such that a task initially executing on one of virtual processors may later execute on another virtual processor. In addition a task executing on one of virtual processors may spawn a new task for execution on another of the virtual processors. In the illustrated example task migrates from virtual processor A to virtual processor B by adding a memory pointer for itself to cross queue B in message that comprises in this example a memory write operation. Run lists of virtual processors may be accessed at any time and with the exception of operations involving heartbeat tasks virtual processors run independently in parallel and do not synchronize their execution of tasks. In some instances virtual processors A B may execute on separate cluster machines. As a result neither of virtual processors A B may access the physical memory space of the other. In such instances message may include a network based message such as a socket write or a cross bar backplane or other switch message for example.

Schedulers may migrate tasks among virtual processors due to a NUMA architecture of virtual processor with virtual processors executing on cores that have non uniform memory access times to caches . In this way schedulers may provide NUMA aware scheduling to reduce overall latency for memory accesses and thereby further improve performance.

To prevent corruption of run lists due to an asynchronous addition of a new task virtual processors include respective cross queues that temporarily store zero or more new tasks for addition to run lists . In operation task executing on virtual processor A determines that it may operate more efficiently on virtual processor B and migrates itself to virtual processor B by locking cross queue B and pushing a memory pointer for task to the cross queue. To spawn a new task on virtual processor B task executing on virtual processor A may create a new task data structure in memory and then push a memory pointer to the new task data structure to cross queue B. Scheduler B runs within virtual processor B to pop the head task of cross queue B and insert the popped task on run list B. By utilizing cross queues in this manner virtual processors may avoid locking respective run lists to read write the run lists yet avoid collisions due to concurrent task execution and migration spawning by separate virtual processors executing in parallel. In some instances to reduce the possibility of collisions with respect to cross queues A virtual processor A may include multiple cross queues e.g. one cross queue per virtual processor in the system.

In some instances task may migrate to virtual processor B because an assignment algorithm executed by the task determines task requires access to an object in partial structure B of cache B associated with virtual processor B. As described above with respect to partial structures store a subset of data for an overall database object for the in memory relational database . In some instances partial structures may represent partial structures alternately or additionally stored to main memory. To avoid locking partial structures during access by tasks executing on virtual processors access to respective partial structures may be limited to tasks executing on the one of virtual processors associated with the partial structure. Task must therefore operate on virtual processor B to access partial structure B. This constraint ensures that access to partial structures by tasks is safe and reliable even though the tasks eschew in many circumstances locking the partial structures and even though multiple different tasks may share the overall database object. Moreover multiple tasks executing on different virtual processors may access the overall database object concurrently by separately accessing different partial structures that together constitute the database object. When however task is unable to complete access to one of partial resources in its allotted time division task may lock the partial resource to ensure the partial resource data remains stable and consistent for the task until its next time division. Alternatively task may lock only an item stored by one of partial structures rather than the full structure. In this way a subsequent task may modify any non locked items of the partial structure.

In some cases tasks of run list A may require resources not immediately available or otherwise be awaiting satisfaction of a dependency in order to continue execution. To avoid congesting the associated core executing virtual processor A such tasks may sleep by adding themselves to sleep list A along with an associated wake up time. Sleep list A stores sleeping tasks ordered by wake up time in an ordered data structure such as a queue table linked list or tree data structure. Each node in sleep list A is thus a memory pointer to a task structure for a sleeping task.

An alarm task and a hardware timer for virtual processor A manage sleeping tasks in sleep list A. The alarm task programs the hardware timer with an awaken time value for the earliest task in sleep list A. When the hardware timer fires the alarm task triggers and adds the earliest task in sleep list A to run list A. In some instances the alarm task modifies run list A to ensure that scheduler A invokes the earliest task next among the tasks within the run list. The alarm task then reprograms the hardware timer with an awaken time value for the next earliest task according to sleep list A. The hardware timer may be driven with a CPU clock having a rate exceeding 1 GHz and thus has sub microsecond periodicity. As a result the alarm task in conjunction with the hardware timer may achieve fine grained task sleep management and virtual processor operation behavior and may thus enhance the utilization of resources by ensuring that tasks awaken and execute within a short latency after their associated awaken time value.

Virtual processors execute respective heartbeat tasks A N heartbeat tasks at a pre defined rate to synchronize an operational position of the virtual processors once for every period defined by the heartbeat rate. In some instances the pre defined rate is 1 Hz. For example scheduler A invokes tasks of run list A and once per second execute heartbeat task A. To synchronize the operational position of virtual processors heartbeat tasks may each access and decrement an atomic variable shared among all instance of the virtual processors. The atomic variable may be initialized with a number of virtual processors corresponding to the number of cores in the system . Each of heartbeat tasks test the atomic variable for zero. When the atomic variable is non zero the heartbeat tasks waits for a signal. When the atomic variable reaches zero due the operation of the final heartbeat task for the particular cycle the final heartbeat task may initiate one or more user level tasks or signal each of virtual processors to resume execution of their respective run lists . In this way the final heartbeat task changes the phase of all tasks to the heartbeat i.e. the time of the system wide signal signal of the final heartbeat task. Heartbeat tasks therefore provide a time window in which the state of every virtual processor is known. Tasks may leverage this time window to perform system wide operations.

For example tasks may set a task hook e.g. a memory pointer to a task structure in memory within each of heartbeat tasks . Upon receiving a signal from the final heartbeat task for a cycle each of the heartbeat tasks waiting on the signal begins executing and executing the hooked task. The hooked task when thus simultaneously executed by each of virtual processors provides a system wide operation. In some cases the final heartbeat task for the cycle alone executes the hooked task. This technique may be useful for scaling out memory performing database checkpoint write and read operations or other periodic tasks such as database journaling logging and archiving. Tasks may wait and signal one another using monitors shared memory or semaphores for example.

Some tasks within run lists do not related to data within caches or another memory partition and therefore may run on any of virtual processors . Such tasks may include a flag in the task structure that indicate the task is moveable. Tasks may self modify the flag after each step to indicate whether a next step for the task must be executed on a particular one of virtual processors .

To improve utilization and reduce congestion of virtual processors tasks self balance to more equally distribute a number of tasks for each of run lists and thus for each of virtual processors . In some instances after performing each step of a task a task determines the length of respective run list and lengths of neighboring run lists . For example a task may determine a length of i.e. a number of tasks stored by run list B and lengths of run lists A C after executing a step of a task. If the task determines the length of run list B exceeds the length of either of run lists A C by a threshold value the task migrates itself if moveable to the shorter of run lists A C. In some instances tasks account for even more remote neighbors that is not just nearest neighbors when performing rebalancing. In this way tasks autonomously self organize in a balanced manner by migrating themselves toward lightly loaded virtual processors and corresponding cores . Tasks may determine lengths of neighboring runs lists by exchange the lengths in message or reading a shared memory value for example.

Steps of tasks may include variable numbers and types of instructions and thus have different execution lengths. In other words the time required to execute each of the steps of tasks can differ from step to step. Steps of tasks execute atomically i.e. from the first instruction of the step to the last instruction of the step without interruption. After completing a step of one of tasks in run list A scheduler A invokes the next step for the next one of tasks in the run list. In this way scheduler A invoking different ones of tasks performs time division multiplexing by step slicing the tasks. That is in contradistinction to time slicing tasks such that each task is provided a short period of time by the kernel during which the task may execute until preempted each of tasks continues executing until the task has completed a step. Step slicing thus ensures the atomicity of the steps of tasks .

Each of tasks maintains a memory pointer step index or other reference to the next step for execution in the associated task. When scheduler A invokes a task the task executes the next step and then sleeps to return control to scheduler A which invokes the next task in run list A. For example task K executes step Cof task K and then returns control to scheduler A which invokes task A. The task A then executes step A. In some instances a single execution thread executes each of tasks using step slicing techniques described above. The single execution thread may nevertheless sleep after each step or after executing a step for tail task K of the run list for example to allow threads for non run list tasks to execute.

Fine grained schedulers enable virtual processors to execute multiple transactions of varying complexity and duration. In general transactions may be characterized as modifying a database e.g. SQL INSERT DELETE and UPDATE statements or as querying the database e.g. an SQL SELECT statement . These transactions may be further characterized according to their execution duration. For example a transaction that updates a single row may be considered a short running transaction while a transaction that queries the entire database and or performs complex extended calculations may be considered a long running transaction. As a still further example a query transaction based on SELECT FUTURE described below in further detail may be considered a perpetually or continuously running transaction. Schedulers may permit interleaving the execution by virtual processors of various combinations of short long and continuously running transactions. In combination with the ability to scale out beyond physical in memory boundaries and to an arbitrary number of cores the techniques may support rich and complex queries in workload mixes that include transactions of varying execution duration particularly in the context of large numbers of transactions received from a large numbers of client connections.

Network tasks support interfacing with clients and additionally enable communication among multiple cluster machines that cooperate to implement one or more accelerator . In this example sockets are the principal communication interface among cluster machines and between a transactional data accelerator and one or more clients. An instance of socket answer task A executing on standalone machine or a cluster machine listens for socket connection requests issued by clients to the system. Upon receiving a socket connection request the socket answer task A spawns new instances of socket reader task B and socket writer task C specific to the socket connection request. The new socket reader task B and socket writer task C cooperate to complete the socket connection handshake and establish a socket connection. The new socket reader task B listens for service requests from the corresponding client. In this way individual tasks that may be executed in parallel by multiple cores implement multiple parallel connection points with the system. The techniques may therefore enable a single system to handle hundreds of thousands of concurrent connections.

Asynchronous completion task D supports socket reader tasks B and socket writer tasks C by enabling asynchronous socket send and receive operations and facilitating high performance client request response input output I O . A system may spawn a new asynchronous completion task D for each socket connection. Cluster machines may interface with one another using network tasks . Host interconnect task F manages socket connections among virtual processor instances on two or more cluster machines of a cluster. An instance of host interconnect task F executes on each of the cluster machines to establish socket connections between virtual processor instances. Host interconnect task F may for example create a full mesh of continuously connected sockets among all virtual processors of the clusters that reside on separate cluster machines. Alternatively host interconnect task F may establish connections between such virtual processors as needed to execute client requests and facilitate system efficiency. To establish a new socket connection host interconnect task F in this example spawns a new instance of socket connect task E for the new socket connection which in turn spawns new instances of socket writer task C and socket reader task B.

Statement execution tasks include tasks that represent example embodiments of tasks described above with respect to . Specifically compiler task A statement execution task B and subordinate execution task may represent example embodiments of compiler task execution task and any of subordinate execution tasks respectfully.

System boot task A initializes a system according to configurable parameters and manages loading at least a portion of in memory relational database from persistent storage. System shutdown task B stores system data including data configured during operation of the system to persistent storage for later restoration. In addition system shutdown task B may manage writing at least a portion of in memory relational database to persistent storage.

Periodic statement task A may be configured to periodically execute an operation. For example an instance of periodic statement task A may be configured to periodically delete from a record table previously executed statements saved to facilitate ACID compliance. This example is a form of housekeeping that streamlines the system by removing superfluous data. User defined task B may be configured with user instructions to execute custom user applications with respect to in memory relational database . In this way customers have access to the internal execution model of the system and techniques of this disclosure provide a highly extensible system to which customers can add custom tasks. The tasking model disclosed herein enable customers and developers to incrementally increase the sophistication of the system by simply adding additional tasks.

Parallelization engine provides a range of internal services. This includes session management transaction management schema control parallelized containers locking parsing error management and dynamic machine code generation. These may be accessed by a toolkit or other application programming interface API to modify the operation of parallelization engine .

Housekeeping tasks administer resources and administer the system. Garbage collector task D performs garbage collection to reclaim memory occupied by objects that are no longer referenced by any process within the system. Garbage collection task D is responsible for finally removing row field data from structures e.g. indices of in memory relational database and reclaiming the memory. Statement execution task B logically removes a row from the database in response to incoming delete statements. However once a row has been marked as logically deleted statement execution task B inserts a pointer to the deleted row into a list of rows to be removed reclaimed by garbage collector task D. A garbage collector task s D applies the assignment algorithm to each row for each index of in memory relational database that references the row. Garbage collector task s D remove the row from each index it is in and then deletes the row structure thereby reclaiming the memory the row occupied.

Heartbeat task B and alarm task E may represent an example embodiment of heartbeat tasks of . Instances of alarm task E each manage a hardware timer and a sleep list of a virtual processor in accordance with techniques described with respect to . License task C ensures the system is operating with a valid license. Statistics task F measures performance and other metrics of the system and communicates the statistics via an instance of socket writer task C to a management entity. For example an instance of statistics task F may time steps executed by threads monitor the number of tasks in the system monitor client request throughput or response time and monitor a client request arrival rate. Monitor Task A periodically checks the status of all other tasks in the system to report errors warnings and to facilitate error correction handling.

Disk tasks provide durability compliance for in memory relational database . Journal writer task A writes state for executed statements to transaction journal a computer readable storage device. Upon a transaction failure or other database related operational failure journal reader task C reads the written state for the previously executed statements and journal restore task B restores if necessary the state to memory to restore the in memory relational database to a known state. An instance of periodic statement task A may periodically determine obsolete journal entries and delete such entries from transaction journal .

Checkpoint related tasks persist and restore portions of in memory relational database to from system checkpoint a computer readable storage device. Checkpoint take task D determines a portion of memory to write to disk and directs checkpoint writer task E to write the portion as a checkpoint to disk. On the event of a database related failure checkpoint restore task F determines one or portions of memory to restore from previously written checkpoints and directs checkpoint reader task G to read the checkpoints and reinsert the checkpoint data to appropriate memory locations. This enables journal reader task C to read and restore only those transactions applied after the checkpoint take task D stored the checkpoint to system checkpoint .

Page writer task H and page reader task I page in memory data to secondary storage represented by data store a computer readable storage device to scale out memory utilized by in memory relational database . Page writer task H identifies stale items e.g. rows of database objects within in memory relational database and upon identifying stale items write data for the stale items to data store . In addition page writer task H subsequently deletes the stale items. When a task executing on the system requires access to items written to data store page reader task I reads the items from the data store and inserts the data for the items using transactions to in memory relational database .

Log writer task J logs system operations to system log a computer readable storage device. Archive tasks K identifies journal entries and or checkpoints made obsolete by subsequent checkpoints and writes the data to tertiary storage represented by archive a computer readable storage device.

Client issues to transactional data accelerator a request statement that references both fields on which indices are indexed. For example the request statement may be an INSERT DELETE or UPDATE SQL statement to respectively insert delete or update a row into of the table object on which the indices are based. As another example the request statement may be a SELECT SQL statement to acquire all rows matching criteria that reference both fields on which indices are indexed. Thus the techniques may permit parallelization of many different types of declarative language e.g. SQL operations for not only querying but also for modifying an in memory database.

Statement execution task receives via a socket connect and a compiler task neither shown in transactions that form an execution plan to execute the request statement from client . Transactions include a first sub transaction for index and a second sub transaction for index . Statement execution task spawns subordinate execution tasks to execute the first and second sub transactions of transactions .

For example in the case of the INSERT SQL statement example above statement execution task first creates and adds new row to the table object according to row data received in the request statement. Statement execution task then performs an assignment algorithm using the field value of the row for the field on which index is based and based on the output of assignment algorithm assigns the field value to core B. Statement execution task spawns subordinate execution task to core B and directs the spawned task to insert an index row for the new data to partial index B. Subordinate execution task adds the index row to partial index B with a memory pointer to the new row added statement execution task to the table object.

In addition statement execution task performs an assignment algorithm using the field value of the row for the field on which index is based and based on the output of assignment algorithm assigns the field value to core D. Statement execution task spawns subordinate execution task to core D and directs the spawned task to insert an index row for the new data to partial index D. Subordinate execution task adds the index row to partial index D with a memory pointer to the new row added statement execution task to the table object. In this way subordinate execution tasks may execute concurrently and insertion of new index rows to indices may occur in parallel rather than serially. In some instances transactions may include sub transactions that each cause tasks to write to partial indices . For example transactions may include sub transactions to write to respective partial indices B D. Nevertheless subordinate execution tasks may execute concurrently to simultaneously modify partial indices B D for the same database object i.e. index . Subordinate execution tasks returns data and or status information to statement execution task which returns a result to client via a socket connect task not shown in .

Client issues to transactional data accelerator a request statement that relates to data for data object . Statement execution task receives via a compiler task not shown transaction that forms an execution plan to execute the request statement from client . Statement execution task performs a clustered assignment algorithm such as a cluster hash function using the data for transaction . The clustered assignment algorithm outputs two indices a first index in the machine dimension and a second index in the core dimension. Statement execution task in this way uses the clustered assignment algorithm to deterministically identify an appropriate core of machines to execute transaction .

In the illustrated example the clustered assignment algorithm outputs machine index and core to indicate a task operating on core Bshould execute transaction . Because statement execution task executes on machine A which is not the same as machine B for core B statement execution task establishes a socket connection between machines A B via respective socket connect tasks A B. Statement execution task then spawns subordinate execution task using socket connect tasks and subordinate execution task executes transaction on partial structure Bassociated with core B. In some cases subordinate execution task may return a transaction result to statement execution task via socket connect tasks . Statement execution task may spawn subordinate execution task on machine B by for example serializing and sending the steps of the task via socket connect tasks . Socket connect tasks thus act in this instance as a proxy for statement execution task .

In the example of system includes a plurality of transactional data accelerators arranged in a three tiered structure having core tier fan out tier and edge tier . Data consumers which may be on the order of millions device each establish a unique query with transactional data accelerators of edge tier . In turn transactional data accelerators of edge tier each establish queries with transactional data accelerators of fan out tier . That is data ingest engines within transactional data accelerators of edge tier establish connections with content delivery engines of transactional data accelerators within fan out tier and provide aggregate queries to the content delivery engines where aggregate queries are each an example of an aggregate transaction. That is each content delivery engine within transactional data accelerators edge tier computes an aggregate query that represents all of data specified the client specific queries received from data consumers . In other words the aggregate query computed by each content delivery engine within fan out tier specifies a plurality of condition sets that correspond to the condition specified by data consumers with which the content deliver engine has established connections.

In one example embodiment database acceleration engine of each accelerator presents SQL based API that has been enhanced to allow data consumers to easily specify continuous queries. For example in one embodiment the SQL based API supports an optional future token to be included within any issued select statement to indicate that the query defined by the select statement is to be continuously applied to new not yet received data. For example a first data consumer may issue a query as follows 

In turn data ingest engines of transactional data accelerators of fan out tier establish connections and provide aggregate queries to content delivery engines of accelerator within core tier . Core tier represents a cluster of one or more transactional data accelerators that operate on a stream of transaction data as described above from one or more sources. If a data change occurs the updated data is automatically pushed from core tier to those data consumers for which the updated data matches the conditions defined by the client s query . At each of tier content delivery engines fan the data out to the data ingest engines for population of the massively parallel in memory database as described herein until the updated is pushed to data consumers . The parallelization techniques described herein allow this process to be extremely fast. For example millions of data consumers can be supported using the example three tier structure of system such that continuously changing data within core tier can be pushed to data consumers on the order of approximately one millisecond. This allows for processed data within core tier to be pushed delivered from in memory database as a targeted data stream to data consumers . Although described with respect to three tiers other tiers may be used. For example example with four tiers data could be pushed to billions of data consumers in a cost effective and timely manner.

In the example of system includes a plurality of transactional data accelerators arranged in a three tiered structure having core tier fan in tier and edge tier . Data sources which may be on the order of millions of devices each establish connections with and send transactional data to transactional data accelerators of edge tier . In turn transactional data accelerators of edge tier each establish connections with and send data to transactional data accelerators of fan in tier . That is content delivery engines within transactional data accelerators of edge tier establish connections with data ingest engines of accelerator within fan in tier and provide data to the data ingest engines . In turn content delivery engines of transactional data accelerators of fan in tier establish connections and provide data to data ingest engines of transactional data accelerators within core tier . Core tier represents a cluster of one or more transactional data accelerators that operate on transaction data as described above. Core tier may process and output the received data to data warehouses or interested clients using private broadcast channels as describe with respect to system of .

In this example data consumers respond to broadcast data delivered by transactional data accelerators . Responses to broadcast data from data consumers represent data for data sources which is ingested by transactional data accelerators processed and used to deliver refined aggregated or otherwise processed data to data consumers . While the operations of transactional data accelerators may be substantially similar in both system of and system of system involves a feedback loop of real time or near real time broadcast data and broadcast data responses and thus illustrates that the techniques of this disclosure as implemented by transactional data accelerators may apply to accelerate the interactive delivery of user enabled query results and other interactive applications.

The techniques described in this disclosure may be implemented at least in part in hardware software firmware or any combination thereof. For example various aspects of the described techniques may be implemented within one or more processors including one or more microprocessors digital signal processors DSPs application specific integrated circuits ASICs field programmable gate arrays FPGAs or any other equivalent integrated or discrete logic circuitry as well as any combinations of such components. The term processor or processing circuitry may generally refer to any of the foregoing logic circuitry alone or in combination with other logic circuitry or any other equivalent circuitry. A control unit comprising hardware may also perform one or more of the techniques of this disclosure.

Such hardware software and firmware may be implemented within the same device or within separate devices to support the various operations and functions described in this disclosure. In addition any of the described units modules or components may be implemented together or separately as discrete but interoperable logic devices. Depiction of different features as modules or units is intended to highlight different functional aspects and does not necessarily imply that such modules or units must be realized by separate hardware or software components. Rather functionality associated with one or more modules or units may be performed by separate hardware or software components or integrated within common or separate hardware or software components.

The techniques described in this disclosure may also be embodied or encoded in a computer readable medium such as a non transitory computer readable medium or computer readable storage medium or device containing instructions. Instructions embedded or encoded in a computer readable medium may cause a programmable processor or other processor to perform the method e.g. when the instructions are executed. Computer readable storage media may include random access memory RAM read only memory ROM programmable read only memory PROM erasable programmable read only memory EPROM electronically erasable programmable read only memory EEPROM flash memory a hard disk a CD ROM a floppy disk a cassette magnetic media optical media or other computer readable storage media. It should be understood that the term computer readable storage media refers to physical storage media and not signals or carrier waves although the term computer readable media may include transient media such as signals in addition to physical storage media.

Various embodiments of the invention have been described. These and other embodiments are within the scope of the following claims.

