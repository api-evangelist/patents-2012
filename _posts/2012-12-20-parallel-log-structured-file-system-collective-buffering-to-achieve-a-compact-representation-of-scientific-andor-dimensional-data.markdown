---

title: Parallel log structured file system collective buffering to achieve a compact representation of scientific and/or dimensional data
abstract: Collective buffering and data pattern solutions are provided for storage, retrieval, and/or analysis of data in a collective parallel processing environment. For example, a method can be provided for data storage in a collective parallel processing environment. The method comprises receiving data to be written for a plurality of collective processes within a collective parallel processing environment, extracting a data pattern for the data to be written for the plurality of collective processes, generating a representation describing the data pattern, and saving the data and the representation.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09122700&OS=09122700&RS=09122700
owner: Los Alamos National Security, LLC
number: 09122700
owner_city: Los Alamos
owner_country: US
publication_date: 20121220
---
This application claims the benefit of U.S. Provisional Application No. 61 578 117 which was filed on Dec. 20 2011 and is incorporated herein by reference in its entirety.

This invention was made with government support under Contract No. DEAC52 06NA25396 awarded by the U.S. Department of Energy. The government has certain rights in the invention.

Current techniques for capturing representations of scientific data as it is stored to stable storage requires scientific applications to use specific application programming interfaces APIs such as SQL NetCDF HDF5 and SciDB. These APIs each store their data in their own format. Additionally in a large scale parallel setting each of these techniques often requires a large percentage of the data to be shuffled around before storing the data to make it efficient for the underlying storage.

The Parallel Log structured File System PLFS was created to break the need for scientific parallel applications to have to align their data for enabling efficient use of the underlying storage system PLFS breaks the need for these science applications to be concerned with alignment locking issues. However this insulation from the particulars of the underlying storage system comes at the cost of storing substantial mapping metadata along with the data.

Therefore there exists ample opportunity for improvement in technologies related to data storage in a collective parallel processing environment.

A variety of technologies related to data storage retrieval and or analysis in a collective parallel processing environment are provided. For example the technologies can be used in a high performance computing HPC environment that uses a parallel file system.

In the technologies described herein various data pattern techniques and solutions and or smart collective buffering techniques and solutions are applied. For example the techniques and solutions described herein can include one or more of the following 

A method can be provided for data storage in a collective parallel processing environment. The method comprises receiving data to be written for a plurality of collective processes within a collective parallel processing environment extracting a data pattern for the data to be written for the plurality of collective processes generating a representation describing the data pattern and saving the data and the analytics enabling representation.

Features and advantages will become apparent from the following detailed description which proceeds with reference to the accompanying figures.

As described herein various data pattern techniques and solutions and or smart collective buffering techniques and solutions are applied. For example the techniques and solutions can be applied to a parallel file system such as the Parallel Log structured File System. The techniques and solutions described herein can be applied to high performance computing HPC and collective parallel processing in relation to storage retrieval and or analysis of data such as dimensional data scientific data and or extreme scale databases.

In some implementations data can be stored and or accessed for multiple collective processes e.g. in a collective parallel processing environment . For example data can be stored or accessed for a plurality of collective processes using a parallel file system. Techniques and solutions can be applied for creating and storing representations of data e.g. in a compact format and analytical methods and derived metadata associated with that data and representation that are generated based on data patterns. Techniques and solutions can be applied for smart collective buffering which can for example move data efficiently between processes e.g. neighboring collective processes to create structure or patterns which can then be represented in a compact manner.

In some embodiments the Parallel Log structured File System as described in Bent et al. PLFS a checkpoint filesystem for parallel applications SC 09 . ACM New York N.Y. USA 12 pages 2009 is extended to implement data pattern techniques and solutions and or smart collective buffering techniques and solutions described herein. This PLFS approach provides the ability to capture a compact representation e.g. similar in concept to a database schema . This approach combines the ability to capture the compact representation in a way that is agnostic to the application programming interface API e.g. any API can easily be accommodated and also in a way that is agnostic to the actual format of the data being stored. Additionally this approach accomplishes the above in a way that drastically reduces the amount of data required to be shuffled around for efficient use of the underlying storage.

In a specific embodiment the PLFS smart collective buffering data pattern approach is implemented as follows a PLFS abstract I O interface ADIO layer is written for Message Passing interface parallel I O MPI IO e.g. which can be the ROMIO MPI IO implementation provided by Argonne National Laboratory and as included in the MPICH2 implementation of the Message Passing Interface MPI standard developed by Argonne National Laboratory which enables PLFS to handle collective write read requests to underlying storage. In this layer collective operations are done to determine lengths and offsets of requested data writes and reads . Logic is inserted into this collective read write code path to capture information about repeating patterns in the data requests and convert those patterns to a compact formula that is then passed to PLFS to use as a map of where data is located. Additionally the collective data operations can be optimized for PLFS so that minimal data is needed to be shuffled between parallel processes to make for an efficient compact representation to be utilized.

5 a way to automatically subdivide large amounts of data e.g. scientific and or dimensional data that allows for easy parallel analysis

As used herein the term collective buffering sometimes referred to as smart collective buffering refers to the various collective buffering and or data pattern techniques and solutions described herein e.g. efficient shuffling or moving of data between collective processes or compute nodes creating structured data patterns recognizing data patterns creating compact representations and or index information etc. . The collective buffering techniques and solutions described herein can be applied to parallel file system technology including PLFS. As should be understood the collective buffering or collective buffering and or data pattern techniques and solutions described herein are different from MPI IO collective buffering that uses a different method for moving data between processes and moves data for a different purpose e.g. to align to the underlying file storage system .

Other high performance computing parallel run time environments such as Partitioned Global Address Space PGAS and the associated PGAS languages such as Unified Parallel C UPC Co Array Fortran CAF X10 and Chapel also are used to express data structure to a parallel computing system and can provide this structure to enable pattern discovery and all the benefits mentioned above.

For decades the performance of database management systems has greatly benefited from two strategies 1 decoupling logical data models from physical data representation and 2 sharing structure of data between applications and database management systems via compact descriptions called schemas. Yet in file based storage systems neither strategy has been adopted. File storage systems implement a byte stream data model that tightly maps to physical data structures making performance highly sensitive to access patterns and have made it difficult to share the structure of data between applications and storage systems.

The Parallel Log structured File System PLFS provides an interposition layer that works with any standard file system that implements the first strategy it decouples logical access patterns from physical access patterns with the result of up to two orders of magnitude performance improvement for several example HPC applications. PLFS allows applications to access data in virtual files that it implements as data in the form of directories of write optimized logs and indices. These indices are used by PLFS to provide read access to virtual files. The size of these indices is proportional to the number of processes writing to a virtual file. For the billions of processes expected in exascale HPC systems these indices become large which can make scaling PLFS read access difficult.

Collective buffering and or data pattern techniques can be applied to file system technology e.g. parallel file system technology to provide more efficient storage and retrieval of data.

In some embodiments PLFS collective buffering and data pattern techniques are applied to the second strategy for storage systems discussed in the above section sharing compact descriptions of the structure of data among applications and file systems that can be called fs schemas. These fs schemas can replace PLFS indices and therefore make PLFS read access scalable to exascale and beyond they can significantly reduce the communication overhead for coordinating collective I O and they allow for transformation and analysis of the data on all levels of the memory hierarchy because the structure of data is well known throughout.

In a specific embodiment MPI IO collective write functions can be extended to capture write patterns on the fly and those patterns can be used to build an index that represents the write data placement. The patterns can be used to build an index that represents the data write placement that is orders of magnitude smaller than the list style index used by PLFS. Furthermore data shuffling can be performed in a more efficient way than used by MPI IO due to the freedom PLFS provides by removing the need for alignment to the underlying storage system. PLFS can then be extended to perform collective buffering different in data movement than the collective buffering performed by MPI IO including using the data patterns to perform data shuffling. The result of these techniques can be a compact representation of extremely large amounts of data derived on the fly and used for looking up where data is located in the write pattern. In addition these techniques can be used to simplify analysis functions using the stored data.

In the techniques and solutions described herein methods systems and environments can be provided data storage in collective parallel processing environments using collective buffering and data pattern techniques.

At a data pattern is extracted discovered from the data e.g. before or after the data is received to be written . For example the data pattern can be based upon structured data to be written e.g. each collective process may be operating on the same size of data such as the same number of floating point values or the same quantity of a complex data structure . In some implementations the data pattern is discovered before any actual data is received from or sent by the collective processes. For example a data write operation can include a communication phase where the pattern or lack thereof can be discovered. Once the pattern is discovered or extracted it can be used to determine where data can be moved to create an efficient pattern e.g. by shuffling or data between neighboring collective processes .

In some situations the data to be written may be unstructured e.g. the collective processes may be operating on different amounts of data . In such situations the data to be written may be moved between neighboring collective processes and or neighboring compute nodes in order to create a pattern e.g. to create structured data . Once the data has been moved the pattern can be extracted .

At a representation of the data pattern is generated e.g. an originally extracted or discovered data pattern or a pattern after the data has been shuffled or restructured . For example the representation can comprise information describing the number of processes the offset in the files being written for the current write the total amount of data being written the amount of data being written per process or per compute node and or the number of compute nodes etc. The representation can also comprise information describing the structure e.g. complex structure of the data. For example the data structure may be a distributed array of 1 000 floating point values across 10 collective processes with each process operating on 100 floating point values from the array .

The representation can be a single index entry e.g. a single PLFS index entry describing the data being written and stored for all the collective processes e.g. millions or billions of collective processes . By recognizing the pattern in structured data or shuffling moving data to create structured data or a data pattern a compact representation can be created e.g. separate index information or meta data does not need to be created for the data written saved for each process and or each compute node . For example if there are a million collective processes each writing their own data a compact representation e.g. a one dimensional index entry can be used with the collective buffering and data pattern techniques described herein instead of for example a million separate index entries for a PLFS implementation that does not use the collective buffering and data pattern techniques described herein.

At the data and representation are saved. For example the data and representation can be saved using PLFS or another type of parallel file system. Once saved reading or analysis of the saved data can be performed efficiently by reading the compact index information in the saved representation.

The environment includes an intermediate layer implementing any of the collective buffering and or data pattern techniques described herein. For example the intermediate layer can analyze data to be written by the collective processes e.g. to determine whether the data is structured or unstructured move or shuffle data between the collective processes e.g. between neighboring collective processes and or between neighboring compute nodes extract data patterns from the data to be written generate representations of the data based on the pattern e.g. generate compact index information such as a single index entry and store the data and representation using data storage such as the parallel file system .

This section describes various concepts involved with high performance computing HPC and collective parallel processing in relation to storage retrieval and or analysis of data. Some of the concepts involve data structures used with HPC applications jitter structured and unstructured data MPI I O collective buffering and standard PLFS.

Imagine we are simulating a cubic meter. We could create a structured distribution of 1M one million cubic centimeters and then assign one process for each centimeter. The processes simulate their centimeter and then they exchange data. They repeat this multiple times per second. The processes also checkpoint their data at a frequency of once per hour for example. The data exchange and the checkpoints are necessarily synchronous in this example but the compute is not.

A first version of this example is illustrated in simplified format in . In at ten processes P through P are depicted with their corresponding compute exchange and checkpoint operations. As depicted at when the processes are nicely synchronized compute exchange and checkpoint operations can be performed efficiently.

A second version of this example is illustrated in simplified format in . In at ten processes P through P are depicted with their corresponding compute exchange and checkpoint operations. As depicted at when the processes are not synchronized compute exchange and checkpoint operations are inefficient. This effect can be called jitter. For example with reference to in the first compute phase P is very slow. The other processes are faster and have to wait until P is done in order to do the exchange phase. In the second compute phase P is the slow one. All of the X s indicate lost cycles where a process was occupying an expensive processor core but not using it.

Remember the example cubic meter that is split into 1M one million cubic centimeters distributed to 1M cores. Imagine that there is a lot of activity within some cubic centimeters dense subcubes and not a lot within others sparse subcubes . The cores operating on dense subcubes are slow getting to the exchange phase. The cores operating on sparse subcubes are fast getting to the exchange phase. For example jitter can causes 30 to 50 lost utilization on a 200M supercomputer.

One way to avoid or reduce jitter is to make the subcubes uniformly populated with activity and therefore irregularly sized . Processing time per subcube is thus uniform or more uniform processes arrive simultaneously or nearly simultaneously at exchanges and jitter is reduced.

Using unstructured data reduces jitter but makes checkpointing harder. With unstructured data subcubes are irregularly sized. Writing them in N 1 strided is very difficult. N N would be good but N N doesn t match the compute model very well. This introduces big headaches for archive N M restart visualization and analysis. N 1 segmented would be ok but also doesn t match the compute model very well and also introduces big headaches for N M restart visualization and analysis. N 1 strided is very hard with unstructured data because the writes are not uniformly sized over space or time.

In the future memory per process looks like it will go down. Memory may become the most precious part of the system which may mean that all codes will be structured. Therefore an I O approach may be needed that works well at immense scale one million to one billion processes for both unstructured Adaptive Mesh Refinement AMR and structured data.

This section describes various techniques for applying data pattern solutions and or collective buffering solutions to PLFS. The techniques described below can be applied to high performance computing HPC and collective parallel processing in relation to storage retrieval and or analysis of data. The techniques described below can be used separately or in combination e.g. in combination with the techniques described in this section and or in combination with techniques described elsewhere herein .

For example the techniques described below can be used if the regular PLFS index is too large to efficiently read distribute. For example PLFS collective buffering techniques described below can be applied to reduce the regular PLFS index to a very small fraction of its regular size. In addition the techniques described below can be applied to optimize reads and facilitate analysis.

MPI IO collective buffering reflects one approach to try and improve system performance when writing unstructured data structures. With MPI IO collective buffering a MPI file write all call provides the offset and length for every proc. It is a synchronous collective call. MPI IO has a static collective buffer size which was set at MPI File open . The user sets the buffer size to a pre determined file system magic number e.g. 64K . There is also a pre determined number of aggregator procs subset of all procs . The non aggregators send data to aggregators. Aggregators hopefully get a magic data size and do a fast write. However a lot of data is potentially moved from non aggregator to aggregators

In the diagram at open time MPI IO establish a static buffer size and the number of aggregators and uses the aggregators for each subsequent write. Assume size is 64K and we are using five aggregators P P P P and P . As depicted in the diagram data will be transferred between processes in order to achieve the 64K blocks for writing. For example the aggregator process P will get a 64K block 47K from P 10K from P i.e. P transfers 10K to P and 7K from P i.e. P transfers 7K to P . The data ranges of the blocks for each 64K write block are depicted in . After all of the data has been exchanged seven 64K write blocks and one 21K write block are created. As a result of the exchange MPI IO collective buffering can achieve good file system performance but at the cost of data transfer as more than 70 of data was transferred between processes in this example.

Because all writes are the same size 47K only one PLFS index entry needs to be created for the entire file. In a specific implementation the following format is used for the PLFS index entry 

Using the example depicted at and the example PLFS index entry format above the single PLFS index entry for the data write would be 

Using this data pattern solution e.g. using the PLFS index entry format above can provide significant savings over using one index entry per write. For example a single PLFS index entry could be used instead of hundreds of thousands of individual index entries or even millions of index entries or more .

The solution depicted at uses only one index entry. PLFS collective writing provides advantages over previous MPI IO collective buffering. For example PLFS collective writing transfers less data than MPI IO collective buffering can use more write streams and does not require knowledge of file system magic numbers. Data is rearranged not to have proper offsets in a single file using collective buffer sizes that make N to 1 work well on the file system. Instead it is rearranged to make sure that each node writes the same amount of data regardless of offsets which causes balance for the file system and it causes PLFS indexing using data patterns to cost almost nothing for writing or reading.

In a specific implementation the following format is used for the PLFS index entry this format is similar to the one described above in the PLFS Collective Writing structured subsection but accounts for a number of nodes writing the data 

Using the example depicted at and the example PLFS index entry format above the single PLFS index entry for the data write by the two nodes would be 

In this example situation depicted at only 30 of the data needs to be moved. A simplified representation of the data movement is depicted by the arrows at . Note that data is aligned to balance write so the entire write does not need to be buffered and much of what will be written is local in the proc e.g. writev can be used . Further one sided operation might make it so that nothing needs to be buffered.

Furthermore only one index entry can be created to reflect the entire write. PLFS collective writing transfers less data than MPI IO collective buffering e.g. than the example 70 discussed above with regard to can use more write streams and does not require knowledge of file system FS magic numbers. Data is rearranged not to have proper offsets in a single file using collective buffer sizes that make N to 1 work well on the file system. Instead it is rearranged to make sure that each proc writes the same amount of data regardless of offsets which causes balance for the file system and it causes PLFS indexing using data patterns to cost almost nothing for writing or reading.

In a specific implementation the following format is used for the PLFS index entry this is the same format as described above in the PLFS Collective Writing structured subsection 

Using the example depicted at and the example PLFS index entry format above the single PLFS index entry for the data write would be 

As depicted at almost no data is moved between nodes. PLFS collective writing transfers less data than MPI IO collective buffering can use more write streams and does not require knowledge of file system magic numbers. Data is rearranged not to have proper offsets in a single file using collective buffer sizes that make N to 1 work well on the file system. Instead it is rearranged to make sure that each node writes the same amount of data regardless of offsets which causes balance for the file system and it causes PLFS indexing using data patterns to cost almost nothing for writing or reading.

In a specific implementation the following format is used for the PLFS index entry this is the same format described above in the PLFS Collective Multi Proc Writing structured 

Using the example depicted at and the example PLFS index entry format above the single PLFS index entry for the data write by the two nodes would be 

MPI IO also has mechanism collective buffering two phase IO for this purpose. The MPI IO collective buffering two phase IO mechanism is depicted in at . At open time MPI IO establishes a static buffer size and the number of aggregators and uses these for each subsequent write. Assume the buffer size is 64K and we are using five aggregators P P P P and P . In the example depicted at there are two sets of data a first set and a second set each 469K. MPI IO transfers the data starting with the first set of 469K to the aggregators to create 64K chunks. In the example data movement is depicted using corresponding fill patterns for the data of P P and P for the first set of data . Specifically the data for P P and part of P of the first set moves to the P buffer another part of the data for P moves to the P buffer and a final part of the data for P moves to the P buffer. Data movement is also depicted for the data for P P and P for the second set of data . Specifically the data for P moves to the P buffer the data for P moves to the P buffer and the data for P is split between the P buffer and the P buffer.

As illustrated in the example with MPI IO and the complex type write situation very little of the data is written by the process or node that produced the data. This means that a large percentage of the data for the write has to move over the network thus reducing efficiency.

The example depicted at the data for processes P through P which are associated with node N are written by N while the data for processes P through P which are associated with N are written by N.

As with the other PLFS collective writing solutions described herein only one PLFS index entry needs to be created. In this example the index entry is created using the format as follows 

The example diagram depicts how data is transferred between processes. Specifically data transfer is depicted using corresponding fill patterns for processes P P and P. The data for P remains at P because P is writing exactly 47K. The data for P also remains with P P is only writing 10K per write . The data for P which is larger than 47K is distributed among P P and P. Data for the remaining processes is distributed in a similar manner.

As illustrated in the example diagram a large portion of the data stays local to the process for writing and there is much less data movement than with MPI IO collective buffering e.g. see . In addition PLFS collective writing complex type unstructured can use only one or a few index entries can use more write streams and does not require knowledge of file system magic numbers.

As depicted in the example diagram the processes are writing different amounts of data e.g. P is writing data of length 47 P is writing data of length 10 and so on for a total amount of 469K data written across the ten processes P through P for each of two writes. In this example diagram data is redistributed so that 2 nodes N and N each write half the data. Note that data moved over the network is lower than MPI IO collective buffering and even lower with Multi Proc given that most of the data stays on node. In this example as depicted at almost no data moves over the network. For example data is redistributed so that you preserve lots of data local to the process and you preserve the complex type in the same log file e.g. with all members of the complex type in one log file which helps with read analysis functions 

In a specific implementation the following format is used for the PLFS index entry this is the same format described above in the PLFS Collective Multi Proc Writing structured 

Using the example depicted at and the example PLFS index entry format above the single PLFS index entry for the data write by the two nodes would be 

These techniques for pattern capture and leveraging compact representation have use in other regimes besides HPC scientific computing simulation and in other run times e.g. besides MPI IO . For example 

In an example embodiment a method implemented at least in part by one or more computing devices is provided for writing reading and or analyzing data in a collective parallel processing environment using a parallel file system the method comprising using one or more parallel log structured file system collective buffering and or data pattern technologies described herein to write read and or analyze data.

In another example embodiment a computer readable storage medium storing computer executable instructions is provided for causing a computing device to perform a method for writing reading and or analyzing data in a collective parallel processing environment using a parallel file system the method comprising using one or more parallel log structured file system collective buffering technologies described herein to write read and or analyze data.

This section describes an example of a computing environment suitable for implementing the technologies described herein.

With reference to the computing environment can include at least one central processing unit and memory . In this basic configuration is included within a dashed line. The central processing unit executes computer executable instructions and may be a real or a virtual processor. In a multi processing system multiple processing units execute computer executable instructions to increase processing power and as such multiple processors can be running simultaneously. The memory may be volatile memory e.g. registers cache RAM non volatile memory e.g. ROM EEPROM flash memory etc. or some combination of the two. The memory stores software that can for example implement the technologies described herein. A computing environment may have additional features. For example the computing environment can include storage one or more input devices one or more output devices and one or more communication connections . An interconnection mechanism not shown such as a bus a controller or a network interconnects the components of the computing environment . Typically operating system software not shown provides an operating environment for other software executing in the computing environment and coordinates activities of the components of the computing environment .

The computer readable storage may be removable or non removable media and includes magnetic disks magnetic tapes or cassettes CD ROMs CD RWs DVDs or any other tangible medium which can be used to store information and which can be accessed within the computing environment . The storage stores instructions for the software which can implement technologies described herein.

The input device s may be a touch input device such as a smartphone or tablet screen keyboard keypad mouse pen or trackball a voice input device a scanning device or another device that provides input to the computing environment . For audio the input device s may be a sound card or similar device that accepts audio input in analog or digital form or a CD ROM reader that provides audio samples to the computing environment . The output device s may be a display printer speaker CD writer or another device that provides output from the computing environment .

The communication connection s enable communication over a communication medium e.g. a connecting network to another computing entity. The communication medium conveys information such as computer executable instructions compressed graphics information or other data in a modulated data signal.

Although the operations of some of the disclosed methods are described in a particular sequential order for convenient presentation it should be understood that this manner of description encompasses rearrangement unless a particular ordering is required by specific language set forth below. For example operations described sequentially may in some cases be rearranged or performed concurrently. Moreover for the sake of simplicity the attached figures may not show the various ways in which the disclosed methods can be used in conjunction with other methods.

Any of the disclosed methods can be implemented as computer executable instructions stored on one or more computer readable storage media tangible computer readable storage media such as one or more optical media discs volatile memory components such as DRAM or SRAM or nonvolatile memory components such as hard drives and executed on a computing device e.g. any commercially available computer including smart phones or other mobile devices that include computing hardware . By way of example computer readable media include memory and or storage . The term computer readable media does not include communication connections e.g. such as modulated data signals and carrier waves.

Any of the computer executable instructions for implementing the disclosed techniques as well as any data created and used during implementation of the disclosed embodiments can be stored on one or more computer readable media. The computer executable instructions can be part of for example a dedicated software application or a software application that is accessed or downloaded via a web browser or other software application such as a remote computing application . Such software can be executed for example on a single local computer e.g. any suitable commercially available computer or in a network environment e.g. via the Internet a wide area network a local area network a client server network such as a cloud computing network or other such network using one or more network computers.

For clarity only certain selected aspects of the software based implementations are described. Other details that are well known in the art are omitted. For example it should be understood that the disclosed technology is not limited to any specific computer language or program. For instance the disclosed technology can be implemented by software written in C Java Perl JavaScript Adobe Flash or any other suitable programming language. Likewise the disclosed technology is not limited to any particular computer or type of hardware. Certain details of suitable computers and hardware are well known and need not be set forth in detail in this disclosure.

Furthermore any of the software based embodiments comprising for example computer executable instructions for causing a computing device to perform any of the disclosed methods can be uploaded downloaded or remotely accessed through a suitable communication means. Such suitable communication means include for example the Internet the World Wide Web an intranet software applications cable including fiber optic cable magnetic communications electromagnetic communications including RF microwave and infrared communications electronic communications or other such communication means.

The disclosed methods apparatus and systems should not be construed as limiting in any way. Instead the present disclosure is directed toward all novel and nonobvious features and aspects of the various disclosed embodiments alone and in various combinations and subcombinations with one another. The disclosed methods apparatus and systems are not limited to any specific aspect or feature or combination thereof nor do the disclosed embodiments require that any one or more specific advantages be present or problems be solved. We therefore claim as our invention all that comes within the scope and spirit of these claims.

