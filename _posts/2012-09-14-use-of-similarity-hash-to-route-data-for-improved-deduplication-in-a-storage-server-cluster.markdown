---

title: Use of similarity hash to route data for improved deduplication in a storage server cluster
abstract: A technique for routing data for deduplication in a storage server cluster includes computing, for each node in the cluster, a value collectively representative of the data stored on the node, such as a “geometric center” of the node. New or modified data is routed to the node which has stored data identical or most similar to the new or modified data, as determined based on those values. Each node stores a plurality of chunks of data, where each chunk includes multiple deduplication segments. A content hash is computed for each deduplication segment in each node, and a similarity hash is computed for each chunk from the content hashes of all segments in the chunk. A geometric center of a node is computed from the similarity hashes of the chunks stored in the node.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08607017&OS=08607017&RS=08607017
owner: NetApp, Inc.
number: 08607017
owner_city: Sunnyvale
owner_country: US
publication_date: 20120914
---
This application is a continuation application of U.S. application Ser. No. 12 606 088 filed Oct. 26 2009 now U.S. Pat. No. 8 321 648 issued Nov. 27 2012 which is incorporated herein by reference in its entirety.

At least one embodiment of the present invention pertains to network storage systems and more particularly to a technique for routing data for improved deduplication in a clustered storage server system.

Network based storage or simply network storage is a common approach to backing up data making large amounts of data accessible to multiple users and other purposes. In a network storage environment a storage server makes data available to client host systems by presenting or exporting to the clients one or more logical containers of data. There are various forms of network storage including network attached storage NAS and storage area network SAN . In a NAS context a storage server services file level requests from clients whereas in a SAN context a storage server services block level requests. Some storage servers are capable of servicing both file level requests and block level requests.

In a large scale storage system such as an enterprise storage network it is common for certain data to be stored in multiple places in the storage system. Sometimes this duplication is intentional but often it is an incidental result of normal operation of the storage system. Therefore it is common that a given sequence of data will be part of two or more different files for example. Data duplication as the term is used herein generally refers to unintentional duplication of data in a given storage device or system. Data duplication generally is not desirable because storage of the same data in multiple places consumes extra storage space which is a valuable and limited resource.

Consequently storage servers in many large scale storage systems have the ability to deduplicate data which is the ability to identify and remove data duplication. In one known approach to deduplication any extra copies of a given sequence of data are deleted and any references e.g. pointers to those duplicate sequences are modified to refer to the one remaining instance of that sequence. A result of this process is that a given sequence of data may end up being shared by two or more files or other types of logical data containers .

Deduplication is closely related to another technique used to reduce the physical storage needed for a given amount of data namely data compression also known as file compression . The primary difference between data compression and deduplication is the scale at which they operate. Data compression typically operates by removing duplicate data sequences as short as two bytes while deduplication typically operates on duplicate sequences of length 1 KB to 4 KB. At the smaller scale of data compression the size of the metadata needed to reconstruct the duplicate data sequences becomes an overriding issue in its effectiveness. Advanced techniques such as arithmetic coding are generally needed to reduce the size of the metadata. In data deduplication on the other hand the metadata is so much smaller than the eliminated duplicate data that it does not significantly contribute to reducing the space efficiency.

The main consideration in choosing the size or average size of the data segments in a deduplication method is the resulting deduplication ratio which is the percentage reduction in physical storage requirements. The space used by the deduplication metadata is overhead that reduces the deduplication ratio. Usually the size of the metadata is proportional to the number of data segments. This means that smaller segments which result in a larger number of segments being required cause a corresponding increase in the metadata. On the other hand choosing a smaller segment size results in more duplication being found and eliminated. A balance must be struck therefore between two extremes 1 choosing segment sizes so small that the large number of segments causes the metadata size to significantly reduce the deduplication ratio and 2 choosing segment sizes so large that large amounts of duplication are not detected also reducing the deduplication ratio.

It has been observed that deduplication segment sizes in the range of 1 KB to 4 KB tend to provide the highest deduplication ratios given typical metadata implementations that require a fixed number of bytes per segment usually around 15 to 30. The implications of this observation are important when deduplicating data across a storage server cluster.

A storage server cluster is a storage system formed from a set of cooperating storage server nodes connected by a switching fabric. This type of configuration can be used to increase storage capacity performance or reliability. Often the goal of such a system is to provide a seamless collection of physical storage in which the end users need not be concerned with which node contains any particular piece of data. This property is called location transparency. Within the context of such a system conventional deduplication techniques can be used to remove duplication that occurs within any one node of the system. However it would improve the storage efficiency of the system to remove or avoid duplication that occurs between different nodes of the cluster as well.

One way to achieve this is to arrange that a newly written data segment that is identical to an existing data segment be assigned to the same node as the existing one so that they can be deduplicated using conventional techniques. The best known way to achieve this is to route data segments to nodes according to their content so that a second data segment with the same content will be sent to the same node as the first copy. This usually involves the use of a mechanism that hashes the content of the segment to a node identifier.

A related problem in a location transparent storage cluster is identifying the node that has a particular data segment requested by a read operation. A read operation will request data by its logical position within the overall data set its address not by its content or its node location. Therefore a hash function that maps data content to a node number is not useful in locating data. Instead a database can be maintained that maps from a logical data segment address to the node that contains that data segment. The size of this database can be significant as will now be explained.

The desire for small data segments to improve the deduplication ratio conflicts with the desire for an efficient method to find the node location of each data segment. The problem is one of scale. Current storage clusters might have hundreds of nodes each containing several terabytes of storage. Such a cluster may thus have a capacity of many petabytes of storage. With a segment size of 1 KB the storage cluster could contain several trillion data segments. Given these numbers a database that maps data segment addresses to node numbers would have to be enormous many times more than the maximum amount of RAM that can be installed in today s servers.

Another problem with the use of small segments is the inefficiency of having to gather data from multiple nodes when servicing a read request. If for example each 1 KB segment is stored on a different node of the cluster this implies that a 16 KB read operation requires communicating with up to 16 different nodes. Having to read segments from multiple nodes to service a given read request can cause significant network latency and bandwidth usage within a cluster.

A technique is introduced here for efficiently routing data for deduplication in a storage server cluster that includes multiple storage server nodes. In general the technique includes computing for each node a value representative of the data stored on the node. This value is called the geometric center of the node. It will be understood that this does not refer to a geometric center in a spatial sense but rather it is a representation of the content stored on the node. When a write request is received at one of the nodes one of the nodes in the cluster is selected to store the new or modified data according to a deduplication criterion based on the computed geometric center values of the nodes and the content of the new or modified data. More specifically the new or modified data will normally be routed to a node whose stored data either includes data identical to the new or modified data or whose stored data includes data that is most similar to the new or modified data as determined based on all of the nodes geometric centers thereby avoiding unnecessary duplication of data across different nodes in the cluster.

To compute the geometric centers in certain embodiments the data objects e.g. files or LUNS stored in each node are initially each divided logically into some number of chunks . Each chunk is then logically divided into multiple data segments also called deduplication segments or just segments . Segments can be fixed size or variable size depending on the embodiment. A chunk is the basic unit of data for purposes of assigning data to a node to reduce duplication whereas a segment is actually the basic unit of deduplication i.e. the basic unit of data sharing. Typically there will be many segments in each chunk such that a chunk is substantially larger than a segment. Using relatively small data sequences segments for deduplication achieves effective deduplication while using much large data sequences chunks as the unit of routing data to particular nodes results in a manageably small data location database and greater read efficiency.

A content hash is computed for each data segment in each chunk stored in each node. A similarity hash is then computed for each chunk as a function of the content hashes of all of the data segments in the chunk. The geometric center of each node is then computed as a function of the similarity hashes of all the chunks stored in the node. The geometric center of each node is stored in a location and manner so that it is accessible to all of the nodes in the cluster.

In certain embodiments the similarity hash is a fixed length vector of small integers computed from the contents of the segments in a chunk. It has the property that if two chunks have many identical segments the integers at each position in their two similarity hashes will have values close to one another. Thus if one views a similarity hash as a point in n dimensional space where n is the length of the vector then the distance or closeness between two similarity hashes in this space is a good indicator of how many identical data segments the two corresponding data chunks have in common.

When a write request is received at any of the nodes in the cluster the receiving node computes a new similarity hash for a chunk that includes a segment to be written the containing chunk and compares it to the geometric center of each of the nodes to determine which of the nodes geometric center is closest to the containing chunk in terms of its content. In one embodiment the new or modified data segment is then written to the node whose data is determined to be closest i.e. the node which has stored data most similar to the write data . Any time data is written to a node its geometric center is updated.

In certain embodiments selection of nodes to store new or modified data is done by using the geometric center values in conjunction with values indicative of the load on the various nodes in the cluster to improve load balancing across the cluster. These load related values are referred to herein as gravitational coefficients of the nodes. As the load e.g. consumption of storage space increases on a given node its gravitational coefficient decreases and vice versa. Accordingly when selecting a node to receive write data the node selection can be made as a function of both the geometric centers of the nodes and their gravitational coefficients. If for example the node whose geometric center is closest to that of the write data has much less free space than other nodes in the cluster making its gravitational coefficient low the write data may instead be directed to a node which is not as close but which is less heavily loaded making its gravitational coefficient high . Thus a node whose geometric center is not closest to that of the write data but whose gravitational coefficient is much higher than that of the other nodes may nonetheless be selected to store the write data to improve load balancing across the cluster.

Groups of similar data can also be moved between nodes when it is convenient or desirable to further improve the cluster s deduplication characteristics. Any time data is moved between nodes the geometric centers and gravitational coefficients of the affected nodes are recomputed.

Thus the technique introduced here uses a similarity hash on relatively large chunks of data each including many deduplication segments to partition the data chunks across the nodes of a storage cluster in a way that improves cluster wide deduplication. The use of relatively large chunks each including many deduplication segments solves the problem of an impractically large data location database. Further it avoids the inefficiency of having to gather individual segments from many different nodes during a read operation. Reading a large amount of data can be done by communicating with a single node to get a chunk.

The technique only needs to keep track of one small piece of information about the content on each node namely the geometric center of the chunks on that node in order to route new data to the optimal node for deduplication. In addition the technique can use an adjustable gravitational attraction parameter to determine when to automatically move clusters of similar objects to less loaded nodes as a load balancing technique.

Other aspects of the technique will be apparent from the accompanying figures and from the detailed description which follows.

References in this specification to an embodiment one embodiment or the like mean that the particular feature structure or characteristic being described is included in at least one embodiment of the present invention. Occurrences of such phrases in this specification do not necessarily all refer to the same embodiment.

The storage server or servers may be for example one of the FAS xxx family of storage server products available from NetApp Inc. The client systems . . are connected to the storage server via the computer network which can be a packet switched network for example a local area network LAN or wide area network WAN . Further the storage server is connected to the disks via a switching fabric which can be a fiber distributed data interface FDDI network for example. It is noted that within the network data storage environment any other suitable numbers of storage servers and or mass storage devices and or any other suitable network technologies may be employed.

The storage server can make some or all of the storage space on the disk s available to the client systems . . in a conventional manner. For example each of the disks can be implemented as an individual disk multiple disks e.g. a RAID group or any other suitable mass storage device s . The storage server can communicate with the client systems . . according to well known protocols such as the Network File System NFS protocol or the Common Internet File System CIFS protocol to make data stored on the disks available to users and or application programs. The storage server can present or export data stored on the disk as volumes to each of the client systems . .. A volume is an abstraction of physical storage combining one or more physical mass storage devices e.g. disks or parts thereof into a single logical storage object the volume and which is managed as a single administrative unit such as a single file system. A file system is a structured e.g. hierarchical set of stored logical containers of data e.g. volumes logical unit numbers LUNs directories files . Note that a file system does not have to include or be based on files per se as its units of data storage.

Various functions and configuration settings of the storage server and the mass storage subsystem can be controlled from a management station coupled to the network . Among many other operations a data object migration operation can be initiated from the management station .

Each of the nodes is configured to include several modules including an N module a D module and an M host each of which can be implemented by using a separate software module and an instance of a replicated database RDB . Specifically node . includes an N module . a D module . and an M host . node .N includes an N module .N a D module .N and an M host .N and so forth. The N modules . .M include functionality that enables nodes . .N respectively to connect to one or more of the client systems over the network while the D modules . .N provide access to the data stored on the disks . .N respectively. The M hosts provide management functions for the clustered storage server system . Accordingly each of the server nodes in the clustered storage server arrangement provides the functionality of a storage server.

The RDB is a database that is replicated throughout the cluster i.e. each node includes an instance of the RDB . The manner of such replication is not germane to the technique introduced here any known or convenient technique for database replication can be used. The various instances of the RDB are updated regularly to bring them into synchronization with each other. The RDB provides cluster wide storage of various information used by all of the nodes including a volume location database VLDB not shown . The VLDB is a database that indicates the location within the cluster of each volume in the cluster i.e. the owning D module for each volume and is used by the N modules to identify the appropriate D module for any given volume to which access is requested. The RDB may actually be implemented as multiple replicated databases that store different kinds of data and or metadata.

The nodes are interconnected by a cluster switching fabric which can be embodied as a Gigabit Ethernet switch for example. The N modules and D modules cooperate to provide a highly scalable distributed storage system architecture of a clustered computing environment implementing exemplary embodiments of the present invention. Note that while there is shown an equal number of N modules and D modules in there may be differing numbers of N modules and or D modules in accordance with various embodiments of the technique described here. For example there need not be a one to one correspondence between the N modules and D modules. As such the description of a node comprising one N module and one D module should be understood to be illustrative only.

The storage controller can be embodied as a single or multi processor storage system executing a storage operating system that preferably implements a high level module such as a storage manager to logically organize the information as a hierarchical structure of named directories files and special types of files called virtual disks hereinafter generally blocks on the disks. Illustratively one processor can execute the functions of the N module on the node while another processor executes the functions of the D module .

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing software program code and data structures associated with the present invention. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures. The storage operating system portions of which is typically resident in memory and executed by the processors s functionally organizes the storage controller by among other things configuring the processor s to invoke storage operations in support of the storage service provided by the node . It will be apparent to those skilled in the art that other processing and memory implementations including various computer readable storage media may be used for storing and executing program instructions pertaining to the technique introduced here.

The network adapter includes a plurality of ports to couple the storage controller to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus can include the mechanical electrical and signaling circuitry needed to connect the storage controller to the network . Illustratively the network can be embodied as an Ethernet network or a Fibre Channel FC network. Each client can communicate with the node over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system to access information requested by the clients . The information may be stored on any type of attached array of writable storage media such as magnetic disk or tape optical disk e.g. CD ROM or DVD flash memory solid state disk SSD electronic random access memory RAM micro electro mechanical and or any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is stored on disks . The storage adapter includes a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance Fibre Channel FC link topology.

Storage of information on disks can be implemented as one or more storage volumes that include a collection of physical storage disks cooperating to define an overall logical arrangement of volume block number VBN space on the volume s . The disks can be organized as a RAID group. One or more RAID groups together form an aggregate. An aggregate can contain one or more volumes file systems.

The storage operating system facilitates clients access to data stored on the disks . In certain embodiments the storage operating system implements a write anywhere file system that cooperates with one or more virtualization modules to virtualize the storage space provided by disks . In certain embodiments a storage manager logically organizes the information as a hierarchical structure of named directories and files on the disks . Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the storage manager to further logically organize information as a hierarchical structure of blocks on the disks that are exported as named logical unit numbers LUNs .

In the illustrative embodiment the storage operating system is a version of the Data ONTAP operating system available from NetApp Inc. and the storage manager implements the Write Anywhere File Layout WAFL file system. However other storage operating systems are capable of being enhanced or created for use in accordance with the principles described herein.

In addition the storage operating system includes a set of layers organized to form a backend server that provides data paths for accessing information stored on the disks of the node . The backend server in combination with underlying processing hardware also forms the D module . To that end the backend server includes a storage manager module that manages any number of volumes a RAID system module and a storage driver system module . At least one of the volumes includes at least one qtree and at least one standard directory .

The storage manager primarily manages a file system or multiple file systems and serves client initiated read and write requests. The RAID system manages the storage and retrieval of information to and from the volumes disks in accordance with a RAID redundancy protocol such as RAID 4 RAID 5 or RAID DP while the disk driver system implements a disk access protocol such as SCSI protocol or FCP.

The backend server also includes a CF interface module to implement intra cluster communication with N modules and or other D modules. The CF interface modules and can cooperate to provide a single file system image across all D modules in the cluster. Thus any network port of an N module that receives a client request can access any data container within the single file system image located on any D module of the cluster.

The CF interface modules implement the CF protocol to communicate file system commands among the modules of cluster over the cluster switching fabric . Such communication can be effected by a D module exposing a CF application programming interface API to which an N module or another D module issues calls. To that end a CF interface module can be organized as a CF encoder decoder. The CF encoder of e.g. CF interface on N module can encapsulate a CF message as i a local procedure call LPC when communicating a file system command to a D module residing on the same node or ii a remote procedure call RPC when communicating the command to a D module residing on a remote node of the cluster. In either case the CF decoder of CF interface on D module de encapsulates the CF message and processes the file system command.

In operation of a node a request from a client is forwarded as a packet over the network and onto the node where it is received at the network adapter . A network driver of layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the storage manager . At that point the storage manager generates operations to load retrieve the requested data from disk if it is not resident in memory . If the information is not in memory the storage manager indexes into a metadata file to access an appropriate entry and retrieve a logical VBN. The storage manager then passes a message structure including the logical VBN to the RAID system the logical VBN is mapped to a disk identifier and disk block number DBN and sent to an appropriate driver e.g. SCSI of the disk driver system . The disk driver accesses the DBN from the specified disk and loads the requested data block s in memory for processing by the node. Upon completion of the request the node and operating system returns a reply to the client over the network .

The data request response path through the storage operating system as described above can be implemented in general purpose programmable hardware executing the storage operating system as software or firmware. Alternatively it can be implemented at least partially in specially designed hardware. That is in an alternate embodiment of the invention some or all of the storage operating system is implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC for example.

The N module and D module each can be implemented as processing hardware configured by separately scheduled processes of storage operating system however in an alternate embodiment the modules may be implemented as processing hardware configured by code within a single operating system process. Communication between an N module and a D module is thus illustratively effected through the use of message passing between the modules although in the case of remote communication between an N module and D module of different nodes such message passing occurs over the cluster switching fabric . A known message passing mechanism provided by the storage operating system to transfer information between modules processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF API.

The technique introduced here has several aspects. First referring to the data stored in each node and any new data that is added to any node in the cluster is divided logically into contiguous sequences called deduplication segments data segments or simply segments . These segments are the basic units of deduplication data sharing. These can be fixed length segments the size of which is chosen in advance or they can be variable length segments the boundaries of which are determined from the data content by using any known or convenient technique for doing so.

The technique introduced here further computes a value called a content hash from the contents of each data segment this is a function computed in such as way that if two data segments have the same content hash value it is very likely they contain the same data. In certain embodiments the content hash is an n bit binary integer i.e. its value is between 0 and 2 where n is a parameter of the method. In certain embodiments n is preferably between 32 and 256.

The technique also groups contiguous sequences of data segments into larger data sequences called chunks which are the basic unit of routing data for deduplication. In one embodiment each chunk consists of a fixed number of adjacent segments with the number chosen in advance i.e. prior to runtime .

The technique then computes a value called a similarity hash for each chunk as a function of the content hashes of all of the segments in the chunk. The similarity hash in one embodiment is an integer vector that contains a fixed number of components where the number of components is the same as the number of bits n in each content hash. This vector is treated as a point in n dimensional space. An important property of a similarity hash is that two chunks that contain many identical segments will have similarity hashes that are close to one another in the n dimensional vector space i.e. closer than would be expected by chance. The similarity hash of a chunk may also be referred to as the position of the chunk in n space. The measure of how close two similarity hashes are i.e. the distance between them can be computed as for example a Euclidean distance between the two vectors in n space or as another example as the sum of the absolute values of the difference at each component of the vectors.

To decide which node of the cluster is most likely to contain data that is most similar to new data that is to be added to the storage cluster the technique keeps track of for each node the geometric center or just center of the positions of all the chunks on the node. When a new chunk of data is added to the cluster it is sent to the node whose center is closest to the position of the chunk.

The technique introduced here can also include a load balancing method which tends to equalize the amount of space and or I O bandwidth used on each node of the cluster. It does so by enhancing the technique as described above with a parameter that is conceptually analogous to gravitational attraction. In certain embodiments an integer acting as a gravitational coefficient is defined and regularly updated for each node. Further the method includes keeping track of a center of gravity of each node treating the position of each of the node s chunks as the center of a particle of equal mass. As the storage used on a node approaches its physical limit or the rate of I O operations to and from the node approaches the bandwidth limit its gravitational coefficient is reduced towards zero. Each new chunk of data tends to be sent other factors being equal to the node that has the strongest gravitational attraction for it based on the distance of the chunk s location from the center of gravity of the node and the node s gravitational coefficient. Any known or convenient technique can be used to measure or estimate the parameters that affect the gravitational coefficient. For example the N module in a node might measure the rate of I O operations for purposes of measuring bandwidth consumption while the D module of the node might measure the amount of storage space consumption on the node.

The result of this technique is that when new data chunks are added to the storage cluster they will tend to be sent to the storage node that has existing chunks of data with the maximum number of identical segments in common with the new chunk. This will provide greater opportunity for conventional intra node deduplication techniques to reduce duplication on each node thereby reducing duplication across the cluster.

As explained above large data sequences files extents or blocks are broken into fixed length or variable length pieces called segments for the purpose of deduplication. The goal of deduplication is that two different segments with the same contents do not exist. In a storage cluster it is desirable to achieve this across all the nodes of the cluster such that no two nodes of the cluster contain a segment with the same contents.

Either of two different approaches to breaking data into segments can be used. The technique introduced here applies equally to both approaches. The first approach involves breaking all sequences of data into fixed size segments. The segment size can be the same as the block size in a block based storage system in which case each block would be a deduplication segment. However if blocks are 4 KB which is commonly the case deduplication will often achieve a higher compression ratio by using smaller segments e.g. 1 KB segments. One advantage of this method of segment definition is simplicity of implementation and simplicity of the metadata. However a disadvantage is that it may miss some opportunities for deduplication. In particular if two long data sequences are identical except that one of them has a few extra bytes inserted at the beginning the fixed segment approach will not detect and eliminate any of that duplication since segment boundaries are different within the two identical sequences.

The second approach for defining segments is to break long sequences of data into variable sized segments based on their content. A well known way of doing this is to compute a rolling checksum while scanning the data sequence and start a new segment every time the rolling checksum value satisfies a certain property e.g. that its low order n bits are all zero. Since the checksum is a pseudo random hash one would expect that its low order 10 bits would all be zero roughly every 1 024 bytes producing an average segment size of 1 024 bytes. A significant property and advantage of this method is that if two data sequences are very similar but their identical sequences are shifted slightly due to the insertion of a small amount of data in one of the two they will be broken into segments most of which are identical between the two copies and the two segments will be almost completely deduplicated.

For each deduplication segment whether fixed size or variable sized a content hash is computed from the contents of the segment. This content hash is a fixed length sequence of bits. In one embodiment it could be a 64 bit checksum computed from the contents. In another it could be the SHA 1 function computed on the contents of the segment. The important property of the hash function is that any two segments with different contents have a very high probability of having different content hash values that is the hash function produces few collisions. Any hash function that operates on contents that are longer than the output of the hash function will have some collisions since there are more different contents than possible function output values. The content hash of a deduplication segment can also be called the segment s fingerprint since it identifies a particular data segment contents with a very high confidence level.

In a simple form of cluster wide deduplication the content of each segment could be hashed to a node number and the data segment would be sent to that node for storage. However there are scalability problems with keeping track of the location of so many small data segments as noted above. Therefore the technique introduced here chooses a different strategy which is described as follows.

As described above if data segments are assigned to nodes of the cluster based on the segments content rather than their addresses then in order for a read operation to find requested segments some form of cluster wide database that maps segment addresses to nodes is needed. To reduce the size of such a database the technique introduced here combines multiple adjacent deduplication segments into chunks as described above. There are a fixed number of deduplication segments in each chunk. In one embodiment there are between 500 and 2000 segments in each chunk. All of the segments within a chunk are sent together to the same node of the storage cluster.

Therefore the cluster wide location database mentioned above only needs to have one entry for each chunk instead of one entry for each data segment within the chunk. This database is called the data location database or simply location database . In certain embodiments as illustrated in each node in the cluster includes an instance of the data location database . The location database may be for example a subset of the RDB . This approach which uses one entry for each chunk in the location database reduces the overall space savings since the same segment may appear in two different chunks that are sent to different nodes however it is a good compromise to ensure that the frequently used entries of the location database can be cached in main memory and not require a disk access. This is important because the location database generally must be accessed for every uncached read operation.

In a block based system chunks and segments can be uniquely identified by the block address and offset where they begin. In a file based system or object based system chunks can be uniquely identified by the file or object in which they occur together with the byte offset of the start of the chunk or segment. The location database therefore maps a block address and byte offset in a block based system or a file handle plus byte offset in a file based system to the node that stores the chunk with that address.

In embodiments in which variable sized deduplication segments are used the location database can be implemented as a b tree for example to allow finding the chunk associated with a given byte address in time that is logarithmic in the number of chunks. In embodiments in which fixed size deduplication segments are used the location database can be implemented as a simple array of node numbers where the position within the array corresponds to the address of the segment. Because of their fixed size and the fact that there are a fixed number of segments in each chunk a segment address can be converted to a chunk number by a simple division operation i.e. segment address number of segments per chunk . The most important piece of information in the table for each chunk is the node number on which it is stored however it could contain other information such as the data and time that the chunk was last modified.

For each chunk a similarity hash is defined based on the content hashes of all of the segments in the chunk. Assuming each content hash is n bits long and there are k segments in each chunk the similarity hash can be an array of n integers each in the range of 0 through k where the integer value at position i is the number of occurrences of the value 1 in bit position i across the content hashes of all of the segments in the chunk. shows a simple example for the case where n 4 and k 5. Note that these values are chosen only for purposes of simplifying description.

In addition to maintaining an instance of the location database each node in the cluster also includes a database of deduplication metadata relating to that node i.e. a deduplication metadata database as illustrated in in practice it can be implemented as multiple databases . In each node the following items of metadata are maintained and stored persistently 

2 The starting address of each chunk stored in the node. This information is used within the node to map a virtual address of the chunk to a physical address of the chunk.

For each node of the cluster an array of integers of the same size as the similarity hash is also maintained. This is called the geometric center of the node as will now be explained. A similarity hash is in one embodiment an array of n integers in the range of 0 through k. It therefore can be treated as a point within the n dimensional hyper cube bounded by the range 0 through k on all axes. Viewed this way the geometric center of the chunks on each node can be obtained by performing a position wise coordinate wise summation of the similarity hashes of all chunks stored in the node and then dividing the result position wise by the number n of chunks stored in the node e.g. p p . . . pn n. It should be clear that two chunks that differ in only a few blocks will have similarity hashes that are nearer to each other in n space than would be expected by chance which is the intuition behind using this abstraction.

The geometric center of a node is a collective characterization of all of the data segments on that node for example an expression of the average of all of the data segments on the node. Thus the system can use it in determining on which node each new chunk of data should be stored in order to increase the amount of deduplication that can be performed.

In the initial state of an empty node the geometric center is not defined by the above rule as there are no chunks stored on it yet. In that case its geometric center is defined to be the origin point in n dimensional space i.e. the point with all coordinates zero.

Initially at the process divides each data object storage node into multiple chunks . At the process then divides each chunk into multiple segments . Next the process computes a content hash for each segment at . As noted the content hashes can be computed using for example a content based checksum or the SHA 1 algorithm. At the process computes a similarity hash for each chunk stored in the node based on the content hashes of the segments of each chunk. The process then computes the geometric center of the node at in the manner described above. At the process stores the geometric center in the deduplication metadata database of this node. Then at the process stores the starting address and node ID of each chunk in the node in the replicated location database .

At the receiving n module identifies the chunk s and segment s that is are affected written by the write request. It is assumed that in a block based system the write request provides a starting block address and one or more blocks to be written. It is further assumed that in a file based system e.g. an NFS or CIFS based system the write request provides a file handle a starting byte offset and one or more bytes of data to be written. The affected chunk s and segment s can be identified easily from the information in the write request although the details of this operation depend on which type of system is used block based or file based and further whether the system uses fixed size segments or variable size segments.

If fixed sized deduplication segments are used all chunks are the same size in bytes since there is also a fixed number of segments per chunk. For example if there are 4 096 bytes per segment and 512 segments per chunk there are exactly 2 097 152 bytes 2 MB in each chunk. In that case whether using a block based system or a file based system the conversion from a byte address in a read or write request to a chunk number involves merely dividing the byte address by the number of bytes per chunk. In a file based system this operation would produce a chunk number within the particular open file identified by a file handle whereas in a block based system it would produce the address of the chunk within the volume.

If variable sized deduplication segments are used one method for converting a byte address to a chunk number is to maintain a b tree on permanent storage that maps byte addresses to chunk information. The b tree can reside on each N module and or it can be part of the data location database . The b tree allows finding the right chunk given a byte address in time that varies logarithmically with the number of chunks. In the case of a block based system there can be one b tree for each block based volume. In the case of a file based system there can be one b tree for each file.

The process then performs operations as will now be described for each affected chunk. These operations can be but do not have to be performed by the n module of the node that received the write request.

At the process computes or recomputes a content hash for each segment affected by the write request. The process then at recomputes the chunk s similarity hash based on the content hashes of the chunk has modified with the new or modified segment s . In one embodiment to do this the receiving n module first requests and receives the existing old content hashes of all the segments in the chunk from the D module of the node in which the chunk currently resides as identified from the location database . The computation can be done by simply subtracting from the old similarity hash the 1 bits in the old content hashes and adding to it the 1 bits in the new content hashes.

Next at the process recomputes the geometric center of the node that contains this chunk this update is reflected at least in the deduplication metadata database of that node. This recomputation can be accomplished as follows. In the case of modifying a chunk the point vector represented by the similarity hash of the old version of the chunk is subtracted out of the sum of all of the points represented by the similarity hashes of all of the chunks on the node recall that the sum of a set of points is defined to be the point whose ith component is the sum of the ith components of all the points in the set . Then the point represented by the similarity hash of the new version of the chunk i.e. the point in n space is added into the sum of all points for the node. This sum is then divided by the number of points to produce the geometric center. In the case of adding a new chunk the new point is added to the sum of the points on the node which is then divided by the new number of points to produce the new geometric center.

Referring still to at the process then identifies the node whose geometric center is closest to the new similarity hash for the chunk. This is a search among a relatively small number of items i.e. the number of nodes in the cluster. For each node this operation involves a computation of the distance between two points in n dimensional space. The distance measure can be but does not have to be Euclidean which involves expensive squaring and square root operations . For example as an alternative in one embodiment the distance measure is the sum of the absolute values of the difference at each component of the vector i.e. 

If the node identified at is the node in which the chunk currently resides then the process proceeds to where the process writes the affected segment s to the chunk in the identified node. The process then ends at least with respect to the current affected chunk .

On the other hand if the node identified at is a different node from that in which the chunk currently resides then the process instead proceeds from to . At the process moves the entire chunk as modified with the new modified segment s from its current node to the node identified at . The process then updates the location database to reflect the new location of the chunk at . This update is reflected in all instances of the location database. The process then recomputes the geometric center of both affected nodes i.e. the node from which the chunk is being moved and the node to which the chunk is being moved. The process then ends at least with respect to the current affected chunk . As mentioned above this process can be extended to also take into account the gravitational attraction of each node when determining where to store a modified chunk as further described below.

At the receiving n module looks up in the location database the node which currently stores each target chunk. At the process then reads the target chunk s from the identified node s and sends the requested data to the requester.

It may be desirable to implement capacity balancing in a storage cluster that implements deduplication by similarity hashing so that one node does not fill up before others. This is likely to happen if for example there are many highly similar large files most of the chunks of which are sent to identical nodes. To accomplish this balancing the geometric center approach described above can be extended to use a notion of gravitational attraction.

Specifically in one embodiment each node maintains a gravitational coefficient that decreases as more chunks are stored on the node in a manner opposite to Newtonian physics where the gravitational coefficient increases with mass . A possible value for the gravitational coefficient is the percentage of free space left on the node. The criterion for selecting a node on which to store a chunk can then become the following Select the node that has the highest gravitational attraction for the chunk which can be computed as the gravitational coefficient of the node divided by the chunk s distance from the geometric center of the node. It is not necessary to use a physically accurate inverse square law of gravitation which would be more computationally expensive.

The techniques introduced above can be implemented by programmable circuitry programmed or configured by software and or firmware or entirely by special purpose circuitry or in a combination of such forms. Such special purpose circuitry if any can be in the form of for example one or more application specific integrated circuits ASICs programmable logic devices PLDs field programmable gate arrays FPGAs etc.

Software or firmware for implementing the techniques introduced here may be stored on a machine readable storage medium and may be executed by one or more general purpose or special purpose programmable microprocessors. A machine readable medium as the term is used herein includes any mechanism that can store information in a form accessible by a machine a machine may be for example a computer network device cellular phone personal digital assistant PDA manufacturing tool any device with one or more processors etc. . For example a machine accessible medium includes recordable non recordable media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices etc. etc.

The term logic as used herein can include for example special purpose hardwired circuitry software and or firmware in conjunction with programmable circuitry or a combination thereof.

Although the present invention has been described with reference to specific exemplary embodiments it will be recognized that the invention is not limited to the embodiments described but can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense.

