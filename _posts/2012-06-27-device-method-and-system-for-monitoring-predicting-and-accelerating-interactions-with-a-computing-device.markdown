---

title: Device, method and system for monitoring, predicting, and accelerating interactions with a computing device
abstract: A device, method and system for monitoring, predicting, and accelerating interactions with a computing device includes determining a current interaction context at a computing device based on interactions occurring at the computing device, predicting a number of potential subsequent interactions, speculatively executing at least some of the potential subsequent interactions, and presenting interactive representations of the speculatively-executed interactions at the computing device.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09046917&OS=09046917&RS=09046917
owner: SRI International
number: 09046917
owner_city: Menlo Park
owner_country: US
publication_date: 20120627
---
This application claims the benefit of and priority to U.S. Provisional Patent Application Ser. No. 61 648 503 filed May 17 2012 which is incorporated herein by this reference in its entirety.

For many people electronic devices are a constant companion. For others computing devices such as laptop or tablet computers desktop computers smartphones and e readers may be used relatively infrequently at their jobs or only to perform a specific task for example. The human device experience can be less than optimal for these users and others. Those who use their devices frequently may find bothersome the number of steps or the amount of time it takes to perform a specific task the repetitive nature of certain operations or the ease with which typographical errors can be made. More casual users or those who seldom use certain features may additionally suffer from lost efficiency due to the difficulty of remembering how to perform a particular task from one session to the next.

Many traditional computing systems understand only discrete pieces of the real world context in which a person is using a computing device. As a result traditional systems often lack the capabilities needed to fully appreciate the bigger picture of the person s current experience and use that understanding to improve the human device interface.

Moreover when compared with traditional desktop or laptop computers the human computer interface offered by many personal electronic devices has limitations. Due to the small amount of real estate available for display screens and controls and in some cases limitations of mobile device operating systems and computing power interacting with a mobile device can be tedious. This can be so particularly when the interaction requires specifying multiple inputs and or accessing multiple software applications on the mobile device.

According to at least one aspect of this disclosure a method for accelerating user interaction with a computing device includes monitoring a current interaction context of the computing device where the current interaction context involves at least one current software application. The method also includes predicting a plurality of interactions with the computing device based on the current interaction context where at least one of the predicted interactions involves a different software application than the current software application. The method also includes at least partially executing the predicted interactions in response to the current interaction context and presenting at the computing device a user selectable representation of each of the at least partially executed predicted interactions.

The current interaction context may relate to at least one interaction with the at least one current software application and the method may include comparing the at least one interaction with a stored interaction model that includes data relating to a plurality of interactions with software applications. The interaction model may include data relating to a plurality of interaction patterns which may include at least one of pre defined interaction patterns learned interaction patterns generated by a computerized algorithmic learning process and a plurality of user specific interactions observed by the computing device. The predicted interactions may include an interaction that involves a software application and at least one parameter or argument and another interaction involving the same software application and at least one different parameter or argument. One or more of the predicted interactions may include a sequence of multiple interactions that can be executed by the computing device in an automated fashion on behalf of a user.

The method may include at least temporarily suspending the execution of a predicted interaction prior to completing the predicted interaction. The user selectable representation of one or more of the predicted interactions may be selectable to continue executing the temporarily suspended predicted interaction. The method may include at least partially executing a predicted interaction and at least partially executing another predicted interaction prior to completing execution of the predicted interaction. The method may include receiving a selection of one of the user selectable representations and repeating the predicting at least partially executing and presenting based on the selection. The method may include updating an interaction model based on the selection where the interaction model includes data relating to a plurality of interactions. The method may include determining an amount of time associated with user performance of at least one of the predicted interactions and at least partially executing the at least one predicted interaction based on the amount of time.

According to another aspect of this disclosure a computing device includes at least one processor and computer circuitry coupled to the at least one processor where the computer circuitry is arranged to cause the at least one processor to perform any of the foregoing methods. The computing device may be or include a personal handheld computing device. According to another aspect of this disclosure at least one machine accessible storage medium includes a plurality of instructions that in response to being executed result in a computing device performing any of the foregoing methods.

In accordance with a further aspect of this disclosure a method for learning and accelerating user interaction with a computing device includes maintaining an interaction model that includes data relating to user interactions with the computing device in relation to one or more software applications monitoring a current interaction context of the computing device learning an interaction involving one or more software applications storing data relating to the learned interaction in the interaction model predicting a plurality of interactions with the computing device based on the current interaction context and the interaction model at least partially executing the predicted interactions in response to the current interaction context and presenting at the computing device a user selectable representation of each of the at least partially executed predicted interactions.

The method may include detecting a user specified beginning of an interaction to be learned and detecting a user specified end of the learned interaction. The method may include determining in an automated fashion based on the current interaction context a beginning and an end of the learned interaction. The method may include generalizing the learned interaction for applicability to interaction contexts that are different from the current interaction context. According to another aspect of this disclosure a computing device includes at least one processor and computer circuitry coupled to the at least one processor where the computer circuitry is arranged to cause the at least one processor to perform any of the foregoing methods. The computing device may be or include a personal handheld computing device. According to another aspect of this disclosure at least one machine accessible storage medium includes a plurality of instructions that in response to being executed result in a computing device performing any of the foregoing methods.

In accordance with a further aspect of this disclosure a computing device includes a processor at least one optical sensor an output device and memory coupled to the processor where the memory has stored therein at least a portion of an interaction model. The interaction model includes data relating to user interactions with the computing device in relation to one or more software applications. The memory also has stored therein at least one computer executable module configured to monitor a current interaction context of the computing device where the current interaction context is defined at least in part by information derived from the at least one optical sensor. The at least one computer executable module is also configured to based on the current interaction context predict a plurality of interactions with the computing device where each interaction involves at least one software application at least partially execute the predicted interactions in response to the current interaction context and present at the output device a user selectable representation of each of the at least partially executed predicted interactions.

The at least one computer executable module may be configured to update the predicted interactions in response to information derived from the at least one optical sensor. The information derived from the at least one optical sensor may relate to implicit behavior of a person with respect to the computing device where the implicit behavior involves a portion of the person not explicitly interacting with the computing device. The at least one computer executable module may be configured to modify the presentation of user selectable representations based on the implicit behavior. The computing device may be or include a personal handheld computing device.

While the concepts of the present disclosure are susceptible to various modifications and alternative forms specific embodiments thereof are shown by way of example in the drawings and are described in detail below. It should be understood however that there is no intent to limit the concepts of the present disclosure to the particular forms disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives consistent with the present disclosure and the appended claims.

Referring to a system for monitoring learning predicting and accelerating interactions with a computing device includes an interaction monitor learner accelerator module interaction accelerator . As explained in greater detail below the illustrative interaction accelerator monitors a current interaction context of the computing device which is defined at least in part by a number of interactions with the computing device. The interactions can include a combination or series of actions performed by the user with the computing device as well as automated events that involve various software applications running on the computing device systems software device hardware and or sensors that may be in communication with the computing device.

The interaction accelerator includes an input acceleration module which interfaces with an interaction model to predict a number of interactions or series of interactions with the computing device that may logically follow a given interaction or series of interactions that have recently occurred at the computing device in view of the current interaction context. The input acceleration module begins executing at least some of the predicted interactions in a speculative fashion that is without definitively knowing which of the predicted interactions might actually be desired by the user to be performed next. Predicted interactions can be speculatively executed to completion or only up to an intermediate point at which the interaction requires user input or approval.

Each of the at least partially executed predicted interactions e.g. is captured or iconified as an interactive representation . Any of the representations can be selected by a user of the computing device to continue or cancel the execution of the corresponding predicted interaction or series of subsequent interactions.

A sequence of predicted interactions with the computing device may be designed by the interaction accelerator to accomplish a particular task or achieve a potentially desirable end result on behalf of the user. The predicted interactions may involve different software applications or combinations thereof. In the illustrative example the predicted interactions may each include a corresponding combination or sequence of individual interactions e.g. user supplied actions or automated events . For instance the predicted interaction may involve an interaction I followed by an interaction J and may be related to the creation of a new calendar appointment using calendar software running on the computing device. The predicted interaction may involve inputs from the interaction J which may be incorporated into subsequent interactions K and L and may be related to the creation of a reply email message using electronic mail software on the computing device. The predicted interaction may involve a series of interactions M N and O which may involve the same or different inputs than the predicted interactions and may be related to an online search for documents or information that may be conducted at the computing device.

Any or all of the interactive representations may be presented to the user concurrently and or in a logical sequence. For instance representations corresponding to predicted interactions that involve executing the same or similar tasks with different inputs may be presented concurrently while representations corresponding to predicted interactions that involve inputs derived from prior interactions may be presented sequentially or in a successive order.

In this way the system uses the current interaction context of the computing device to predict and speculatively pre execute subsequent interactions that may be desired by or helpful to the user. Further the system intelligently determines appropriate inputs for predicted interactions based on the current interaction context thereby reducing the number of inputs needing to be manually supplied by the user and further accelerating the user s interactions with the computing device.

Referring still to the interactions are analyzed by an interaction monitoring module in the illustrative embodiment. While shown in as a sub module of the interaction accelerator the interaction monitoring module may be a separate module that operates relatively independently from the interaction accelerator . For instance in some embodiments the interaction monitoring module may continuously monitor and analyze the interactions e.g. as they occur and post interaction data to the interaction model such that the interaction accelerator need only interface with the interaction model to determine the current interaction context and predict subsequent interactions.

In some embodiments an interaction learner module executes a process by which the system learns interactions that a user may find helpful to have automated e.g. those that may be repetitive in nature or frequently conducted in response to certain triggering events. Such learned interactions may be stored in the interaction model and performed by the computing device in an automated fashion at a later time e.g. in response to the then current interaction context. In this way the system can continuously improve its understanding of the current interaction context of the computing device and thereby predict next interactions more intelligently.

Further details regarding the interaction model are described below following a discussion of some of the types of interactions that may be monitored by various embodiments of the system . Further details regarding the interaction monitoring module are discussed below with reference to . In addition some exemplary techniques for learning interactions by the system are discussed below with reference to . Additional details regarding the input acceleration module are described below with reference to . An illustrative example of a computing device that may be used in connection with the system is shown in described below.

The interaction monitoring module analyzes and categorizes or classifies the various interactions of which it becomes aware in one or a number of different ways. In some embodiments the interaction monitoring module creates associations between or among interactions that may appear to be related or logically connected in some way based on the current interaction context. For example some interactions may be associated with one another as a combination or logical sequence of interactions according to spatial proximity e.g. relative closeness of position on a display screen temporal proximity e.g. closeness in time of performance execution frequency of occurrence recency of occurrence and or other factors. Some interactions may be classified based on a particular software application type of software application or combination of software applications or modules that are involved in the interaction e.g. an integrated email calendar contacts application vs. a web browser vs. a real time text messaging application . Some interactions may be classified based on semantic meanings associated with certain aspects of the interaction e.g. an email message may be classified as a meeting request based on semantic information associated with content that appears in the body of the message . In some embodiments interactions may be annotated with information that indicates the order or position of the interaction within a larger sequence or combination of interactions. As another example some interactions may be classified according to whether the interaction is initiated by a user by a software application or by some other component of the computing device. Results of the interaction monitoring module s monitoring and analysis of the interactions are stored in the interaction model and may be used by the interaction accelerator to determine the interaction context of the computing device at any given point in time.

Referring now to an illustrative embodiment of the system includes a computing device having hardware e.g. processor memory input output subsystems peripheral devices etc. systems software e.g. operating systems device drivers inter process communication IPC mechanisms etc. framework middleware e.g. application programming interfaces or APIs object libraries etc. and user interface and applications software . The user interface and applications software may include a variety of different kinds and types of software applications or user level e.g. client portions thereof. Some illustrative examples of user interface and applications software include graphical user interface operating system front ends office or personal productivity applications e.g. word processors spreadsheets slide presentation software calendars contacts task lists etc. web based or mobile device applications e.g. web browsers search engines travel or restaurant research and reservation applications ticket ordering and purchasing applications map and driving direction applications online shopping applications weather stock quotes and news applications communication applications e.g. email applications phone text messaging and voice natural language interfaces multimedia applications e.g. still camera video camera music player video player and or others.

Each of the interactions may include one or a number of different types of atomic user actions and or application or system level events A B and or C. As such the term interaction may refer to singular actions or events or combinations sequences or series thereof which may be directed to the completion of a particular task objective or result that may be desired by the user of the computing device. Interactions A represent interactions of a user with one or more features or aspects of the user interface and applications software running on the computing device. Some examples of user interactions A include opening a document sending an email message following a hyperlink and entering a search query. In general the interactions A of the person with the system may be of a variety of different types. For example the interactions A may involve the input or selection of content or data e.g. creating an email message filling in an online form selecting an item from a list of choices the activation or deactivation of application level controls e.g. save document send email and or the manipulation of user interface objects e.g. moving a scroll bar cursor or mouse pointer resizing a window relocating an icon .

The user interactions A may be explicit or active or implicit or passive . Some examples of explicit or active interactions include certain gestures e.g. swipe tap drag and other methods of physically contacting a keyboard keypad mouse touchscreen hardpanel control or other input mechanism of the computing device e.g. to activate a user interface control enter or select data or select a displayed option .

Some examples of implicit or passive user interactions A include interactions that may be detected by one or more sensors or other peripheral components of the computing device such as optical sensors proximity sensors accelerometers and or others. For instance optical sensors may detect that a person s eyes are focused on a particular area of the display screen without the person explicitly interacting with the computing device. Similarly proximity sensors may detect movement of a portion of the user s body e.g. hands arms toward or away from a particular portion of a touch sensitive display screen without the person explicitly interacting with the display screen. Accelerometers may detect changes in the manner in which the user is holding or carrying the computing device.

As the arrows A are bidirectional interactions A also represent interactions i.e. user level software application events of the user interface and applications software with the user . Some examples of software interactions A include displaying an email notification displaying a fill in form and displaying search results.

Interactions B represent user level software application events and or communications with the framework middleware and vice versa which may be responsive to interactions A and or interactions C. Interactions C represent system level events and or communications with the framework middleware and vice versa which may be responsive to interactions A and or interactions B. 

As should be appreciated by those skilled in the art user level interactions A are generally converted by the various user interface and applications software to a form B that can be understood by the framework middleware and or systems software . As a simple example the user level interaction of activating a send button in a user interface window that includes an email message addressed to John may be converted by the user interface and applications software to a SendMail Msg1 IPAddressforJohnsComputer command that can be understood by the framework middleware as the functional equivalent of the user level command corresponding to send email message to John. 

Through the use of an API object libraries and or other mechanisms the framework middleware may convert the SendMail command to a form C that can be understood by the systems software . The systems software may convert the programming code created by the framework middleware to machine level instructions that can be executed by the hardware .

As an example in some embodiments of the system the hardware may include one or more optical sensors e.g. one way or two way cameras proximity sensors a global positioning system GPS transceiver a microphone and or other sensors or peripheral components. The systems software may handle the GPS data images or other information detected by the sensors or peripheral components and translate it to a form C which can be converted by the framework middleware to a form B which can be used by one or more of the user interface and applications software . Any of the foregoing types of interactions A B C and or others may be of interest to and therefore monitored analyzed and processed by the interaction accelerator or a component thereof e.g. A or B described below at any given point in time depending on the current interaction context and or requirements of a particular design or implementation of the system .

In the embodiment the interaction accelerator includes a front end or client portion interaction monitor learner acceleration module IMLA A and a back end or middleware framework portion interaction accelerator B. The illustrative IMLA A is part of the user interface and applications software layer of the system while the illustrative interaction accelerator B is part of the framework middleware layer . As such the IMLA A can interface with a person using the system with user interface and applications software or with systems software to monitor and interpret interactions A or B initiated by the person or the software to monitor and interpret interactions C to present the representations and or perform other operations that may be related to the person or the software interacting with the system .

In some embodiments interactions A and or B may be initially captured and or processed by the IMLA A or by instrumented user level software applications such as any of those illustrated in the user interface and applications software layer and or others. Generally speaking instrumenting an application refers to the use of programming code to interface with a software application for the purpose of monitoring the activity of the software application at runtime and reporting data relating to that activity to a log file or database for example. For instance in some embodiments individual user level software applications can be instrumented so that upon executing interactions determined to be of interest data pertaining to the type of interaction conducted e.g. create new email message and arguments and or parameters associated with the interaction e.g. addressee subject content body are reported by the instrumented software application to and stored in the interaction model .

In other embodiments the interaction accelerator framework middleware B may alternatively or in addition periodically query any one or more of the individual user interface and applications software or systems software directly to obtain the desired interaction data and then report the information to the interaction model using e.g. a push data transmission paradigm wherein data is posted to the interaction model automatically as it occurs or on a periodic basis or a pull data transmission paradigm wherein data is posted to the interaction model in response to requests from the interaction model .

In some embodiments e.g. irrespective of whether individual software applications are instrumented the interaction accelerator B is configured to monitor analyze and process interactions B and or C in addition to interactions A. This enables the interaction accelerator B to monitor interactions involving different software applications at the same time. In addition hardware or system level interactions C that involve one or more of the user interface and applications software can be monitored analyzed and processed by the interaction accelerator B. Thus in some embodiments the interaction accelerator B can through the execution of computing logic make inferences about the current interaction context based on sensor data e.g. interactions C and interaction data collected from the user interface and applications software e.g. interactions A and or B .

For instance based on inputs from optical sensors integrated with a computing device the interaction accelerator B may derive information about the location of the user s current attention relative to other portions of the display screen. The interaction accelerator B may query one or more of the user interface and applications software or use other mechanisms such as the semantic visualization model discussed below to gather additional information about what is then being displayed on the computing device at the location of the user s gaze and or what functions are currently being performed by the user interface and applications software . Combining the gaze information with other details e.g. interactions A and or B or other information about the content displayed at the location of the user s gaze the interaction accelerator B may conclude for example that the user is working in a particular software application that a notification generated by the computing device is not receiving the user s attention or that something displayed at a particular location on the display screen may be important to the user in the current context.

The interaction accelerator B may predict subsequent interactions based on these and other similar combinations of information. As one example the interaction accelerator B may determine e.g. via sensor data that the user is currently looking at a specific portion of the display screen e.g. a specific part of an email message and then analyze the content displayed at the location on the display screen that corresponds to the user s gaze. Based on that analysis the interaction accelerator B may store portions of the analyzed content along with an association with the current context in the interaction model for current and or future use.

As another example the interaction accelerator B may generate a predicted next interaction involving the creation of a new calendar appointment that incorporates content displayed at the location of the user s recent focus e.g. subject attendees location and instantiate those details in the predicted interaction. Or the interaction accelerator B may in a predicted interaction use recently looked at content to automatically fill in the fields of a online form e.g. by automatically copying the looked at content and inserting it in the appropriate fields thereby leaving the user with only the task of reviewing and approving the automatically filled in form.

As a further example the system may generate inferences based on sensor data in combination with other interaction information and store those inferences in the interaction model in the form of templates or rules. For instance sensor data may indicate that the user s attention is currently turned to a web browser application. Based on previously executed sequences of interactions probabilistic data previously defined templates or rules etc. the system may conclude that the user s probable next actions relate directly to content that was viewed very recently by the user on the computing device. In analyzing the recently viewed content in view of the interaction model the system may generate one or more of the following predicted interactions a follow a hyperlink to a web address contained in a recently viewed document e.g. email b conduct a web search on a recently viewed phrase or image or c visit a social media site to access information about a person whose name or address was recently viewed e.g. the addressee or sender of an email . Similarly if the user browses a web page containing interactive fields to be filled out e.g. a restaurant reservation site a template or rule may indicate to the system that the user s probable next actions include copying recently viewed text e.g. proper names addresses dates etc. and entering it into corresponding fields of the form. In these and other ways the system can using the interaction model analyze content that has had the user s attention very recently on the computing device and automatically copy and incorporate the recently viewed content into candidate next interactions.

As another example the systems software may detect an incoming email message via a network interface of the computing device. Data relating to the incoming message may be relayed to an email application as interactions C. Pre cognitively to the user the interaction accelerator B may automatically characterize the incoming email message as a meeting request e.g. based on message content and or other aspects of the current interaction context of the computing device gleaned from the interaction model and automatically generate a calendar appointment incorporating the message content as a predicted interaction. In this example the interaction accelerator B is able to predict a next interaction that alleviates the need for the user to attend to an incoming email message and manually transfer information from the message to a calendar appointment.

In these and other instances the interaction accelerator B can analyze and process interactions C relaying information from the systems software or the hardware and also analyze and process interactions B of the corresponding user interface and applications software with the framework middleware which may be responsive to the interactions C and vice versa to obtain an even greater understanding of the current interaction context of the computing device.

Referring further to the illustrative interaction model includes one or more indexed or otherwise searchable stores of knowledge e.g. databases lookup tables or the like which contain data relating to interactions with a computing device. In general each of the stores of knowledge is configured for keyword text and or meta data searching and sorting of the interaction data in a number of ways using currently available techniques or similarly suitable later developed techniques . The interaction data includes associations with user level characterizations of the interactions so that interactions can be understood at a high level or linked or associated with one another based on a user level understanding of the interaction e.g. even if the program code required to execute the interactions at a lower level is ambiguous or different . For example open email message may be the user level characterization associated with both an interaction involving a web based email program and an interaction involving a desktop email client even though the commands used by these programs may be different or edit the 2012 report may be the user level characterization associated with an OpenDoc rpt.doc interaction with a word processing application.

The illustrative interaction patterns and or templates knowledge store includes patterns of interactions and or interaction templates that have been previously established. Some examples of previously established patterns and or templates include sequences of interactions that the user has explicitly taught or trained the computing device how to do using automated learning by demonstration technology. The knowledge store may also include stored interaction patterns and or templates that have been similarly created by other persons such as system administrators or other users. Some examples of learning by demonstration technology and its application to workflow or task automation are described in U.S. Patent Application Publication No. 2009 0307162 to Bui et al. which is incorporated herein by this reference. The system is by no means limited to such techniques other similarly suitable methods may be used equally as well.

Interaction patterns and or templates may be established and or prioritized based on any suitable criteria for determining the relevance of an interaction to the current context such as the recency or frequency of the interactions historical usage patterns usability analyses crowd sourcing and or other methods. As a simple example if studies show that when a population of users receives a notification of a new email message the next most likely next interaction according to that population is that the user opens the new email message then a receive email notification open message may be stored as a pre defined interaction pattern or template in the knowledge store .

However if a particular user has established an interaction pattern or template e.g. via learning by demonstration technology in which a probable sequence of interactions involves for example moving the incoming message to an unread mail folder instead of opening the message then the interaction model may be likely to consider the user s own interaction pattern as more relevant to the current interaction context than the crowd sourced interaction pattern. In general many different factors can be used to determine the degree to which a particular interaction pattern or template may be relevant to the current interaction context. Those factors that are more personal to the user may be considered to be of potentially greater significance. Both the interaction patterns and or templates and indicators of their relevance to the current interaction context may be stored in the interaction model or more particularly the knowledge store .

The illustrative interaction rules knowledge store includes data that may be used by the interaction accelerator to monitor learn predict speculatively execute and or present the predicted next interactions to the user. In some embodiments the interaction rules knowledge store may include rules that pertain to the monitoring and or learning of interactions. For example rules may be defined and stored in the knowledge store that can be used to determine whether a particular monitored interaction is more or less relevant or irrelevant to a current interaction context. In addition the knowledge store may include rules that help the system determine when to initiate and conclude interaction learning.

In some embodiments the knowledge store may specify limits for the number of next interactions to predict speculatively execute or present based on various aspects of the current interaction context. As another example the knowledge store may specify limits on how far to speculatively execute a predicted interaction. For instance if a predicted interaction involves sending an email entering personal information or agreeing to a purchase or other activity that typically requires the user s approval the knowledge store may include rules that permit the predicted interaction to execute in an automated fashion up to a point that requires the user s approval.

In some embodiments interactions learned by the interaction learner module are stored in the user specific learned interactions knowledge store . The illustrative knowledge store may include data relating to interactions with the computing device that the particular user of the computing device has taught the computing device implicitly through observation by the interaction monitoring module as discussed below.

In some embodiments a semantic model knowledge store associates semantic information with interactions and or the associated arguments and parameters. The semantic information may include the user level characterizations of interactions mentioned above as well as associations between or among interactions arguments parameters and aspects of the current interaction context that enable the system to better understand what the user is seeing and or doing with the computing device at a given point in time at a high level.

As an example when a user accesses content using a web browser semantic information associated with the actual content viewed by the user may be stored in the semantic model . For instance rather than simply recording that the user opened a web browser and followed a particular URL the system may obtain semantic information associated with the particular web page where the content has been previously tagged with such information using a markup language such as HTML XML RDF or OWL . Such semantic information may provide further details about the content actually displayed by the web browser at a particular point in time.

As another example parameters and or arguments may have different semantic meanings depending on the context in which they are used. For instance as shown in in the context of an email message the argument Caf Z may simply be a topic that was mentioned in the body of the email message but in the context of a calendar appointment the argument Caf Z may indicate the location of a meeting and in the context of a restaurant reservations software application Caf Z may indicate the name of a particular restaurant at which the user desires to make a reservation. Such associations of semantic information with particular arguments and or parameters of an interaction are stored in the semantic model in some embodiments.

In some embodiments the semantic model includes a semantic visualization mechanism by which the interaction accelerator is able to determine what content is displayed at particular locations on a display screen of a computing device at which the user s gaze may be focused without potentially resorting to instrumented applications. Some examples of content that can be detected and analyzed by the semantic visualization mechanism include text graphics images and interactive user interface elements such as buttons knobs fill in form fields scroll bars and the like.

In some embodiments screen scraping and or other technologies may be used to determine content displayed at a particular location e.g. x y coordinates on the display while gaze tracking technology mentioned elsewhere herein may be used to identify the on screen location s to be analyzed. The semantic model then associates semantic meanings with the content identified by the semantic visualization mechanism. For example if the semantic visualization mechanism determines that the user is focusing on a send button shown on the display screen the semantic model may associate semantic information such as email message sent with the interaction. Thus in some embodiments detailed information about visually displayed content that currently has the user s attention and its semantic meaning is collected analyzed and stored in the interaction model for use in determining the current interaction context.

In some embodiments a probabilistic model knowledge store includes data relating to the likelihood or probability that certain interactions logically follow other interactions and or the likelihood or probability that certain interactions may be desired to be executed with certain combinations of arguments or parameters. For instance where a calendar interaction follows a view email message interaction the system may infer based e.g. on the pre defined interaction patterns templates the user specific learned interactions the user s past history or frequency of performing similar interactions etc. that the next logical interaction is to create a new calendar appointment using information contained in the email message. Further the specific information to be pulled from the email message and incorporated as inputs into the calendar appointment may be informed by the probabilistic model . For instance the interaction accelerator may have recently concluded based on gaze tracking and semantic visualization inputs that it is highly likely that a proper noun that the user was staring at on the display is the name of a restaurant that corresponds to the desired location of the meeting and may update the probabilistic model accordingly. Based on the high likelihood of the association of the stared at proper noun with the meeting location as determined by the interaction accelerator the proper noun may be instantiated as the meeting location in the calendar appointment.

However if information in the email message is ambiguous the system may generate a number of possible subsequent interactions using various permutations of the ambiguous information. As an example if the subject line of the email message includes the word Paris the system may generate a number of predicted interactions including a calendar appointment with John Paris as an attendee another calendar appointment specifying Paris France as the location and yet another calendar appointment specifying the Paris Ill. factory as an agenda item. The probabilistic model knowledge store may contain information that enables the system to determine e.g. based on past interactions current context information etc. that of the three predicted interactions creating a calendar appointment with John Paris as an attendee is the most likely desired next interaction. As described in the aforementioned U.S. Patent Application Publication No. 2009 0307162 to Bui et al. a Logical Hidden Markov Model is one example of a probabilistic model. The present disclosure is in no way limited to that technique rather any similarly suitable probabilistic model s may be used.

In some embodiments a contextual model stores information relating to real time interactions with the computing device e.g. interactions that are monitored by the interaction monitoring module . In some embodiments the contextual model includes a short term memory STM model and a long term memory LTM model which organize past interactions according to their temporal proximity to the current interaction context.

In general interaction data is initially stored in the STM and then transferred to the LTM as it ages. In some embodiments an assumption is made that information in the STM is the most relevant to the current interaction context however information in the LTM may also be considered pertinent. For instance the LTM may be considered more relevant for users who use the computing device or a particular feature thereof more infrequently. Rules for determining when interaction data has aged sufficiently to initiate a transfer from the STM to the LTM or for determining the extent to which the STM and or LTM may be used to influence the current interaction context can be configured for individual users and or according to the requirements of a particular design or implementation of the system and stored in the interaction model e.g. in the rules knowledge store .

Referring now to an illustrative data model for at least a portion of the interaction model is shown. The data model includes an interaction knowledge store and an arguments parameters knowledge store . The illustrative interaction knowledge store includes data relating to interactions such as an associated software application an indication of whether the interaction was initiated by the user or the system the program instruction command function or procedure involved in the interaction user level semantic information associated with the interaction the arguments and or parameters associated with the interaction or a reference or pointer thereto and an indicator of the preceding interaction or a reference or pointer thereto . The illustrative arguments parameters knowledge store includes data relating to arguments and or parameters e.g. input data values function calls procedures variables etc. user level semantic information and a reference or pointer to the interaction s with which the particular arguments or parameters are associated.

The interaction knowledge store and arguments parameters knowledge store are associated with one another according to a many to many relationship paradigm. In other words the data model contemplates that interactions can be executed multiple times with different combinations of parameters and arguments and that the same parameters and arguments may be relevant to multiple different interactions albeit with different semantic meanings potentially . As a result interactions can be generalized for applicability to multiple various interaction contexts e.g. using different arguments and or parameters and interactions involving specific combinations of arguments or parameters can be analyzed for applicability to the current interaction context. In addition the illustrative data model includes associations between interactions and preceding interactions. Accordingly in the illustrative embodiment potential next interactions can be identified based on the preceding interactions data.

In the specific example shown in the interaction knowledge store contains data relating to an interaction that is made up of a series of atomic interactions including a notification of a new email message generated by an email application of the computing device followed by an opening of a new calendar appointment by the user followed by use of a web browser by the user to visit and use a restaurant reservation software application to make a reservation at a restaurant whose name appeared in the body of the email message and in the location field of the calendar appointment. The gaps between the numbers identifying the interactions e.g. IDs 998 1000 1003 illustrate that in some embodiments intervening non relevant interactions may be ignored or discarded by the system e.g. during interaction monitoring and or analysis .

Referring now to an illustrative method executable as computerized programs routines logic and or instructions by one or more of the various modules of the system e.g. the interaction monitoring module to monitor interactions with the computing device is shown. At block the method monitors interactions of various kinds with the computing device including potentially system level events that relate to software applications and user level actions with a variety of different software applications as described above. In some embodiments interaction monitoring may run continuously e.g. as a background process while in other embodiments an affirmative action of the user may be required to turn the interaction monitoring on or off.

At block the method analyzes any one or more aspects e.g. knowledge stores of the interaction model to determine whether a given interaction e.g. application or system level event or user action is one that may be of interest for learning or acceleration purposes. For instance as mentioned above interactions that are considered less relevant to the current context may be ignored by the method . On the other hand if a match is found between a monitored interaction and information contained in the interaction model the method may proceed to either block or block .

At block the method determines whether to initiate interaction learning. Interaction learning may be initiated and concluded explicitly by the user e.g. by activation of a user interface control input of start and end control sequences etc. or implicitly by the system e.g. in response to the current interaction context and or information in the interaction model . The interaction learner module is invoked in response to the initiation of interaction learning. In some embodiments interaction learning records and analyzes streams of interaction data received from instrumented software applications or from the interaction accelerator B. Analysis of the data streams is performed using one or more automated learning techniques. The aforementioned Bui et al. U.S. Patent Application Publication No. 2009 0307162 describes some examples of techniques for learning interactions utilizing computerized algorithmic learning processes including inference methods mixed initiative or active learning synchronous and asynchronous question and answer techniques and learning by using knowledge base queries such as queries to the knowledge stores to complete workflows. These and or other techniques may be used by the interaction learner module . However the present disclosure is in no way limited to the foregoing techniques rather any similarly suitable automated learning methods may be used.

At block the method determines whether input acceleration should be initiated based on a monitored interaction or more generally the current interaction context. For example input acceleration may be initiated if a monitored interaction corresponds to one or more interactions stored in the interaction patterns templates the user specific learned interactions and or the contextual model .

In some embodiments system utilization data which may be obtained e.g. from hardware or systems software is used to determine whether and when to automatically initiate continue or conclude input acceleration. For instance in some embodiments input acceleration may proceed if the computing device has a sufficient number or percentage of idle resources e.g. in the range of about 15 40 depending on the system configuration but may be at least temporarily suspended of the system utilization is closer to capacity. Rules pertaining to system utilization may be embodied in the knowledge store .

The input acceleration module is invoked if the method determines based on the applicable criteria that input acceleration should be initiated. If the method does not initiate either interaction learning or input acceleration then the method returns to block and continues monitoring user actions and system and or application level events. Although not specifically shown in it should be appreciated by those skilled in the art that in some embodiments interaction monitoring can continue even while interaction learning or input acceleration proceeds in other words the monitoring of interactions can occur concurrently or in parallel with any interaction learning or input acceleration that may have been initiated or which is in progress. As such in some embodiments it is possible that multiple threads of input acceleration can be initiated within a short span of time. In such event a later initiated input acceleration process replaces the earlier initiated input acceleration process es and modifies the earlier input acceleration processes to account for additional interaction data inputs e.g. applications instructions commands parameters arguments that may be associated with the later interaction.

Referring now to an illustrative method executable as computerized programs routines logic and or instructions by one or more of the various modules of the system e.g. the input acceleration module to predict and accelerate interactions with the computing device is shown. In the illustrative embodiments the method is initiated at block of as described above.

At block the method predicts one or more interaction pipelines each of which represents an interaction or series of interactions that may be desired by the user based on the current interaction context which includes information gleaned from the interaction monitoring of block of and or the interaction model as discussed above . For instance in some embodiments the method determines based on the interaction model a number of stored interaction pipelines e.g. patterns templates or learned interactions that contain or match at least a portion of a currently monitored interaction and follows each of the matched stored interaction pipelines to predict next interactions or interaction pipelines. In other words prediction of a subsequent interaction can involve predicting e.g. concurrently multiple interaction pipelines and or predicting subsequent interactions within a given interaction pipeline.

The aforementioned Bui et al. U.S. Patent Application Publication No. 2009 0307162 describes some examples of techniques for comparing monitored interaction data to stored patterns or templates. Those techniques and or others may be used by the method but the method is not limited thereby. Other similarly suitable techniques may be used equally as well.

In some embodiments semantic information associated with a monitored interaction may be analyzed and compared with associations stored in the semantic model to determine a next logical interaction. In some embodiments where there are multiple possible subsequent interaction pipelines or multiple possible subsequent interactions in a given pipeline the probabilistic model may be accessed to determine which of the possibilities may be the most suitable candidates.

In some embodiments the contextual model may be accessed to determine previous interactions that may be relevant to the current interaction context and or to determine previous interactions that may be appropriate or useful for predicting next interactions or next interaction pipelines. For example the STM may be accessed to determine e.g. at a semantic level various interactions that the user is currently performing or has recently performed at the computing device potentially involving multiple different software applications . The interaction patterns templates and or the user specific learned interactions then may be accessed to look for a pattern template or learned interaction or interaction pipeline that includes or closely matches one or more of the interactions or interaction pipelines determined from the STM . If a pattern template or learned interaction or is found in the knowledge store or the knowledge store that includes or closely matches one or more of the interactions or interaction pipelines obtained from the STM then one or more next interactions or interaction pipelines may be predicted in view of one or more interactions that follow the matching interaction in the matching pattern template or learned interaction.

In some embodiments interactions or interaction pipelines may be predicted based on implicit or passive behavior of the user with respect to the computing device. For instance the computing device may be equipped with a proximity sensor that can monitor the user s movements even though the user may not be physically contacting the computing device. As an example a proximity sensor may detect that a person s hand or finger is moving closer to or farther away from a particular control application window or content displayed on a display screen of the computing device.

Based on the inputs from the proximity sensor the predicted interactions or interaction pipelines can be localized based on the user s movements or projected movements toward or away from such display elements. For instance if multiple windows are currently open on the display e.g. a calendar window an email window a document processing window and a web browser and the proximity sensor determines that the user s hand is moving toward the web browser window and away from the other windows the predicted interactions may include a number of possible interaction pipelines each of which involves use of the web browser but with each pipeline involving a different combination of inputs e.g. parameters and or arguments .

As noted above an interaction pipeline includes a number of atomic interactions e.g. user actions system level events and or application level events that are executable in a logical order e.g. sequentially by the computing device in an automated fashion e.g. on behalf of the user or without the user s explicit or active involvement . Interactions in a pipeline are generally those that logically follow one another according to a stored interaction pattern or template or a learned interaction for example. In some cases the interaction pipeline may include interactions that use as inputs the output of one or more prior interactions in the pipeline or data from other pipelines. Alternatively or in addition one or more interactions in a pipeline may use as inputs the same inputs as other interactions or pipelines.

Some examples of interaction pipelines that can be automated by the system are shown in . Referring to an exemplary predicted next interaction pipeline involves the following atomic interactions opening a new calendar appointment and inserting the name of the sender of the email message read by the user at interaction into the attendee field of the appointment form. As shown by the dashed arrow the system may pre cognitively e.g. before the user becomes aware of the email message proceed directly from the system level interaction of receiving an email message to the predicted interaction e.g. based on the content of the email message and or other aspects of the current interaction context in some embodiments.

Similarly an exemplary next interaction pipeline involves opening a new calendar appointment inserting the email sender s name into the attendee field and inserting the content of the email message into the description field of the appointment. An exemplary interaction pipeline involves launching a restaurant reservations application. On a mobile device this may involve simply activating an icon associated with the restaurant reservations application. On a desktop or laptop device however it may require launching a web browser and entering the Uniform Resource Locator URL for the restaurant reservations application e.g. opentable.com or similar for example. The interaction pipeline also involves using the restaurant reservations application to create a dinner reservation based on information contained in the email message of interaction and or information contained in the calendar appointment of interaction or interaction . Another exemplary next interaction pipeline involves returning to the email application and initiating a reply to the email message of interaction . The interaction pipeline may further involve incorporating data from the results of the interaction into the email reply e.g. the dinner reservation details . Referring to exemplary predicted next interactions and each similarly involve a pipeline of atomic interactions.

Referring further to the method begins speculatively executing each of the predicted interaction pipelines generated at block described above in response to the current interaction context. In general this means that the predicted interaction pipelines are at least partially executed e.g. in an automated fashion by the input acceleration module even though the method does not yet know whether the user desires them to be executed. In some embodiments intelligence gleaned from the interaction model may be used by the method to direct the speculative execution process. For example the number of predicted next interactions that are speculatively executed may be determined based on the current interaction context and or other factors.

Some embodiments of the method may compute or estimate the time required for a user to perform an interaction manually compare the computed or estimated time to a computed or estimated time for the system to perform the same interaction and then based on the time required for user performance e.g. if the time for the user to perform is significantly longer than the time it would take for the system to perform the same interaction proceed with execution of the interaction with or without affirmation from the user. Such a feature may be useful in contexts in which time limits are desired or imposed on users for completing an interaction for example during the purchase of theatre or concert tickets through an online ticket broker financial transactions and online auctions.

More generally in some embodiments the method is implemented using program optimization techniques parallel processing techniques which may enable portions of the method to run on parallel hardware for example and or other techniques for expediting the speculative execution process so that the predicted next interaction pipelines can be presented to the user in a timely manner e.g. while they are still relevant to the current interaction context .

For instance in accordance with the pipelining paradigm it is not always necessary for predicted next interactions or interaction pipelines to fully complete before other predicted interactions or pipelines begin executing. Predicted interactions and or interaction pipelines can execute concurrently in some embodiments. Where multiple predicted interaction pipelines use the same inputs embodiments of the method can begin executing a subsequent interaction as soon as the required inputs are available e.g. as soon as another interaction completes its memory access rather than waiting until execution of the other interaction has finished.

Further in accordance with a branch prediction paradigm where the speculative execution involves executing multiple interactions or interaction pipelines of the same type with different parameters or arguments for example it is not always necessary for the method to wait for affirmation from the user as to the actual desired parameters or arguments or to wait for some other condition to be satisfied . Rather some embodiments of the method are configured to make a determination as to which branch e.g. interaction pipeline and combination of parameters or arguments may be the most suitable given the current interaction context and or other factors and based on that determination begin executing that pipeline in an automated fashion.

While speculative execution of the predicted interactions can theoretically continue until an interaction pipeline is fully executed as discussed above interaction pipelines are typically only executed to a point that would be acceptable to the user. Further as it is not yet known whether the user actually desires the predicted interaction predicted interactions are typically executed only to a point at which they remain innocuous or reversible if not selected by the user.

Some examples of interactions that can be speculatively executed include conducting an online search e.g. using an Internet search engine or a local search mechanism such as a document search or email find feature following a URL inserting data into appropriate fields of a fill in form opening an email message or document launching a software application and or others. For instance speculative execution may include automatically preparing a suggested reply to an email message or filling out an online form but not sending the reply or submitting the form without the user s approval.

Moreover in some embodiments usability studies and or other information may indicate user preferences of various populations which may be applied to the speculative execution. For instance in some populations users may find it disconcerting if a computing device executes more than two or three interactions without prompting the user for input. As such speculative execution may be subject to such limits in some embodiments.

Referring further to the method generates the user selectable representations which embody the speculatively executed predicted interactions at block . Inasmuch as the speculatively executed predicted interactions are not likely to have been fully executed the interactive representations embody the results of the speculatively executed interactions up to the point at which execution was at least temporarily suspended e.g. pending user approval in some embodiments. For instance in some embodiments the arguments and or parameters associated with a speculatively executed interaction may be concatenated and stored in a string variable associated with a thumbnail image or graphical icon user interface control or other suitable indicator that represents the speculatively executed interaction such that if the thumbnail image is selected by the user the string can be parsed and execution of the interaction can continue with the appropriate arguments and or parameters.

In some embodiments the interactive representations may be pre constructed by monitoring running applications and saving snapshots of prior executions of the predicted interactions in the interaction model . For example a thumbnail image representing the home page of an entity may be constructed from a saved snapshot e.g. a screen capture of a prior visit to the entity s homepage. Similarly a thumbnail image representing a software application may be constructed from a saved snapshot of a prior interaction with the software application. Although not directly corresponding to the output of a current speculatively executed predicted next interaction such a thumbnail should be sufficient to allow the user to identify the speculatively executed predicted next interaction especially when the image is reduced in size and fidelity to create the thumbnail image.

In other embodiments particularly embodiments using instrumented applications and or the interaction accelerator middleware B thumbnail images may be generated from a complete description of the speculatively executed interaction s execution state. In such embodiments information corresponding to the current interaction context may be associated with the thumbnail image. Generally speaking the presentation of the predicted interactions and or the interactive representations may take any suitable form of which the system is capable including but not limited to textual graphical and or audio output. Accordingly user selection of any one of the representations may be accomplished in any way of which the system is capable including clicking touching pointing gestures motion gaze or any other suitable selection or activation methods e.g. active explicit or passive implicit methods . For example U.S. patent application Ser. No. 13 399 210 to Senanayake et al. filed Feb. 17 2012 describes a heads up display input device that may be used in some embodiments.

Referring to block in some embodiments information relating to the current interaction context may be used to inform the method s mode of presenting the user selectable representations to the user. In other words the method may determine how to present the representations based on the current interaction context. For instance in some embodiments some of the interactive representations may be presented more prominently in terms of e.g. ordering location sizing highlighting color etc. than others to reflect a likelihood of the corresponding predicted interaction being desired by the user as determined by the method and such presentations may be adjusted in real time as the current interaction context changes.

As discussed above in some embodiments inputs from hardware devices such as sensors may be incorporated into the current interaction context. In some embodiments one or more optical sensors driven by gaze tracking technology incorporated into the system may track the user s gaze relative to various locations on a display screen of the computing device. Depending on the location of the user s focus as determined by e.g. optical sensors the method may adjust the presentation of the interactive representations .

For example if it appears to the gaze tracking technology that the user s attention is directed to a particular representation the method may increase the prominence of that representation e.g. by fish bowl zooming or foregrounding while potentially decreasing the prominence of other representations . Some illustrative examples of gaze tracking technology that may be incorporated into the system are described in U.S. patent application Ser. No. 13 158 109 to Senanayake et al filed Jun. 10 2011 and U.S. patent application Ser. No. 13 399 210 to Senanayake et al filed Feb. 17 2012 however this disclosure is not limited thereby. Other suitable gaze tracking technology may be used equally as well.

The presentation of user selectable representations may similarly be adjusted in real time in response to inputs from proximity sensors of the computing device e.g. in response to a user s movement toward or away from specific portions of a display . The foregoing describes some examples in which the method can adapt the presentation of the representations based not only on explicit or active interactions of the user but also in response to implicit or passive user interactions with the computing device such as those that may be detected by gaze tracking proximity sensing and or other sensing technologies.

Referring now to an exemplary embodiment of interactive representations is shown. The illustrative display is a display screen of a computing device. The display includes a portion that displays information relating to a prior or current interaction with the computing device e.g. a calendar appointment . User selectable representations generated by the input acceleration module as described above are displayed in an area of the display that overlays part of the display portion . The interactive representation is displayed more prominently than the representations thereby reflecting a judgment made by the method that the predicted interaction corresponding to the representation is more likely than the others to be desired by the user as discussed above.

In the illustrative embodiment the representations are arranged in order according to inputs that are required by the speculative execution of the corresponding predicted interactions. The first representation listed in left to right order embodies an interaction that involves using a mobile device application to find a restaurant that has a table available for a party of three on the date indicated in the calendar appointment where the party of three input is derived from the list of attendees on the calendar appointment . In other words the representation embodies the speculative execution of the Find Restaurant mobile application using the calendar appointment data as inputs.

The next representation listed in left to right order corresponds to a predicted interaction that also uses as inputs the date and party of three information gleaned from the calendar appointment and involves the Find Restaurant mobile application. The interaction also uses as inputs some of the results of the speculative execution of the interaction namely the name of a restaurant found as a result of the speculative execution of the interaction and the time at which the restaurant was determined by the speculative execution of the interaction to have a table available for a party of three.

The interactive representation corresponds to a predicted interaction that uses inputs from the calendar appointment the interaction and the interaction . That is the interaction applies the information accumulated during speculative execution of the interactions and indicating that a reservation has been made at a particular restaurant at a particular date and time to predict a subsequent interaction that involves generating a map or driving directions and traffic information for the user s travel to the identified restaurant at the specified time. In this example as well as others the system is able to relieve the user of the task of re entering or copying and pasting inputs that are already available to the computing device as having been entered by the user or generated by the system in other applications or at other locations on the user interface .

In some embodiments for example on computing devices having small display screens such as smartphones and other mobile electronic devices the display area for the user selectable representations may be scrollable so that all of the representations need not be displayed at the same time. As indicated by the graphical arrows which may be embodied as user interface controls the display may be configured to scroll the user selectable representations e.g. in an angular fashion e.g. to simulate a rotating tray or lazy susan type apparatus . In other embodiments more conventional horizontal or vertical in other embodiments scrolling may be used.

Upon selection activation of a user selectable representation by the user execution of the predicted interaction may continue automatically or upon a further explicit approval by the user as may be the case in interactions that involve financial transactions for example . User selectable representations that are not selected by the user and or data relating thereto may be stored in the interaction model for future consideration by the system or discarded while the results of the speculative execution of the unselected predicted next interactions may be canceled deleted cleared or otherwise nullified according to the conventions of the particular software application s involved.

Referring further to at block the method updates the interaction model in accordance with the results of the presentation of the user selectable representations . In some embodiments the user s responses to the options presented at block are analyzed by the system in order to adaptively modify one or more aspects of the interaction model . For instance in some embodiments likelihood scores or probabilities associated with interaction patterns or templates stored in the knowledge store and or learned interactions stored in the knowledge store may be updated based on the selection or lack thereof made by the user at block . Following block the illustrative method returns to block where interaction acceleration may be performed based on the new current interaction context which may include interaction s that occurred at block and or other interactions monitored by the system .

Referring again to examples of accelerated interactions are shown in comparison to a more typical interaction in which interaction acceleration has not been applied . Referring to the typical interaction includes a number of user performed interactions and a number of system or application level interactions . The typical interaction shows that a number of interactions with a computing device can be required to execute even simple tasks such as adding a new appointment to an online calendar. In addition the amount of time required to complete a multiple interaction task is proportional to the number of interactions that are required by the task.

The accelerated interaction also includes a number of user interactions and a number of system level interactions however the number of user interactions is reduced as a result of the interaction acceleration. A number of interaction pipelines are predicted in response to the go to calendar interaction .

Each of the interaction pipelines is predicted based on the current interaction context which may include the system s activity of receiving a new email message and or the user s activity of reading the email message and accessing a calendar application . Once the user selectable representations of each of the predicted interaction pipelines are presented at the user simply selects the desired next interaction edits the input data as may be needed and accepts the interaction . In response the system completes the execution of the accepted interaction e.g. by saving a new calendar appointment to memory saving a dinner reservation to memory or sending a reply to the email message .

The branch prediction techniques discussed above can be used to limit the number of predicted interactions to those deemed most likely to be relevant to the current interaction context and or to similarly limit the number of speculatively executed interactions. Moreover the pipelining techniques discussed above can be used to increase the throughput of the interaction accelerator . For example any of the predicted interaction pipelines may begin executing as soon as the interaction pipeline is finished accessing the read email message and the predicted pipelines can be executed in any suitable order.

Referring to the exemplary accelerated interaction illustrates an interaction pipeline that includes multiple instances of input acceleration. As in the above examples the accelerated interaction includes a number of user performed interactions and a number of system or application level interactions . In response to the current interaction context which may include among other things the interactions at least the predicted interactions are generated by the system using arguments and or parameters gleaned from the current interaction context. e.g. the read email message .

In this example the predicted interactions involve the same software application and task create a dinner reservation but different arguments and or parameters e.g. input data . Hypothetically at interaction the user selects the option corresponding to the predicted next interaction . The predicted interaction involves creating a restaurant reservation using a particular combination of arguments and or parameters A some of which may have been obtained from the original email message and others of which may be determined during the process of speculatively executing the interaction .

For example the arguments and or parameters A may include the name of a restaurant contained in the original email message as well as date time and number of persons information generated during speculative execution of the interaction . In response to the user s selection of reservation A the system uses the parameter argument combination A as input to the next iteration of input acceleration which generates at least the predicted next interactions .

The dashed arrow illustrates that in some embodiments branch prediction techniques may be employed to determine that the user is highly likely to accept the reservation A that the potential adverse effect of the system doing so on behalf of the user if the user does not agree is minimal or easily reversible and that interaction is the most likely next interaction to follow the interaction . In such event the system may proceed at least speculatively directly from interaction to interaction e.g. without the user s direct involvement rather than from interaction to interactions .

Similarly the dashed arrow indicates that in some embodiments the system may determine based on the current interaction context that the next most likely interaction desired by the user after creating the calendar appointment A is to go ahead and look for available tickets e.g. for a movie following dinner . If the system determines that the potential adverse effect of the system doing so on behalf of the user is minimal or easily reversible the system may at least speculatively proceed directly e.g. without the user s involvement from interaction to interaction .

Referring now to an illustrative embodiment of the system is shown. Portions of the system are embodied in a computing device . Portions of the system may be embodied as a stand alone software application or may be embedded in or accessed by one or more other applications. For example portions of the system may be incorporated into other systems or interactive software applications. Such applications or systems may include for example operating systems middleware or framework software and or applications software. Further the system may be local to a particular computing device or portions of the system may be distributed across multiple computing devices .

The illustrative computing device includes at least one processor memory and an input output I O subsystem . In the illustrative embodiment the computing device is embodied as a personal electronic device such as a mobile or handheld computing device smartphone personal digital assistant laptop computer tablet computer or desktop computer. In other embodiments the computing device may be embodied as any type of computing device such as a server an enterprise computer system a network of computers a combination of computers and other electronic devices or other electronic device.

The illustrative processor includes one or more processor cores e.g. microprocessors but typically at least two cores . Although not specifically shown it should be understood that the I O subsystem typically includes among other things an I O controller a memory controller and one or more I O ports. The processor and the I O subsystem are communicatively coupled to the memory . The memory may be embodied as any type of suitable memory device such as a dynamic random access memory device DRAM synchronous dynamic random access memory device SDRAM double data rate dynamic random access memory device DDR SDRAM and or other volatile memory device.

The I O subsystem is communicatively coupled to at least one touch sensitive display e.g. a touchscreen virtual keypad etc. one or more other input or user control devices e.g. a physical keyboard or keypad a microphone hardpanel controls etc. at least one data storage one or more sensors e.g. proximity sensors accelerometers etc. at least one camera or optical sensor e.g. a multi camera sensor system other output devices e.g. a speaker LED display screen etc. one or more other peripheral devices e.g. GPS transceiver sound graphics or media adaptors etc. and at least one network interface .

The network interface communicatively couples the computing device to one or more remote computing devices via one or more networks . The network s may include a local area network wide area network personal cloud enterprise cloud public cloud and or the Internet. Accordingly the network interface may include a wired or wireless Ethernet adapter Wi Fi adapter or other suitable device s as may be needed pursuant to the specifications and or design of the particular network s . The remote computing device s may be embodied as any suitable type of computing device such as for example a server an enterprise computer system a network of computers a combination of computers and other electronic devices or other electronic devices.

The data storage may include one or more hard drives or other suitable data storage devices e.g. memory cards memory sticks and or others . In the illustrative embodiment portions of systems software e.g. an operating system etc. framework middleware e.g. APIs object libraries etc. interaction monitor learner accelerator and interaction model reside in the data storage . Portions of the systems software the framework middleware the interaction monitor learner accelerator and the interaction model may be copied to the memory during operation for faster processing or other reasons. Further in some embodiments portions of any of the systems software the framework middleware the interaction monitor learner accelerator and the interaction model are specially configured for use in connection with a particular hardware platform or configuration such as a particular implementation of a mobile computing device or a particular mobile computing device operating system.

In some embodiments the interaction monitor learner accelerator may include a local portion which may include a client application or front end user interface and a back end or server application and a remote portion which may similarly include portions of the client application or front end user interface and back end or server application . Similarly the interaction model may include aspects that are local to the computing device such as user specific learned interactions while other aspects of the interaction model are located remotely and accessible via the network s . For example some aspects of the interaction model that may be stored remotely include interaction patterns and templates and or learned interactions that may have applicability to a broader population of users. Such interaction patterns templates and or learned interactions may for instance be uploaded to an app store or market for sharing sale or exchange with other computing device users.

The computing device may include other components sub components and devices not illustrated in for clarity of the description. In general the components of the computing device are communicatively coupled as shown in by one or more signal paths which are represented schematically as bidirectional arrows. Such signal paths may be embodied as any type of wired or wireless signal paths capable of facilitating communication between the respective devices and components. For example the signal paths may be embodied as any number of wires wireless communication channels intervening devices and or other components.

In the foregoing description numerous specific details are set forth in order to provide a more thorough understanding of the present disclosure. It will be appreciated however by one skilled in the art that embodiments of the disclosure may be practiced without such specific details. In some instances details such as control structures and full software instruction sequences have not been shown in order not to obscure the disclosure. Those of ordinary skill in the art with the included descriptions should be able to implement appropriate functionality without undue experimentation.

References in the specification to one embodiment an embodiment an illustrative embodiment etc. indicate that the embodiment described may include a particular feature structure or characteristic but every embodiment may not necessarily include the particular feature structure or characteristic. Moreover such phrases are not necessarily referring to the same embodiment. Further when a particular feature structure or characteristic is described in connection with an embodiment it is submitted that it is within the knowledge of one skilled in the art to effect such feature structure or characteristic in connection with other embodiments whether or not explicitly described.

Embodiments in accordance with the disclosure may be implemented in hardware firmware software or any combination thereof. Embodiments may also be implemented as instructions stored using one or more machine readable media which may be read and executed by one or more processors. A machine readable medium may include any mechanism for storing or transmitting information in a form readable by a machine e.g. a computing device . For example a machine readable medium may include read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices and or others.

In the drawings specific arrangements or orderings of schematic elements such as those representing devices instruction blocks and data elements may be shown for ease of description. However it should be understood by those skilled in the art that the specific ordering or arrangement of the schematic elements in the drawings is not meant to imply that a particular order or sequence of processing or separation of processes is required. Further the inclusion of a schematic element in a drawing is not meant to imply that such element is required in all embodiments or that the features represented by such element may not be included in or combined with other elements in some embodiments.

In general schematic elements used to represent instruction blocks or modules may be implemented using any suitable form of machine readable instruction such as software or firmware applications programs functions modules routines processes procedures plug ins applets widgets code fragments and or others and that each such instruction may be implemented using any suitable programming language library application programming interface API and or other software development tools. For example some embodiments may be implemented using Java C C and or other programming languages.

Similarly schematic elements used to represent data or information may be implemented using any suitable electronic arrangement or structure such as a database data store table record array index hash map tree list graph file of any file type folder directory or other grouping of files header web page meta tag and or others.

Further in the drawings where connecting elements such as solid or dashed lines or arrows are used to illustrate a connection relationship or association between or among two or more other schematic elements the absence of any such connecting elements is not meant to imply that no connection relationship or association exists. In other words some connections relationships or associations between elements may not be shown in the drawings so as not to obscure the disclosure. Also for ease of illustration a single connecting element may be used to represent multiple connections relationships or associations between elements. For example where a connecting element represents a communication of signals data or instructions it should be understood by those skilled in the art that such element may represent one or multiple signal paths as may be needed to effect the communication.

While the disclosure has been illustrated and described in detail in the drawings and foregoing description such an illustration and description is to be considered as exemplary and not restrictive in character it being understood that only illustrative embodiments have been shown and described and that all changes and modifications that come within the spirit of the disclosure are desired to be protected. Further while aspects of the present disclosure may be described in the context of particular forms of computing devices and systems it should be understood that the various aspects have other applications for example in other computing devices or in any application in which it is desired to improve the human computer system interface.

