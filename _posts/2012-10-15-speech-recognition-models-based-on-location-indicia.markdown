---

title: Speech recognition models based on location indicia
abstract: Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for performing speech recognition using models that are based on where, within a building, a speaker makes an utterance are disclosed. The methods, systems, and apparatus include actions of receiving data corresponding to an utterance, and obtaining location indicia for an area within a building where the utterance was spoken. Further actions include selecting one or more models for speech recognition based on the location indicia, wherein each of the selected one or more models is associated with a weight based on the location indicia. Additionally, the actions include generating a composite model using the selected one or more models and the respective weights of the selected one or more models. And the actions also include generating a transcription of the utterance using the composite model.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08831957&OS=08831957&RS=08831957
owner: Google Inc.
number: 08831957
owner_city: Mountain View
owner_country: US
publication_date: 20121015
---
This application claims the benefit of U.S. Provisional Application Ser. No. 61 678 277 filed on Aug. 1 2012 the entirety of which is hereby incorporated by reference as if fully set forth therein.

Speech recognition typically relies on speech recognition models of the language spoken. However for a given language or dialect of a given language words may be used differently depending on where the words are uttered. For example when a person is in their living room they may frequently use words relating to TV shows and control of media players whereas when the person is in their kitchen they might use words relating to types of food or cooking. Typical language models used in speech recognition do not typically account for the location specific context where words are spoken particularly for variations of word use within a building such as a home.

In general an aspect of the subject matter described in this specification may involve an automated speech recognition engine ASR that performs speech recognition using models that are based on where within a building a speaker makes an utterance. In some implementations the speech recognition models may be composite models based on two or more context specific models. The ASR engine may transcribe the utterance into text for example to be used for a voice query or convert the utterance into a command that can be executed by a processor.

In some aspects the subject matter described in this specification may be embodied in methods that include the actions of receiving data corresponding to an utterance and obtaining location indicia for an area within a building where the utterance was spoken. Further actions include selecting one or more models for speech recognition based on the location indicia wherein each of the selected one or more models is associated with a weight based on the location indicia. Additionally the actions include generating a composite model using the selected one or more models and the respective weights of the selected one or more models. And the actions also include generating a transcription of the utterance using the composite model.

Another aspect of the subject matter may be embodied in methods that include the actions of receiving an utterance at a client device and obtaining at the client device location indicia for an area within a building where the utterance was spoken. The actions also include communicating from the client to device to a server data corresponding to the utterance and the location indicia for the area within the building where the utterance was spoken. And the actions include receiving at the client device a transcription of the utterance. In some aspects the transcription of the utterance was generated using a composite model and the composite model was generated using one or more models and respective weights of the one or more models that were selected based on the location indicia.

Other versions include corresponding systems apparatus and computer programs configured to perform the actions of the methods encoded on computer storage devices.

These and other versions may each optionally include one or more of the following features. For instance some implementations involve receiving data corresponding to the utterance from a client device and receiving location indicia for the area within the building where the utterance was spoken from the client device. The location indicia may be location data based on short range wireless radio transmissions received at the client device.

Some implementations involve generating one or more candidate transcriptions of the utterance using a location independent language model and then based on comparing the one or more candidate transcriptions with phrases in one or more location dependent language models identifying one or more candidate areas within the building.

Certain implementations involve receiving data corresponding to the utterance from a processing system at the building and receiving location indicia for the area within the building where the utterance was spoken from the processing system at the building. In some aspects the location indicia is location data obtained from the processing system. The processing system localizes the utterance using a microphone array arranged in the building where the microphone array is operatively coupled to the processing system.

Additionally in some implementations the selected models for speech recognition are language models and the composite models are composite language models. In some implementations the selected models for speech recognition are acoustic models and the composite models are composite acoustic models. And in some implementations the selected models for speech recognition are language models and acoustic models and the composite speech models are composite language models and composite acoustic models.

The details of one or more implementations of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other potential features aspects and advantages of the subject matter will become apparent from the description the drawings and the claims.

As described below ASR engines may bias speech recognition models or select entirely different speech recognition models based on the area e.g. room in a building e.g. the user s home where a user makes an utterance. This may involve several features. First client devices and or systems of transponders installed in buildings are used to estimate users locations as they move around the buildings and make utterances. Second during a training phase ASR engines perform speech recognition using location independent speech recognition models and collect transcriptions of utterances from the various locations throughout the buildings. Third the transcriptions can be used to generate location dependent speech recognition models which can then be used to perform speech recognition. In some aspects transcriptions from multiple users and or multiple buildings may be combined to generate aggregate location dependent speech recognition models. For example transcriptions of multiple utterances made in or near kitchens located in various buildings can be combined to generate kitchen specific speech recognition models.

As used in the present disclosure a location dependent speech recognition model is a model that is biased toward topics words noises and or acoustic signatures that are correlated with a given area of a building. An area as used in the present disclosure refers to a functional portion of a building e.g. a kitchen dining room bedroom living room bathroom office . Areas of the same type e.g. kitchens may have different sizes and acoustic characteristics e.g. reverberation in different buildings but may share similar noise signatures and vocabularies. For example a kitchen in a studio apartment may have different acoustic characteristics than a kitchen in a large single family home but may still have common noises and semantic topics. Thus a location dependent model associated with a kitchen area may include words that are related to food and cooking topics and or noises that are most often found in kitchens e.g. microwaves blenders dishwashers etc. . A location independent speech recognition model is a model that is not biased toward any particular area of a home.

Additionally a phrase or phrases spoken by a person are referred to in the present disclosure as utterances. In general utterances may contain one or more words. An utterance can represent any type of voice communication such as voice based instructions commands or actions e.g. to operate a home automation system voice based search queries dictation dialogue systems or any other input that uses transcribed speech or that invokes a software application using transcribed speech to perform an action.

Initially in the user makes an utterance i.e. Apples and Spice while located in the living room . For example the user may have a home automation system that controls an entertainment system in the living room including a music player e.g. an MP3 player and the user may intend to play the Greatest Hits album of a band called Apples Spice. In response to the utterance the client device encodes the utterance into audio signals and obtains location indicia by detecting a signal from the wireless radio transponder . The client device then transmits the audio signals and location indicia e.g. the identifier of the wireless radio transponder to the processing system via the network . The processing system is executing an ASR engine that receives the location indicia and the audio signals.

Assuming that the ASR engine has been trained the location determination component accesses the location indicia and derives an estimated location of the user when the utterance was spoken. In this example since the location determination component received the identifier of wireless radio transponder associated with the living room the location determination component would output an estimated location corresponding to the living room . This estimated location is then output to the acoustic model generator and the language model generator . The acoustic model generator selects from the set of acoustic models a location dependent acoustic model associated with the living room area. The language model generator selects from the set of language models a location dependent language model that is also associated with the living room area.

The ASR engine then uses the selected acoustic model and language model to convert the audio signals corresponding to the utterance into a transcription. In this case the language model for the living room includes the phrase Apples Spice which the ASR engine identifies as the best match for the utterance . For example the living room language model may be programmed with a user s media library e.g. movies music television shows video games etc. . It should be noted that the words apples and spice are also in the language model for the kitchen which could cause ambiguity in the absence of additional context information. Advantageously because the ASR engine bases the selection of the language model on location indicia which indicate that the user was in the living room rather than the kitchen when the utterance was spoken the ASR engine may readily resolve the ambiguity.

Finally the ASR engine may communicate the transcription Apples Spice via the output component . For example the output component may provide an interface to a media player control application executing on the processing system which in turn can cause the media player to queue up The Greatest Hits of Apples Spice album for playback.

In more detail the user is located in a building e.g. at the user s home having multiple areas e.g. the living room and the kitchen . While in the living room the user speaks an utterance into the client device e.g. Apples and Spice . The client device is an electronic device that is under control of a user and that is capable of requesting and receiving resources over the network. A client device examples of which include the device of and may include one or more processing devices and may be or include a mobile telephone e.g. a smartphone a laptop computer a handheld computer a tablet computer a network appliance a camera an enhanced general packet radio service EGPRS mobile phone a media player a navigation device an email device a wearable computer a game console an interactive or so called smart television or a combination of any two or more of these data processing devices or other data processing devices.

The user may record the utterance by for example pressing a button on the client device to initiate a dialogue before speaking speaking the utterance and then releasing the button on the mobile device. In another example the client device may be activated by the speaker s utterance. In another example the user may select a user interface control e.g. a microphone icon on the client device before speaking the utterance. As yet another example the microphone may be activated before a user speaks or may be in an always on state to allow the user to speak without manually activating the microphone. For instance a client device may use a continuous recording buffer.

The client device then converts the user s utterance into a series of audio signals e.g. samples or waveforms that may be for example two second or more snippets of relatively high quality audio such as 16 kHz lossless audio. For example the client device may create a sound file or a data stream when recording an utterance. Additionally in response to receiving the utterance the client device initiates a process to obtain location indicia for the area of the building where the utterance was spoken. Location indicia as referred to in the present disclosure are data that indicate a relative likelihood that a signal was obtained from a particular location.

In some implementations such as the example shown in the location indicia may be data describing radio signals transmitted from wireless radio transponders e.g. wireless radio transponder identifiers and signal strengths that were received by a client device located in a particular area of a building. For example wireless radio transponders may be positioned in various areas throughout the building as described below. The wireless radio transponders may be any suitable short range wireless radio system such as for example Bluetooth Near Field Communications NFC or WiFi.

Wireless radio transponders may be installed in various areas throughout a building and then associated with the respective areas of the building. For example the wireless radio transponder may be associated with the living room area and the wireless radio transponder may be associated with the kitchen area . The wireless radio transponders can be associated with different areas of the building using any suitable means. For example the processing system may provide an interface so that users can tag each wireless radio transponder as being in a certain category of room e.g. kitchen living room bedroom office dining room bathroom . In some implementations the processing system may present a web interface through which users can interact via a Web browser. In other aspects the processing system may be accessible via a graphical user interface on the client device such as a mobile application executing on the client device.

Once the client device encodes the utterance into audio signals and obtains the location indicia for the area within the building where the utterance as spoken the client device transmits the audio signals and location indicia to the processing system via a network . In some implementations the client device may establish a communication session with the processing system and may send the audio signals and location indicia to the processing system during the session. In alternative implementations the client device obtains and transmits location indicia periodically e.g. every 30 seconds every 1 minute every 5 minutes regardless of whether an utterance was received. In other implementations the client device may transmit location indicia when the client device identifies a change e.g. the client device detects i a new wireless radio transponder identifier ii a change in relative signal strengths of wireless radio transponders or iii that a wireless radio transponder has ceased to be detected. In such implementations the change in location indicia may indicate that the client device has been relocated to a different area of the building.

The processing system receives audio signals corresponding to utterances and location indicia which are used to perform speech recognition and or train location dependent speech recognition models. The processing system may be one or more computing devices e.g. servers that include one or more processors and computer readable storage media that among other capabilities convert speech to text using an ASR engine . The ASR engine may be a software implemented input output system that processes speech into text. The ASR engine may be for example software code such as a library a platform a software development kit or an object.

The processing system may be physically located within the same building as the user or may be located remotely. The network can therefore include one or more networks. The network s may provide for communications under various modes or protocols such as Global System for Mobile communication GSM voice calls Short Message Service SMS Enhanced Messaging Service EMS or Multimedia Messaging Service MMS messaging Code Division Multiple Access CDMA Time Division Multiple Access TDMA Personal Digital Cellular PDC Wideband Code Division Multiple Access WCDMA CDMA2000 General Packet Radio System GPRS or one or more television or cable networks among others. For example the communication may occur through a radio frequency transceiver. In addition short range communication may occur such as using a Bluetooth WiFi or other such transceiver. In addition in some implementations one several or all of the functions of the processing system described in the present disclosure may be performed by the client device .

The ASR engine may operate in two modes a training mode and an operational mode. In training mode the ASR engine i performs speech recognition using location independent e.g. unbiased speech recognition models and ii collects data correlating transcriptions of utterances with locations where the utterances were made to train location dependent speech recognition models. In operational mode the ASR engine performs speech recognition using the trained location dependent speech recognition models. However in some implementations the ASR engine continues to train the location dependent speech recognition models while in operational mode. The ASR engine may determine which mode to operate in based on several inputs. For example the user may be prompted to select a mode via an interface with the processing system . Alternatively or in addition the ASR engine may include a predetermined threshold e.g. a certain number of utterances in a given area or a certain number of identified words in a given area for switching between modes. In some implementations external training data may be provided to the ASR engine that may trigger the ASR engine to switch modes. For example the ASR engine may receive trained location dependent speech recognition models from an application server that aggregates location dependent speech recognition models from multiple buildings and users.

The ASR engine includes a variety of components e.g. software modules or libraries for performing speech to text conversion and training location dependent speech recognition models. In particular the ASR engine includes a location determination component that estimates the location of the client device when the utterance was made. The location determination component outputs the estimated location to an acoustic model generator and a language model generator . During training mode the acoustic model generator collects utterances for various categories of rooms to update the set of acoustic models . Also the acoustic model generator may provide a location independent acoustic model for performing speech recognition. During operational mode the acoustic model generator generates a location dependent acoustic model from a set of acoustic models . During training mode the language model generator collects utterances for various categories of rooms to update the set of language models . Also the language model generator may provide a location independent acoustic model for performing speech recognition. During operational mode the language model generator generates a location dependent language model from a set of language models . The ASR engine uses the acoustic model and the language model to convert the audio signals from the utterance into a transcription. An output component then outputs the transcription for example to the client device or to another component of the processing system e.g. a home automation system .

In more detail the location determination component estimates the location of the client device when the utterance was made based on the location indicia e.g. the signals detected from the wireless radio transponders . For example the client device may have detected wireless radio transponder and the location indicia could therefore include the corresponding wireless transponder identifier e.g. Transponder 1 and in some implementations signal strength e.g. 60 dBm . The location determination component may therefore determine that the client device was in the living room area when the utterance was spoken.

In some implementations the client device may detect more than one transponder and the signals from multiple transponders may be weighted to estimate a location. For example the client device may detect signals from two wireless radio transponders and determine the signal strengths e.g. received signal strength indicators RSSI of the signals received. The client device could transmit location indicia describing both signals to the processing system which could use the relative difference between the signal strengths to estimate the location of the client device when the utterance was spoken.

For example assume the client device detects signals from wireless radio transponder and wireless radio transponder . The client device could determine that the signal from wireless radio transponder has an RSSI of 60 dBm and the signal from wireless radio transponder has an RSSI of 63 dBm. Because the signal strength of wireless radio transponder is 3 dB more than the signal from wireless radio transponder this indicates that the signal from wireless radio transponder is twice as strong as the signal from wireless radio transponder and that the client device was probably closer to transponder than transponder . Accordingly the location determination component could weight the location associated with transponder e.g. living room area more heavily than that associated with transponder e.g. kitchen area . These weights may correspond to probabilities or likelihoods that the utterance was spoken in the associated area. Thus using a weighting based on relative signal strength the location indicia could indicate that the area where the utterance was spoken was the living room with 67 likelihood and the kitchen with 33 likelihood. Table 1 below illustrates location indicia according to this example.

The output of the location determination component can then be used to generate statistical models for performing speech recognition or to train speech recognition models. In general the ASR engine uses statistical models to process speech. The ASR engine may use an acoustic model for processing audio signals to generate sequences of sub words. Acoustic models may be implemented as for example a Hidden Markov Model HMM or a Gaussian Mixture Model GMM . In some aspects as described below the ASR engine may use a composite acoustic model generated by an acoustic model generator based on the location indicia. The acoustic model generator described in more detail below is a component that receives the output of the location determination component generates an acoustic model from a library of acoustic models and outputs an acoustic model for use by the ASR engine . In training mode the acoustic model generator may train location dependent acoustic models.

The ASR engine uses the sequences of sub words from the acoustic model as inputs to a language model . The language model processes the sequences of sub words to determine one or more words that best match the sub word sequences. For example the ASR engine may compare the sub word sequences to a vocabulary of words that are included in the language model . In some aspects as described below the ASR engine may use a composite language model generated by a language model generator based on the location indicia. The language model generator described in more detail below is a component that receives the output of the location determination component generates a language model from a set of language models and outputs a language model for use by the ASR engine . In training mode the language model generator may train location dependent language models.

Once the ASR engine generates a transcription of the utterance an output component routes the transcription to the appropriate systems. In some implementations the output component may transmit the transcription back to the client device . In other implementations the output component may send the transcription to another component executing on the processing system such as for example a home automation system a voice based search service a dictation application a word processing application or any other application that uses transcribed speech or that invokes a software application using transcribed speech to perform an action. In some implementations the output component may submit a transcription of the speaker s utterance to a search service. A search service identifies resources by crawling and indexing resources provided by the content publishers on Web sites. Data about the resources can be indexed based on the resource to which the data corresponds. Indexed and optionally cached copies of the resources that match input keywords can be retrieved and output e.g. in response to a search query.

Referring to in some implementations rather than wireless radio transponders a microphone array may be used to determine the location of the user when the utterance is spoken. As an example in the system of the user makes an utterance i.e. Apples and Spice while located in the living room . However instead of detecting the utterance with a client device a voice detection device installed in the living room detects the utterance . The voice detection device converts the utterance into audio signals. Then it transmits the audio signals and location indicia e.g. the identifier of the voice detection device and or the sound pressure level of the utterance to the processing system via the network . The processing system is executing an ASR engine that operates as described above to convert the utterance into a voice command corresponding to The Greatest Hits of Apples Spice. 

A voice detection device may be any suitable component or set of components suitable to detect an utterance convert the utterance into audio signals and communicate the audio signals and location indicia to the processing system . For example a voice detection device may be a microphone that is operatively coupled to the processing system . In other implementations a voice detection device may include a microphone an analog to digital converter a processor computer readable storage media and a network interface capable of communicating with the processing system via the network . In some implementations a voice detection device may include one or more user input output components e.g. buttons an LCD and or a presence sensitive display . The microphones may be any suitable acoustic to electric transducer for converting sound into audio signals such as unidirectional bidirectional or omnidirectional wired or wireless microphones.

Voice detection devices e.g. a microphone array may be installed in various areas throughout a building and then associated with the respective areas of the building. For example the voice detection device may be associated with the living room area and the voice detection device may be associated with the kitchen area . The voice detection devices can be associated with different areas of the building using any suitable means. For example the processing system may provide an interface so that users can tag each voice detection device as being installed in a certain category of room e.g. kitchen living room bedroom office dining room bathroom . In some implementations the processing system may present a web interface through which users can interact via a Web browser. In other aspects the processing system may be accessible via a graphical user interface on the client device such as a mobile application executing on the client device. In some aspects one or more of the voice detection devices may include directional microphones aimed at specific areas of the building in which case utterances coming from the directional microphone may be more highly correlated with a specific area.

The user may initiate the utterance by for example pressing a button on the voice detection device to initiate a dialogue before speaking speaking the utterance and then releasing the button. In another example the voice detection device may be activated by the speaker s utterance. As yet another example the microphone may be activated before a user speaks or may be in an always on state to allow the user to speak without manually activating the microphone. For instance a client device may use a continuous recording buffer.

The voice detection device converts the user s utterance into a series of audio signals e.g. samples or waveforms that may be for example two second or more snippets of relatively high quality audio such as 16 kHz lossless audio. For example the voice detection device may create a sound file or a data stream when recording an utterance.

Additionally in response to receiving the utterance the voice detection device obtains location indicia. The location indicia may include data describing an identifier of the voice detection devices and or the sound pressure level e.g. dB re 20 Pa RMS of utterances as received at the voice detection devices. For example assume the voice detection device detects the utterance with a sound pressure level of 60 dB. Assume that the voice detection device also detects the utterance but with a sound pressure level of 57 dB. Because the sound pressure level at voice detection device is 3 dB more than the sound pressure level at voice detection device this indicates that the sound pressure level at voice detection device is twice as strong as the sound pressure level at voice detection device and that the utterance was probably made closer to voice detection device than voice detection device . Accordingly the location determination component could weight the location associated with voice detection device e.g. living room area more heavily than that associated with voice detection device e.g. kitchen area . These weights may correspond to probabilities or likelihoods that the utterance was spoken in the associated area. Thus using a weighting based on relative sound pressure level the location indicia could indicate that the area where the utterance was spoken was the living room with 67 likelihood and the kitchen with 33 likelihood. Table 2 below illustrates location indicia according to this example.

In some implementations rather than using identifiers of wireless radio transponders or voice detection devices the utterances themselves may be used as location indicia. For example either the client device or the voice detection devices could generate audio signals corresponding to the utterance and transmit them to the processing system . The ASR engine could then generate one or more candidate transcriptions of the utterance using location independent speech recognition models. Then the ASR engine could compare the candidate transcriptions with phrases in one or more location dependent language models. The area associated with the best matching location dependent language model could then be used as the estimated location where the utterance was made. For example if the transcription using the location independent speech models was Apples Spice and this phrase is only found in a kitchen language model then the location could be determined to be the kitchen. If the phrase is found in multiple language models then the location estimate could weight the associated areas based on the probability of the words occurrence to generate a weighted location estimate e.g. 33 kitchen 67 living room or could choose the area associated with the language model having the highest probability for the word as the estimated location.

As discussed above a client device or a voice detection device provides location indicia to the processing system which stores the data in a suitable memory. The location determination component accesses the stored location indicia and generates an estimated location for where in the building the associated utterance was made. In particular a building area mapper component e.g. a software library or function parses the location indicia to generate the estimated location. The estimated location may be in the form of a likelihood or probability that the utterance was made in various categories of rooms. The estimated location may be output in any suitable data structure such as for example an object or set of variables.

For example assume that the location indicia include only one identifier e.g. for wireless radio transponders and or voice detection devices . The building mapper could then generate an estimated location within the building based on the area associated with the identifier. Thus if the location indicia includes an identifier for a wireless radio transponder associated with the living room area then the estimated location could be 1.0 living room.

Assume that the location indicia include two identifiers. The building area mapper could then generate an estimated location within the building based on interpolating the areas associated with the identifiers. Thus if the location indicia includes an identifier for a wireless radio transponder associated with the living room area and an identifier for a wireless radio transponder associated with the kitchen area then the estimated location could be 0.50 living room and 0.50 kitchen.

Assume further that the location indicia include two identifiers and the corresponding signal strengths e.g. the RSSI at the wireless radio transponders or the sound pressure level at voice detection devices . The building area mapper could then generate an estimated location within the building based on interpolating the areas associated with the identifiers with a weighting based on the relative signal strengths. Thus if the location indicia includes an identifier for a wireless radio transponder associated with the living room area with an RSSI of 60 dBm and an identifier for a wireless radio transponder associated with the kitchen area with an RSSI of 63 dBm then the estimated location could be 0.67 living room and 0.33 kitchen. Note that this example of determining estimated location based on signal strengths is for illustrative purposes and implementations may involve more fine tuned and subtle models to improve accuracy of the estimated location. Also while described for example purposes as receiving location indicia from only two sources in implementations location indicia could be received from any number of sources.

Upon receiving an estimated location the acoustic model generator performs operations that depend on whether the ASR engine is operating in training mode or operational mode. In training mode the acoustic model generator receives training data e.g. the audio signal for the utterance that is associated with the location indicia and trains one or more of the acoustic models stored in the set of acoustic models . Also the acoustic model generator provides a location independent acoustic model for use in performing speech recognition. In operational mode the acoustic model generator obtains e.g. selects or generates a location dependent acoustic model based on the estimated location for use in performing speech recognition.

The set of acoustic models may be stored in a file structure e.g. Network File System or in a database e.g. MySQL PostgreSQL MS SQL Server MongoDB or any other suitable data structure that can be accessed by the processing system . In some implementations the processing system may store and access the stored set of acoustic models via web services such as representational state transfer REST style services.

The acoustic models in the set may be initially populated with previously trained acoustic models that match various acoustic spaces typically encountered by users. In training mode the acoustic model generator receives audio signals associated with utterances and makes a determination whether the corresponding acoustic spaces match previously stored acoustic models. Analyzing the audio signals may provide information about acoustic characteristics of the surrounding space. Those characteristics may include the size of the room noise sources such as ventilation ducts or exterior windows and reverberation characteristics.

If the acoustic space does not match a previously stored acoustic model the acoustic model generator may initialize and adapt a new acoustic model. Based on the size of the surrounding space the acoustic model generator may also adjust an acoustic model to account for reverberation. This adjustment may be done in a variety of ways including using model adaptation such as maximum likelihood linear regression to a known target. The target transformation may have been estimated in a previous encounter at that location or may be inferred from the reverberation time associated with the space. Once the location is adequately modeled the acoustic model generator stores the acoustic model in the set of acoustic models .

In operational mode when the ASR engine performs speech recognition the acoustic model generator selects location dependent acoustic models identified by the location estimate. For example if the location estimate identifies the living room and kitchen as the likely areas where the utterance was made then the acoustic model generator selects the living room acoustic model and the kitchen acoustic model .

The acoustic model generator then generates an acoustic model using the selected location dependent acoustic models. In particular composite acoustic models can be linked to one or more base acoustic models which correspond to the acoustic models stored in the set of acoustic models . The links between the composite acoustic models and base acoustic models can be weighted. In some examples the sum of the weights of the links from one composite acoustic model can be 1 or can be normalized to 1.

The acoustic model generator may merge base acoustic models to form composite acoustic model using any suitable technique. For example assume that each acoustic model includes a set of Gaussian distributions and associated Hidden Markov Models HMMs . The acoustic model generator may generate a composite acoustic model by mapping the Gaussian distributions and HMMs using weights based on the estimated location. Thus if the estimated location is 0.67 living room and 0.33 kitchen then the Gaussian distributions and HMMs could be mapped to generate a composite acoustic model having the Gaussian distributions and HMMs of the living room acoustic model weighted at 67 and the Gaussian distributions and HMMs of the kitchen acoustic model weighted at 33 .

It will be understood that although a particular number and configuration of composite acoustic models base acoustic models and links are shown other numbers and configurations are possible. For example sufficient composite acoustic models may exist that every weighted combination of base acoustic models has a linked composite acoustic model . In some examples composite acoustic model can be linked to more or fewer base acoustic models . In some examples different link weights between composite acoustic models and base acoustic models may exist such as positive integers probabilities or dimensional distance e.g. W X Y Z values for four dimensional space .

In some implementations composite acoustic models can be created on demand such as when location estimate is received that does not correspond to a previously stored acoustic model. The composite acoustic models can persist by being stored in a memory structure accessible by the acoustic model generator . In some implementations a number of possible composite acoustic models can be pre generated. This may be used for example when pre processing time is available and or when few base acoustic models are expected.

In the set of language models stores location dependent language models including a kitchen language model a living room language model an office language model and a bedroom language model . As illustrated the kitchen language model includes food and cooking related words such as blender apples and oven the living room language model includes media and entertainment related words such as television movies and music the office language model includes office related words such as email fax and dictation and the bedroom language model includes sleep related words such as alarm sleep and lights. 

In the set of language models stores a core language model and a variety of topic specific language models relating to web browsing media and food respectively. Each of the topics may be associated with one or more areas and composite language models can be generated using the core language model and one or more of the topic specific language models. As illustrated the core language model includes general words relating to building control such as lights NC and heat the web browsing language model includes words such as shopping news and celebrities the media language model includes words such as television movie and music and the food language model includes words such as popcorn apples and oven. 

The location determination component provides an estimated location as discussed above. Upon receiving an estimated location the language model generator performs operations that depend on whether the ASR engine is operating in training mode or operational mode. In training mode the language model generator receives training data e.g. transcriptions made using location independent language models that are associated with the location indicia and trains one or more of the language models stored in the set of language models . Also the language model generator provides a location independent language model for use in performing speech recognition. In operational mode the language model generator obtains e.g. selects or generates a location dependent language model based on the estimated location for use in performing speech recognition.

The set of language models may be stored in a file structure e.g. Network File System or in a database e.g. MySQL PostgreSQL MS SQL Server MongoDB or any other suitable data structure that can be accessed by the processing system . In some implementations the processing system may store and access the stored set of language models via web services such as representational state transfer REST style services.

The language models in the set may be populated with previously trained language models that include commonly used words corresponding to categories of rooms or topics. For example a third party web service may combine transcriptions from multiple users and or multiple buildings to generate aggregated location dependent language models for potential categories of rooms e.g. kitchen living room office bedroom . In addition language models for some categories of rooms may be populated by referring to content or products owned by a user. For example the ASR engine could access titles from a user s media library e.g. movies music television shows video games etc. to populate language models for categories of rooms that might have a media player e.g. a living room or bedroom . Similar techniques could be used to populate the topic specific language models. In training mode the language model generator receives transcriptions associated with estimated locations and trains the corresponding location dependent language models and or topic specific language models using the words from the transcription.

Referring to the sample system of in operational mode the language model generator selects location dependent language models identified by the location estimate. For example if the location estimate identifies the living room and kitchen as the likely areas where the utterance was made then the language model generator selects the living room language model and the kitchen language model

The language model generator then generates a language model using the selected location dependent language models. In particular composite language models can be linked to one or more base language models which correspond to the language models stored in the set of language models . The links between the composite language models and base language models can be weighted. In some examples the sum of the weights of the links from one composite language model can be 1 or can be normalized to 1.

Although a particular number and configuration of composite language models base language models and links are shown other numbers and configurations are possible. For example sufficient composite language models may exist that every weighted combination of base language models has a linked composite language model . In some examples composite language model can be linked to more or fewer base language models . In some examples different link weights between composite language models and base language models may exist such as positive integers probabilities or dimensional distance e.g. W X Y Z values for four dimensional space .

In some implementations composite language models can be created on demand such as when location estimate is received that does not correspond to a previously stored language model. The composite language models can persist by being stored in a memory structure accessible by the language model generator . In some implementations a number of possible composite language models can be pre generated. This may be used for example when pre processing time is available and or when few base language models are expected.

Referring to the sample system of in operational mode the language model generator generates a composite location dependent language model in two steps 1 generating area specific language models from topic specific language models and 2 generating a composite location dependent language model from the area specific language models.

In the first step the language model generator creates area specific language models e.g. kitchen language model living room language model and office language model by combining topic specific language models using various weights. Each area specific language model may include the core language model to varying degrees. In some implementations the topic specific language models can be linked to one or more area specific language models . The links between the composite language models and area specific language models can be weighted. In some examples the sum of the weights of the links from one composite language model can be 1 or can be normalized to 1.

The weights may be predetermined based on empirical analysis and or they may be trained in the training mode. The weights also may be periodically updated based on a user s habits. For example if the user primarily performs web browsing in the living room then the weight for the web browsing topics could increase over time based on this usage. As another example during training the language model generator may determine whether a backend semantic service responds to a given utterance. A backend semantic service may be a function of a home automation system a search service an application or any other service that may accept voice commands. For example if a television related backend semantic service responds to a given utterance then that utterance can be identified as corresponding to a media topic related language model. If the media topic related language models are associated with the living room then this would indicate that the utterance was likely made in the living room. The language model generator can persist these weights in any suitable memory structure such as a database or file system.

As illustrated a sample kitchen language model includes weights of 75 of the food language model and 25 of the core language model a sample living language model includes weights of 50 of the media language model 25 of the web browsing language model and 25 of the core language model and a sample office language model includes weights of 50 of the web browsing language model and 50 of the core language model .

In the second step the language model generator generates a location dependent composite language model using the selected area specific language models identified in the location estimate from the location determination component . In particular composite language models can be linked to one or more area specific language models which were generated in the first step. The links between the composite language models and area specific language models can be weighted. In some examples the sum of the weights of the links from one composite language model can be 1 or can be normalized to 1.

Although a particular number and configuration of composite language models area specific language models topic specific language models and links are shown other numbers and configurations are possible. For example sufficient composite language models may exist that every weighted combination of area specific language models has a linked composite language model . In some examples composite language model can be linked to more or fewer area specific language models . In some examples different link weights amongst composite language models area specific language models and topic specific language models may exist such as positive integers probabilities or dimensional distance e.g. W X Y Z values for four dimensional space .

In some implementations composite language models can be created on demand such as when location estimate is received that does not correspond to a previously stored language model. The composite language models can persist by being stored in a memory structure accessible by the language model generator . In some implementations a number of possible composite language models can be pre generated. This may be used for example when pre processing time is available and or when few area specific language models are expected.

The language model generator may merge language models using any suitable technique. For example assume each language model includes words and associated counts e.g. frequency for each word. The language model generator could use a count merging strategy such as maximum a posteriori MAP adaptation. Such a count merging strategy could generate a probability of the word p w h as shown in Equation 1 below 

In Equation 1 hw is an n gram ending in word w with a context h c hw and c hw are the counts of hw in a first and second language model respectively and x is a constant that controls the contribution of each language models corpus to the combined language model.

Alternatively the language model generator may generate composite language models using a language model union strategy. This could generate a probability of a word p w as shown in Equation 2 below max 1 2 

In Equation 2 p w and p w are the probabilities of w in a first and second language model respectively and x is a constant that controls the contribution of each language models corpus to the combined language model.

In more detail the process begins in step when the ASR engine receives data corresponding to an utterance from a client device or a voice detection device . For example the audio signals may be for example snippets of relatively high quality audio such as 16 kHz lossless audio.

As described above in step the ASR engine then obtains location indicia for an area in the building where the utterance was spoken. For example the location indicia may be identifiers of wireless radio transponders or voice detection devices and may also include signal strengths e.g. RSSI and or sound pressure level . In some implementations the location indicia may be location data based on short range wireless radio transmissions received at a client device. Alternatively the location indicia may be generated from the utterance. In particular the ASR engine may generate one or more candidate transcriptions of the utterance using a location independent language model. Then the ASR engine may based on comparing the one or more candidate transcriptions with phrases in one or more location dependent language models identify one or more candidate areas within the building. In such implementations the ASR engine receives data corresponding to the utterance from a processing system at the building. In yet other implementations the location indicia may be location data obtained from the processing system. In such implementations the processing system may localize the utterance using a microphone array e.g. voice detection devices arranged in the building that is operatively coupled to the processing system.

Next in step the ASR engine selects one or more speech models e.g. acoustic models and or language models for speech based on the location indicia. Each of the selected speech models may be associated with a weight based on the location indicia. For example if the ASR engine received identifiers corresponding to the kitchen and the living room areas at 33 and 67 respectively then the selected speech models for the kitchen and living room would be selected and weighted accordingly.

The ASR engine then generates a composite speech model using the selected one or more speech models and the respective weights in step . The ASR engine may combine the speech models e.g. acoustic models and or language models as described in described in more detail above. Finally in step the ASR engine generates a transcription of the utterance using the composite speech model.

For situations in which the systems discussed herein collect personal information about users the users may be provided with an opportunity to opt in out of programs or features that may collect personal information e.g. information about a user s preferences or a user s current location . In addition certain data may be anonymized in one or more ways before it is stored or used so that personally identifiable information is removed. For example a user s identity may be anonymized.

Embodiments of the subject matter the functional operations and the processes described in this specification can be implemented in digital electronic circuitry in tangibly embodied computer software or firmware in computer hardware including the structures disclosed in this specification and their structural equivalents or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs i.e. one or more modules of computer program instructions encoded on a tangible nonvolatile program carrier for execution by or to control the operation of data processing apparatus. Alternatively or in addition the program instructions can be encoded on an artificially generated propagated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus. The computer storage medium can be a machine readable storage device a machine readable storage substrate a random or serial access memory device or a combination of one or more of them.

The term data processing apparatus encompasses all kinds of apparatus devices and machines for processing data including by way of example a programmable processor a computer or multiple processors or computers. The apparatus can include special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit . The apparatus can also include in addition to hardware code that creates an execution environment for the computer program in question e.g. code that constitutes processor firmware a protocol stack a database management system an operating system or a combination of one or more of them.

A computer program which may also be referred to or described as a program software a software application a module a software module a script or code can be written in any form of programming language including compiled or interpreted languages or declarative or procedural languages and it can be deployed in any form including as a standalone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program may but need not correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data e.g. one or more scripts stored in a markup language document in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules sub programs or portions of code . A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.

The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by and apparatus can also be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit .

Computers suitable for the execution of a computer program include by way of example can be based on general or special purpose microprocessors or both or any other kind of central processing unit. Generally a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. Generally a computer will also include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. However a computer need not have such devices. Moreover a computer can be embedded in another device e.g. a mobile telephone a personal digital assistant PDA a mobile audio or video player a game console a Global Positioning System GPS receiver or a portable storage device e.g. a universal serial bus USB flash drive to name just a few.

Computer readable media suitable for storing computer program instructions and data include all forms of nonvolatile memory media and memory devices including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry.

To provide for interaction with a user embodiments of the subject matter described in this specification can be implemented on a computer having a display device e.g. a CRT cathode ray tube or LCD liquid crystal display monitor for displaying information to the user and a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input. In addition a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user for example by sending web pages to a web browser on a user s client device in response to requests received from the web browser.

Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component e.g. as a data server or that includes a middleware component e.g. an application server or that includes a front end component e.g. a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification or any combination of one or more such back end middleware or front end components. The components of the system can be interconnected by any form or medium of digital data communication e.g. a communication network. Examples of communication networks include a local area network LAN and a wide area network WAN e.g. the Internet.

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other.

While this specification contains many specific implementation details these should not be construed as limitations on the scope of what may be claimed but rather as descriptions of features that may be specific to particular embodiments. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover although features may be described above as acting in certain combinations and even initially claimed as such one or more features from a claimed combination can in some cases be excised from the combination and the claimed combination may be directed to a subcombination or variation of a subcombination.

Similarly while operations are depicted in the drawings in a particular order this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order or that all illustrated operations be performed to achieve desirable results. In certain circumstances multitasking and parallel processing may be advantageous. Moreover the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.

A home automation system is a software firmware and or hardware system that integrates electrical devices in a house with each other. Home automation may include centralized control of lighting HVAC heating ventilation and air conditioning appliances and other systems. Home automation systems may also control of domestic activities such as home entertainment systems houseplant and yard watering pet feeding and or domestic robots. Devices in the home may be connected through a computer network to allow control by a processing system and may allow remote access from the internet.

Particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example the processes depicted in the accompanying figures do not necessarily require the particular order shown or sequential order to achieve desirable results. In certain implementations multitasking and parallel processing may be advantageous. Other steps may be provided or steps may be eliminated from the described processes. Accordingly other implementations are within the scope of the following claims.

