---

title: Executing transactions in distributed storage systems
abstract: A method of executing a transaction in a distributed storage system includes, for data chunks of a read set of the transaction, reading data of the data chunks of the read set through remote direct memory access and determining a validity of the read data by evaluating a version and a lock of each data chunk of the read set. For data chunks of a write set of the transaction, the method includes setting locks on the data chunks of the write set, writing data to the locked data chunks through remote direct memory access, releasing the locks of the locked data chunks, and incrementing a version number of each released data chunk.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08676851&OS=08676851&RS=08676851
owner: Google Inc.
number: 08676851
owner_city: Mountain View
owner_country: US
publication_date: 20120830
---
A distributed system generally includes many loosely coupled computers each of which typically include a computing resource e.g. processor s and storage resources e.g. memory flash memory and or disks . A distributed storage system overlays a storage abstraction e.g. key value store or file system on the storage resources of a distributed system. In the distributed storage system a server process running on one computer can export that computer s storage resources to client processes running on other computers. Remote procedure calls RPC may transfer data from server processes to client processes.

A remote procedure call is a two sided software operation initiated by client software executing on a first machine and serviced by server software executing on a second machine. Servicing storage system requests e.g. read data in software may require an available processor which may place a significant limitation on a distributed storage system. In the case of a distributed storage system this means a client process cannot access a remote computer s storage resources unless the remote computer has an available processor to service the client s request. Moreover the demand for processor resources and storage resources in a distributed system often do not match. In particular computing resource i.e. processors may have heavy and or unpredictable usage patterns while storage resources may have light and very predictable usage patterns. When a server s processor s are heavily utilized there may be no processors available to service a storage request when it arrives. In this case the storage request waits for completion of other software tasks executed by the processor s or preemption by a kernel before the storage request can be serviced by a processor even though there may be plenty of storage bandwidth to service the request immediately. Alternatively one or more dedicated processors could service such storage requests at the expense of efficiency and decreased processing capacity for nominal processing tasks.

Generally coupling processor and storage resources can result in high and or unpredictable latency especially if the distributed system s processor resources are heavily utilized.

One aspect of the disclosure provides a method of executing a transaction in a distributed storage system. For data chunks of a read set of the transaction the method includes reading data of the data chunks of the read set through remote direct memory access and determining a validity of the read data by evaluating a version and a lock of each data chunk of the read set. For data chunks of a write set of the transaction the method includes setting locks on the data chunks of the write set writing data to the locked data chunks through remote direct memory access releasing the locks of the locked data chunks and incrementing a version number of each released data chunk.

Implementations of the disclosure may include one or more of the following features. In some implementations the method includes aborting the transaction when at least one data chunk of the write set fails to receive a lock or when the read data is invalid. For any data chunks having an unreleased lock the method may include resetting those data chunks to an uninitialized state and releasing their locks e.g. for a non durable transaction . The method may include rereading the data of the data chunks of the read set when the previous read is invalid.

To allow a durable transaction the method may include reading existing data of the data chunks of the write set before writing new data to the data chunks of the write set and writing the existing data of the data chunks of the write set to a durable intent log. The method may include aborting the transaction when at least one data chunk of the write set fails to receive a lock or when the read data is invalid. For any data chunks having an unreleased lock the method includes retrieving their data stored in the durable intent log writing the retrieved data to the corresponding data chunks e.g. to reconstruct the data and releasing the locks of those data chunks.

The method may include reading an initial version number and an initial lock value associated with each data chunk of the read set. After reading the data the method includes reading a final version number and a final lock value associated with each data chunk of the read set and determining the read data as valid when the initial version number matches the final version number and the initial lock value matches the final lock value. Setting locks on the data chunks of the write set may include for each data chunk retrieving a lock word associated with the data chunk comparing data of the lock word with compare data and writing swap data to the lock word when the lock word data and the compare data match. The swap data may include a client identifier. The method may include accessing metadata associated with each data chunk that includes a version number and a lock word containing a lock value.

The method may include identifying one or more memory locations corresponding to the data chunks and transmitting remote direct memory access requests to each identified memory location. In some examples the method includes receiving at least one file descriptor. Each file descriptor maps data stripes and data stripe replications of a file on memory hosts for remote direct memory access of that file on the memory hosts. The stripe replications are data chunks. The method may include accessing a file map mapping files to file descriptors. The method may include receiving a key e.g. in the file descriptor that allows access to the data chunks on the memory hosts.

Another aspect of the disclosure provides a distributed storage system that includes memory hosts a curator in communication with the memory hosts and a transaction executing on at least one computing processor and in communication with the memory hosts and the curator. Each memory host has non transitory memory for storing data chunks of files. The curator manages striping of files across the memory hosts. The transaction executes a commit operation. For data chunks of a read set of the transaction the commit operation includes reading data of the data chunks of the read set through remote direct memory access and determining a validity of the read data by evaluating a version and a lock of each data chunk of the read set. For data chunks of a write set of the transaction the commit operation includes setting locks on the data chunks of the write set writing data to the locked data chunks through remote direct memory access releasing the locks of the locked data chunks and incrementing a version number of each released data chunk.

In some implementations the transaction aborts the commit operation when at least one data chunk of the write set fails to receive a lock or when the read data is invalid. For any data chunks having an unreleased lock the curator may reset those data chunks to an uninitialized state and releases their locks. The commit operation may include rereading the data of the data chunks of the read set when the previous read is invalid.

To allow a durable transaction the commit operation may include reading existing data of the data chunks of the write set before writing new data to the data chunks of the write set and writing the existing data of the data chunks of the write set to a durable intent log. The commit operation may include aborting the transaction when at least one data chunk of the write set fails to receive a lock or when the read data is invalid. For any data chunks having an unreleased lock the method includes retrieving their data stored in the durable intent log writing the retrieved data to the corresponding data chunks e.g. to reconstruct the data and releasing the locks of those data chunks.

The commit operation may include reading an initial version number and an initial lock value associated with each data chunk of the read set. After reading the data the commit operation includes reading a final version number and a final lock value associated with each data chunk of the read set and determining the read data as valid when the initial version number matches the final version number and the initial lock value matches the final lock value. Setting locks on the data chunks of the write set may include for each data chunk retrieving a lock word associated with the data chunk comparing data of the lock word with compare data and writing swap data to the lock word when the lock word data and the compare data match. The swap data may include a client identifier.

The commit operation may include accessing metadata associated with each data chunk. The metadata includes a version number and a lock word containing a lock value. The commit operation may include identifying one or more memory locations corresponding to the data chunks and transmitting remote direct memory access requests to each identified memory location.

In response to a memory access request by a client in communication with the memory hosts and the curator the curator may provide the client a file descriptor mapping data stripes and data stripe replications of a file on the memory hosts for remote direct memory access of the file on the memory hosts. The stripe replications are data chunks. The curator may store a file map mapping files to file descriptors and the transaction may access the file map. The transaction may receive a key from the curator for accessing the data chunks on the memory hosts. Each memory host may include a network interface controller in communication with its memory and servicing remote direct memory access requests.

Another aspect of the disclosure provides a computer program product encoded on a non transitory computer readable storage medium including instructions that when executed by a data processing apparatus cause the data processing apparatus to perform operations of a method of executing a transaction in a distributed storage system. For data chunks of a read set of the transaction the method includes reading data of the data chunks of the read set through remote direct memory access and determining a validity of the read data by evaluating a version and a lock of each data chunk of the read set. For data chunks of a write set of the transaction the method includes setting locks on the data chunks of the write set writing data to the locked data chunks through remote direct memory access releasing the locks of the locked data chunks and incrementing a version number of each released data chunk.

Implementations of the disclosure may include one or more of the following features. In some implementations the method includes aborting the transaction when at least one data chunk of the write set fails to receive a lock or when the read data is invalid. For any data chunks having an unreleased lock the method may include resetting those data chunks to an uninitialized state and releasing their locks. The method may include rereading the data of the data chunks of the read set when the previous read is invalid.

To allow a durable transaction the method may include reading existing data of the data chunks of the write set before writing new data to the data chunks of the write set and writing the existing data of the data chunks of the write set to a durable intent log. The method may include aborting the transaction when at least one data chunk of the write set fails to receive a lock or when the read data is invalid. For any data chunks having an unreleased lock the method includes retrieving their data stored in the durable intent log writing the retrieved data to the corresponding data chunks e.g. to reconstruct the data and releasing the locks of those data chunks.

The method may include reading an initial version number and an initial lock value associated with each data chunk of the read set. After reading the data the method includes reading a final version number and a final lock value associated with each data chunk of the read set and determining the read data as valid when the initial version number matches the final version number and the initial lock value matches the final lock value. Setting locks on the data chunks of the write set may include for each data chunk retrieving a lock word associated with the data chunk comparing data of the lock word with compare data and writing swap data to the lock word when the lock word data and the compare data match. The swap data may include a client identifier. The method may include accessing metadata associated with each data chunk that includes a version number and a lock word containing a lock value.

The method may include identifying one or more memory locations corresponding to the data chunks and transmitting remote direct memory access requests to each identified memory location. In some examples the method includes receiving at least one file descriptor. Each file descriptor maps data stripes and data stripe replications of a file on memory hosts for remote direct memory access of that file on the memory hosts. The stripe replications are data chunks. The method may include accessing a file map mapping files to file descriptors. The method may include receiving a key e.g. in the file descriptor that allows access to the data chunks on the memory hosts.

The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects features and advantages will be apparent from the description and drawings and from the claims.

Referring to in some implementations a distributed storage system includes loosely coupled memory hosts e.g. computers or servers each having a computing resource e.g. one or more processors in communication with storage resources e.g. memory flash memory dynamic random access memory DRAM phase change memory PCM and or disks . A storage abstraction e.g. key value store or file system overlain on the storage resources allows scalable use of the storage resources by one or more clients . The clients may communicate with the memory hosts through a network . Rather than having a processor of a memory host e.g. a server execute a server process that exports access of the corresponding storage resource e.g. non transitory memory to client processes executing on the clients the clients may directly access the storage resource through a network interface controller NIC of the memory host . In other words a client process executing on a client may directly interface with one or more storage resources without requiring execution of a routine of any server processes executing on the computing resources . This offers a single sided distributed storage architecture that offers relatively high throughput and low latency since clients can access the storage resources without interfacing with the computing resources of the memory hosts .

A network interface controller also known as a network interface card network adapter or LAN adapter may be a computer hardware component that connects a computing resource to the network . The network controller implements communication circuitry using a specific physical layer OSI layer 1 and data link layer layer 2 standard such as Ethernet Wi Fi or Token Ring. This provides a base for a full network protocol stack allowing communication among small groups of computers on the same LAN and large scale network communications through routable protocols such as Internet Protocol IP . Both the memory hosts and the client may each have a network interface controller for network communications. A host process executing on the computing processor of the memory host registers a set of remote direct memory accessible regions of the memory with the network interface controller . The host process may register the remote direct memory accessible regions of the memory with a permission of read only or read write. The network interface controller of the memory host creates a client key for each registered memory region 

In some implementations the network is an InfiniBand network which is a switched fabric communications link generally used in high performance computing and enterprise data centers. It features high throughput low latency quality of service failover and scalability. The InfiniBand architecture specification defines a connection between processor nodes and high performance I O nodes such as storage devices. The InfiniBand network conveys remote direct memory access RDMA requests from a client to a memory host . At the memory host an RDMA capable InfiniBand network interface controller NIC performs reads and writes of the storage resource e.g. DRAM . RDMA uses zero copy OS bypass to provide high throughput low latency access to data e.g. 4 GB s of bandwidth and 5 microsecond latency . The distributed storage system may use RDMA remote procedure calls or other data access methods to access data.

Referring to in some implementations the distributed storage system includes multiple cells each cell including memory hosts and a curator in communication with the memory hosts . Each cell may also include one or more stripe doctors e.g. processes for managing and or repairing stripes one or more slowpoke clients e.g. clients or virtual clients used for assessing system performance and a console for monitoring and managing the cell .

The curator e.g. process may execute on a computing processor e.g. server connected to the network and manages the data storage e.g. manages a file system stored on the memory hosts controls data placements and or initiates data recovery. Moreover the curator may track an existence and storage location of data on the memory hosts . Redundant curators are possible. In some implementations the curator s track the striping of data across multiple memory hosts and the existence and or location of multiple copies of a given stripe for redundancy and or performance. In computer data storage data striping is the technique of segmenting logically sequential data such as a file in a way that accesses of sequential segments are made to different physical storage devices e.g. cells and or memory hosts . Striping is useful when a processing device requests access to data more quickly than a storage device can provide access. By performing segment accesses on multiple devices multiple segments can be accessed concurrently. This provides more data access throughput which avoids causing the processor to idly wait for data accesses.

In some implementations an application programming interface API interfaces between a client e.g. a client process and the curator . In some examples the client communicates with the curator through one or more remote procedure calls RPC . In response to a client request the API may find the storage location of certain data on memory host s and obtain a key that allows access to the data. The API communicates directly with the appropriate memory hosts via the network interface controllers to read or write the data e.g. using remote direct memory access . In the case that a memory host is non operational or the data was moved to a different memory host the client request fails prompting the client to re query the curator .

Referring to in some implementations the curator stores and manages file system metadata . The metadata includes a file map that maps files to file descriptors . The curator may examine and modify the representation of its persistent metadata . The curator may use three different access patterns for the metadata read only file transactions and stripe transactions. Read only access allows the curator to examine a state of the metadata with minimal contention. A read only request returns the most recent state of a file but with no synchronization with concurrent updates. The read only access may be used to respond to lookup requests from clients e.g. for internal operations such as file scanning .

Referring also to in some implementations the memory hosts store file data . The curator may divide each file and its data into stripes and replicate the stripes for storage in multiple storage locations. A stripe replica is also referred to as a chunk or data chunk . Mutable files may have additional metadata stored on the memory host s such as lock words and version numbers. The lock words and versions numbers may be used to implement a distributed transaction commit protocol.

File descriptors stored by the curator contain metadata such as the file map that maps the stripes to data chunks i.e. stripe replicas stored on the memory hosts . To open a file a client sends a request to the curator which returns a file descriptor . The client uses the file descriptor to translate file chunk offsets to remote memory locations . After the client loads the file descriptor the client may access the file s data via RDMA or another data retrieval method.

Referring to RDMA is a connection based process to process communication mechanism so RDMA connections typically do not support authentication or encryption by themselves. As a result the distributed storage system may treat the RDMA connections as secure resources. In order for a client process to access the memory of a host process through RDMA the network interface controller of the memory host executes a connection handshake with a network interface controller of the client process to establish the RDMA capable connection between the host process and the client process . The RDMA connection handshake may implement a higher level secure protocol that evaluates the identities of the host and client processes as known at the time of creation of the trusted RDMA connection . After an RDMA capable connection is established the client process or the host process can unilaterally break the connection . If either the client process or the host process dies the client and or the memory host via operating systems can tear down the corresponding RDMA connection s .

Access to file data e.g. data chunks stored in remote memory locations may be controlled by access control lists . Each access control list may have a unique name a list of data chunks and a list of clients that have permission to read and write the data chunks associated with that access control list . In some examples the access control list provides an access permission level for each associated client or each associated data chunk . The memory hosts may receive the access control lists through a secure communication channel and can be enforced by the memory hosts using protection domains . Each RDMA accessible memory region registered with the network interface controller of each memory host is associated with a protection domain . In some implementations when the curator allocates memory for the data chunks it associates the allocated memory regions of the data chunks with one or more protection domains . A memory host may have many protection domains associated with various regions of its memory . Each protection domain may also have one or more associated connections .

When a client instantiates a memory access request for a file stored on one or more of the memory hosts the client requests a file descriptor from the curator to identify which memory host s store the data chunks of the file . In addition to mapping data chunks of the file to memory regions of memory hosts the file descriptor may also include a client key for accessing those data chunks . The client then searches a connection cache for any open RDMA capable connections to the identified memory hosts . If each memory host fails to have an open connection with the client that is in the same protection domain as the requested data chunk s the client sends a connection request to any memory hosts not having the necessary open connection s .

In response to receiving a connection request from a client process of a client to access a data chunk e.g. to access a memory region storing the data chunk the host process may establish a remote direct memory access capable connection with the client process when both the client and the requested data chunk are associated with the same access control list received by the memory host . The client process may include the access control list in the connection request . The host process may associate the established open connection with a protection domain and the client process may store the open connection in the connection cache . The connection is capable of accessing via RDMA only the memory regions associated with its protection domain . The network interface controller of the memory host may tear down the connection upon receiving an RDMA request having an address for unregistered memory .

In the example shown in first and second clients send memory access requests to a memory host over respective first and second RDMA connections . The memory host has first and second protection domains associated with its memory . The first protection domain is associated with first and second memory regions e.g. storing corresponding first and second data chunks and the first RDMA connection while the second protection domain is associated with a third memory region e.g. storing a corresponding third data chunks and only the second RDMA connection

The first client sends first and second memory access requests over the first RDMA connection to the memory host . The first memory access request is for accessing the second memory region for the second data chunk and the second memory access request is for accessing the third memory region for the third data chunk . The first memory access request succeeds because the second memory region belongs to the same protection domain as the first connection . The second memory access request fails because the third memory region belongs to a different protection domain the second protection domain rather than the protection domain of the second memory access request i.e. the first protection domain .

The second client sends third and fourth memory access requests over the second RDMA connection to the memory host . The third memory access request is for accessing the first memory region for the first data chunk and the fourth memory access request is for accessing the third memory region for the third data chunk . In this case both memory access requests succeed because the RDMA connection of the second client belongs to the protection domains of both the first memory region and the third memory region

Referring again to in some implementations the curator can create copy resize and delete files . Other operations are possible as well. To service a copy request from a client the curator creates a new file descriptor having a state initially set to COPY PENDING. The curator may set initialize one or more of the following fields size owner group permissions and or backing file. The curator populates a stripes array of the file descriptor with empty stripes and then commits the file descriptor to its file map . Committing this information to the file map allows the curator to restart a resize operation if the curator crashes or a tablet containing the file system metadata migrates to another curator . Once the curator commits the file descriptor to the file map the curator responds to the client copy request by informing the client that the copy operation has been initiated. The curator initiates memory host pull chunk operations which instruct memory hosts to allocate a new chunk and to read chunks of the backing file into the memory of the memory hosts . When a pull chunk operation returns successfully the curator adds the new chunk to the appropriate stripe in the file descriptor . The curator commits the stripe with the new chunk to the file map .

In the case of a crash or a migration incrementally updating the file descriptors allows a new curator to restart a copy operation from the location the prior curator stopped. This also allows clients to check the status of a copy operation by retrieving the file descriptor e.g. via a lookup method and inspecting the number of stripes in the file descriptor populated with chunks . Once all chunks have been copied to the memory hosts the curator transitions the state of the file descriptor to READ and commits it to the file map .

The curator may maintain status information for all memory hosts that are part of the cell . The status information may include capacity free space load on the memory host latency of the memory host from a client s point of view and a current state. The curator may obtain this information by querying the memory hosts in the cell directly and or by querying slowpoke clients to gather latency statistics from a client s point of view. In some examples the curator uses the memory host status information to make rebalancing draining recovery decisions and allocation decisions.

The curator s may allocate chunks in order to handle client requests for more storage space in a file and for rebalancing and recovery. The curator may maintain a load map of memory host load and liveliness. In some implementations the curator allocates a chunk by generating a list of candidate memory hosts and sends an allocate chunk request to each of the candidate memory hosts . If the memory host is overloaded or has no available space the memory host can deny the request. In this case the curator selects a different memory host . Each curator may continuously scan its designated portion of the file namespace examining all the metadata every minute or so. The curator may use the file scan to check the integrity of the metadata determine work that needs to be performed and or to generate statistics. The file scan may operate concurrently with other operations of the curator . The scan itself may not modify the metadata but schedules work to be done by other components of the system and computes statistics.

For each file descriptor the file scan may ensure that the file descriptor is well formed e.g. where any problems may indicate a bug in either the curator or in the underlying storage of the metadata update various statistics such as the number of files stripes chunks and the amount of storage used look for stripes that need recovery determine if the file descriptor contains chunks that are candidates for rebalancing from overfull memory hosts determine if there are chunks on draining memory hosts determine if there are chunks that are candidates for rebalancing to under full memory hosts determine chunks that can be deleted and or determine if the file descriptor has a pending resize or copy operation but there is no active task within the curator working on the operation.

In some implementations the distributed storage system supports two types of files immutable and mutable. Immutable files rely on a disk based file system for persistence and fault tolerance. A client may copy immutable files into the file system of the distributed storage system . On the other hand a client may write mutable files into the file system of the distributed storage system using the application programming interface API . The storage system may or may not be durable. The distributed storage system may have strict data loss service level objectives SLOs that depend on the files level of replication. When a stripe is lost the curator may allocate new storage for the lost stripe and mark the data as uninitialized. A client attempting to read an uninitialized stripe receives an uninitialized data error. At this point the client can reinitialize the stripe s data.

The file descriptor may provide the state of a file . A file can be in one of the following states READ READ WRITE DELETED or CREATE COPY RESIZE PENDING. In the READ state clients can read the file but not write to the file . Read only files are read only for the entire life time of the file i.e. read only files are never written to directly. Instead read only files can be copied into the file system from another file system. A backing file may be used to restore data when a memory host crashes consequently the backing file persists for the entire life time of the file . In the READ WRITE state clients with the appropriate permissions can read and write a mutable file s contents. Mutable files support concurrent fine grain random writes. Random and sequential write performance may be comparable. Writes are strongly consistent that is if any client can observe the effect of a write then all clients can observe the effect of a write. Writes can also be batched into transactions. For example a client can issue a batch of asynchronous writes followed by a sync operation. Strong consistency and transactional semantics ensure that if any client can observe any write in a transaction then all clients can observe all writes in a transaction. In the DELETED state the file has been deleted. The chunks belonging to the file are stored in a deleted chunks field and wait for garbage collection. The CREATE COPY RESIZE PENDING state denotes a file has a create copy or resize operation pending on the file.

An encoding specified by a file encoding protocol buffer of the file descriptor may be used for all the stripes within a file . In some examples the file encoding contains the following fields data chunks which provides a number of data chunks per stripe stripe length which provides a number of bytes per stripe and sub stripe length which provides a number of bytes per sub stripe. The sub stripe length may be only valid for READ WRITE files. The data for a file may be described by an array of stripe protocol buffers in the file descriptor . Each stripe represents a fixed region of the file s data identified by an index within the array. The contents of a stripe may include an array of chunk protocol buffers each describing a chunk within the stripe including a chunk handle an identity of the memory host holding the chunk and a current state of the chunk . For RDMA purposes the chunk protocol buffers may also store a virtual address of the chunk in the memory host and a client key e.g. a 32 bit key. The client key is unique to a chunk on a memory host and is used to RDMA read that chunk .

Stripes can be further divided into sub stripes with associated sub stripe metadata . Each sub stripe may include an array of sub chunks each having corresponding associated sub chunk metadata .

Chunks can be in one of the following states OK Recovering Migrating Source and Migrating Destination. In the OK state the contents are valid and the chunk contributes to the replication state of the corresponding stripe . Clients may update all chunks in a good state. In the Recovering state the chunk Recovering is in the process of being recovered. The chunk Recovering does not count towards the replicated state of the corresponding stripe and the data in the chunk is not necessarily valid. Therefore clients cannot read data from chunks in the Recovering state. However all transactions not reaching their commit point at the time a chunk state changes to the Recovering state must include the Recovering chunk in the transaction in order to ensure that the chunk s data is kept up to date during recovery.

In the Migrating Source state the chunk is in the process of migrating. A migrating source attribute may provide a location from which the chunk is migrating. The source chunk counts towards the replication of the stripe and the data in the chunk is valid and can be read. In the Migrating Destination state the chunk is in the process of migrating. A Migrating Destination attribute provides the location to which the chunk is migrating. The source chunk does not count towards the replicated state of the stripe and the chunk is not necessarily valid. Therefore clients cannot read from chunks in the Migrating Destination state. However all transactions not reaching their commit point at the time a chunk s state changes to the Migrating Destination state must include the Migrating Destination chunk in the transaction in order to ensure the chunk s data is kept up to date as it is being migrated.

Each file descriptor may have a dead chunks array. The dead chunks array holds additional chunks that are no longer needed such as the chunks that made up a file that has since been deleted or made up previous instances of the file . When the file is deleted or truncated the chunks from all the stripes are moved into this dead chunks array and the stripes are cleared. The chunks in the dead chunks array are reclaimed in the background.

The application programming interface may facilitate transactions having atomicity consistency isolation durability to a degree such that the transaction may be serializable with respect to other transactions. ACID atomicity consistency isolation durability is a set of properties that guarantee that database transactions are processed reliably. In the context of databases a single logical operation on the data is called a transaction. Atomicity requires that each transaction is all or nothing if one part of the transaction fails the entire transaction fails and the database state is left unchanged. An atomic system guarantees atomicity in each and every situation including power failures errors and crashes. Consistency ensures that any transaction brings the database from one valid state to another. Any data written to the database must be valid according to all defined rules including but not limited to constraints cascades triggers and any combination thereof. Isolation ensures that no transaction should be able to interfere with another transaction. One way of achieving this is to ensure that no transactions that affect the same rows can run concurrently since their sequence and hence the outcome might be unpredictable. This property of ACID may be partly relaxed due to the huge speed decrease this type of concurrency management entails. Durability means that once a transaction has been committed it will remain so even in the event of power loss crashes or errors. In a relational database for instance once a group of SQL statements execute the results need to be stored permanently. If the database crashes immediately thereafter it should be possible to restore the database to the state after the last transaction committed.

Referring to in some implementations the application programming interface API includes a reader class and a transaction class . A client may instantiate a reader inheriting the reader class to execute a read or batches of reads on the memory hosts in a cell . Moreover the client may instantiate a transaction inheriting the transaction class to execute one or more reads and or writes. The reads and writes in a transaction may be to different files in a cell but in some implementations all reads and writes in a transaction must be to files in the same cell . Executed reads may be snapshot consistent meaning that all reads in a transaction can see a snapshot of the file at a logical instant in time. Writes can be buffered until the client tries to commit the transaction

Referring to in response to receiving a write memory access request for a file a transaction may acting as a writer write or modify data of the file e.g. of chunks and or sub chunks . After the write operation the transaction may compute a checksum of the modified data and associate the checksum with the modified data e.g. with the chunks and or sub chunks . In some examples the transaction stores the checksum in the sub chunk metadata for the modified sub chunk . The transaction may execute a hash function such as a cryptographic hash function to compute the checksum . Moreover the hash function may be configured for randomization. Each checksum may be a word having at least 64 bits. A network interface controller servicing the remote direct memory access requests on a corresponding memory host may determine the checksum of any data accessed on its memory host .

When a client adds a file read request to the reader e.g. via a transaction the reader translates the read request into a RDMA read network operation and stores a state of the network operation in memory allocated for the reader . Reads that cross chunk boundaries get translated into multiple RDMA operations.

In some implementations to translate a file read request into a RDMA read network operation the reader computes a target stripe number from a file offset of the read request . The reader may use the stripe number to index into a chunk handle cache. The chunk handle cache returns a network channel to access the corresponding chunk and a virtual address and r key of the chunk . The reader stores the network channel and r key directly in an operation state of the RDMA read. The reader uses the virtual address of the chunk and the file offset to compute the virtual address within the chunk to read. The reader computes the offset into a memory block supplied by the client e.g. a receiving memory block for each RDMA read operation . The reader may then initialize an operation status.

While buffering new reads the reader may calculate and store a running sum of the amount of metadata that will be retrieved to complete the read. This allows metadata buffer space to be allocated in one contiguous block during execution minimizing allocation overhead.

In response to receiving a memory access request from the client the transaction may retrieve a file descriptor from the curator that maps requested data chunks of a file on memory hosts for remote direct memory access of those data chunks on the memory hosts . The file descriptor may include a client key for each data chunk of the file . Moreover each client key allows access to the corresponding data chunk on its memory host .

Referring to in some implementations the reader executes a read operation in two phases. In the first phase the reader reads the data and associated metadata of a file . In the second phase the reader validates that the data read in the first phase satisfies data consistency constraints of the reader . In the first phase the reader identifies one or more memory locations corresponding to the data and transmits its RDMA read operations. While iterating through and transmitting RDMA reads the reader initializes and transmits RDMA reads to read sub chunk metadata and to read data needed to compute checksums of the sub chunks such as of the first and last sub chunks in an unaligned file access. After the data and metadata are received the reader may check lock words in the sub chunk metadata to ensure that the sub chunks were not locked while the data was being read. If a sub chunk was locked the reader rereads the sub chunk and its corresponding metadata . Once the reader finds reads all of the sub chunk locks in an unlocked state the reader computes the sub chunk checksums and compares the computed checksums with the checksums read from the sub chunk metadata .

In other words for detecting read write conflicts the reader in response to receiving a read memory access request for data of a file stored in the memory hosts of a cell may compute a first checksum of the data compare the first checksum with a second checksum associated with the data e.g. stored in the metadata of the corresponding sub chunk and allow a read operation on the data when the first and second checksums match. The reader may execute a hash function such as a cryptographic hash function to compute the checksums . The reader may read the data and metadata associated with the data after receiving the read write request and before processing the read write request . Moreover the reader may determine whether the data was locked while reading the data for example by evaluating a lock word and or a version number stored in the metadata . The reader rereads the data and associated metadata when the data was locked while previously reading the data .

While checksums are commonly used to guard against hardware error or even software error using it to guard against what is actually normal operation poses certain additional requirements. Since a conflict may not be a rare event the chance of getting a coincidentally matching checksum can be minimized by having checksum size large enough to provide a relatively small probability of a coincidental match. In some examples a 64 bit checksum is sufficient since checking a random bad checksum every nanosecond may produce a false positive less than once every five centuries which is much less frequent than the rates of other types of system failures. Additionally a hash function for computing the checksum may produce different numbers for all common modifications of the data. For example simply adding up all the data would not suffice since a change that simply re ordered some of the data would not change the checksum. However a cryptographic hash functions which by design does not allow simple modifications of the data to produce any predictable checksum may be sufficient.

A sub chunk checksum may fail a compare for one of three reasons 1 the data read was corrupted by a concurrent write 2 the data was corrupted while in transit to the client or 3 the data stored in the memory host is corrupt. Cases 1 and 2 are transient errors. Transient errors are resolved by retrying the sub chunk read. Case is a permanent error that may require the client to notify the curator of a corrupt sub stripe

To differentiate between a transient error and a permanent error the client may re read the sub chunk data and the sub chunk metadata . The reader then checks a sub chunk lock word and re computes and compares the sub chunk checksum . If the checksum error still exists and a sub chunk version number has changed since the sub chunk was initially read then the checksum compare failure was likely caused by a concurrent write so the reader retries the sub chunk read. If the version number has not changed since the sub chunk was initially read then the error is permanent and the reader notifies the curator and the curator tries to reconstruct the data of the chunk . If the curator is unable to reconstruct the chunk data the curator replaces the old chunk with a new uninitialized chunk .

Unlike locking the checksum compare method for detecting read write conflicts does not actually care if a conflicting write existed as long as the data is consistent. For example if the data is being overwritten with identical data or if a write is preparing to start but has not actually begun or has just finished the locking method will cause the read to fail unnecessarily while the checksum compare will allow the read to succeed. Since the time between locking and unlocking may be much greater than the duration of an actual write this can be a significant improvement.

The reader does not know which version of the data it has read and it may not matter. If it is advantageous to have the read obtain a version number this may be done without an additional round trip latency penalty if the version number itself is covered by the checksum . Although computing checksums may incur a nontrivial penalty in processor time both for the reader and the writer a checksum may be necessary anyway to guard against hardware errors depending on the implementation.

Sub chunk locks may become stuck due to a client trying to execute a transaction but crashing during a commit protocol of the transaction . A reader can detect a stuck lock by re reading the sub chunk lock word and version number . If a sub chunk lock word and version number do not change during some time out period then the sub chunk lock is likely stuck. When the reader detects a stuck lock it notifies the curator of the stuck lock and the curator recovers the sub stripe and resets the stuck lock.

Referring also to in some implementations after the reader validates each sub chunk lock word and or checksum the reader may proceed to the second phase of executing the read operation i.e. the validation phase . To validate the values the reader rereads sub chunk metadata and rechecks if the sub chunk lock words are unlocked and the sub chunk version numbers have not changed since the version numbers were initially read during the first phase of the read operation. In other words the reader may read an initial version number and an initial lock value associated with each data chunk of a read set of the transaction . After reading the data the reader reads a final version number and a final lock value associated with each data chunk of the read set and determines the read data as valid when the initial version number matches the final version number and the initial lock value matches the final lock value

If the reader is associated with a transaction the reader may reread the metadata associated with all sub chunks read by the transaction . If a single sub chunk version number mis compares the reader returns an error. If all sub chunk version numbers are the same the reader discards the prefix and suffix of the reader memory block in order to trim extraneous data read to compute the checksum of the first and last sub chunks in the read. The reader may set a status to OK and returns to the client .

If the reader encounters an error on a network channel while reading data or metadata of a chunk the reader may select a different chunk from the chunk handle cache and notifies the curator of a bad memory host. If no other good chunks exist from which the reader can read the reader may wait to receive a response to the error notification it sent to the curator . The response from the curator may contain an updated file descriptor that contains a new good chunk to read from.

In some implementations the transaction class uses validation sets to track which sub stripes have been read by the transaction . Each read of a transaction adds the version numbers of all sub stripes read to a validation set of the transaction . The transaction may validate the validation set in two cases 1 as part of the commit protocol and 2 the validation phase of reads of a transaction . A transaction may fail to commit if the commit protocol finds that any sub stripe version number differs from the number recorded in the validation set . Validation of the full validation set before data is returned to the client allows early detection e.g. before the commit phase of a doomed transaction . This validation also prevents the client from getting an inconsistent view of file data.

A transaction may provide a synchronous serializable read operation e.g. using a reader . In some examples a reader is instantiated and associated with the transaction . Read results of the reader return the latest committed data. As such uncommitted writes of the same transaction are not seen by a read of that transaction

A transaction may buffer data for a later transaction commit. The transaction class translates a buffer write request into one or more prepare write network operations. One network operation is needed for each stripe touched by the write operation. Processing a buffer write request may involve preparing sub stripe lock network operations. One lock operation is needed for each sub stripe touched by the requested write. These operations are buffered for transmission during the transaction commit. The transaction may translate buffer write requests into network operations and execute identify or coalesce writes that affect the same region of a file . The transaction may apply write operations in the same order by the memory hosts for all chunks to ensure that all replicas are consistent.

The transaction may provide a commit operation that results in all reads and writes in the transaction being schedulable as a single atomic serializable operation. In some implementations the transaction commit protocol proceeds through a lock phase a validate phase a write phase and an unlock phase. During the lock phase the sub stripe lock network operations which were created in response to buffer write requests are sent. Each sub stripe lock operation executes an atomic compare and swap operation on the lock word in all replicas . If the contents of the lock word match the specified compare data e.g. a client identifier the lock word is written with the specified swap data and the previous contents of the word are returned. If the client succeeds in writing its unique client ID into the metadata lock word it has successfully taken the lock. If the transaction fails to take the lock for any sub stripe in the write set the commit fails and is aborted. The commit protocol proceeds to the validate phase once all sub stripe locks are held.

During the validate phase the transaction may read the version number out of the metadata for all sub stripes referenced in the validation set and comparing the version numbers to the version numbers recorded in the validation set. If a version number does not match the sub stripe was written by another transaction after it was read by this transaction so the transaction fails. In this case the reader releases the locks it holds and returns a transaction conflict error to the client . Once all version numbers in the validation set have been validated the client writes the buffered write data of the transaction to each replica and updates the metadata associated with each sub stripe written by the transaction during the write phase. Updating metadata of a sub stripe may include computing and writing a new check word and incrementing the version number of the sub stripe . Once all data and metadata has been updated the transaction releases the locks that it holds during the unlock phase.

File transaction access may provide exclusive read write access to the state of a file descriptor . Updates to the file state may be applied at the end of a transaction and are atomic. File transaction access can be used for operations such as creating finalizing and deleting a file . These operations may require the curator to communicate with other components such as memory hosts and thus a file transaction access may last for several seconds or more. While active the file transaction access blocks any other operations that need to modify the state of the file descriptor . Read access may not be blocked.

To reduce contention stripe transaction access may provide relatively finer grain synchronization for operations that only need to modify the state of a single stripe with the file descriptor . This mode can be used for stripe operations such as opening closing rebalancing and recovering. There can be many concurrent stripe transactions for different stripes within a file but stripe transactions and file transactions are mutually exclusive. Within a stripe transaction the curator may examine the state of a stripe and various fields of the file descriptor that remain immutable for the duration of the transaction such as the file encoding and instance identifier. The stripe transaction access does not provide access to fields that can change underfoot such as the state of other stripes . Operations may hold only one active transaction at a time to avoid deadlock. Moreover transactions may only atomically commit on a single file .

Referring again to the distributed storage system may include one or more stripe doctors in each cell that fix and recover stripe data. For example each cell may have several stripe doctors that execute rebalancing and recovery operations. Additionally or alternatively the distributed storage system may include one or more slowpoke clients that monitor a cell s performance from a client s prospective. A slowpoke client provides latency information to the curator s for chunk allocation and rebalancing. For example each slowpoke client collects latency statistics for every memory host in a cell. A cell may have several slowpoke clients to improve network path coverage and fault tolerance. Since it is unlikely that the curator may completely lose the latest latency statistics for a memory host in the cell curators can make chunk allocation and rebalancing decisions without the slowpoke latency information.

In some implementations the method includes associating the established connection with a protection domain having an associated memory region storing the data chunk . The connection is capable of accessing only memory regions associated with its protection domain . Moreover the protection domain is capable of being associated with one or more connections . The method may include allocating memory for the data chunk e.g. via the curator and associating the allocated memory with the protection domain .

The method may include executing a connection handshake with the client to establish the remote direct memory access capable connection . In some examples the method includes evaluating an identity of the client such as by executing a client authentication routine. If a received remote direct memory access request includes an address for unregistered memory the method may include tearing down the connection .

The method may include registering remote direct memory accessible regions of memory with a network interface controller of the memory host . The remote direct memory accessible memory regions may be registered with a permission of read only or read write.

In some implementations the method includes allocating memory for the data chunk e.g. via the curator and associating the data chunk with the access control list . The method may include assigning an access permission for each client or each data chunk associated with the access control list .

In response to receiving a memory access request from the client the method may include returning a file descriptor that maps data chunks of the file on memory hosts for remote direct memory access of the data chunks on the memory hosts . The file descriptor may include a client key for each data chunk of the file . Moreover each client key allows access to the corresponding data chunk on its memory host .

The method may include receiving a file descriptor mapping data chunk to the memory hosts . The file descriptor may include a client key allowing access to the corresponding data chunk on its memory host . The method may include sending a memory access request for multiple data chunks stored in the memory of the memory host where two or more of the data chunks are associated with different access control lists .

In some implementations the method includes accessing a file map mapping files to file descriptors to return the file descriptor in response to the memory access request . The method may include returning location information of data on the memory hosts in response to the client memory access request . The method may include returning a key to allow access to data on the memory hosts in response to the client memory access request . In some examples the method includes allocating storage of a data stripe on the memory hosts . The method may include dividing the file into data stripes and replicating each data stripe into multiple storage locations of the memory hosts .

In some implementations the method includes providing at least one of a file state attribute providing a state of a file a data chunks attribute providing a number of stripe replicas per stripe a stripe length attribute providing a number of bytes per stripe and a sub stripe length attribute providing a number of bytes per sub stripe in the file descriptor . The method may include providing 710 in the file descriptor an array of stripe protocol buffers each describing a data stripe replica within a data stripe

Servicing storage requests in hardware provides a number of advantages such as having relatively simple storage requests e.g. read write . Implementing such functionality in an application specific integrated circuit ASIC can be much more efficient than implementing the functionality in software running on a general purpose processor. This efficiency improvement means storage requests can be serviced in less time and occupy fewer circuits when compared to implementing the same functionality in software running on a general purpose processor. In turn this improvement means a distributed storage system can achieve lower latency and higher throughput without increasing the cost of the system.

Servicing storage requests in the network interface hardware e.g. NIC decouples processor resources and storage resources . A client can access storage resources on a memory host without available processor resources . This allows system builders and administrators to operate a distributed storage system with high processor utilization with low and predictable latencies and without stranding storage resources. In some implementations the distributed storage system can provide an average read latency of less than 10 microseconds an average read throughput of greater than 2 million operations per second per client and average write latency of less than 50 microseconds and or an average write throughput of greater than 500 thousand operations per second per client.

In some implementations the method includes executing a hash function such as a cryptographic hash function to compute at least one of the checksums . The method may include associating metadata with the data e.g. the sub chunk where the metadata includes a checksum word containing the checksum associated with the data . In some examples the method includes reading the data and metadata associated with the data after receiving the read write request and before processing the read write request . The method may include determining whether the data was locked while reading the data and or evaluating a lock word and a version number stored in the metadata . When the data was locked while previously reading the data the method may include rereading the data and associated metadata .

The method may include identifying one or more memory locations corresponding to the data and transmitting remote direct memory access requests to each identified memory location . Moreover the method may include receiving a file descriptor of a file containing the data . The file descriptor maps data stripes and data stripe replications of the file on memory hosts for remote direct memory access. The method may include accessing a file map mapping files to file descriptors to return the file descriptor . In some examples the method includes receiving a key e.g. in the file descriptor allowing access to the data on the memory hosts .

In some implementations the method includes aborting the transaction when at least one data chunk of the write set fails to receive a lock or when the read data is invalid. For any data chunks having an unreleased lock the method may include resetting e.g. via the curator those data chunks to an uninitialized state and releasing their locks . The method may include rereading the data of the data chunks of the read set when the previous read is invalid. In some examples the method includes reading existing data of the data chunks of the write set before writing new data to those data chunks and writing the existing data of the data chunks of the write set to a durable intent log to allow later reconstruction of the data if the write operation fails so as to provide a durable transaction

The method may include aborting the transaction when at least one data chunk of the write set fails to receive a lock or when the read data is invalid. For any data chunks having an unreleased lock the method may include marking the data as uninitialized and releasing the lock for non durable transactions or retrieving their existing data stored in the durable intent log writing the retrieved data to the corresponding data chunks to reconstruct the data and the releasing the locks of those data chunks to provide a durable transaction

The method may include reading an initial version number and an initial lock value associated with each data chunk of the read set . After reading the data the method includes reading a final version number and a final lock value associated with each data chunk of the read set and determining the read data as valid when the initial version number matches the final version number and the initial lock value matches the final lock value . Setting locks on the data chunks of the write set may include for each data chunk retrieving a lock word associated with the data chunk comparing data of the lock word with compare data and writing swap data e.g. a unique client identifier to the lock word when the lock word data and the compare data match. The method may include accessing metadata associated with each data chunk that includes a version number and a lock word containing a lock value.

Various implementations of the systems and techniques described here can be realized in digital electronic circuitry integrated circuitry specially designed ASICs application specific integrated circuits computer hardware firmware software and or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and or interpretable on a programmable system including at least one programmable processor which may be special or general purpose coupled to receive data and instructions from and to transmit data and instructions to a storage system at least one input device and at least one output device.

These computer programs also known as programs software software applications or code include machine instructions for a programmable processor and can be implemented in a high level procedural and or object oriented programming language and or in assembly machine language. As used herein the terms machine readable medium and computer readable medium refer to any computer program product apparatus and or device e.g. magnetic discs optical disks memory Programmable Logic Devices PLDs used to provide machine instructions and or data to a programmable processor including a machine readable medium that receives machine instructions as a machine readable signal. The term machine readable signal refers to any signal used to provide machine instructions and or data to a programmable processor.

Implementations of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry or in computer software firmware or hardware including the structures disclosed in this specification and their structural equivalents or in combinations of one or more of them. Moreover subject matter described in this specification can be implemented as one or more computer program products i.e. one or more modules of computer program instructions encoded on a computer readable medium for execution by or to control the operation of data processing apparatus. The computer readable medium can be a machine readable storage device a machine readable storage substrate a memory device a composition of matter effecting a machine readable propagated signal or a combination of one or more of them. The terms data processing apparatus computing device and computing processor encompass all apparatus devices and machines for processing data including by way of example a programmable processor a computer or multiple processors or computers. The apparatus can include in addition to hardware code that creates an execution environment for the computer program in question e.g. code that constitutes processor firmware a protocol stack a database management system an operating system or a combination of one or more of them. A propagated signal is an artificially generated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.

A computer program also known as an application program software software application script or code can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data e.g. one or more scripts stored in a markup language document in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules sub programs or portions of code . A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.

The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by and apparatus can also be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit .

Processors suitable for the execution of a computer program include by way of example both general and special purpose microprocessors and any one or more processors of any kind of digital computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally a computer will also include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. However a computer need not have such devices. Moreover a computer can be embedded in another device e.g. a mobile telephone a personal digital assistant PDA a mobile audio player a Global Positioning System GPS receiver to name just a few. Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory media and memory devices including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry.

To provide for interaction with a user one or more aspects of the disclosure can be implemented on a computer having a display device e.g. a CRT cathode ray tube LCD liquid crystal display monitor or touch screen for displaying information to the user and optionally a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input. In addition a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user for example by sending web pages to a web browser on a user s client device in response to requests received from the web browser.

One or more aspects of the disclosure can be implemented in a computing system that includes a backend component e.g. as a data server or that includes a middleware component e.g. an application server or that includes a frontend component e.g. a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification or any combination of one or more such backend middleware or frontend components. The components of the system can be interconnected by any form or medium of digital data communication e.g. a communication network. Examples of communication networks include a local area network LAN and a wide area network WAN an inter network e.g. the Internet and peer to peer networks e.g. ad hoc peer to peer networks .

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other. In some implementations a server transmits data e.g. an HTML page to a client device e.g. for purposes of displaying data to and receiving user input from a user interacting with the client device . Data generated at the client device e.g. a result of the user interaction can be received from the client device at the server.

While this specification contains many specifics these should not be construed as limitations on the scope of the disclosure or of what may be claimed but rather as descriptions of features specific to particular implementations of the disclosure. Certain features that are described in this specification in the context of separate implementations can also be implemented in combination in a single implementation. Conversely various features that are described in the context of a single implementation can also be implemented in multiple implementations separately or in any suitable sub combination. Moreover although features may be described above as acting in certain combinations and even initially claimed as such one or more features from a claimed combination can in some cases be excised from the combination and the claimed combination may be directed to a sub combination or variation of a sub combination.

Similarly while operations are depicted in the drawings in a particular order this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order or that all illustrated operations be performed to achieve desirable results. In certain circumstances multi tasking and parallel processing may be advantageous. Moreover the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.

A number of implementations have been described. Nevertheless it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly other implementations are within the scope of the following claims. For example the actions recited in the claims can be performed in a different order and still achieve desirable results.

