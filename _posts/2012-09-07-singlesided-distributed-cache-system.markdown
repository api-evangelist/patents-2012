---

title: Single-sided distributed cache system
abstract: A distributed cache system including a data storage portion, a data control portion, and a cache logic portion in communication with the data storage and data control portions. The data storage portion includes memory hosts, each having non-transitory memory and a network interface controller in communication with the memory for servicing remote direct memory access requests. The data control portion includes a curator in communication with the memory hosts. The curator manages striping of data across the memory hosts. The cache logic portion executes at least one memory access request to implement a cache operation. In response to each memory access request, the curator provides the cache logic portion a file descriptor mapping data stripes and data stripe replications of a file on the memory hosts for remote direct memory access of the file on the memory hosts through the corresponding network interface controllers.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09164702&OS=09164702&RS=09164702
owner: Google Inc.
number: 09164702
owner_city: Mountain View
owner_country: US
publication_date: 20120907
---
A distributed system generally includes many loosely coupled computers each of which typically include a computing resource e.g. processor s and storage resources e.g. memory flash memory and or disks . A distributed storage system overlays a storage abstraction e.g. key value store or file system on the storage resources of a distributed system. In the distributed storage system a server process running on one computer can export that computer s storage resources to client processes running on other computers. Remote procedure calls RPC may transfer data from server processes to client processes.

A remote procedure call is a two sided software operation initiated by client software executing on a first machine and serviced by server software executing on a second machine. Servicing storage system requests e.g. read data in software may require an available processor which may place a significant limitation on a distributed storage system. In the case of a distributed storage system this means a client process cannot access a remote computer s storage resources unless the remote computer has an available processor to service the client s request. Moreover the demand for processor resources and storage resources in a distributed system often do not match. In particular computing resource i.e. processors may have heavy and or unpredictable usage patterns while storage resources may have light and very predictable usage patterns.

Typical logic implementing a distributed cache system can be divided between client and server jobs. Server jobs placed on machines across a cluster respond to remote procedure calls RPC from clients instructing the server jobs to store or retrieve cache data on the corresponding machines on which the jobs reside. The server jobs may require access not only to low latency storage capacity but also to computation time of a central processing unit CPU . The CPU time is required to process RPCs compute cache placement policies mappings from cache blocks to local storage addresses manage cache eviction policies in order to manage limited cache storage space and provide concurrency control amongst many concurrent requests server jobs are often multi threaded in order to provide low latency and high throughput .

This coupling of storage and computation requirements for cache server jobs can cause low utilization and or high latency in general purpose computing clusters where server jobs are co located with other jobs on the cluster s nodes which is counter to the very purpose of the distributed cache. For example nodes in the cluster with unused random access memory RAM that could be used by cache servers may not have any spare CPU cycles with which to serve cache requests. In this case the stranded RAM goes unused. On the other hand nodes with spare CPU cycles can experience contention for those cycles which leads to high latencies for the cache server s RPCs. In this case the only remedy may be to accept the high latencies or run the nodes at lower CPU utilization.

One aspect of the disclosure provides a distributed cache system that includes a data storage portion a data control portion and a cache logic portion in communication with the data storage and data control portions. The data storage portion includes memory hosts each having non transitory memory and a network interface controller in communication with the memory for servicing remote direct memory access requests. The data control portion includes a curator in communication with the memory hosts. The curator manages striping of data across the memory hosts. The cache logic portion executes at least one memory access request to implement a cache operation. In response to each memory access request the curator provides the cache logic portion a file descriptor mapping data stripes and data stripe replications of a file on the memory hosts for remote direct memory access of the file on the memory hosts through the corresponding network interface controllers.

Implementations of the disclosure may include one or more of the following features. In some implementations the cache logic portion includes a cache service having a cache data layer storing cache data in files and a cache indexing layer indexing the cache data stored in the files. Each data storing file may implement a fixed size first in first out queue having a front and a back with a tail pointer providing an offset to the front of the queue referred to as a circular data file. Other replacement polices such as least recently used LRU random not most recently used etc. may be used as well. The cache service may shard the data into a set of data files e.g. circular data files each file storing cache entries each cache entry comprising cache data a cache tag and a cache fingerprint.

The cache indexing layer may include a set associative tag map indexing the cache data stored in the data files. The set associative tag map maps cache tag fingerprints to locations of the corresponding cache data in the data files.

In some implementations the cache logic portion includes a transaction interface and a cache interface. The transaction interface executes an atomic transaction including at least one of a read operation or a write operation on files stored on the memory hosts. The cache interface interfaces between a client and the transaction interface for executing at least one of a look up operation an insert operation or a remove operation. The transaction executes a commit operation. For data chunks of a read set of the transaction the commit operation includes reading data of the data chunks of the read set through remote direct memory access and determining a validity of the read data by evaluating a version and a lock of each data chunk of the read set. For data chunks of a write set of the transaction the commit operation includes setting locks on the data chunks of the write set writing data to the locked data chunks through remote direct memory access releasing the locks of the locked data chunks and incrementing a version number of each released data chunk.

In some implementations the transaction aborts the commit operation when at least one data chunk of the write set fails to receive a lock or when the read data is invalid. For any data chunks having an unreleased lock the curator may reset those data chunks to an uninitialized state and releases their locks. The commit operation may include rereading the data of the data chunks of the read set when the previous read is invalid.

To allow a durable transaction the commit operation may include reading existing data of the data chunks of the write set before writing new data to the data chunks of the write set and writing the existing data of the data chunks of the write set to a durable intent log. The commit operation may include aborting the transaction when at least one data chunk of the write set fails to receive a lock or when the read data is invalid. For any data chunks having an unreleased lock the method includes retrieving their data stored in the durable intent log writing the retrieved data to the corresponding data chunks e.g. to reconstruct the data and releasing the locks of those data chunks.

In some implementations the curator returns location information of data on the memory hosts in response to the client memory access request. The curator may return a key to allow access to data on the memory hosts in response to the client memory access request. The curator may allocate storage of a data stripe on the memory hosts. Each file stored on the memory hosts may be divided into data stripes and each data stripe may be replicated into multiple storage locations of the memory hosts. In some implementations the curator stores a file map mapping files to file descriptors.

The file descriptor may include at least one of the following a file state attribute providing a state of a file a data chunks attribute providing a number of stripe replicas per stripe a stripe length attribute providing a number of bytes per stripe and a sub stripe length attribute providing a number of bytes per sub stripe. In some examples the file descriptor includes an array of stripe protocol buffers each describing a data stripe replica within a data stripe. Each stripe protocol buffer may include at least one of a stripe replica handle an identity of the memory host holding the stripe replica and a current state of the stripe replica.

Another aspect of the disclosure provides a method of accessing a distributed cache system. The method includes receiving a cache operation from a client executing at least one memory access request and for each memory access request returning a file descriptor mapping data stripes and data stripe replications of a file on memory hosts for remote direct memory access of the file on the memory hosts. The method also includes executing on a computing processor a transaction comprising at least one of a read operation or a write operation on files stored on the memory hosts to implement the cache operation.

In some implementations the cache operation includes at least one of a look up operation an insert operation or a remove operation. The method may include storing data in files on the memory hosts and indexing the data stored in the files. The method may include sharding the data into a set of data files. Each file stores cache entries and each cache entry includes cache data a cache tag and a cache fingerprint. In some examples the method includes mapping cache tag fingerprints to locations of the corresponding cache data in the data files.

The insert operation may include receiving data receiving a cache tag associated with the data selecting a data file for insertion of the data and executing a push back operation to add a cache entry including the data and the cache tag at a back of a queue of the selected circular file. The queue has a first in first out eviction policy.

The push back operation may include reading an offset of a back of the queue of the data file from a corresponding tail pointer writing the cache entry to the data file at the offset and updating the offset in the tail pointer. The method may also include reading a region of the data file being overwritten to identify overwritten cache tags and updating a set associative tag map.

Updating the set associative tag map may include reading a set of the tag map corresponding to the cache tag of the written cache entry inserting an entry corresponding to the tag at a back of the set and dropping an entry at a front of the set. Additionally or alternatively updating the set associative tag map may include reading a set of the tag map corresponding to the evicted cache tags and removing entries from the set corresponding to the evicted cache tags. The method may include receiving a cache tag reading a set of a tag map containing a fingerprint of the cache tag and removing an entry from the set of the tag map corresponding to the fingerprint.

In some implementations the method includes accessing a file map mapping files to file descriptors to return the file descriptor in response to the memory access request. The method may include returning location information of data on the memory hosts in response to the client memory access request. The method may include returning a key to allow access to data on the memory hosts in response to the client memory access request. In some examples the method includes allocating storage of a data stripe on the memory hosts. The method may include dividing the file into data stripes and replicating each data stripe into multiple storage locations of the memory hosts.

For data chunks of a read set of the transaction the method may include reading data of the data chunks of the read set through remote direct memory access and determining a validity of the read data by evaluating a version and a lock of each data chunk of the read set. For data chunks of a write set of the transaction the method may include setting locks on the data chunks of the write set writing data to the locked data chunks through remote direct memory access releasing the locks of the locked data chunks and incrementing a version number of each released data chunk.

In some implementations the method includes aborting the transaction when at least one data chunk of the write set fails to receive a lock or when the read data is invalid. For any data chunks having an unreleased lock the method may include resetting those data chunks to an uninitialized state and releasing their locks e.g. for a non durable transaction . The method may include rereading the data of the data chunks of the read set when the previous read is invalid.

To allow a durable transaction the method may include reading existing data of the data chunks of the write set before writing new data to the data chunks of the write set and writing the existing data of the data chunks of the write set to a durable intent log. The method may include aborting the transaction when at least one data chunk of the write set fails to receive a lock or when the read data is invalid. For any data chunks having an unreleased lock the method includes retrieving their data stored in the durable intent log writing the retrieved data to the corresponding data chunks e.g. to reconstruct the data and releasing the locks of those data chunks.

The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects features and advantages will be apparent from the description and drawings and from the claims.

A caching system may store the results of high latency computational operations e.g. disk seeks database query results etc. in storage devices with significantly lower latency e.g. dynamic random access memory DRAM or flash storage for later retrieval. A distributed cache system may store its data across remote resources of an entire computer cluster. The computer cluster may reduce the latency of computational tasks by serving as many results as possible from low latency storage and increase the utilization of these resources throughout the cluster by putting otherwise unused resources to work as cache storage.

Referring to in some implementations a distributed cache system includes loosely coupled memory hosts e.g. computers or servers each having a computing resource e.g. one or more processors or central processing units CPUs in communication with storage resources e.g. memory flash memory dynamic random access memory DRAM phase change memory PCM and or disks that may be used for caching data. A storage abstraction e.g. key value store or file system overlain on the storage resources allows scalable use of the storage resources by one or more clients . The clients may communicate with the memory hosts through a network e.g. via RPC .

The single sided distributed cache system may eliminate the need for any cache server jobs for responding to remote procedure calls RPC from clients to store or retrieve cache data on their corresponding memory hosts and may rely on specialized hardware e.g. network interface controllers to process remote requests instead. Single sided refers to the method by which most of the request processing on the memory hosts may be done in hardware rather than by software executed on CPUs of the memory hosts . Rather than having a processor of a memory host e.g. a server execute a server process that exports access of the corresponding storage resource e.g. non transitory memory to client processes executing on the clients the clients may directly access the storage resource through a network interface controller NIC of the memory host . In other words a client process executing on a client may directly interface with one or more storage resources without requiring execution of a routine of any server processes executing on the computing resources . This offers a single sided distributed storage architecture that offers relatively high throughput and low latency since clients can access the storage resources without interfacing with the computing resources of the memory hosts . This has the effect of decoupling the requirements for storage and CPU cycles that typical two sided distributed cache systems carry. The single sided distributed cache system can utilize remote storage resources regardless of whether there are spare CPU cycles on that memory host furthermore since single sided operations do not contend for server CPU resources a single sided system can serve cache requests with very predictable low latency even when memory hosts are running at high CPU utilization. Thus the single sided distributed cache system allows higher utilization of both cluster storage and CPU resources than traditional two sided systems while delivering predictable low latency.

In some implementations the distributed cache system includes a cache logic portion a data control portion and a data storage portion . The cache logic portion may include a cache application programming interface API e.g. software library that lies between a client application process executing on a client accessing a distributed cache e.g. storage resources on the memory hosts and a transaction API e.g. a single sided transactional system client library that is responsible for accessing the underlying data via single sided operations. The data control portion may manage allocation and access to cache storage with tasks such as allocating cache storage registering cache storage with the corresponding network interface controller setting up connections between the client s and the memory hosts handling errors in case of machine failures etc. The data storage portion may include the loosely coupled memory hosts 

In some implementations the distributed cache system stores cache data in dynamic random access memory DRAM and serves the cache data from the remote hosts via remote direct memory access RDMA capable network interface controllers NICs . A network interface controller also known as a network interface card network adapter or LAN adapter may be a computer hardware component that connects a computing resource to the network . The network controller implements communication circuitry using a specific physical layer OSI layer 1 and data link layer layer 2 standard such as Ethernet Wi Fi or Token Ring. This provides a base for a full network protocol stack allowing communication among small groups of computers on the same LAN and large scale network communications through routable protocols such as Internet Protocol IP . Both the memory hosts and the client may each have a network interface controller for network communications. A host process executing on the computing processor of the memory host registers a set of remote direct memory accessible regions of the memory with the network interface controller . The host process may register the remote direct memory accessible regions of the memory with a permission of read only or read write. The network interface controller of the memory host creates a client key for each registered memory region 

In some implementations the network is an InfiniBand network which is a switched fabric communications link generally used in high performance computing and enterprise data centers. It features high throughput low latency quality of service failover and scalability. The InfiniBand architecture specification defines a connection between processor nodes and high performance I O nodes such as storage devices. The InfiniBand network conveys remote direct memory access RDMA requests from a client to a memory host . At the memory host an RDMA capable InfiniBand network interface controller NIC performs reads and writes of the storage resource e.g. DRAM . RDMA uses zero copy OS bypass to provide high throughput low latency access to data e.g. 4 GB s of bandwidth and 5 microsecond latency . The distributed storage system may use RDMA remote procedure calls or other data access methods to access data.

The single sided operations performed by the network interface controllers may be limited to simple reads writes and compare and swap operations none of which may be sophisticated enough to act as a drop in replacement for the software logic implemented by a traditional cache server job to carry out cache requests and manage cache policies. The cache API translates commands such a look up or insert data commands into sequences of primitive NIC operations that correctly implements a desired caching policies e.g. placement and replacement . The cache API interfaces with the transaction API to interact with the data control and data storage portions of the distributed cache system . For data structures that would implement a cache in the memory of a single isolated machine the distributed cache system lays out these data structures across the address spaces of the memory hosts rather than in the virtual address space of a single machine . The client s may access and mutate the data structures following the same algorithms as if they were local to their own machine allowing the cache API translate the requested reads and writes into remote memory accesses.

The distributed cache system may include a co located software process to register memory for remote access with the network interface controllers and set up connections with client processes . Once the connections are set up client processes can access the registered memory via engines in hardware of the network interface controllers without any involvement from software on the local CPUs of the corresponding memory hosts .

Referring to in some implementations the distributed cache system includes multiple cells each cell including memory hosts and a curator in communication with the memory hosts . The curator e.g. process may execute on a computing processor e.g. server connected to the network and manages the data storage e.g. manages a file system stored on the memory hosts controls data placements and or initiates data recovery. Moreover the curator may track an existence and storage location of data on the memory hosts . Redundant curators are possible. In some implementations the curator s track the striping of data across multiple memory hosts and the existence and or location of multiple copies of a given stripe for redundancy and or performance. In computer data storage data striping is the technique of segmenting logically sequential data such as a file in a way that accesses of sequential segments are made to different physical storage devices e.g. cells and or memory hosts . Striping is useful when a processing device requests access to data more quickly than a storage device can provide access. By performing segment accesses on multiple devices multiple segments can be accessed concurrently. This provides more data access throughput which avoids causing the processor to idly wait for data accesses.

In some implementations the transaction API interfaces between a client e.g. the cache API which interfaces with the client process and the curator . In some examples the client communicates with the curator through one or more remote procedure calls RPC . In response to a client request the transaction API may find the storage location of certain data on memory host s and obtain a key that allows access to the data. The transaction API communicates directly with the appropriate memory hosts via the network interface controllers to read or write the data e.g. using remote direct memory access . In the case that a memory host is non operational or the data was moved to a different memory host the client request fails prompting the client to re query the curator .

Referring to in some implementations the curator stores and manages file system metadata . The metadata includes a file map that maps files to file descriptors . The curator may examine and modify the representation of its persistent metadata . The curator may use three different access patterns for the metadata read only file transactions and stripe transactions. Read only access allows the curator to examine a state of the metadata with minimal contention. A read only request returns the most recent state of a file but with no synchronization with concurrent updates. The read only access may be used to respond to lookup requests from clients e.g. for internal operations such as file scanning .

Referring also to in some implementations the memory hosts store file data . The curator may divide each file and its data into stripes and replicate the stripes for storage in multiple storage locations. A stripe replica is also referred to as a chunk or data chunk . Mutable files may have additional metadata stored on the memory host s such as lock words and version numbers. The lock words and versions numbers may be used to implement a distributed transaction commit protocol.

File descriptors stored by the curator contain metadata such as the file map that maps the stripes to data chunks i.e. stripe replicas stored on the memory hosts . To open a file a client sends a request to the curator which returns a file descriptor . The client uses the file descriptor to translate file chunk offsets to remote memory locations . After the client loads the file descriptor the client may access the file s data via RDMA or another data retrieval method.

Referring to RDMA is a connection based process to process communication mechanism so RDMA connections typically do not support authentication or encryption by themselves. As a result the distributed storage system may treat the RDMA connections as secure resources. In order for a client process to access the memory of a host process through RDMA the network interface controller of the memory host executes a connection handshake with a network interface controller of the client process to establish the RDMA capable connection between the host process and the client process . The RDMA connection handshake may implement a higher level secure protocol that evaluates the identities of the host and client processes as known at the time of creation of the trusted RDMA connection . After an RDMA capable connection is established the client process or the host process can unilaterally break the connection . If either the client process or the host process dies the client and or the memory host via operating systems can tear down the corresponding RDMA connection s .

Access to file data e.g. data chunks stored in remote memory locations may be controlled by access control lists . Each access control list may have a unique name a list of data chunks and a list of clients that have permission to read and write the data chunks associated with that access control list . In some examples the access control list provides an access permission level for each associated client or each associated data chunk . The memory hosts may receive the access control lists through a secure communication channel and can be enforced by the memory hosts using protection domains . Each RDMA accessible memory region registered with the network interface controller of each memory host is associated with a protection domain . In some implementations when the curator allocates memory for the data chunks it associates the allocated memory regions of the data chunks with one or more protection domains . A memory host may have many protection domains associated with various regions of its memory . Each protection domain may also have one or more associated connections .

When a client instantiates a memory access request for a file stored on one or more of the memory hosts the client requests a file descriptor from the curator to identify which memory host s store the data chunks of the file . In addition to mapping data chunks of the file to memory regions of memory hosts the file descriptor may also include a client key for accessing those data chunks . The client then searches a connection cache for any open RMDA capable connections to the identified memory hosts . If each memory host fails to have an open connection with the client that is in the same protection domain as the requested data chunk s the client sends a connection request to any memory hosts not having the necessary open connection s .

In response to receiving a connection request from a client process of a client to access a data chunk e.g. to access a memory region storing the data chunk the host process may establish a remote direct memory access capable connection with the client process when both the client and the requested data chunk are associated with the same access control list received by the memory host . The client process may include the access control list in the connection request . The host process may associate the established open connection with a protection domain and the client process may store the open connection in the connection cache . The connection is capable of accessing via RDMA only the memory regions associated with its protection domain . The network interface controller of the memory host may tear down the connection upon receiving an RDMA request having an address for unregistered memory .

In the example shown in first and second clients send memory access requests to a memory host over respective first and second RDMA connections . The memory host has first and second protection domains associated with its memory . The first protection domain is associated with first and second memory regions e.g. storing corresponding first and second data chunks and the first RDMA connection while the second protection domain is associated with a third memory region e.g. storing a corresponding third data chunks and only the second RDMA connection

The first client sends first and second memory access requests over the first RMDA connection to the memory host . The first memory access request is for accessing the second memory region for the second data chunk and the second memory access request is for accessing the third memory region for the third data chunk . The first memory access request succeeds because the second memory region belongs to the same protection domain as the first connection . The second memory access request fails because the third memory region belongs to a different protection domain the second protection domain rather than the protection domain of the second memory access request i.e. the first protection domain .

The second client sends third and fourth memory access requests over the second RDMA connection to the memory host . The third memory access request is for accessing the first memory region for the first data chunk and the fourth memory access request is for accessing the third memory region for the third data chunk . In this case both memory access requests succeed because the RDMA connection of the second client belongs to the protection domains of both the first memory region and the third memory region

Referring again to in some implementations the curator can create copy resize and delete files . Other operations are possible as well. To service a copy request from a client the curator creates a new file descriptor having a state initially set to COPY PENDING. The curator may set initialize one or more of the following fields size owner group permissions and or backing file. The curator populates a stripes array of the file descriptor with empty stripes and then commits the file descriptor to its file map . Committing this information to the file map allows the curator to restart a resize operation if the curator crashes or a tablet containing the file system metadata migrates to another curator . Once the curator commits the file descriptor to the file map the curator responds to the client copy request by informing the client that the copy operation has been initiated. The curator initiates memory host pull chunk operations which instruct memory hosts to allocate a new chunk and to read chunks of the backing file into the memory of the memory hosts . When a pull chunk operation returns successfully the curator adds the new chunk to the appropriate stripe in the file descriptor . The curator commits the stripe with the new chunk to the file map .

In the case of a crash or a migration incrementally updating the file descriptors allows a new curator to restart a copy operation from the location the prior curator stopped. This also allows clients to check the status of a copy operation by retrieving the file descriptor e.g. via a lookup method and inspecting the number of stripes in the file descriptor populated with chunks . Once all chunks have been copied to the memory hosts the curator transitions the state of the file descriptor to READ and commits it to the file map .

The curator may maintain status information for all memory hosts that are part of the cell . The status information may include capacity free space load on the memory host latency of the memory host from a client s point of view and a current state. The curator may obtain this information by querying the memory hosts in the cell directly and or by querying a client to gather latency statistics from a client s point of view. In some examples the curator uses the memory host status information to make rebalancing draining recovery decisions and allocation decisions.

The curator s may allocate chunks in order to handle client requests for more storage space in a file and for rebalancing and recovery. The curator may maintain a load map of memory host load and liveliness. In some implementations the curator allocates a chunk by generating a list of candidate memory hosts and sends an allocate chunk request to each of the candidate memory hosts . If the memory host is overloaded or has no available space the memory host can deny the request. In this case the curator selects a different memory host . Each curator may continuously scan its designated portion of the file namespace examining all the metadata every minute or so. The curator may use the file scan to check the integrity of the metadata determine work that needs to be performed and or to generate statistics. The file scan may operate concurrently with other operations of the curator . The scan itself may not modify the metadata but schedules work to be done by other components of the system and computes statistics.

For each file descriptor the file scan may ensure that the file descriptor is well formed e.g. where any problems may indicate a bug in either the curator or in the underlying storage of the metadata update various statistics such as the number of files stripes chunks and the amount of storage used look for stripes that need recovery determine if the file descriptor contains chunks that are candidates for rebalancing from overfull memory hosts determine if there are chunks on draining memory hosts determine if there are chunks that are candidates for rebalancing to under full memory hosts determine chunks that can be deleted and or determine if the file descriptor has a pending resize or copy operation but there is no active task within the curator working on the operation.

In some implementations the distributed storage system supports two types of files immutable and mutable. Immutable files rely on a disk based file system for persistence and fault tolerance. A client may copy immutable files into the file system of the distributed storage system . On the other hand a client may write mutable files into the file system of the distributed storage system using the transaction application programming interface API . The storage system may or may not be durable. The distributed storage system may have strict data loss service level objectives SLOs that depend on the files level of replication. When a stripe is lost the curator may allocate new storage for the lost stripe and mark the data as uninitialized. A client attempting to read an uninitialized stripe receives an uninitialized data error. At this point the client can reinitialize the stripe s data.

The file descriptor may provide the state of a file . A file can be in one of the following states READ READ WRITE DELETED or CREATE COPY RESIZE PENDING. In the READ state clients can read the file but not write to the file . Read only files are read only for the entire life time of the file i.e. read only files are never written to directly. Instead read only files can be copied into the file system from another file system. A backing file may be used to restore data when a memory host crashes consequently the backing file persists for the entire life time of the file . In the READ WRITE state clients with the appropriate permissions can read and write a mutable file s contents. Mutable files support concurrent fine grain random writes. Random and sequential write performance may be comparable. Writes are strongly consistent that is if any client can observe the effect of a write then all clients can observe the effect of a write. Writes can also be batched into transactions. For example a client can issue a batch of asynchronous writes followed by a sync operation. Strong consistency and transactional semantics ensure that if any client can observe any write in a transaction then all clients can observe all writes in a transaction. In the DELETED state the file has been deleted. The chunks belonging to the file are stored in a deleted chunks field and wait for garbage collection. The CREATE COPY RESIZE PENDING state denotes a file has a create copy or resize operation pending on the file.

An encoding specified by a file encoding protocol buffer of the file descriptor may be used for all the stripes within a file . In some examples the file encoding contains the following fields data chunks which provides a number of data chunks per stripe stripe length which provides a number of bytes per stripe and sub stripe length which provides a number of bytes per sub stripe. The sub stripe length may be only valid for READ WRITE files. The data for a file may be described by an array of stripe protocol buffers in the file descriptor . Each stripe represents a fixed region of the file s data identified by an index within the array. The contents of a stripe may include an array of chunk protocol buffers each describing a chunk within the stripe including a chunk handle an identity of the memory host holding the chunk and a current state of the chunk . For RDMA purposes the chunk protocol buffers may also store a virtual address of the chunk in the memory host and a client key e.g. a 32 bit key. The client key is unique to a chunk on a memory host and is used to RDMA read that chunk .

Stripes can be further divided into sub stripes with associated sub stripe metadata . Each sub stripe may include an array of sub chunks each having corresponding associated sub chunk metadata .

Chunks can be in one of the following states OK Recovering Migrating Source and Migrating Destination. In the OK state the contents are valid and the chunk contributes to the replication state of the corresponding stripe . Clients may update all chunks in a good state. In the Recovering state the chunk Recovering is in the process of being recovered. The chunk Recovering does not count towards the replicated state of the corresponding stripe and the data in the chunk is not necessarily valid. Therefore clients cannot read data from chunks in the Recovering state. However all transactions not reaching their commit point at the time a chunk state changes to the Recovering state must include the Recovering chunk in the transaction in order to ensure that the chunk s data is kept up to date during recovery.

In the Migrating Source state the chunk is in the process of migrating. A migrating source attribute may provide a location from which the chunk is migrating. The source chunk counts towards the replication of the stripe and the data in the chunk is valid and can be read. In the Migrating Destination state the chunk is in the process of migrating. A Migrating Destination attribute provides the location to which the chunk is migrating. The source chunk does not count towards the replicated state of the stripe and the chunk is not necessarily valid. Therefore clients cannot read from chunks in the Migrating Destination state. However all transactions not reaching their commit point at the time a chunk s state changes to the Migrating Destination state must include the Migrating Destination chunk in the transaction in order to ensure the chunk s data is kept up to date as it is being migrated.

Each file descriptor may have a dead chunks array. The dead chunks array holds additional chunks that are no longer needed such as the chunks that made up a file that has since been deleted or made up previous instances of the file . When the file is deleted or truncated the chunks from all the stripes are moved into this dead chunks array and the stripes are cleared. The chunks in the dead chunks array are reclaimed in the background.

The transaction API may facilitate transactions having atomicity consistency isolation durability to a degree such that the transaction may be serializable with respect to other transactions. ACID atomicity consistency isolation durability is a set of properties that guarantee that database transactions are processed reliably. In the context of databases a single logical operation on the data is called a transaction. Atomicity requires that each transaction is all or nothing if one part of the transaction fails the entire transaction fails and the database state is left unchanged. An atomic system guarantees atomicity in each and every situation including power failures errors and crashes. Consistency ensures that any transaction brings the database from one valid state to another. Any data written to the database must be valid according to all defined rules including but not limited to constraints cascades triggers and any combination thereof. Isolation ensures that no transaction should be able to interfere with another transaction. One way of achieving this is to ensure that no transactions that affect the same rows can run concurrently since their sequence and hence the outcome might be unpredictable. This property of ACID may be partly relaxed due to the huge speed decrease this type of concurrency management entails. Durability means that once a transaction has been committed it will remain so even in the event of power loss crashes or errors. In a relational database for instance once a group of SQL statements execute the results need to be stored permanently. If the database crashes immediately thereafter it should be possible to restore the database to the state after the last transaction committed.

Referring to in some implementations a cache service instance includes a set of files contained within a directory of the file system of the distributed cache system . Thus a cache service instance can be identified by a file path containing the files e.g. passing a cache path parameter to open a handle to a cache instance . The cache service may include two caching layers a cache data layer and a cache indexing layer . The cache data layer stores cache data in data files and the cache indexing layer indexes the cache data stored in the data files . The cache indexing layer may have a set associative placement policy and a first in first out FIFO replacement policy within each set of a tag map . The cache data layer may be sharded into many separate files e.g. circular data files . An instance of a cache service may include two types of data structures laid out in the files of the distributed storage system a set of circular data files and a set associative tag map . Moreover the cache service may be associated with a cell of distributed cache system .

The cache data layer includes circular data files which contain the actual cached data . Each cache entry in the circular data files may contain the corresponding cache data a cache tag and a fingerprint to aid in identifying the cache data entry during look up and insert operations for example. The circular data files may have a fixed size and evictions may occur in a first in first out FIFO order when additional space is needed. Each circular data file includes a FIFO queue having a front and a back . An offset to the front of the FIFO queue of the circular data file can be stored in an auxiliary tail pointer file . Thus the construct of a circular data file may actually include two physical files. The caching service may shard the cache data into a set of many circular data files via consistent hashing. This allows tuning of the cache for contention as concurrent insertion operations to the data files may contend for the back of the queue . Circular data files in a given cache file path may be identified by a 32 bit integer identifier so that pointers to data files can be limited to a fixed small size.

The set associative tag map may function as an index of the cache data stored in the data files . The set associative tag map maps cache tag fingerprints to locations of the corresponding cache data in the circular data files i.e. a struct containing a data file id offset and size of the cache entry . The caching service may implement the mapping as a set associative cache where the cache tags are 64 bit fingerprints and the data payloads are pointers to regions of circular data files e.g. memory regions . Eviction within each set of the tag map may be handled in a FIFO order. In practice the tag map can be sized to ensure that conflict misses caused by its restrictive placement policy are rare so that the tag map can be considered a mostly complete index for the data contained in the circular data files . Moreover the tag map may be stored in a file .

In some implementations the cache API implements cache operations including look up insert and remove operations. The cache look up operation of a cache tag identifies the cache tag in the tag map by fingerprinting the cache tag to obtain a corresponding fingerprint and then applying a mapping function to map the fingerprint to a set in the tag map . The client may read the set from the cache and search for a cache entry matching the fingerprint . If a result is not found the look up operation reports a cache miss if the result is found the look up operation reads a region of the corresponding circular data file pointed to by the cache entry . The look up operation checks that the data read actually corresponds to the cache tag and returns a hit with the corresponding data as illustrated in . In some implementations the cache look up operation does not need to modify any globally shared state e.g. book keeping of most recently used timestamps but rather can be a read only operation. Alternatively cache look up operation may modify some shared space shared statistics counters or states that may help implement some other caching policy such as least recently used LRU random not most recently used etc. File access may be implemented with a transactional protocol that ensures that a cache access is atomic and serializable with respect to concurrent mutations.

Referring to in some implementations the cache insert operation receives a cache tag with the data and selects a circular data file for insertion of the data by taking a consistent hash of the cache tag . The cache insert operation executes a push back operation to add a cache entry containing the cache tag and the data to the back of the FIFO queue of the selected circular data file . The push back operation includes reading an offset of the back of the FIFO queue of the circular data file from a corresponding tail pointer file . If there is not enough room between the offset and an end of the circular data file to store the cache entry the push back operation wraps around to the beginning i.e. the front of the circular data file by taking the offset to be zero. The push back operation also includes reading a region of the circular data file that will be overwritten and parsing out a set of cache tags being evicted. The push back operation includes writing the cache entry the cache tag and data pair to the circular data file at the offset and writing a new offset to the tail pointer e.g. the previous offset a size of the cache entry .

Referring to in some implementations after inserting the cache entry i.e. the tag and data pair into the circular data file the insert operation updates the tag map by reading the set corresponding to the tag e.g. as in the look up operation inserting an entry e.g. a pointer for the tag at the back of that set removing an existing entry for the cache tag if it already exists and dropping the entry at the front of the FIFO order in the set . The new entry may point to the offset of the circular data file .

The insert operation may read the tag map sets look for cache tags evicted from the circular data file and remove the corresponding entries from their corresponding sets . The insert operation may use the transaction API so that all writes are applied atomically upon successfully committing the transaction. The transaction commit protocol ensures that the operations are serializable with respect to other concurrent mutations.

The remove operation may receive a cache tag and read the tag map set containing the fingerprint for the cache tag . If the cache tag fingerprint is in the set the remove operation removes the corresponding entry from that set adjusting the FIFO ordering to fill the resulting hole. The remove operation may not do anything to the corresponding bytes in the circular data file . Instead those bytes may be left stranded until reclaimed by a subsequent insert operation pushing the front of the FIFO queue past this range of the circular data file .

The cache service may check accesses to the underlying files for uninitialized data resulting from a data loss event e.g. a memory host going down . The granularity at which uninitialized data occurs may be at the set level for the tag map the entire pointer for the circular data file s tail pointer or a single sub stripe for the circular data file .

For handling uninitialized data encountered on any reads executed during the look up operation the cache service may return a miss to the client since the requested data either no longer exists if the data loss occurred in the desired region of the circular data file or cannot be readily found if the data loss occurred in the set that the tag fingerprint should reside in . The client may insert the data back into the cache after reading it from a backing data source.

For data loss encountered while executing the insert operation the cache service may ensure that the data structures are restored to a valid state during the operation so that future operations work as intended. To do so the cache service may handle data loss encountered in each stage of the insert operation. When encountering a data loss while reading the tail pointer of the circular file the cache service resets the tail pointer to zero e.g. essentially starting over the FIFO ordering at the beginning of the circular data file . As result future insert operations remove some elements out of FIFO order within that circular data file .

When encountering a data loss while reading the region of circular data file into which the inserted data will be written it is not possible to parse out the cache tags that will be evicted nor is it possible to know where is the next cache entry to be evicted after the uninitialized region lies which is necessary to process the next insert operation . In some implementations the cache service scans forward looking for a magic number to identify the beginning of a cache entry . In other implementations the cache service sets the tail pointer to the end of the cache entry being written stops processing overwritten entries for eviction until the circular data file is filled and wraps around to the beginning of the FIFO queue of the circular data file once more. This has the effect of leaving invalid pointers i.e. set entries in the tag map due to leaving overwritten cache tags in the tag map . The cache service addresses this possibility in the look up operation by validating the data read from the circular data files and reporting a miss if the read data does not match the requested tag . In this manner the cache service lazily evicts the invalid set entries pointers from the tag map .

When encountering a data loss while reading the tag map set in order to push the new tag to the back of the FIFO queue of the circular data file the cache service may treat the set as empty. This strands the cache entries in the circular data file pointed to by the lost set entries pointers but the capacity can be reclaimed once the front of the FIFO queue of the circular data file reaches those bytes.

A client may instantiate a cache service with parameters such as a file path in which the circular data files are created a cache size e.g. a total size of the circular data files in bytes a number of shards e.g. a number of circular data files to create and an average block size e.g. an average size of a cache access in bytes . The average block size may be used to choose the number of tag map entries based on the expected number of tags that the cache has capacity to store adjusted by a multiple to accommodate conflict misses . A client may reconfigure the cache service as well. For example the client may adjust the cache size number of shards block size and or a number of tag map entries . The client may reconfigure the cache service during active use by client applications. A client may delete a cache service using its file path .

For tuning the cache service the client may consider the following parameters 1 total cache data size equal to number of data files times size per data file 2 number of data files 3 number of entries in the tag map and 4 set associativity of the tag map .

Capacity misses can occur in both the tag map or in the data file . The latter may occur when the total size in bytes of the data files is less than that of the working set . The former may occur when the number of tags accessed by the working set is greater than the maximum number of entries in the tag map .

Conflict misses may occur in the tag map due to its bounded set associativity and FIFO replacement policy. To mitigate these conflict misses the client may increase the set associativity. The tradeoff for increasing the set associativity is an increase in bandwidth since reads may execute on an entire set at a time increased processing time to search a set for a tag s fingerprint and increased probability of transactional conflicts since transactional read and write sets become larger . In some implementations to mitigate the number of conflict misses in the tag map the client may increase the number of sets while holding the set associativity constant.

Conflict misses can also occur in the data files due to their FIFO replacement policy as well as the placement policy of sharding entries to the set of many circular data files . Since data may be assigned to data files via consistent hashing the load may be reasonably well distributed amongst the circular data files . Provided that each data file has enough capacity to store a reasonably large number of entries the effects of these conflict misses may be small.

Increasing the number of data files reduces write contention for the front of each of the FIFO queues . A client accessing a cache service with too few data files may see a large number of transactional conflicts when trying to insert values into the cache i.e. non ok status on calling transaction commit . On the other hand if the cache is sized to require very few insertions or only a few tasks actually perform insertions then the need for many data files is lessened.

Referring to in some implementations a client may use a transaction object to read from or write to the cache . In some examples the transaction must be active i.e. started but not committed or in an error state when passed to a cache service method. No mutations caused by calls to these methods may be visible to other tasks accessing the cache until the client successfully commits the transaction at which point the distributed cache system may check the transaction for serializability with respect to other concurrent transactions

The transaction application programming interface API includes a reader class and a transaction class . A client may instantiate a reader inheriting the reader class to execute a read or batches of reads on the memory hosts in a cell . Moreover the client may instantiate a transaction inheriting the transaction class to execute one or more reads and or writes. The reads and writes in a transaction may be to different files in a cell but in some implementations all reads and writes in a transaction must be to files in the same cell . Executed reads may be snapshot consistent meaning that all reads in a transaction can see a snapshot of the file at a logical instant in time. Writes can be buffered until the client tries to commit the transaction

Referring to in response to receiving a write memory access request for a file a transaction may acting as a writer write or modify data of the file e.g. of chunks and or sub chunks . After the write operation the transaction may compute a checksum of the modified data and associate the checksum with the modified data e.g. with the chunks and or sub chunks . In some examples the transaction stores the checksum in the sub chunk metadata for the modified sub chunk . The transaction may execute a hash function such as a cryptographic hash function to compute the checksum . Moreover the hash function may be configured for randomization. Each checksum may be a word having at least 64 bits. A network interface controller servicing the remote direct memory access requests on a corresponding memory host may determine the checksum of any data accessed on its memory host .

When a client adds a file read request to the reader e.g. via a transaction the reader translates the read request into a RDMA read network operation and stores a state of the network operation in memory allocated for the reader . Reads that cross chunk boundaries get translated into multiple RDMA operations.

In some implementations to translate a file read request into a RDMA read network operation the reader computes a target stripe number from a file offset of the read request . The reader may use the stripe number to index into a chunk handle cache. The chunk handle cache returns a network channel to access the corresponding chunk and a virtual address and r key of the chunk . The reader stores the network channel and r key directly in an operation state of the RDMA read. The reader uses the virtual address of the chunk and the file offset to compute the virtual address within the chunk to read. The reader computes the offset into a memory block supplied by the client e.g. a receiving memory block for each RDMA read operation . The reader may then initialize an operation status.

While buffering new reads the reader may calculate and store a running sum of the amount of metadata that will be retrieved to complete the read. This allows metadata buffer space to be allocated in one contiguous block during execution minimizing allocation overhead.

In response to receiving a memory access request from the client the transaction may retrieve a file descriptor from the curator that maps requested data chunks of a file on memory hosts for remote direct memory access of those data chunks on the memory hosts . The file descriptor may include a client key for each data chunk of the file . Moreover each client key allows access to the corresponding data chunk on its memory host .

Referring to in some implementations the reader executes a read operation in two phases. In the first phase the reader reads the data and associated metadata of a file . In the second phase the reader validates that the data read in the first phase satisfies data consistency constraints of the reader . In the first phase the reader identifies one or more memory locations corresponding to the data and transmits its RDMA read operations. While iterating through and transmitting RDMA reads the reader initializes and transmits RDMA reads to read sub chunk metadata and to read data needed to compute checksums of the sub chunks such as of the first and last sub chunks in an unaligned file access. After the data and metadata are received the reader may check lock words in the sub chunk metadata to ensure that the sub chunks were not locked while the data was being read. If a sub chunk was locked the reader rereads the sub chunk and its corresponding metadata . Once the reader finds reads all of the sub chunk locks in an unlocked state the reader computes the sub chunk checksums and compares the computed checksums with the checksums read from the sub chunk metadata .

In other words for detecting read write conflicts the reader in response to receiving a read memory access request for data of a file stored in the memory hosts of a cell may compute a first checksum of the data compare the first checksum with a second checksum associated with the data e.g. stored in the metadata of the corresponding sub chunk and allow a read operation on the data when the first and second checksums match. The reader may execute a hash function such as a cryptographic hash function to compute the checksums . The reader may read the data and metadata associated with the data after receiving the read write request and before processing the read write request . Moreover the reader may determine whether the data was locked while reading the data for example by evaluating a lock word and or a version number stored in the metadata . The reader rereads the data and associated metadata when the data was locked while previously reading the data .

While checksums are commonly used to guard against hardware error or even software error using it to guard against what is actually normal operation poses certain additional requirements. Since a conflict may not be a rare event the chance of getting a coincidentally matching checksum can be minimized by having checksum size large enough to provide a relatively small probability of a coincidental match. In some examples a 64 bit checksum is sufficient since checking a random bad checksum every nanosecond may produce a false positive less than once every five centuries which is much less frequent than the rates of other types of system failures. Additionally a hash function for computing the checksum may produce different numbers for all common modifications of the data. For example simply adding up all the data would not suffice since a change that simply re ordered some of the data would not change the checksum. However a cryptographic hash functions which by design does not allow simple modifications of the data to produce any predictable checksum may be sufficient.

A sub chunk checksum may fail a compare for one of three reasons 1 the data read was corrupted by a concurrent write 2 the data was corrupted while in transit to the client or 3 the data stored in the memory host is corrupt. Cases 1 and 2 are transient errors. Transient errors are resolved by retrying the sub chunk read. Case 3 is a permanent error that may require the client to notify the curator of a corrupt sub stripe

To differentiate between a transient error and a permanent error the client may re read the sub chunk data and the sub chunk metadata . The reader then checks a sub chunk lock word and re computes and compares the sub chunk checksum . If the checksum error still exists and a sub chunk version number has changed since the sub chunk was initially read then the checksum compare failure was likely caused by a concurrent write so the reader retries the sub chunk read. If the version number has not changed since the sub chunk was initially read then the error is permanent and the reader notifies the curator and the curator tries to reconstruct the data of the chunk . If the curator is unable to reconstruct the chunk data the curator replaces the old chunk with a new uninitialized chunk .

Unlike locking the checksum compare method for detecting read write conflicts does not actually care if a conflicting write existed as long as the data is consistent. For example if the data is being overwritten with identical data or if a write is preparing to start but has not actually begun or has just finished the locking method will cause the read to fail unnecessarily while the checksum compare will allow the read to succeed. Since the time between locking and unlocking may be much greater than the duration of an actual write this can be a significant improvement.

The reader does not know which version of the data it has read and it may not matter. If it is advantageous to have the read obtain a version number this may be done without an additional round trip latency penalty if the version number itself is covered by the checksum . Although computing checksums may incur a nontrivial penalty in processor time both for the reader and the writer a checksum may be necessary anyway to guard against hardware errors depending on the implementation.

Sub chunk locks may become stuck due to a client trying to execute a transaction but crashing during a commit protocol of the transaction . A reader can detect a stuck lock by re reading the sub chunk lock word and version number . If a sub chunk lock word and version number do not change during some time out period then the sub chunk lock is likely stuck. When the reader detects a stuck lock it notifies the curator of the stuck lock and the curator recovers the sub stripe and resets the stuck lock.

Referring also to in some implementations after the reader validates each sub chunk lock word and or checksum the reader may proceed to the second phase of executing the read operation i.e. the validation phase . To validate the values the reader rereads sub chunk metadata and rechecks if the sub chunk lock words are unlocked and the sub chunk version numbers have not changed since the version numbers were initially read during the first phase of the read operation. In other words the reader may read an initial version number and an initial lock value associated with each data chunk of a read set of the transaction . After reading the data the reader reads a final version number and a final lock value associated with each data chunk of the read set and determines the read data as valid when the initial version number matches the final version number and the initial lock value matches the final lock value

If the reader is associated with a transaction the reader may reread the metadata associated with all sub chunks read by the transaction . If a single sub chunk version number mis compares the reader returns an error. If all sub chunk version numbers are the same the reader discards the prefix and suffix of the reader memory block in order to trim extraneous data read to compute the checksum of the first and last sub chunks in the read. The reader may set a status to OK and returns to the client .

If the reader encounters an error on a network channel while reading data or metadata of a chunk the reader may select a different chunk from the chunk handle cache and notifies the curator of a bad memory host. If no other good chunks exist from which the reader can read the reader may wait to receive a response to the error notification it sent to the curator . The response from the curator may contain an updated file descriptor that contains a new good chunk to read from.

In some implementations the transaction class uses validation sets to track which sub stripes have been read by the transaction . Each read of a transaction adds the version numbers of all sub stripes read to a validation set of the transaction . The transaction may validate the validation set in two cases 1 as part of the commit protocol and 2 the validation phase of reads of a transaction . A transaction may fail to commit if the commit protocol finds that any sub stripe version number differs from the number recorded in the validation set . Validation of the full validation set before data is returned to the client allows early detection e.g. before the commit phase of a doomed transaction . This validation also prevents the client from getting an inconsistent view of file data.

A transaction may provide a synchronous serializable read operation e.g. using a reader . In some examples a reader is instantiated and associated with the transaction . Read results of the reader return the latest committed data. As such uncommitted writes of the same transaction are not seen by a read of that transaction

A transaction may buffer data for a later transaction commit. The transaction class translates a buffer write request into one or more prepare write network operations. One network operation is needed for each stripe touched by the write operation. Processing a buffer write request may involve preparing sub stripe lock network operations. One lock operation is needed for each sub stripe touched by the requested write. These operations are buffered for transmission during the transaction commit. The transaction may translate buffer write requests into network operations and execute identify or coalesce writes that affect the same region of a file . The transaction may apply write operations in the same order by the memory hosts for all chunks to ensure that all replicas are consistent.

The transaction may provide a commit operation that results in all reads and writes in the transaction being schedulable as a single atomic serializable operation. In some implementations the transaction commit protocol proceeds through a lock phase a validate phase a write phase and an unlock phase. During the lock phase the sub stripe lock network operations which were created in response to buffer write requests are sent. Each sub stripe lock operation executes an atomic compare and swap operation on the lock word in all replicas . If the contents of the lock word match the specified compare data e.g. a client identifier the lock word is written with the specified swap data and the previous contents of the word are returned. If the client succeeds in writing its unique client ID into the metadata lock word it has successfully taken the lock. If the transaction fails to take the lock for any sub stripe in the write set the commit fails and is aborted. The commit protocol proceeds to the validate phase once all sub stripe locks are held.

During the validate phase the transaction may read the version number out of the metadata for all sub stripes referenced in the validation set and comparing the version numbers to the version numbers recorded in the validation set. If a version number does not match the sub stripe was written by another transaction after it was read by this transaction so the transaction fails. In this case the reader releases the locks it holds and returns a transaction conflict error to the client . Once all version numbers in the validation set have been validated the client writes the buffered write data of the transaction to each replica and updates the metadata associated with each sub stripe written by the transaction during the write phase. Updating metadata of a sub stripe may include computing and writing a new check word and incrementing the version number of the sub stripe . Once all data and metadata has been updated the transaction releases the locks that it holds during the unlock phase.

File transaction access may provide exclusive read write access to the state of a file descriptor . Updates to the file state may be applied at the end of a transaction and are atomic. File transaction access can be used for operations such as creating finalizing and deleting a file . These operations may require the curator to communicate with other components such as memory hosts and thus a file transaction access may last for several seconds or more. While active the file transaction access blocks any other operations that need to modify the state of the file descriptor . Read access may not be blocked.

To reduce contention stripe transaction access may provide relatively finer grain synchronization for operations that only need to modify the state of a single stripe with the file descriptor . This mode can be used for stripe operations such as opening closing rebalancing and recovering. There can be many concurrent stripe transactions for different stripes within a file but stripe transactions and file transactions are mutually exclusive. Within a stripe transaction the curator may examine the state of a stripe and various fields of the file descriptor that remain immutable for the duration of the transaction such as the file encoding and instance identifier. The stripe transaction access does not provide access to fields that can change underfoot such as the state of other stripes . Operations may hold only one active transaction at a time to avoid deadlock. Moreover transactions may only atomically commit on a single file .

In some implementations the method includes associating the established connection with a protection domain having an associated memory region storing the data chunk . The connection is capable of accessing only memory regions associated with its protection domain . Moreover the protection domain is capable of being associated with one or more connections . The method may include allocating memory for the data chunk e.g. via the curator and associating the allocated memory with the protection domain .

The method may include executing a connection handshake with the client to establish the remote direct memory access capable connection . In some examples the method includes evaluating an identity of the client such as by executing a client authentication routine. If a received remote direct memory access request includes an address for unregistered memory the method may include tearing down the connection .

The method may include registering remote direct memory accessible regions of memory with a network interface controller of the memory host . The remote direct memory accessible memory regions may be registered with a permission of read only or read write.

In some implementations the method includes allocating memory for the data chunk e.g. via the curator and associating the data chunk with the access control list . The method may include assigning an access permission for each client or each data chunk associated with the access control list .

In response to receiving a memory access request from the client the method may include returning a file descriptor that maps data chunks of the file on memory hosts for remote direct memory access of the data chunks on the memory hosts . The file descriptor may include a client key for each data chunk of the file . Moreover each client key allows access to the corresponding data chunk on its memory host .

The method may include receiving a file descriptor mapping data chunk to the memory hosts . The file descriptor may include a client key allowing access to the corresponding data chunk on its memory host . The method may include sending a memory access request for multiple data chunks stored in the memory of the memory host where two or more of the data chunks are associated with different access control lists .

In some implementations the method includes accessing a file map mapping files to file descriptors to return the file descriptor in response to the memory access request . The method may include returning location information of data on the memory hosts in response to the client memory access request . The method may include returning a key to allow access to data on the memory hosts in response to the client memory access request . In some examples the method includes allocating storage of a data stripe on the memory hosts . The method may include dividing the file into data stripes and replicating each data stripe into multiple storage locations of the memory hosts .

In some implementations the method includes providing at least one of a file state attribute providing a state of a file a data chunks attribute providing a number of stripe replicas per stripe a stripe length attribute providing a number of bytes per stripe and a sub stripe length attribute providing a number of bytes per sub stripe in the file descriptor . The method may include providing in the file descriptor an array of stripe protocol buffers each describing a data stripe replica within a data stripe

Servicing storage requests in hardware provides a number of advantages such as having relatively simple storage requests e.g. read write . Implementing such functionality in an application specific integrated circuit ASIC can be much more efficient than implementing the functionality in software running on a general purpose processor. This efficiency improvement means storage requests can be serviced in less time and occupy fewer circuits when compared to implementing the same functionality in software running on a general purpose processor. In turn this improvement means a distributed storage system can achieve lower latency and higher throughput without increasing the cost of the system.

Servicing storage requests in the network interface hardware e.g. NIC decouples processor resources and storage resources . A client can access storage resources on a memory host without available processor resources . This allows system builders and administrators to operate a distributed storage system with high processor utilization with low and predictable latencies and without stranding storage resources. In some implementations the distributed storage system can provide an average read latency of less than 10 microseconds an average read throughput of greater than 2 million operations per second per client and average write latency of less than 50 microseconds and or an average write throughput of greater than 500 thousand operations per second per client.

In some implementations the method includes executing a hash function such as a cryptographic hash function to compute at least one of the checksums . The method may include associating metadata with the data e.g. the sub chunk where the metadata includes a checksum word containing the checksum associated with the data . In some examples the method includes reading the data and metadata associated with the data after receiving the read write request and before processing the read write request . The method may include determining whether the data was locked while reading the data and or evaluating a lock word and a version number stored in the metadata . When the data was locked while previously reading the data the method may include rereading the data and associated metadata .

The method may include identifying one or more memory locations corresponding to the data and transmitting remote direct memory access requests to each identified memory location . Moreover the method may include receiving a file descriptor of a file containing the data . The file descriptor maps data stripes and data stripe replications of the file on memory hosts for remote direct memory access. The method may include accessing a file map mapping files to file descriptors to return the file descriptor . In some examples the method includes receiving a key e.g. in the file descriptor allowing access to the data on the memory hosts .

In some implementations the method includes aborting the transaction when at least one data chunk of the write set fails to receive a lock or when the read data is invalid. For any data chunks having an unreleased lock the method may include resetting e.g. via the curator those data chunks to an uninitialized state and releasing their locks . The method may include rereading the data of the data chunks of the read set when the previous read is invalid. In some examples the method includes reading existing data of the data chunks of the write set before writing new data to those data chunks and writing the existing data of the data chunks of the write set to a durable intent log to allow later reconstruction of the data if the write operation fails so as to provide a durable transaction

The method may include aborting the transaction when at least one data chunk of the write set fails to receive a lock or when the read data is invalid. For any data chunks having an unreleased lock the method may include marking the data as uninitialized and releasing the lock for non durable transactions or retrieving their existing data stored in the durable intent log writing the retrieved data to the corresponding data chunks to reconstruct the data and the releasing the locks of those data chunks to provide a durable transaction

The method may include reading an initial version number and an initial lock value associated with each data chunk of the read set . After reading the data the method includes reading a final version number and a final lock value associated with each data chunk of the read set and determining the read data as valid when the initial version number matches the final version number and the initial lock value matches the final lock value . Setting locks on the data chunks of the write set may include for each data chunk retrieving a lock word associated with the data chunk comparing data of the lock word with compare data and writing swap data e.g. a unique client identifier to the lock word when the lock word data and the compare data match. The method may include accessing metadata associated with each data chunk that includes a version number and a lock word containing a lock value.

Various implementations of the systems and techniques described here can be realized in digital electronic circuitry integrated circuitry specially designed ASICs application specific integrated circuits computer hardware firmware software and or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and or interpretable on a programmable system including at least one programmable processor which may be special or general purpose coupled to receive data and instructions from and to transmit data and instructions to a storage system at least one input device and at least one output device.

These computer programs also known as programs software software applications or code include machine instructions for a programmable processor and can be implemented in a high level procedural and or object oriented programming language and or in assembly machine language. As used herein the terms machine readable medium and computer readable medium refer to any computer program product apparatus and or device e.g. magnetic discs optical disks memory Programmable Logic Devices PLDs used to provide machine instructions and or data to a programmable processor including a machine readable medium that receives machine instructions as a machine readable signal. The term machine readable signal refers to any signal used to provide machine instructions and or data to a programmable processor.

Implementations of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry or in computer software firmware or hardware including the structures disclosed in this specification and their structural equivalents or in combinations of one or more of them. Moreover subject matter described in this specification can be implemented as one or more computer program products i.e. one or more modules of computer program instructions encoded on a computer readable medium for execution by or to control the operation of data processing apparatus. The computer readable medium can be a machine readable storage device a machine readable storage substrate a memory device a composition of matter effecting a machine readable propagated signal or a combination of one or more of them. The terms data processing apparatus computing device and computing processor encompass all apparatus devices and machines for processing data including by way of example a programmable processor a computer or multiple processors or computers. The apparatus can include in addition to hardware code that creates an execution environment for the computer program in question e.g. code that constitutes processor firmware a protocol stack a database management system an operating system or a combination of one or more of them. A propagated signal is an artificially generated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.

A computer program also known as an application program software software application script or code can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data e.g. one or more scripts stored in a markup language document in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules sub programs or portions of code . A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.

The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by and apparatus can also be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit .

Processors suitable for the execution of a computer program include by way of example both general and special purpose microprocessors and any one or more processors of any kind of digital computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally a computer will also include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. However a computer need not have such devices. Moreover a computer can be embedded in another device e.g. a mobile telephone a personal digital assistant PDA a mobile audio player a Global Positioning System GPS receiver to name just a few. Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory media and memory devices including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry.

To provide for interaction with a user one or more aspects of the disclosure can be implemented on a computer having a display device e.g. a CRT cathode ray tube LCD liquid crystal display monitor or touch screen for displaying information to the user and optionally a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input. In addition a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user for example by sending web pages to a web browser on a user s client device in response to requests received from the web browser.

One or more aspects of the disclosure can be implemented in a computing system that includes a backend component e.g. as a data server or that includes a middleware component e.g. an application server or that includes a frontend component e.g. a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification or any combination of one or more such backend middleware or frontend components. The components of the system can be interconnected by any form or medium of digital data communication e.g. a communication network. Examples of communication networks include a local area network LAN and a wide area network WAN an inter network e.g. the Internet and peer to peer networks e.g. ad hoc peer to peer networks .

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other. In some implementations a server transmits data e.g. an HTML page to a client device e.g. for purposes of displaying data to and receiving user input from a user interacting with the client device . Data generated at the client device e.g. a result of the user interaction can be received from the client device at the server.

While this specification contains many specifics these should not be construed as limitations on the scope of the disclosure or of what may be claimed but rather as descriptions of features specific to particular implementations of the disclosure. Certain features that are described in this specification in the context of separate implementations can also be implemented in combination in a single implementation. Conversely various features that are described in the context of a single implementation can also be implemented in multiple implementations separately or in any suitable sub combination. Moreover although features may be described above as acting in certain combinations and even initially claimed as such one or more features from a claimed combination can in some cases be excised from the combination and the claimed combination may be directed to a sub combination or variation of a sub combination.

Similarly while operations are depicted in the drawings in a particular order this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order or that all illustrated operations be performed to achieve desirable results. In certain circumstances multi tasking and parallel processing may be advantageous. Moreover the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.

A number of implementations have been described. Nevertheless it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly other implementations are within the scope of the following claims. For example the actions recited in the claims can be performed in a different order and still achieve desirable results.

