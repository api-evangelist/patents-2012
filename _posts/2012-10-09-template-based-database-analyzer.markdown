---

title: Template based database analyzer
abstract: An automated database analyzer is uses templates to accommodate multiple database languages, such as SQL and its dialects. The templates are combined with metadata that define a database schema and operations on the database schema. An SQL file instantiates the database schema on a database system being tested. Operations on the database schema may then be performed to assess the performance of the database system being tested.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09311345&OS=09311345&RS=09311345
owner: SAP SE
number: 09311345
owner_city: Walldorf
owner_country: US
publication_date: 20121009
---
Unless otherwise indicated herein the disclosures in this section are not admitted to be prior art by inclusion in this section.

Database products are becoming an increasing common element of a business enterprise. In memory database technologies such as the SAP HANA database produce are becoming more popular. Database designers have a wide range of technologies and design choices for implementing backend database systems in an enterprise. However each database product has its strong points and its weak points. A database designer must therefore be able to select the most suitable database product for their particular application.

Typically database designers spend quite a bit of time researching and identifying database candidates. Evaluation tools exist to assess a database s performance metrics but the limited scope of functionality provided the by the evaluation tools limits the usefulness of the performance metrics. For example typical evaluation tools focus on one or a few specific areas and so the database designer can assess only a small part of the entire database product. Evaluation tools are typically specific for a given database product and so testing across different database products with the same tool is not easily accomplished. Typical evaluation tools include 

Database products typically have their own native performance benchmarks functionality specs. simple test tools and so on. However the tools cannot accommodate different combinations of hardware environments and application requirements. In addition there usually is no support for secondary development because of limited and proprietary application programming interfaces APIs to the database product.

Disclosed embodiments relate to methods and systems for generating and running test scenarios to assess performance in a database system. In the following description for purposes of explanation numerous examples and specific details are set forth in order to provide a thorough understanding of the present disclosure. It will be evident however to one skilled in the art that the present disclosure as defined by the claims may include some or all of the features in these examples alone or in combination with other features described below and may further include modifications and equivalents of the features and concepts described herein.

In some embodiments the database analyzer may comprise a system console a structured query language SQL generator an analysis runner a trace manager and a report center . The system console may serve as a management interface for functions performed by the database analyzer . In some embodiments a user e.g. a database administrative user may access the system console using a command line interface CLI . It will be appreciated of course that in other embodiments other interface designs may be used.

The user may access the database analyzer functionality via the system console . The system console may control operation of the other modules that comprise the database analyzer in response to the user s input. The system console may coordinate actions among the modules. The user may obtain current process status information of test runs being performed by the database analyzer performance information about ongoing or previous test runs and so on. The user may initiate single runs of a test scenario or a loop run of the test scenario.

As will be discussed below a test run refers to running a test scenario and a test scenario refers to a series of database operations on the DBMS . A test scenario may specify a database schema to be created in the DBMS and a set e.g. hundreds to thousands of database statements that are applied to the database schema to simulate a live environment of users of the database schema. Running a test scenario therefore serves to test the capabilities of the DBMS the performance of the DBMS and so on. A single run refers to executing the test scenario once and generating performance results from performance data collected during the run while a loop run refers to executing the test scenario multiple times accumulating the performance data from each execution run and generating performance results from the accumulated performance data e.g. by averaging the data . In a typical use case the user may initially perform a single run of the test scenario to verify the database operations of the test scenario and to identify any logic errors allowing for the user to troubleshoot and refine the test scenario as needed. After the test scenario is verified performance assessment may be conducted by performing one or more loop runs of the test scenario. Additional details of a test scenario will be described below.

Continuing with the SQL generator may serve to generate SQL files that constitute the test scenarios. Although disclosed embodiments show the use of SQL as the database language of the DBMS it is noted that database languages other than SQL may be supported. An SQL file comprises the SQL statements for exercising the DBMS in accordance with a test scenario. The test scenario may be embodied in the SQL statements that comprise an SQL file. The test scenario may include performing SELECT operations JOIN operations various aggregation operations and so on. The test scenario may comprise any combination and number of SQL statements.

In some embodiments the SQL generator may comprise a template provider a metadata provider and a generator . The template provider may access a data store not shown of template files to provide a selected template file to the generator . The metadata provider likewise may access a data store not shown of metadata files to provide a selected metadata file to the generator . The generator may then use information contained in the selected template file and the selected metadata file to produce an SQL file . Template files and metadata files will be discussed in more detail below.

The analysis runner may scan the SQL file to run the test scenario embodied in the SQL statements in the SQL file against the DBMS . Accordingly in some embodiments the analysis runner may interface with the DBMS via the database API in order to issue SQL statements to the DBMS and to receive responses from the DBMS.

In some embodiments the analysis runner may include several test modules for running the test scenario. Each test module may test the DBMS in a specific way to assess a particular aspect of the DBMS. For example a functional stability module may assess how well the DBMS can handle certain functions such as aggregation of large database tables and so on. A performance module may assess the execution times of operations in the DBMS when processing database operations e.g. SELECT operations JOIN operations and so on. A memory usage module may assess the memory management capability of the DBMS e.g. peak memory usage may be monitored. And so on. Accordingly the analysis runner may interface with the database performance API in order to gather performance data and other environment information collectively referred to herein as performance data that is generated and provided by the DBMS .

In some embodiments the trace manager may receive performance data collected by the analysis runner . The trace manager may store the performance data for each run whether from a single run or multiple runs of a loop run. The trace manager may also store and manage system logs and other information related to runs of a test scenario.

The report center may receive data as a data stream from the trace manager and prepare reports on performance of the DBMS . In some embodiments the report center may comprise three configurable components. A data collector may choose the data that a user wants to generate a report for. In some embodiments the data collector may be a file scanning and data filtering application implemented in standard Java file manipulating API. The user may define a filter type that the data collector uses to retrieve data in the data stream from the trace manager . Merely as an example suppose the user wants to report two days worth of data from a run of a test scenario. A date filter may be used to filter for data matching the two days of interest. Additional data filters may be used to filter for specific kinds of data e.g. memory usage data processor usage data and so on.

The report center may include an analyzer that performs various calculations on the data received by the data collector . Calculations may include averages and variances statistical analyses trend analyses predictive modeling and so on. Analytics generated by the analyzer may be provided to a report generator to report on the analytics. The analytics may be presented in any suitable format. For example the Flex reporting tool may be used to render reports for delivery to web users. The report may be formatted in an Excel file and so on. The analytics may include charts such as bar charts pie charts line charts and so on.

At database analyzer may receive a metadata file e.g. the metadata provider provides metadata file to the generator . As will be explained in more detail below a metadata file defines a test scenario and specifies a database schema and database operations to be performed on the database schema. The user may specify via the system console a metadata file from among a set of predefined metadata files. In some embodiments the user may modify a predefined metadata file which the metadata provider may then provide to the generator as a customized metadata file. In some embodiments the user may define their own customized metadata file from scratch. The customized metadata file may be added to the repository of predefined metadata files for future use.

At the database analyzer may obtain data values that will be loaded into the database tables that comprise the database schema specified in the metadata file received at . In some embodiments the data values may be obtained from the user s actual database system not shown in the figures . Merely as an example suppose the DBMS is a candidate database system that the user is considering as a replacement for their existing database system. Accordingly the data values that are to be loaded into the database schema may be obtained from the user s existing database system. For example the database analyzer may be interfaced to the user s existing database system using a suitable API. At the user may specify which database tables in their existing database system to obtain data values from.

In other embodiments the data values at may be randomly generated by one or more random value generators in the generator . For example the generator may include a random generator to generate random text another random generator to generate random numbers and so on. The use of randomly generated data values may be preferable where the data is sensitive e.g. where issues of private information may be involved. For example employee salaries medical information and so on may very well be private sensitive information that should remain confidential. Accordingly the data values used in a test scenario that are deemed sensitive or confidential may be replaced by randomly generated values. In some embodiments the data values obtained at may comprise data values from the user s existing data base system and randomly generated data values.

At the database analyzer may generate a test scenario that comprises a set of SQL statements stored in an SQL file. For example the user may enter a command from the system console to initiate generating a test scenario using the specified template file the specified metadata file and a source of data values from the user s existing database system randomly generated or some combination of both .

The test scenario may specify a database schema that is created on the DBMS being assessed and database statements for accessing and otherwise operating on the database schema. Accordingly in some embodiments the database analyzer e.g. via the generator may generate at a set of SQL statements for creating the database schema e.g. a series of SQL CREATE statements. The SQL statements may include INSERT statements to load the database schema with data values. These SQL statements may constitute a PREPARE section in the SQL file. At the database analyzer may generate a set of SQL statements that access or otherwise operate on the database schema. These statements constitute the database operations that exercise the DBMS and may form the basis on which performance of the DBMS will be assessed. These statements may be stored in an EXECUTE section of the SQL file. In some embodiments clean up SQL statements may be generated at for deleting the database schema after testing of the DBMS has concluded. These statements may constitute a CLEAN section of the SQL file.

In accordance with principles of the present disclosure the generator may use the grammar and syntax specified in the template file received at and specifications in the metadata file received at to generate grammatically and syntactically correct SQL statements PREPARE section that can be executed by the DBMS to instantiate a database schema in the DBMS and to load data into the instantiated database schema. Likewise the generator may generate correct SQL statements EXECUTE section that can be executed by the DBMS to access and operate on the database schema which form the basis for assessing the DBMS.

When the user wants to run a test scenario the user may issue a command at the system console to run a test scenario. The user may identify the test scenario of interest and specify a single run or a loop run of N runs of the test scenario. The user may designate which test modules to use for testing the DBMS . In some embodiments for example each test module may have a corresponding configuration file containing test instructions which control data collection by the analysis runner . Merely as an example the functional stability module may have an instruction record when statement execution time 5 seconds to identify statements with slow execution times. The user may configure the configuration file to specify the kind of information to be recorded during a run.

The database analyzer may then access the corresponding SQL file and execute the different sections in the accessed SQL file. Thus at the database analyzer e.g. via the analysis runner may scan the accessed SQL file to identify the SQL statements in the PREPARE section of the accessed SQL file. The SQL statements may then be executed to create an instantiation of the database schema on the DBMS . For example the analysis runner may issue the SQL statements comprising the PREPARE section of the accessed SQL file to the DBMS via the database API . The SQL statements may create the database schema and may insert data values into the database schema.

At the database analyzer may execute the EXECUTE section of the accessed SQL file to perform testing of the DBMS . For example the database analyzer e.g. via the analysis runner may scan the accessed SQL file to identify the SQL statements in the EXECUTE section of the accessed SQL file. In the case of a single run the analysis runner may scan through the EXECUTE section of the accessed SQL file once to send each SQL statement into to the DBMS . In the case of a loop run the analysis runner may scan through the EXECUTE section of the accessed SQL file as many times as the user had specified.

As the DBMS performs database operations the DBMS may generate and collect data relating to its performance resource usage and so on. Such information may be made available outside of the DBMS via the database performance API . Accordingly as the DBMS operates at the analysis runner may gather performance data at from the DBMS. The analysis runner may provide the performance data at to be recorded and otherwise managed by the trace manager .

At the end of running a test scenario the user may issue a command via the system console to generate a report at . The user may delete the database schema at . In some embodiments deletion of the database schema may require an explicit command from the user. Since creating a database schema may take a long time automatically deleting the database schema after a test run may not be desirable.

A common database language is SQL. However many dialects of SQL have evolved over the years and different database vendors may use different SQL dialects. Accordingly in accordance with principles of the present disclosure template files specify the grammatical and syntactical structure of the particular dialect of SQL that the DBMS uses. It will be appreciated of course that the template files may represent database languages other than SQL.

In some embodiments a repository of template files may be provided for different SQL dialects. For example one or more template files may be defined for the SQL dialects used in SAP database systems template files may be defined for MySQL database systems and so on. Thus depending on the particular DBMS being assessed the user may select a predefined template file that specifies the SQL grammar and syntax for that DBMS. Some DBMS s may support multiple languages in which case there may be template files one for each language. In some embodiments a template builder tool may be provided to allow the user to modify a predefined template file to modify the SQL grammar or syntax if a particular DBMS is not supported by an existing template file.

As explained above the template file informs the generator how to generate grammatically and syntactically correct SQL statements. In accordance with the present disclosure the metadata file informs the generator what SQL statements to generate. The metadata file contains information metadata statements that specify operations to be performed on the DBMS but not how the operations are expressed the how is the job of the template file . Thus metadata file may contain metadata statements that specify a database schema to be instantiated on the DBMS . For example the metadata statements may specify the name of the database schema. The metadata statements may specify the database tables that comprise the database schema including the name of each database table the data fields for each database table the data types of each data field and so on. The metadata file may also include metadata statements for specifying operations to be performed on or accesses to be made on the instantiated database schema. The SQL statements that instantiate the database schema in the DBMS constitute the PREPARE section of the SQL file and SQL statements that operate on or otherwise access the instantiated database schema constitute the EXECUTE section of the SQL file.

The example shown in represents a fragment of a metadata file showing the metadata statements for a database table called PERSON. As can be seen the metadata statements also specify attributes of the constituent data fields such as data type data size and so on. The metadata statements may be represented as a Java collection. shows another embodiment in which the metadata statements are represented as a JavaScript Object Notation JSON file.

In some embodiments the user may create a metadata file to define the database schema that they would like to instantiate on the DBMS that is being evaluated. The user may specify in the metadata file the SQL statements for accessing and operating on the database schema. In other embodiments the database analyzer may include one or more predefined metadata files. Such predefined metadata files may be used to define generic test scenarios and thus avoid the user having to develop their own test scenarios. For example a predefined metadata file may adopt the well known pet store scenario used in testing J2EE applications as a generic test scenario that can be used as a standard test.

The data storage device may comprise a non transitory computer readable medium having stored thereon computer executable program code . The computer executable program code may be executed by the CPU to cause the CPU to perform steps of the present disclosure for example the steps set forth in . The data storage device may store data structures such as object instance data runtime objects and any other data described herein. In some embodiments for example the data structures may include template files metadata files and SQL files .

A user e.g. database administrator may interact with the computer system using suitable user interface devices . They may include for example input devices such as a keyboard a keypad a mouse or other pointing device and output devices such as a display. In some embodiments the user interface devices may comprise display device and a keyboard to access the system console via a command line interface.

All systems and processes discussed herein may be embodied in program code stored on one or more non transitory computer readable media. Such media may include for example a floppy disk a CD ROM a DVD ROM a Flash drive magnetic tape and solid state Random Access Memory RAM or Read Only Memory ROM storage units. It will be appreciated that embodiments are not limited to any specific combination of hardware and software. Elements described herein as communicating with one another are directly or indirectly capable of communicating over any number of different systems for transferring data including but not limited to shared memory communication a local area network a wide area network a telephone network a cellular network a fiber optic network a satellite network an infrared network a radio frequency network and any other type of network that may be used to transmit information between devices. Moreover communication between systems may proceed over any one or more transmission protocols that are or become known such as Asynchronous Transfer Mode ATM Internet Protocol IP Hypertext Transfer Protocol HTTP and Wireless Application Protocol WAP .

The database analyzer of the present disclosure provides a diversity of analysis modules such as stability performance memory usage and so on. The use of templates and metadata reduces the user s effort in setting up simulations of existing database behavior. The templates may support existing popular databases such as SQL MySQL Oracle MaxDB etc. The user can edit the template e.g. using a template builder tool to customize a template and thus cover a wide range of SQL dialects. The user can edit the metadata to implement arbitrary and complex logic. This allows the user to design test scenarios to identify potential processing and memory bottlenecks in the DBMS that is being tested.

For system architects or application developers the database analyzer can be used as a professional tool for doing case analysis. Detailed analysis reports including performance comparison and stability test can help them to make a correct decision as to which backend database solution should be used.

The above description illustrates various embodiments of the present disclosure along with examples of how aspects of the present disclosure may be implemented. The above examples and embodiments should not be deemed to be the only embodiments and are presented to illustrate the flexibility and advantages of the present disclosure as defined by the following claims. Based on the above disclosure and the following claims other arrangements embodiments implementations and equivalents will be evident to those skilled in the art and may be employed without departing from the spirit and scope of the disclosure as defined by the claims.

