---

title: Augmented reality using projector-camera enabled devices
abstract: An augmented reality scene may be registered onto an arbitrary surface. A camera may capture an image of the arbitrary surface. The camera may analyze the surface geometry of the arbitrary surface. In some embodiments, a processing computing device may analyze data captured by the camera and an adjacent camera to reconstruct the surface geometry of the arbitrary surface. A scene may be registered to a three dimensional coordinate system corresponding to the arbitrary surface. A projector may project the scene onto the arbitrary surface according to the registration so that the scene may not display as being distorted.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09064312&OS=09064312&RS=09064312
owner: THE REGENTS OF THE UNIVERSITY OF CALIFORNIA
number: 09064312
owner_city: Oakland
owner_country: US
publication_date: 20120924
---
This application claims benefit under 35 U.S.C. 119 e of U.S. Provisional Application having No. 61 538 414 filed Sep. 23 2011 which is hereby incorporated by reference herein in its entirety.

The invention described herein was made in the performance of official duties by one or more employees of the University of California University system and the invention herein may be manufactured practiced used and or licensed by or for the government of the State of California without the payment of any royalties thereon or therefor. The funding source or government grant number associated with inventions described herein is NSF IIS 0846144.

The embodiments of the present invention relate to image projection and more specifically to an augmented reality using projector camera enabled devices.

Lighting real three dimensional objects for example a rock wall that has texture and is generally uneven to create a compelling augmented reality experience is becoming popular for several applications like cultural heritage restoration architectural lighting geospatial analysis scientific visualization and theatrical lighting. A compelling experience may depend on the efficiency of augmenting the real three dimensional surface with high resolution vibrant and dynamic imagery. There may be a challenge in re creating a scene or an image on uneven surfaces. Elements of the scene may suffer from distortion as the image source does not compensate for the variations in the surface.

Accordingly there is a need for a system that may provide an augmented reality while compensating for non uniform surfaces.

According to one aspect of the present invention a system comprises a projector a camera coupled to the projector the camera being disposed to capture an image projected onto an arbitrary surface by the projector and a processing computing device coupled to the camera configured to recognize the underlying surface geometry of the arbitrary surface by analyzing data in the image captured by the camera and an adjacent camera register a scene within a three dimensional coordinate system for display on the arbitrary surface and control the projector to display the registered scene undistorted onto the arbitrary surface.

According to another aspect of the invention a method comprises capturing a set of images on an arbitrary surface while patterns are being projected on the arbitrary surface recognizing a surface geometry of the arbitrary surface and registering a scene corresponding to the surface geometry of the arbitrary surface.

According to a further aspect of the invention a computer readable storage medium includes a computer readable code when executed by a processing computing device configured to capture an image of an arbitrary surface analyze a surface geometry from the image of the arbitrary surface for surface height variations and register a scene corresponding to the surface height variations of the arbitrary surface.

These and other features aspects and advantages of the present invention will become better understood with reference to the following drawings description and claims.

The following detailed description is of the best currently contemplated modes of carrying out exemplary embodiments of the invention. The description is not to be taken in a limiting sense but is made merely for the purpose of illustrating the general principles of the invention since the scope of the invention is best defined by the appended claims.

As will be appreciated by one skilled in the art aspects of the present invention may be embodied as a computer program product. Accordingly aspects of the present invention may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects. Furthermore aspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium s having computer readable program code embodied thereon.

The computer readable medium may be a computer readable storage medium. A computer readable storage medium may be for example a non transitory or a tangible medium. Computer program instructions may also be stored in a computer readable medium that can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function act specified in the flowchart and or block diagram block or blocks. A computer readable storage medium may be for example an electronic magnetic optical electromagnetic infrared or semiconductor system or device.

Computer program instructions may also be loaded onto a computer a processing computing device a memory or other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

Broadly embodiments of the subject technology may provide an augmented reality on arbitrary surfaces. Embodiments may automatically register multiple projector camera enabled devices in a fully distributed manner on non planar surfaces. The term registration will be understood to refer to image registration for example by transforming sets of data into a coordinate system.

Referring to a system is shown according to an exemplary embodiment if the present invention. The system may include at least one projector and at least one camera . In some embodiments the system may include a plurality of cameras coupled to the projector . A processing computing device that includes a processor and a memory also referred to as a non tangible computer readable media may be coupled to the camera .

In some embodiments the camera may have a one to one correspondence with the projector . In some embodiments the projector and camera may be coupled together into a physical unit. In some embodiments the camera may be separate from the projector . In some embodiments the camera may correspond to multiple projectors . For example as shown in a camera may be coupled to one or more projectors either by hardwiring or wirelessly. The processing computing device in each camera may thus also control one or more projectors in the system . In an exemplary embodiment a plurality of projectors and cameras may be configured to provide an augmented reality scene on an arbitrary surface . The arbitrary surface may be represented as three dimensional coordinates in images captured by the camera s . The three dimensional coordinates of the arbitrary surface may be stored as a file in the memory . The arbitrary surface may be for example non planar uneven irregular or rough. The projectors may be disposed in a non uniform distribution for example in a non grid formation when pointed at the arbitrary surface . The projectors may be disposed to point at and project images onto the arbitrary surface . The cameras may be disposed to capture an image s of the arbitrary surface and images projected onto the arbitrary surface . In some embodiments the projector may be uncalibrated prior to projecting an image. Thus no prior knowledge of the arbitrary surface may be necessary when using the system . In some embodiments the camera may be uncalibrated when first acquiring an image of the arbitrary surface . Thus it may be appreciated that projectors and cameras may be pointed at any arbitrary surface without any pre stored data on the arbitrary surface and a stored scene may be registered to the underlying geometry when projected onto the arbitrary surface without distortion in a facilitated point and shoot manner.

The processing computing device may be configured to manage the camera s functionality. The camera may be configured to communicate wirelessly. For example the camera may include an IP address that can be referenced within a network. The processing computing device may also be configured to relay data among cameras in the system so that adjacent cameras may communicate either wirelessly or by hardwired fashion with one another.

In some embodiments the processing computing device may be configured to analyze data from the camera and control the projector according to the analyzed data. The processing computing device may be configured to analyze information from an image captured by the camera to recognize the underlying surface geometry of the arbitrary surface . The processing computing device may be configured to analyze the information related to the underlying surface geometry of the arbitrary surface for example for surface height variations. The processing computing device may register the scene from projectors onto the arbitrary surface in a three dimensional coordinate system so that projection of the image accounts for surface variations according to the three dimensional coordinate system and may be projected undistorted. Registering may include for example associating points in the scene to corresponding three dimensional coordinates along the surface geometry of the arbitrary surface so that the image appears as an undistorted three dimensional scene . For example when the underlying surface geometry of the arbitrary surface for example a rocky wall is analyzed the projectors may project onto the rocky wall a scene of a waterfall cascading down over the arbitrary surface so that the underlying surface is indiscernible from the water flow.

In an exemplary embodiment the camera may be disposed to capture an image of the arbitrary surface . In some embodiments the camera may be disposed so that its field of view captures only a portion of the arbitrary surface . In some embodiments two or more cameras may be disposed to capture overlapping portions of the arbitrary surface . The projector may be configured to project an image registered to the arbitrary surface that may be a portion of the overall scene . The camera may be configured to operate at a field of view that may be larger than the image projected by the projector . In embodiments using a plurality of projectors and cameras each camera may be disposed to capture the image of the projector corresponding to said camera as well as some portion of an image projected by a projector adjacent to the corresponding projector . Each camera may be configured to analyze a portion of the arbitrary surface where there may be some overlap between adjacent portions captured within the field of view of each camera .

Referring now to a method of providing relief lighting on an arbitrary surface in a system by groups of camera projector units labeled as PPP is shown according to an exemplary embodiment of the present invention. shows exemplary image projection and capture by projectors and cameras in the system . Each projector may project onto the arbitrary surface within a projector field of view shown as fields of view and for adjacent projectors . Each camera may capture an image of the arbitrary surface within camera field of view shown as fields of view and for adjacent cameras . The camera projector units may comprise the camera associated with the projector . The camera projector units may use plug and play type technology so that the projector may be interchangeably coupled with any other camera . The method may comprise a section of offline calibration and a section of online image correction.

In an exemplary embodiment N projectors to N cameras used in the camera projector units . Every part of the arbitrary surface may be covered by at least one projector and one camera while the number of total projectors and total cameras may be different. The coverage area of each camera may overlap with the coverage area of at least one other camera. The area covered by the projectors and the cameras may form a contiguous region. In an exemplary embodiment the cameras should be selected to have minimal non linear lens distortion or the lens distortion may be computed before determining camera calibration. For example a fisheye lens may be used. Additionally the projectors used should have minimal non linear lens distortion or their lenses distortion may be computed before camera calibration.

The processing computing device may perform configuration identification of projectors in the system in step . Configuration identification may include recovering connectivity data among the camera projector units in the context of overlap in their display space or sensing space. For example adjacent projectors may each project an image that partially overlaps within a projector overlap region shown as the dashed line ellipse on the arbitrary surface . Adjacent cameras may capture respective images that partially overlap within a camera overlap region shown as the dotted line ellipse on the arbitrary surface . The processing computing device may detect the IP address associated with adjacent cameras and may determine the location of the adjacent cameras . The processing computing device may determine the projector corresponding to adjacent cameras .

In an exemplary embodiment defining the configuration of the projectors and cameras may involve using two undirected graphs a camera adjacency graph A and a projector adjacency graph A. An edge in A i.e. A i j 1 may denote sufficient overlap in the field of view FOV of cameras represented as cameras Ci and Cj for example those cameras belonging to the pairs of camera projector units within the group to allow the use of structure from motion SfM techniques to recover their relative positions. Formally A i j 1 if the FOV of camera Ci covers more that 0

The processing computing device may provide geometric registration of the underlying geometry of the arbitrary surface . Geometric registration may include a step of relative camera calibration. Geometric registration may recover the parameters intrinsic and extrinsic of each camera Ci with respect to all its adjacent cameras. For each pair of camera projector units i and j such that A i j 1 the set of correspondences between cameras may be recovered and then used to recover the relative camera calibration parameters.

To recover the correspondences between cameras Ci and Cj binary structured light patterns SLP from Pi and Pj may be used. Each of these patterns may be seen by both cameras Ci and Cj allowing the recovery of the correspondences between Ci and Cj as follows. First camera projector unit j may process the images captured by camera Cj of SLP projected by projector Pi to find the correspondences Pi Cj. This may then be inverted via interpolation to find the correspondences Cj Pi. Similarly camera projection unit i may process the images captured by Ci of the same SLP projected by Pi to find correspondences Pi Ci. The correspondences between the two cameras Cj Ci may be recovered by first using Cj Pi followed by using Pi Ci. Similarly the SLP projected by Pj may be used to find more correspondences. Thus cameras Ci and Cj may communicate correspondences with each other creating a larger set of correspondences.

The priority assigned by the sorted IP addresses may be used in communicating information among the camera projection units . The camera projection unit with the highest priority may put up its SLP broadcasting it to all other camera projector units initiating a capture in all the adjacent camera projector units . The removal of SLP may also be communicated to all camera projector units via a broadcast message. Every camera projector unit may wait for all higher priority adjacent camera projector units to complete before it starts its SLP. This scheme may allow non overlapping camera projector units to put up their SLP for capture simultaneously making the process time efficient especially when considering a large number of camera projector units .

Once a camera projector unit i has recovered its correspondences with its neighbor camera projector unit j it may use an iteratively weighted least square method to find the fundamental matrix between Ci and Cj denoted by Fij. Assuming the same focal length camera projector unit i may then perform a self calibration routine to recover its camera focal length. Using this information each camera projector unit i may estimate a robustness factor rij for the estimated focal length with its neighbor camera projector unit j. The estimated focal length and rij may be broadcast to all camera projector units . On receiving this information from other camera projector units camera projector unit i may compute a weighted average of all the focal lengths weighted by a robustness factor rij. This may allow convergence to a consistent and robust estimate of f across all camera projector units . With a known Fijs and K where F may represent a fundamental matrix between camera views i and j and K may represent an intrinsic camera matrix each camera projector unit i may compute its essential matrix with respect to an adjacent camera projector unit j using the quantity KFijK. The corresponding points between cameras Ci and Cj and the essential matrix may be used to recover the relative rotation Rij and translation Tij of the camera pairs up to a scale factor. The recovered relative pose of camera Ci with respect to camera Cj may be denoted by Rij Tij where Rij is a 3 3 rotation matrix and Tij is a 3 1 translation vector.

In step the processing computing device may perform a partial geometric reconstruction of the underlying geometry of the arbitrary surface for all the points that are observed by a pair of cameras that are calibrated in the step relative camera calibration. For each calibrated pair of cameras the processing computing device may run stereo reconstruction to determine the three dimensional location of the data points that are observed by both of the cameras. The set of points determined from the different camera pairs may have different scale factors.

In step the processing computing device may perform a relative projector calibration. A calibration matrix may be computed for each projector with respect to one of the cameras . For each projector all the projector pixels that are covered by at least one camera pair may be reconstructed from the set of data points determined in the step of partial geometric reconstruction. These reconstructed data points may have different scale factors if observed by different camera pairs. The processing computing device may apply a non linear optimization to the reconstructed data points to recover the calibration matrix of the projector with respect to an arbitrary camera from one of these camera pairs. During this process the processing computing device may also recover a ratio of the scale factors for different camera pairs that observe the same projector.

In step the processing computing device may perform a complete camera calibration. In this step the processing computing device may compute the parameters of all the projectors and cameras relative to one of the cameras. For example one of the cameras may be designated as a reference camera. The processing computing device may use the computed relative camera calibration for camera projector units calculated in step to compute the parameters. The processing computing device may use the relative projector calibration and scale factors computed in step .

A linear optimization may be applied to values representing the relative camera calibration the relative projector calibration and the scale factors to determine a calibration matrix for the camera projector units with respect to the reference camera while satisfying all the relative calibration parameters. Each camera projector unit may find the position and orientation of its camera Ci with respect to a global coordinate system. This may be denoted with a 3 4 matrix Ei Ri Ti . To recover Ei for all an N number of camera projector units leading to 12N unknowns. This can be expressed as Ei in terms of the already computed relative calibration parameter with neighbor Cj as

In step the processing computing device may perform a complete geometric surface reconstruction of the underlying geometry of the arbitrary surface. For each camera projector unit i its projector and camera may already be calibrated with respect to the global coordinate system as determined per the previous steps in the method . The projector and camera within each camera projector unit may form a stereo pair looking at the surface geometry Si that falls in the FOV of projector Pi. The set of correspondences between the projector camera pair Ci Pi may be used along with the complete camera calibration and the projector calibration to perform a three dimensional stereo reconstruction of points to recover the surface geometry Si with respect to the global coordinate system. All the cameras Cj and projectors Pj whose field of view or image projection overlap with projector Pi may be considered. This may assure that the surface in the overlap region of multiple camera projector units may be reconstructed identically. The surface geometry of arbitrary surface may thus be reconstructed.

The geometric registration portion of method may be adjusted for N projectors and M cameras. For example every part of the arbitrary surface may be covered by at least one projector and two cameras while the number of projectors and cameras in the system may be different. The area covered by the projectors and the cameras may form a contiguous region. The cameras used may be pre calibrated for lens distortion being used in camera calibration. In an exemplary embodiment the camera lens may be selected with the least amount of non linear distortion available. For example a lens with non linear distortion that may be used may include a fisheye lens. Since each part of the arbitrary surface is covered by more than one camera steps and may be employed and steps and may not necessarily be performed to recover the complete geometry of the surface which can be then used to seamlessly register the projectors.

In step self color calibration may be performed. Self color calibration may recover a common camera transfer function t and the projector color parameters t W and B for each camera projector unit i. The projector camera self calibration may be performed as follows. All camera projector units may project white images synchronized via broadcast communications. Each camera Ci may capture this white image at multiple exposures to estimate the camera transfer function. The transfer function recovered by each camera Ci may then be broadcast to all the camera projector units which may then be averaged by each camera projector unit to find the common transfer function t. For each camera projector unit i of non overlapping areas projector Pi may project different intensities of white and the corresponding camera Ci may capture an image with the highest exposure that does not result in saturated areas. The captured images may be linearized using the function t. This image may be converted to XYZ space using an sRGB to XYZ conversion to extract the total tri stimulus value which provides an estimate of the total brightness at each camera pixel given by the addition of the XYZ channels. This value may be averaged across all the camera pixels that fall in the non overlapping region. This may provide a set of correspondences between the intensity projected by a projector and that captured by a linear camera. These values may be normalized which may provide an estimate of the projector transfer function t.

For each camera projector unit i the maximum brightness in red green and blue from Pi may be projected which may be captured by camera Ci. These images may be linearized using the previously recovered function t and then converted to the XYZ space using a 3 3 matrix for an sRGB to XYZ conversion. Thus XYZ values corresponding to the red green and blue channels at each pixel of the captured image may form the three rows of a matrix that provides the two dimensional gamut Wi at that pixel. These values may be averaged across all the pixels to find the two dimensional gamut Wi of projector Pi.

Each camera projector unit i may use camera Ci to recover the maximum brightness profile of its own projector Pi denoted by Bi. For this the same red green and blue images projected by projector Pi may be used and captured by camera Ci while recovering the two dimensional gamut Wi. The color of the captured images may be linearized and converted to XYZ space as described previously. At every pixel the total tri stimulus value X Y Z may be found which may provide an estimate of the maximum brightness at that pixel. Finding this across all the pixels in the captured image may provide Bi in the coordinate system of camera Ci. The set of correspondences Ci Pi may be used to warp Bi to the coordinate space of projector Pi.

Gamut smoothing may be achieved in an iterative distributed algorithm in two steps two dimensional gamut smoothing followed by brightness smoothing. In each iteration in each of these steps there may be a set of camera projector units R that have completed the smoothing operation and hence form a registered subregion of the display. In the first iteration R may be a singleton set consisting of the camera projector unit with the highest priority. The highest priority projector may be designated as P in the first iteration R P1 In each of the subsequent iterations a P Rwith highest priority that is adjacent to at least one element of R completes the smoothing algorithm. Thus one camera projector unit gets added to R in each iteration increasing the registered subregion of the display. The process may stop when all the N number of camera projector unit are included in R and hence the whole display is registered.

In step the online image correction on the surface may be performed such that overlapping pixels from different projectors may display the same content in a seamless manner. To generate the image Ii to be projected by projector Pi the geometric registration may be performed in two different ways view dependent geometric registration for single user applications and view independent geometric registration for multiuser applications.

In multiuser applications the image Ii may be wallpapered on the arbitrary surface . For every location X Y Z on the arbitrary surface color may be determined based on the specific characteristics of that location. A simple example may include indicating the height of the points in a relief map using different colors. In this case the color may be decided solely based on the Y coordinate of the pixels. Another example may include pasting different map layers over the arbitrary surface e.g. satellite imagery a road map or a traffic map.

To achieve a view dependent registration for single user applications the virtual camera or viewpoint with projection matrix V may be defined. This can be different from the viewpoint of any of the calibrating cameras. A two pass rendering approach maybe employed. The three dimensional scene may be rendered from this virtual camera using projection matrix V. To generate the image Ii for each projector at any pixel x y the corresponding three dimensional coordinates of the surface geometry S x y may be found. The three dimensional coordinate may be found using the projection matrix V to find the corresponding pixel from the image rendered in the first pass. The color at this pixel maybe used to generate the image Ii.

In step the image Ii may be attenuated using and which may represent attenuation maps for R red G green and B blue channels. In step the image Ii may be linearized using the projector transfer function tto achieve the color registration.

It should be understood of course that the foregoing relates to exemplary embodiments of the invention and that modifications may be made without departing from the spirit and scope of the invention as set forth in the following claims.

